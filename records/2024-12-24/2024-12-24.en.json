[
  {
    "id": 42497093,
    "title": "What happened to the world's largest tube TV? [video]",
    "originLink": "https://www.youtube.com/watch?v=JfZxOuc9Qwk",
    "originBody": "What Happened to the World's Largest Tube TV? Watch later Share Transforming a 90's Hot Wheels PC into a Modern Gaming Beast 22:04 Portable Wii & GameCube Reacts to What You Play 10:45 0:00 0:00 / 35:45•Watch full videoLive",
    "commentLink": "https://news.ycombinator.com/item?id=42497093",
    "commentBody": "What happened to the world's largest tube TV? [video] (youtube.com)564 points by ecliptik 23 hours agohidepastfavorite257 comments neilv 21 hours agoIt's a well done storytelling, but two odd thoughts/questions about it... As I was watching it, there was the drama of whether it would be saved from imminent destruction, and it actually seemed unlikely that they could, but their approach was to be... secretive about it. It turned out that they wanted it for themselves, and didn't that create a conflict of interest? By keeping it quiet, they increased the chance that they would obtain it themselves (and the YouTube story to tell about it), but increased the likelihood that the TV would be lost entirely (because other efforts wouldn't be brought)? Fortunately the gamble worked out, and the TV wasn't destroyed. There's also a possibly related matter, in how Sony stopped talking with them. Is it possible that Sony and/or Japanese government aren't very happy to learn that a possibly unique museum piece, of one of the heights of Sony achievement, was quietly removed from the country, to the US, by a YouTube influencer? I applaud preserving this rare artifact, and compliment the storytelling, but did have these couple odd thoughts. reply rjmill 20 hours agoparentFrom the interview with the TV's original owner, this seemed like his ideal outcome. The owner had seen discussions of the TV online and knew it was a big deal. But he still couldn't get rid of it until this guy came along. The owner even said he wanted the TV to go to someone who would use, appreciate, and take care of it. The video clearly demonstrates all of the above. If the TV ended up in some museum, forever powered off, that would be even more tragic in some ways. I didn't get the impression that anyone was bamboozled or cheated. reply rasz 10 hours agorootparent> But he still couldn't get rid of it until this guy came along Yep. There are always droves of \"it belongs in a museum\" crowds, but when you ask if they want it there is only silence. reply gyomu 4 hours agorootparentThe sad reality is that there are countless more things in the world that belong in museums than there is museum space/staff to properly take care of it. reply syntheticnature 3 hours agorootparentOr money. Note the Living Computer Museum basically collapsing after Paul Allen's death. reply walrus01 2 hours agorootparentThis was, sadly, a conscious choice made by Allen long before his death. Same as with his airplane and tank collection. He had plenty of time and legal advice to set it up with an endowment that could allow for its continued yearly operational budget and chose not to do so. His heirs don't care about his personal toy collection so it's been sold off. reply ghaff 1 hour agorootparentThe same thing basically happened with Malcolm Forbes' collections. It's perfectly normal for heirs to just not value things you've collected in the same way you did. reply int0x29 13 minutes agorootparentWhich is why if you actually care you create an independent and well funded organization before you die so your heirs can't sell it all off. reply ghaff 1 hour agorootparentprevOn any thread where the topic of various \"collectibles\" that surely someone wants comes up, there are tons of people who are \"you can't just toss it\" but somehow thy never want to take them off your hands themselves. I totally understand the impulse but it's just not realistic to preserve everything. reply Onavo 1 hour agorootparentIt's like the computer history museum that closed reply unscaled 18 hours agoparentprev> There's also a possibly related matter, in how Sony stopped talking with them. Is it possible that Sony and/or Japanese government aren't very happy to learn that a possibly unique museum piece, of one of the heights of Sony achievement, was quietly removed from the country, to the US, by a YouTube influencer? I didn't read that as Sony being pissed off by. Occam's razor says it's more likely to be your regular corporate dysfunction. Japanese corporations do seem as a whole to be more concerned about preserving their history than US ones, and Sony did have a small museum called ソニー歴史資料館 (the Sony Archive), but that Museum closed down in 2018[1]. Meanwhile, Toyota has six different Museum dedicated to its history and the history of the industries it participated in (including textile — Toyota was a major textile machinery manufacturer before it was an automotive company). Sony still seems to display some of the archive's content in its headquarters, but I'm unclear how much of it. In general, closing the museum shows that preservation is perhaps important, but not very high on their priority list. But even if preservation was a top goal, you still can't expect every employee on the PR department to be dedicated to that. PR departments are generally more concerned with current events, and may view such an interview as a distraction that isn't worth their time. [1] https://nakamura.yokohama/sony-history-museum-36870.html reply II2II 21 hours agoparentprevWith respect to keeping quiet about it: it may not have been selfless, but it may also have drawn so much attention to it that the owner of the set wouldn't have wanted to deal with it. After all, he had already dealt with one person who didn't follow through. As for the Sony not talking bit, it can probably be chalked up to corporate policy. Large organizations rarely let staff speak on matters when it may be construed as being speaking for the corporation. reply neilv 21 hours agorootparentTrue. Although, would a call to a museum of Japanese technology/industry, or to Sony HQ, have had a better chance to preserve it? (More likely to save it, less likely for it to be destroyed in handling and shipping.) As well as keep it in country? Perhaps the current owners will be reached by a museum, and decide to repatriate it. I imagine that the right museum home could be a win for everyone. reply Laforet 20 hours agorootparentThe other parties you mentioned would probably have less motivation to preserve it, let alone restore it to a fully functional state. I find it rather bizarre that many posters here seem to think that it’s morally preferable for the TV set to rot in Japan rather than getting the proper care in the hands of an American collector, all because of some imaginary cultural baggage. reply patcon 20 hours agorootparentHeh it strikes me that while the stakes of this \"relic\" are kinda low, it echos the conversations about institutions like the British Museum possessing historic artefacts :) some claim there is moral argument for it keeping its artefacts, because Britain can best preserve them and protect them from damage. Responsibility and autonomy to preserve one's own heritage (with the associated risk of failing to do so) is a longstanding ethical dilemma between cultures, and the answers aren't so clear imho! (This argument is much more compelling for museums, rather than Sony) reply Laforet 19 hours agorootparentYes, I am aware of those arguments and I am inclined to agree with you. Compared to cultural artifacts which are mostly neutral in terms of externalities, relics of the industrial era suffer more from the cobra effect. Others in this thread have bought up the future of ICEs and classic car preservation. Back in the early 2000s the US government offered people cash incentives to dispose of their fuel inefficient cars, and by disposal they meant running the engine with an abrasive liquid instead of oil until it is totally ruined beyond repair. Mechanics will tell you horror stories of rare car models being destroyed this way so the owners can claim a few hundred bucks from the DOT. I'm sure car collectors had a field day back then but with such a glut in the market they could not save everything that's worth saving. Shank Mods was able to obtain a copy of the service manual in English from somebody in the US. This fact probably means that the TV was sold on (or imported to) the domestic US market for a while. (Sony have always allowed individuals to order parts through an authorised service centre, and the latter often insist on requesting a repair manual first even if you are 100% sure of the part number) It's very likely that a number of them existed in the US only to be unceremoniously thrown out by their owners when LCD TVs became more popular. I bet nobody batted an eyelid when that happened. reply to11mtm 17 hours agorootparent> Others in this thread have bought up the future of ICEs and classic car preservation. Back in the early 2000s the US government offered people cash incentives to dispose of their fuel inefficient cars, and by disposal they meant running the engine with an abrasive liquid instead of oil until it is totally ruined beyond repair. Mechanics will tell you horror stories of rare car models being destroyed this way so the owners can claim a few hundred bucks from the DOT. I'm sure car collectors had a field day back then but with such a glut in the market they could not save everything that's worth saving. But what else happened with that? The glut ended. Used cars got more expensive relative to quality. And now the cost of a 'reliable used car' is far more than inflation adjusted for the time passed. getting back on topic... > unceremoniously thrown out by their owners when LCD TVs became more popular. I bet nobody batted an eyelid when that happened. IDK about all that, during the 'LCD Phase-in' everyone I knew either donated theirs and/or moved CRTs into smaller rooms when they replaced a working one. Especially if it was 'Decent' TV, i.e. Progressive scan and component input... Let alone if the thing cost as much new as a very nice car of the day. The sheer responsibility of it (thinking more, you really can't throw this thing out unceremoniously, at minimum it's part of a house or business space eviction proceeding...) has some weight, ironically. reply pantalaimon 5 hours agorootparent> everyone I knew either donated theirs and/or moved CRTs into smaller rooms when they replaced a working one. But you can’t do that with a 400lbs behemoth of a TV, it would fill the entire room. This beast is highly impractical and still only 480p. Even those smaller CRTs got disposed of quickly as soon as the 2nd generation of flat screens arrived as they already took up way too much space. reply Laforet 16 hours agorootparentprev> everyone I knew either donated theirs and/or moved CRTs into smaller rooms when they replaced a working one. That might have happened for a while but by 2008-ish CRTs were being dumped left right and center. My city runs a annual kerbside collection program for large appliances and furniture, and I distinctly remember metal scavengers cruising the street gutting old CRTs people have left out for the copper coils, leaving whatever remains to be collected as hazardous e-waste. Around the same time, my parents got rid of a 16:10 CRT IDTV they bought in the 90s and semi-forced me to throw out a 21 inch IBM P275 I had because \"it's using too much power\". In any case I doubt any corporate (or rich household) owner of a 47 inch CRT back then would think too much about replacing it with a larger screen that took up less space. After all it's just another piece of asset that has depreciated to zero value on their books. reply to11mtm 16 hours agorootparent> That might have happened for a while but by 2008-ish CRTs were being dumped left right and center.That might have happened for a while but by 2008-ish CRTs were being dumped left right and center. Maybe I just grew up poorer than you but it took longer than that in my world. > my parents got rid of a 16:10 CRT IDTV they bought in the 90s Yeah meanwhile some of us had to deal with a Zenith TV that would 'jump' with a PS1 and other consoles on the RF/AV output because 'lord knows why'. > and semi-forced me to throw out a 21 inch IBM P275 I had because \"it's using too much power\". Given the other context of your comments I doubt this is a confession of contribution of hubristic affluence contributing to our modern disposable society but I feel like this underscores the point I'm trying to make in my reply. Resourceful not-well-off people used to really appreciate repairable things, and the worst thing C4C did was get rid of a lot of not-fuel-efficient vehicles that were at least cheap to repair. The video of that TV and the pair further underscores it. Everything on decently laid out boards. Nowadays an LCD tv, sometimes a part can go bad and it's so integrated that even 15 years ago it could be a 30 min solder job, nowadays it's cuck the whole shebang. > In any case I doubt any corporate (or rich household) owner of a 47 inch CRT back then would think too much about replacing it with a larger screen that took up less space. After all it's just another piece of asset that has depreciated to zero value on their books. Corporate maybe but I'd guess any smart corporation would try to load the 'disposal' costs of a 440 pound object onto the taker somehow. Similar for any rich household that wanted to keep wealth for more than a generation or two. reply Retric 6 hours agorootparent> not-fuel-efficient vehicles that were at least cheap to repair. You don’t need to drive that much for fuel inefficiency to get really expensive. Even 10k miles/year which is well below average at 10MPH vs 30MPH @ 3$ / gallon is an extra 2,000$ / year, and adjusted for inflation gas is currently fairly cheap. Inflation adjusted in 2011 and 2012 gas was over 5$/gallon. We might see consistent low gas prices intended to delay the EV transition (or the could spike), but these cars were already old 15 years ago when the program happened. reply Laforet 15 hours agorootparentprev> Given the other context of your comments I doubt this is a confession of contribution of hubristic affluence contributing to our modern disposable society but I feel like this underscores the point I'm trying to make in my reply. Let me assure you that none of what I said was meant to diminish your point of view which I agree with mostly. What I was trying to convey was that people’s mindsets were rather different during the last decade of CRT. CRT had been around since the end of WWII, it may have gotten bigger over the years but the form it took on largely remained the same so there was a sense of continuity as people handed down old TVs when they got something nicer. When cheap LCD TVs came to the market it represented something more akin to a paradigm shift as people with limited space at home could now easily own screens 30 inches and up. My parents are actually rather frugal with my dad borders on being a tech hoarder who insist on keeping every single cell phone and laptop he ever owned somewhere in his garage. However even he was unable to justify the sheer bulk and running cost of CRT TVs back in that period. Even if he were to give it away there would have been very few takers of any. Therefore it’s not inconceivable that this model could have been sold in the US or even few more places outside Japan. Most of them simply disappeared without a trace because at some point they were probably worth less than the space it occupies, and people were overly eager to embrace the flat panels without realising that they are not getting some of the utilities back. reply c22 2 hours agorootparentI keep all my old cell phones too, but I had to get rid of a run of them from around 1998 - 2008 because the plastic started turning sticky a while back. reply eru 15 hours agorootparentprev> Others in this thread have bought up the future of ICEs and classic car preservation. Back in the early 2000s the US government offered people cash incentives to dispose of their fuel inefficient cars, and by disposal they meant running the engine with an abrasive liquid instead of oil until it is totally ruined beyond repair. Could you elaborate? reply Laforet 15 hours agorootparenthttps://www.businessinsider.com/henry-blodget-this-is-too-ab... reply eru 15 hours agorootparentThanks! Those were wild times. I remember they also had a similar scheme in Germany. Absolute madness (and that's even if you ignore the useless damage to old cars.) They should have just printed more money to juice the economy, instead of these wild schemes to give subsidies to specific industries. reply rasz 9 hours agorootparentprevhttps://en.wikipedia.org/wiki/Scrappage_program Cash for Clunkers - 700,000 cars SCRAPPED by the USA Government https://www.youtube.com/watch?v=2ZMJ_oNtzzE UK had its own program in 2009 https://www.banpei.net/2010/04/07/wtf-mr2-sw20-in-british-ca... All the cars lost to the 2009 Scrappage Scheme - The UK SCRAPPED all these rare cars?! https://www.youtube.com/watch?v=NLLNOUUqCUc reply ToucanLoucan 19 hours agorootparentprev> Back in the early 2000s the US government offered people cash incentives to dispose of their fuel inefficient cars, and by disposal they meant running the engine with an abrasive liquid instead of oil until it is totally ruined beyond repair. So. Fucking. Stupid. As though Joe Consumer with a V8 Mustang he puts a few thousand miles on per year is the boogeyman of climate change, and not, hell just off the dome: - Every standing military on planet Earth - The global shipping industry - The fossil fuel industry reply Reason077 14 hours agorootparent> \"As though Joe Consumer with a V8 Mustang he puts a few thousand miles on per year is the boogeyman of climate change\" Scrappage schemes target the smokey, rusty shit-boxes that are worth next to nothing. Not Joe Mustang's prized V8, which would be worth far more than the value of the incentive anyway. And when it comes to old cars, reducing local air pollution is often the major concern. Not just climate change. reply ssl-3 11 hours agorootparentCash for Clunkers did exactly what it was intended to do: It screwed up the used car market for a very long time, simply by decreasing supply while demand remained. People still needed cars, and everything is relative. When used car prices go up relative to that of new cars, then new cars become relatively inexpensive. This helps sell more new cars. And back in the time of \"too big to fail\" auto industry bailouts, selling more new cars was kind of important. edit: And remember, there were restrictions for Cash for Clunkers. The car had to be less than 25 years old, it had to run, and it had to have been registered and insured for the last 12 months. It was deliberately designed to thin the pool of functional used vehicles. This program claimed revered cars like Audi Quattros and BMW E30s...along with V8 Mustangs. And once turned in, they were all quite purposefully destroyed: Sodium silicate replaced the engine oil and they were run at WOT until they seized, and then they were crushed just to be sure. reply jmb99 14 hours agorootparentprevOr the manufacture of new vehicles to replace perfectly serviceable old ones. reply layla5alive 18 hours agorootparentprevAnd agriculture reply RhodesianHunter 19 hours agorootparentprevI just don't think ancient artifacts are comparable to an old TV. reply bratwurst3000 18 hours agorootparenthmmm i dont know. ancient artifacts sometimes highlight the technical and artistic possibilities of the time. In my opinion this tv represents very good consumer culture in the 80s as do amphitheaters in rome and greece their consumer culture. reply rtpg 18 hours agorootparentprevThough I don't think anyone would have wanted it, I think there's a bit of a false dichotomy there. Maybe in theory there would have been a place for this in a curated space in Japan... if not for it being so massive at least. Ultimately if it was a TV designed in Japan, having it on display at a local tech museum would be nice. I just don't know where it would go that could deal with the space and the weight. Closest thing I could think of is the NTT museum, which is ginormous... but it's mostly about NTT's stuff. \"Some other company in Japan made big TVs\" is a bit less interesting than, say, some older tabulation machines they have there. reply rtpg 18 hours agorootparentprevTo be quite honest I don't think there are many museums that would want that CRT. CRTs are notoriously a massive pain in the ass. Retro computing museums and the like have their CRTs, but they don't really have the space for it. It probably does make sense in the house of a massive hoarder. reply numpad0 18 hours agorootparentprevJapan's really ill situated for industrial museums. Land is at premium, summer steam is brutal, disasters are routine, and public support is weak. It's also just one of the world's best for Sony - they make a lot of bests(with many asterisks too). One thing I only understood after I've bought a 3D printer is, someone wanting an obsolete product is weird from creator perspective. I still fully understand consumer side sentiments, and also am aware of vital importance of reference data archives, but I'd rather want audiences to seek the latest and greatest than asking me about a shelf bracket that I stopped making some time ago. So I think it's an okay outcome. The TV lives on. Someday Sony might buy it back, or it might get transferred to some other museums. That's good enough. The only stretch goal left is an interview with its creators or their autobiography(s). But that would be a cherry on top. reply thrdbndndn 17 hours agorootparent> Japan's really ill situated for industrial museums. Land is at premium, summer steam is brutal, disasters are routine, and public support is weak. Japan’s suitability for industrial museums can be debated, but saying “summer steam is brutal, disasters are routine” as reasons is ridiculous. This is the 21st century, not the Middle Ages. Besides, Japan already has plenty of industrial museums. reply shiroiushi 16 hours agorootparentprev>Japan's really ill situated for industrial museums. Land is at premium, summer steam is brutal, disasters are routine, and public support is weak. This makes absolutely no sense. Japan is full of museums of all kinds, including really weird ones you'd never see in America. Not far from me, there's a museum of miniatures, a museum about sewers, a museum about tap water, a museum about subways, and a museum with an indoor recreation of an entire village from ~300 years ago. And the summers here are better than most southern US states like Florida or Arizona, and disasters much less routine than Florida. reply sneak 19 hours agorootparentprevHe tried contacting Sony several ways, but Sony dgaf about anything these days. reply Aurornis 19 hours agoparentprevThey posted on Twitter to find people who wanted to get involved > With no time to lose, Shank posted a call for help on Twitter, hoping someone in Osaka could investigate. Enter Abebe, a stranger who volunteered to check the location. The restaurant was about to be demolished. I don’t see any problems with this process or outcome. I think you’re comparing this outcome to an imagined alternative reality (going into a museum) that wasn’t even an option. reply msephton 10 hours agorootparentExactly. The idea alone is worthless. The guy in the video has the idea and executed on it. reply philistine 13 hours agorootparentprevUltimately, Indiana Jones does not exist. Only collectors. reply mrob 18 hours agoparentprevIf I was in charge of a big corporation that still made displays, I would not want to preserve CRTs because it could hurt the narrative that modern technology is strictly superior to old technology. If people thought about CRTs in a positive light they might realize that no modern display can match them in latency and motion quality when it comes to displaying 60fps content (as found in console and arcade games). I'd prefer that all CRTs were destroyed and forgotten. reply jpsouth 17 hours agorootparentI don’t think any large screen manufacturer would give a second thought to this, the average consumer will still want the 4K, HDR, flat screen that is wall mountable. The market the CRTs would steal is practically non existent, surely. I’d love this in my house for retro gaming purposes, but I’d still have my LG C/Gx or Samsung N95x or whatever the newest, fanciest models are for movies and modern use cases. reply Springtime 17 hours agorootparentprevAs much I appreciated the experience of no input latency CRTs they always gave me headaches after some hours due to the refresh rate flicker. LCDs were an immense relief even despite having very noticeable input latency for the same Hz (eg: cursor movement, which one gets accustomed to). reply geerlingguy 15 hours agorootparentAnd that high frequency whine that many people (myself included) can hear, that gets infuriating after a few hours of a TV remaining on. reply karashi 11 hours agorootparentprevI’d compare this to large format film cameras. By raw resolution, large format film cameras are still far and above what is achievable digitally. Yet, of course, no one would argue that they pose a threat to the practicality and efficiency of digital, and few people appreciate/care about/need so much resolution. reply pantalaimon 5 hours agorootparentAnd those cameras don’t take up a good part of the room! reply fallous 11 hours agorootparentprevI know I moved into the LCD monitor era kicking and screaming because the CRTs I used with my computers were far superior for text sharpness and didn't cause me near the eye-strain when doing long programming sessions. reply to11mtm 17 hours agorootparentprevGood point TBH. reply shiroiushi 16 hours agorootparentprevThere's no need for this. If you want to make sure consumers don't want to return to CRTs, all you have to do are the following: 1) point out how heavy they are. Give them a facsimile to lift to show them, after making them sign a waiver that they may permanently injure their back doing so. 2) show them how deep they are, and how far away from the wall they must sit because of this. 3) show them two power meters, showing the power consumption of a CRT and a modern LCD for comparison. Also show the actual costs for that power, and how much typical usage of these displays will cost per day and per year. The last one alone should dissuade most people from wanting to go backwards. Most people don't give two shits about latency, and modern LCDs with >= 120 fps capability already exist. reply macintux 14 hours agorootparentI nearly collapsed while moving my CRT out of the house. I have no recollection of the size, but putting it on my shoulder by myself was a terrible idea, and I’m very lucky I didn’t injure myself. Nothing could persuade me to voluntarily go back to CRTs. reply shiroiushi 14 hours agorootparentThe only really good reason I can see to use a CRT is because you want to fix/rebuild one of the old 1980s vector arcade games (like Tempest or Star Wars) and want it to be a truly authentic reproduction. reply philistine 13 hours agorootparentprevIt's even easier than that. You can get a 43-inch LCD for 300$. CRTs, with their inherent complexity, can NEVER compete on price. reply shiroiushi 13 hours agorootparentYeah, I left out the price aspect. Forget a 43-inch CRT: how about a 85-inch CRT? You can get an LCD (or better yet, OLED) TV this size easily for not that much money. But it's basically impossible to even make a CRT this size, and even if you could, it would be so expensive, heavy, and large it would be completely impractical. Lots of people now have 50-85\" TVs in their living rooms, but those are all impossible for CRT technology. However, the OP was trying to claim CRTs are superior because of latency and refresh rate for gaming applications, specifically, so I was just focusing on those aspects. The refresh rate part is silly; high-refresh-rate LCDs and OLEDs are common now. The latency part might have some validity, but compared to all the other factors it's really not that important. reply mrob 3 hours agorootparentFor maximum motion quality the refresh rate needs to match the frame rate. Modern gaming LCDs can beat CRTs in refresh rate, but only a minority of games support such high frame rates. For any given refresh rate the CRT will always have better motion quality. reply xbmcuser 13 hours agorootparentprevnot really true anymore as the latest oled tech surpasses crt in almost every spec. And the spec it does not the difference is detectable by devices not human senses so practically makes no difference. reply mrob 5 hours agorootparentThe difference isn't subtle. This is perfectly sharp and clear on a CRT, but blurry on an OLED: https://www.testufo.com/ reply LastTrain 5 hours agoparentprevThe email he shared that he was sending to Sony was obnoxious “this is a chance for some wicked awesome free PR for Sony..” so it is kind of no wonder they stopped talking to him. Other than that, he never said he was doing it for the good of humanity or anything, he just wanted it and found a way to make it happen, I admire the pluck. reply nharada 42 minutes agorootparentIt might be pretty on the nose but I don't see why that would make them stop talking to him. Wouldn't that be the reason they'd approve a corporate interview in the first place? I doubt they'd do it for no reason reply thousand_nights 19 hours agoparentprevi don't get the skepticism, yes a youtuber did a thing but without them probably no one would have cared and the TV would've ended up destroyed in the rubble of the building he even went to the lengths of calling up different CRT experts trying getting them to fix it all this negativism just feels like older people being all \"zoomers bad\" because the medium is not what they prefer. maybe we should just be happy to pass the torch and glad that younger generations even have interest in this sort of thing reply tantalor 20 hours agoparentprev> conflict of interest (nit) Please don't use \"conflict of interest\" that way (casually). It should only apply to situations where there are actual legal or ethical obligations in opposition. Nobody owes the online CRT community anything. reply neilv 20 hours agorootparentPoint understood, but do you think there's no obligations to communities or societies, other than those codified in law, contracts, or some (professional?) ethics? If those other obligations existed, could we say \"conflict of interest\" about them, or is there a better term or phrasing? reply giantfrog 18 hours agoparentprevThat’s not a conflict of interest, it’s just an interest reply to11mtm 17 hours agoparentprev> It turned out that they wanted it for themselves, and didn't that create a conflict of interest? By keeping it quiet, they increased the chance that they would obtain it themselves (and the YouTube story to tell about it), but increased the likelihood that the TV would be lost entirely (because other efforts wouldn't be brought)? Based on the timeline there was limited time to act. Additionally, given they did some public 'reach-out' posts (that wound up finding them the thing) there were theoretically others that could have tried to handle it via their own channels. Per the YT video's 'sponsorship', I'll note that shipping a ~450 pound TV and ~150-200 pound stand overseas in general is not a cheap, or easily logistical task given the timeframe. Esp if it's on the 2nd floor of a building to start (can't just do a simple hand hydraulic lift for the hard parts.) > There's also a possibly related matter, in how Sony stopped talking with them. Is it possible that Sony and/or Japanese government aren't very happy to learn that a possibly unique museum piece, of one of the heights of Sony achievement, was quietly removed from the country, to the US, by a YouTube influencer? Overthinking it perhaps. Sony has a lot of divisions and it's hard to get live assistance from them even if you are a current user of their products, at least speaking from personal experience with a couple different lines. ----- That said, the YT video drew things out way too much for drama's sake and it made me glad I have ad-free. reply eboynyc32 16 hours agoparentprevOh god who cares!! reply ahartmetz 17 hours agoparentprevI also had an odd feeling avout several other enthusiasts travelling to the guy's place presumably at their own cost, spending a lot of time to repair / tune up the thing, and in the end, our hero just adds it to his collection. reply roywiggins 51 minutes agorootparentGetting a chance to work on this unique device is probably its own reward for them. reply cheema33 3 hours agorootparentprevIf I were passionate about something, I would fly in to play with it and tweak it on my own dime. Did you get the impression that somebody was swindled in this process? reply kcb 20 hours agoparentprevEh, what standard are we holding people to? You ever shop for a used car(maybe even some rare spec of a sports car)? When you finally found a good deal did you shout in the streets and put out an ad to make sure no one else is around to make a greater offer? reply ranger_danger 19 hours agorootparent~~Plus, who plays out a mental moral dilemma with a historical museum any time they want to buy something?~~ Actually I think this might be a false equivalency OP, because this isn't just any old used car. I think it's fair to at least stop and question whether this should go to some greater good or not. reply pantalaimon 5 hours agorootparentIt’s the equivalent to an old sports car that was impractical when it was first released, but the pinnacle of its time. reply ranger_danger 19 hours agoparentprevIt's interesting that they say they had such a hard time finding help, I have never heard about this entire endeavor until now, and the video mentions them desiring contacts at Sony with the display division, which I happen to have, and would have helped if I had known about it. reply 2muchcoffeeman 21 hours agoparentprevI agree. At the beginning I thought this was a conservation effort. Turns out to be the modern equivalent of colonisers stealing local artefacts. Why export this at all!? reply xp84 20 hours agorootparentToday I learned that carefully preserving an artefact that neither its owner nor anyone else in its origin country wanted = “colonizers stealing.” reply throwawaysleep 20 hours agorootparentThis is the same for a lot of supposed “theft” by museums. Lots of “priceless” objects now were at the time junk, so they were thrown away. reply TacticalCoder 19 hours agorootparent> This is the same for a lot of supposed “theft” by museums. Not to mention that in many countries art pieces predating a certain era are simply destroyed (on the ground that they're older than a particular religion). And most of the pharaohs' tombs were pillaged and unique pieces were melt by actual thiefs for their gold. These evil, evil, museums displaying these around the world for any visitor to see when you think these could have been melt for gold by thieves or simply destroyed because they were impure! Evil western civilization. That western civilization is so evil it must be replaced! reply Loughla 19 hours agorootparentAlternate theory, both are true. Western societies took advantage of multiple other societies to plunder their treasures. Those same societies didn't have the infrastructure and/or care to preserve these things themselves. Sometimes two things can be true. reply throwawaysleep 19 hours agorootparentIf they do not care about them, they are not \"treasures\" by the standards of those cultures but rather \"waste.\" reply HeWhoLurksLate 17 hours agorootparentNot necessarily true- the majority of people don't know about 3-2-1 backup strategy and I've seen hundreds of \"help! my { phoneSD cardcomputer } died and I lost all my family photos\" posts reply prmoustache 7 hours agorootparentprevConservation or not, that TV has been given out by its owner so there is no theft involved. Neither has it been moved out of the country by colons or illegaly. And it is a damn TV. A big one for sure but it isn't Moctezuma II headdress nor are those Devatas carved from Banteay Srei cambodian temple. reply throwawaysleep 21 hours agorootparentprevThis example is what makes much of the \"stealing\" claim bogus, both for this and many artifacts. The Japanese owner wanted it gone and considered it trash. It wasn't some beloved item. Even Sony didn't care. And so much of what is considered \"stolen\" was given away by someone in that culture as trash. reply yehat 8 hours agorootparentThat's the standard excuse of a thief. \"I'm not stealing it, I'm saving it\". Better stop the excusing. reply throwawaysleep 8 hours agorootparentExcept that the owner is the one giving it away. The current owner doesn't claim theft. The only people claiming theft are a third party that never owned the property in question or at the time gave it away freely. reply simonw 22 hours agoprevHere's the (fantastic) YouTube video that this is a recap of: https://www.youtube.com/watch?v=JfZxOuc9Qwk reply tptacek 21 hours agoparentWas this done by the same person as made the video, or is it blogspam of it? (I'm asking because people are complaining about it elsewhere). reply Sniffnoy 20 hours agorootparentI have no information that you don't, but it looks to be blogspam of it -- it always refers to Shank as a separate party, it doesn't claim to have had any involvement in what happened. reply dang 14 hours agorootparentOK, we'll switch to the youtube link above. (Submitted URL was https://obsoletesony.substack.com/p/the-journey-to-save-the-....) Thanks all! reply hn92726819 20 hours agorootparentprevHere's the real one: https://m.youtube.com/watch?v=JfZxOuc9Qwk reply zdw 19 hours agorootparentDiscussion here: https://news.ycombinator.com/item?id=42489600 reply jader201 20 hours agoparentprevSuch a well done video, thanks for sharing. I even happily watched a very well executed sponsor ad. reply rwmj 21 hours agoparentprevA similar but not as large (merely 37\") CRT: https://www.youtube.com/watch?v=5o7R8oJEZhY reply indigoabstract 22 hours agoparentprevThanks, really good story. It unfolds like an Indiana Jones movie for priceless antique CRTs. reply a12k 22 hours agoprevAs a child in the early 90s (maybe 1993), I nearly got crushed under one of these trying to connect my Nintendo to the AV cables on the back. It was against the wall in an alcove and the only way to access was to rotate it slightly and lean it forward to reach the connections on the back (which I couldn’t see, only feel). It tipped off the shelf and onto me, partially supported by the shelf and partially by me. I didn’t want to get in trouble because it was so nice, so I just kind of squatted there pinned under it trying to lever it back. Thankfully my dad walked by, noticed, and kept into action. And here I still am today. reply unsnap_biceps 21 hours agoparentAre you saying as a child you were able to move and hold up a 400 lbs tv or are you talking about a smaller tv? reply Avamander 21 hours agorootparentTilting or rotating a TV is different from lifting it (especially if there isn't much friction by design?) and might require much less force. reply a12k 21 hours agorootparentYes. This was more like continuously jerking my weight backwards with all my might while holding a front corner to maneuver the TV inch by inch into a diagonal orientation, until on the last jerk it went an inch too far. reply a1o 21 hours agorootparentprev400 lbs -> 181 kg reply semiquaver 19 hours agoparentprevJust curious, are you certain it was this model or just “a large CRT?” This model retailed for $40,000 in the US (100K adjusted for inflation) and only a small number (reportedly in the low double digits) were ever sold. https://www.chicagotribune.com/1990/03/06/to-get-the-big-pic... reply jsheard 18 hours agorootparentAs the video mentions this model is so incredibly rare that previously there were only two known photos of retail units in the wild - and one of those photos was of the very unit that the guy in this story eventually managed to acquire. The other photo is a mystery, nobody knows who took it or whether that unit is still intact. reply pantalaimon 4 hours agorootparentIt was also incredibly expensive - most people rich enough to buy it wouldn’t typically post their living room online in the early 90ies reply stronglikedan 2 hours agorootparentprevI think most will infer \"a large CRT\" after reading OPs comment. reply RicoElectrico 19 hours agorootparentprevWow, goes to show how people are gullible to their \"memories\", never stopping to question them. This could explain the \"communism was better\" ramblings you get from old farts quite well... I... just don't get it. What I remember from my young is not that much but it all definitely happened and does not need any artistic license. reply skhr0680 18 hours agorootparentIt's a great understatement to say that the end of the Eastern Bloc could have been handled a whole lot better, especially from the perspective of people who would have been established or even happy with their lives under Communism: age 40+, educated, successful career at the Trabbi factory, just got to the top of the waiting list for an apartment, etc. * This comment is not an endorsement of totalitarian governments reply IAmGraydon 17 hours agorootparentAre you guys lost? We're talking about a very fucking large TV here. reply rob74 20 hours agoparentprevCrushing was probably not the only danger you were in there - even if the thing would have just fallen and imploded next to you, that could have been pretty dangerous as well... reply userbinator 15 hours agorootparentIt is very difficult to break a CRT from the front, even deliberately. The neck is fragile but a CRT TV falling on its face (which is what tends to happen as they're very front-heavy) is far more likely to break the case or the boards inside than the tube. reply garbagewoman 13 hours agoparentprev“One of these” $40,000 tvs, sure reply pests 11 hours agorootparentProbably meant the category of CRT TVs, not that exact model. reply Tiberium 22 hours agoprevAm I overthinking it or is this blog post heavily AI-edited? The way the text is very similar to what modern GPT models would give you. This paragraph was the last straw that made me think so: >This story isn’t just about a TV; it’s about preserving history and celebrating the people who make it possible. Shank’s journey serves as a reminder of the lengths we’ll go to honor the past and connect through shared enthusiasm. Also >Shank Mods’ video is not just a celebration of retro tech but a love letter to the communities that keep these technologies alive. From the daring extraction to the meticulous restoration, every moment of this story is a testament to what can be achieved with determination and collaboration. reply vunderba 20 hours agoparentThat last one is a huge tipoff: > \"Shank Mods’ video is not just a celebration of retro tech but a love letter to the communities that keep these technologies alive. From the daring extraction to the meticulous restoration, every moment of this story is a testament to what can be achieved with determination and collaboration\" Not just a X but a Y From the A to the B GPT LOVES this kind of verbose garbage - it's the non-fiction equivalent of purple prose and reads like a 6th grader desperately trying to pad out their MLA-formatted 5 paragraph essay. reply sentientslug 20 hours agoparentprevYes, it’s obvious AI writing. The fact that some people can’t tell is actually scary. Eventually (soon?) none of us will be able to tell. reply sourraspberry 19 hours agorootparent> The fact that some people can’t tell is actually scary. It really is, and I see more and more of it in Reddit comments, and even at work. I had some obvious AI writing sent to me by a lawyer on the other side of a dispute recently and I was pissed - I don't mind if you want to use it to help you (I do myself), but at least have the decency to edit so it doesn't read like ChatGPT trash. reply Aurornis 12 hours agorootparent> It really is, and I see more and more of it in Reddit comments, and even at work. I have a morbid fascination with how bad Reddit has become. LLMs have supercharged the problem, but even before ChatGPT became popular Reddit was full of ragebait, reposts, lies, and misinformation. The scary and fascinating thing to me is that so many people eat that content right up. You can drop into the front page (default subreddits or logged out) and anyone with basic adult level understanding of the world can pick out obvious lies and deliberate misinformation in many of the posts. Yet 1000s of people in the comments are getting angry over obviously fabricated or reposted AITA stories, clear ragebait in /r/FluentInFinance, and numerous other examples. Yet a lot of people love that content and can’t seem to get enough of it. reply geocrasher 16 hours agorootparentprevIt won't be long before you'll have people who learn English with ChatGPT and then it'll get even more confusing. reply smt88 12 hours agorootparentThis is certainly already happening because TikTok and YouTube are packed with AI content reply userbinator 15 hours agorootparentprevIf you're below-average, AI writing looks great. If you've above, it looks horrible. That goes not just for writing but anything else created by AI --- it's the average of its training data, which is also going to be average in quality. reply avidiax 15 hours agorootparentI didn't notice that this was AI myself. I tend to start skimming when the interesting bits are spread out. There's two variations of this that are very common: * Watering down - the interesting details are spread apart by lots of flowery language, lots of backstory, rehashing and retelling already established points. It's a way of spreading an cup of content into a gallon of text, the same way a cup of oatmeal can be thinned. * High fiber - Lots of long-form essays are like this. They start with describing the person being interviewed or the place visited as though the article were a novel and the author is paid by the word. Every person has some backstory that takes a few paragraphs. There is some philosophizing at some point. The essay is effectively the story of how the essay was written and all the backstory interviews rather than a treatise on the supposed topic. It's basically loading up your beef stew with cabbage; it is nutritive but not particularly dense or enjoyable. Both are pretty tedious. AI can produce either one, but it can only hallucinate or fluff to produce more content than its inputs. As such, AI writing is a bit like a reverse-compression algorithm. reply Pet_Ant 20 hours agorootparentprevMore likely it'll be normalised until we all start to think of it as normal and start to write like that ourselves. reply rsynnott 20 hours agorootparentI doubt it, because it is a style that people who’re bad at writing already use. Like, our magical robot overlords did not make it up wholesale; plenty examples of that particular sort of stylistic suck were already out there. (I am semi-convinced that the only job that’ll really be impacted by LLMs is estate agent copywriters, because estate agents already love that awful style.) reply jandrese 19 hours agorootparentprevI can never be 100% sure it is AI writing or someone who cheated their English homework using AI and thinks normal people write like that. reply KerrAvon 18 hours agorootparentIt's never the latter until the current crop of high school students graduate. Most students couldn't have used it until 2022; it didn't exist. reply walrus01 2 hours agorootparentprevWhat's worse is that this obvious AI writing is going to become a part of new AI training datasets, as it gets scraped, so we'll end up with some kind of ouroborus of AI slop. reply thinkingemote 21 hours agoparentprevIt's in the third person and is frequently mentioning the third party in most sections and it appears (to me) to be written by that same party. The third party is presented as a human entity but not particularly human. There's nothing in the article about that entity which one should expect in such a format. Feels like it's written as if it's a press release. Normally a press release would have notes for editors with biography and additional info. Feels off. reply Tiberium 21 hours agorootparentI think you should try using GPT-4o for writing text - it'll generate blog posts in a style that's very similar to this. Just a random example: https://chatgpt.com/share/6769d176-af34-8006-9c47-e40f1efca0... You can clearly see lots of similarities, especially the \"Why it matters\" section. Of course the substack post fed the actual video transcript to the model to write or refine the contents, but it's still very obvious. reply thinkingemote 21 hours agorootparentYes I wouldn't be surprised, you are not overthinking. reply infotainment 22 hours agoparentprevI really enjoy reading this blog in general, but I do agree with you that it absolutely has that AI-assisted-writing style. Looking at this and other posts, they often feel like if one prompted ChatGPT with something like \"please write a timeline of the Walkman\". I think they may want to dial it back for a more natural feeling. reply Animats 21 hours agorootparentIt has that \"stretched to maximize Youtube engagement revenue\" feel. There is apparently an SEO advantage to \"long form\" Youtube videos. You also have to hit 4,000 viewing hours per year before Google pays out.[1] So there's an incentive to bloat videos with background material. That's why so many Youtube videos have a collection of stock photos and clips at the beginning giving a history of something, before they get to the new thing. Now we need local crap blockers which will delete that crap. Good AI problem. [1] https://www.72works.com/marketing/how-long-should-a-youtube-... reply Eisenstein 19 hours agorootparentSolving problems caused a tech company using measurements that were turned into requirements by using AI to get around the effects of them is hilarious. I love the upcoming tech arms racing which is just going to be developing new technical solutions to problems cause by technical solutions. It's more convenient because it removes the inconvenience created by the thing that makes your life more convenient? At what point is it not worth it any more and people just avoid it all and start reading books again? I think something like that is a probable (though hyperbolically illustrated) outcome. reply ahaucnx 20 hours agoparentprevYes, I immediately noticed that it's likely AI written and I thought that this really discounts an otherwise great story. What I mean is that if the author does not put in the least effort to make it not AI sounding, how much does the author actually care about his/her content? reply Retr0id 21 hours agoparentprevMy personal verdict is \"not AI\" reply avidiax 14 hours agorootparentI see lots of passages that scream AI. Some selections: > Retailing for $40,000 (over $100,000 today), it pushed the boundaries of what CRTs could achieve, offering professional-grade performance. \"Professional-grade\" huh? There are professional TV watchers? It's not a studio reference monitor. It's just a regular TV but bigger. > The urgency was palpable. Where does one palpate urgency? > Against the odds, Abebe found the CRT still in place, fully operational, and confirmed that the restaurant owner was looking for a way to get rid of it. We establish later that it wasn't fully operational at all. And what odds? We didn't establish any. The TV is rare, and we later establish that the original owner knew it. > What follows is a race against time to coordinate the TV's extraction, involving logistics experts, a moving team, and a mountain of paperwork. > Abebe, the man who made the rescue possible, turned out to be the director of Bayonetta Origins: Cereza and the Lost Demon. His selfless dedication during the final months of the game’s development exemplifies the power of shared passions. Cool detail, but irrelevant, even if followed by breathless admiration fluff. > This story isn’t just about a TV; it’s about preserving history and celebrating the people who make it possible. I don't recall anybody being celebrated. They got a cool TV. Cool. reply russelg 14 hours agorootparent>I don't recall anybody being celebrated. They got a cool TV. Cool. The original video gives plenty of appreciation to the people who made moving it possible, the shop owner, and the people who restored it to perfect working condition. reply Retr0id 6 hours agorootparentprevThe TV was bought and used by a business, it doesn't get much more \"pro\" than that (someone should remind Apple's marketing team). But we could argue about semantics all day, humans make vaguely inaccurate statements all the time. > Where does one palpate urgency? Most frequently in metaphors. https://books.google.com/ngrams/graph?content=urgency+was+pa... reply rzzzt 5 hours agorootparentprev> Where does one palpate urgency? Ask an urologist! reply overboard2 21 hours agoparentprevIt does seem strange, but there's a decent chance the author is ESL or just has an unusual writing style reply sourraspberry 19 hours agoparentprev100% AI drivel. You take the video transcript, ask ChatGPT to write a short blogpost about it, and this is what you get. reply xvector 22 hours agoparentprevit reads fine to me reply natepeters 1 hour agoprevLove seeing old CRTs like this preserved. I am not a CRT collector, but as a NES dev I keep a small 13\" around and use it reqularly for dev purposes and for showing off my games at conventions. 13\" is the perfect size in my opinion. Does not take up too much space and is easy to lug around to shows. I fear the day that mine dies because small 13\" models in good condition are getting harder to find for a decent price. Seems like some people caught on and are selling them on FB Marketplace for high prices and advertising them as \"Retro Gaming TVs\". reply sapphire42 19 hours agoprevThis is a great story, but why does content that is clearly LLM-generated continually make it to the HN front page? reply jsjohnst 22 hours agoprevI had one of the 36” Sony Wega Trinitron CRTs for years. Weighed well over 200lbs, which combined with the shape, made it a really “fun” thing to move. reply bumby 22 hours agoparentThe geometry was a killer when trying to move it because you couldn't wrap your arms around the thing. When faced with moving one by myself down the stairs to my apartment, I was forced to (carefully) roll it downhill. reply doubled112 20 hours agorootparentRelatable! When I was about 14, my mom got a new TV and I got the 27” Trinitron. I was simultaneously excited and terrified. I would have to move it. My arms were too short to get around it. Somehow we made it down the basement stairs without help. By “we” I mean the TV and I. I got it across the room and onto the TV stand. 33 year old me would definitely need an Advil after. reply bloomingeek 3 hours agoparentprevWe inherited one of these from my in-laws, it was a beast. After about a year, it finally died so my son and I loaded it up and took it to Best Buy for free recycling. (this was about 15 years ago.) When the clerk come out with a trolley to collect the tv, we offered to help, but he said he would get it and that was that. I was impressed. reply dekhn 21 hours agoparentprevMe too. It was an anchor. I had a couple of movers nearly drop it once. Getting it out of my house was a great accomplishment (I felt like a great weight had been lifted). At the time it was a definite improvement in video quality (IIRC my first real 1080p, coupled with HDTV) and I still find it crazy I can buy larger, better screens that are lighter and cheaper. Clearly, you can scale up tubes but it's just not going to win against LCD or LED. reply Eisenstein 21 hours agorootparentIt wasn't actually 1080p but 1080i, meaning it interlaced each field. It worked well for CRTs because of the way they operated, but it is a different standard. * https://en.wikipedia.org/wiki/FD_Trinitron/WEGA reply johngossman 22 hours agoparentprevI'll add my voice. I bought one from a friend for $36 (a dollar an inch) while waiting for flatscreens to come down in price. It bent my TV stand and I ended up keeping it a couple extra years because I didn't want to move it out of the house. Eventually we put it on Craigslist for free (with a warning about the weight) and two very large men showed up and carried it away. reply bloomingkales 21 hours agoparentprev27 inch Wega here, dating myself. Mom: \"Dont sit too close to that thing\" Fast forward 20 years, a 27 inch monitor is right up on my face, contemplating a 32 or 43. reply kstrauser 11 hours agorootparentI had that same one. Fun fact: it had a special \"anamorphic\" mode. You know how widescreen movies on 4:3 displays are cropped? Someone had the idea that maybe instead of cropping them, you could use all of the resolution must just direct the electron beam to display it on middle 3/4 (vertically) of the screen. There, an extra 33% better vertical resolution and brightness for free? There weren't a whole lot of DVDs mastered that way, but when you could get one, and your DVD player supported it, and your TV supported it, it looked freaking fantastic. reply sgerenser 7 hours agorootparentThat’s actually not true, the majority of widescreen DVDs were mastered in Anamorphic format. The players themselves were then responsible for squishing down to letterboxed or doing an automated form of “pan and scan” which most people thought was terrible. If you were lucky though, you had a TV capable of doing the anamorphic adjustment and then you’d get the higher resolution as you stated. reply DCH3416 21 hours agorootparentprev> contemplating a 32 or 43 Definitely a 32. 43 is a bit much. Edit: Unless you're an office manager and plan on watching football most of the day. reply bloomingeek 3 hours agorootparentSame here, 32\". I'm using a swiveling TV wall mount also, it really frees up a lot of space under the monitor. reply phkahler 21 hours agorootparentprevI use a 55\" 4k curved TV. The upper portion is too high to do computer work but I move unused windows up there. It's on a desk opposite the couch so I also use it as a TV. Ignore the other commenter, there is no such thing as too big as long as there are enough pixels! reply beAbU 20 hours agorootparentprev32 is enough that you need to rotate your head if you want to see all parts of the screen. I have a 32\" 4k screen and its a bit annoying, I get cricks in my neck, so I tend to only really use a centre 1080p sized area on the screen, with my winXP era wallpaper showing through around it. Tbh I'll prefer 27\" 4k. 43 might be a bit better because you can move the screen a little farther away. reply aidenn0 20 hours agorootparentSurely it depends on the sitting distance? I have 2 27\" 19:10 screens next to each other and do not need to move my head to see all parts of the screen. reply theshackleford 20 hours agorootparentprevIt’s a factor of size and distance. I have an 80cm deep desk with a 32” and it’s fine. In fact it’s nicer in that I can sit a little further back than a 27” which ultimately is better for my eyes. reply rconti 20 hours agorootparentprevI have a 40, it's great. Fewer pixels and width than my previous 3x 27\" 4k setup, but more height. reply bluedino 22 hours agoparentprevI found a 32\" on the curb, heaved it into the back of my truck, and got it home. It worked great, I thought about how much of a pain it would be to drag into the house and up the stairs to the gaming room, and decided I'd just find a 19-27\" to use for old consoles. Ended up selling it on Craigslist for $250. reply corysama 19 hours agoparentprevI had a 36\" RCA. https://lowendmac.com/2019/rca-mm36100-amazing-under-the-rad... 190 pounds. It could do 800x600. The Dreamcast with a VGA adapter looked incredible on it. reply anyfoo 21 hours agoparentprevMe too, I loved that thing. One of the first things I saved up for when I started earning my own money, so it was extra special. I had the fully \"decked out\" version with better speakers, two tuners (picture in picture or two pictures side by side), and tons of other features. Glorious picture quality, and the tube was completely flat (but still very deep, of course). reply phkahler 21 hours agoparentprevMy BIL had one of those. He asked me to help bring in his new bazillion inch LCD so I drove over. Turned out the first task was to move that old CRT into his basement... reply whalesalad 21 hours agoparentprevmy wife just got an enterprise grade treadmill (used from a fitness center) that weighs 600 lbs. moving that thing around is a nightmare. reply karakot 22 hours agoprevprev discussion https://news.ycombinator.com/item?id=42489600 reply ChrisMarshallNY 7 hours agoprevI used to have a Samsung 27” HDTV CRT. I think I brought it, in the late ‘90s. Back then, LCD/plasma monitors of similar size, cost thousands (my, how times have changed). Big, heavy honker, and suffered from chronic fringing, around the edges. I gave it away, in the mid-oughts, which required a pickup truck, and two strong men. I don’t miss it, at all. My job was for an imaging company, and we had a massive HDTV CRT in our showroom. I think it was around 32”. It was a Sony. That was in the early ‘90s. reply nothacking_ 21 hours agoprevAnother day, another LLM generated blog post on the front page. I'm not opposed to AI tools on principle, but why does this article exist? It's not because the author had anything interesting to say. It's not because the AI had anything interesting to say. It's a summary of a Youtube video because... clicks or something. reply noprocrasted 19 hours agoparentCounterpoint (as someone who watched the 30 mins video originally): some people may not have time to watch said video and can read the AI-generated summary quicker and then decide if the video is worth watching. reply 1970-01-01 14 hours agoprevIt seems this TV is more rare than special. Sony was again making up to 42\" models in the 2000s. Not quite at 43\" however those were 1080i, 16:9 flat screen CRTs with a plethora of analog and digital ports in the rear. reply pansa2 12 hours agoparentThey never quite made another model this big, though. The 2000s sets are 42” tube (40” visible) whereas this one is 45” tube (43” visible). reply dylan604 21 hours agoprevThe glass optics on these and other large screen CRTs is something that always impress[es|ed] me. From the older screens that had more of a circular image all the way to these \"flat\" CRTs, there were lots of improvements in everything except weight. It took a lot of glass to get the flat front, but was far from flat on the inside. reply dotancohen 20 hours agoparent...something that continues to impress me. reply dylan604 20 hours agorootparentThat's not a very l33t way of writing it though reply metadat 3 hours agoprevWhy must the vacuum be made out of lead rather than e.g. steel? reply Edman274 2 hours agoparentWhen vacuum tubes have high voltages applied to them, they generate x rays. The glass envelope is impregnated with lead so as to reduce the amount of x ray radiation that is emitted. The primary source of x ray radiation from TVs had been from their other components other than the tube itself but the tube was still a source of ionizing radiation. reply metadat 14 minutes agorootparentGot it, thanks! reply romanhn 20 hours agoprevAmazing story, got sucked into watching the whole video despite not knowing much about the hobby. A random little bit stood out to me, when the president of Sony made a personal promise to fix the TV after it stopped working (a while back). Now that's dedication to quality and customer satisfaction. reply suzzer99 13 hours agoprevI worked for a stereo store in San Francisco in the late-90s. We didn't have to deliver these, but we did have to deliver the 36\" Sony XBRs, which weighed over 200 lbs and were just a delight to drag up 4 flights of stairs with two people. reply consumer451 20 hours agoprevI used to live in Key West. A lot of amazing things were put out on the curb there. The best that we found was a Sony 34XBR910 HD widescreen CRT! I had no idea that a widescreen HD CRT existed until my friend brought one home. As far as I know, this was the pinnacle of CRT displays. Here is a video about that same model: https://www.youtube.com/watch?v=0ccUF1eeIz4 reply thomasfl 6 hours agoprevBuy CRT displays now! In a few year they will be sought after collector items. reply donatj 6 hours agoparentWe're well into that territory already. Go on Marketplace and try to find a decent Trinitron for less than $200 reply heyjamesknight 17 hours agoprevMy first college roommate brought the largest CRT I've ever seen. It looks a LOT like this one. He passed away this last year, otherwise I'd ask him how large it was. It took FIVE adults to carry up the two stories to our apartment! But man, that thing was awesome back in 2007. reply dzuc 22 hours agoprevIs there really no market for a modern CRT tailor-made for retro gaming? Or is it just not feasible? reply ggreer 21 hours agoparentCRTs used to be cheap because they were made in high volumes and had a large ecosystem of parts suppliers. If you were to make a CRT today, you'd need to fabricate a lot more parts yourself, and the low volume production would require charging very high prices. You'd also have to deal with more stringent environmental laws, as CRTs contain many toxins, including large amounts of lead. It's much cheaper to emulate CRT effects so that they work with any display technology. Modern LCDs and OLEDs have fast enough response times that you can get most CRT effects (and omit the ones you dislike, such as refresh flicker). And you don't have to deal with a heavy, bulky display that can implode and send leaded glass everywhere. reply mrob 21 hours agorootparentUnfortunately, the flicker is essential for the excellent motion quality CRTs are renowned for. If the image on the screen stays constant while you eyes are moving, the image formed on your retina is blurred. Blurbusters has a good explanation: https://blurbusters.com/faq/oled-motion-blur/ CRT phosphors light up extremely brightly when the electron beam hits them, then exponentially decay. Non-phosphor-based display technologies can attempt to emulate this by strobing a backlight or lighting the pixel for only a fraction of the frame time, but none can match this exponential decay characteristic of a genuine phosphor. I'd argue that the phosphor decay is the most important aspect of the CRT look, more so than any static image quality artifacts. There is such a thing as a laser-powered phosphor display, which uses moving mirrors to scan lasers over the phosphors instead of an electron beam, but AFAIK this is only available as modules intended for building large outdoor displays: https://en.wikipedia.org/wiki/Laser-powered_phosphor_display reply crazygringo 19 hours agorootparentBut why would the flicker be considered \"excellent motion quality\"? In real life, there's no flicker. Motion blur is part of real life. Filmmakers use the 180-degree shutter rule as a default to intentionally capture the amount of motion blur that feels natural. I can understand why the CRT would reduce the motion blur, in the same way that when I super-dim an LED lamp at night and wave my hand, I see a strobe effect instead of smooth motion, because the LED is actually flickering on and off. But I don't understand why this would ever be desirable. I view it as a defect of dimmed LED lights at night, and I view it as an undesirable quality of CRT's. I don't understand why anyone would call that \"excellent motion quality\" as opposed to \"undesirable strobe effect\". Or for another analogy, it's like how in war and action scenes in films they'll occasionally switch to a 90-degree shutter (or something less than 180) to reduce the motion blur to give a kind of hyper-real sensation. It's effective when used judiciously for a few shots, but you'd never want to watch a whole movie like that. reply CarVac 18 hours agorootparentSample-and-hold causes smearing when your eyes track an image that is moving across the screen. That doesn't happen in the real world: if you follow an object with your eyes it is seen sharply. With strobing, moving objects still remain sharp when tracked. reply mrob 19 hours agorootparentprevYou're correct, but sadly most games and movies are made with low frame rates. Even 120fps is low compared to what you need for truly realistic motion. Flicker is a workaround to mitigate this problem. The ideal solution would be 1000fps or higher on a sample-and-hold display. reply crazygringo 18 hours agorootparent> Flicker is a workaround to mitigate this problem. Isn't motion blur the best workaround to mitigate this problem? As long as we're dealing with low frame rates, the motion blur in movies looks entirely natural. The lack of motion blur in a flicker situation looks extremely unnatural. Which is why a lot of 3D games intentionally try to simulate motion blur. And even if you're emulating an old 2D game designed for CRT's, I don't see why you'd prefer flicker over sample-and-hold. The link you provided explains how sample-and-hold \"causes the frame to be blurred across your retinas\" -- but this seems entirely desirable to me, since that's what happens with real objects in normal light. We expect motion blur. Real objects don't strobe/flicker. (I mean, I can get you might want flicker for historical CRT authenticity, but I don't see how it could be a desirable property of displays generally.) reply mrob 18 hours agorootparent>Isn't motion blur the best workaround to mitigate this problem? Motion blur in real life reacts to eye movement. When you watch a smoothly moving object, your eye accurately tracks it (\"smooth pursuit\") so that the image of that object is stationary on your retina, eliminating motion blur. If there are multiple objects moving in different directions you can only track one of them. You can choose where you want the motion blur just by focusing your attention. If you bake the motion blur into the video you loose this ability. reply crazygringo 18 hours agorootparentI guess it just comes down to aesthetic preference then. If there's motion blur on something I'm tracking in smooth pursuit, it doesn't seem particularly objectionable. (I guess I also wonder how accurate the eye's smooth pursuit is -- especially with fast objects in video games, surely it's only approximate and therefore always somewhat blurry anyways? And even if you're tracking an object's movement perfectly, it can be still be blurry as the video game character's arms move, its legs shift, its torso rotates, etc.) Whereas if there's a flicker/strobe effect, that feels far more objectionable to me. At the end of the day, my eyes are used to motion blur so a little bit extra on an object my eye is tracking doesn't seem like a big deal -- it still feels natural. Whereas strobe/flicker seems like a huge deal -- extremely unnatural, jumpy and jittery. reply jtuple 20 hours agorootparentprevYou should be able to emulate close to CRT beam scanout + phosphor decay given high enough refresh rates. Eg. given a 30 Hz (60i) retro signal, a 480 Hz display has 16 full screen refreshes for each input frame, while a 960 Hz display has 32. 480 Hz already exists, and 960 Hz are expected by end of the decade. You essentially draw the frame over and over with progressive darkening of individual scan lines to emulate phosphor decay. In practice, you'd want to emulate the full beam scanout and not even wait for full input frames in order to reduce input lag. Mr. Blurbuster himself has been pitching this idea for awhile, as part of the software stack needed once we have 960+ Hz displays to finally get CRT level motion clarity. For example: https://github.com/libretro/RetroArch/issues/6984 reply aidenn0 20 hours agorootparent> Eg. given a 30 Hz (60i) retro signal, a 480 Hz display has 16 full screen refreshes for each input frame, while a 960 Hz display has 32. 480 Hz already exists, and 960 Hz are expected by end of the decade. Many retro signals are 240p60 rather than 480i60. Nearly everything before the Playstation era. reply crazygringo 19 hours agorootparentprevI assume the problem here is that the resulting perceived image would be quite dark. You'd need a screen that had a maximum brightness 10x more than normal, or something to that effect. reply noprocrasted 19 hours agorootparentprevIs there actually a fundamental physical limit in modern (O)LED displays not being able to emulate that “flicker”, or is merely that all established display driver boards are unable to do it because it isn’t a mainstream requirement? If so, it would still be much cheaper to make an FPGA-powered board that drives a modern panel to “simulate” (in quotes because it may not be simulating, instead merely avoiding to compensate for by avoiding the artificial persistence) the flicker than bootstrapping a modern CRT supply chain? reply tavavex 12 hours agorootparentThe reason why this is a difficult problem is that physically emulating the flicker requires emulating the beam and phosphor decay, which necessitates a far higher refresh rate than just the input refresh rate. You'd need cutting-edge extremely high refresh rate monitors. The best monitor I found runs at 500hz, but pushing the limits like that usually means concessions in other departments. Maybe you could do it with that one. reply ryansouza 12 hours agorootparentprevMy LG has something like that, OLED motion pro. I believe it displays blank frames given the panel runs at higher than 24fps. Medium is noticeably darker but oleds have plenty of brightness for my viewing space and it makes slow pans look much nicer. High is even darker but adds noticeable flicker to my eyes reply aidenn0 20 hours agorootparentprev72Hz is already a huge improvement in flicker from 60Hz though, and certainly maintains excellent motion quality. reply mrob 20 hours agorootparentBut the refresh rate needs to match the frame rate to get the best motion quality. If you display the same frames multiple times you'll get ghost images trailing the motion. Lots of games are locked to lower frame rates, and there's barely any 72fps video. reply rsynnott 20 hours agorootparentprevAnd even then, they weren’t that cheap, or at least good ones weren’t. Even with the benefit of mass production, this one cost $40k in today’s money. reply hn92726819 20 hours agorootparentNo, it's $100,000 in today's money Source @1:59: https://m.youtube.com/watch?v=JfZxOuc9Qwk&t=119 reply jdboyd 21 hours agoparentprevLooking at that Dallibor Farney company and how hard it is for them to get new nixie tubes to be a sustainable business, I shudder to think how much more effort it would be to get new, high quality CRTs off the ground. It would be cool though. A good start might be bringing back tube rebuilding more widely. reply noprocrasted 19 hours agorootparentAlso, see the visit to one of the last CRT refurbishing facilities out there: https://m.youtube.com/watch?v=YqGaEM9sjVg reply equestria 21 hours agoparentprevI think it's one of these things that people like to talk about in the abstract, but how many people really want a big CRT taking up space in their home? Modern OLED displays are superior in every way and CRT aesthetics can be replicated in software, so a more practical route would be probably to build some \"pass-through\" device that adds shadow mask, color bleed, and what-have-you. A lot cheaper than restarting the production of cathode-ray tubes. reply indigo945 21 hours agorootparentI recently bought a big CRT to take up space in my home. Yes, of course, \"objectively\" speaking, an OLED display is superior. It has much better blacks and just better colors with a much wider gamut in general. But there's just something about the way a CRT looks - the sharp contrast between bleeding colors and crisp subpixels, the shadows that all fade to gray, the refresh flicker, the small jumps the picture sometimes makes when the decoding circuit misses an HBLANK - that's hard to replicate just in software. I've tried a lot of those filters, and it just doesn't come out the same. And even if it did look as nice, it would never be as cool. Retro gaming has to be retro. And to be honest, the CRT plays Netflix better as well. It doesn't make you binge, you see? Because it's a little bit awful, and the screen is too small, and you can't make out the subtitles if you sit more than two meters away from the screen, and you can't make out anything if you sit closer than that. Does that mean we have to restart the production of cathode-ray tubes? Hopefully not. But you can't contain the relics of an era in a pass-through device from jlcpcb. reply tom_ 20 hours agorootparentIf the display is working and the input layout isn't changing, you shouldn't accept any jumps at all. If the sync signals are coming at the same rate, the display should remain steady. (Well - as steady as you get with a CRT.) If they don't: it's broken. reply jtuple 21 hours agorootparentprev> Modern OLED displays are superior in every way and CRT aesthetics can be replicated in software, so a more practical route would be probably to build some \"pass-through\" device that adds shadow mask, color bleed, and what-have-you. OLEDs are still behind on motion clarity, but getting close. We finally have 480 Hz OLEDs, and seem to be on track to the 1000Hz needed to match CRTs. The Retrotink 4k also exists as a standalone box to emulate CRTs and is really great. The main problem being it's HDMI 2.0 output, so you need to choose between 4k60 output with better resolution to emulate CRT masks/scan lines, or 1440p120 for better motion clarity. Something 4k500 or 4k1000 is likely needed to really replace CRTs completely. Really hoping by the time 1000 Hz displays are common we do end up with some pass-through box that can fully emulate everything. Emulating full rolling CRT gun scan out should be possible at that refresh rate, which would be amazing. reply mrob 21 hours agorootparent1000Hz is enough to match CRT quality on a sample-and-hold display, but only when you're displaying 1000fps content. A great many games are limited to 60fps, which means you'll need to either interpolate motion, which adds latency and artifacts, or insert black frames (or better, black lines for a rolling scan, which avoids the latency penalty), which reduces brightness. Adding 16 black frames between every image frame is probably going to reduce brightness to unacceptable levels. reply MichaelZuo 19 hours agorootparentHow many nits of brightness did high end CRTs reach? reply mrob 19 hours agorootparentThe brightest CRTs were those used in CRT projectors. These had the advantage of using three separate monochrome tubes, which meant the whole screen could be coated in phosphor without any gaps, and they were often liquid cooled. Direct-view color CRTs topped out at about 300 nits, which is IMO plenty for non-HDR content. reply tadfisher 20 hours agorootparentprevWhy stop there? We can simulate the phosphor activation by the electron beam quite accurately with 5 million FPS or so. reply Sporktacular 19 hours agorootparentprevAnd the difference between 480 and 1000 Hz is perceptible? reply mrob 19 hours agorootparentFor smooth and fast motion, yes. Although I don't have such fast displays for testing, you can simulate the effect of sample-and-hold blur by applying linear motion blur in a linear color space. A static image (e.g. the sample-and-hold frame) with moving eyeballs (as in smooth pursuit eye tracking) looks identical to a moving image with static eyeballs, and the linear motion blur effect gives a good approximation of that moving image. reply jevogel 21 hours agorootparentprevSuch products exist: https://www.retrotink.com/shop reply rwmj 21 hours agoparentprevYou should probably watch one of the old films about how CRTs were made. It's not a simple process and basically would require setting up a whole factory to mass produce them. reply Animats 20 hours agorootparentHobbyist-level production of monochrome TV tubes is possible, but a big effort. Some of the early television restorers have tried.[1] Color, though, is far more complicated. A monochrome CRT just has a phosphor coating inside the glass. A color tube has photo-etched patterns of dots aligned with a metal shadow mask. CRT rebuilding, where the neck is cut off, a new electron gun installed, and the tube re-sealed and evacuated, used to be part of the TV repair industry. That can be done in a small-scale workshop. There's a commercial business which still restores CRTs.[2] Most of their work is restoring CRTs for old military avionics systems. But there are a few Sony and Panasonic models for which they have parts and can do restoration. [1] http://earlytelevision.org/crt_project.html [2] https://www.thomaselectronics.com reply rtpg 18 hours agoparentprevA practical thing about costs is likely shipping. There aren't many consumer products that would be more costly to move around, so you're looking at something as messy as a fridge to sell at the high end. I imagine one could target smaller CRTs as an idea though. reply kcb 20 hours agoparentprevThe whole supply chain is dead. No way the demand is great enough to justify rebooting it. reply hinkley 21 hours agoparentprevI know there have been conversations here about simulating crt subpixels on hidpi displays. There are some games that used subpixel rendering to achieve better antialiasing. With hidpi you at least have a chance of doing it well. reply reason-mr 16 hours agoprevOMG. I have one of the original SGI 24” 1080p flat screen CRTs (went with the onyx2) in storage. One wonders what that’s worth :) fundamentally a better tube .. reply pansa2 14 hours agoparentWidescreen? If so it’s likely a Sony GDM-FW900 in disguise - would probably sell for a couple of thousand nowadays. reply cogman10 22 hours agoprevOh interesting. I'm like 90% sure my shop teacher had one of these! He had a giant ass CRT in his home (took up like half the living room in his tiny house). He got it from a facilities friend at a university that he was friendly with in like ~00s. They were getting rid of all these because flat-screens and projectors were much more in vogue at the time and these behemoths were simply dated. I wonder if he still has it. reply ajross 22 hours agoprevIt's a little sad to see CRTs withering into nothingness. The devices just don't last. The glass is obviously fragile. But even if you keep it padded and safe, the coils of the deflection yoke are thin magnet wire operated at high voltage, and after decades of thermal cycles and the resulting rubbing eventually the barrier between two drops enough and they short, catastrophically. And you can't really repair that in any feasible way. There are hundreds or thousands of windings, which have to duplicate exactly the configuration from the factory (and then probably be calibrated by processes that are lost to history). A dead CRT is just a useless hunk of glass, forever. They're all dying. And that's kind of sad. reply thowawatp302 21 hours agoparent> But even if you keep it padded and safe, the coils of the deflection yoke are thin magnet wire operated at high voltage The coils in the deflection yoke are run at 24-100V. The acceleration voltage is the high voltage one. > There are hundreds or thousands of windings, which have to duplicate exactly the configuration from the factory (and then probably be calibrated by processes that are lost to history). Tubes are very not exact compared to solid state devices— to replace a deflection yoke, it has to be of similar deflection angle and inductance, all the rest of the adjustment has to be done anyway. It’s hard but pales in comparison to the impossibility manufacturing a new CRT vacuum tube. reply ajross 18 hours agorootparent> The coils in the deflection yoke are run at 24-100V. They aren't the kV scale killer voltages, no. My memory was closer to 200, but sure. That's still \"high voltage\" for magnet wire, and a short will rapidly destroy the coil. I've had three different monitors go to that kind of failure. One day you turn them on and... nope. reply nucleardog 22 hours agoparentprev> The glass is obviously fragile. Ever broke one? Like 2/3 of the weight is that front glass. It's _thick_. When I was younger and dumber (well, at least younger) I tried breaking one. Took a running swing at the screen with a wrecking bar. It bounced off and all I got for my trouble was a sore shoulder. reply smitelli 21 hours agorootparentThe fun way to do it is to pull the deflection yoke off and shear the neck of the tube. I was pretty far away the only time I experienced somebody do that, but it sounded like a rifle round. reply londons_explore 22 hours agorootparentprevI believe the thick front (leaded) glass is to try to block the produced x-rays. People were starting to get scared of the cancer those xrays might produce, and I suspect CRT manufacturers predicted a huge court settlement for cancers caused by TV's with insufficient shielding. So far, it seems that hasn't materialized - not, I suspect because those xrays didn't cause cancer, but because it is simply impossible to produce any kind of evidence of cause/effect. reply mrob 21 hours agorootparentOnly the oldest CRTs used leaded glass for the front, because leaded glass gradually turns brown on exposure to X-rays. More modern CRTs used glass with barium and strontium for X-ray shielding in the front. They still used leaded glass for the back and sides, presumably as a cost saving. I don't see any reason why you couldn't use the barium-strontium glass for the whole thing. Alternatively, CRTs could be made with ceramic bodies like Tektronix used to do. The energy of the X-rays produced is limited by the CRT's acceleration voltage. The electrons get almost all of their energy from the field produced by the acceleration voltage. Electrons can produce photons when they hit matter, and one electron produces at most one photon, so by conservation of energy the X-ray cannot have greater energy. Smaller CRTs typically use low acceleration voltages, which means the X-rays are low energy and thus easy to block. reply ahartmetz 20 hours agorootparentprevAFAIK, the shielding was also just very effective. \"Soft\" x-rays (below 50-100 kV or so) are rather easy to shield and what screens had was pretty overkill. reply brendoelfrendo 22 hours agorootparentprevIn the YouTube video they explain that CRTs have a layer of safety glass in front of the actual screen to protect viewers in the event that the screen implodes. You were actually trying to break through multiple pieces of glass! I've taken a crowbar to a broken CRT before for fun and can confirm that it takes a lot more effort than one might think. reply mrob 21 hours agorootparentIt depends on the CRT. Some use steel bands wrapped around the edge of the faceplate and tightened to keep the glass in compression where it's strongest. reply aidenn0 20 hours agorootparentprev> Ever broke one? Yes, drop one from a few feet, and the immense weight will do the work for you. reply emchammer 21 hours agoparentprevCRT phosphor chemistry was very sophisticated and mature, and there were many phosphors to choose from by the 1970s depending on the application. Maybe someday a flat panel screen will be produced with some warm and slow characteristics of CRTs without the drawbacks. reply bane 16 hours agoprevIn 2006 or so I bought a house at the top of the real estate market (whereupon it quickly crashed and we enjoyed the wild ride of refinancing and property value swings until we finally unloaded the place at cost - at least it was a really nice neighborhood). The real estate agents, as a token of their thanks for allowing them to claw back 6% of an outrageously priced house, gave us back some of the money in the form of a $2500 gift card to BestBuy. Of course, I immediately used it to buy a state of the art Samsung DLP rear projection TV with more inputs than you could shake a stick at including then new HDMI and VGA. I still have that TV, it looks pretty good for 720p and 46\" or so, and has a chromecast dongle permanently stuck in its HDMI port to make it useable. It works amazing as an impromptu VGA monitor, and old games console system as well. The cost, with stand, was something like $2400 plus some change and I was left with a few dollars at the end. I wanted to finish off the gift card so I looked around the store. There, off in the corner was an absolutely massive Sony CRT tv with a yellow sticker on the side. \"$1.72\". I gasped. \"Is that TV really $1.72?\" \"Yup, the future is these DLP or these Plasma TVs, we're getting rid of our CRTs\" Instant purchase, closed out the card, set the delivery dates for both and waited. A week later two guys showed up \"we got two TVs, one of them if fcking heavy, where do we put 'em?\" The DLP went of course into the living room without any fuss, but the Sony...well that was the heavy one. It took two guys, working hard, to move all 39\" of it up a flight of stairs into an upper bedroom. It sat in its place until we sold the house and decided to move. That's when I learned what a monster it was. For absolutely foolhardy reasons, I decided to junk it, so I had to take it to the curb. I tried to lift it. No go. I was like trying to lift Mjolnir or free Excalibur. I had a friend come over. It took us about an hour to move it down that flight of stairs and drag* it on piece of plastic to the curb. The trash people, even prepped for an unusually heavy pickup, had to make three attempts at it before they could get it. During that time it was at the curb, two cars stopped and tried to pick it up before giving up. Looking up the specs now, it looks like it was probably somewhere north of 300 lbs (136 kg). I see on Ebay that today it's probably worth around $600-$1000. But damn, if it wouldn't cost that much to move it within the same county. I still have a 24\" I'll keep until I'll die for old gaming reasons, but man, that other monster was too big. A guy at work the other day moved a 72\" TV by himself, like it was nothing. There's a reason some tech falls away. reply rconti 12 hours agoparentNo doubt on the 300lbs. My dad and I hauled our brand-new 35\" Toshiba when I was a teen. I clearly remember it was 198lbs per the spec sheet. reply webwielder2 22 hours agoprevInteresting to see what people are passionate about. reply echelon 22 hours agoparentIf you play retro video games from the NES / SNES / N64 / Gamecube era on original hardware, a CRT is the way to go. People that play competitive Smash Bros Melee will only play on CRTs. reply Klonoar 18 hours agorootparentWe don’t only play on CRTs, we just still use them in tournament formats. reply bumby 21 hours agorootparentprevWhat's the rationale? Is there a performance benefit or nostalgia? reply nntwozz 21 hours agorootparentIt looks much better on CRT. https://www.reddit.com/r/gaming/comments/wr31qd/my_crt_vs_my... \"…scanlines were used to blend “pixels” together, plus “pixels” on a CRT tend to bleed color slightly and artists would also use that to their advantage.\" reply onlypassingthru 21 hours agorootparentprevAside from the visuals (4:3 to 16:9, etc), converting the analog console signals into digital formats for your flatscreen creates lag, enough to often ruin the gameplay. reply AndrewDavis 21 hours agorootparentEven though I have a CRT and NES, I bought one of the NES minis when they released. I played some Mario Bros 3 and... I kept dying. Jumping too late led to running into holes and enemies. It was so bizarre, I couldn't believe how bad I'd gotten. Tried the next day, same deal. Then I had a thought re delays. Pulled out my NES and hooked it up to the CRT and all that stopped There was sufficient delay in the NES mini and modern TV it made a huge difference. I'm sure I could retrain myself, but it was honestly stunned at how much of a difference it made reply exitb 21 hours agorootparentIt’s difficult to overstate just how little lag there is in such setups. These systems had no frame buffer whatsoever - everything rendered on the fly. You could potentially affect a frame after it already started. That said, if you ever get an urge to play Mario on modern hardware, try run ahead emulation. It’s quite magical. reply vunderba 20 hours agorootparentprevI've always found the litmus test of choice for measuring lag is NES Punch-out - your performance in that game is heavily dependent on lightning fast reaction time and any additional latency towards the later stages will 100% get you KO'd. reply sparky_z 21 hours agorootparentprevFor competitions, the performance benefit is zero time lag between controller inputs and the screen output. Also, it's very very difficult to get the \"look\" right on an hdtv. The original graphics were intended to be displayed on a slightly \"fuzzy\" CRT, and if you care about the aesthetic, just transferring those same graphics to an hd-tv display often doesn't look right in a bunch of different ways. (Pixel aspect ratio, aliasing, frame blending effects, color bloom effects, interlacing artifacts, etc.) It's a very deep rabbit hole you can go down. reply rwmj 21 hours agorootparentprevLow latency, and it looks like how the game designer intended it to look. reply qingcharles 21 hours agorootparentprevI grabbed a 40\" Sony to play lightgun games on. Sadly the 40\" have a framebuffer and I didn't have a chance to find a way around it. The 43\" in the post has a bypass. reply EA-3167 22 hours agoparentprevWithout a shred of judgement or sarcasm, yeah I agree, it's a big part of what I enjoy about scrolling through New here. reply dateSISC 21 hours agoprevyeah absolutely epic, watched it yday reply binary132 14 hours agoprevThe whole time I was reading this article, I just wanted to know the history of the soba restaurant that was being demolished. I bet there’s an interesting story there too. reply ascorbic 9 hours agoparenttbh probably not. There are a lot of soba restaurants, and a lot of demolitions. Buildings are typically demolished after 20-30 years to make way for new ones. reply Koshkin 21 hours agoprev [–] At least these are not banned, as the ICEs no doubt will be in 11 years... reply tehjoker 21 hours agoparentThe problem with ICE isn't being able to buy them, it's buying them in huge quantities. You'll probably be able to buy them for sports cars or some other low volume commodity. reply ggreer 21 hours agorootparentOf the existing plans to ban the purchase of gas vehicles in the future, do any have exemptions for low volume production of enthusiast vehicles? California's plan (which 17 other states follow) seems to only have exemptions for heavy duty vehicles.[1] My guess is that enthusiasts will get around these laws by modifying old vehicle frames. New emissions and safety standards tend to grandfather old vehicles in, so as long as the VIN says it was made before a certain date, you can avoid having curtain airbags, backup cameras, tire pressure monitoring systems, electronic stability control, etc. (There requirements are why new cars have so many computers in them.) 1. https://www.gov.ca.gov/wp-content/uploads/2020/09/9.23.20-EO... reply jjulius 21 hours agorootparentOP implied buying engines as single units, an item unto itself - \"you'll probably be able to buy them for sports cars\". The California executive action is explicitly for the sale of new vehicles. reply kalleboo 15 hours agorootparentprevThe problem will also be getting gasoline, if just 90% of the cars on the road are electric, a lot of the gasoline infrastructure will go away. reply jjulius 21 hours agoparentprev [–] Apples, meet oranges. Or sand. Yeah, apples and sand. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A YouTube influencer rescued the world's largest tube TV, a rare Sony model, from destruction, relocating it from Japan to the US for personal use.- The decision sparked debate over whether such unique artifacts should be preserved in museums rather than private collections.- The situation underscores the difficulties in preserving rare items and the intricate issues surrounding ownership and cultural heritage."
    ],
    "points": 564,
    "commentCount": 257,
    "retryCount": 0,
    "time": 1734983374
  },
  {
    "id": 42499567,
    "title": "The number pi has an evil twin",
    "originLink": "https://mathstodon.xyz/@johncarlosbaez/113703444230936435",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Only available when logged in. mathstodon.xyz is one of the many independent Mastodon servers you can use to participate in the fediverse. Administered by: Server stats: mathstodon.xyz: About · Status · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.3.1 SearchLive feeds Mastodon is the best way to keep up with what's happening. Follow anyone across the fediverse and see it all in chronological order. No algorithms, ads, or clickbait in sight. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=42499567",
    "commentBody": "The number pi has an evil twin (mathstodon.xyz)360 points by pkaeding 15 hours agohidepastfavorite150 comments soneca 7 hours ago> ” This ∞-shaped curve is called a 'leminscate', and ϖ is called the 'lemniscate constant'. I'll show you the leminiscate in my next post.” This got me confused, so I went to check. Apparently ”lemniscate” is the correct spelling. reply johncarlosbaez 3 hours agoparentFixed - thanks. reply mst 9 hours agoprevHaving that shape become more important to a civilisation than the circle because it has something to do with the geometry of hyperspace seems like it could be an interesting conceit for a sci-fi setting. reply pavel_lishin 4 hours agoparentThe Anvil of the Stars, by Greg Bear, featured a race of aliens whose mathematics weren't based on integers. reply TuringTest 4 hours agoparentprevBob Shaw's Night Walk has something like that as a major plot point. It's not aliens but humans, and it's not an 8-loop geometry, but without spoiling it too much it's safe to say that discovering how hyperspace works is the central concept guiding the story. reply tibbetts 6 hours agoparentprevSounds like a Greg Egan writing prompt. reply dmd 6 hours agorootparentBaez and Egan are close friends, so don’t be surprised if you see it pop up. reply szundi 6 hours agorootparentprevPeople just prompt themselves reply metaphor 13 hours agoprev> This ∞-shaped curve is called a 'leminscate', and ϖ is called the 'lemniscate constant'. I'll show you the leminiscate in my next post. Two of these...do not belong? reply bregma 7 hours agoparentShakespeare often spelt the same word differently at different times. If it was good enough for Billy Shakespeare, it should be good enough for modern-day mathematicians, forsooth. reply somat 1 hour agorootparent\"It is a damn poor mind that can think of only one way to spell a word.\" -- Andrew Jackson Unfortunately Daniel Webster ruined that for the rest of us. reply initramfs 1 hour agorootparentprevI find it hard to believe that Shakespeare would spell the same wird dyfferntli as if heez noom is Sheikhspier een uh deefirind koontri. reply stogot 3 hours agorootparentprevThis might feed the “Shakespeare was not one person” theory reply zdragnar 2 hours agorootparentThe first of Shakespeare plays predate the first published English documentary. It was uncommon for spellings to be inconsistent or change between writings to be easier for a particular audience (in this case, actors) to be able to read. reply drivers99 1 hour agorootparentI guess you mean: first published English dictionary and It wasn't uncommon / It was common reply zdragnar 37 minutes agorootparentYes, I was rather tired and typing on my phone required more correcting of the autocorrect feature than I could manage. reply adrian_b 12 hours agoparentprev\"Lemniscate\" is the correct spelling. All the other variants are mistyped. reply yard2010 11 hours agorootparentIt's quite funny imo that someday english people were like \"forget about latin or german, greek is lit! Let's use greek\" reply msravi 2 hours agorootparentWhy stop at greek or arabic when you can go all the way to sanskrit? The words for sine and cosine derive from the sanskrit jiva (meaning bowstring, i.e., the chord of a circle)[1]. Sine and cosine were respectively jya and koti-jya, which got transcribed into arabic without the vowel (where it meant nothing). They then pronounced the vowel in the wrong place, calling it jeb (which meant pocket or fold in arabic)[2]. Then this wrong word got translated into latin as sinus (fold), and hence we have sine and cosine! 1. https://en.m.wikipedia.org/wiki/Jy%C4%81,_koti-jy%C4%81_and_... 2. https://en.m.wikipedia.org/wiki/Sine_and_cosine#Etymology reply rsynnott 8 hours agorootparentprevA healthy mixture was always preferred in maths and science. This is occasionally taken to extremes; the name reverse transcriptase, an enzyme used by retroviruses, is a combo of English, Latin and Greek! Arabic is also popular, particularly in maths. reply flobosg 3 hours agorootparent> Television? The word is half Greek and half Latin. No good will come of this device. ―C. P. Scott reply xanderlewis 7 hours agorootparentprevIs it? I can only think of (the very frequently noted) ‘algebra’ and ‘algorithm’. reply rsynnott 6 hours agorootparentAlso ‘zero’, and ‘cipher’ (which, oddly, derive from the same word). And ‘average’. There are a few of them. reply dizhn 5 hours agorootparentNot math but I just learned alkali is the word for \"ash\" in Arabic. reply dudeinjapan 3 hours agorootparentprevAnd “alcohol”, frequently consumed at science and math conferences reply Sharlin 1 hour agorootparent\"Alcohol\" has a very interesting etymology, too. reply ajmurmann 5 hours agorootparentprevDolphin, music (from muse), logic, ethics, physics, mathematics, pharmacy, angel, comedy, drama. The list of Greek loan words that are shared by many European languages goes on and on Edit: I think almost every word with \"ph\" in it is from Greek and \"th\" in languages other than English. reply FredPret 2 hours agorootparentIf you add all Latin words with Greek origins, most European languages are really forms of Greek reply lolinder 3 hours agorootparentprevThey're asking about Arabic loanwords. reply nicwilson 7 hours agorootparentprevazimuth is the only other one I can think of off the top of my head reply jjtheblunt 2 hours agorootparentprevThe sheriff says \"hold my beer\". reply ajmurmann 5 hours agorootparentprevDon't most European languages use Landis loan words from both Latin and Greek? Both used to be taught in classical education. reply laurent_du 10 hours agorootparentprevWhat makes you think it was the English? I am pretty sure it comes from continental Europe. reply cgio 8 hours agorootparentprevLatin is lemniscus, so someday Latin people were like “let’s use Greek” reply dmurray 6 hours agorootparentLatin writers have been like \"let's use Greek\" at least since Virgil, so modern writers can be excused for getting their roots mixed up. reply saghm 13 hours agoparentprevIt's so evil that it defies spelling reply dotancohen 10 hours agoparentprevI understand the confusion. Lemons smell good. The second root, on the other hand, far less pleasant. reply brookst 13 hours agoparentprevEven the word has evil twins reply Netcob 7 hours agoparentprevNot to be confused with the \"lemonscape\", a hallucinated world you enter when you've eaten too many lemons. reply thaumasiotes 12 hours agoparentprevLemniscate. https://en.wiktionary.org/wiki/%CE%BB%CE%B7%CE%BC%CE%BD%CE%A... reply GistNoesis 12 hours agoprevAnd to protect you from it, you can use the following lucky clover charm (polar plot r=cos(2theta) ): https://www.wolframalpha.com/input?i=+plot+r%3Dcos%282theta%... whose perimeter can also define a constant 4*E(-3) ~ 4 * 2.4221 https://www.wolframalpha.com/input?i=plot+r%3Dcos%282theta%2... reply divbzero 11 hours agoprevπ is derived from the circle, which is defined by distance from a single point. ϖ is derived from the lemniscate of Bernoulli, which is defined by distances from two points. Is there an analogous constant that is derived from a shape defined by distances from three points? reply dahart 2 hours agoparentYes, definitely. Pi is just the perimeter of the circle, and varpi is the perimeter of the lemniscate. If you use three points, you get three tear-drops, and you can compute the perimeter of that. Let’s call it a trilemniscate. ;) Here’s a 3d plot of it. If you rotate to view it from +Z downward, then you’ll see the trilemniscate, which is where the volume intersects with the XY plane. Note I subtracted 1 from the product in order to visualize the plane intersection. (And you can turn off the 3 points version and turn on the 2 points version to compare.) https://www.desmos.com/3d/dl9v2vqbqb One interesting note about 2 points vs 3 points. The area inside the lemniscate and trilemniscate is the same! (True for more points, as long as they’re evenly space on a circle). The perimeter, of course, goes to infinity as you add more points. reply VHRanger 3 hours agoparentprevI mean the concept of distance from 3 points introduces a mess of metrics or even measure theory. 2 points always have a shortest path between each other, so the constant is about this fact. For 3 points you have the whole universe of possible triangle shapes to contend with. reply vitus 2 hours agorootparentShortest path between two points still depends on your metric. For instance, if you're constrained to travel along the surface of Earth, your shortest path is going to travel along a great circle, rather than pass through the interior of the sphere. That said, you could, for instance, pick the three vertices of an equilateral triangle (using the Euclidean distance as your metric of choice, as we do in order to derive the lemniscate and the circle), and again deal with the product of the distances from each vertex. You again start with small circles around each vertex, which eventually expand to a single looping curve, and then into ovals encircling the entire triangle. https://en.wikipedia.org/wiki/Cassini_oval#Generalizations https://en.wikipedia.org/wiki/Polynomial_lemniscate#Erd%C5%9... reply jovial_cavalier 3 hours agorootparentprevIt's easy to generalize this to more points. https://www.desmos.com/calculator/fo7tqlfjgo reply clort 11 hours agoparentprevit sounds like you are suggesting it might be turtles all the way down? reply cl3misch 6 hours agoprev> I'm not enough of a cultural relativist to believe there's a civilization that cares more about the shape ∞ than the shape ◯. Maybe these are \"logarithmic\" beings, as opposed to us \"linear\" beings? The lemniscate is based on geometric mean, which is basically multiplicative mean and/or mean in log-space -- as opposed to the additive mean in linear space. If we assume we are linear beings good at intuitive addition but somewhat bad at intuitive multiplication, there could exist beings which live in log-space and whose minds are based on multiplication. Their circle would be the lemniscate. reply tibbetts 6 hours agoparentHumans are actually intuitively log scale thinkers. That is, humans without the kind of early arithmetic training that Westerners get will think more in terms of ratios than differences. There are theories it is more evolutionarily adaptive. https://www.scientificamerican.com/article/a-natural-log/ reply jcelerier 6 hours agorootparentIsn't it also related to our physical perception? Both hearing and vision at least have somewhat logarithmic properties (e.g. response to point-source brightness, and hearing frequency response) reply dxbydt 1 hour agoprevaside: As the Professor points out, the ratio of pi to its evil twin is ~1.198, the arithmetic-geometric mean of sqrt(2) and 1. The geometric part involves a square root, and square roots are expensive. So I was like, well, if the AM converges to GM, then due to AM-GM-HM inequality, it must converge to the harmonic mean as well. And the HM does not need an expensive square root! https://imgur.com/a/UkxkPzW Its quite wild that the AM GM convergence is almost immediate - in just 2 steps, whereas to get a decent convergence for the Gauss's constant via HM, you need like 15 steps.You can dispense with expensive operators like square root but you end up paying for it with numerous iterations. reply yukioikeda 11 hours agoprevIt seems obvious that these are not twins. We can only say that π and ϖ are two among the infinite multitude of siblings ϖₙ. reply slippy 10 hours agoprevHmm. Why only 2? Why not 3 points? Can you find an interesting curve produced by a constant product of distances from N points? Maybe even in higher dimensions, for 1 point, you have a sphere. What is the shape for 2 points? Is it more like an hourglass-like double droplet? reply amelius 10 hours agoparentThere is a generalization: > Back before Twitter became a Nazi bar, I issued a challenge there: find a whole series of numbers like pi, each with its own bunch of formulas. @duetosymmetry took me up on this and invented the numbers ϖₙ: (...) reply plank 5 hours agorootparentYes. But the question remains: is there a geometrical analogue? reply dclaw 2 hours agoprevThis discussion helped me discover my new favorite map. https://en.wikipedia.org/wiki/File:Peirce_Quincuncial_Projec... reply ComputerGuru 1 hour agoprevThe post mentions that ϖ is called “varpi”; I just wanted to add that this is actually short for “variant of pi”, also known as an “archaic form of pi” from old Greek writing. reply flatline 52 minutes agoparentI read it as “omega-bar.” reply initramfs 1 hour agoprevhttps://en.wikipedia.org/wiki/Bizarro reply sapphicsnail 12 hours agoprevIf I saw ϖ in the wild I would have assumed it was an omega (ω) with a macron over it. Makes me wonder how many more varient Greek letters are out there. reply wombatpm 10 hours agoparentAncient, Ancient Greek had three additional letters: an F like character, a double lambda character, and P sounding character that looked like a lollipop. In case you need some additional symbols reply perihelions 4 hours agorootparentI guess these are the letters? https://en.wikipedia.org/wiki/Archaic_Greek_alphabets Ϝ Digamma Ͱ Heta Ϻ San Ϙ Koppa Ͷ Tsan, Digamma Ͳ Sampi reply rsynnott 7 hours agorootparentprevConsiderate of them; very helpful for future mathematicians. reply nuancebydefault 4 hours agoparentprevAs psted now and then on HN:our alphabet has a variant as well: ampersand (per se: and). reply Pinus 9 hours agoparentprevAny actual Greeks around here? I always wondered what π looks like when jotted down in, say, a shopping list... reply xico 7 hours agorootparentThis is actually a normal manuscript π as taught in Greek school. See https://www.typotheque.com/articles/modern-handwriting-a-his... reply efortis 11 hours agoprevInfinity symbol with Lissajous curve: x = Asin(at + delta) y = Bsin(bt) https://ericfortis.github.io/lissajous/?preset=Infinity reply jan_g 11 hours agoparentInteresting! I can see it in two ways: (1) as elongated U-shaped ellipsis that rotates sideways and (2) as bent lemniscate that rotates vertically. reply BearOso 3 hours agoprevI thought it might be e. e is often used to model unbounded growth, so it's chaotic, while pi is harmonic. Plus, evil starts with 'e', so why not. \"Laugh with me Jocko!\" \"Eeeeeeeeeeeeee!\" reply anyfoo 2 hours agoparentCan’t have harmonics (i.e. harmonic oscillations, or any oscillations really) without e, though. sine and cosine are both sums of e, and if you look at the beauty of analytical sinusoid signals (which only have one component in the entire spectrum, lacking their negative frequency one) it’s just one exponential and nothing else. reply divbzero 11 hours agoprevThe Fourier transform is composed of trigonometric sines and cosines. There must be an analogous transform composed of lemniscate sines and cosines? reply ttoinou 9 hours agoparentYou could try to make a transform based on a sum of lemniscates in the complex plane reply big-green-man 12 hours agoprevYou just blew my mind. I'm taking a dive on this. reply yason 7 hours agoprevOfftopic but oh boy was that page difficult to scroll. Up/down arrows jump to the next post and page up/down isn't too helpful for reading. I have the keyboard overrides forbidden in my browser but obviously the web page can still bind events to keys not usually reserved for browser shortcuts... So, the usual navigation breaks up, leaving me to learn one particular site's idiosyncratic behaviour in the user interface space. No thanks, I just left. Some people saw this right from the start. I remember the time when disallowing javascript would mostly spare you from unnecessary and irritating opt-ins, and you could still consume the actual content of the page using the browser as basically a text reader with hyperlinks, like originally intended. Now you can no longer, in effect, do that as pages consider the browser a VM to present themselves, and this just leads to a tug of war between the browser and its users vs the page creators. Both assume a level of control of a more than Turing complete medium and there's no compromise into that. The working solutions I see are either you write programs that run in the browser-VM to implement web stores etc. or you write effectively HTML 1.0 level structured documents to deliver information and leave the presentation to the browser-reader. Back in the old days HTML was a huge step up from text files and proprietary hypertext documents but these days I'm more like hoping everything was ultimately, mostly plaintext. reply NoboruWataya 7 hours agoparent> Up/down arrows jump to the next post and page up/down isn't too helpful for reading. I didn't experience this at all on Firefox, up/down and page up/down scrolled in the normal way. reply davorak 4 hours agorootparentThe issue existed from me in both firefox and chrome. Click on outside columns will result in normal scroll. Click or highlight in the center column will result in the jumpy scroll that does not quite scroll one comment at a time with up/down arrow. reply RobotToaster 7 hours agorootparentprevIt kinda happens to me on firefox, one press of the down arrow scrolls so \"Here's a formula for the lemniscate in polar coordinates\" in the first reply is at the top of the screen, not helpful. reply kuschkufan 1 hour agoparentprevhere's a nickel, get a new browser. no idea why i even go for bait like this. because i like doing unpaid support work i guess. i tested in firefox and chrome. both work fine and don't do it like op decribes - no keybinds, keys behave normal. maybe one of the dudes from yesterdays thread that had his own chatgpt programmed browser extensions installed that break the web for him. reply cluckindan 3 hours agoprevSide by side, there is a clear parallel to monopolar and bipolar fields. Is this found in any version of Maxwell’s equations? reply nthingtohide 10 hours agoprevChange pi to ϖ in this setup. 2022 - Non-Euclidean Doom: What happens to a game when pi is not 3.14159… https://youtu.be/_ZSFRWJCUY4?t=406 reply seba_dos1 9 hours agoparentSuch a promising yet disappointing talk. reply sourcepluck 8 hours agoprev> This ∞-shaped curve is called a 'leminscate', and ϖ is called the 'lemniscate constant'. I'll show you the leminiscate in my next post. I think others have commented, but this three-way spelling certainly got a chuckle from me. reply aap_ 2 hours agoprevWow, pomega is such a terrible name for it! reply layer8 3 hours agoprevIs there something like ThreadReaderApp for Mastodon? reply candlemas 6 hours agoprev>On our planet, it was Bernoulli, Euler and Gauss who discovered this math. You don't say. Newton must have been sick that day. reply AlecBG 10 hours agoprevThe lemniscate really looks like a homoclinic orbit in a 2d dynamics problem reply Morizero 12 hours agoprevIs there an abstraction of a leminscate/consonant with 3+ center points? reply fisian 12 hours agoparentYes, the Lissajous https://en.wikipedia.org/wiki/Lissajous_curve which also can be turned vertical (to look like an 8 instead of ∞). reply mandarax8 4 hours agorootparentBut this doesnt have the property that the product of the distances to the focal points is constant no? reply mandarax8 4 hours agoparentprevHeres two examples for 3 and 6 points in 2D, 3D respectively: https://en.wikipedia.org/wiki/Cassini_oval#Generalizations These are symmetric as well though. reply SubiculumCode 12 hours agoprevIs there an evil twin to the set of prime numbers? reply OscarCunningham 10 hours agoparentThere are the Lucky Numbers https://en.wikipedia.org/wiki/Lucky_number. Generated by a variant of the Sieve of Eratosthenes, they're believed to have a similar distribution to the primes while not having similar multiplicative properties. reply fisian 11 hours agoparentprevThere are the anti prime numbers (also called highly composite numbers). reply block_dagger 11 hours agoparentprevEvery even number? reply ralusek 11 hours agorootparent2? reply TomK32 11 hours agoprevAm I the only one who expected the evil twin to be 3 ? reply incognito124 10 hours agoparentFor some reason, I imagined a number where every digit of pi was transformed into a [9-digit] and that it has special properties. This one is more magical, though. reply doffen 10 hours agoprev> Back before Twitter became a Nazi bar, Would've been a better thread without this irrelevant aside, which isn't even true anyway. reply nthingtohide 9 hours agoparent> Hence neither a man's contemporaries nor the man himself can form any final estimate of him or of his fitting position, because their knowledge is too imperfect. History often reverses the decision of contemporaries. Probably true about Elon. reply I_complete_me 7 hours agorootparentBut I think mainly in the direction of demotion. Offhand I can't think of examples of someone ... oh, wait Van Gogh. reply nthingtohide 2 hours agorootparentBaruch Spinoza is another. He was excommunicated. reply mnsc 8 hours agoparentprevCuriously that made the thread better for me and the author's opinion about Twitter is exactly as true as the opposite opinion, that it is now the unfiltered source of objective truth. Or do you believe your opinions on the threads value or twitters reputation is special? reply veltas 6 hours agorootparentI also found it extremely helpful that the author virtue signalled to agree with me, so I know whether I am supposed to like it or not. reply johnp314 2 hours agoparentprevSince it's an \"evil twin\" should we not expect to find it in an alleged Nazi bar? reply rsynnott 7 hours agoparentprevIt’s a metaphor (ironically originating _on_ Twitter, not _about_ Twitter, pre-Musk); essentially, once you allow Nazis in a bar, they metastasize, and pretty soon you’re a Nazi bar. It’s perfectly applicable to the current state of twitter. reply himgl 4 hours agorootparentIt's not at all applicable. Makes me wonder if you even use Twitter if you're making claims like that. More likely you're just parroting nonsense from your echo chamber. reply rsynnott 20 minutes agorootparentI was a Twitter user from 2007 to late 2022. That idiot wasted no time in ruining it; by Dec 2022 it was very clearly time to go. reply nuancebydefault 4 hours agorootparentprevNobody uses twitter these days. reply himgl 2 hours agorootparentWhat a daft claim to make. A simple web search would have informed you that Twitter has over half a billion monthly active users. reply nuancebydefault 2 hours agorootparentHmm you mean X right? reply himgl 11 minutes agorootparentSame thing. reply xigoi 4 hours agorootparentprevTwitter already contained a lot of hateful speech before Elon acquired it. reply lern_too_spel 1 hour agorootparentThose tweets would typically be demoted instead of promoted. reply gosub100 1 hour agorootparentprevwhat metaphor was it called back when they allowed far-left hate speech but censored, shadow-banned or otherwise slowed stories that their secret thought-control departments didn't like? what would you call that? reply veltas 6 hours agorootparentprevI absolutely agree, a dead 1920's German nationalist movement is exactly why we shouldn't allow free speech online. reply mongol 9 hours agoparentprevYes, that is when I stopped reading. I left Twitter recently, but I would not call it a Nazi bar. It is just not for me, any longer. reply Johanx64 8 hours agoparentprevCrikey! The dude can't even write a maths article without delving into tribal politics garbage halfway through. This is depressing. reply Bengalilol 10 hours agoprevmupi (mutant pi) or piet (pi evil twin) would be better names reply Qem 4 hours agoparentPizarro = Pi + Bizarro. Also there was an evil person that beared this name, Francisco Pizarro, the conquistador that kickstarted the genocide against the Incas. See https://en.m.wikipedia.org/wiki/Francisco_Pizarro reply notpushkin 12 hours agoprev> I'm not enough of a cultural relativist to believe there's a civilization that cares more about the shape ∞ than the shape ◯. Rumor has it there is one civilization of lizard-people out there. One is in fact running a company here on Earth with this shape as a logo! /s reply kvdveer 12 hours agoparentYou mean Arduino is ran by the Illuminate? reply ta988 11 hours agorootparentyes you can blink leds reply bnetd 6 hours agoprev\"Back before Twitter became a Nazi bar [...]\" ... Thank you, but no thank you. reply BeetleB 13 hours agoprevI follow John on Mastodon. He never fails to disappoint. reply hinkley 13 hours agoparentThen why do you follow him? reply barrell 12 hours agorootparentOddly enough, “never fails to disappoint” can have the meaning “never disappoints” as well as “routinely disappoints”. I’ve never thought about that one before reply heresie-dabord 11 hours agorootparentNative EN parser here. I would never consider this usage correct except as a rhetorical (facetious) insult. People may well repeat it without understanding the original nor their mistake. Although if enough people bust the syntax, it may attract descriptivist reporting, as with the widely observed malapropism \"irregardless\". https://english.stackexchange.com/questions/139448/never-fai... https://en.m.wikipedia.org/wiki/Irregardless reply barrell 10 hours agorootparentIt’s not a matter of correctness, but of understanding. OP definitely intended to imply the content does not disappoint, and used a colloquialism most native speakers would understand reply slippy 10 hours agorootparentI am a native speaker and got the gist and saw the paradox, and found the phrasing a bit tortured by the triple negative. Thank you for explaining that this was a colloquialism. Now I have to go look up the etymology... And upon further inspection, this usage is actually a misnegation. \"It is a veiled insult: an ironic form of insult delivery which is misinterpreted as flattery to the buffoon who is targeted by it, much to the entertainment of anyone else within earshot who understands the true meaning.\" reply hinkley 8 hours agorootparentprevOnly in the same sense that “could care less” is understandable but also means the opposite of the intention. reply hinkley 8 hours agorootparentprevI have only ever heard it used as a high brow burn, and a wickedly hard one at that. reply kazinator 11 hours agorootparentprevI've never failed to win a game of mahjong against a bunch of grannies a Chinatown back room joint. I've never tried such a thing; therefore, I've never failed. reply scubbo 9 hours agorootparentprevFrankly, I could care less reply jrmann100 11 hours agorootparentprevHere's a StackExchange thread on this exact mix-up (a \"misnegation\"): https://english.stackexchange.com/questions/139448/never-fai... reply Agingcoder 10 hours agorootparentprevThis is the first time I come across this mistake / non-mistake so I misunderstood your comment. Are you sure it’s a common enough misnegation for people to understand what you meant ? reply barrell 10 hours agorootparentI didn’t use the expression, I don’t think I would have myself, but it didn’t even strike me as odd until I read the comment by hinkley. Did you read the original comment and think BeetleB follows John and thinks all of his content is disappointing? reply jacknews 10 hours agorootparentprevNever heard that one, but maybe it's like 'could care less', which has acquired the opposite of it's actual meaning (the phrase should be 'could not care less') by repeated incorrect use. reply reshlo 11 hours agorootparentprev> can have the meaning “never disappoints” How? reply kazinator 11 hours agorootparentA complete stranger who has nothing whatsoever to do with you, who has never tried to do anything for you, nor has been expected to, has never disappointed you. They've also never failed to disappoint you, because they have not failed in any regard whatsoever. This is an example of a vacuous truth. I've never failed an airliner landing. While that may sound like I'm boasting of being a good pilot, in fact I'm not a pilot at all, and I've never attempted such a thing. Another vacuous truth. Every crow in an empty set of crows is white. Also, every crow in an empty set of crows is black. Propositions universally quantified over an empty set are all vacuously true. Statements with always and never are universally quantified over some set of events. If that set is empty it leads to vacuous truths. \"Every time I've seen a crow, it has always been white\" is vacuously true if I've never seen a crow. I.e. the set of crows I've seen is empty, and consequently is a true statement that they're all white. reply reshlo 7 hours agorootparent> A complete stranger who has nothing whatsoever to do with you, who has never tried to do anything for you, nor has been expected to, has never disappointed you. They've also never failed to disappoint you, because they have not failed in any regard whatsoever. Nobody who uses the phrase ever means it in this way. The point of using the statement is to convey that you are familiar with the person’s history. As another commenter has already pointed out, “has never failed to disappoint” is not the same statement as “never fails to disappoint”. The habitual present can’t refer to empty sets, as it is only used to refer to repeated actions. reply kazinator 40 minutes agorootparent> Nobody who uses the phrase ever means it in this way. That is true. Outside of formal logic situations, deliberately uttered vacuous truths are only ever used by nerds to be clever, or for sarcasm, or insult and such. Someone habitually using \"never fails to disappoint\" intended as a compliment has somehow latched onto an incorrect idiom; they likely intend something slightly funny like \"never manages to disappoint\" (tries hard to disappoint, but never does, due to being so good!). Or maybe it's supposed to be a deliberately funny mixup of \"never fails\" and \"never disappoints\". reply seba_dos1 9 hours agorootparentprev> A complete stranger who has nothing whatsoever to do with you, who has never tried to do anything for you, nor has been expected to, has never disappointed you. They've also never failed to disappoint you, because they have not failed in any regard whatsoever. \"never failed\" != \"never fails\" reply nuancebydefault 3 hours agorootparentprevThey said they were a follower of them though. reply barrell 10 hours agorootparentprev¯\\_(ツ)_/¯ linguistic drift. Technically it means you routinely disappoint, but it’s often used idiomatically to mean the opposite reply ykonstant 11 hours agorootparentprevWhat a country! reply BeetleB 3 hours agoparentprevHeh. This comment blew up on me. Yes, it was a typo. reply d_burfoot 3 hours agoprev [–] > Back before Twitter became a Nazi bar Why does this guy think it's acceptable to bitterly insult so many people, in an offhand way, by making comments like this? What does he think he's gaining? I think HN should have a policy for submitted content that is along the lines of the policies in place for comments. If the content violates the rules (\"Please don't use Hacker News for political or ideological battle\") it should be flagged and removed, like a comment would be. reply ryanmcgarvey 3 hours agoparentIs it a political statement if it's also a statement of fact? Sure, the comment has some color to it, I'll concede that, but one can no longer post these kinds of things on Twitter and get the honest engagement from community members one used to. It's no longer a welcoming place for this kind of discussion. reply BearOso 2 hours agorootparentWith no account, I can no longer read comment chains on Twitter. It will only show the direct comment linked to. If you go to the user's page, all you see are the promoted tweets. There's no way to access the timeline sequentially anymore. With those restrictions, you're writing only to a captive audience if you post on Twitter. So you are technically correct, you literally cannot post these things on Twitter. reply foogazi 2 hours agoparentprev [–] > Why does this guy think it's acceptable to bitterly insult so many people, Won’t someone think of the people? My mom is on X - I don’t see how that offhand remark insults her > I think HN should have a policy for submitted content that is along the lines of the policies in place for comments We can already flag and vote - what more censorship do you want ? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The lemniscate constant, denoted as ϖ, is associated with the ∞-shaped lemniscate curve and is a counterpart to the number pi.",
      "Discussions highlight its potential applications in science fiction, its spelling variations, and its mathematical importance.",
      "The conversation also explores cultural and historical influences on language and mathematics, particularly Greek and Arabic contributions."
    ],
    "points": 360,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1735011682
  },
  {
    "id": 42500475,
    "title": "38th Chaos Communication Congress",
    "originLink": "https://events.ccc.de/congress/2024/infos/index.html",
    "originBody": "38th Chaos Communication Congress Information Channels Info Pages Event Blog Communication & Contact Newcomers Fundamentals and Frequently Asked Questions Junghacker*innen-Tag Azubi-Hacker*innen-Tag Chaos Mentors / Chaospat:innen Prepare Tickets Bildungsurlaub Getting there Contribute Assemblies Participate Lightning Talks Style Guide Experience and Visit Facilities Venue Map and Navigation Bars Hackertours Talks Support and Safety Get Help! Principles of our event and community Accessibility Cameras and Privacy COVID-19 & Infection Protection Hub Events (everything, including talks) Self-organized sessions (SoS) Assemblies Projects Bulletin Board Wiki Backoffice (Maschinenraum) Hub HowTo What is the Hub? How to be an Assembly Events Projects Badges and you Quicklinks Fahrplan (talk schedule) Abfahrplan (music shedule) Chat (Matrix) Livestreams Recordings Phonebook Engelsystem Eventblog Legal Notice & Privacy Policy 38th Chaos Communication Congress Infos 38th Chaos Communication Congress The 38th Chaos Communication Congress (38C3) takes place in Hamburg on 27–30 Dec 2024, and is the 2024 edition of the annual four-day conference on technology, society and utopia organized by the Chaos Computer Club (CCC) and volunteers. Congress offers lectures and workshops and various events on a multitude of topics including (but not limited to) information technology and generally a critical-creative attitude towards technology and the discussion about the effects of technological advances on society. Starting in 1984, Congress has been organized by the community and appreciates all kinds of participation. You are encouraged to contribute by volunteering, setting up and hosting hands-on and self-organized events with the other components of your assembly or presenting your own projects to fellow hackers. Information Channels Info Pages That’s the site you are currently reading. We want to publish all relevant information in short and condensed form here. The pages will be updated as soon as new information becomes available or when information changes, they serve as the single source of truth regarding information about Congress. Event Blog The event blog and these info pages are the main information channels for the event. We will gradually publish more information here, usually accompanied by a post on the event blog explaining what’s new. If you don’t want to miss any updates, subscribe to the blog’s RSS feed. Communication Channels & Contact Find infos how to get in contact & chat with other participants and the organizing teams on our Communication page. Built with MkDocs using a theme provided by Read the Docs. Source",
    "commentLink": "https://news.ycombinator.com/item?id=42500475",
    "commentBody": "38th Chaos Communication Congress (ccc.de)308 points by joeig 10 hours agohidepastfavorite163 comments TheAceOfHearts 10 hours agoWhat talks are people most interested in? It looks like Joscha Bach [0] is continuing his \"From Computation to Consciousness\" series. I've enjoyed some of his older talks so I'll probably check out this new entry as well. A few of the science talks related to biology also seem really interesting, although it reminds me of how much I'm lacking in understanding and knowledge of the topic. It looks like there's a big focus on mixing generative AI with biology research, and I don't know enough to disambiguate whether there truly innovative work happening or if this is an attempt to ride the AI hype cycle. Does anyone here have experience and knowledge on the topic to suggest whether the talks are worth checking out? [0] https://fahrplan.events.ccc.de/congress/2024/fahrplan/talk/3... reply n_plus_1_acc 6 hours agoparentI would expect most of them bring critical and sceptical about AI, just like the general vibe here on HN. reply ElectRabbit 9 hours agoparentprevFor me it's all the embedded and crypto stuff. reply stogot 4 hours agorootparentCrypto as in keys, not crypto as in coins? Can’t believe I have to ask that these days reply FlorianRappl 8 hours agoprevThis is actually one of the few conferences where I really like pretty much every talk. These are not mainstream, yet all of them convey so much knowledge and information. End of the year is always reserved for watching CCC talks... reply rurban 10 hours agoprevLivestreams will be here https://streaming.media.ccc.de/38c3/ reply joeig 10 hours agoprevSchedule: https://fahrplan.events.ccc.de/congress/2024/fahrplan/schedu... Many talks are usually available in the live stream and on demand. reply firefax 5 hours agoprev>To see our schedule, please either enable JavaScript or go here for our NoJS schedule. Great design. Too often sites force me to temporarily whitelist their JS to get basic info. reply lolinder 5 hours agoparentThis is way better than usual, but still a step down from what could be: just load the NoJS schedule with the initial page load and then replace it with your React app or whatever it is if JS is enabled. You can have the same banner at the top letting people know there's more functionality with JS. The only downside is more info over the wire, but it's not that much more, especially as a fraction of the already-slow load times for the JavaScript version. reply diggan 5 hours agorootparentI'm guessing they, like most websites, try to cater to the most common type of visitor of their website. Most of which has JavaScript enabled, even though I'm sure the percentage of visitors with JS enabled is much much lower than on other websites. reply lolinder 4 hours agorootparentRight, but what they actually did strikes me as a slightly weird halfway measure for supporting the NoJS use case. Optimizing away the NoJS page does not gain any significant amount of speed for the average-case visitor, but it does communicate to the NoJS user that while they're at least a citizen (an improvement over most of the web to be sure!) they're still a second-class citizen. Again, this is much better than nothing. I'm just pointing out that this design pattern isn't as good as it could be, and if someone reading this wants to implement full support for the NoJS users they can do so by just making the NoJS landing page be the default and replacing it with the interactive app's content root. reply Zolomon 10 hours agoprevThe ticketing system was horrible, it really was not fair this year around. reply dewey 9 hours agoparentIf you are part of a hacker space you could always get tickets through that quite easily. I think it’s great that they are supporting their original core audience like that. reply pantalaimon 8 hours agorootparentThose hacker spaces only get a limited contingent too, and the hacker space will expect you to help set up their assembly if you get the ticket through them. reply nicce 6 hours agorootparent> and the hacker space will expect you to help set up their assembly if you get the ticket through them. I don’t think that it is bad to demand that. The whole event is organized by volunteers and it is massive yet well done. I have been a volunteer too. reply diggan 5 hours agorootparentprevI can't help but read your \"will expect you to help\" part as trying to say something negative. But if they helped you get a ticket, helping them for an hour or two to setup their space feels like the least you could do. reply aveao 4 hours agorootparentprevI'm a paying, full member of muccc (munich) and ccchh (hamburg), and neither expect you to help with assembly if you get tickets thru them. reply dewey 8 hours agorootparentprevYes, limited contingent, but that's also expected with a limited number of seats at the congress. I'm part of two hacker spaces and none of them expected any help with setting up their assembly. reply DanielleMolloy 4 hours agorootparentprevWhere was that, and when? I got tickets through hackerspaces and was never expected to do anything for them. reply croemer 9 hours agorootparentprevI tried to become member of my local one and they never replied to my application for membership. reply dewey 8 hours agorootparentHave you checked if they have an open day? Many have a dedicated public day every week where you can just drop by and if you are interested someone will probably happily assist you with becoming a member. Otherwise joining their IRC and asking there could also be an option. reply tgsovlerkhgsel 8 hours agorootparentprevTry showing up physically on their open evenings, which most spaces have. reply sevg 10 hours agoparentprevOut of curiosity, what was horrible and unfair about it? (Genuine question, I don’t know anything about how it was done.) reply jazzyjackson 9 hours agorootparentIt's really a pretty strong web of trust system, if you know anyone who's volunteered previous years there are vouchers floating around by word of mouth. If you don't know anyone in the community, purchasing a ticket online is a crapshoot. IMO this is a feature not a bug. reply aveao 6 hours agorootparent1) Volunteers (who worked 15+ hrs on last congress for free) get two tickets with priority. 2) Hackerspaces get a number of tickets with priority, thru replicating tickets (you can only buy one per day). The amount of replicating tickets given depends on size of area (a hackerspace can be tasked with giving vouchers to relevant people in their city). There's a limited quantity of these tickets, separate from amount of vouchers. The main point of the congress is primarily to bring the chaos crowd together, then the rest of interested parties. That model may not fit your vision, but chaos events year after year succeed at not leaving out the core crowd. Also, this year I have bought 2 tickets on public sale for friends without issues (best way to get tickets is to group up with your friends). It's always taken high effort to buy tickets for c3, needing to be exactly on time (not 30 seconds late) to each sale round. 37c3 was an exception due to low sales, caused by skepticism in the event returning after years of not happening. This year that trust was regained and buying tickets became hard again. reply wkat4242 8 hours agorootparentprevFor me the price of accommodation is pretty prohibitive. It's one of the most expensive times of the year and Germany is also one of the most expensive countries in Europe. I've looked at it once but the hotels were too expensive for me (I think it was in Hamburg then). But it's ok, I can watch it online. I don't like travelling during this busy period anyway. reply namibj 7 hours agorootparentThere is affordable accomodation: nearby gymnasiums where you bring a \"bed\" and \"duvet\" and \"towel+shampoo\" and get by with like 5 bucks a night. reply kalium-xyz 7 hours agorootparentI dont think thats for everyone. Speaking as someone who has done it. reply crest 6 hours agorootparentIt's an option for those on a tight budget aka \"beggars can't be choosers\". Hotels are a lot cheaper if you book a (fully) refundable stay months in advance instead of waiting until you have a ticket. reply aveao 5 hours agorootparentFriends who booked months earlier still paid a bit of a premium. Much cheaper than now of course, but you should still budget for €150/night or so. reply mr_mitm 6 hours agorootparentprevOf course not, it's for people who can't spend more than five bucks a night. reply wkat4242 6 hours agorootparentprevOh yeah I'm too old for that though :) reply seb1204 6 hours agorootparentLive life, give it a go. reply wkat4242 3 hours agorootparentI've done that in the past (I lived in youth hostels for years in fact) but I can't now. Especially because I sleep with a CPAP machine as well. reply croes 7 hours agorootparentprevMost expensive? https://www.numbeo.com/cost-of-living/rankings_by_country.js... Some things are pretty cheap compared to other countries, meat for instance reply wkat4242 6 hours agorootparentTrue but I'm in Spain. Last time I looked a hotel or guesthouse was around 200€ a night which is really a lot for me. Of course this is a time when lots of people travel. But I didn't mean to complain, just to say it's not only the ticket availability that's a factor. And this is not something the organisation can help of course. It's a German event so of course it's in Germany. And the tickets themselves are well priced. And like I said, I really appreciate them making everything available online. I do tend to go to the Dutch hacker camps that are being held once every 4 years. reply bergen 8 hours agorootparentprevWe were a group of 5 and there were 3 separate presales for public. We all got tickets and it worked out pretty decently, like last year. reply pantalaimon 8 hours agorootparentprevIt’s an inventive to volunteer reply aveao 4 hours agorootparentOther way around: Easier tickets is an incentive for volunteers, as a bit of a payment in kind. It helps people known to volunteer come back, and it encourages them to keep volunteering year after year. You need to work, honestly, ridiculous amounts of hours to get a voucher (15-20 hours), and so it'd not be a very good incentive for average person. You can absolutely get tickets on public sales by knowing when sales start, refreshing on the dot and doing the captcha as fast as you can. Reducing your latency (ethernet, fresh browser profile without extensions) helps. Add some friends into the mix and you'll get all the tickets you need. reply Vespasian 7 hours agorootparentprevIt really isn't. Tickets are not that hard to get and you are not required to do anything once you have one. I believe some core orga members (aka The people contracting the venue, getting permissions and renting equipment etc) are guaranteed a ticket but almost everybody else has to buy their own. The fact that the \"hacker space community\" gets preferential treatment is very much intended. reply weinzierl 8 hours agorootparentprevFrom what I have heard it used to be quite easy to get tickets in the previous post-pandemic years. Before the pandemic and especially before Leipzig it used to be very hard sometimes. Now is the first year that seems to me where demand picked up it was not a given to get a ticket. This might have contributed to find the system unfair. reply aveao 4 hours agorootparentThere was only one other congress after the pandemic and that was 37c3 (last year). That one indeed did not sell out and was an outliar. I did go to 36c3 (end of dec 2019, right before pandemic) and the difficulty of buying tickets was the same as this year. reply DanielleMolloy 4 hours agorootparentInteresting that it did not sell out. I've heard there was a lot of concern in the community about covid. reply aveao 4 hours agorootparentI think it was a number of reasons, covid was one of them. - Congress on years where there's cccamp tends to be planned by people who are understandably more exhausted. 37c3 and cccamp23 were on same year and 37c3 slogan was appropriately \"resource extension\". - The venue was back from Leipzig to Hamburg (CCH was doing renovations for a while, so it was moved to CCL for a few years). - 4 years between events lead to both changes in the orga people, and general concern that the older set of people had been at this point less familiar with running an event of this scale (\"can they still do it and make it feel the same?\"). - Covid precautions were a divisive topic. Some didn't want precautions, some wanted more precautions. Ultimately some measures were taken, but none were mandatory. We have distributed free tests and masks last year, we'll do it again this year, though mainly aimed at volunteers, so you should bring your own mask and test if you can, see the info page^1 for full recommendations :) All this lead to people being unsure if 37c3 would be good and not coming. But I think that trust has since been regained, seeing as this year sold out really fast (same as past years). I think one other issue was that chaos community is getting older and many are having families, which makes going to congress between christmas and NYE difficult. We did, imo, really good outreach since then and now there's more young people joining chaos communities again. Still, coming to congress is costlier for younger people that earn less (175eur for ticket but cheaper options are available on request, plus 400-600eur in hotels, plus trains/flights/visas etc). (I was/am part of the infection protection team at 37c3 and 38c3 but am speaking on personal capacity.) ^1: https://events.ccc.de/congress/2024/infos/corona.html reply zhouzhao 9 hours agorootparentprevAlso curios, since I got 3 trickets for me and friends. It's as always, go somewhere with a good ping, refresh at the specific time. Worked in every sale.. reply Eduard 8 hours agorootparentprevfirst come first serve principle on three occasions. refresh the browser a few milliseconds too late, and you will not be able to get a ticket. The \"slide the slider\" captcha is an accessibility barrier. reply ognarb 8 hours agorootparentFrom their FAQ: > I’m blind/I have problems with my visions and I can’t navigate your captcha. > Please contact us. https://events.ccc.de/congress/2023/infos/tickets.html#faq reply DanielleMolloy 4 hours agoparentprevThe underlying problem is the lack of tickets. The only way accommodate more attendees and grow would have been to move permanently to a fairground like Leipzig. Unfortunately, there seems to be no willingness to do so. I found the Leipzig events phenomenal and would like to understand the reasons behind this decision, but can only find speculation.. maybe fairgrounds are simply too expensive? Leipzig’s hotel situation is worse due to having to connect to the fairground outside town. However, due to Leipzigs location at the intersection of two major historic European trade routes (fyi: via imperii and via regia, still has the largest head railway station in Europe), it has much better connections than Hamburg to the rest of Germany and Europe, including Berlin. Also Leipzig (and the fairground itself) have train connections to three airports including BER.. reply dlmotol 3 hours agorootparentThe hotel in Leipzig was easy because the tram went straight to the city center 24/4 . As far as I know people don't like the huge space and I agree. The magic of the Congress in Berlin or Hamburg wasn't there in Leipzig. Also the logistics center of the CCC is somewhere around Berlin. The size of Leipzig also motivated a few not so fitting people to that conference too. reply towawy 6 hours agoparentprevI'm also still searching. If anyone is looking to part with a ticket, feel free to send me an offen. Contact is in my profile reply gsich 6 hours agoparentprevThe system was the same as last year. reply jdndene 8 hours agoparentprevHow was it not fair? While I would prefer limiting the attendence by price and not by come first, get first, I understand their reasoning But getting a ticket this year was very easy, I got three on two separate occasions. reply seb1204 6 hours agorootparentBy price? So only the rich kids can go? Sounds really the opposite attitude than what I hear from CCC folks reply diggan 4 hours agorootparentThere are \"friends\" tickets (or called something similar) since they want everyone to be able to come, regardless of socioeconomic background. I feel like if there is any organization trying to make the conference audience as broad as possible, it's probably CCC. reply sourcepluck 9 hours agoprevKeep meaning to go to this. What a great thing, the talks are often wonderful! reply namibj 7 hours agoparentThen get your calendar entries for 39c3 and subscribe to the relevant information channels to be notified of when those details have been worked out. reply wunderwuzzi23 5 hours agoprevStill bummed that the CFP was only 10 days this year, and I totally missed it. reply Y_Y 9 hours agoprevI love watching CCC every years though I rarely catch it live. Since we have a couple of days to wait for this year's talk, I'll ask. Does it bother anyone else that some talks are in German, or does it bother anyone else that they find themselves bothered that the talks are in German? I love all languages great and small, natural and formal and so I'm conflicted on subjects like this. I find young German speakers to be generally both good at English and pragmatic about choice of language and I'm glad to see that they haven't ceded the ground entirely. On the other hand you might consider a language as a network whose utility grows with number of nodes. Or you believe in the inevitability of Gresham's Law driving out good currency with bad. I have sympathy for this point, and in an emergency in a mixed nationality group would certainly shout \"fire\" in English first. As an example I'll be interested to watch this one about data protection for age verification: https://fahrplan.events.ccc.de/congress/2024/fahrplan/talk/S... but feel it would be a shame if the reach was limited by the choice of language. AFAIR they don't do multilingual restreams (automated or otherwise) like some other online events. (Übrigens kann ich doch Deutsch, trotzdem mein Standpunkt bleibt.) reply lispm 6 hours agoparentI live in Hamburg/Germany, where the CCC is hosted. It would feel strange to go to a German hacker congress in a German city and people would expect that everything is in English, first. It's organized by volunteers from various CCC groups from all over Germany and not by a commercial congress provider. The result may look like a \"professional\" congress, but that's the result by countless hours of volunteers enabling it. Generally there is much more to the congress, then the talks you can see online. It's a large gathering (> 14k participants with >2500k volunteer organizers) of people from all kinds of German grassroots hacker domains... with a lot of groups meeting there and presenting themselves. The Hamburg congress center, where it takes place, is not the largest location possible, but it currently seems to be the best mix of a congress center in the middle of a large German city with lots of rooms and space for all the groups. Having CCC talks translated to and from English/... is fine, as it's actually done. https://events.ccc.de/en/2024/11/26/call-for-interpreters-tr... says: > We interpret ALL talks in the three main halls and the two community stages live and in real-time. German talks are interpreted into English, and vice versa. Our work is transmitted live in the lecture halls, streamed to the Internet, and recordings are published on CCC sites and YouTube. We have another channel where we interpret into more languages, this is transmitted and published in the same way. reply korfuri 8 hours agoparentprevHi, I'm a long standing CCC interpreter (volunteer of course). We aim to interpret live 100% of the talks in German (* we do not always interpret things that aren't strictly speaking talks, like poetry readings or performance art, although we try to make these as accessible as we can). We also interpret various talks into other languages - we have a sizeable team working on French and Spanish interpretation, and depending on volunteer availability, we are keen to be target any spoken language. For this talk you are interested in, I'm very confident it will be interpreted into English. You can find our work on media.ccc.de both on the streams and recorded talks. If you're attending live we have lower latency audio streams available on-site, check out c3lingo.org. reply miki123211 8 hours agorootparentThank you, I didn't realize this existed. I have accessibility needs (screen reader user) that make subtitles less fun to use than they would otherwise be, and I speak no German, so those interpreted recordings would be really great for me. I assume you don't publish on Youtube? That's how I usually consume CCC content, mostly due to YT's recommendation algorithms and excellent cross-device \"continue playback\" support. reply crest 6 hours agorootparentThe CCC is a big believer in self hosting there is a live streaming platform during the event and media.ccc.de afterwards. reply Y_Y 5 hours agorootparentMy preferred YouTube client (NewPipe) even includes a CCC backend too. reply korfuri 7 hours agorootparentprevI'm glad we can be of service to you! There used to be a live captioning team writing subtitles but I think that effort stopped. I heard people were looking at AI-based captioning but I have not kept up with the plans for this year. I know in previous years talks were published on YT, I assume this will still be the case this year. Normally the VOC team does miracles to publish incredibly quickly too, so you should be able to have the recorded talks online same-day. reply namibj 7 hours agorootparentprevUntil very recently YouTube did not support multiple user-choice audio tracks on a video, so they would have needed to be uploaded separately. Maybe it will change this year, though! reply bdhcuidbebe 5 hours agorootparentanton petrov channnel that i follow recently started offering multiple auto dubbed audio track, which confused me as it would auto select the german dub for me. i dont know german ;-) reply Y_Y 5 hours agorootparentprevThat's great to hear and I think it's admirable work you are doing. Thanks for correcting my misconception and making this excellent content more accessible. reply phlo 9 hours agoparentprev> AFAIR they don't do multilingual restreams (automated or otherwise) like some other online events. They do. There's a team of interpreters at CCC who translate most talks between English and German, and to some other languages. As with most things CCC, it's all volunteer-driven and done on a best-effort basis, but it's there. When interpretation is available, I prefer talks that are given in whichever language the presenter is most comfortable in. Presenting in front of a large audience is stressful enough as it is, so IMO it makes a ton of sense _not_ to worry about doing that in a second/third language, and delegating that part of the presentation to someone else instead. reply sourcepluck 9 hours agoparentprevI think it's great. I've good passable German, not enough to really follow the details of a fast-paced, complicated talk, so I'm practically shut off from German media (at this level, anyway). But still, diversity is good, and the American-Anglification of all things cultural and linguistical is not necessarily a net positive. As in, I'm aware there's good points to it, but it's not an entirely clear picture, and some alternative things existing is probably good. How many 12-year old Germans who dislike English for one reason or another found CCC and had major positive changes occur in their lives, ya know. reply HeckFeck 7 hours agorootparentSome thoughts are too complex for English. Three genders means they are always one step ahead of the French. Shunning Latin root words means you can be more efficient. German persists for good reasons. reply mr_mitm 6 hours agorootparentCan you give an example of a thought that cannot be expressed in English? That sounds like quite the claim without anything backing it up. I speak fluent German for what it's worth. reply davrosthedalek 5 hours agorootparentFor what it's worth, I only have an example for the other way around: Accuracy vs precision. It's \"Genauigkeit\". But there is a subtle difference in meaning in English that is lost in German without additional explanation. reply bratwurst3000 4 hours agorootparentyou can translate many english „complicated“ words one to one in german because german has a anglo saxon in it. for example accuracy is akkurat precission is prezision hypocrite is hypokrit etc german has a crazy big vocabulary but most words arent used and in generell the language is simplified in everyday use. for example everyone has angst but noone has bange anymore. btw good word for anxious in german. i once also had the impression that many nuances are lost in translation but then i had to realize germans are not using all the words of the vocabulary. reply davrosthedalek 3 hours agorootparentWe have akkurat, which is only an adjective, there is not really a Noun version. But in any case, we would not give it the same meaning shift as you have in English, at least how it's used in physics. Learning German is hard in the beginning, because of the more challenging grammar and \"Donaudampfschifffahrtskapitaensmuetzenfabrikantengattin\" but then becomes easier, as there a lot less word plays and figures of speech. It's quite regular. English is easy to get into, but to master it, you have to know them all. And the random pronunciation, of course. Ja, Bange is uncommon. Really only used in \"Da wird mir Angst und Bange!\". and \"Bange machen\". It is still amazing to me that \"Giving up the ghost\" is a figure of speech in both English and German. reply bratwurst3000 5 hours agorootparentprevthe only i can think of is sentence with comatas. In german you can have as many „hauptsätze“ in one sentence as long they are separated by a comma. for example what thomas berndhard does is crazy. its one sentence per page. i think in german you can be more precise in one sentence. but that will be a hell of a sentence. reply dpassens 5 hours agorootparentCan't you just use a semicolon to do the same in English? And even if not, just because an idea is split over several sentences doesn't mean you can't communicate it. That's what paragraphs are for. reply mr_mitm 5 hours agorootparentYou can, and probably should. Texts like Kant are borderline incomprehensible even to native Germans and seem to only serve the author's need for displaying their intelligence. The ideas they are conveying are not that complex. Relativistic quantum theory or string theory can be expressed in English (plus math, while it's not like math wouldn't be needed in German) just fine, so I really don't understand where these supposed limitations of English are. reply bratwurst3000 4 hours agorootparentyeah i am with you. Sometimes they overdoe it for no reason. I also prefer english to learn sth. Only because it is more straightforward and simpler to communicate. I love it reply dpassens 5 hours agorootparentprev> Some thoughts are too complex for English. English just adopts all the good German words. Every time I, a native German speaker, have struggled to express something in English it was either because my personal vocabulary was lacking or because of societal differences, never because English is somehow inferior. reply davrosthedalek 5 hours agorootparentAnd German has adopted all the good English words. Like Handy (cell phone), Beamer (projector), and body bag (fanny bag). Oh wait.... reply dpassens 4 hours agorootparentI've never heard a fanny pack called a body bag in German. It's a Bauchtasche in my family. reply davrosthedalek 3 hours agorootparentMight have been a regional thing. But I think there was an ad on tv... Didn't live long, was too embarrassing :) Of course Handy is Schwaebisch: Ai handy kai Schnurr? reply davrosthedalek 6 hours agoparentprevAs a native German speaker living in the US, I have mixed feelings: - Talks in German feel slightly less, hmm, professional? Maybe it's the field I grow up in (physics). Like, ok, in your studies you speak German, but when you grow up and go to big-boy conferences, it's English. The Congress should be big-boy league. - Also, realistically, many technical terms are English, so the a German talk might have to make hard choices. CPU, not \"zentrale Recheneinheit\" - At the same time, I like that it lowers the barriers for entry. Speakers and visitors who are not fluent in English can participate. - Personally, though, I prefer a good fluent German presentation from a cringy English one. As a German native speaker, typical German mistakes in English irritate me more than others, I think, so I found some talks given in English hard to follow. Pet peeve: Technology. I think that's a better Shibboleth than Flash-Thunder-Welcome. - Realistically though, the congress is a worldwide phenomenon, and important, and as English is the current lingua franca (hah), I think they should encourage English presentations, or at least English slides. (Because they provide translated audio in the recordings!) - Lastly, I can only encourage everyone to learn more languages, at a young age. I kick myself I didn't. But even in languages which are relatively close, like English and German (compared to, say, Japanese), I found that one language has concepts the other one doesn't, and that changes how you think about things. For example, English has accuracy and precision, with, at least in physics, different meanings. In German, it's both \"Genauigkeit\". (Edited: Formatting) reply lispm 5 hours agorootparent> \"Realistically though, the congress is a worldwide phenomenon\" Actually it is an open meeting of the German members of the CCC. What you see online is an important part of the congress, but it is only a part of the whole thing. Much more it is a gathering of activists. Thus its from the CCC for CCC. Guests are welcome. Since the talks maybe interesting to a larger audience they are streamed and translated in real-time. Visit the congress in Hamburg and you are beamed to an alternate reality for four days and nights, meeting all kinds of interesting people in person. That experience is a bit more mind-blowing than seeing translated talks online. ;-) People from outside Germany are visiting the congress. reply aveao 4 hours agorootparentComing to my first congress (36c3) was special in this regard: Congress was all about talks for me before, but I only went to 2 talks. Last year I went to none. This year I plan to go to none too. Congress is a gathering ground for likeminded people from all sorts of interests. Talks can be watched at home (except for ones that are not recorded at the request of the talk-givers), but being in the room with likeminded people, talking with them, learning from them is IMO irreplaceable. reply davrosthedalek 5 hours agorootparentprev>> \"Realistically though, the congress is a worldwide phenomenon\" >People from outside Germany are visiting the congress. My point exactly! International people want to take part, best in-person, and they are more able to take part if as much as possible is available in the common-denominator language. I applaud CCC for all the effort they put in to make it such an awesome and accessible event! Maybe they do this already: I'd ask all presenters to prepare slides in English, either as primary, or as a backup. So that the English stream could show the English slides. Hey, I'd even volunteer to help translate! I may be too used to physics conferences. The German physics society (btw the largest physicists society in the world) has a yearly conference (well, multiple, split by topic), and they tend to be attended by many non-Germans. So many of the talks are in English, especially the main ones. But there are many many parallel session filled with short talks. Many of these are \"my first talk at a conference\" type of talks, and are in German. That's perfectly OK. That's part of the role of that conference: Every student submission is accepted. Some are good, most are not great, both from content (because you don't present world changing news in a short talk in a parallel session) and style (because giving good talks requires experience). That's all OK, and important. And it's important to lower the stress level by not also requiring English. Of course, as a \"grown up\", you go there to support your students, to chat and meet colleagues, and it's great for that. But I am also a lot at \"higher level\" conferences, with a more robust vetting of talks or even only invite only talks. The talk quality is much better. They are 100% in English. So in my brain, I connect \"talk is in German\" with \"everybody can give a talk\", and \"my-first-talk-ever\", but not with \"guaranteed super high quality talks\". But CCC has to sort out many talks, AFAIU, so \"everybody can give a talk\" isn't true. reply diggan 4 hours agorootparentprev> The Congress should be big-boy league. I'm not sure how many people actively involved with the community who agrees with this. It's a grassroots movement and I'm fairly sure many (most?) people want it to remain as such. reply jyounker 6 hours agoparentprevIt is a German conference. It was started by Germans. It is organized first and foremost by Germans. It happens in Germany. I find it arrogant to think that presenters should speak a language other than German. reply Y_Y 5 hours agorootparentHave you been to a German university? Plenty of technical subjects will be taught in English even if the speaker and audience are German. There surely us a kind of arrogance that demands Germans speak English, but I don't see anyone in this thread expressing that. (I won't even begin to talk about the variety of dialects and identities contained within the modern German state, except to say that there is plenty of arrogance to be found there for those who seek it.) reply Scarblac 9 hours agoparentprevOn the one hand, communication is easiest if everybody uses the same language. On the other, the dominance of English is one thing that gives US too much cultural power. I think I'm happy that there is still content in other languages out there. reply usr1106 8 hours agorootparentCommunication is not good if everybody speaks the same language at a different level. I have worked in English for close to 30 years and been at many international conferences, presented myself a couple of times. Still I feel handicapped in some presentations/discussions that are dominated by native speakers not using offshore English. reply davrosthedalek 6 hours agorootparentHmm, not sure, I'm non-native English speaker as well. I have a lot more problems understanding/being understood by non-native English speakers from a different language background than I or they have with a native (American) English speaker. Or do you mean America with offshore? reply Y_Y 4 hours agorootparentWhat you say matches my experience. I grew up speaking \"local\" English, but now my day-to-day language is \"international\" English. I find it funny when people compliment me on my clear and simple English, because when I talk to my family it's anything but. In my job I deal with tons of non-native speakers, and in some cases I'll end up \"translating\" between two speakers who can't understand each other, even though it's all nominally English. This is especially the case when they're from different parts of the world and accent and native language strongly influence their speech. reply miki123211 7 hours agorootparentprev> On the other, the dominance of English is one thing that gives US too much cultural power. I think you got the causation wrong; the dominance of English is due to the US having cultural power, not the other way around. If this were true, you'd see Ireland, Canada and Australia also having significant cultural power, and this just isn't the case. reply cjs_ac 7 hours agorootparentI think the dominance of English comes from the United Kingdom's significant cultural power in the nineteenth century (cf. French's status as the language of diplomacy deriving from France's cultural power in the eighteenth century). The US is just riding on Britain's (England's, really) cultural coattails. (The US's economic dominance is, at present, unrivalled, however.) reply bratwurst3000 5 hours agorootparenti dont know anyone who speaks English because of that. all my friends and all people i met do it because US culture stuff like tv music articles. also here in europe its because of USA. Noone speaks british english here. With english we mean US english. also the other reason is work and travell and thats also mostly US english. if britain would speak chinese noone would speak chinese. everyone would speeak us English. Its crazy that people still have a britain centric worldview. that ship sailed 100 years ago and then it became the american century. reply rsynnott 7 hours agoparentprev> Does it bother anyone else that some talks are in German, or does it bother anyone else that they find themselves bothered that the talks are in German? There’s generally live translation (into English, German, and sometimes French). The translations are also available for download (eg https://media.ccc.de/c/37c3/). reply bleakenthusiasm 8 hours agoparentprevThey usually do Translation live, so I'd be very surprised if they didn't offer them in the stream as well. They always offer English and I think some talks have been live-transcribed, too, for people with hearing disabilities, but that might be automated by now. Personally I think it's good you are not forced to present in English. I know enough people who are not comfortable enough with English to present in it. There are also some niche topics that have a focus on Germany. For these sometimes German brings a bit of nuance/local flair that you can't really translate. For these I'm happy that German is available as the original and then the translators will do their best to provide an English second best. To some degree I find it inevitable that conferences situated in countries that are not native English speaking will have some program points in the local language. As long as they offer help with understanding the content, I don't see an issue with this, regardless of how big/influential they are. reply crest 6 hours agoparentprevThere is a realtime translation team that's available as an audio track in the life streams as. The translation is done be volunteers. The last ~5 years they managed to translate all the German and English slots on the official stages. Some less common language options may be provided later e.g. Spanish, French, Chinese, funni dialects like Plattdeutsch, etc. reply Y_Y 4 hours agorootparentThanks for the info, I don't know how I wasn't aware. May it serve as a warning, I put myself in mortal danger recently by referring to Plattdeutsch as a dialect (rather than a language). I have no skin in that game but some people feel strongly about it. I'm general though I'd say that: > a shprakh iz a dyalekt mit an armey un flot[0] (Here I've rendered the Yiddish in Latin rather than Hebrew for emphasis.) [0] https://en.wikipedia.org/wiki/A_language_is_a_dialect_with_a... reply zxexz 8 hours agoparentprevI've been to CCC (and hope to continue doing so again when life ceases to get in the way), and it's never bothered me in the slightest. The Congress is organized by the Chaos Computer Club, founded in Germany and consists primarily of decentralized clubs/associations that themselves are German-speaking. The talks (often) live-dubbed English, and post-talk the talks are translated as well (both the live translation and the post-recording subtitling are done by volunteers, BTW). The majority of congress-goers speak German as a first language, and frankly many of those who don't speak German still attend German talks - thanks to the translators (you can even get a sense of this by watching the talks on the website; there are many instances of English being used for questions and even answers in the Q&A sections of German talks). P ersonally, I believe the world would be less interesting if everything of interest was in the same language - I think all (major, at least - those with a budget for translators/enough volunteers to translate) conferences should allow the speakers to give their talks in the language they are comfortable in. Anecdotally, I've never had passable German conversationally, but have studied the language a fair bit, and watching so many German talks with translation (both remotely and in-person) actually passively brought up my understanding of the language that I could understand most of what was being said; to the point that I felt comfortable over the years passively understanding the German language outside of the congress in most situations. Sure, being able to to speak in another language comfortably is ideal, but being able to listen, even just passively, in another language really feels like a superpower. reply dewey 9 hours agoparentprevWhen I was there where was always a group of people sitting in the first row live-translating the talks. That was many years ago so this is probably even better now. reply namibj 9 hours agoparentprevI just checked; they do in fact upload the live dub audio streams, see this talk from last year which was given in English but this is the German dub: https://media.ccc.de/v/37c3-12340-37c3_infrastructure_review... reply sneak 9 hours agoparentprevJust because people are good at English doesn't mean their English skills are up to giving a permanently-archived talk to tens of thousands of people at the 1st or 2nd most important hacker con on the planet. I'm all for giving a presentation in the language you're best at. Let machine translation (or manual translation) pick up the slack, not one person's possibly-mediocre ESL skills. I also can speak German but I avoid doing so when technical accuracy is paramount, because sometimes, small details really matter. reply hengheng 9 hours agorootparentI'm a native speaker and I hate how people have been stumbling through their presentations, lacking basic vocabulary, being dragged along by their bullet points. It wasn't even the accent, it was that the language occupied their minds so much that they couldn't think about what they had to say or how they were saying it. Lots of talks felt like watching a ninth grader in front of the classroom. Now some of these guys weren't amazing presenters in German either, but my point is that they should not have to be. Some people simply are dry explainers and they don't naturally entertain. Nothing wrong with that at all, especially with such nerdy topics. But in those cases, I'd much prefer a lengthy blog article, or maybe a podcast interview with a capable host. Please, CCC, don't send these people on a stage that they don't enjoy when you could also work with them to get their stuff across. And with how much is consumed as VOD, I don't think the live talk brings a lot to the table anymore. There could be live Q&A sessions for those who read article or heard the podcast interview. (I'd argue the same for an academic context by the way - anything above seminar size is either a celebration or a sermon, but not a discussion.) My best experience at CCC was to literally stumble over bunnie on the side of a hallway where he was sitting against the wall, introducing his own ARM laptop to a dozen of people or two. And don't tell me this doesn't scale - he has also gotten his message across in ways that scale. But there was finally a natural way of asking questions in a back and forth manner that wasn't in front of a 500 people audience. reply davrosthedalek 4 hours agorootparent>Please, CCC, don't send these people on a stage that they don't enjoy when you could also work with them to get their stuff across. Hmm, maybe this is actually a problem the community can address. For our conferences, people who are new to giving talks normally have a practice run or two at their universities with local people. Then they get feedback how to improve the talk. It doesn't work wonders, but it does help quite a bit. Not only does the feedback help improve the slides, the style of presentation, it also builds confidence. Maybe CCC/somebody could organize optional \"training sessions\" where people can give test-runs. I'd volunteer to listen and give feedback. Or maybe a tutorial: \"How to give a good congress talk\" reply aveao 4 hours agorootparentThese sessions would help few imo. It'd eat into people's congress time (few arrive before the event due to christmas), making it less appealing. Many also prepare their slides and talk last minute. Not a whole lot that can be done there. reply davrosthedalek 3 hours agorootparentYeah, I meant this to be an online thing a week or two in advance. Doesn't help against last minute slides, but from my experience, the people doing presentations for the first time, don't prepare them so late. reply Sebb767 8 hours agorootparentprev> Please, CCC, don't send these people on a stage that they don't enjoy when you could also work with them to get their stuff across. The speakers are not selected by the CCC; they have to apply with a synopsis of their talk. reply dingensundso 8 hours agorootparentAnd then some of the talks they are selected. reply sneak 8 hours agorootparentprevThis is the conference equivalent of \"this meeting could have been an email\" - \"this talk could have been a blog post\". Perhaps you could do us all the service of making a blog post for each of the CCC talks this year? :D reply pantalaimon 8 hours agorootparentSounds like the job for a LLM reply ulnarkressty 7 hours agoparentprevI have commented about this in the past and got downvoted for it. There is a reason why virtually all international conferences require presentations and talks to be held in English. If this was a German-only event it would be understandable, however as it is written on their website the CCC congress is \"the biggest European hacker gathering and has grown into one of the most important conferences on digital transformation.\" Another argument put forward is the existence of English dubs for every video. I have found the quality of these dubs varies a lot, since they are run by volunteers and not people experienced in live translation. In some talks there can be segments without any translation, and in one case the translator even \"gave up\" because the speaker was talking too fast. reply lispm 5 hours agorootparent> \"the biggest European hacker gathering and has grown into one of the most important conferences on digital transformation.\" \"biggest European hacker gathering\" -> still most participants are from Germany, that's where the Chaos Computer Club is located. The club itself is organized into local groups all over the country (plus some in neighbor countries). reply aveao 4 hours agorootparentprevIt's a balance. Congress aims to be accessible ticket cost-wise to all. Professional interpreters cost a lot of money, and as such it's done by volunteers. There's little to be done while maintaining the spirit of the event. Nothing prevents people from redubbing talks either, which may honestly be a good community effort. reply Simon_O_Rourke 8 hours agoprevThere's a goodly proportion of proper hacker talks at it, like \"Hacking the RP2350\". And I'm glad there's fewer (but still some) of the usual carpetbagging talks, of the \"Ethics and Hacking the RP2350\" or worse still of the \"Sexuality, Gender and Hacking the RP2350\" variety. reply blacklion 6 hours agoparentThere are a lot of \"Red\" (Ethics, Society & Politics) talks. And a lot of \"Yellow\" (Art & Beauty) too. I don't say it is bad thing, as all \"our\" tries to be \"be out-of-politic\" and \"don't bring politic to our beloved technology\" is why we are where we are now (in rather sad world, IMHO). reply lispm 5 hours agoparentprevYou have a narrow view of \"proper hacker talks\" and the purpose of the Chaos Computer Club. The congress is representative for what is of interest for CCC members. reply mauricioc 7 hours agoparentprevNow I really want to go to a \"Sexuality, Gender and Hacking the RP2350\" talk. Maybe next year? reply flawn 9 hours agoprevBanger program once again! WOW! reply a9ex 8 hours agoprevWhich talks are you looking forward to watch? reply sylware 5 hours agoprevWhat about the 0-days trading backroom? Still a thing since now you have more special services agents than anything else... or mafia... reply bijant 5 hours agoprevnext [6 more] [flagged] arrrg 4 hours agoparentYou went into women only spaces with the obvious intent to troll. Quote: „Deshalb ist der Chaos Computer Club nach seiner Satzung und nach dem Willen der Mitglieder eine galaktische Gemeinschaft für alle Lebensformen. Als solche wollen wir allen Teilnehmenden eine sichere und schöne Erfahrung auf unseren Veranstaltungen bieten, unabhängig von Alter, geschlechtlicher und sexueller Identität, körperlichen und geistigen Voraussetzungen, ethnischer, regionaler und/oder religiöser Zugehörigkeit bzw. Herkunft, äußerlicher Erscheinung oder sozioökonomischer Stellung. Wer sich dieser Offenheit nicht verpflichtet fühlt, hat bei uns nichts zu suchen.“ Translation: “That is why the Chaos Computer Club, according to its constitution and the will of its members, is a galactic community for all life forms. As such, we want to offer all participants a safe and enjoyable experience at our events, regardless of age, gender and sexual identity, physical and mental abilities, ethnic, regional and/or religious affiliation or origin, physical appearance or socio-economic status. Anyone who does not feel committed to this openness has no place with us.” Anyone who goes into protected spaces with the intent to troll and score political points is an obvious (metaphorical) bomb thrower who should have no place in any events that want to provide all participants a “safe and enjoyable experience”. “Open to all creatures” does obviously not mean anyone can come. Anyone who themselves cannot be open to all creatures has to be aggressively excluded from such events. That is the only way to defend openness. reply NetOpWibby 4 hours agorootparentYIKES And dude says he was thrown out for “political reasons” as if it was just a minor thing. SMDH He needs to get a grip. reply diggan 5 hours agoparentprevEvents like these have always been politicized, but I guess in a different direction than what you are personally seeing now. Is there any 3rd party articles/posts about the events you're describing? reply mxfh 4 hours agorootparentIf you think the ccc was ever not political, I don't know what to tell you. I just hope you mistook the congress for some random tech-sponsored event for lack of better knowledge. It's founder worked for taz, probably the largest most left-wing newspaper in the BRD at the time. The founding call was puplished in there, the club never shied away from positioning itself clearly on the left in political discussions. https://en.wikipedia.org/wiki/Wau_Holland reply diggan 4 hours agorootparentYes, this is more or less what I was trying to say, but painting it even broader and saying Hacker events like these as a whole are political in nature, not just CCC. reply dtquad 9 hours agoprev [–] 38 years of advocacy for more degrowth and regulations in Europe. reply usr1106 8 hours agoparentUnfortunately not enough. I hate living in a European colony exploited by unethical US oligopolists. reply dralley 4 hours agorootparentHaving some domestic competition would be preferable, would it not? reply andrepd 8 hours agoparentprevNo need to sell it to me, I'm already sold! reply xnacly 9 hours agoparentprevCan you elaborate? reply tgsovlerkhgsel 8 hours agorootparentCCC has a mix of talks from deeply technical to political/societal, and the latter are heavily left/environmentalist slanted. reply wkat4242 8 hours agorootparentYeah it's pretty activistic in a progressive way but that goes hand in hand with hacker culture. At least in Europe. It's part of the attraction for me. Though I don't go to CCC but I do to other events. reply xnacly 4 hours agorootparentprevYeah but thats a good thing :) reply ralfd 8 hours agoparentprev [–] Your snark is a bit unfair. But I also noted how far-left and anti-capitalistic many talks/speaker are. I guess it shows the power differential between US vs Europe/Germany and also the difference between disrupting American and risk-averse European culture? Super smart people in the Bay Area found startups to get rich (and invent the future). Super smart people in Germany can only go into academia and research how to decolonize digital activism. reply jeffreygoesto 8 hours agorootparentMaybe it is a valid position to ask where the money for \"get rich\" comes from and if society would be different if it wasn't all about that money trickling upwards? Have you been to schools or nursing homes lately, having to let dear relatives go there? reply cbmuser 8 hours agorootparentHave been to any actual socialist country? Have experienced what scarcity means and what it feels like to be locked into your own country without a real chance of leaving until you reach retirement age? reply Ladsko 7 hours agorootparentThe point is to meet somewhere in the middle and find compromises that prohibit uncontrolled growth of wealth and power by single individuals without crippling the economy too much. It does not have to be one extreme or the other. reply ahartmetz 8 hours agorootparentprevI have yet to see someone advocating for actual socialism. There is a huge difference between social democracy and socialism, but it's a popular argumentation technique in the US to pretend that they are the same. reply mytailorisrich 8 hours agorootparentThere are socialist parties in every European countries, and some of them get a lot of votes. In Germany, Die Linke is anti-capitalist and advocates \"actual socialism\" and current has 39 MPs in the Bundestag. In France, LFI (France Unbowed) is anti-capitalist and advocates \"actual socialism\" and currently has 71 MPs in the French parliament (second largest party by number of MPs...). There are also several smaller parties that are more \"hardcore\", including the historic French Communist Party that still has 8 MPs and 14 Senators... Now I do agree that social democracy is not socialism, but again, plenty of socialists in Europe, too. On a related note, when people claim that they are \"anti-capitalist\" (which seems to be more popular than claiming to be socialist) it does not really leave many alternatives, just semantic flavours of socialism. reply CrLf 7 hours agorootparentPerhaps it's important to point out that socialism != communism. I think this is something the US really doesn't understand about Europe. Socialism is about putting people first and making sure no one is left behind by society, which is the opposite of communism (and capitalism). In fact, US capitalism is much closer to communism regarding societal outcomes (social injustice, power concentration) than European socialism. It is very much possible to be anti-capitalist and anti-communist at the same time . reply davrosthedalek 5 hours agorootparent> Socialism is about putting people first and making sure no one is left behind by society, which is the opposite of communism (and capitalism). The problem with that sentence is that you can say the same sentence with socialism/communism/capitalism in any order and you would find people who would sign it. And to some degree, maybe all would be right. reply mytailorisrich 7 hours agorootparentprev> Socialism is about putting people first and making sure no one is left behind by society No, that's not what socialism is but I won't develop here because the definition is so available and well-known. reply lispm 6 hours agorootparentThe SPD in Germany claims the following: https://www.spd.de/programm/grundsatzprogramm --- Demokratischer Sozialismus Unsere Geschichte ist geprägt von der Idee des demokratischen Sozialismus, einer Gesellschaft der Freien und Gleichen, in der unsere Grundwerte verwirklicht sind. Sie verlangt eine Ordnung von Wirtschaft, Staat und Gesellschaft, in der die bürgerlichen, politischen, sozialen und wirtschaftlichen Grundrechte für alle Menschen garantiert sind, alle Menschen ein Leben ohne Ausbeutung, Unterdrückung und Gewalt, also in sozialer und menschlicher Sicherheit führen können. Das Ende des Staatssozialismus sowjetischer Prägung hat die Idee des demokratischen Sozialismus nicht widerlegt, sondern die Orientierung der Sozialdemokratie an Grundwerten eindrucksvoll bestätigt. Der demokratische Sozialismus bleibt für uns die Vision einer freien, gerechten und solidarischen Gesellschaft, deren Verwirklichung für uns einedauernde Aufgabe ist. Das Prinzip unseres Handelns ist die soziale Demokratie. --- reply CrLf 6 hours agorootparentprevYou may want to look into the ideologies of European political parties that have \"socialist\" in their names, instead of relying on definitions from the Soviet revolution. Socialism in Europe is social democracy. The only difference between \"socialist\" and \"social democratic\" parties in Europe is how fractionally close to the right or left side of the center line they are. reply dragonwriter 2 hours agorootparent> Socialism in Europe is social democracy. It’s a mix that includes social democracy and democratic socialism, as well as things to the right of the former (Britain’s Labour is still nominally socialist) and left of the latter. reply davrosthedalek 5 hours agorootparentprevFor the more central block parties, this is correct. But for many ultra-left and ultra-right parties, this is not necessarily true. There are true Marxist or Stalinist blocks in many of the ultra-left. There are straight-up fascists in the right wing \"national socialist\" parties. reply lispm 4 hours agorootparentultra-right parties may have \"socialist\" in their name, but they are typically not in a sense connected to Marx&Hegel. Example: the \"National-Sozialistische Deutsche Arbeiterpartei\" (Hitler's NSDAP) was not marxist. reply davrosthedalek 3 hours agorootparentYes, and I never said so. But the post I replied to implied that socialist=social democratic. And neither the marxist/communists in the far-left parties (and sometimes the whole party) nor the fascists in the far-right parties are social democratic. (Of course there is no such thing as a \"social democracy\", in the sense that the government structure is modified from a \"non-social\" democracy. But there can be democrats who push for a socially oriented governance. For example: Let's have affordable healthcare. Yes, that means that it cost more for rich people.. Democrats here means \"people who want to take part in a democracy\". not the US party. Not that they are liberal in the European sense either.) ) reply lispm 2 hours agorootparent> But the post I replied to implied that socialist=social democratic That's actually largely the case in Western/Middle Europe. > marxist/communists in the far-left parties (and sometimes the whole party) Which are often not seen as socialist. Social/socialist typically signals that the party is inside the system supporting political spectrum. \"communist/marxist\" usually signals that the party is at least partly outside the system supporting political spectrum. reply tcfhgj 4 hours agorootparentprevwhat is ultra-left? reply mytailorisrich 4 hours agorootparentprevThe definition of socialism does not change and has not changed, and it's not \"social democracy\". Many European political parties that have \"socialist\" in their names are historically socialist but have all but abandonned that ideology in favour of social democracy (i.e they have moved right) because, as we know, socialism was tried and it failed so there has been a lot of soul-searching on the left since the fall of the USSR and al. That does not mean that there aren't socialists anymore, including in major parties. reply lispm 6 hours agorootparentprevFor the US reader: > 39 MPs in the Bundestag Out of currently 733 MPs with a parliament with \"proportional representation\", where the number of seats is proportional to the number of votes (Germany-wide, not local). Die Linke thus has 5.3% seats in the Bundestag. Thus this is not \"a lot of votes\" in relation to the voting population. > \"anti-capitalist\" (which seems to be more popular than claiming to be socialist) Anti-capitalism is found in right-wing parties, too. Like the German AFD. reply davrosthedalek 5 hours agorootparentI don't think this gives an accurate picture. Even a 5% party can have an outsized influence on the politics. And it's not clear what the next election will bring. > Anti-capitalism is found in right-wing parties, too. Like the German AFD. Well, many/most of their proponents now seem to be fans of an older party which had national socialism in the name, so no surprise. In reality, the market rules and social net in most of Europe and US are not /that/ different. Both allow private ownership of production, both have market economy. Yes, the US says it's a free market, but it isn't. It's maybe free-er. Germany has a \"social market economy\", which mostly means that some (insurance) costs are lifted from the incur-er and distributed socially. Both have a social security equivalent, with Germany better coverage for unemployment, and US better retirement, AFAICT. reply lispm 4 hours agorootparent> Even a 5% party can have an outsized influence on the politics Die Linke does not have an \"outsized influence\". It's also shrinking. It may seem to have \"outsized influence\" for someone from the US or the UK, with their different voting system, which practically creates a two-party system. In a proportional representation system smaller parties have influence, too - for example by being a coalition member. Die Linke has not been a member of a coalition in Germany, so far, and it is not expected that this will change. > which mostly means that some (insurance) costs are lifted from the incur-er and distributed socially That's a very narrow view. Try to get a German-style workers council at an US company. Good luck! reply davrosthedalek 4 hours agorootparent> Die Linke does not have an \"outsized influence\". It's also shrinking. Yes, it's shrinking because Sarah exited left, came back in on the right, and now has her own party with blackjack and racists. > It may seem to have \"outsized influence\" for someone from the US or the UK, with their different voting system, which practically creates a two-party system. In a proportional representation system smaller parties have influence, too - for example by being a coalition member. Well, my point is that without explanation, 5% sounds like \"completely irrelevant\" for many people. So I guess we are in agreement. But even if you are familiar with the German system, do you not think that both FDP and Greens had an outsized influence in the just ending coalition compared to the SPD, in relation to their relative voting percentage? And historically, the FDP and CSU have a lot more influence that what would be proportional to their vote share compared to the bigger partners. I am not saying this is bad, I am saying that even a 5% party can have a relatively large impact on politics in the German system. Die Linke has not been in a coalition on the Bund level, but it certainly was so in the Laender. While Laender are a lot less powerful compared to US states, that's not nothing. >That's a very narrow view. Try to get a German-style workers council at an US company. Good luck! Implementation detail. An alternative are strong unions. Some US unions are stronger than German unions. Ask the teachers about the \"Dritte Weg\". My point is not: US and Germany are the same. My point is: It's a gradual difference. Not a complete systems change. reply lispm 3 hours agorootparent> And historically, the FDP and CSU have a lot more influence that what would be proportional to their vote share compared to the bigger partners. I don't think the influence is \"outsized\". Any party with 5% shares AND being in a coalition has much more influence than a party with 5% AND not in a coalition. A party with 4.9% may have very little influence, when not in a coalition and not even represented in the Bundestag. There are steps from very little influence to normal influence. The CSU never had that much special influence, since they were basically the CDU with a different name, but in Bavaria. It appeared larger because it was historically a different party, but basically only as an historic accident. The politics of CDU and CSU are largely the same. The CSU (only in Bavaria) getting more persons into the government may look like \"more influence\", but is largely the same policy as the CDU (in Germany minus Bavaria). The FDP has left the current coalition, exactly BECAUSE they thought their influence was too low and they had to agree to too many unwanted compromises. > My point is: It's a gradual difference. Not a complete systems change. The currently policy landscape looks very different to me. Ultra-rich billionaires ruling US politics. reply rsynnott 7 hours agorootparentprevThere is a significant middle-ground between Libertarian Utopia/Dystopia (delete as appropriate) and North Korea, you realise? reply andrepd 2 hours agorootparentprevAlmost nobody except fringe groups means \"Marxism-Leninism\" when they say \"socialism\". I'm sure you understand this. reply ahartmetz 8 hours agorootparentprevI'm not sure if the ones researching how to decolonize digital cat spaces are the super smart ones. reply imiric 7 hours agorootparentprev [–] > I also noted how far-left and anti-capitalistic many talks/speaker are. Why is this suprising? The hacker culture is rooted in anti-establishment philosophy, spun off from the hippy movement of the 1960s. This is not a regional thing. It's more than a bit ironic that this site is called \"hacker news\", yet is hosted by a company that holds opposite ideals. :) The term \"hacker\" has largely lost its original meaning, most notably from being vilified by mainstream media. Hack the planet! \\m/ reply BoingBoomTschak 4 hours agorootparent [–] The establishment has changed \"a bit\", though. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The 38th Chaos Communication Congress (38C3) is scheduled to take place in Hamburg from December 27-30, 2024, organized by the Chaos Computer Club.- This annual event emphasizes the intersection of technology, society, and utopia, offering lectures, workshops, and discussions on technology's impact.- Participants are invited to engage by volunteering, hosting events, or presenting projects, with key information accessible on the event's info pages and blog."
    ],
    "commentSummary": [
      "The 38th Chaos Communication Congress (CCC) is attracting attention with its diverse talks on AI, biology, and embedded systems, highlighting Joscha Bach's \"From Computation to Consciousness\" series.- The event is known for its non-mainstream discussions and will be livestreamed, addressing challenges like ticketing and the balance between technical and societal topics.- CCC is a key event for the hacker community, focusing on openness and inclusivity, and exploring political and cultural aspects, including language use."
    ],
    "points": 308,
    "commentCount": 163,
    "retryCount": 0,
    "time": 1735028149
  },
  {
    "id": 42498648,
    "title": "Build a Low-Cost Drone Using ESP32",
    "originLink": "https://www.digikey.com/en/maker/projects/a-step-by-step-guide-to-build-a-low-cost-drone-using-esp32/8afccd0690574bcebfa0d2ad6fd0a391",
    "originBody": "BlockedDigiKey(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl+ '&gtm_auth=zFdOlQnyIEZbUF1L2qMfoQ&gtm_preview=env-1&gtm_cookies_win=x';f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-53KHTQK');var dataLayer = dataLayer || [];dataLayer.push({'event': 'page_view','content_group1': 'Error','content_group2': '403', //error code'page_title': 'Error 403', //error code'page_type': 'ERR','page_sub_type': 'ERR', 'page_id': '403', //sitecore page id if available or error code if not'page_state': '403 Blocked' //A summary of the error reason (if available)});body{background-color:#f5f5f5}#errorHeader{width:100%;z-index:102;margin:0 auto;left:0;right:0;top:0;background:#222;height:100px;position:absolute}#errorHeader .logoContainer{text-align:center;max-width:1160px;padding:0 20px;margin:0 auto;position:relative;height:100px}#errorHeader .logo{border-width:0;position:absolute;left:0;right:0;top:26px;width:172px;height:48px;margin:0 auto}.circleContainer{height:300px;width:300px;text-align:center;position:absolute;top:155px;margin:50px auto;left:0;right:0}.circle{background-color:#fff;border-radius:50%;border:solid 8px #1A1552;width:calc(100% - 16px);height:calc(100% - 16px);box-shadow:4px 0 5px rgba(0,0,0,.3),0 4px 4px rgba(0,0,0,.2),0 -1px 2px rgba(255,255,255,.3),1px 2px 2px rgba(0,0,0,.2)}.errorNum{font-family:'Roboto Bold','Roboto Regular','Roboto',sans-serif;font-weight:700;font-style:normal;font-size:72px;color:#1A1552;position:relative;top:107px}.textAndButton{text-align:center;position:absolute;top:560px;margin:0 auto;left:0;right:0;font-family:'Roboto',sans-serif;font-style:normal;color:#222}.headline{font-weight:700;font-size:22px;padding:0 20px;}.content{font-weight:400;font-size:18px;margin:20px 0 0 0; padding:0 20px; }.content *{max-width:500px;margin-left:auto;margin-right:auto;}.reportBtnContainer a{color:#1A1552;box-shadow:inset 0 0 0 2px #ccc;min-width:120px!important;padding:12px 40px!important;border-radius:22px;appearance:none;-moz-appearance:none;-webkit-appearance:none;text-decoration:none;font-weight:700;box-sizing:border-box;background-color:#fff;display:inline-block;font-size:13px;line-height:1;padding:8px 24px;margin:30px 0 30px 0;min-width:101px;text-align:center;border-radius:100px;border:0 solid;transition:all .15s ease-out;cursor:pointer;outline:none}.reportBtnContainer a:hover{background-color:#f5f5f5!important}@media screen and (max-width:400px) {.circleContainer{height:200px;width:200px;margin:0 auto;}.textAndButton{top:400px;}.errorNum{top:55px;}}@media (prefers-color-scheme: dark) {body{background-color:#333}.textAndButton{color:#ccc;}.circle{background-color:#222;border-color:#ccc}.errorNum{color:#ccc;}}403Reference ID: 0.61a7cb17.1735066925.27069952Client IP: 52.190.187.37Report Error//add the page url to the feedback button if availableif (document.querySelectorAll(\".reportBtnContainer\").length > 0) {var pageUrl = document.location.href;var feedbackbutton = document.querySelector(\".reportBtnContainer a\");feedbackUrl = feedbackbutton.getAttribute(\"href\") + \"&url=\" + pageUrl;feedbackbutton.setAttribute('href', feedbackUrl);}",
    "commentLink": "https://news.ycombinator.com/item?id=42498648",
    "commentBody": "Build a Low-Cost Drone Using ESP32 (digikey.com)298 points by m3at 18 hours agohidepastfavorite97 comments the__alchemist 5 hours agoNote to anyone unafamiliar: There is a thriving \"FPV\" ecosystem of drones that can be DIYed. Example common setup, you can mix+match: - Small square PCB with the main flight control MCU (STM32), and some sensors - Smalls square PCB with motor drivers - Carbon fiber frame - Small PCB with a LoRa radio - Camera and video transmission system. (90s-security-cam style analog, or digital. - Brushless DC motors, props etc Uses Betaflight, ArduPilot, iNav, or PX4 firmware. Or, you could write your own. The PCB-frame in the article is neat and has obvious convenience advantages, but I speculate that it would not be stiff enough for desirable controllable characteristics under high accel situations. reply hylaride 3 hours agoparentAnd a lot of it is all open source! ESC software: - https://github.com/am32-firmware - https://github.com/mathiasvr/bluejay Flight controller (you mentioned these): - https://github.com/betaflight - https://github.com/ArduPilot - https://github.com/iNavFlight Control link: - https://github.com/ExpressLRS (also uses ESP32/ESP82 chips) Radio Controllers: - https://github.com/EdgeTX 5+ years ago the vast majority of this stuff was proprietary-only and getting into the hobby cost thousands of dollars. Now you could start at ~$500 (big price factor for FPV is the goggles, but cheap analog ones can be had for ~$100). reply nullstyle 3 hours agorootparentWith luck the OpenIPC cameras that are starting to come on the market will find traction and the entire hardware stack will run on open source firmware. https://www.youtube.com/watch?v=EdjI3ZQsmCA reply the__alchemist 2 hours agorootparentThis is so cool; LFTI! reply Kadin 3 hours agoparentprevThis is all true, but just to set expectations: the open source ecosystem seems to be lagging the proprietary world pretty significantly, unless there's some corner where development is really chugging along that's not making it out to the rest of the hobbyist market. Though there have been incremental improvements in flight control software, and video subsystems have moved (mostly) from analog to 2.4/5.8 GHz and digital, the overall architecture is pretty similar to what it was 5+ years ago. You have a hobby R/C transmitter and receiver driving PWM outputs (through the flight controller, typically an STM32) to hobby-type ESCs which control the motors. The ESCs are microcontroller-driven and can be reflashed, but painstakingly and annoyingly. Telemetry is typically separate from control, which is separate from video. Everything is very short-range and non-IP. In comparison, a COTS quadcopter from DJI has a single backhaul from the airframe to the controller which does control, video, and and telemetry. And the video is impressively low-latency. (I'm pretty sure they use a WiFi-type chipset and just spew raw vendor frames, and the receiver picks up what it can, best effort. You could do this with an ESP32 in ESP-NOW mode, I suspect?) I've seen some efforts to reverse-engineer the DJI protocol but I'm not aware of a fully compatible implementation or equivalent in the OSS world. And at the upper end of the commercial/proprietary space you have systems with out-of-the-box autonomy, multiple backhauls over IP -- so they can use LOS/BLOS radio, LTE, SATCOM, whatever you want -- integration with navigation beacon systems to reduce GPS dependence, hybrid motor/generators, redundant power systems, the whole shebang. There's no real reason aside from developer interest that this situation exists, as far as I can see. The components are mostly all available. A Raspberry Pi running a decent RTOS would have orders of magnitude more processing capacity than an STM32 and could easily do the sort of multi-sensor fusion that the commercial systems do. LTE modems are cheap. A bigger hexacopter or fixed-wing could easily loft one of the small Starlink dishes, if someone wanted to. Stuff like \"perching\" (landing and recharging from solar panels) is entirely possible. But from what I can tell, the cutting edge of open source drones is happening behind closed doors in Ukraine and Iran. Happy to be corrected if there's new stuff that I'm not tracking, but the gap between the \"art of the possible\" and current practice seems large. Lots of opportunity though, is the other way to view it. reply qazxcvbnmlp 2 hours agorootparentEverything here is possible, the gap in implementation is that it’s a) expensive and b) non trivial engineering work. There is vanishingly small overlap between the people whom have the capital for parts, the understanding of the engineering needed, free time to do it and desire to do it for free. The people whom have the true multidisciplinary understanding to do robotics well can usually also consult (with little difficulty finding work) for $$$s per hour and get the same “problem solving satisfaction”. Open source software shortcuts a couple of these limitations because you can work on it with little investment over than time. reply the__alchemist 2 hours agorootparentprevThat is consistent with my experience. My highlights: - Current betaflight devevelopers and leaders are incompetent (They inherited the code base from others), and are slow, and varying degrees of willing, to add higher-level flight-control mechanics (semi-autonomous modes etc). - The Ardupilot and PX4 configuration systems, and general experience, are user-hostile. Note that the converses of these hold: Betaflight has a reasonably-good user experience, and Ardupilot and PX4 have extensive higher-level flight control mechanics. If only you could get the pros of both together! Regarding hardware, frames generally accommodate flight controllers and ESCs elegantly, but other hardware like cameras, radios, and especially batteries, feel clumsy to assemble in a safe and consistent way. reply bodhi_mind 42 minutes agorootparentFwiw, betaflight for acro flight is probably the best. Tuning is dead simple. Filtering is possibly the best. That said, I don’t need much else because I just fly acro mode 100% of the time. reply dheera 54 minutes agoparentprevAlso > All-in-one PCB: Doesn’t need any 3D printed parts or such I actually am fine 3D printing and laser cutting stuff at home but I don't have the stuff to make a PCB and don't have the hand skills to do anything more than through-hole soldering. reply helpfulContrib 3 hours agoparentprev> There is a thriving \"FPV\" ecosystem of drones that can be DIYed. As someone who has for decades built flying things which could be drone'ified any day of the week, it is sort of also necessary to point out that even before drones became so widespread and commonplace, rcgroups.com has been the ecosystem in which to find oneself. And indeed, the \"model airplane/remote control flight\" subject has been prosperous and flourishing as a hobby for decades too .. just feast yourself on the categories here: https://www.rcgroups.com/forums/index.php A very earnest exploration of the various sub-forums will reveal some extraordinary designs - some which, indeed, break the 'norm' for what a flying thing should look like, in respect to a more casual view. Magnus, aerostat, Fettler are pretty good search terms... reply mrtksn 7 hours agoprevFun. I'm looking into turning my old iPhone into a drone as it has great hardware already to do higher level tasks and use ESP32 for the more real time stuff like actually driving the motors based on sensor input. If you think about it, an old iPhone 6 comes with GPS, gyro, accelerometer, multiple cameras, pretty powerful processors, bluetooth + wifi + LTE, sound + light, ambient + proximity sensors. Get rid of the case, and you have a great mini computer that can be aware of its surroundings and communicate. On more modern iPhones, you can even use advanced tech like ARKit to have great spatial understanding of your drone and environment and do autonomous drones. With an iPhone 15, you can even get spatial video. How amazing would that be? I wish Apple provided a straightforward way to unlock(like remove restrictions on the OS level) old phones and use them for DIY projects. reply Max-q 3 hours agoparentDo you really need the phone? The dual core 240MHz ESP32 seems to be able to do the job at lower weight and power consumption. Maybe the old phone is better used as a controller? reply mrtksn 2 hours agorootparentI got an ESP32 WROOM 32U board that I intent to use it as the flight controller which will keep the drone within its flight envelope and do the maneuvering upon receiving commands from the phone. I find the phone appealing because I want try to make it somewhat autonomous, like im RTS games where you give a unit a command to go somewhere and it figures the path out by itself and avoids obstacles without direct input. The phone has quite a lot of processing power and sensors and IMHO doing it on board will be more interesting. Also, I'm not sure that the connection will be always stable and high bitrate make the drone a thin client. reply szundi 6 hours agoparentprevNot a realtime os though reply mrtksn 5 hours agorootparentTrue but, IMHO, a lot of the higher level task should be able to handle it. I guess you can have a simple real time IC to handle the flight envelope to provide stable flight and then use the iPhone to do the advanced operations. For example, if you are building a drone that is mapping the environment and follows you through a bike ride, does it really matter if the 3D environment it creates for autonomous navigation is slightly off? You can continuously compensate for it, stick with moving averages and avoid extreme moves. reply sokoloff 3 hours agorootparentThat real-time controller still needs the IMU sensors, which is maybe what GP was responding to. (Your iPhone could have an additional set of them, but the ones on the phone don't do any good to the real-time subsystem.) reply mrtksn 2 hours agorootparentOh, I got an accelerometer card for that. It's very small and cheap, there are also cards that contain a gyro + barometer. So if the phone ones are not real-time enough cheap options exist. reply bschwindHN 5 hours agorootparentprevHence a microcontroller for flight control. The iPhone can just decide on the higher level tasks and send control signals to the microcontroller, similar to how a microcontroller delegates tasks to the underlying hardware peripherals. reply numpad0 3 hours agorootparentprevimo \"realtime\" is such a misnomer. It should be re-termed as \"jitter minimized\" or \"loop interval stabilized\" or something along that. reply ChuckMcM 11 hours agoprevFun! I built a Crazyflie[1] back in the day which was bespoke 2.4GHz protocols (no ESP32 at the time) so this is a great upgrade to that. Also the use of a single low side MOSFET as the motor controller makes it simpler and cheaper at the expense of some moves that BLDC motors give you. All in all, at $10 - $15 that is a great deal and I'm wondering if one will show up in a Hackerbox[2] as that is exactly the kind of thing they do. I have had a lot of fun playing with the CF microdrones, I'm definitely going to build one of these too. [1] https://github.com/bitcraze/crazyflie-firmware [2] https://hackerboxes.com/ reply phoronixrly 17 hours agoprevWhat a great time for this article! The US is having a mass hysteria event and it turns out you can churn out DYI drones for the fat sum of $12-13 each? What a time to be alive! Edit: Hmm, considering that people are taking stars for UFOs lately, maybe a cheap drone is an overkill and a 20-pack of Chinese sky lanterns would be more than enough to keep the average US neighbourhood in a state of constant fear / see how long it takes for you to get to the front page of /r/UFOs... reply abracadaniel 15 hours agoparentJudging from the posts I’ve seen, others have already started. Someone had a drone with a lit Roman candle going the other day. Or it was FAA compliant aliens. One of the two. reply ok654321 14 hours agorootparentnext [10 more] [flagged] igor47 14 hours agorootparentWelcome to hacker News! Yes, we find ignorance funny. I think people adding fuel to the fire are the ones freaking out about aliens or Iranian mother ships. Pointing out that the this is a hysteria event is more akin to pouring water on the flames. reply meaydinli 11 hours agorootparentNo, it is not \"Hacker News\" to paint a large swath of people as \"ignorant\". The proper way would have been to publish a detailed, technical analysis and present your ideas along with your proof to the greater community and facilitate a discussion. reply lioeters 8 hours agorootparentI want to live in this world, where we're all part of a larger collective endeavor to discover the truth, to be more informed, learn continuously, and evolve together. That's the old-school philosophy and mission of science for the betterment of humanity. reply stavros 9 hours agorootparentprevI like your optimism. Personally, I have found that, if someone freaks out about \"alien drones\", trying to facilitate a discussion is almost futile. reply pjc50 8 hours agorootparentprevLarge swathes of people are ignorant, though. It's just that now they can demonstrate that loudly on social media. reply omgwtfbyobbq 10 hours agorootparentprevI think it's kinda both. Besides, everyone's ignorant about something. https://xkcd.com/1053/ reply phoronixrly 9 hours agorootparentprevMy guy, there are government officials that are sharing photos of the Orion constellation stating that it's a drone swarm just standing there over their property... reply asadalt 14 hours agorootparentprevi see it as just accelerating normalizing cheap drones everywhere. reply croes 11 hours agorootparentprevhttps://x.com/ufoofinterest/status/1867668810171605394 https://x.com/DJSnM/status/1867763872465400226 https://x.com/TakeThatClouds/status/1867866332034932915 https://x.com/MatthewCappucci/status/1868052013164134899 It’s like throwing matches into a dumpster fire. reply diggan 5 hours agoparentprev> The US is having a mass hysteria event What is this in reference to? The Chinese weather-balloon drama was years ago, wasn't it? reply mathgeek 4 hours agorootparenthttps://apnews.com/article/drones-new-jersey-what-to-know-e6... reply diggan 4 hours agorootparentThanks! > Authorities say many of the drone sightings have actually been legal drones, manned aircraft, helicopters and even stars. > many state and municipal lawmakers have called for stricter rules about who can fly unmanned aircraft — and for the authority to shoot them down. Hard to find a more American article than this. reply ActorNightly 8 hours agoparentprevThe interesting/scary part is that its not that hard to weaponise these drones. You can make one drop a home made explosive pretty easily, fully autonomous. and then dump itself into a body of water. All for way less cost than a gun. reply idunnoman1222 4 hours agorootparentI don’t know what drone you’re talking about but you absolutely cannot make a tiny Wi-Fi drone autonomous or drop things for less than a cost of a gun, every part of your statement is wrong. Currently anyone smart enough to build drone with 7 inch propellers and programmable GPS waypoints, improvised explosives … 3d printed release mechanism .. probably has a pretty good paying job and doesn’t see the value proposition of blowing things up You know what you can easily do drive a truck into a crowd of people or fill your car with explosives in park it under a building and then blow up the entire building, but people aren’t doing that every day but continue whingeing about drones reply Max-q 3 hours agorootparentAutonomous isn’t a problem, the open source firmwares for drones include fully autonomous mode. I don’t know if this particular one does but it should be easy to port. The size and range, however, doesn’t make this particular drone very scary. reply numpad0 2 hours agorootparentprevI find it funny that the US is so obsessed with infantry firearms and surgical strikes that people think those are the best \"ways to go\" for certain things. I guess not a bad thing considering we don't need any improved means for those stuffs... reply JKCalhoun 5 hours agorootparentprevIt's not scary when it is just a hypothetical. If this were happening with some frequency it would be concerning. Until then I suspect there is something you are overlooking, oversimplifying. (EDIT: I'm assuming you are talking about non-military use.) reply bubaumba 6 hours agorootparentprev> home made explosive With this a lot of damage can be done even without drone. As for weaponized it's not a future, it's a reality in Ukraine for years now. Defense against them is difficult to impossible. A bodyguard who can sacrifice himself may sometimes work. reply anigbrowl 14 hours agoprevOr if you don't want to do it from scratch, you can get a programmable readymade for a little more: https://shop.m5stack.com/products/m5stamp-fly-with-m5stamps3... The included software stack is very basic, dig around on Japanese nerd Twitter for open source avionics. reply rahimnathwani 8 hours agoparentRight, but it's 3x the price and out of stock :( reply teruakohatu 17 hours agoprevThis is amazing. Even the landing gear (struts?) is part of the PCB. I hope the author considers selling kits or outsourcing kits to SeedStudio. I live in a country where digikey order shipping is quite pricey. The author estimates the BOM to be a little under US$13. At that price it would be fun to try create a swarm for DIY drone lights show. [1] https://circuitdigest.com/microcontroller-projects/DIY-wifi-... reply jdboyd 11 hours agoparentFWIW, making just 10 might drive that $13 price down quite a bit. Although, it looks like 1 unit might be closer to $50 (at least for the suppliers I might use), but $150 for 10. I think costs could be cut somewhat though. The USB->serial chip is nearly $6, but differently packaged it can be $4.40 for 1 or $3.99/ea for 10, and alternative chips that seem like they should be good enough can be cheaper still. The voltage regulator they chose is $1/ea for 500ma, while the one I would normally go to is $0.22/ea for 1000ma (dropping down to $0.13/ea for 10). reply numpad0 17 minutes agorootparentDoes it need a regulator at all? ESP32 should be able to run on raw battery voltage, assuming the ESP is necessary. RealTek TX2/RX2 + PIC10/ATTiny10/CH32V003 could be even cheaper if user is okay with a dedicated transmitter. reply pjc50 8 hours agorootparentprevIf you're making lots, you could uncouple the programming interface which isn't needed while in flight, thereby saving both BOM and weight. reply awestroke 10 hours agorootparentprevWhich is your go-to voltage regulator? reply frognumber 15 hours agoprevIt's odd, considering this is digikey, that there isn't a \"Buy now\" button. I'd totally do that if I got everything shipped to me, and knew I wasn't forgetting something. reply wat10000 15 hours agoparentThere’s a small link at the bottom to “Add all DigiKey Parts to Cart.” But one of the parts is apparently already obsolete and unavailable, and two more have minimum quantities above what’s needed here, so it’s not great. reply rkagerer 12 hours agorootparentBut one of the parts is apparently already obsolete and unavailable I love Digikey, but unfortunately this is not uncommon at all. reply CamperBob2 2 hours agorootparentTo answer the question asked by the [dead] commenter: because every other electronic component distributor site (with the exception of Mouser, which is basically DigiKey with a different stylesheet) is much worse. reply asadalt 16 hours agoprevthis is amazing. on similar note, I have spent last few months trying to fit visual inertial odometry into esp32. Combining that with this would be insane (and so cheap!) reply timschmidt 16 hours agoparentI've had similar thoughts and have been working on firmwares for the esp32. My contact info is in my profile. Hollar at me and lets compare notes. reply asadalt 16 hours agorootparenti don’t see your contact info. reply timschmidt 15 hours agorootparentMy mistake. Updated. reply no_time 3 hours agoprevI wonder what differs in the hardware (other than obviously using the newer esp32) compared to the implementation in this vid: https://www.youtube.com/watch?v=3n76iMTHXuE tldw: he experienced significant packet latency while the motors were spinning, making the drone uncontrollable. reply timonoko 13 hours agoprevI played with €25 foldable wifi drone from Lidl until EU started requiring €30 fee annual for a camera drone. I cannot think much practical use for drone without a camera. Fly-fishing might be one, but I need to program it so that it drops the line and returns home the moment it feels fish yanking. reply wyan 9 hours agoparentEU doesn't require a €30 annual fee for a camera drone, though. Your country might. Mine surely doesn't. reply VagabundoP 8 hours agoparentprevDrone reg here in Ireland (over 250g or with a camera its required) €38 for two years. https://www.iaa.ie/general-aviation/drones/drone-register reply eptcyka 11 hours agoparentprevHow much lift can a 30€ drone produce? A 600 gram trout could easily drag the drone underwater unless it is ridiculously overpowered. reply timonoko 9 hours agorootparent\"drops the line\" == does not even try, me myself reel the fish in. reply curiousgeorgio 12 hours agoprevIs this just someone reposting espressif's esp-drone (https://github.com/espressif/esp-drone) and passing it off as their own (and DigiKey posting it on their site)? They talk about making a custom PCB, but it looks pretty much the same. The repository linked from the article (https://github.com/Circuit-Digest/ESP-Drone) has some issues claiming there's malware in it, and the commit history looks a little suspicious, but I could be wrong. reply nick__m 2 hours agoparentSince those who filed the issue did not even stated which file is affected this is pure speculation but the virus issue really look like a false positive. The pre-built firmware checked into a repo could easily trigger an anti-virus. The repo is mostly made of plain text files, the zip and the bin don't look required for anything so if your feeling paranoid delete them before building! reply stavros 9 hours agoparentprevWow, yeah, I thought Espressif just wrote the firmware, but this drone is really really similar to the one posted, PCB struts and all. reply asadalt 12 hours agoparentprevdamn you are right! reply fitsumbelay 13 hours agoprevlove everything here but I'm skeptical of real time control via wifi. for me there's always been a noticeable delay in video streaming and receiving control signal so I'm curious how this works? reply ok654321 14 hours agoprevThe feature question is: can it transport an RPG head? reply Onawa 13 hours agoparent> Do you guys find this funny? Adding fuel to the fire? If you can't beat them, join them it seems? reply picture 13 hours agorootparentI just saw the two comments in question and find it to be absolutely hilarious. A serious answer to your question - no, these are in the \"whoop\" weight-class. Warhead carrying drones for anti-personnel and anti-armor are usually 10 inch and up, meaning they are using \"10 inch\" frames and likely 3115 motors, with 6S battery packs. Of course, there's endless variation, but that seems to be a optimum combination reply zeroCalories 13 hours agorootparentUkrainians and Russians use 7\" drones for dropping grenades and suicide charges. This does not seem too far off? reply stavros 9 hours agorootparentWhoops and 7\" drones are pretty far apart (depending on what you consider \"far\", I guess). Whoops can barely lift their own weight, 7\" drones can carry maybe a kilo. reply fullspectrumdev 36 minutes agorootparentA 7” drone can happily carry about 1.5kg which is more than sufficient to carry a RPG shell. reply lazide 6 hours agorootparentprevMost RPG warheads are pretty heavy. reply zeroCalories 13 hours agorootparentprevMaybe he was just looking for people that agree with him? reply stavros 9 hours agorootparentI find it hilarious to think that this was just an honest question and we downvoted him unfairly. reply AstroJetson 15 hours agoprevIs the custom circuit board something that mortals can get made? That seems to be the sticky point to me. reply wiml 14 hours agoparentYup. About fifteen years ago there was an explosion in small-run PCB services that cater to hobbyists (and cottage-scale professionals). If you don't need them overnight you can get good quality PCBs made at a very reasonable price. reply ElectRabbit 10 hours agoparentprev99% of hobbyist PCBs are can be ordered from JLCPCB nowadays. As long the PCBs have no components on them it's very cheap. reply stavros 9 hours agorootparentEven if they have components on them, that's only $1-$2 extra per board (depending on the components), JLCPCB will assemble all the common components (resistors, capacitors, some more exotic ones) for very cheap, and you can solder the rest yourself. Their PCB assembly saves so much time, I don't want to waste half an hour soldering 20 0805 resistors to save $0.30 any more. reply nick__m 2 hours agorootparentprevJLCPCB assembly services are so cheap as long as you don't use components marked as extended! I don't solder passive components anymore and now mostly use 0402 (that i am not able to solder reliably) instead of the bigger 0805. If you value your time and don't derive pleasure from soldering passive components, try the assembly services! reply asadalt 15 hours agoparentprevjust upload it to jlcpcb and get it delivered with components soldered. reply asadalt 14 hours agoprevi wish this didn’t use mpu6050 imu, which is obsolete and unavailable it seems. but i guess they used it due to existing code/drivers widely available for it and esp32. reply jauntywundrkind 12 hours agoprevFeels like a poor fit, given the limited number of cores available. Would be awesome to see rp2350 or some such, where there are very low power io cores available that can do work whether the main core is on or not. Embedded really is one of the best places for many-core, but it's so so rare there are good offload architectures and puny Programmable IO systems. Should out to folks like Silego/Dialog/Renesas with their GreenPAK; ultra tiny but interesting mixed signal little bits of programmable logic with a healthy dollop of peripherals! reply crote 3 hours agoparentCalling the RP2350's PIO units \"low power io cores\" is quite an exaggeration. Although they are technically turing-complete with a lot of hacking, they are absolutely awful at any kind of compute. Heck, you probably don't even want to let it handle UART parity calculation! If anything the ESP32's Ultra-Low-Power Coprocessor would be perfect for such applications - but realistically it isn't worth the effort. Compute power usage is going to be negligible compared to what is needed for wifi and rotors, and running multiple realtime tasks on a single core isn't exactly rocket science either. reply not_the_fda 6 hours agoparentprevYou don't need more cores. Ardupilot runs on much less capable hardware https://ardupilot.org/copter/docs/common-autopilots.html#com..., we've sent people to the moon with less capable hardware. More cores just make things more complicated. reply amelius 9 hours agoprevWith the power budget of a drone almost any board will do. reply joshu 16 hours agoprevnote: appears to use an outdated esp-idf. worth checking to see if esp-drone has been ported a newer esp-idf. reply iandanforth 17 hours agoprev [–] Meta: This has to be one of the most aggressively blocked pages I've encountered as it refuses to render any content if you have an adblocker on (uBlock at least) and resists several forms of archiving. reply RamblingCTO 9 hours agoparentI also had the worst anti robot thing ever. I had to hold a button for what feels like 10s. Who does that? reply macrocosmos 17 hours agoparentprevI'm able to access the site with uBlock Origin. reply prmoustache 8 hours agorootparentSame here on firefox/linux. Is that IP dependent? reply downrightmike 14 hours agorootparentprevSame here reply pjc50 8 hours agoparentprevOdd, it's just fine for me on firefox/ublock origin. reply 0xEF 6 hours agorootparentI'm also using firefox/ublock but I'm getting the mentioned agressively-blocked page. I'm also using a VPN, if that makes any difference. I also see the extremely light \"Press & Hold\" message to prove I am not a bot. I missed it the first few times I loaded the page because my eyes just don't pick up on things like that very easily (I'm the guy who turns the brightness up in video games with a lot of shadow). Not only does this press & hold action seem to be ineffective, but they made a message that some humans have trouble seem to keep bots out? Smdh. Which is disappointing, because I've ordered from digikey countless times, both for work and for personal projects. There should be absolutely no reason they clamp down on people using adblockers since they are highly profitable already. Guess I'll be looking for a different parts source. reply moepstar 12 hours agoparentprevuBlock on, pihole on the net - not sure what it dislikes more. I see a screen (light grey on white) which reads something along to \"Press & hold\" some button to confirm i'm human. Which does nothing, because of adblocking? Well, f* you then - let me find something different to read :) reply stavros 9 hours agorootparentThis will work (make sure you check the video at the bottom): https://circuitdigest.com/microcontroller-projects/DIY-wifi-... reply ElectRabbit 10 hours agoparentprev [–] Even with uBlock and Cookie AutoDelete disabled it doesn't work. OK. Then I'll order my stuff from Mouser in the future. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The DIY drone ecosystem is expanding, with affordable components such as flight control microcontroller units (MCUs), motor drivers, and cameras available for customization.",
      "Open-source software like Betaflight and ArduPilot supports these DIY builds, offering alternatives to proprietary systems like DJI, though they currently lag in some areas.",
      "The ESP32 microcontroller is a cost-effective option for drone projects, with entry-level setups costing around $500, despite challenges like real-time control via WiFi."
    ],
    "points": 298,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1734999639
  },
  {
    "id": 42498634,
    "title": "Making AMD GPUs competitive for LLM inference (2023)",
    "originLink": "https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference",
    "originBody": "Home Making AMD GPUs competitive for LLM inference Aug 9, 2023 • MLC Community TL;DR MLC-LLM makes it possible to compile LLMs and deploy them on AMD GPUs using ROCm with competitive performance. More specifically, AMD Radeon™ RX 7900 XTX gives 80% of the speed of NVIDIA® GeForce RTX™ 4090 and 94% of the speed of NVIDIA® GeForce RTX™ 3090Ti for Llama2-7B/13B. Besides ROCm, our Vulkan support allows us to generalize LLM deployment to other AMD devices, for example, a SteamDeck with an AMD APU. Background There have been many LLM inference solutions since the bloom of open-source LLMs. Most of the performant inference solutions are based on CUDA and optimized for NVIDIA GPUs. In the meantime, with the high demand for compute availability, it is useful to bring support to a broader class of hardware accelerators. AMD is one potential candidate. Discussion on the Hardware and Software From the spec comparison, we can see that AMD’s RX 7900 XTX is a good match for NVIDIA’s RTX 4090 and RTX 3090 Ti. All have 24GB memory, which means they can fit models of the same size. All have similar memory bandwidth. 4090 has 2x more FP16 performance than 7900 XTX, while 3090 Ti has 1.3x more FP16 performance than 7900 XTX. Lantency sensitive LLM inference is mostly memory bound, so the FP16 performance is not a bottleneck here. RX 7900 XTX is 40% cheaper than RTX 4090. It is harder to compare the price of 3090Ti as that was a previous generation. We put it here as a reference point to provide more information. At a high-level, we can find that AMD 7900 XTX is comparable to RTX 3090 Ti from the hardware spec perspective. Hardware is not necessarily the reason why AMD lagged in the past. The main gaps were due to a lack of software support and optimizations for the relevant models. There are two factors in the ecosystem that starts to bring changes to the picture: AMD is trying to catch up with investments in the ROCm stack. Emerging technologies like machine learning compilation helps to reduce overall cost of more universal software support across backends. In this post, we are taking a deep look at how well AMD GPUs can do compared to a performant CUDA solution on NVIDIA GPUs as of now. Machine Learning Compilation for ROCm What is machine learning compilation (MLC). Machine learning compilation is an emerging technology that compiles and automates the optimization of machine learning workloads. Instead of crafting specific kernels for each individual backend like ROCm or CUDA, an MLC solution automatically generate code for different backends. Here we leverage MLC-LLM, an ML compilation-based solution that offers high-performance universal deployment for LLMs. MLC-LLM builds on top of Apache TVM Unity, a machine-learning compilation stack that offers productive Python-first development and universal deployment. MLC-LLM brings state-of-the-art performance for a wide variety of backends, including CUDA, Metal, ROCm, Vulkan, and OpenCL, spanning both server-class GPUs to mobile (iPhone and Android). At a high level, the framework lets the user take open language models and compiles it with Python-based workflow, including APIs to transform computational graphs, optimize the layout and scheduling of GPU kernels, and deploys it natively on platforms of interest. MLC for AMD GPUs and APUs. There are several possible ways to support AMD GPU: ROCm, OpenCL, Vulkan, and WebGPU. ROCm stack is what AMD recently push for and has a lot of the corresponding building blocks similar to the CUDA stack. Vulkan is the latest graphics standard and offers the widest range of support across GPU devices. WebGPU is the latest web standard that allows the computation to run on web browsers. While there are so many possible ways, few ML software solutions that build for solutions other than CUDA, largely due to the engineering cost to replicate a stack for a new hardware or GPU programming model. We support automatic code generation without having to recraft GPU kernels for each and bring support to all these ways. This being said, the performance still depends on how good the low-level GPU runtimes are and their availability in each platform. We pick ROCm for Radeon 7900 XTX and Vulkan for Steamdeck’s APU. We find that ROCm stack works out of the box. Thanks to the productive Python-based development pipeline in TVM unity, we spent a few more hours to further bring an optimized version. We made the following things to bring ROCm support: Reuse the whole MLC pipeline for existing targets (such as CUDA and Metal), including memory planning, operator fusion, etc. Reuse a generic GPU kernel optimization space written in TVM TensorIR and re-target it to AMD GPUs. Reuse TVM’s ROCm code generation flow that generates low-level ROCm kernels through LLVM. Finally, export generated code as a shared or static library that can be invoked by CLI, Python and REST APIs. Benchmark with MLC Python Package We benchmarked the Llama 2 7B and 13B with 4-bit quantization. And we measure the decoding performance by setting a single prompt token and generating 512 tokens. All the results are measured for single batch inference. For single batch inference performance, it can reach 80% of the speed of NVIDIA 4090 with the release of ROCm 5.6. Note on the comparison: How strong is our CUDA baseline? It is the state-of-the-art for this task to the best of our knowledge. We believe there is still room for improvements, e.g. through better attention optimizations. As soon as those optimizations land in MLC, we anticipate both AMD and NVIDIA numbers improved. If such optimizations are only implemented on NVIDIA side, it brings the gap up from 20% to 30%. And therefore, we recommend putting 10% error bar when looking at the numbers here Try it out yourself We provide prebuilt wheels and instructions to reproduce our results on your own devices. To run those benchmarks, please ensure that you have an AMD GPU with ROCm 5.6 or above running in Linux. Follow the instructions here to install a prebuilt MLC package with ROCm enabled: Run the Python script below that uses our MLC package to reproduce performance numbers: from mlc_chat import ChatModule # Create a ChatModule instance that loads from `./dist/prebuilt/Llama-2-7b-chat-hf-q4f16_1` cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\") # Run the benchmarks output = cm.benchmark_generate(\"Hi\", generate_length=512) print(f\"Generated text:{output}\") print(f\"Statistics: {cm.stats()}\") # Reset the chat module by # cm.reset_chat() MLC-LLM also provides a CLI that allows you to chat with the model interactively. For ROCm it requires to build the CLI from source. Please follow the instructions here to build the CLI from source. Running on SteamDeck using Vulkan with Unified Memory Let us also look into a broader set of AMD devices, more specifically, SteamDeck equipped with an AMD APU. While the GPU VRAM available in ROCm is capped to 4GB in BIOS, the Mesa Vulkan driver has robust support that allows the buffer to go beyond the cap using unified memory up to 16GB, which is sufficient to run 4bit-quantized Llama-7B. These results shed some light on how a broad spectrum of AMD devices can be supported for more diverse set of of consumers. Discussion and Future Work Hardware availability has become a pressing issue in the age of generative AI. ML compilation can help by bringing high-performance universal deployment across hardware backends. Given the presented evidences, with the right price and availability, we think AMD GPUs can start to be useful for LLM inference. Our study focuses on consumer-grade GPUs as of now. Based on our past experiences, MLC optimizations for consumer GPU models usually are generalizable to cloud GPUs (e.g. from RTX 4090 to A100 and A10g). We are confident that the solution generalizes across cloud and consumer-class AMD and NVIDIA GPUs, and will also update our study once we have access to more GPUs. We also encourage the community to build solutions on top of the MLC universal deployment flow. This post is part of the ongoing effort that brings high-performance universal deployment via MLC. We are also actively working on several areas that can generalize our study. Enable batching and multi-GPU support; Integration with PyTorch ecosystem; Empowering more quantization and model architectures; Bringing in more automatic optimizations on more hardware backends. Our final takeaway is that machine learning system engineering is a continuous problem. NVIDIA is still leading the field with continuous innovations, and we anticipate the landscape to change with new hardware such as H100 and, more importantly, software evolutions. So the key question is not only about building the right solution now but also how to catch up and bring ML engineering to new platforms continuously. Productivity in machine learning engineering is the key here. Thanks to the Python-first ML compilation development flow, we get ROCm-optimized support in a few hours. We anticipate related approaches to become even more useful as we explore more ideas to bring universal deployments and solve the hardware availability problem. Links Please refer to our project page for a detailed guide on how to try out the MLC LLM deployment. The source code of MLC LLM is available on our official GitHub repository. You are also more than welcome to join the Discord Channel for further discussion. Acknowledgement The overall MLC projects are only possible thanks to the shoulders of open-source ecosystems that we stand on. We would love to continue developing and supporting the open-source ML community. We want to thank the Apache TVM community and developers of the TVM Unity compiler. The open-source ML community members made these models publicly available. PyTorch and Hugging Face communities make these models accessible. We would like to thank the teams behind RedPajama, Dolly, Vicuna, SentencePiece, LLaMA, and Alpaca. We also would like to thank OpenCL, Vulkan, C++, Python, and Rust communities that enable this project.",
    "commentLink": "https://news.ycombinator.com/item?id=42498634",
    "commentBody": "Making AMD GPUs competitive for LLM inference (2023) (mlc.ai)241 points by plasticchris 18 hours agohidepastfavorite145 comments pavelstoev 14 hours agoThe problem is that performance achievements on AMD consumer-grade GPUs (RX7900XTX) are not representative/transferrable to the Datacenter grade GPUs (MI300X). Consumer GPUs are based on RDNA architecture, while datacenter GPUs are based on the CDNA architecture, and only sometime in ~2026 AMD is expected to release unifying UDNA architecture [1]. At CentML we are currently working on integrating AMD CDNA and HIP support into our Hidet deep learning compiler [2], which will also power inference workloads for all Nvidia GPUs, AMD GPUs, Google TPU and AWS Inf2 chips on our platform [3] [1] https://www.jonpeddie.com/news/amd-to-integrate-cdna-and-rdn.... [2] https://centml.ai/hidet/ [3] https://centml.ai/platform/ reply llm_trw 11 hours agoparentThe problem is that the specs of AMD consumer-grade GPUs do not translate to computer performance when you try and chain more than one together. I have 7 NVidia 4090s under my desk happily chugging along on week long training runs. I once managed to get a Radeon VII to run for six hours without shitting itself. reply mpreda 5 hours agorootparent> I have 7 NVidia 4090s under my desk I have 6 Radeon Pro VII under my desk (in a single system BTW), and they run hard for weeks until I choose to reboot e.g. for Linux kernel updates. I bought them \"new old stock\" for $300 apiece. So that's $1800 for all six. reply highwaylights 4 hours agorootparentHow does the compute performance compare to 4090’s for these workloads? (I release it will be significantly lower, just try to get as much of a comparison as is possible). reply crest 2 hours agorootparentThe Radeon VII is special compared to most older (and current) affordable GPUs in that it used HBM giving it memory bandwidth comparable to modern cards ~1TB/s and has reasonable FP64 (1:4) throughput instead of (1:64). So this card can still be pretty interesting for running memory bandwidth intensive FP64 workloads. Anything affordable afterward by either AMD or Nvidia crippled realistic FP64 throughput to below what a AVX-512 many-core CPU can do. reply nine_k 52 minutes agorootparentIf we speak about FP64, are your loads more like fluid dynamics than ML training? reply cainxinth 4 hours agorootparentprevThe 4090 offers 82.58 teraflops of single-precision performance compared to the Radeon Pro VII's 13.06 teraflops. reply adrian_b 2 hours agorootparentOn the other hand, for double precision a Radeon Pro VII is many times faster than a RTX 4090 (due to 1:2 vs. 1:64 FP64:FP32 ratio). Moreover, for workloads limited by the memory bandwidth, a Radeon Pro VII and a RTX 4090 will have about the same speed, regardless what kind of computations are performed. It is said that speed limitation by memory bandwidth happens frequently for ML/AI inferencing. reply tspng 10 hours agorootparentprevWow, are these 7 RTX 4090s in a single setup? Care to share more how you build it (case, cooling, power, ..)? reply ghxst 7 hours agorootparentYou might find the journey of Tinycorp's Tinybox interesting, it's a machine with 6 to 8 4090 GPUs and you should be able to track down a lot of their hardware choices including pictures on their Twitter and other info on George his livestreams. reply llm_trw 6 hours agorootparentprevBasically this but with an extra card on the x8 slot for connecting my monitors: https://www.youtube.com/watch?v=C548PLVwjHA There's a bunch of similar setups and there are a couple of dozen people that have done something similar on /r/localllama. reply osmarks 5 hours agorootparentprevMost of these are just an EPYC server platform, some cursed risers and multiple PSUs (though cryptominer server PSU adapters are probably better). See https://nonint.com/2022/05/30/my-deep-learning-rig/ and https://www.mov-axbx.com/wopr/wopr_concept.html. reply Keyframe 5 hours agorootparentLooks like a fire hazard :) reply adakbar 8 hours agorootparentprevI'd like to know too reply zozbot234 9 hours agoparentprevIt looks like AMD's CDNA gpu's are supported by Mesa, which ought to suffice for Vulkan Compute and SYCL support. So there should be ways to run ML workloads on the hardware without going through HIP/ROCm. reply shihab 17 hours agoprevI have come across quite few startups who are trying a similar idea: break the nvidia monopoly by utilizing AMD GPUs (for inference at least): Felafax, Lamini, tensorwave (partially), SlashML. Even saw optimistic claims like CUDA moat is only 18 months deep from some of them [1]. Let's see. [1] https://www.linkedin.com/feed/update/urn:li:activity:7275885... reply pinsiang 14 hours agoparentAMD GPUs are becoming a serious contender for LLM inference. vLLM is already showing impressive performance on AMD [1], even with consumer-grade Radeon cards (even support GGUF) [2]. This could be a game-changer for folks who want to run LLMs without shelling out for expensive NVIDIA hardware. [1] https://blog.vllm.ai/2024/10/23/vllm-serving-amd.html [2] https://embeddedllm.com/blog/vllm-now-supports-running-gguf-... reply MrBuddyCasino 4 hours agorootparentFun fact: Nvidia H200 are currently half the price/hr of H100 bc people can’t get vLLM to work on it. https://x.com/nisten/status/1871325538335486049 reply adrian_b 2 hours agorootparentThat seems like a CPU problem, not a GPU problem (due to Aarch64 replacing x86-64). reply ryukoposting 17 hours agoparentprevPeculiar business model, at a glance. It seems like they're doing work that AMD ought to be doing, and is probably doing behind the scenes. Who is the customer for a third-party GPU driver shim? reply dpkirchner 17 hours agorootparentCould be trying to make themselves a target for a big acquihire. reply to11mtm 15 hours agorootparentCynical take: Try to get acquired by Intel for Arc. reply dogma1138 13 hours agorootparentIntel is in a vastly better shape than AMD, they have the software pretty much nailed down. reply lhl 11 hours agorootparentI've recently been poking around with Intel oneAPI and IPEX-LLM. While there are things that I find refreshing (like their ability to actually respond to bug reports in a timely manner, or at all) on a whole, support/maturity actually doesn't match the current state of ROCm. PyTorch requires it's own support kit separate from the oneAPI Toolkit (and runs slightly different versions of everything), the vLLM xpu support doesn't work - both source and the docker failed to build/run for me. The IPEX-LLM whisper support is completely borked, etc, etc. reply moffkalast 6 hours agorootparentI've recently been trying to get IPEX working as well, apparently picking Ubuntu 24.04 was a mistake, because while things compile, everything fails at runtime. I've tried native, docker, different oneAPI versions, threw away a solid week of afternoons for nothing. SYCL with llama.cpp is great though, at least at FP16 since it supports nothing else but even Arc iGPUs easily give 2-4x performance compared to CPU inference. Intel should've just contributed to SYCL instead of trying to make their own thing and then forgot to keep maintaining it halfway through. reply lhl 5 hours agorootparentMy testing has been w/ a Lunar Lake Core 258V chip (Xe2 - Arc 140V) on Arch Linux. It sounds like you've tried a lot of things already, but case it helps, my notes for installing llama.cpp and PyTorch: https://llm-tracker.info/howto/Intel-GPUs I have some benchmarks as well, and the IPEX-LLM backend performed a fair bit better than the SYCL llama.cpp backend for me (almost +50% pp512 and almost 2X tg128) so worth getting it working if you plan on using llama.cpp much on an Intel system. SYCL still performs significantly better than Vulkan and CPU backends, though. As an end-user, I agree that it'd be way better if they could just contribute upstream somehow (whether to the SYCL backend, or if not possible, to a dependency-minized IPEX backend). the IPEX backend is one of the more maintained parts of IPEX-LLM, btw. I found a lot of stuff in that repo that depend on versions of oneKit that aren't even downloadable on Intel's site. I couldn't help but smirk when I heard someone say \"Intel has their software nailed down.\" reply moffkalast 4 hours agorootparentWell that's funny, I think we already spoke on Reddit. I'm the guy who was testing the 125H recently. I guess there's like 5 of us who have intel hardware in total and we keep running into each other :P Honestly I think there's just something seriously broken with the way IPEX expects the GPU driver to be on 24.04 and there's nothing I can really do about it except wait for them to fix it if I want to keep using this OS. I am vaguely considering adding another drive and installing 22.04 or 20.04 with the exact kernel they want to see if that might finally work in the meantime, but honestly I'm fairly satisfied with the speed I get from SYCL already. The problem is more that it's annoying to integrate it directly through the server endpoint, every projects expects a damn ollama api or llama-cpp-python these days and I'm a fan of neither since it's just another layer of headaches to get those compiled with SYCL. > I found a lot of stuff in that repo that depend on versions of oneKit that aren't even downloadable on Intel's site. I couldn't help but smirk when I heard someone say \"Intel has their software nailed down.\" Yeah well the fact that oneAPI 2025 got released, broke IPEX, and they still haven't figured out a way to patch it for months makes me think it's total chaos internally, where teams work against each other instead of talking and coordinating. reply indolering 11 hours agorootparentprevTell that to the board. reply bboygravity 11 hours agorootparentprevSomeone never used intel killer wifi software. reply dangero 13 hours agorootparentprevMore cynical take: Trying to get acquired by nvidia reply dizhn 11 hours agorootparentPerson below says they (the whole team) already joined Nvidia. reply shiroiushi 15 hours agorootparentprevMore cynical take: this would be a bad strategy, because Intel hasn't shown much competence in its leadership for a long time, especially in regards to GPUs. reply rockskon 14 hours agorootparentThey've actually been making positive moves with GPUs lately along with a success story for the B580. reply kimixa 10 hours agorootparentB580 being a \"success\" is purely a business decision as a loss leader to get their name into the market. A larger die on a newer node than either Nvidia or AMD means their per-unit costs are higher, and are selling it at a lower price. That's not a long-term success strategy. Maybe good for getting your name in the conversation, but not sustainable. reply bitmasher9 3 hours agorootparentIt’s a long term strategy to release a hardware platform with minimal margins in the beginning to attract software support needed for long term viability. One of the benefits of being Intel. reply 7speter 29 minutes agorootparentprevI don’t know if this matters but while the B580 has a die comparable in size to a 4070 (~280mm^2), it has about half the transistors (~17-18 billion), iirc. reply jvanderbot 6 hours agorootparentprevI was reading this whole thread as about technical accomplishment and non-nvidia GPU capabilities, not business. So I think you're talking about different definitions of \"Success\". Definitely counts, but not what I was reading. reply schmidtleonard 13 hours agorootparentprevYeah but MLID says they are losing money on every one and have been winding down the internal development resources. That doesn't bode well for the future. I want to believe he's wrong, but on the parts of his show where I am in a position to verify, he generally checks out. Whatever the opposite of Gell-Mann Amnesia is, he's got it going for him. reply sodality2 13 hours agorootparentMLID on Intel is starting to become the same as UserBenchmark on AMD (except for the generally reputable sources)... he's beginning to sound like he simply wants Intel to fail, to my insider-info-lacking ears. For competition's sake I really hope that MLID has it wrong (at least the opining about the imminent failure of Intel's GPU division), and that the B series will encourage Intel to push farther to spark more competition in the GPU space. reply oofabz 12 hours agorootparentprevThe die size of the B580 is 272 mm2, which is a lot of silicon for $249. The performance of the GPU is good for its price but bad for its die size. Manufacturing cost is closely tied to die size. 272 mm2 puts the B580 in the same league as the Radeon 7700XT, a $449 card, and the GeForce 4070 Super, which is $599. The idea that Intel is selling these cards at a loss sounds reasonable to me. reply KeplerBoy 5 hours agorootparentAt a loss seems a bit overly dramatic. I'd guess Nvidia sells SKUs for three times their marginal cost. Intel is probably operating at cost without any hopes of recouping R&D with the current SKUs, but that's reasonable for an aspiring competitor. reply 7speter 26 minutes agorootparentIt kinda seems they are covering the cost of throwing massive amounts of resources trying to get Arc’s drivers in shape. reply tjoff 9 hours agorootparentprevThough you assume the prices of the competition are reasonable. There are plenty of reasons for them not to be. Availability issues, lack of competition, other more lucrative avenues etc. Intel has neither, or at least not as much of them. reply derektank 13 hours agorootparentprevWait, are they losing money on every one in the sense that they haven't broken even on research and development yet? Or in the sense that they cost more to manufacture than they're sold at? Because one is much worse than the other. reply rockskon 11 hours agorootparentThey're trying to unseat Radeon as the budget card. That means making a more enticing offer than AMD for a temporary period of time. reply dboreham 15 hours agorootparentprev> Could be trying to make themselves a target for a big acquihire. Is this something anyone sets out to do? reply ryukoposting 15 hours agorootparentIt definitely is, yes. reply seeknotfind 15 hours agorootparentprevYes. reply tesch1 17 hours agorootparentprevAMD. Just one more dot to connect ;) reply dylan604 16 hours agorootparentprevIt would be interesting to find out AMD is funding these other companies to ensure the shim happens while they focus on not doing it. reply bushbaba 15 hours agorootparentAMD is kind of doing that funding by pricing its GPUs low and/or giving them away at cost to these startups reply shmerl 16 hours agorootparentprevIs this effort benefiting everyone? I.e. where is it going / is it open source? reply britannio 7 hours agorootparentSome of the work from Tinycorp is: https://github.com/tinygrad/7900xtx reply llama-mini 13 hours agoparentprevFrom Lamini, we have a private AMD GPU cluster, ready to serve any one who want to try MI300x or MI250 with inference and tuning. We just onboarded a customer to move from openai API to on-prem solution, currently evaluating MI300x for inference. Email me at my profile email. reply 3abiton 5 hours agoparentprevMy understanding is that once JAX takes off, the cuda advantage is gone for nvidia. That's a big if/when though. reply jsheard 17 hours agoparentprevTinygrad was another one, but they ended up getting frustrated with AMD and semi-pivoted to Nvidia. reply nomel 15 hours agorootparentThis is discussed in the lex Friedman episode. AMD’s own demo would kernel panic when run in a loop [1]. [1] https://youtube.com/watch?v=dNrTrx42DGQ&t=3218 reply kranke155 9 hours agorootparentInteresting. I wonder if focusing on GPUs and CPUs is something that requires two companies instead of one, whether the concentration of resources just leads to one arm of your company being much better than the other. reply noch 13 hours agorootparentprev> Tinygrad was another one, but they ended up getting frustrated with AMD and semi-pivoted to Nvidia. From their announcement on 20241219[^0]: \"We are the only company to get AMD on MLPerf, and we have a completely custom driver that's 50x simpler than the stock one. A bit shocked by how little AMD cared, but we'll take the trillions instead of them.\" From 20241211[^1]: \"We gave up and soon tinygrad will depend on 0 AMD code except what's required by code signing. We did this for the 7900XTX (tinybox red). If AMD was thinking strategically, they'd be begging us to take some free MI300s to add support for it.\" --- [^0]: https://x.com/__tinygrad__/status/1869620002015572023 [^1]: https://x.com/__tinygrad__/status/1866889544299319606 reply jroesch 17 hours agoprevNote: this is old work, and much of the team working on TVM, and MLC were from OctoAI and we have all recently joined NVIDIA. reply sebmellen 17 hours agoparentIs there no hope for AMD anymore? After George Hotz/Tinygrad gave up on AMD I feel there’s no realistic chance of using their chips to break the CUDA dominance. reply comex 15 hours agorootparentMaybe from Modular (the company Chris Lattner is working for). In this recent announcement they said they had achieved competitive ML performance… on NVIDIA GPUs, but with their own custom stack completely replacing CUDA. And they’re targeting AMD next. https://www.modular.com/blog/introducing-max-24-6-a-gpu-nati... reply behnamoh 15 hours agorootparentAh yes, the programming language (Mojo) that requires an account before I can use it... reply melodyogonna 12 hours agorootparentMojo no longer requires an account to install. But that is irrelevant to the conversation because this is not about Mojo but something they call MAX. [1] 1. https://www.modular.com/max reply steeve 1 hour agorootparentprevWe (ZML) have AMD MI300X working just fine, in fact, faster than H100 reply latchkey 16 hours agorootparentprevhttps://x.com/dylan522p/status/1871287937268383867 reply krackers 16 hours agorootparentThat's almost word for word what geohotz said last year? reply refulgentis 16 hours agorootparentWhat part? I assume the part where she said there's \"gaps in the software stack\", because that's the only part that's attributed to her. But I must be wrong because that hasn't been in dispute or in the news in a decade, it's not a geohot discovery from last year. Hell I remember a subargument of a subargument re: this being an issue a decade ago in macOS dev (TL;Dr whether to invest in opencl) reply bn-l 16 hours agorootparentprevI went through the thread. There’s an argument to be made in firing Su for being so spaced out as to miss an op for their own CUDA for free. reply hedgehog 15 hours agorootparentNot remotely, how did you get to that idea? reply refulgentis 13 hours agorootparentKids this days (shakes fist) tl;dr there's a non-unsubstantial # of people who learn a lot from geohot. I'd say about 3% of people here will be confused if you thought of him as less than a top technical expert across many comp sci fields. And he did the geohot thing recently, way tl;dr: acted like there was a scandal being covered up by AMD around drivers that was causing them to \"lose\" to nVidia. He then framed AMD not engaging with him on this topic as further covering-up and choosing to lose. So if you're of a certain set of experiences, you see an anodyne quote from the CEO that would have been utterly unsurprising dating back to when ATI was still a company, and you'd read it as the CEO breezily admitting in public that geohot was right about how there was malfeasance, followed by a cover up, implying extreme dereliction of duty, because she either helped or didn't realize till now. I'd argue this is partially due to stonk-ification of discussions, there was a vague, yet often communicated, sense there was something illegal happening. Idea was it was financial dereliction of duty to shareholders. reply dismalaf 16 hours agorootparentprevIMO the hope shouldn't be that AMD specifically wins, rather it's best for consumers that hardware becomes commoditized and prices come down. And that's what's happening, slowly anyway. Google, Apple and Amazon all have their own AI chips, Intel has Gaudi, AMD had their thing, and the software is at least working on more than just Nvidia. Which is a win. Even if it's not perfect. I'm personally hoping that everyone piles in on a standard like SYCL. reply quotemstr 15 hours agorootparentprevThe world is bigger than AMD and Nvidia. Plenty of interesting new AI-tuned non-GPU accelerators coming online. reply grigio 10 hours agorootparentI hope, name some NPU who can run a 70B model.. reply llm_trw 17 hours agorootparentprevNot really. AMD is constitutionally incapable of shipping anything but mid range hardware that requires no innovation. The only reason why they are doing so well in CPUs right now is that Intel has basically destroyed itself without any outside help. reply adrian_b 39 minutes agorootparentIn CPUs, AMD has made many innovations that have been copied by Intel only after many years and this delay had an important contribution to Intel's downfall. The most important has been the fact that AMD has predicted correctly that big monolithic CPUs will no longer be feasible in the future CMOS fabrication technologies, so they have designed the Zen family since the beginning with a chiplet-based architecture. Intel had attempted to ridicule them, but after losing many billions they have been forced to copy this strategy. Also in the microarchitecture of their CPUs AMD has made the right choices since the beginning and then they have improved it constantly with each generation. The result is that now the latest Intel big core, Lion Cove, has a microarchitecture that is much more similar to AMD Zen 5 than to any of the previous Intel cores, because they had to do this to get a competitive core. In the distant past, AMD has also introduced a lot of innovations long before they were copied by Intel, but it is true that those had not been invented by AMD, but they had been copied by AMD from more expensive CPUs, like DEC Alpha or Cray or IBM POWER, but Intel has also copied them only after being forced by the competition with AMD. reply ksec 15 hours agorootparentprevEverything is comparative. AMD isn't perfect. As an Ex Shareholder I have argued they did well partly because of Intel's downfall. In terms of execution it is far from perfect. But Nvidia is a different beast. It is a bit like Apple in the late 00s where you take business, forecast, marketing, operation, software, hardware, sales etc You take any part of it and they are all industry leading. And having industry leading capability is only part of the game, having it all work together is completely another thing. And unlike Apple where they lost direction once Steve Jobs passed away and weren't sure about how to deploy capital. Jensen is still here, and they have more resources now making Nvidia even more competitive. It is often most people underestimate the magnitude of the task required, ( I like to tell the story again about an Intel GPU engineer in 2016 arguing they could take dGPU market shares by 2020, and we are now 2025 ), over estimate the capability of an organisation, under estimate the rival's speed of innovation and execution. These three thing combined is why most people are often off the estimate by an order of magnitude. reply llm_trw 15 hours agorootparentYeah, no. We are in the middle of a monopoly squeeze by NVidia on the most innovative part of the economy right now. I expect the DOJ to hit them harder than they did MS in the 90s given the bullshit they are pulling and the drag on the economy they are causing. By comparison if AMD could write a driver that didn't shit itself when it had to multiply more than two matrices in a row they'd be selling cards faster than they can make them. You don't need to sell the best shovels in a gold rush to make mountains of money, but you can't sell teaspoons as premium shovels and expect people to come back. reply ksec 13 hours agorootparent>We are in the middle of a monopoly squeeze by NVidia on the most innovative part of the economy right now. I am not sure which part of Nvidia is monopoly. That is like suggesting TSMC has a monopoly. reply vitus 5 hours agorootparent> That is like suggesting TSMC has a monopoly. They... do have a monopoly on foundry capacity, especially if you're looking at the most advanced nodes? Nobody's going to Intel or Samsung to build 3nm processors. Hell, there have been whispers over the past month that even Samsung might start outsourcing Exynos to TSMC; Intel already did that with Lunar Lake. Having a monopoly doesn't mean that you are engaging in anticompetitive behavior, just that you are the only real option in town. reply Vecr 7 hours agorootparentprevWill they? Given the structure of global controls on GPUs, Nvidia is a de-facto self funding US government company. Maybe the US will do something if GPU price becomes the limit instead of the supply of chips and power. reply kadoban 14 hours agorootparentprevWhat effect did the DOJ have on MS in the 90s? Didn't all of that get rolled back before they had to pay a dime, and all it amounted to was that browser choice screen that was around for a while? Hardly a crippling blow. If anything that showed the weakness of regulators in fights against big tech, just outlast them and you're fine. reply shiroiushi 14 hours agorootparentprev>I expect the DOJ to hit them harder than they did MS in the 90s given the bullshit they are pulling and the drag on the economy they are causing. It sounds like you're expecting extreme competence from the DOJ. Given their history with regulating big tech companies, and even worse, the incoming administration, I think this is a very unrealistic expectation. reply perching_aix 16 hours agorootparentprevAnd I'm supposed to believe that HN is this amazing platform for technology and science discussions, totally unlike its peers... reply zamadatix 16 hours agorootparentThe above take is worded a bit cynical but is their general approach to GPUs lately across the board e.g. https://www.techpowerup.com/326415/amd-confirms-retreat-from... Also I'd take HN as being being an amazing platform for the overall consistency and quality of moderation. Anything beyond that depends more on who you're talking to than where at. reply petesergeant 16 hours agorootparentprevMaybe be the change you want to see and tell us what the real story is? reply perching_aix 9 hours agorootparentWe seem to disagree on what the change in the world I'd like to see is like, which is a real shocker I'm sure. Personally, I think that's when somebody who has no real information to contribute doesn't try to pretend that they do. So thanks for the offer, but I think I'm already delivering on that realm. reply llm_trw 15 hours agorootparentprevI don't really care what you believe. Everyone whose dug deep into what AMD is doing has left in disgust if they are lucky and bankruptcy if they are not. If I can save someone else from wasting $100,000 on hardware and six months of their life then my post has done more good than the AMD marketing department ever will. reply AnthonyMouse 13 hours agorootparent> If I can save someone else from wasting $100,000 on hardware and six months of their life then my post has done more good than the AMD marketing department ever will. This seems like unuseful advice if you've already given up on them. You tried it and at some point in the past it wasn't ready. But by not being ready they're losing money, so they have a direct incentive to fix it. Which would take a certain amount of time, but once you've given up you no longer know if they've done it yet or not, at which point your advice would be stale. Meanwhile the people who attempt it apparently seem to get acquired by Nvidia, for some strange reason. Which implies it should be a worthwhile thing to do. If they've fixed it by now which you wouldn't know if you've stopped looking, or they fix it in the near future, you have a competitive advantage because you have access to lower cost GPUs than your rivals. If not, but you've demonstrated a serious attempt to fix it for everyone yourself, Nvidia comes to you with a sack full of money to make sure you don't finish, and then you get a sack full of money. That's win/win, so rather than nobody doing it, it seems like everybody should be doing it. reply llm_trw 11 hours agorootparentI've tried it three times. I've seen people try it every six months for two decades now. At some point you just have to accept that AMD is not a serious company, but is a second rate copycat and there is no way to change that without firing everyone from middle management up. I'm deeply worried about stagnation in the CPU space now that they are top dog and Intel is dead in the water. Here's hoping China and Risk V save us. >Meanwhile the people who attempt it apparently seem to get acquired by Nvidia Everyone I've seen base jumping has gotten a sponsorship from redbull, ergo. everyone should basejump. Ignore the red smears around the parking lot. reply AnthonyMouse 20 minutes agorootparent> At some point you just have to accept that AMD is not a serious company, but is a second rate copycat and there is no way to change that without firing everyone from middle management up. AMD has always punched above their weight. Historically their problem was that they were the much smaller company and under heavy resource constraints. Around the turn of the century the Athlon was faster than the Pentium III and then they made x86 64-bit when Intel was trying to screw everyone with Itanic. But the Pentium 4 was a marketing-optimized design that maximized clock speed at the expense of heat and performance per clock. Intel was outselling them even though the Athlon 64 was at least as good if not better. The Pentium 4 was rubbish for laptops because of the heat problems, so Intel eventually had to design a separate chip for that, but they also had the resources to do it. That was the point that AMD made their biggest mistake. Their next chip was one designed to compete with the Pentium 4, a power-hungry monster designed to hit high clock speeds at the expense of performance per clock. But the reason more people didn't buy the Athlon 64 wasn't that they couldn't figure out that a 2.4GHz CPU could be faster than a 2.8GHz CPU, it was all the anti-competitive shenanigans Intel was doing behind closed doors to e.g. keep PC OEMs from featuring systems with AMD CPUs. Meanwhile by then Intel had figured out that the Pentium 4 was, in fact, a bad design, when their own Pentium M laptops started outperforming the Pentium 4 desktops. So the Pentium 4 line got canceled and Bulldozer had to go up against the Pentium M-based Core, which nearly bankrupted AMD and compromised their ability to fund the R&D needed to sustain state of the art fabs. Since then they've been climbing back out of the hole but it wasn't until Ryzen in 2017 that you could safely conclude they weren't on the verge of bankruptcy, and even then they were saddled with a lot of debt and contracts requiring them to use the uncompetitive Global Foundries fabs for several years. It wasn't until Zen4 in 2022 that they finally got to switch the whole package to TSMC. So until quite recently the answer to the question \"why didn't they do X?\" was obvious. They didn't have the money. But now they do. reply Const-me 6 hours agorootparentprev> I've tried it three times Have you tried compute shaders instead of that weird HPC-only stuff? Compute shaders are widely used by millions of gamers every day. GPU vendors have huge incentive to make them reliable and efficient: modern game engines are using them for lots of thing, e.g. UE5 can even render triangle meshes with GPU compute instead of graphics (the tech is called nanite virtualized geometry). In practice they work fine on all GPUs, ML included: https://github.com/Const-me/Cgml reply perching_aix 9 hours agorootparentprevI'd be very concerned if somebody makes a $100K decision based on a comment where the author couldn't even differentiate between the words \"constitutionally\" and \"institutionally\", while providing as much substance as any other random techbro on any random forum and being overwhelmingly oblivious to it. reply lofaszvanitt 13 hours agorootparentprevIt had to destroy itself. These companies do not act on their own... reply zamalek 16 hours agoprevI have been playing around with Phi-4 Q6 on my 7950x and 7900XT (with HSA_OVERRIDE_GFX_VERSION). It's bloody fast, even with CPU alone - in practical terms it beats hosted models due to the roundtrip time. Obviously perf is more important if you're hosting this stuff, but we've definitely reached AMD usability at home. reply throwaway314155 18 hours agoprev> Aug 9, 2023 Ignoring the very old (in ML time) date of the article... What's the catch? People are still struggling with this a year later so I have to assume it doesn't work as well as claimed. I'm guessing this is buggy in practice and only works for the HF models they chose to test with? reply Const-me 17 hours agoparentIt’s not terribly hard to port ML inference to alternative GPU APIs. I did it for D3D11 and the performance is pretty good too: https://github.com/Const-me/Cgml The only catch is, for some reason developers of ML libraries like PyTorch aren’t interested in open GPU APIs like D3D or Vulkan. Instead, they focus on proprietary ones i.e. CUDA and to lesser extent ROCm. I don’t know why that is. D3D-based videogames are heavily using GPU compute for more than a decade now. Since Valve shipped SteamDeck, the same now applies to Vulkan on Linux. By now, both technologies are stable, reliable and performant. reply jsheard 17 hours agorootparentIsn't part of it because the first-party libraries like cuDNN are only available through CUDA? Nvidia has poured a ton of effort into tuning those libraries so it's hard to justify not using them. reply Const-me 17 hours agorootparentUnlike training, ML inference is almost always bound by memory bandwidth as opposed to computations. For this reason, tensor cores, cuDNN, and other advanced shenanigans make very little sense for the use case. OTOH, general-purpose compute instead of fixed-function blocks used by cuDNN enables custom compression algorithms for these weights which does help, by saving memory bandwidth. For example, I did custom 5 bits/weight quantization which works on all GPUs, no hardware support necessary, just simple HLSL codes: https://github.com/Const-me/Cgml?tab=readme-ov-file#bcml1-co... reply boroboro4 15 hours agorootparentOnly local (read batch size 1) ML inference is memory bound, production loads are pretty much compute bound. Prefill phase is very compute bound, and with continuous batching generation phase is getting mixed with prefill, which makes whole process altogether to be compute bound too. So no, tensor cores and all other shenanigans absolutely critical for performant inference infrastructure. reply Const-me 15 hours agorootparentPyTorch is a project by Linux foundation. The about page with the mission of the foundation contains phrases like “empowering generations of open source innovators”, “democratize code”, and “removing barriers to adoption”. I would argue running local inference with batch size=1 is more useful for empowering innovators compared to running production loads on shared servers owned by companies. Local inference increases count of potential innovators by orders of magnitude. BTW, in the long run it may also benefit these companies because in theory, an easy migration path from CUDA puts a downward pressure on nVidia’s prices. reply idonotknowwhy 14 hours agorootparentMost people running local inference do so thorough quants with llamacpp (which runs on everything) or awq/exl2/mlx with vllm/tabbyAPI/lmstudio which are much faster to than using pytorch directly reply lhl 5 hours agoparentprevIt depends on what you mean by \"this.\" MLC's catch is that you need to define/compile models for it with TVM. Here is the list of supported model architectures: https://github.com/mlc-ai/mlc-llm/blob/main/python/mlc_llm/m... llama.cpp has a much bigger supported model list, as does vLLM and of course PyTorch/HF transformers covers everything else, all of which work w/ ROCm on RDNA3 w/o too much fuss these days. For inference, the biggest caveat is that Flash Attention is only an aotriton implementation, which besides being less performant sometimes, also doesn't support SWA. For CDNA there is a better CK-based version of FA, but CK doesn't not have RDNA support. There are a couple people at AMD apparently working on native FlexAttention, os I guess we'll how that turns out. (Note the recent SemiAccurate piece was on training, which I'd agree is in a much worse state (I have personal experience with it being often broken for even the simplest distributed training runs). Funnily enough, if you're running simple fine tunes on a single RDNA3 card, you'll probably have a better time. OOTB, a 7900 XTX will train at about the same speed as an RTX 3090 (4090s blow both of those away, but you'll probably want more cards and VRAM of just move to H100s). reply mattfrommars 14 hours agoprevGreat, I have yet to understand why does not the ML community really push or move away from CUDA? To me, it feel like a dinosaur move to build on top of CUDA which is screaming proprietary nothing about it is open source or cross platform. The reason why I say its dinosaur is, imagine, we as a dev community continued to build on top of Flash or Microsoft Silverlight... LLM and ML has been out for quiet a while, with AI/LLM advancement, the transition must have been much quicker to move cross platform. But this hasn't yet and not sure when it will happen. Building a translation layer on top CUDA is not the answer either to this problem. reply idonotknowwhy 14 hours agoparentFor me personally, hacking together projects as a hobbiest, 2 reasons : 1. It just works. When i tried to build things on Intel Arcs, i spent way more hours bikeshedding ipex and driver issues than developing 2. LLMs seem to have more cuda code in their training data. I can leverage claude and 4o to help me build things with cuda, but trying to get them to help me do the same things on ipex just doesn't work. I'd very much love a translation layer for Cuda, like a dxvk or wine equivalent. Would save a lot of money since Arc gpus are in the bargain bin and nvidia cloud servers are double the price of AMD. As it stands now, my dual Intel Arc rig is now just a llama.cpp inference server for the family to use. reply FloatArtifact 10 hours agorootparentWhat kind of model learn and what's its token output on intel gpu's? reply dwood_dev 14 hours agoparentprevExcept I never hear complaints about CUDA from a quality perspective. The complaints are always about lock in to the best GPUs on the market. The desire to shift away is to make cheaper hardware with inferior software quality more usable. Flash was an abomination, CUDA is not. reply AnthonyMouse 12 hours agorootparentFlash was popular because it was an attractive platform for the developer. Back then there was no HTML5 and browsers didn't otherwise support a lot of the things Flash did. Flash Player was an abomination, it was crashy and full of security vulnerabilities, but that was a problem for the user rather than the developer and it was the developer choosing what to use to make the site. This is pretty much exactly what happens with CUDA. Developers like it but then the users have to use expensive hardware with proprietary drivers/firmware, which is the relevant abomination. But users have some ability to influence developers, so as soon as we get the GPU equivalent of HTML5, what happens? reply wqaatwt 10 hours agorootparent> users have to use expensive hardware with proprietary drivers/firmware What do you mean by that? People trying to run their own models are not “the users” they are a tiny insignificant niche segment. reply AnthonyMouse 1 hour agorootparentThere are far more people running llama.cpp, various image generators, etc. than there are people developing that code. Even when the \"users\" are corporate entities, they're not necessarily doing any development in excess of integrating the existing code with their other systems. We're also likely to see a stronger swing away from \"do inference in the cloud\" because of the aligned incentives of \"companies don't want to pay for all that hardware and electricity\" and \"users have privacy concerns\" such that companies doing inference on the local device will have both lower costs and a feature they can advertise over the competition. What this is waiting for is hardware in the hands of the users that can actually do this for a mass market price, but there is no shortage of companies wanting a piece of that. In particular, Apple is going to be pushing that hard and despite the price they do a lot of volume, and then you're going to start seeing more PCs with high-VRAM GPUs or iGPUs with dedicated GDDR/HBM on the package as their competitors want feature parity for the thing everybody is talking about, the cost of which isn't actually that high, e.g. 40GB of GDDR6 is less than $100. reply xedrac 14 hours agorootparentprevMaybe the situation has gotten better in recent years, but my experience with Nvidia toolchains was a complete nightmare back in 2018. reply claytonjy 14 hours agorootparentThe cuda situation is definitely better. The nvidia struggles are now with the higher-level software they’re pushing (triton, tensor-llm, riva, etc), tools that are the most performant option when they work, but a garbage developer experience when you step outside the golden path reply cameron_b 1 hour agorootparentI want to double-down on this statement, and call attention to the competitive nature of it. Specifically, I have recently tried to set up Triton on arm hardware. One might presume Nvidia would give attention to an architecture they develop, but the way forward is not easy. For some version of Ubuntu, you might have the correct version of python ( usually older than packaged ) but current LTS is out of luck for guidance or packages. https://github.com/triton-lang/triton/issues/4978 reply latchkey 16 hours agoprevPreviously: Making AMD GPUs competitive for LLM inference https://news.ycombinator.com/item?id=37066522 (August 9, 2023 — 354 points, 132 comments) reply melodyogonna 2 hours agoprevModular claims that it achieves 93% GPU utilization on AMD GPUs [1], official preview release coming early next year, we'll see. I must say I'm bullish because of feedback I've seen people give about the performance on Nvidia GPUs 1.https://www.modular.com/max reply lasermike026 17 hours agoprevI believe these efforts are very important. If we want this stuff to be practical we are going to have to work on efficiency. Price efficiency is good. Power and compute efficiency would be better. I have been playing with llama.cpp to run interference on conventional cpus. No conclusions but it's interesting. I need to look at llamafile next. reply lxe 16 hours agoprevA used 3090 is $600-900, performs better than 7900, and is much more versatile because CUDA reply Uehreka 15 hours agoparentReality check for anyone considering this: I just got a used 3090 for $900 last month. It works great. I would not recommend buying one for $600, it probably either won’t arrive or will be broken. Someone will reply saying they got one for $600 and it works, that doesn’t mean it will happen if you do it. I’d say the market is realistically $900-1100, maybe $800 if you know the person or can watch the card running first. All that said, this advice will expire in a month or two when the 5090 comes out. reply idonotknowwhy 14 hours agorootparentI've bought 5 used and they're all perfect. But that's what buyer protection on ebay is for. Had to send back an Epyc mobo with bent pins and ebay handled it fine. reply Sparkyte 6 hours agoprevMore players in the market the better. AI shouldn't be owned by one business. reply mrcsharp 11 hours agoprevI will only consider AMD GPUs for LLM when I can easily make my AMD GPU available within WSL and Docker on Windows. For now, it is as if AMD does not exist in this field for me. reply lhl 11 hours agoprevJust an FYI, this is writeup from August 2023 and a lot has changed (for the better!) for RDNA3 AI/ML support. That being said, I did some very recent inference testing on an W7900 (using the same testing methodology used by Embedded LLM's recent post to compare to vLLM's recently added Radeon GGUF support [1]) and MLC continues to perform quite well. On Llama 3.1 8B, MLC's q4f16_1 (4.21MB weights) performed +35% faster than llama.cpp w/ Q4_K_M w/ their ROCm/HIP backend (4.30MB weights, 2% size difference). That makes MLC still the generally fastest standalone inference engine for RDNA3 by a country mile. However, you have much less flexibility with quants and by and large have to compile your own for every model, so llama.cpp is probably still more flexible for general use. Also llama.cpp's (recently added to llama-server) speculative decoding can also give some pretty sizable performance gains. Using a 70B Q4_K_M + 1B Q8_0 draft model improves output token throughput by 59% on the same ShareGPT testing. I've also been running tests with Qwen2.5-Coder and using a 0.5-3B draft model for speculative decoding gives even bigger gains on average (depends highly on acceptance rate). Note, I think for local use, vLLM GGUF is still not suitable at all. When testing w/ a 70B Q4_K_M model (only 40GB), loading, engine warmup, and graph compilation took on avg 40 minutes. llama.cpp takes 7-8s to load the same model. At this point for RDNA3, basically everything I need works/runs for my use cases (primarily LLM development and local inferencing), but almost always slower than an RTX 3090/A6000 Ampere (a new 24GB 7900 XTX is $850 atm, used or refurbished 24 GB RTX 3090s are in in the same ballpark, about $800 atm; a new 48GB W7900 goes for $3600 while an 48GB A6000 (Ampere) goes for $4600). The efficiency gains can be sizable. Eg, on my standard llama-bench test w/ llama2-7b-q4_0, the RTX 3090 gets a tg128 of 168 t/s while the 7900 XTX only gets 118 t/s even though both have similar memory bandwidth (936.2 GB/s vs 960 GB/s). It's also worth noting that since the beginning of the year, the llama.cpp CUDA implementation has gotten almost 25% faster, while the ROCm version's performance has stayed static. There is an actively (solo dev) maintained fork of llama.cpp that sticks close to HEAD but basically applies a rocWMMA patch that can improve performance if you use the llama.cpp FA (still performs worse than w/ FA disabled) and in certain long-context inference generations (on llama-bench and w/ this ShareGPT serving test you won't see much difference) here: https://github.com/hjc4869/llama.cpp - The fact that no one from AMD has shown any interest in helping improve llama.cpp performance (despite often citing llama.cpp-based apps in marketing/blog posts, etc is disappointing ... but sadly on brand for AMD GPUs). Anyway, for those interested in more information and testing for AI/ML setup for RDNA3 (and AMD ROCm in general), I keep a doc with lots of details here: https://llm-tracker.info/howto/AMD-GPUs [1] https://embeddedllm.com/blog/vllm-now-supports-running-gguf-... reply dragontamer 18 hours agoprevIntriguing. I thought AMD GPUs didn't have tensor cores (or matrix multiplication units) like NVidia. I believe they are only dot product / fused multiply and accumulate instructions. Are these LLMs just absurdly memory bound so it doesn't matter? reply boroboro4 15 hours agoparentThey absolutely do have similar cores to tensor cores, it's called matrix cores. And they have particular instructions to utilize them (MFMA). Note I'm talking about DC compute chips, like MI300. LLMs aren't memory bound in production loads, they are pretty much compute bound too, at least in prefill phase, but in practice in general too. reply almostgotcaught 15 hours agorootparentYa people in these comments don't know what they're talking about (no one ever does in these threads). AMDGPU has had MMA and WMMA for a while now https://rocm.docs.amd.com/projects/rocWMMA/en/latest/what-is... reply ryao 17 hours agoparentprevThey don’t, but GPUs were designed for doing matrix multiplications even without the special hardware instructions for doing matrix multiplication tiles. Also, the forward pass for transformers is memory bound, and that is what does token generation. reply dragontamer 17 hours agorootparentWell sure, but in other GPU tasks, like Raytracing, the difference between these GPUs is far more pronounced. And AMD has passable Raytracing units (NVidias are better but the difference is bigger than these LLM results). If RAM is the main bottleneck then CPUs should be on the table. reply IX-103 17 hours agorootparent> If RAM is the main bottleneck then CPUs should be on the table That's certainly not the case. The graphics memory model is very different from the CPU memory model. Graphics memory is explicitly designed for multiple simultaneous reads (spread across several different buses) at the cost of generality (only portions of memory may be available on each bus) and speed (the extra complexity means reads are slower). This makes then fast at doing simple operations on a large amount of data. CPU memory only has one bus, so only a single read can happen at a time (a cache line read), but can happen relatively quickly. So CPUs are better for workloads with high memory locality and frequent reuse of memory locations (as is common in procedural programs). reply dragontamer 12 hours agorootparent> CPU memory only has one bus If people are paying $15,000 or more per GPU, then I can choose $15,000 CPUs like EPYC that have 12-channels or dual-socket 24-channel RAM. Even desktop CPUs are dual-channel at a minimum, and arguably DDR5 is closer to 2 or 4 buses per channel. Now yes, GPU RAM can be faster, but guess what? https://www.tomshardware.com/pc-components/cpus/amd-crafts-c... GPUs are about extremely parallel performance, above and beyond what traditional single-threaded (or limited-SIMD) CPUs can do. But if you're waiting on RAM anyway?? Then the compute-method doesn't matter. Its all about RAM. reply webmaven 17 hours agorootparentprevRAM is (often) the bottleneck for highly parallel GPUs, but not for CPUs. Though the distinction between the two categories is blurring. reply schmidtleonard 17 hours agorootparentprevCPUs have pitiful RAM bandwidth compared to GPUs. The speeds aren't so different but GPU RAM busses are wiiiiiiiide. reply teleforce 16 hours agorootparentCompute Express Link (CXL) should mostly solve limited RAM with CPU: 1) Compute Express Link (CXL): https://en.wikipedia.org/wiki/Compute_Express_Link PCIe vs. CXL for Memory and Storage: https://news.ycombinator.com/item?id=38125885 reply schmidtleonard 16 hours agorootparentGigabytes per second? What is this, bandwidth for ants? My years old pleb tier non-HBM GPU has more than 4 times the bandwidth you would get from a PCIe Gen 7 x16 link, which doesn't even officially exist yet. reply teleforce 14 hours agorootparentYes CXL will soon benefit from PCIe Gen 7 x16 with expected 64GB/s in 2025 and the non-HBM bandwidth I/O alternative is increasing rapidly by the day. For most inferences of near real-time LLM it will be feasible. For majority of SME companies and other DIY users (humans or ants) with their localized LLM should not be any issues [1],[2]. In addition new techniques for more efficient LLM are being discover to reduce the memory consumption [3]. [1] Forget ChatGPT: why researchers now run small AIs on their laptops: https://news.ycombinator.com/item?id=41609393 [2] Welcome to LLMflation – LLM inference cost is going down fast: https://a16z.com/llmflation-llm-inference-cost/ [3] New LLM optimization technique slashes memory costs up to 75%: https://news.ycombinator.com/item?id=42411409 reply schmidtleonard 5 hours agorootparentNo. Memory bandwidth is the important factor for LLM inference. 64GB/s is 4x less than the hypothetical I granted you (Gen7x16 = 256GB/s), which is 4x less than the memory bandwidth on my 2 year old pleb GPU (1TB/s), which is 10x less than a state of the art professional GPU (10TB/s), which is what the cloud services will be using. That's 160x worse than cloud and 16x worse than what I'm using for local LLM. I am keenly aware of the options for compression. I use them every day. The sacrifices I make to run local LLM cut deep compared to the cloud models, and squeezing it down by another factor of 16 will cut deep on top of cutting deep. Nothing says it can't be useful. My most-used model is running in a microcontroller. Just keep those expectations tempered. (EDIT: changed the numbers to reflect red team victory over green team on cloud inference.) reply Dylan16807 13 hours agorootparentprev> 4 times the bandwidth you would get from a PCIe Gen 7 x16 link So you have a full terabyte per second of bandwidth? What GPU is that? (The 64GB/s number is an x4 link. If you meant you have over four times that, then it sounds like CXL would be pretty competitive.) reply schmidtleonard 5 hours agorootparenthttps://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889 Memory Size: 24 GB Memory Type: GDDR6X Memory Bus: 384 bit Bandwidth: 1.01 TB/s Bandwidth between where the LLM is stored and where your matrix*vector multiplies are done is the important figure for inference. You want to measure this in terabytes per second, not gigabytes per second. A 7900XTX also has 1TB/s on paper, but you'll need awkward workarounds every time you want to do something (see: article) and half of your workloads will stop dead with driver crashes and you need to decide if that's worth $500 to you. Stacking 3090s is the move if you want to pinch pennies. They have 24GB of memory and 936GB/s of bandwidth each, so almost as good as the 4090, but they're as cheap as the 7900XTX with none of the problems. They aren't as good for gaming or training workloads, but for local inference 3090 is king. It's not a coincidence that the article lists the same 3 cards. These are the 3 cards you should decide between for local LLM, and these are the 3 cards a true competitor should aim to exceed. reply Dylan16807 23 minutes agorootparentA 4090 is not \"years old pleb tier\". Same for 3090 and 7900XTX. There's a serious gap between CXL and RAM, but it's not nearly as big as it used to be. reply adrian_b 2 hours agorootparentprevAlready an ancient Radeon VII from 5 years ago had 1 terabyte per second of memory bandwidth. Later consumer GPUs have regressed and only RTX 4090 offers the same memory bandwidth in the current NVIDIA generation. reply Dylan16807 21 minutes agorootparentRadeon VII had HBM. So I can understand a call for returning to HBM, but it's an expensive choice and doesn't fit the description. reply throwaway314155 17 hours agoparentprev> Are these LLMs just absurdly memory bound so it doesn't matter? During inference? Definitely. Training is another story. reply sroussey 18 hours agoprev[2023] Btw, this is from MLC-LLM which makes WebLLM and other good stuff. reply aussieguy1234 12 hours agoprevI got a \"gaming\" PC for LLM inference with an RTX 3060. I could have gotten more VRAM for my buck with AMD, but didn't because at the time alot of inference needed CUDA. As soon AMD is as good as Nvidia for inference, I'll switch over. But I've read on here that their hardware engineers aren't even given enough hardware to test with... reply leonewton253 15 hours agoprev [–] This benchmark doest look right. Is it using the tensor cores in the Nvidia gpu? AMD does not have AI cores so should run noticeably slower. reply nomel 15 hours agoparent [–] AMD has WMMA. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MLC-LLM facilitates the compilation and deployment of Large Language Models (LLMs) on AMD GPUs using ROCm, achieving performance close to NVIDIA's high-end GPUs.- The AMD Radeon RX 7900 XTX offers competitive performance, reaching 80% of NVIDIA RTX 4090's speed and 94% of RTX 3090 Ti's for specific models, while being significantly more affordable.- The project highlights the potential of AMD GPUs for LLM inference, with ongoing improvements in software support and plans for future enhancements like batching and multi-GPU support."
    ],
    "commentSummary": [
      "AMD's consumer GPUs, such as the RX7900XTX, differ in architecture from their datacenter GPUs like the MI300X, affecting performance levels.- There is a growing effort to integrate AMD's CDNA architecture and HIP (Heterogeneous-compute Interface for Portability) support into deep learning platforms, aiming to challenge Nvidia's dominance in AI workloads.- Despite increased interest, challenges such as software support and performance consistency persist, leading some developers to revert to Nvidia, which maintains a significant advantage with its CUDA (Compute Unified Device Architecture) platform."
    ],
    "points": 241,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1734999438
  },
  {
    "id": 42498462,
    "title": "Why are cancer guidelines stuck in PDFs?",
    "originLink": "https://seangeiger.substack.com/p/why-are-cancer-guidelines-stuck-in",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"seangeiger.substack.com\",cType: 'non-interactive',cRay: '8f72d8045c9e67b5',cH: 'knc76qa40n4n8jWIVo91xf0RXZGduROwaCT0Fs9EYHE-1735066926-1.2.1.1-OZPEvDPguFM5kQodz7B6CjdRH8Ga68Wo3Ygt2qYUchrFRtV3d5q8ZyAt7JEVPJAA',cUPMDTk: \"\\/p\\/why-are-cancer-guidelines-stuck-in?__cf_chl_tk=XVrySHpE5yDTXEQDHh9qdMCOW19yK2QKOxJFc9aAZSM-1735066926-1.0.1.1-ypGZbUYsDnQEXjQ4NK7Ck2D0pFJshtBoL0LOS454yJQ\",cFPWv: 'b',cITimeS: '1735066926',cTTimeMs: '1000',cMTimeMs: '120000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/why-are-cancer-guidelines-stuck-in?__cf_chl_f_tk=XVrySHpE5yDTXEQDHh9qdMCOW19yK2QKOxJFc9aAZSM-1735066926-1.0.1.1-ypGZbUYsDnQEXjQ4NK7Ck2D0pFJshtBoL0LOS454yJQ\",md: \"vFRpHEWLDO2oBXPkYDJ.9OF8YSuguozJVdjMrNbk7iU-1735066926-1.2.1.1-igw2CPjIxNmMtwqTEXyi..9rpBxYKwp8YsUlb_uXaSpJ85uwSOf_R2hnzkc3AMmLs8b2iS9WqR5taLzpryUvu.XC4HAlxgE51YTObFFqvo5BOGzqkER6UbYuH8AX_kgNotTH5Yr7stf_D.i_0HxHyqHGEgUWrbAAFGZd1HsaO_666Cp4GuZtoBFVH.wMGzBg7g1ITdT_OmElQjQeGtyarkpXZli1P.GNgaH6PYezjnxZP.4y6N9gDL310_OJ.M7aw2vQ4r31q5gHP2bjv2PRo.yZKiDDo6di27q8fx6jjBhPEp_Yl0.nHEltyFROk77.ouRUc_xxfPg6BRArnD8nSz.OaKA7goJzDNss2bxTeAK_4bOE4Rnkfz3Ws1RR658SPUEXt.F9yAoodtysx7bFVYivhwMuf9CBNtdAqiL9ORJyNvLrHuWVhLeazZrZ3S.rLY4.v756XS9.FlAD0ccdZBBYz5sW5HPHzb1IgYekZ8ga5CBrSDze63nb8u0MgDeCllu_J_ndUQM9Dh9J.K_xgQU.vyo.aE6j8Dk6rjHMPrqa2u3Ry4vMwnIvnPrk6XvUDCLRVsqKASubfOuSHFPWPVFdtxWUrTt7W4jN2SgQWrEEFyYjI7yIh5alVDkxPgVEofwLc31EyaQ.7ool9ZrpRuPWiRGCJO1XRM_eSuOvdfnAD0uz2M_qwAWy0hJxpEoC.2.iondl9J1bX_99ATbuLxV.LCOKELZYjDxrFj4D1Qldk7KzYfw1KYBN.dffkvmu265sZo_zsUIWF3dM6Qn5_Md43PFU9mEKlF2MBkGVpLttYLmzTRUhMeTaMWazfMpPTxzZaL9dKrRSly8ZJ9SN9gGVPVAjShH8GvCVWjoMhyphvio6GnbwDkbYh4GoU7do2yzsIRH51HDuUu4bi5LHo0Pg1v1lLXwEJ0KeLY_kZ..5YZlB7vNbDCHCqn3h2NpKukn8uKdTFzHpMJvdCfK60pPbrEzW1BLvDj6EjeTKLS0t6lr5NL0d9z4OMmiaG.I.e7V58ZIwt76dTjd7GzjTqVQML.kWIaPRG9RGbXbOeBx2d4yhFahc5mskt9b4UKiQ74SrFADcNrASb.gR2VuMYU6S7lohvU1gywt4U3GZa80TCbltcJ1sRfiJIDebsN41Zc2iCHUGbDTh2pJVqYTa_f33DAbmdXPrj4gz.GwB9PzyLFtFp92Q3MtE5AUdPQSeXniyDuh3kNjgPxSf4m0nwmdv2Y0Xx26n55.21WpeeFLuDrpAyn.WnaBUrP5LvNJYbms0cCfRNqGePCxn6pYuCT7CFqolMm2lyrwkjEYmdvS3hCnyEK.wWlDDEqgMUJFANVt6rLHG5DA25piPitSsI8RIBHb5u8ylZ0g_ONqgNUsIlftkrOB_sXRdSfviS_diYTHUlKnB9KeowcrENPuhjIgejY352t97wQFcCXSGcCQzUMHwOTDdIs75aBdWlM64KqDFNIOa3_MPD00KSQnrc4UI6YQEqEo87AWx6UTnWrEsgJWs9MorByBcqZc55hlMbqC_kfxYjFLv3KiMW49sYe.o_wkV8k9egAzgkN.bZHQB0gJJWXV8dknskzhlXI3NM8zVT21tfrAgFkq1d.g5OcaK4kZRQAFKyFeNuDldMvE1LqRmlBgQKl8JZIxqwgjjwyEyGBwSQDkAgYUm_A3I1TLG072_kFCidJUYJ5ShwNbzorxLm9BgGxztfMBPNx80g97n94ZSPlrdD3V8F2E9vhA980PVHuHnenQdRpZY.nLco4UdqOQXC5.UU3fLsZQ2owUYfwFHmqL0N2pkAzQ8m7BESzrhFAyVdq0Tbxh0qncMSWxhW8Y2zeCAHuGF4f3kKANmWOv5Q2v5sTO5hER17ZP4vp168WTAtHOj.4yhhjMHt8bHbBBc3HIKdAIiwBOs\",mdrd: \"bFeq8GGz7bxB.lj.7W0iCaKlx0Di.xbrZgMM5nhw8OI-1735066926-1.2.1.1-_OOMosrSFS0Yo7S.r_j7wxBPpjsKTDadqMQp5gEE.qNZ1b7w8cN2wF.l5qkoegafCE7Ykj2dfcKkX0R5WXfU0YdIUKnFlSERoJWCb5qry_LiZd5UdDuhmBkgbbmDtZXjaDHij0fYRg1Zr7x3f_VeA7cv.aTlV7_STdaEoXa4dM6E4fHJRqCFtmh899x._YOwCaRUtf3tzxc6SopbAMSrEIt5s3bh5Xdh8FavA2e6aTTJnd8sPt__puOkbe3hygOKBfuo5ZoagRP04nHs.Bf_hOTY4Wm_NeYdh6z2WKc4jfE.Q4KGRAwc_BnrdShwzcKoDzbCsqu13scPD9ms02eVXQMkUIClYrqUs7rVAG1lUSr5kIbISbUfUP.toWgc3jCdJvY85T08.Twhs7O.sW5wWJ2VSX6DRAa.yWUrc1dAIIj0FoEDpNY22NOIPBh_ywneaW8Evos4BozCE36ZrjVspx8NF0NJCvKBOC_GMG5lfo.aB57uNOeTYnm0RoU8hDGJvpBnGaEYBQWMbHgQjvqyz35PvLXJJ4izPz0E3XHN2q_oyJVRkB7e8SoqtlSqnxQGMd3AADLXvHEwsO4_B.YTe54LrVUd.puiDI6Pfe0INoXx38Lin5.E6t3JVEsjyMNbrwbiP7jxg.WM67P.OIddJkvfwH4p2WFT9NZ1IFW_.CWOarkDQOHLBXqkFC_.LlmroTVroABAmOXrtb._og8uRa3C6_Uzf8RGvUJiKgBcmRHd8ziAkW8O4JUI_3Tsxd4HWbvAJmfaxtIwg7ysKW.azLEicpOidBBZayJ7wFv11S9wmwgmwCMade8VwXm0LGuW621VNxca7OTL5n3H4c8g5z.HNIuv536DyCMBftgnOZc_ipmf8ON6OuUhkfjjwWTycnsUBKXwm53xmS_hsd.ZmRFL5c8X3VSaulOm2FRw3SUtw1gDxOljBQBxFkDVS5JHYfKVxIr8KqxDI90svx9rWgvFn2ZmJMNipktpAN3C1o4KSllkYkYCqE8uW0aVL1dyr5keCxN2Pip6fiWj1vS02EbcK7hvLJ7j2iBzHwUQpuHdC6.AIwfXTengBQX7tblCLViVe11hGL2ZS3aCnrWbJJeD0_y4sQx_lkjo4i5g6rcMQ5L2AMKgBaPgDpCtkKEw4Y54jI7h.xZ4cMHBaZGrJxOv_tL0Po9Kyo6Skrku.ofiIkTdLoC1OTEnKn0SWS.z9YoxifkESD3mMH_6Bfe1_RWSN06G2oV9YxEip1s3fxFv0CP5MHLr8yxRENWrcawSf0Gm0Dvy4hL2.RxuQ6Z2wrSlRmEyDgR5sl2.C1L5dfS.fUwbmn7mRlOMTsy77DnVY2CJ2OrSnqzkKIx4YXRkLCG9hp.sypntwH7kLPVGuaQ0mbX3yzlKgfrCtoFU1gj0aFbRDmVHyr02iR_ezsOv2eSguxkJqp0pkeXWbrn6S1RFppnE5fAYZ_s1dN0EuYuu_RGt4pJTe46JN_FBpp.Sk2Jz28O6Jwe5cgAcNlEQQQfCJHklTLiSEVFd2.TxwH_heMP6h6n7xC7mf6vUKfoyTIBnbGbGK854TwXnMQBYCyDZ3.wD3p2sC2bQ3xPoaIDnpGuM0fRPnZsGJ1QFg5uvDlrY4lMPsSGVoKaQT0b8KACZZegSluBttaYxFH8wpspwvs0p4Uy0vISaX4xQseEoWU40IDHCFEyIIynKZG3ddFdD_e2XdslMAXnv_ZfWghWDDOh5mH6IaX5tJVX.siB_hSKjcH3l6z64LjujlRSCiktmhkQh7rgGu88qF01CrK.Jsib_q65cr9Oo7VH_JpslXEUCeHODtFGouHqWEAxAp5XHJkEHeBeKAjFI34NmMpxM1vsyhVYEh8wJbG9R.HEwya9ElbKHTUiaG7EqPWBHt3esS5Zf5vCZyYZ.fBDxKU0MkBx7NfABdb_IGHbGm2pnCpKHuGo1spTtYYgvZxXdhEdvcTJYUWYg26d1io__ts6hlcPFhxuzTQVYrkhpx1KwK_vUZLY2xbAgeW60jEBpbdwajLhvMJlrpj1Pv0AJXeq.NQe3n9QrE3R4kKpfQpn_e7Fg0g2qf91HSFzw3P.nRdCfOWh93aXTBJJkrC827wpGScft_dh1qtdDjTlFTWTxEQGagLGwftMrgl2Hl2oQObZmXw_iNIg1898mibszoaMoRuJATnCDkpRRkeehH8rumgvY76G.IhUui5nuHJ9M1d3yqqAkTTEkEvBuY8mHJF8QjjfBB3M89_4BEeB.Ahl.Nn0DX4PuZXQ5Rw8N2hkH0xNXlbj4NcKFP7.mjkVGXQKEn1pO7FwMksRAS44PuHR6zTDN_uCZX3wAaGc_d7dn0SOWiDiIPZYwHJ4s754YE1FgsRlIMFjO4rdoo5YRROJZTIclXcLaiV2z3T7T6GTUOiApa4yoWlVR.YwFgjyuWIAb\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8f72d8045c9e67b5';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/why-are-cancer-guidelines-stuck-in?__cf_chl_rt_tk=XVrySHpE5yDTXEQDHh9qdMCOW19yK2QKOxJFc9aAZSM-1735066926-1.0.1.1-ypGZbUYsDnQEXjQ4NK7Ck2D0pFJshtBoL0LOS454yJQ\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42498462",
    "commentBody": "Why are cancer guidelines stuck in PDFs? (seangeiger.substack.com)228 points by huerne 19 hours agohidepastfavorite107 comments prepend 18 hours agoI’d rather have the pdf than a custom tool. Especially considering the tool will be unique to the practice or emr. And likely expensive to maintain. PDFs suck in many ways but are durable and portable. If I work with two oncologists, I use the same pdf. The author means well but his solution will likely be worse because only he will understand it. And there’s a million edge cases. reply slaucon 17 hours agoparentHey author here! Appreciate the feedback! Agreed on importance of portability and durability. I'm not trying to build this out or sell it as a tool to providers. Just wanted to demo what you could do with structured guidelines. I don't think there's any reason this would have to be unique to a practice or emr. As sister comments mentioned, I think the ideal case here would be if the guideline institutions released the structured representations of the guidelines along with the PDF versions. They could use a tool to draft them that could export in both formats. Oncologists could use the PDFs still, and systems could lean into the structured data. reply killjoywashere 16 hours agorootparentThe cancer reporting protocols from the College of American Pathologists are available in structured format (1). No major laboratory information system vendor properly implements them, properly, and their implementation errors cause some not-insignificant problems with patient care (oncologists calling the lab asking for clarification, etc). This has pushed labs to make policies disallowing the use of those modules and individual pathologists reverting to their own non-portable templates in Word documents. The medical information systems vendors are right up there with health insurance companies in terms of their investment in ensuring patient deaths. Ensuring. With an E. (1) https://www.cap.org/protocols-and-guidelines/electronic-canc... reply jjmarr 12 hours agorootparentIt doesn't look like the XML data is freely accessible. If I could get access to this data as a random student on the internet, I'd love to create an open source tool that generates an interactive visualization. reply all2 16 hours agorootparentprev> The medical information systems vendors are right up there with health insurance companies in terms of their investment in ensuring patient deaths. Ensuring. With an E. Can you expand on this? reply righthand 14 hours agorootparentMedical information system vendors only care about making a profit, not implementing actual solutions. The discrepancies between systems can lead to bad information which can cost people their life. reply ethbr1 2 hours agorootparentAs an analogy, imagine if the consequence of Oracle doing Oracle-as-usual things was worse medical outcomes. But they did them anyway for profit. That's basically medical information system vendors. The fact that the US hasn't pushed open source EMRs through CMS is insane. It's literally the perfect problem for an open solution. reply caboteria 2 hours agorootparentIt's worse than that. VistA is a world-class open source EMR that the VA has been trying to kill for decades. reply zo1 11 hours agorootparentprevPeople could potentially properly implement them if they were open and available: \"Contact the CAP for more information about licensing and using the CAP electronic Cancer Protocols for cancer reporting at your institution.\" This stinks of the same gate-keeping that places like NIST and ISO do, charging you for access to their \"standards\". reply prepend 6 hours agorootparentAren’t all NIST standards free as they are a government body? reply fl0id 8 hours agorootparentprevFor liability reasons alone, you cannot just have random people working on health/lab stuff and the requisite vendors have access to these standards. reply joshuaissac 2 hours agorootparentAccording to what killjoywashere said, the vendors do not want to implement these standards. So if CAP wants the standards to be relevant, they should release them for random people to implement. reply PoignardAzur 12 hours agorootparentprevI mean, you're attributing malice, but it could just be that reliably implementing the formats is a really really hard problem? reply TheAceOfHearts 11 hours agorootparentHow about fixing the format? Something that is obviously broken and resulting in patient deaths should really be considered a top priority. It's either malice or masskve incompetence. If these protocols were open there would definitely be volunteers willing to help fix it. reply prepend 6 hours agorootparentI think there are more options than malice or incompetence. My theory is difficulty. There’s multiple countries with socialized medicine and no profit motive and it’s still not solved. I think it’s just really complex with high negative consequences from a mistake. It takes lots of investment with good coordination to solve and there’s an “easy workaround” with pdfs that distributes liability to practitioners. reply ethbr1 2 hours agorootparentHealthcare suffers from strict regulatory requirements, underinvestment in organic IT capabilities, and huge integration challenges (system-to-system). Layering any sort of data standard into that environment (and evolving it in a timely manner!) is nigh impossible without an external impetus forcing action (read: government payer mandate). reply PoignardAzur 8 hours agorootparentprevYou seem to think that the default assumption is that fixing the format is easy/feasible, and I don't see why. Do you have domain knowledge pointing that way? It's a truism in machine learning that curating and massaging your dataset is the most labor-intensive and error-prone part of any project. I don't why that would stop being true in healthcare just because lives are on the line. reply mort96 10 hours agorootparentprevIncompetence at this level is intentional, it means someone doesn't think they'll see RoI from investing resources into improving it. Calling it malice is appropriate I feel. reply layer8 3 hours agorootparentIf there is no ROI, investing further resources would be charity work. I don’t think it’s accurate to call a company not doing so malicious. reply WitCanStain 3 hours agorootparentNot actively malicious perhaps, but prioritising profits over lives is evil. Either you take care to make sure the systems you sell lead to the best possible outcomes, or you get out of the sector. reply layer8 2 hours agorootparentThe company not existing at all might be worse though? I think it’s too easy to make blanket judgments like that from the outside, and it would be the job of regulation to counteract adverse incentives in the field. reply prepend 6 hours agorootparentprevI believe you have good intentions, but someone would need to build it out and sell it. And it requires lots of maintenance. It’s too boring for an open source community. There’s a whole industry that attempts to do what you do and there’s a reason why protocols keep getting punted back to pdf. I agree it would be great to release structured representations. But I don’t think there’s a standard for that representation, so it’s kind of tricky as who will develop and maintain the data standard. I worked on a decision support protocol for Ebola and it was really hard to get code sets released in Excel. Not to mention the actual decision gates in a way that is computable. I hope we make progress on this, but I think the incentives are off for the work to make the data structures necessary. reply Dalewyn 17 hours agorootparentprev>Agreed on importance of portability and durability. I think \"importance\" is understating it, because permanent consistency is practically the only reason we all (still) use PDFs in quite literally every professional environment as a lowest common denominator industrial standard. PDFs will always render the same, whether on paper or a screen of any size connected to a computer of any configuration. PDFs will almost always open and work given Adobe Reader, which these days is simply embedded in Chrome. PDFs will almost certainly Just Work(tm), and Just Working(tm) is a god damn virtue in the professional world because time is money and nobody wants to be embarrassed handing out unusable documents. reply abtinf 16 hours agorootparentPDFs generally will look close enough to the original intent that they will almost always be usable, but will not always render the same. If nothing else, there are seemingly endless font issues. reply lstamour 16 hours agorootparentIn this day and age that seems increasingly like a solved problem to most end users, often a client-side issue or using a very old method of generating a PDF? Modern PDF supports font embedding of various kinds (legality is left as an exercise to the PDF author) and supports 14 standard font faces which can be specified for compatibility, though more often document authors probably assume a system font is available or embed one. There are still problems with the format as it foremost focuses on document display rather than document structure or intent, and accessibility support in documents is often rare to non-existent outside of government use cases or maybe Word and the like. A lot of usability improvements come from clients that make an attempt to parse the PDF to make the format appear smarter. macOS Preview can figure out where columns begin and end for natural text selection, Acrobat routinely generates an accessible version of a document after opening it, including some table detection. Honestly creative interpretation of PDF documents is possibly one of the best use cases of AI that I’ve ever heard of. While a lot about PDF has changed over the years the basic standard was created to optimize for printing. It’s as if we started with GIF and added support to build interactive websites from GIFs. At its core, a PDF is just a representation of shapes on a page, and we added metadata that would hopefully identify glyphs, accessible alternative content, and smarter text/line selection, but it can fall apart if the PDF author is careless, malicious or didn’t expect certain content. It probably inherits all the weirdness of Unicode and then some, for example. reply layer8 3 hours agoparentprevI agree. However, since the PDF format supports structured data, one could in principle have it both ways, within a single file. reply queuebert 1 hour agorootparent^ This. See, e.g., https://lab6.com/ for some interesting tricks with the PDF format. reply Spooky23 14 hours agoparentprevI think there’s value if it can scale down. Community oncologists have limited technology resources as compared to a national cancer center. If we can make their lives easier, it can only be a good thing. That said, I like published documents like PDFs - systems usually make it hard to conii ok are the June release from the September release. reply KPGv2 17 hours agoparentprevYou say this, but on the other hand, the author alleges that the places that use these custom tools achieve better outcomes. You didn't address this point one way or the other. Do you think this is a completely fabricated non-explanation? It's not like the link says \"the worst places use these custom tools.\" reply crazygringo 17 hours agoparentprevExactly. The PDF's work. They won't break. You can see all the information with your own eyes. You can send them by e-mail. A wizard-type system hides most of the information from you, it might have bugs you aren't aware of, if you want to glance at an alternative path you can't, it's going to be locked into registered users, the system can go down. I think much more intelligent computer systems are the future in health care, but I doubt the way to start is with yet another custom tool designed specifically for cancer guidelines and nothing else. reply crabmusket 14 hours agorootparent> it's going to be locked into registered users, the system can go down I didn't see anything in the screenshots presented that wouldn't be doable in a single HTML file containing the data, styles and scripts? This is a countercultural idea but it fits so many use cases; it's a tragedy we don't do this more often. The two options are either PDF or SaaS. reply ajsnigrutin 5 hours agorootparentprev> The PDF's work. They won't break. Not just that, PDFs are one of the few formats, where i'm willing to bet my own money, that they'll still work in 10 or 20 years. Even basic html has changed, layouts look different depending on many factors, and even the -ing doesn't work anymore. reply ahardison 16 hours agoparentprevTotally valid concerns. If you have time, I would like to show you my solution to get your thoughts as I believe I have found ways to mitigate all of your concerns. Currently I am using STCC (Schmitt-Thompson Clinical Content). I Have sent you some of the PDF's we use for testing. reply akoboldfrying 18 hours agoparentprevThe author is proposing that the DAG representation be in addition to the PDF: >The organizations drafting guidelines should release them in structured, machine-interpretable formats in addition to the downloadable PDFs. My opinion: Ideally the PDF could be generated from the underlying DAG -- that would give you confidence that everything in the PDF has been captured in the DAG. reply maxerickson 17 hours agorootparentYou could generate the document from the graph and then attach it as data. reply JumpCrisscross 9 hours agorootparent> could generate the document from the graph and then attach it as data Much easier for doctors to draft PDFs than graphs. reply zahlman 16 hours agoparentprevIt would, I imagine, be much easier to generate a PDF from the tool's internal flowchart representation than the other way around. reply pcrh 18 hours agoprevThe fundamental idea here is that doctors find it difficult to ensure that their recommendations are actually up-to-date with the latest clinical research. Further, that by virtue of being at the centre of action in research, doctors in prestige medical centres have an advantage that could be available to all doctors. It's a pretty important point, sometimes referred to as the dissemination of knowledge problem. Currently, this is best approached by publishing systematic reviews according to the Cochrane Criteria [0]. Such reviews are quite labour-intensive and done all too rarely, but are very valuable when done. One aspect of such reviews, when done, is how often they discard published studies for reasons such as bias, incomplete datasets, and so forth. The approach described by Geiger in the link is commendable for its intentions but the outcome will be faced with the same problem that manual systematic reviews face. I wonder if the author considered included rules-based approaches (e.g. Cochrane guidelines) in addition to machine learning approaches? [0] https://training.cochrane.org/handbook reply resource_waste 17 minutes agoparentIt amazes me that AI isnt a borderline requirement for being a doctor. Think of how much info is outdated or just wrong. reply slaucon 17 hours agoparentprevHey author here--Cochrane reviews are great. NCCN guidelines and Cochrane Reviews serve complementary roles in medicine - NCCN provides practical, frequently updated cancer treatment algorithms based on both research and expert consensus, while Cochrane Reviews offer rigorous systematic analyses of research evidence across all medical fields with a stronger focus on randomized controlled trials. The NCCN guidelines tend to be more immediately applicable in clinical practice, while Cochrane Reviews provide a deeper analysis of the underlying evidence quality. My main goal here was to show what you could do with any set of medical guidelines that was properly structured. You can choose any criteria you want. reply liontwist 16 hours agoparentprev> doctors find it difficult to ensure that their recommendations are actually up-to-date with the latest clinical research Doctors care about as much this as software engineers care about the latest computer science research. A few curious ones do. But the general attitude is they already did tough years of school so they don’t have to anymore. reply refurb 15 hours agorootparentI worked with oncologists and this isn’t true. Oncology has a rapidly changing treatment landscape and it’s common for oncologists to be discussing the latest paper that has come out. If you’re an oncologist and not keeping up with the literature you’re going to be out of date in your decisions in about 6 months from graduation. reply liontwist 11 hours agorootparentFunny enough that last paragraph is also said of software engineers too. Neither are true. reply mort96 10 hours agorootparentYeah, non-programmers seem to think everything is changing so quickly all the time yet here I am writing in a 40 year old language against UNIX APIs from the 70s ¯\\_(ツ)_/¯ reply easytigerm 4 hours agoprevThe OP will be pleased to know that they’re not the first person to think of this idea. Searching for “computable clinical guidelines” will unearth a wealth of academic literature on the subject. A reasonable starting point would be this paper [1]. Indeed people have been trying since the 70s, most notably with the famous MYCIN expert system. [2] As people have alluded to and the history of MYCIN shows, there’s a lot more subtlety to the problem than appears on the surface, with a whole bunch of technical, psychological, sociological and economic factors interacting. This is why cancer guidelines are stuck in PDFs. Still, none of that should inhibit exploration. After all, just because previous generations couldn’t solve a problem doesn’t mean that it can’t be solved. [1] https://pmc.ncbi.nlm.nih.gov/articles/PMC10582221/ [2] https://www.forbes.com/sites/gilpress/2020/04/27/12-ai-miles... reply adolph 3 hours agoparentTo the author: The above is a high quality comment with worthy areas to study. Additionally I would draw your attention to NCCN’s “Developer API” which is not interesting technologically but how it reflects the IP landscape. https://www.nccn.org/developer-api reply awinter-py 8 minutes agopreva decision tree is just a csv trapped in amber. share the actual data reply londons_explore 18 hours agoprevDecision trees work for making decisions... But they don't work as well as other decisionmaking techniques... Random forests, linear models, neural nets, etc. are all decision making techniques at their core. And decision trees perform poorly for complex systems where lots of data exists - ie. human health. So why are we using a known-inferior technique simply because it's easier to write down in a PDF file, reason about in a meeting, or explain to someone? Shouldn't we be using the most advanced mathematical models possible with the highest 'cure' probability, even if they're so complex no human can understand them? reply epcoa 17 hours agoparent> complex systems where lots of data exists Not a lot of high quality data exists for human health. Clinical guidelines for many diseases are built around surprisingly scant evidence many times. > even if they're so complex no human can understand them? That’ll be wonderful to explain in court when they figure out it was just data smuggling or whatever other bias. reply epistasis 15 hours agorootparentIn cancer there's an abundance of clinical trials with high quality data, but it is all very complex in terms of encoding what the clinical trial actually encoded. Go to a clinical cancer conference and you will see the grim reality of 10,000s of people contributing to the knowledge discovery process with their cancer care. There is an inverse relationship between the number of people in a trial and the amount of risk that goes into that trial, but it is still a massive amount of data that needs to be codified into some sensible system, and it's hard enough for a person to do it. > That’ll be wonderful to explain in court when they figure out it was just data smuggling or whatever other bias. What do you mean by this? I'm not aware of any data smuggling that has ever happened in a clinical trial. The \"bias\" is that any research hypothesis comes from the fundamentally biased position of \"I think the data is telling me this\" but I've seen very little bias of truly bad hypotheses in cancer research like those that have dominated, say Alzheimer's research. Any research malfeasance should be prosecuted to the fullest, but I don't think cancer research has much of it. This was a huge scandal, but I don't think it pointed to much in the way of bad research in the end: https://www.propublica.org/article/doctor-jose-baselga-cance... reply epcoa 12 hours agorootparentBy smuggling and bias I meant in an ML model. Smuggling was a bit informal, but referring to models overfit on unintended features or artifacts. reply londons_explore 6 hours agorootparentbut we have well established ways to deal with those... test/validation sets, n-fold validation, etc. Even if there was some overfitting or data contamination that was undetected, the result would most probably still be better than a hand-made decision tree over the same data... reply epcoa 1 hour agorootparentOk, until you can sue the AI you need to find a doctor ok putting their license behind saying “I have no idea how this shiny thing works”. There are indeed some that will, but not a consensus. reply wizzwizz4 4 hours agorootparentprevHand-made decision trees are open to inspection, comprehension, and adaption. There is no way to adapt an opaque ML model to new findings / an experimental treatment except by producing a new model. reply s1artibartfast 17 hours agoparentprevDinner generation is usually based on decision tree models as well, so they match the resolution of the available data. The practice of real world medicine often interpolates between these data points. reply wizzwizz4 17 hours agoparentprevModels too complex for humans to understand don't, in practice, have a high 'cure' probability. reply troysk 9 hours agoprevI find the web(HTML/CSS) the most open format for sharing. PDFs are hard to be consumed on smaller devices and much harder to be read by machines. I am working on a feature at Jaunt.com to convert PDFs to HTML. It shows up as reader mode icon. Please try it out and see if it is good enough. I personally think we need to do much better job. https://jaunt.com reply ErigmolCt 9 hours agoparentPDFs can be notoriously difficult to work with on smaller devices reply queuebert 1 hour agoprevAs a cancer researcher myself, I'd point out that some branches of the decision trees in the NCCN guidelines are based on studies in which multiple options were not statistically significantly different, but all were better than the placebo. In those cases, the clinician is free to use other factors to decide which arm to take. A classic example of this is surgery vs radiation for prostate cancer. Both are roughly equally effective, but very different experiences. reply grumbel 11 hours agoprevSame reason why datasheets are still PDFs. It's a reliable, long lasting and portable format. And while it's kind of ridiculous that we are basically emulating paper, no other format fills that niche. It's the niche HTML should be able to fill, since that was its original purpose, but isn't, since all focus over the last 20 or so years has been on everything else, but making HTML a better format for information exchange. Trivial things like bundling up a complex HTML document into a single file don't have standard solutions. Cookies stop working when you are dealing with file:// URLs and a lot of other really basic stuff just doesn't work or doesn't exist. Instead you get offshot formats like ePUB that are mostly HTML, but not actually supported by most browser. reply epistasis 15 hours agoprev> With properly structured data, machines should be able to interpret the guidelines. Charting systems could automatically suggesting diagnostic tests for a patient. Alarm bells and \"Are you sure?\" modals could pop up when a course of treatment diverges from the guidelines. And when a doctor needs to review the guidelines, there should be a much faster and more natural way than finding PDFs I have implemented this computerized process twice at two different startups over the past decade. I would not want the NCCN to do it. The NCCN guidelines are not stuck in PDFs, they are stuck in the heads of doctors. Once the NCCN guidelines get put into computerized rules, they start to be guided by those computerized rules, a second influence that takes them away from the fundamental science. So while I totally agree that there should be systemtticization of the rules, it should be entirely secondary and subservient to the best frontier knowledge about cancer, which changes extremely frequently. Annually after every ASCO (major pan-cancer conference) and every disease specific conference (e.g. the San Antonio breast cancer conference), and occasionally during the year when landmark clinical trials are published the doctors need to update their knowledge from the latest trials and their continuing medical education, which is entire body of knowledge that is complementary to the edges of what the NCCN publishes. Having spanned both computer science and medicine for my entire career, I trust doctors to be able to update their rules far faster than the programmers and databases. Please do not get the NCCN guidelines stuck in spaghetti code that a few programmers understand, rather than open in PDFs with lots of links that anybody can go and chase after. Edit: though give me a week digesting this article and I may change my mind. Maybe the NCCN should be standardizing clinical variables enough such that the rules can trivially be turned into rules. That would require that the hypotheses that a clinical trial fits into those rules however, and that's why I need a week of digestion to see if it may even be possible... reply ok654321 39 minutes agoprevWhat is people's fucking issue with PDFs that there's complaining? reply ramoz 5 hours agoprevCool tool. From my experience the PDF was easy to traverse. The hardest part for me was understanding that treatment options could differ (i.e. between the _top_ hospitals treating the cancer). And there were a few critical options to consider. NCCN paths were traditional, but there is in between decisions to make or alternative paths. ChatGPT was really helpful in that period. \"2nd\" opinions are important... but again you ask the top 2 hospitals and they differ in opinion, any other hospital is typically in one of those camps. reply upghost 18 hours agoprevIt's so much worse than you could possibly imagine. I worked for a healthcare startup working on patient enrollment for clinical oncology trials. The challenges are amazing. Quite frankly it wouldn't matter if the data were in plaintext. The diagnostic codes vary between providers, the semantic understanding of the diagnostic information has different meanings between providers, electronic health records are a mess, things are written entirely in natural language rather than some kind of data structure. Anyone who's worked in healthcare software can tell you way more horror stories. I do hope that LLMs can help straighten some of it out but anyone whos done healthcare software, the problems are not technical, they are quite human. That being said one bright spot is we've (my colleagues, not me) made a huge step forward using category theory and Prolog to discover the provably optimal 3+3 clinical oncology dose escalation trial protocol[1]. David gave a great presentation on it at the Scryer Prolog meetup[2] in Vienna. It's kind of amazing how in the dark ages we are with medicine. Even though this is the first EXECUTABLE/PROGRAMMABLE SPEC for a 3+3 cancer trial, he is still fighting to convince his medical colleagues and hospital administrators that this is the optimal trial because -- surprise -- they don't speak software (or statistics). [1]: https://arxiv.org/abs/2402.08334 [2]: https://www.digitalaustria.gv.at/eng/insights/Digital-Austri... reply sebmellen 17 hours agoparentHave you read Jake Seliger’s pieces on oncology clinical trials https://jakeseliger.com/. reply upghost 16 hours agorootparentOh wow. No, that's heart breaking. I'll have to read up on this. Reminds me of David explaining the interesting and somewhat surprisingly insensitive language the oncology literature uses towards folks going through this. Its there for historical reasons but slow to change. It also shows how important getting dose escalation trials are. The whole point is finding the balance point where \"cure is NOT worse than the disease\". A bad dose can be worse than the cancer itself, and conducting the trials correctly is extremely important... and this really underscores the human cost. Truly heartbreaking :( reply slaucon 17 hours agoparentprevThis is a fascinating idea! reply gmueckl 3 hours agoprevSoftware that gives treatment instructions may be a medical device requiring FDA approval. You may be breaking the law if you give it to a medical professional without such approval. reply whiterock 7 hours agoprevWhy can this not just be a website? Isn‘t this a perfect use case for HTML and hyperlinks? reply gcanyon 3 hours agoprevThe real question is: why is everything stuck in PDFs, and the more important meta-question is: why don't PDFs support meta-data (they do, somewhat). So much of what we do is essentially machine-to-machine, but trapped in a format designed entirely for human-to-human (also lump in a bit of machine-to-human). Adobe has had literally a third of a century to recognize this need and address it. I don't think they're paying attention :-/ reply layer8 3 hours agoparentPDFs can have arbitrary files embedded, like XML and JSON. It also supports a logical structure tree (which doesn’t need to correspond to the visual structure) which can carry arbitrary attributes (data) on its structure elements. And then there’s XML Forms. You can really have pretty much anything machine-processable you want in a PDF. One could argue that it is too flexible, because any design you can come up with that uses those features for a particular application is unlikely to be very interoperable. reply queuebert 1 hour agoparentprevPDFs are essentially compressed Postscript, which is Turing complete, so a PDF in theory can do anything you want. reply osmano807 18 hours agoprevI know it's not the same, but in many areas we have this \"follow the arrows\" system in many guidelines. For some examples, see the EULAR guidelines with it's fluxograms for treatments and also AO Surgery Reference with a graphical approach to select treatments based on fracture pattern, avaliable materials and skill set. I think that's a logical and necessary step to join medical reasoning and computer helpers, we need easier access to new information and more importantly to present clinical relevant facts from the literature in a way that helps actual patient care decision making. I'm just not too sure we can have generic approaches to all specialties, but it’s nice seeing efforts in this area. reply LorenPechtel 18 hours agoprevThe real problem is that the guidelines are written for humans in the first place. Workarounds like this shouldn't be needed, to go from a machine friendly layout to a human friendly one is usually quite easy. And from what he says a decision tree isn't really the right model in the first place. What about no tree, just a heap of records in a SQL database. You do a query on the known parameters, if the response comes back with only one item in the treatment column you follow it. If it comes back with multiple items you look at what would be needed to distinguish them and do the test(s). reply mav3ri3k 6 hours agoprevExcellent read. This consolidated and catalyzed my my spurious thoughts around personal information management. The input is generally markdown/pdf but over time highly useless for a single person. Thete would be value if it is passed through such a system over time. reply guipsp 3 hours agoprevI have to ask: did the author contact any medical professional when writing this article? Is this really something that needs to be fixed, and will his solution actually fix it? It seems to me that ignoring the guideline is a physician decision, and when it is ignored (for good or for bad), it is not because the guidelines are not available in json. reply a1o 18 hours agoprevI parsed some mind maps that were constructed with a tool and exported as pdfs (original sources were lost a long time ago) and I used python with tesseract for the text and opencv and it worked alright. I am curious why the author went with LLMs, but I guess with the mentioned amount of data it wasn't hard to recheck everything later. reply gibsonf1 3 hours agoprevThe idea of adding hallucination to medical advice seems very dangerous. reply schu 10 hours agoprevWould love to take a look at the code, in particular at how the data extraction and transformation is implemented. As a side note, the German associations of oncology publish their guidelines here (HTML and SVG graphs): https://www.onkopedia.com/de/onkopedia/guidelines reply breytex 8 hours agoprevShouldn't the end goal be just to train an ai on all the pdfs and give the doctors an interface to plug in all the details and get a treatment plan generated by that ai? Working on the data structure feels like an intermediate solution on the way to that ai which is not really necessary. Or am I missing something? reply prmoustache 8 hours agoparentI am not sure patients and doctors are interested in adding hallucination generators to the list of their problems. reply fl0id 8 hours agoparentprevYour end goal maybe. Not patients or doctors goal for sure. reply pjc50 8 hours agoparentprevHow does your treatment AI get its liability insurance? reply inopinatus 18 hours agoprev> The whole set of guidelines for a type of cancer breaks down into a few disjointed directed graphs Nothing undermines medicine quite so thoroughly as yet another astronaut trying to force it into a data structure. reply prepend 18 hours agoparentComically, I worked in this space and initially tried to get decision support working with data structures and code sets and such. I ended up only really contributed adding version numbers to the pdf. So at least people knew they had the latest and same versions. And that took a year, to get versions added to guideline pdfs. reply johnisgood 17 hours agorootparentThat is wild, one would think versioning is extremely important. They tend to just put the timestamp in the filename (sometimes), which I guess is better than nothing. Don't signed PDFs include a timestamp, however? reply prepend 6 hours agorootparentGetting in the file name was kind of easy. But I meant adding it visually in the pdf guidance so readers could tell. Just numbers in the lower left corner. Or maybe right. The guideline was available via url so the filename couldn’t change. reply xh-dude 13 hours agoprevThe author makes a great case for machine-interpretable standards but there is an enormous amount of work out there devoted to this, it’s been a topic of interest for decades. There’s so much in the field that a real problem is figuring out what solutions match the requirements of the various stakeholders, more than identifying the opportunities. reply rmrfchik 10 hours agoprevBecause writers don't think about readers. PDF is one of the worst formats for science/technical info, but yet. I've dumped a lot of papers from arxiv because it formatted as 2-column non zoomable PDF. reply noonanibus 17 hours agoprevForgive me if I'm mistaken, but isn't this exactly what the FHIR standard is meant to address? Not only does it enable global inter-health communication using a standardized resource, but it's already adopted in several national health services, including (but not broadly), America. Is this not simply a reimplementation, but without the broad iterations of HL7? reply nradov 15 hours agoparentRight, it would make more sense to use HL7 FHIR (possibly along with CQL) as a starting point instead of reinventing the wheel. Talk to the CodeX accelerator about writing an Implementation Guide in this area. The PlanDefinition resource type should be a good fit for modeling cancer guidelines. https://codex.hl7.org/ https://www.hl7.org/fhir/plandefinition.html reply joshuakelly 13 hours agorootparentThis is the comment I was looking for. You would aim to use CQL expressions inside of a PlanDefinition, in my estimate. This is exactly what AHRQ's, part of HHS, CDS Connect project aims to create / has created. They publish freely accessible computable decision support artifacts here: https://cds.ahrq.gov/cdsconnect/repository When they are fully computable, they are FHIR PlanDefinitions (+ other resources like Questionnaire, etc) and CQL. Here's an example of a fully executable Alcohol Use Disorder Identification Test: https://cds.ahrq.gov/cdsconnect/artifact/alcohol-screening-u... There's so much other infrastructure around the EHR here to understand (and take advantage of). I think there's a big opportunity in proving that multimodal LLM can reliably generate these artifacts from other sources. It's not the LLM actually being a decision support tool itself (though that may well be promising), but rather the ability to generate standardized CDS artifacts in a highly scalable, repeatable way. Happy to talk to anyone about any of these ideas - I started exactly where OP was. reply osmano807 3 hours agorootparentI downloaded and opened an CDS for osteoporosis from the link (as a disease in my specialty), I need an API key to view what a \"valueset\" entails, so in practice I couldn't assert if the recommendation aligns with clinical practice, nor in the CQL provided have any scientific references (even a textbook or a weak recommendation from a guideline would be sufficient, I don't think the algorithm should be the primary source of the knowledge) I tried to see if HL7 was approachable for small teams, I personally became exhausted from reading it and trying to think how to implement a subset of it, I know it's \"standard\" but all this is kinda unapproachable. reply tdeck 17 hours agoprevGraphViz has some useful graph schema languages that could be reused for something like this. There's DOT, a delightful DSL, and some kind of JSON format as well. You can then generate a bunch of different output formats and it will lay out the nodes for you. reply epistasis 15 hours agoparentOf all the challenges with this, graph layout is beyond trivial. It does not rank as a problem, intellectual challenge, or even that interesting. The challenges are all about what goes in the nodes, how to define it, how to standardize it across different institutions, how to compare it to what was tested in two different clinical trials, etc. And if the computerized process goes into clinical practice, how is that node and its contents robustly defined so that a clinician sitting with a patient can instantly understand what is meant by it's yes/no/multiple choice question in terms that have been used in recent years at the clinician's conferences. Addressing the challenges of constructing the graph requires deep understanding of the terms, deep knowledge of how 10 different people from different cultural backgrounds and training locations interpret highly technical terms with evolving meanings, and deep knowledge of how people could misunderstand language or logic. These guidelines codify evolving scientific knowledge where new conceptions of the disease get invented at every conference. It's all at the edge of science where every month and year we have new technology to understand more than we ever understood before, and we have new clinical trials that are testing new hypotheses at the edge of it. Getting a nice visual layout is necessary, but in no way sufficient for what needs to be done to put this into practice. reply graphviz 5 hours agorootparentNot ... even that interesting? reply graphviz 4 hours agorootparentModularity is an excellent way of attacking complex problems. We can all play with algorithms that can carry on realistic conversations and create synthetic 3D movies, because people worked on problems like making transistors the size of 10 atoms, figuring out how processors can predict branches with 99% accuracy, giving neural nets self-attention, deploying inexpensive and ridiculously fast networks all over the planet, and a lot of other stuff. For many of us, curing cancer may someday become more important than almost anything else a computer can help us to do. It's just there are so many building blocks to solving truly complex problems; we must respect all that. reply joshz404 13 hours agoprevYou might be interested in checking out the WHO SMART Guidelines. Nothing on cancer yet AFAIK, but it's evolving. reply rukshn 12 hours agoparentI was also thinking about FHIR and SMART guidelines. But the whole system is mess. And the whole SMART guideline system is controlled by 2-3 gatekeepers who don’t listen to any ideas other than their own reply hashishen 5 hours agoprevFunny i just had the thought the other day about how we as a society need to move past the pdf format or even just update it to be editable in traditional document software. The fact that Google docs will export as a pdf and not have it saved in the documents is proof its gotten to a point of inefficiency and that's just one example reply bsder 14 hours agoprevGee, before talking about complex stuff like decision trees, how about we start with something really simple like not requiring a login to download the stupid PDF from NCCN? reply dogmatism 16 hours agoprevThis is all predicated on the guidelines actually reflecting best practices reply jdlyga 17 hours agoprevPDFs are a universal, machine readable format. reply GeneralMayhem 15 hours agoparentPDFs are the opposite of machine-readable if you want to do anything other than render them as images on paper or a screen. They're only slightly more machine-readable than binary executables. I hate, hate, hate, hate, hate the practice of using PDFs as a system of record. They are intended to be a print format for ensuring consistent typesetting and formatting. For that, I have no quarrel. But so much of the world economy is based on taking text, docx (XML), spreadsheets, or even CSV files, rendering them out as PDFs, and then emailing them around or storing them in databases. They've gone from being simply a view layer to infecting the model layer. PDFs are a step better than passing around screenshots of text as images - when they don't literally consist of a single image, that is. But even for reasonably-well-behaved, mostly-text PDFs, finding things like \"headers\" and \"sections\" in the average case is dependent on a huge pile of heuristics about spacing and font size conventions. None of that semantic structure exists, it's just individual characters with X-Y coordinates. (My favorite thing to do with people starting to work with PDFs is to tell them that the files don't usually contain any whitespace characters, and then watch the horror slowly dawn as they contemplate the implications.) (And yes, I know that PDF/A theoretically exists, but it's not reliably used, and certainly won't exist on any file produced more than a couple years ago.) Now, with multi-modal LLMs and OCR reaching near-human levels, we can finally... attempt to infer structured data back out from them. So many megawatt-hours wasted in undoing what was just done. Structure to unstructure to structure again. Why, why, why. As for universality... I mean, sure, they're better than some proprietary format that can only be decrypted or parsed by one old rickety piece of software that has to run in Win95 compatibility mode. But they're not better than JSON or XML if the source of truth is structured, and they're not better than Markdown or - again - XML if the source is mostly text. And there are always warts that aren't fully supported depending on your viewer. reply sswatson 16 hours agoparentprevThey’re only machine-readable in the very weak sense that all computer files are machine-readable. reply fasa99 3 hours agoprevWAIT ... Hole up... what have we here: https://www.nccn.org/compendia-templates/compendia/nccn-comp... TLDR: The NCCN surely has a clean pretty database of these algorithms. They output these junky pdfs for free. Want cleaner \"templates\" data? Pay the toll please. What we have here is a walled garden. Want the treatment algorithm? Here muck through this huge disaster of 999 page pdfs. Oh you want the underlying data? Well, well, it's going to cost you. What we have here is not so much different than the paywalls of an academic journal. Some company running a core service to an altruistic industry and skimming a price. OP is just writing an algorithm to unskim it. And nobody can really use it without making the thing bulletproof lest a physician mistreat a cancer. To my sentiment this is yet another unethical topic in healthcare. These clunky algorithms, if a physician uses them, slows the process and introduces a potential source of error, ultimately harming patients. Harming patients for increased revenue. The physicians writing and maintaining the guidelines look the other way given they get a paycheck off it, plus the prestige of it all, similar to some scenarios in medicine itself. The natural thing to do is crack open the database and let algorithms utilize it. This whole thing of dumping data in an obstruse and machine-challenging format, then a rube goldberg machine to reverse the transformation, it's not right. Anyway I mention this because there seems to be a thought of \"these pdfs are messy lets clean them\" without looking at what's really going on here. reply hulitu 12 hours agoprev [–] > With properly structured data, machines should be able to interpret the guidelines. Yeah, right. And then say \"Die\". /s The guidelines shall be structured properly. It is not rocket science. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Cancer guidelines are frequently stored in PDF format due to their durability and portability, despite the limitations in usability and integration with medical systems.- Transitioning to structured data could improve usability, but challenges include the complexity of medical information, lack of standardization, and regulatory hurdles.- The debate centers on balancing the reliability of PDFs with the potential advantages of dynamic, machine-interpretable data solutions, considering the need for consistent updates and error management in automated systems."
    ],
    "points": 229,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1734996997
  },
  {
    "id": 42497173,
    "title": "Complete decompilation of Lego Island",
    "originLink": "https://github.com/isledecomp/isle",
    "originBody": "LEGO Island Decompilation Development VlogContributingMatrixForumsPatreon This is a functionally complete decompilation of LEGO Island (Version 1.1, English). It aims to be as accurate as possible, matching the recompiled instructions to the original machine code as much as possible. The goal is to provide a workable codebase that can be modified, improved, and ported to other platforms later on. Status Both ISLE.EXE and LEGO1.DLL are completely decompiled and, to the best of our knowledge, are functionally identical to the originals. However, work is still ongoing to improve the accuracy, naming, documentation, and structure of the source code. While there may still be unresolved bugs that are not present in retail, the game should be fully playable with the binaries derived from this source code. Due to various complexities with regard to the compiler, these binaries are not a byte-for-byte match of the original executables. We remain hopeful that this can be resolved at some point. Building This project uses the CMake build system, which allows for a high degree of versatility regarding compilers and development environments. For the most accurate results, Microsoft Visual C++ 4.20 (the same compiler used to build the original game) is recommended. Since we're trying to match the output of this code to the original executables as closely as possible, all contributions will be graded with the output of this compiler. These instructions will outline how to compile this repository using Visual C++ 4.2 into highly-accurate binaries where the majority of functions are instruction-matching with retail. If you wish, you can try using other compilers, but this is at your own risk and won't be covered in this guide. Prerequisites You will need the following software installed: Microsoft Visual C++ 4.2. This can be found on many abandonware sites, but the installer can be a little iffy on modern versions of Windows. For convenience, a portable version is available that can be downloaded and used quickly instead. CMake. A copy is often included with the \"Desktop development with C++\" workload in newer versions of Visual Studio; however, it can also be installed as a standalone app. Compiling Open a Command Prompt (cmd). From Visual C++ 4.2, run BIN/VCVARS32.BAT x86 to populate the path and other environment variables for compiling with MSVC. Make a folder for compiled objects to go, such as a build folder inside the source repository (the folder you cloned/downloaded to). In your Command Prompt, cd to the build folder. Configure the project with CMake by running: cmake-G \"NMake Makefiles\" -DCMAKE_BUILD_TYPE=RelWithDebInfo Visual C++ 4.2 has issues with paths containing spaces. If you get configure or build errors, make sure neither CMake, the repository, nor Visual C++ 4.2 is in a path that contains spaces. Replacewith the source repository. This can be .. if your build folder is inside the source repository. RelWithDebInfo is recommended because it will produce debug symbols useful for further decompilation work. However, you can change this to Release if you don't need them. Debug builds are not recommended because they are unlikely to be compatible with the retail LEGO1.DLL, which is currently the only way to use this decompilation for gameplay. NMake Makefiles is most recommended because it will be immediately compatible with Visual C++ 4.2. For faster builds, you can use Ninja (if you have it installed), however due to limitations in Visual C++ 4.2, you can only build Release builds this way (debug symbols cannot be generated with Ninja). Build the project by running nmake or cmake --buildWhen this is done, there should be a recompiled ISLE.EXE and LEGO1.DLL in the build folder. Note that nmake must be run twice under certain conditions, so it is advisable to always (re-)compile using nmake && nmake. If you have a CMake-compatible IDE, it should be pretty straightforward to use this repository, as long as you can use VCVARS32.BAT and set the generator to NMake Makefiles. Usage Simply place the compiled ISLE.EXE and LEGO1.DLL into LEGO Island's install folder (usually C:\\Program Files\\LEGO Island or C:\\Program Files (x86)\\LEGO Island). Alternatively, LEGO Island can run from any directory as long as ISLE.EXE and LEGO1.DLL are in the same directory, and the registry keys (usually HKEY_LOCAL_MACHINE\\Software\\Mindscape\\LEGO Island or HKEY_LOCAL_MACHINE\\Software\\Wow6432Node\\Mindscape\\LEGO Island) point to the correct location for the asset files. Contributing If you're interested in helping or contributing to this project, check out the CONTRIBUTING page. Additional Information Which version of LEGO Island do I have? Right click on LEGO1.DLL, select Properties, and switch to the Details tab. Under Version you should either see 1.0.0.0 (1.0) or 1.1.0.0 (1.1). Additionally, you can look at the game disc files; 1.0's files will all say August 8, 1997, and 1.1's files will all say September 8, 1997. Version 1.1 is by far the most common, especially if you're not using the English or Japanese versions, so that's most likely the version you have. Please note that some localized versions of LEGO Island were recompiled with small changes despite maintaining a version number parallel with other versions; this decompilation specifically targets the English release of version 1.1 of LEGO Island. You can verify you have the correct version using the checksums below: ISLE.EXE md5: f6da12249e03eed1c74810cd23beb9f5 LEGO1.DLL md5: 4e2f6d969ea2ef8655ba3fc221a0c8fe CONFIG.EXE md5: 92d958a64a273662c591c88b09100f4a",
    "commentLink": "https://news.ycombinator.com/item?id=42497173",
    "commentBody": "Complete decompilation of Lego Island (github.com/isledecomp)216 points by foxtacles 22 hours agohidepastfavorite30 comments indigo945 21 hours agoThis project goes back to the LEGO Island Rebuilder [1] by (some of?) the same authors, which fixes several bugs in the original game release by patching it in memory (iirc). These fixes include some involved ones, like for the wonky framerate-dependent controls. MattKC, who developed much of this original work, has a nice Youtube channel full of video postmortems for some of these [2]. It's kind of fun just to watch him poke around with a hex editor, unraveling the arcane mysteries of a long-sunken civilization of Win95 developers. [1]: https://github.com/isledecomp/LEGOIslandRebuilder [2]: https://www.youtube.com/@MattKC reply jeroenhd 19 hours agoparentI've learned of MattKC through a video of his about how he ported .NET to Windows 95 (mostly). I love the dedication content like this shows off. In an age of ever decreasing attention spans, it's nice to see someone going through the grunt work for something other than pure financial gain. reply ddtaylor 18 hours agoparentprevMattKC is a good YouTuber in my opinion. His videos are simple, fun and extremely information dense. I haven't had time but I purchased a Wii-U remote at a garage sale because it had all the pieces and was paired together. Days later I see MattKC on stream hacking the Bluetooth! reply bri3d 21 hours agoprevThe tooling and infrastructure in this project are pretty interesting as these things go. It's always cool to see how each decompilation project springs up with different ideas and goals - this one seems very focused on 1:1 accuracy, with a side-project for compatibility / cross-platform reimplementation: * https://github.com/isledecomp/reccmp is a lint tool which compares compiled function reimplementations with the original binary and produces an automated report detailing the instruction level accuracy of the re-implementation, while dealing with all of the fun of C++. * https://github.com/isledecomp/SIEdit is a resource editor for the bizarre RIFF-esque resource streaming format the original developer (Mindscape) seems to have invented. Also while we're on the subject of vintage LEGO games, I've recently been quite into playing Manic Miners, a complete Unreal Engine remake (not decompilation/reimplementation, an actual ground-up recreation!) of Rock Raiders. I'm hoping someone does Alpha Team next; it was a quite fun puzzle game but incredibly buggy. reply Belphemur 6 hours agoparentFor those looking for the game, it's on itch.io : https://baraklava.itch.io/manic-miners reply mileycyrusXOXO 18 hours agoparentprevI’m gonna have to get Manic Miners, lots of fond memories playing Rock Raiders with my friends reply mclau156 21 hours agoparentprevI am seeing more reason to re-make in Unreal, Unity, Godot, Blender lately, these softwares are becoming increasingly more beginner friendly and downloading 3D assets and programming 3D skeleton animations are becoming easier reply stravant 20 hours agoprevI did a few thousand lines of this. In particular it was interesting learning about D3D retained mode as I did that part. What a weird piece of rendering history. Worth a search if you haven't heard about it before: D3DRM. reply pjc50 7 hours agoparentThe top search hit for \"d3d retained mode\" is now https://www.legoisland.org/wiki/Direct3D_Retained_Mode , going full circle. reply teeray 20 hours agoprevYou can build a mountain, if you do it brick by brick... reply datadrivenangel 20 hours agoparentPapa told mama and laura told nick... reply mastercheif 16 hours agoprevThank you to everyone who worked on this. One of my favorite games growing up, I’m glad to know it’ll be around to show my kid. reply mdtrooper 12 hours agoprevI love these kind of things, for years I want to learn decompile old games....but equal other things I do not know what it is the first steps or tools. reply jonhohle 3 hours agoparentIf you’re interested, I’ve been decompiling Castlevania: Symphony of the Night live Monday through Thursday at 11am pacific time on twitch for several months - https://www.twitch.tv/madeupofwires I’m happy to talk about the tools and process or anything anyone else in chat wants to know about. I have about 10kloc contributed and worked on tooling and build, but still have a lot to learn myself. reply taspeotis 14 hours agoprevI had aspirations to decompile another MSVC 4.2 game (FireFight) and I got stymied on CMake - among other things. This repo looks like a good reference. reply antics9 18 hours agoprevThe game looks like Roblox and is just as creepy too: https://youtu.be/xyqXZDyR-RA reply stavros 18 hours agoparentWait, why is Roblox creepy? reply treve 13 hours agorootparentIt's been called a casino for kids. There's decent reporting around this. I wouldn't let my kid near this, or anything that has a game currency tied to a real-world currency. reply madjam002 1 hour agorootparentSuch a shame it’s like this now because I played it extensively as a kid back in 06 and 07 and it helped inspire me to learn programming. reply stavros 10 hours agorootparentprevAhh interesting, thank you! reply homarp 2 hours agorootparentprevsee https://news.ycombinator.com/item?id=28247034 and https://news.ycombinator.com/item?id=32014754 reply stavros 2 hours agorootparentThank you! reply voidfunc 19 hours agoprev [–] I liked this game when I was a kid but I remember being massively disappointed that there wasn't more building. reply codetrotter 18 hours agoparentMy two favorite games for some time were Lego Island and Lego Loco. Lego Loco is a city builder and railroad builder game. Something like a more basic SimCity and Railroad Tycoon crossover maybe. I really liked Lego Loco because you can build a whole city out of Lego to your own liking. So I had Lego building in Lego Loco and I had Lego Island with all the fun stories and things you could do there, like chasing Pepper the criminal with a helicopter and using donuts and pizzas to help the police on ground and the skateboarder. reply robotnikman 16 hours agorootparentI loved Lego Loco. You could even set the game as a screensaver, and it would basically play whatever map you were on whenever your screensaver turned on! I kind of miss screensavers actually reply pesus 17 hours agorootparentprevWow, I've seen anyone mention Lego Loco before! That was a great game. I was horrible at it, but the art style and atmosphere was great. reply AstroJetson 15 hours agorootparentprevLego Loco was cool in there was a network version that let you send your train to other people. reply Dalewyn 16 hours agorootparentprevBrickster's the criminal, Pepper chased him with a rebuilt police helicoptor lobbing pizzas and donuts to help Nick and Laura on the ground catch Brickster. Also, second you on LEGO Loco. Of the original trilogy (Creator, Loco, and Chess), Loco by far was the most fun. Kudos to the Chess King though, saying the Knights are BMX bikeriders still hasn't been surpassed. reply nemothekid 17 hours agoparentprev [–] This was a favorite of mine as a kid as well - I remember revisiting it after seeing a YouTube video of someone doing a technical breakdown. I realized that this game had maybe less than half an hour of content! I remember losing hours to this game. reply bombcar 16 hours agorootparent [–] What we didn’t have in content we made up in replayability! So many “Great games” from that era had a sandbox mode or other replayability. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The project involves a complete decompilation of LEGO Island (Version 1.1, English), aiming to replicate the original machine code as closely as possible.",
      "It utilizes CMake for building, with Microsoft Visual C++ 4.20 recommended for optimal results, and involves setting up the environment, configuring with CMake, and building with nmake.",
      "Contributions are encouraged, focusing specifically on the English release of version 1.1, with ongoing improvements to the decompiled ISLE.EXE and LEGO1.DLL files."
    ],
    "commentSummary": [
      "A GitHub project has successfully decompiled the classic game Lego Island, enhancing previous efforts to fix bugs in the original game.- MattKC, a YouTuber recognized for technical content, played a significant role in this decompilation project, which emphasizes accuracy and includes tools like a lint tool and a resource editor.- The project has ignited interest in remaking vintage LEGO games, with fans expressing nostalgia for titles such as Rock Raiders and Lego Loco, highlighting the community's appreciation for preserving and improving these classic games."
    ],
    "points": 216,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1734984217
  },
  {
    "id": 42500482,
    "title": "Intel shareholders file case asking ex CEO, CFO to return 3 years of salary",
    "originLink": "https://www.cfodive.com/news/intel-shareholders-yank-exceo-cfo-compensation-foundry/736193/",
    "originBody": "An article from Dive Brief Intel shareholders look to yank back ex-CEO, CFO compensation CFO and co-interim CEO David Zinsner, along with the company’s former CEO, misled shareholders about the financial performance of Intel’s foundry unit, shareholders allege. Published Dec. 20, 2024 Grace Noto Editor post share post print email license A sign is posted in front of Intel headquarters on August 01, 2024 in Santa Clara, California. Justin Sullivan via Getty Images Dive Brief: Intel Corporation shareholders are asking for the disgorgement of “all profits, benefits, and other compensation” obtained by ex-CEO Pat Gelsinger, CFO and current co-interim CFO David Zinsner and other company leadership, arguing the leaders breached their fiduciary and contractual duties, according to a shareholder derivative lawsuit filed Tuesday. Filed in the United States District Court of the Northern District of California, the suit by shareholder LR Trust on behalf of Intel alleges that both Gelsinger and Zinsner breached their fiduciary duties as officers of the company by issuing misleading disclosures and failing to accurately report financials related to the company’s foundry business. Gelsinger and Zinsner, as well as other named defendants, which include both current and past members of the company’s board, “exposed the Company to significant liability under various federal securities laws by their misconduct,” according to the suit. “As a result of the individual defendants’ breaches of fiduciary duty and other misconduct, Intel has sustained substantial damages and irreparable injury to its reputation,” the suit says, noting that the officers received “unjust enrichment” stemming from their misconduct. Dive Insight: The suit coincides with efforts by the chipmaker to regain the trust of its shareholders after it failed to execute a turnaround plan spearheaded by Gelsinger. A 40-year veteran of the Santa Clara, California-based company, Gelsinger abruptly resigned from his position as CEO and a member of the board effective Dec. 1 after the company reported a record quarterly loss of $16.6 billion for its third quarter, with losses related to the turnaround efforts, CFO Dive previously reported. The company subsequently appointed Zinsner and Intel Products CEO Michelle Johnston Holthaus as co-interim CEOs, with Zinsner continuing to serve as CFO, as it continues to move forward with its restructuring efforts, targeting $10 billion in cost savings. The restructuring, which also includes wide-scale layoffs throughout the business, is also widely focused on the company’s foundry business — a key element of the shareholder derivative suit. Gelsinger’s turnaround plan included a shift in Intel’s foundry strategy, with the ex-CEO looking to spin off the unit into its own independent business with the goal of allowing Intel foundry to produce chips for its competitors, CFO Dive previously reported. However, Gelsinger, Zinsner and other company leaders misled shareholders about the financial performance of the foundry unit, the suit alleges. Both officers pointed to the foundry unit as a “significant tailwind” for Intel’s business in various statements and company filings, including during the earnings report for the chipmaker’s full-year 2023 results, according to the suit. However, in a retrospective revision to the company’s financials filed in April, the chipmaker revealed Intel Foundry to be one of its main cost centers — with the division losing $7 billion in 2023, according to the filing with the Securities and Exchange Commission. The recast sent Intel’s shares spiraling down by 9.2% at the time, according to the suit. The news was also followed by a class action suit alleging shareholders were mislead regarding those losses related to its Foundry business, according to a report at the time by The Register. As a result, the chipmaker “has been and will continue to be exposed to significant losses due to the wrongdoing complained of herein, yet the board has not caused the company to take action to recover for the company the damages it has suffered and will continue to suffer thereby,” the December shareholder derivative suit alleges. As well as Zinsner and Gelsinger, the suit named multiple current and former board members as defendants. Other defendants include Lip-Bu Tan, a former member of the board who abruptly stepped down from his position in August due to concerns related to Gelsinger’s turnaround plan, according to a report at the time by Reuters cited by the suit. The semiconductor manufacturer has remained focused on its foundry business following its leadership shift. Intel is still seeking to be a “world-class foundry,” Zinsner said during a conference a few days after his appointment to co-interim CEO. As such, it’s also likely Gelsinger’s permanent successor as CEO will have “some capability” around foundry, he said at the time. Intel declined to comment on the suit. Weiss Law, the attorneys for the plaintiffs, did not immediately respond to requests for comment. purchase licensing rights Filed Under: Financial Reporting, Compliance, Leadership",
    "commentLink": "https://news.ycombinator.com/item?id=42500482",
    "commentBody": "Intel shareholders file case asking ex CEO, CFO to return 3 years of salary (cfodive.com)199 points by YetAnotherNick 10 hours agohidepastfavorite104 comments chollida1 9 hours ago- No serious hedge funds or other activists involved in this, ie no adults involved, check - Fully audited financials not being questioned, check - CEO and CFO constantly acknowledging the Foundary unit wasn't profitable in quarterly updates, check. The former CEO was brought in to to try and keep Intel a fully integrated business that designed and manufactured chips. He was very clear upfront about his plan to not divest the foundry and instead to try and build it back up. It didn't' work, that's not a crime. He left because his plan didn't work. That's what happens to CEO's who try and fail to implement a business plan. This is just an ambulance chaser lawsuit that doesn't look like it will go anywhere at all. If there was any merit at all then the board would also be involved in going after the former CEO and CFO. reply szundi 8 hours agoparentSome investors made a wrong bet and trying to save face in front of their clients - spending even more money of those clients on lawsuits reply bradleyjg 6 hours agorootparentNope, this isn’t being driven by any shareholders. The lawyers want a big payday and found a client of convenience. If they win they’ll ask the count for a percentage of the value of the judgment using an innumerate formula for value. reply skeeter2020 4 hours agorootparentthat's not how lawyers get paid, outside of the ones advertise on TV. They bill massive hours that gets paid off the top, and some of it even if they lose. reply bradleyjg 4 hours agorootparentNot in this kind of case. reply Almondsetat 6 hours agorootparentprevIndividual gains, collectivized losses reply robertlagrant 5 hours agorootparentNo, everyone got paid who was working there. But if it'd worked, the profits would've been taxed. As it is, the losses are kept private. reply coliveira 4 hours agorootparentLosses are always public, these investors will use the loss against gains and therefore will pay less taxes, reducing the amount of government revenue. reply robertlagrant 10 minutes agorootparentYes, the government doesn't get to take your money if you made a loss. That's not the same as what you're implying. The investors lost. reply lupusreal 4 hours agorootparentprevCollectivized or public losses come into play when the government bails out failing companies so that shareholders don't get hosed for a bad investment. Shareholders getting hosed is the opposite of that; private loses. reply MoreMoore 8 hours agoparentprev> It didn't' work, that's not a crime. He left because his plan didn't work. That's what happens to CEO's who try and fail to implement a business plan. We don't know that it didn't work. We also don't know that selling it off would've been significantly better, so it's weird to pretend that keeping the foundry part of Intel was this big failure. Intel is a cargo ship that doesn't turn on a dime. But the board and investors don't have the patience for long-term strategy that could get Intel out of its downward spiral. The CEO leaving is just another symptom of that, and a sign that Intel is just continuing to dig its own grave, damn the consequences. Just look at all the other projects Intel has tried and failed in the past 20ish years. Zero patience for anything that doesn't bring in billions and isn't immediately successful. We're all just waiting for Intel to give up on the Arc GPUs because of it. reply vasco 7 hours agorootparentMaybe you have insider knowledge but if you say you don't know - how do you know what the CEO leaving is a symptom of? (I don't know) It's very easy to say that the CEO was trying for real and everyone else was against him as he is an engineer so it feels good to say it, but is there truth to the fact that it's just lack of patience and \"shortsighted\" thinking by the board, or is that just what people repeat because it sounds good to say? Pat was there from Feb 2021 to Dec 2024. In that time he fired 5k+ people (https://www.warntracker.com/company/intel) and now had plans to fire 15k more. We can also look at earnings during his tenure, I don't need to tell you when he joined because you'll know from looking at this: https://m.macrotrends.net/stocks/charts/INTC/intel/eps-earni... So in my simple view, we have a guy that came in with an ambitious plan to burn many billions, fire a bunch of people and turn the company around. He did the first two but in 3.5 years couldn't do the actual goal, and was just starting to present some kind of phase two with more layoffs and potentially more burning of money. Plus all the million factors people on the outside wouldn't know. So what we know is the plan wasn't working yet at least, but who knows if it ever will. it's only shortsighted if you interrupt a good plan, if you interrupt a bad one you are a visionary. reply adrian_b 6 hours agorootparent> \"but in 3.5 years couldn't do the actual goal\" For a very long time, i.e. for a few years, the actual goal had been announced as planned to be achieved towards the end of 2025, when the CMOS fabrication process Intel 18A should become usable for the mass production of Intel server CPUs and laptop CPUs. If Pat Gelsinger had remained the CEO of Intel, only in 1 year from now it would have become possible to decide whether he has been a good CEO or a bad CEO. Now, after he has been ousted prematurely, nobody will ever be able to say with certainty whether Pat Gelsinger would have succeeded to redress the fate of Intel or not. Since he has become CEO, all the intermediate milestones towards the stated goal have been achieved in time, even if with various problems, which were nevertheless more or less predictable, because in the past Intel has never been able to do better than this when transitioning to new fabrication processes and new product architectures. While he has diminished the earnings for shareholders, that was a very good thing to do, because before him Intel has wasted huge amounts of money by giving them to the shareholders instead of using them wisely to remain a competitive company. Instead of looking at shareholder earnings, you should look at the revenues of the Intel employees. I do not know if this is true, but another poster on this forum has said that Gelsinger has given a raise after becoming CEO, attempting to correct the situation where Intel is well known as a company that provides inadequate compensations to its employees, and he had promised that he will try in the future to improve this. If that is true, it has been something much more valuable than attempting to please the shareholders who have lead the company towards losing its competitivity in the market. reply vasco 5 hours agorootparentI don't think anything you said disagreed with me. I agree giving more time could've resulted in a good outcome. But he didn't do it in the time he was given and they didn't want to give him more time. It's just what it is. My point is that without more information it is hard to know if it'd have worked out. It's very appealing for an engineer as myself to root for an engineer CEO that comes back and turns it around, I know it's a story that everyone would love, that's why when the defence is only that, I suspect that people may just be romanticizing the situation the same way I would / do. I'm also not sure someone saying \"I need X time\" means the board needs to treat this as some holy proclamation. A lot of the CEO job is also convincing the board and the markets that the change is coming and at 3.5 out of 5 years I guess the board still wasn't convinced and the markets certainly don't seem to be pricing in a turnaround. reply robertlagrant 2 hours agorootparent> But he didn't do it in the time he was given Is that true? How long did they agree it would take originally? reply ksec 44 minutes agorootparent5 node in 4 years. And we are just approaching end of 4 soon with 18A coming up. The first 4 node are already delivered on schedule. It remains to be seen where 18A will end up. But it isn't far off. So I have no idea where \"he didn't do it in time\" came from. reply edoceo 7 hours agorootparentprevYep. The difference between persistence and stubbornness is the outcome. reply kllrnohj 6 hours agorootparentprev> but in 3.5 years couldn't do the actual goal ...which means he was allowed to release maybe 1 if even that of Intel's bread and butter, CPUs. They have a lead time of somewhere between 3-5 years. And that's not including fab development, which takes even longer. So was he \"burning billions\" or was he investing billions into what made Intel a dominant force in the first place? We'll probably never know, but 3.5 years isn't anywhere close to long enough in this industry. reply godzillabrennus 5 hours agorootparentInvestment of cash without profits to make those investments is still a burn… Pat still deserved longer. Intel is basically a storied brand that needs to be run like a startup because all of its legacy revenue products/models are dying. reply kllrnohj 49 minutes agorootparent> Investment of cash without profits to make those investments is still a burn… there was, though? That's why net income averaged zero, and cash on hand is flat the last 4 years. He didn't run Intel at a loss. reply rowanG077 7 hours agorootparentprevTBH 3.5 years is a fart in the wind for change in a company the size of Intel. reply vasco 7 hours agorootparentOn the other hand it's around 7% of a regular person's career. I didn't intend the number to have some ulterior motive as I don't know how long it should take, it is what it is. But it was long enough for the board. reply Octoth0rpe 7 hours agorootparentprev> We're all just waiting for Intel to give up on the Arc GPUs because of it. I'm sure all kinds of radical cost cutting is being proposed internally, but I'd be surprised if this was on the line. My thinking is that Arc GPUs are the \"walk before you run\" product that will lead to Intel producing AI accelerators, which is a reasonable bet re: a pathway to profitability. reply MoreMoore 7 hours agorootparentA lot of projects are \"walk before you run\". Didn't stop Intel from killing them and wasting all their time, money and effort. That's Intel's problem. I don't want them to kill Arc. It's what they need long-term. I simply don't believe they have the persistence and long-term vision to keep at it despite lackluster sales and lack of interest. reply bbarnett 7 hours agorootparentprev\"AI accelerators\" are at peak profitability right now. Within a few years, certainly by 2030, chip optimizations and improvements will see Raspberry Pis or your phone doing AI work. I expect we may even aee AI cores on phones, by 2036. reply prionic6 6 hours agorootparentPhones are already getting NPUs / TPUs / Neural Engines / whatever fancy marketing term they can come up with. reply nickpeterson 4 hours agorootparentprevI think software optimizations are going to heavily impact gpu/ai-accelerator demand. It feels like we’re leaving the ‘brute force’ make it work era and entering the ‘make it work on my phone without killing battery life” era. I wouldn’t be shocked if within a few years there just isn’t the need for custom hardware at all. reply dismalaf 6 hours agorootparentprevSome phones have TPUs on them today... reply pantalaimon 8 hours agorootparentprev> Zero patience for anything that doesn't bring in billions and isn't immediately successful. That reminds me a lot of Google reply api 7 hours agorootparentThe explosive growth of so many tech companies has set the bar for everything in some investors minds. If it doesn’t go absolutely vertical it is a failure. Unfortunately this rules out everything that involves anything physical (factories, cars, planes, fabs, etc.) and anything requiring research. In the end it leads to an economy that chases fads and devolves into a pure casino. reply linotype 4 hours agorootparentWell hopefully we don’t put anyone with a track record of bankrupting casinos in charge. reply api 3 hours agorootparentThe people we elect represent the zeitgeist. Trump is about attention trolling and gambling. reply lotsofpulp 6 hours agorootparentprevThe top businesses required many years, decades even, of heavy investments into physical infrastructure, including the things you listed plus data centers and global networking over land and under oceans. https://companiesmarketcap.com/ The only one that sort of went vertical was Meta, but even they had to build tons of data centers and networking infrastructure. reply highwaylights 7 hours agorootparentprevIronically I think this is the nail in the coffin. Pushing the whole company towards 18A as soon as possible made a lot of sense. Arc made a lot of sense (as it becomes competitive when you get to 18A). We’re not there yet. Sacking the CEO, throwing him under the bus, and then publicly demanding salary back just reflects really badly on Intel. I don’t know the details of what’s happened behind the scenes and if there was any impropriety, that being said: Intel under Pat was in a bad place, but it wasn’t a public clown show. reply ksec 42 minutes agorootparent>Sacking the CEO, throwing him under the bus, and then publicly demanding salary back just reflects really badly on Intel. Now that shines a new light. Not sure where you have this idea but definitely some food for thought. Basically someone doesn't like Pat. I wouldn't be surprised if it was ex-Intel Board members. reply e3bc54b2 4 hours agorootparentprevIntel is strategically important corporation. Investors know that. They want to keep extracting profits, knowing that when (not if) things go dogshit (even more so than today), the public will bail them out. Since this CEO worked on other stupid things like long term technical viability, instead of pursuing the higher priority (investor returns), he was shown the way out. 2008 was an eye opening year for a lot of rich people. reply coliveira 4 hours agorootparentprevWhatever Intel is now, is the direct outcome of poor leadership by Pat Gelsinger. That was his entire job, and he failed. reply Wytwwww 4 hours agorootparentDo you think Intel would have done better with him than without him? Because they were basically doing nothing for 5-10 years. How long do you think it would take to fix a company like that? I mean I'm not necessarily defending Gelsinger but throwing away the entire board and most of upper management would have been be the first step if they were about changing anything.. reply pbhjpbhj 6 hours agorootparentprev> It didn't' work, that's not a crime. // Well yh, when you're being paid a normal wage. In USA that sounds like maybe $200k for proven senior engineers? So, just return anything they took above that - which I'm guessing is going to round to the full wage. Honestly, I don't really understand why the people are taking so much more than their share even when successful. reply highcountess 8 hours agoparentprevI don’t think it is accurate to think that the board must be involved for there to be merit. The board represents a totally different set of interests and priorities, many of which may even directly contradict and even clash with those of shareholders, even if they are shareholders themselves. The nepotistic, incestuous, little club of boards and CEOs is almost hostile to shareholder interests once forward momentum stalls, at which point divergent priorities emerge. If the board wants to remain in the board carousel club, they can’t be joining lawsuits against CEOs, when they often themselves are CEOs that frequent the same few exclusive clubs and triple gated communities and celebrate holidays at the same exclusive hotels. There is clearly a stratification there that you are either dismissing or are not aware of. Let me put it this way, the nobility will always hold to each other over any challenges from the merchant class, even if when everyone is making money, there is some mutually beneficial mingling. reply bboygravity 7 hours agoparentprevWhy not just invest in a company where the CEO and board take no salary to begin with? They exist. reply highwaylights 7 hours agorootparentWouldn’t be relevant here, as there are no CEOs anywhere who don’t take compensation. If they’d been taking their compensation as stock grants then they’d want that back instead. reply spacebanana7 7 hours agorootparentMany founder CEOs take little or no salary or direct compensation for their jobs. Owning a big percentage of the stock is enough motivation to make the company more valuable. Mark Zuckerberg, Steve Jobs and Jack Dorsey have all at times taken $1 annual salaries. reply ponector 7 hours agorootparentBut Zuckerberg still received more than 24 million in benefits from Meta. Another bluff. It is like saying you have 1$ salary while company is covering all your personal expenses. Actually it will be better, more transparent if he received 24M salary instead of using benefits and bragging about $1. reply oytis 6 hours agorootparentI guess that's very tax-efficient too. reply missedthecue 7 hours agorootparentprevWasn't that all security reply spacebanana7 3 hours agorootparentIt seems so, in so far as that it’s roughly the same as the number in this 2021 article. https://www.thetimes.com/article/facebook-mark-zuckerberg-se.... Note that is a pretty comfortable form of security, including jets. reply matwood 5 hours agorootparentprevOwning company stock or getting a salary doesn't really matter in this case. Both are compensation. People also get up in arms on stock grants. Look at the recent Musk case. Regardless of what you think of Musk, any TSLA investor should be ecstatic with what he did with the stock. He took most of his salary in stock and now they want it back. The main reason the numbers sound big is because the stock grew so much. reply fastball 4 hours agorootparentprevDon't forget Jeff Bezos – he took 80k/year the whole time he was CEO of Amazon. reply coliveira 4 hours agorootparentIf I was the owner of Amazon I would also \"pay myself\" $1. It doesn't make any difference. reply risyachka 4 hours agorootparentprevIt has nothing to do with motivation. They have $1 salary and then get a ton of dividends or sell stock (both of which are taxed at waay lower rate than salary) So they are very well compensated at all times. I honestly don’t understand why they like to mention this $1 bs all the time. reply yieldcrv 9 hours agoparentprevIf these guys cant use the SEC whistleblower bounty program then there isn't a case This is just something petty reply chollida1 9 hours agorootparent> If these guys cant use the SEC whistleblower program then there isn't a case I think you are confusing a few things. The whistleblower program is for insiders. This lawsuit is being brought by outside shareholders. This has nothing at all to do with whistleblowers as there aren't insiders with insider knowledge. This suit is being brought by outsiders who are unhappy with Intel's performance. There is no insider here trying to expose anything. reply yieldcrv 1 hour agorootparentYou don’t have to be an insider, independent researchers have pointed out irregularities, improperly disclosed things and won bounties reply ozim 7 hours agorootparentprevYeah good luck hiring next CEO to Intel. reply okasaki 6 hours agoparentprev> It didn't' work, that's not a crime. He left because his plan didn't work. That's what happens to CEO's who try and fail to implement a business plan. Is this the famous risk that we keep hearing CEOs take that require such high compensation? Failing and leaving with 10s or 100s of M? reply Retric 6 hours agorootparentThe “risk” is based on alternatives and what gaining a reputation as a failure if you join a sinking ship at a company going through CEO’s quickly does to your future prospects. These boards want proven superstars for many reasons and there’s a limited pool. Basically suppose you’re faced with a choice between a low risk CEO position that pays 5m/year you could do for the next 10+ years at a stable company and Intel who is most likely going to dump you. How much are you asking Intel per year, and how much do you want if they decide to terminate your contract for failing at achieving unrealistic goals in an unrealistic timeframe? Those golden parachutes are often the result of an overly friendly board, but in some cases they are more easily justified. reply lotsofpulp 6 hours agorootparentprevIt is the risk that you won’t be able to convince one of the handful of people capable of understanding the most technologically advanced manufacturing processes in the world, having the right connections in the industry, to lead a culture change to right a ship. Also, Gelsinger did not get $100M+. https://finance.yahoo.com/news/pat-gelsinger-lost-massive-14... reply PittleyDunkin 5 hours agorootparentTbf most of this is out of the hands of the CEO, outside the delusions of MBAs reply lotsofpulp 5 hours agorootparentAnd outside the delusions of random internet commenters, businesses that fail to pay market prices for quality labor, especially at the cutting edge, do not succeed. Whether it be CEO or IC electrical engineer/software developer. That is one of the reasons Intel is where it is, overpaying for MBA talent and underpaying for engineering talent. A person capable of turning around Intel could easily get work at another tech company earn millions with far less risk and far less publicity. Intel needs some of the smartest people in the US working for them, it should have had the reputation of being among the highest paying employers. reply PittleyDunkin 2 hours agorootparentCEOs aren't paid for they're labor (as if they're even capable of labor!). They're paid to be a liable wad of meat. reply arthurcolle 9 hours agoparentprevup or down? reply Dalewyn 8 hours agoparentprev>He left because his plan didn't work. He didn't leave, he was fired in the same way that a Drill Sergeant goes \"I'm gonna need 10 volunteers...\" and then orders 10 unlucky SOBs to gear up. reply fidotron 8 hours agoprevI have been telling everyone for the last decade that Intel proper is a lost cause, and it's partly due to attitudes like this around it which would lead to the CEO etc. taking the heat. * Intel liked to project an aura of their inevitable success, and people inside and outside the company totally believe it. This attracted the sort of hangers on that then added so much corporate gunk that anyone that tried to deal with it would never be allowed to run the ship which they are now sinking. That is on top of the competiton waking up, which was far more predictable than having a single company dominate in the way they had. * https://montrealrampage.com/king-ludd-12-glad-not-to-be-the-... reply hinkley 7 hours agoparentI sold INTC and bought TSMC about five years ago. Everybody got nice Christmas presents this year because I sold some of my TSMC shares to rebalance my portfolio. It doubled this year and is 65% above its previous plateau. reply A_D_E_P_T 9 hours agoprevThe complaint itself is here: > https://storage.courtlistener.com/recap/gov.uscourts.cand.44... What's interesting, and what the media generally hasn't reported (probably because the dollar amounts are low,) is that the board of directors is also under fire and every member of the board is named in the complaint. \"Demand Upon Defendant ___ Is Excused\" refers to the rule that in a shareholder derivative lawsuit the plaintiff is generally required to make a formal demand on the board of directors to remedy the alleged wrongdoing before filing suit. But there is an exception called \"demand futility.\" If the plaintiff can convincingly argue that the board members are too conflicted or otherwise unable to act impartially, the demand requirement is \"excused.\" It doesn't mean that they're not pursuing damages from the board directly, even though the amounts involved are small. reply amarcheschi 9 hours agoprevFlashback to 2022 https://www.pcgamer.com/intel-ceo-says-amd-is-in-the-rearvie... Now, in those years I obsessively spent a lot of time noticing how Intel earnings started to diverge with the sentiment that the ceo tried to pass to the investors. Regardless of how this goes, I'm just happy I could stay away from Intel and buy amd stocks instead (I eventually sold them) reply bradleyjg 6 hours agoprevWe should get rid of derivative lawsuits. There’s plenty of money sloshing around in the securities space. Lawyers can work with activist investors to pay their bills if there’s a real case. But the system of suing on behalf of shareholders that never asked you to represent them needs to go. reply ksec 49 minutes agoprevBefore I go into the article. I think the front page algorithm is getting silly. >198 points by YetAnotherNick 9 hours ago101 comments This is currently at 159 on page 6. I left home and thought I should come back and read it on HN. The points and comments ratio are no way over the top. But it seems to be dropping fast on front page. This is also on Classic currently at 151. It doesn't make any sense. reply bhaney 10 hours agoprevThat must be an awful lot of salary if it's worth the reputational damage of bringing public attention to this. reply lazide 6 hours agoparentEh, or they want their names in the news for ‘doing something’, even if it’s ultimately pointless. reply caseyy 1 hour agoprevIn Japan it's not unheard of for chief executives to take pay cuts when their plans don't work out. If there are bonuses in good times, would it not incentivize executives to perform better if there was also docked pay and executive layoffs in bad times? I know some of you might think no known executive would take that job, and I actually know first hand that's largely true. I also know some that definitely would, but that's besides the point. As an investor, you don't want to hire those c-suites that are allergic to accountability. reply daft_pink 9 hours agoprevI’m not convinced that clawing back salary after you fire people is a good and decent public policy. What if everyone who sucked at their job had to payback 3 years salary? Doesn’t really make sense. reply gosub100 1 hour agoparentI don't think paying people tens-of-millions of dollars in salary is good public policy. he didn't contribute anywhere near that amount of value to the company. If CEOs want to make this kind of money it had better be tied to results only. what kind of logic is \"paid if you succeed, paid if you fail\" ? reply lazide 8 hours agoparentprevFor normal folks? For sure. And the law doesn’t allow it for normal folks. This type of thing is only possible when special rules are made for people with enough power they can hide massive fraud/malfeasance in some cases. This lawsuit seems unlikely to go anywhere though. And frankly, will just increase asking price for CEO salaries. reply signa11 7 hours agorootparentjust like doctors insurance, are there ceo insurance as well ? reply lazide 7 hours agorootparentE&O (errors and omissions), and D&O (directors and officers) insurance yes. reply RamblingCTO 9 hours agoprevhow to loose trust of future c level executives: speedrun. Seriously, who would work there after that? If it was gross negligence or something, ok. But not \"plan didn't work\". reply kayge 4 hours agoparentHell I'll do it, for a bargain too. Pay me $10 million (salary+bonus) for 1 year and let's see how it goes ;) reply atoav 6 hours agoprevWhat feels like once in a decade we see the \"responsibilties\" impact an executives life, responsibility which they frequently cite to justify their high salaries. This should happen far more often. Instead the failures of those people are often carried by states and thus by all of us. reply kopirgan 9 hours agoprevLawyers will make 3 years salary reply caseyy 4 hours agoprevIs this... CEOs being forced into accountability for their actions instead of thousands they lay off? Well, one can dream. reply grecy 4 hours agoparentMore recently those consequences had a lot more of a bang… reply a012 9 hours agoprevIf Gelsinger’s turnaround plan didn’t work and even got sued for then I think this is the fubar moment for Intel. R.I.P reply oefrha 9 hours agoparent> even got sued In a country of frivolous lawsuits, getting sued is almost meaningless. Big companies are sued for all sorts of bullshit all the time. Check back if and when there’s something concrete. reply dathinab 8 hours agorootparentThrough outside of patent and copyright law very many of the law suites commonly brought up as \"frivolous\" are actually very reasonable law suites and both the reason they are well known and them being known as \"frivolous\" is because of company propaganda so that they can push for more forced arbitration. reply smolder 7 hours agoparentprevI think it was FUBAR before Pat came back. That was always going to be a tough ship to right. I'm not sure how much you can credit him with stumbles and successes since, but at least Arc is looking decent with the most recent release. I also think the 285k and ilk aren't such bad chips as a first foray into external fabrication. They're strong all-around performers despite some minor regressions, and they're working in a smaller power budget. reply qwytw 4 hours agorootparent> but at least Arc is looking decent with the most recent release To be fair that really remains to be seen. It took them how many years to ship a single decent card? Sure it's very competitive with last gen Nvidia and AMD GPUs but it's not clear if that's the case only because Intel is willing to sell them with very low margins or even at a loss. reply heisenbit 6 hours agoprevYou mean the shareholders who extracted the money over the years which is now missing when we come to the place in Moore‘s Law where the investment requirements are going through the roof? reply phtrivier 9 hours agoprevAs a laymen, my prior is that it that the suit has a 0% chance of ending with the top executives giving all the money back. Are there existing cases where it happened ? I would like to evaluate the probability that they will give _some_ money back. Same question : where do I look for existing case ? Does that ever happen ? reply highcountess 8 hours agoparentIs not quite zero, even if there is no technical cause, because the suit itself may just be a pressure vehicle and the CEO may feel pressure among his social strata to give some back as a type of penance as condition to remain in good graces and not slam the door shut on any future opportunities. It’s a club, and they want to remain in it. reply jacknews 8 hours agoprevLet's see where Intel is in 5 years now the MBAs, finance and legal are running the show again. Pat may not have turned the ship around, but it looked like he was starting to stop the rot at least. reply PedroBatista 6 hours agoprevLooks like Intel is royally screwed. Not much because of their performance as a company ( although it's ugly ), but because of the board and their major shareholders. If they went after previous CEOs from ~2015 to ~2020, I can see some of their points. Not sure if a lawsuit now would be of their best $interest$ but they were also too busy raking in money during that time-frame, no amount of lies would matter. Either way this type of stuff at this critical period in Intel's life might be the swan song for what we know as Intel today. reply aetherspawn 9 hours agoprevWere they bad at the job, or was AMD and Apple just better? I don’t see how that can be fair. reply nic547 9 hours agoparentAMD and Apple have no Foundry reply lnsru 55 minutes agorootparentAMD got rid of the foundries in the past. The other company is GlobalFoundries and looks like to be doing fine. Intel tried to do the same 15 years later, but didn’t succeed. It’s really hard to have both things right - semiconductor manufacturing process and prefect processor design. reply jl6 8 hours agoprevThe only way this could go anywhere is if the employees in question had a very clear performance contract, or if there was significant malfeasance. reply nubinetwork 6 hours agoprevI hope Pat respectfully says no after the board forced him out... reply Havoc 6 hours agoprevGood old American fix it with a lawsuit approach. reply anoncow 6 hours agoprevIt is sad to see what is happening to Intel. reply wtcactus 9 hours agoprevFirst they shamelessly out the only engineer that could and wanted to be Intel’s CEO in a moment of crisis before his plan could actually be applied. Now, they go after him because - like he told them right at the beginning - short term profits were sacrificed in order to ensure Intel’s long term aims. Good luck finding anyone capable to be the next CEO after these two debacles in less than 1 month. reply lupusreal 8 hours agoprev [–] This is dumb and obviously won't work. I don't expect random shareholders to know better, but what about their lawyers? Why is it okay for their lawyers, who should know better, to waste their clients money and harass these executives with a lawsuit that they know is baseless? reply ChrisMarshallNY 7 hours agoparent [–] That happens all the time. The lawyers make money, which is why they do it. Not just lawyers, either. I suspect a great many of us were ordered to work on projects that we knew were “dead man walking” disasters, because our bosses ordered it. reply lupusreal 4 hours agorootparent [–] I understand the motive of the lawyers, but why do the law and the licensing bodies allow them to get away with this? If a dentist gets caught drilling healthy teeth he'll be sent to prison, or at least lose his license. Baseless legal threats may not be bodily harm, but on many occasions they've driven people to suicide. Lawyers are utterly lawless. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel shareholders are seeking the return of compensation from former CEO Pat Gelsinger and CFO David Zinsner, accusing them of misleading shareholders about the financial performance of Intel's foundry unit.",
      "The lawsuit claims a breach of fiduciary duties, following Intel's failed turnaround plan and a record $16.6 billion quarterly loss, which led to a 9.2% drop in Intel's shares.",
      "The foundry unit, initially expected to drive growth, was later identified as a significant cost center, prompting restructuring efforts by Zinsner, now co-interim CEO, while Gelsinger has resigned."
    ],
    "commentSummary": [
      "Intel shareholders have filed a lawsuit against the former CEO and CFO, demanding the return of three years of salary due to a failed business plan.",
      "Critics argue the lawsuit is unfounded, as the executives had acknowledged the challenges faced by Intel's Foundry unit.",
      "The lawsuit underscores tensions between shareholders and company leadership, with some viewing it as a frivolous action motivated by financial gain."
    ],
    "points": 199,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1735028274
  },
  {
    "id": 42501102,
    "title": "Tokyo released point cloud data of the entire city for free",
    "originLink": "https://twitter.com/spatiallyjess/status/1871342549958537326",
    "originBody": "Today I learned the city of Tokyo released point cloud data of the entire city for free for anyone to download 👀 pic.twitter.com/MgMCMYOkJV— jess ᯅ (@spatiallyjess) December 23, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42501102",
    "commentBody": "Tokyo released point cloud data of the entire city for free (twitter.com/spatiallyjess)157 points by taubek 8 hours agohidepastfavorite15 comments reustle 2 hours agohttps://www.swisstopo.admin.ch/en/switzerland-in-3d > Switzerland is one of the first countries to possess a detailed 3D buildings model covering the whole country. This digital model of Switzerland consists of approx. 70 million 3D objects. Besides every single building in Switzerland and the Principality of Liechtenstein, bridges, cable cars, forests, individual trees and geographical names are also represented in 3D. Two movement modes enable interactive navigation through space. Discover digital Switzerland from the air in flight mode or take a virtual stroll around a 3D model of your own village or neighbourhood. reply cdaringe 1 hour agoparentCrashed on my old iPhone 6SE, but could tell it was getting cool! reply _____k 7 hours agoprevLink: https://info.tokyo-digitaltwin.metro.tokyo.lg.jp/3dmodel/ reply Retr0id 3 hours agoparentWhat do I need to click to get the pointcloud version? I'm just getting regular untextured polygons. reply numpad0 40 minutes agorootparentSupposed to be [1][2] according to [0] but only by lots of clicking through GUI? It's really intuitive. GSI(Geospatial Information Authority of Japan) download site is a lot better, though they're more geography focused. 0: https://twitter.com/tocho_digital/status/1697474739583746474 1: https://catalog.data.metro.tokyo.lg.jp/dataset/t000029d00000... 2: https://catalog.data.metro.tokyo.lg.jp/dataset/t000029d00000... 3: https://fgd.gsi.go.jp/download/menu.php reply Timon3 2 hours agorootparentprevThe table \"Posted data on 3D Viewer\" contains a row \"Point cloud data\" with several links, e.g. \"Viewing the LP point cloud of the Tokyo Metropolitan Government area in the viewer\": https://3dview.tokyo-digitaltwin.metro.tokyo.lg.jp/#share=s-... reply yarri 41 minutes agoprevBackground on the Tokyo government’s digital twin program, including sourcing and maintenance efforts https://github.com/tokyo-digitaltwin/roadmap_v1.0/blob/main/... reply cpa 2 hours agoprevIn france, the national geodata institute (IGN) has captured lidar data of the whole country (20 points per km2 if memory serves, in the OP it’s 30p per km2). https://diffusion-lidarhd.ign.fr/visionneuse/?copc=https:%2F... reply lxdlam 2 hours agoprevI must say this is tremendous. There are many different AIGC explorations in 3D topics, with such high quality dataset, it will greatly assist current workflow and accelerate the 3D creative evolution. reply Havoc 33 minutes agoprevHow do they collect point cloud data at scale? reply lbotos 11 minutes agoparentspeculating: Lidar from a plane? https://www.reddit.com/r/Albany/comments/1hdxkz0/comment/m1z... Imagine a route like this, except many lanes. I was trying to find a pic I saw the other day of one over NYC reply lbotos 9 minutes agorootparentslightly better: https://www.reddit.com/r/flightradar24/comments/u59zc2/off_t... https://www.yellowscan.com/knowledge/drone-flight-planners-t... reply JCharante 2 hours agoprevGreat for cgi and video games reply bamboozled 5 hours agoprevSo fun to explore, it has to be the greatest city on earth, just a marvel in so many ways. reply yieldcrv 3 hours agoprev [–] Feels like where Google Earth was 22 years ago reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tokyo has made available free point cloud data of the entire city, allowing for public download.",
      "Point cloud data is a collection of data points in space, often used in 3D modeling and geographic information systems (GIS).",
      "This release is significant as it provides valuable resources for developers, researchers, and urban planners interested in 3D modeling and spatial analysis of Tokyo."
    ],
    "commentSummary": [
      "Tokyo has made available free point cloud data of the entire city, enabling interactive 3D navigation and exploration.- This initiative is similar to those in Switzerland and France, where detailed 3D models and lidar data have been released.- These datasets are particularly valuable for applications in computer-generated imagery (CGI), video games, and other 3D technologies."
    ],
    "points": 157,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1735037499
  },
  {
    "id": 42498982,
    "title": "Demystifying Debuggers, Part 2: The Anatomy of a Running Program",
    "originLink": "https://www.rfleury.com/p/demystifying-debuggers-part-2-the",
    "originBody": "Share this post Digital Grove Demystifying Debuggers, Part 2: The Anatomy Of A Running Program Copy link Facebook Email Notes More Demystifying Debuggers, Part 2: The Anatomy Of A Running Program On the concepts involved in a running program. What happens, exactly, when you double click an executable file, or launch it from the command line, and it begins to execute? Ryan Fleury Dec 23, 2024 29 Share this post Digital Grove Demystifying Debuggers, Part 2: The Anatomy Of A Running Program Copy link Facebook Email Notes More 3 3 Share From day one using a modern home computer, users are exposed to the concept of a program. Support for separate programs is, after all, the main value-add of multitasking operating systems. But—if we take a peek under the hood—a program is a high-level term which refers to many lower level mechanisms and concepts, and it isn’t obvious from the outset how they’re all arranged. To unpack debuggers—programs which analyze the execution of other programs—it’s important that we first unpack the concept of a program, so that we’re familiar with the details of programs that a debugger must contend with. Programs are the virtualized equivalent of cartridges for an old video game console, like the Nintendo Entertainment System. The NES didn’t have a multitasking operating system, and it only executed a single program while it was turned on—whatever one was stored on the cartridge that the player installed. In this context, the program executing on the system had full availability to all of the system’s resources. There was no code running of which the program couldn’t be aware. Programs, in the context of a multitasking operating system, are a bundle of mechanisms used to approximately provide the same thing virtually as the NES provided to the program stored on the cartridge physically. Of course, multitasking operating systems also provide ways for these programs to communicate and interact (that is indeed the point), but at some level they must still exist independently, as different physical cartridges do. Because programs, unlike cartridges, can be executing on the same chip at the same time, and thus contend for the same resources, there are many additional software concepts that operating systems use to virtualize independent program execution: A virtual address space — A range of virtual addresses, for which the platform provides a mapping to physical addresses. Programs are built to interact with virtual addresses, which are entirely independent from addresses in other virtual address spaces. Virtual address spaces can be much larger than, for example, physical RAM limitations. A thread of execution — A bundle of state which is used to initialize the CPU to coherently execute a sequence of instructions. Threads of execution are scheduled by the platform, such that many threads can execute on a small, fixed number of cores. An executable image — A sequence of bytes encoding data in a platform-defined format, to encode executable machine instructions, as well as relevant headers and metadata. An independent code package’s non-live representation—a blueprint for execution. A loader — The part of an operating system responsible for parsing executable images—blueprints for execution—and instantiating them, so that the code encoded in the images may be actually executed. A module — The loaded equivalent of an executable image. One process can load several modules, although a process is always initialized by the loading of one specific module (the initial executable image). Modules can be both dynamically loaded and unloaded. A process — An instance of a live, running program. Instantiated by the platform’s loader using the initial executable image to determine how it’s initialized, and what code is initially loaded. The granularity at which operating systems assign virtual address spaces. The container of several modules, and threads of execution. Let’s unpack all of this. Virtual Address Spaces A range of virtual addresses, for which the platform provides a mapping to physical addresses. Programs are built to interact with virtual addresses, which are entirely independent from addresses in other virtual address spaces. Virtual address spaces can be much larger than, for example, physical RAM limitations. Whether it’s through the easy or hard way, all programmers learn about pointers. When I first learned about pointers, I understood them as being used to encode integers, with the integers being addresses, which address bytes within memory, in linear order. Address 0 comes before 1, which comes before 2, and so on. In other words, I was under the impression that physical memory, and its relationship to addresses, was structured like this: This is a fine mental model to begin with. But it isn’t accurate. When many independent programs execute on a single machine, it isn’t difficult to imagine one of them getting an address wrong. In fact, sometimes, it feels like “getting addresses wrong” is the only thing anybody talks about these days. If all of these programs shared a single memory space, this could easily lead to one program stomping over data that another program is using. It could also lead to, for example, a malicious program—let’s call it ryans_game.exe—reading information from chrome.exe, browsing a page from chase.com with all of your sensitive information on it. This is purely hypothetical! Virtual address spaces are used to mediate between different programs accessing the same resource—physical memory. Addresses can be understood as integers, and as such, they are linearly ordered, and they do each refer to sequential bytes—but these bytes are sequential in virtual address space, not in physical memory. Virtual address spaces are implemented with a mapping data structure known as a page table. Page tables can be used to translate a virtual address to a physical address. They can then be used directly by the CPU in order to do address translation. For instance, if a CPU core were to execute a mov (move) instruction, to load 8 bytes from address 0x1000 into a register, then before issuing a read from physical memory, the CPU would first treat 0x1000 as a virtual address, and translate it into a physical address, which might be completely different—like 0x111000. “Page tables” are called as such, because they map from virtual to physical addresses at page-size granularity. A system’s page-size varies—on an x64 Windows PC, it’ll be 4 kilobytes. On an iPhone, it’ll be 16 kilobytes. Operating systems also expose larger page sizes under some circumstances. This means the relationship between physical memory and an address—as used by a program, as a virtual address—looks more like this: If a virtual address cannot be mapped to a physical address, then a “page fault” exception is issued by the CPU, and execution is interrupted. If this is done by a program’s code, then execution will be transferred to the operating system’s code, which can take measures to address the cause of the exception and resume, or do whatever else it deems appropriate. This provides a great deal of flexibility to operating systems. An operating system can move memory allocated by one program to disk—what’s known as “paging out”, or “swapping out”—if it expects that memory to not be accessed in the near future. It can then use that physical memory for more frequently accessed addresses, in any of the active virtual address spaces. If a page fault occurs when code attempts to access memory which has been paged out, then the operating system can simply page that memory back in, and resume execution. Thus, even though hundreds—if not thousands—of programs can be executing at once, the operating system can make much more efficient use of physical memory, given its analysis of which addresses in which spaces are needed, and when. This is critical in building operating systems which can support the execution of many programs, where all programs are contending for the same physical hardware. It also provides a great deal of flexibility to programs, as it can be used to implement virtual address spaces which are much larger than physical memory. Nowadays, nearly every consumer CPU—from phones, to game consoles, to PCs, to laptops—is a 64-bit processor. For PCs and laptops running on 64-bit CPUs, the CPU and operating system normally provide a 48-bit address space. On some server systems, it is larger, and on some mobile and console platforms, it is smaller. Taking a 48-bit address space as an example—48 bits allow the representation of 248 different values (each bit multiplies the number of possible values by 2). Since each value refers to a different potential byte, that is enough address space to refer to 256 terabytes. To understand this further, let’s dissect the “page table” data structure a bit more. First, let’s assume a 48-bit address space, and a 4 kilobyte page size (the usual configuration on x64 Windows systems). As I said, page tables map from virtual to physical addresses at page-size granularity. Because of our 4 kilobyte page size, we can infer that the bottom 12 bits of any address are identical for both a virtual address and a physical address (212 = 4096 = 4 kilobytes). This leaves 36 remaining bits, of each address, to map from virtual to physical addresses. These bits are used to index into several hierarchical levels, within the page table—it is actually a hierarchical data structure, despite its name, which sounds like it implies a flat table. To understand why, imagine, first, a naïve page table implementation, which simply stores a 64-bit physical address, for each value in this 36-bit space. This, unsurprisingly, would require an unrealistically large amount of storage. Instead, we can notice that the page table need only map virtual addresses which have actually been allocated. At the outset, none are allocated. When a virtual address space allocation is made, a hierarchical data structure allows the page table implementation to only allocate nodes in the hierarchy which are actually touched, by that one allocation. Each node in the hierarchy can simply be a table of 64-bit addresses which point to children nodes (or, at the final level, it can store each page’s physical address). If each node is a 512-element table, and each element is a 64-bit address (8 bytes), then each node requires 4096 bytes, which is our page-size! Because 29 = 512, we can slice our 36-bits into 4 table indices—each 9 bits—and use that to traverse the page table. The first 9 bits indexes into the first level, the next into the second, the next into the third, the next into the fourth—the fourth provides the base address of the containing page of our address, and then the bottom 12 bits can be used as an offset from that base. For each virtual address space, the operating system manages this page table structure. Before the operating system prepares the CPU to execute code for one program, it can supply this table, such that the CPU can appropriately issue memory reads and writes to physical addresses for the appropriate virtual address space. The end result is that each program can, in effect, live in its own universe of virtual addresses, as if it had access to the entire system’s memory space, and if that memory space far exceeded the limitations of a system’s random access memory (RAM) capacity. Threads Of Execution A bundle of state which is used to initialize the CPU to cohesively execute a sequence of instructions. Threads of execution are scheduled by the platform, such that many threads can execute on a small, fixed number of cores. Beyond a page table, a CPU core requires other information to coherently execute code. For instance, it requires the “instruction pointer” (or “program counter”)—this is a register, which stores the virtual address of the next instruction which should execute, in a given instruction stream. After each instruction is executed, the value in this register is updated to reflect the base address of the next subsequent instruction. On x64, this is known as the rip register. When using a debugger, you’ll often see golden arrows, pointing to lines of source code or disassembly. This directly visualizes the location of the instruction pointer. There are several other registers, used for a variety of purposes, including general purpose slots for computations. The state of all such registers is called a “register state”, or “register file”. One register state is paired exclusively with one instruction stream, from one program—a register state should only change if a single instruction stream performs work which causes it. But a CPU has only a fixed number of cores, be it 1, 2, 4, 8, 12, 16, 32, and so on—yet operating systems support a much larger number of programs executing simultaneously. Or, at least, it seems like they execute simultaneously. The operating system implements this illusion—of hundreds if not thousands (if not more—unfortunately…) programs running simultaneously on a small, fixed number of cores—by scheduling work from these programs. One CPU core will perform work for one program, for some period of time—it will be interrupted, and the operating system can make the decision to schedule work from another program, for example. A thread of execution is the name given to the execution state for one instruction stream. Each contains one register state, which includes the instruction pointer, and thus a stream of instructions—among whatever other state each operating system deems appropriate. In other words, operating systems do not just schedule programs—they schedule threads. When an operating system schedules a thread, it incurs a “context switch”—this is the process of storing the CPU core state for whatever thread was executing to memory, and initializing that core to execute work for the thread which will execute. Executable Images A sequence of bytes encoding data in a platform-defined format, to encode executable machine instructions, as well as relevant headers and metadata. An independent code package’s non-live representation—a blueprint for execution. On Windows, you’ll find executable images stored on the filesystem with a .exe, or a .dll extension. These files are stored in the Portable Executable (PE) format. The difference between .exe and .dll is that the former is used to signify that an executable image is a viable initial module for a process, whereas the latter is used to signify that an executable image is only to be loaded dynamically as an additional module for a process. On Linux systems, there is a similar structure—executable images are stored on the filesystem (the extension convention varies—sometimes there is no extension for the equivalent of Windows’ .exe, sometimes there is a .elf extension, and for the equivalent of Windows’ .dll, the extension is generally .so). These files are stored in the Executable and Linkable Format (ELF). When I say “these files are stored in” a particular format, what I mean is that the associated operating system’s loader expects files in that format. In order to produce code which can be loaded on a platform out-of-the-box, one must package that code in the format which is expected by that platform. It’s not in this series’ scope to comprehensively dissect either the PE or the ELF formats. But to justify the definition and concepts I’ve provided, let’s investigate the PE format using a simple example. First, consider the following code: // sample.c void WinMainCRTStartup(void) { int x = 0; } This can be built with the following command, using the Visual Studio Build Tools: cl /nologo /Zi sample.c /link /NODEFAULTLIB /INCREMENTAL:NO /SUBSYSTEM:WINDOWS This command will produce an executable image, containing machine code. This machine code could be disassembled (for instance, using a debugger)—that would show something like this: sub rsp, 0x18 ; - push 24 bytes onto the stack, for locals mov dword ptr [rsp], 0x00 ; - set the 4 bytes we are using of the stack ; for `x` to 0 add rsp, 0x18 ; - pop the 24 bytes we pushed off the stack ret ; - return to the caller of our main function Even if we know nothing else about the PE format, we do know that these instructions need to be encoded somewhere in the file. We can identify how these are encoded using a disassembler tool as well, which should have an ability to visualize the machine code bytes which were parsed to form each instruction: The above image shows the disassembled instructions in the RAD Debugger, as well as the bytes from which they were parsed. If you took a look at the disassembly yourself, and were confused by the add [rax], al instruction everywhere surrounding the actual code, the code bytes also clear that mystery up—that is simply the instruction one obtains when parsing two sequential zero bytes. Given the above, we know that the generated machine code is encoded with 16 bytes. Each byte can be represented with two hexadecimal digits: 48 83 ec 18 c7 04 24 00 00 00 00 48 83 c4 18 c3 If we look at the generated EXE with a memory viewer, we can, indeed, find this sequence of bytes. We know that this sequence of bytes is the primary “payload”—the actual program code. Everything else in the file is used to either instruct the loader how to correctly prepare a process for this code to execute, or to associate various metadata with the code. For example, if you scan around the file, you’ll find the full path to the debug information file (PDB) for the executable image. The executable image also must store data to which code refers. We can see this by inserting a recognizable pattern into a global variable: // sample.c static char important_data[] = {0x12, 0x34, 0x56, 0x78, 0x90}; void WinMainCRTStartup(void) { int x = important_data[0]; } We can also easily find the corresponding data in the PE file: If you investigate formats like PE or ELF more closely, what you’ll find is that various categories of data—code, initialized global variables—are separated into sections. Each section has a name, which is also encoded in the file. In PE, .text, for example, encodes all of the machine code (rather than, well, text…). .data stores data for initialized global variables. .rdata stores the same, but is separated to be allocated in read-only pages, such that code cannot modify that data. .pdata and .xdata encode information about how, given a procedure, one may unwind a thread, to—for example—produce a call stack, which is simply recreating the information of which functions called which other functions in order to get a thread of execution to its current point in a procedure. But we’ll dig into that topic in a later post. .edata and .idata encode information about exports and imports, respectively, which associate strings (“symbol names”) with locations in the file. An export is used by DLLs, for example, to export functions which can be dynamically loaded by name, by code in an executable or other DLL, and called. An import is used by either executables or DLLs to specify functions from other modules with which it must dynamically “link”. When implementing a debugger, the precise details of formats like PE and ELF become relevant—but this should be a sufficient introduction for those unfamiliar with the basics. Loaders & Modules A loader — The part of an operating system responsible for parsing executable images—blueprints for execution—and instantiating them, so that the code encoded in the images may be actually executed. A module — The loaded equivalent of an executable image. One process can load several modules, although a process is always initialized by the loading of one specific module (the initial executable image). Modules can be both dynamically loaded and unloaded. More than a debugger, a loader must be highly aware of executable image format details, because it has the task of parsing those images and making preparations, such that the code contained in the executable image can be executed. A loader executes when a program is initially launched, or when actively-executing code requests to dynamically load another image—for instance, via LoadLibrary (Windows) or dlopen (Linux). To understand this, let’s build a toy executable image format, and write our own loader, which parses our format, rather than PE or ELF. Consider the following code, from earlier: // sample.c void WinMainCRTStartup(void) { int x = 0; } And its disassembly: > c:/devel/sample/sample.c > { {48 83 ec 18} sub rsp, 0x18 > int x = 0; {c7 04 24 00 00 00 00} mov dword ptr [rsp], 0x00 > } {48 83 c4 18} add rsp, 0x18 {c3} ret Our toy format can have a simple header, at the beginning of the image, containing the following values, in order: An 8-byte signature, denoting that the file is in our format—must always be 54 4f 59 45 58 45 00 00—encoding the ASCII text TOYEXE, followed by two zero bytes. An 8-byte offset into the file, encoding where in the file all readable-and-writable global data is stored—the “data section” An 8-byte offset into the file, encoding where in the file all read-only global data is stored—the “read-only data section” An 8-byte offset into the file, encoding where in the file all executable data is stored— the “code section” Each section size is determined by taking the next subsequent section offset (or the file size, in the case of the final section), and subtracting from it the section offset. If sections contain no data in any case, they will simply have the same offset as the next section. Given this simple format, our full executable file for the simple example program can be encoded with the following bytes: {54 4f 59 45 58 45 00 00} (magic) {20 00 00 00 00 00 00 00} (read/write data offset) {20 00 00 00 00 00 00 00} (read-only data offset) {20 00 00 00 00 00 00 00} (executable data offset) {48 83 ec 18 c7 04 24 00 00 00 00 48 83 c4 18 c3} (executable data) In this case, our data sections are completely empty, because no global data is used by the code. Every section offset begins at offset 0x20 (or 32 bytes) into the file—or, directly after the header. The executable data section, being the final section, occupies the remainder of the file. Our “loader” can define the format’s header with the following structure: typedef struct ToyExe_Header ToyExe_Header; struct ToyExe_Header { U64 magic; // must be {54 4f 59 45 58 45 00 00} U64 rw_data_off; // read/write U64 r_data_off; // read U64 x_data_off; // executable }; It can begin by reading the file, and extracting the header: // open file, map it into the process address space HANDLE file = CreateFileA(arguments, GENERIC_READ, 0, 0, OPEN_EXISTING, 0, 0); U64 file_size = 0; if(file != INVALID_HANDLE_VALUE) { DWORD file_size_hi = 0; DWORD file_size_lo = GetFileSize(file, &file_size_hi); file_size = (((U64)file_size_hi) = sizeof(*header)) { header = (ToyExe_Header *)file_base; } It can then allocate memory, big enough for the image’s data, and copy the file’s contents into that address range. // allocate memory for all executable data - ensure it is all // writeable, executable, and readable void *exe_data = VirtualAlloc(0, file_size, MEM_RESERVE|MEM_COMMIT, PAGE_EXECUTE_READWRITE); // copy file's data into memory CopyMemory(exe_data, file_base, file_size); Given the header’s information encoding where in the executable data the code is stored, we can now call into that code directly: // call the code void *x_data = (U8 *)exe_data + header->x_data_off; ((void (*)())x_data)(); And it actually works! But there is, as you might expect, more minutiae to this in practice. Per-Section Memory Protections In this example, I’ve allocated all of the executable’s data with identical memory protections—all bytes in the executable’s data are legal to read, write, and execute. The point of having different sections at all is to organize data by how it will be accessed and used, so that—for instance—our “read-only data section” can actually be read-only (such that, if any code were to attempt writing to it, it would fail). Because memory protections are assigned at page granularity, each individual section, after it’s loaded by our toy loader, must be at least one page size (so that we can assign appropriate protections to each section), and it must be aligned to page boundaries. But, were we actually designing a format, to require all sections be at least one page size (which is normally 4 kilobytes, if not larger), at least in the executable image itself (as it’s stored in the filesystem), can be fairly wasteful for smaller executables. Instead of our loaded image being a flat copy from the image file: We can adjust it to being an expansion for each section to page granularity, and a copy: To do this, we can introduce a distinction between unloaded sections (that which stored in an executable image), and loaded sections (that which are loaded in memory, when a process executes). So far, our toy format has one notion of “offset”. We can break that down into two notions of offset, into two separate spaces—”unloaded space” and “loaded space”. These are generally called “file space” and “virtual space” (where “virtual” refers to a process’ “virtual address space”). Thus, instead of one type of offset, we can have file offsets, or virtual offsets. In code, instead of using off as our naming convention, we can explicitly encode which space we’re working within, by prefixing a name with either f or v. For example, “offsets” can now be referred to as either foff for file offsets, or voff for virtual offsets. This distinction of unloaded and loaded images is the reason for the separation between the terms image and module. We call the image the “cold” equivalent of the data, and we call the module the “hot”—the loaded—equivalent of the data. We can rewrite our header structure as follows, to encode both the locations of section data within the image, and to encode where the section data should be arranged within memory before execution: typedef struct ToyExe_Header ToyExe_Header; struct ToyExe_Header { U64 magic; // must be {54 4f 59 45 58 45 00 00} U64 padding; // (round up to 64 bytes) U64 rw_foff; // read/write (file) U64 r_foff; // read (file) U64 x_foff; // executable (file) U64 rw_voff; // read/write (virtual) U64 r_voff; // read (virtual) U64 x_voff; // executable (virtual) }; Our test program can then be adjusted to the following bytes, assuming 4 kilobyte pages: {54 4f 59 45 58 45 00 00} (magic) {00 00 00 00 00 00 00 00} (padding) {20 00 00 00 00 00 00 00} (read/write data file offset) {20 00 00 00 00 00 00 00} (read-only data file offset) {20 00 00 00 00 00 00 00} (executable data file offset) {00 10 00 00 00 00 00 00} (read/write data virtual offset) {00 10 00 00 00 00 00 00} (read-only data virtual offset) {00 10 00 00 00 00 00 00} (executable data virtual offset) {48 83 ec 18 c7 04 24 00 00 00 00 48 83 c4 18 c3} (executable data) And our loader can be adjusted to perform the “expansionary copy”: // unpack f/v dimensions of each section (and header) U64 fdata_hdr_size = sizeof(*header); U64 fdata_rw_size = header->r_foff - header->rw_foff; U64 fdata_r_size = header->x_foff - header->r_foff; U64 fdata_x_size = file_size - header->x_foff; U64 vdata_hdr_size = fdata_hdr_size; U64 vdata_rw_size = header->r_voff - header->rw_voff; U64 vdata_r_size = header->x_voff - header->r_voff; U64 vdata_x_size = fdata_x_size; // round up virtual sizes to 4K boundaries vdata_hdr_size+= 4095; vdata_rw_size += 4095; vdata_r_size += 4095; vdata_x_size += 4095; vdata_hdr_size-= vdata_hdr_size%4096; vdata_rw_size -= vdata_rw_size%4096; vdata_r_size -= vdata_r_size%4096; vdata_x_size -= vdata_x_size%4096; // calculate total needed virtual size, allocate U64 vdata_size = (vdata_hdr_size + vdata_rw_size + vdata_r_size + vdata_x_size); U8 *vdata = (U8 *)VirtualAlloc(0, vdata_size, MEM_RESERVE|MEM_COMMIT,PAGE_READWRITE); // unpack parts of virtual data U8 *vdata_hdr = vdata + 0; U8 *vdata_rw = vdata + header->rw_voff; U8 *vdata_r = vdata + header->r_voff; U8 *vdata_x = vdata + header->x_voff; // unpack parts of file data U8 *fdata = (U8 *)file_base; U8 *fdata_hdr = fdata + 0; U8 *fdata_rw = fdata + header->rw_foff; U8 *fdata_r = fdata + header->r_foff; U8 *fdata_x = fdata + header->x_foff; // copy & protect CopyMemory(vdata_hdr, fdata_hdr, fdata_hdr_size); CopyMemory(vdata_rw, fdata_rw, fdata_rw_size); CopyMemory(vdata_r, fdata_r, fdata_r_size); CopyMemory(vdata_x, fdata_x, fdata_x_size); DWORD old_protect = 0; VirtualProtect(vdata_hdr, vdata_hdr_size, PAGE_READONLY, &old_protect); VirtualProtect(vdata_rw, vdata_rw_size, PAGE_READWRITE, &old_protect); VirtualProtect(vdata_r, vdata_r_size, PAGE_READONLY, &old_protect); VirtualProtect(vdata_x, vdata_x_size, PAGE_EXECUTE, &old_protect); And—since it’s easy to notice that this is getting rather repetitive for each section—we can table-drive this “expansionary copy”. Doing so will eliminate most of the per-section duplication: // gather all information for all boundaries between all sections (& header) struct { U64 foff; U64 voff; DWORD protect_flags; } boundaries[] = { {0, 0, PAGE_READONLY}, {header->rw_foff, header->rw_voff, PAGE_READWRITE}, {header->r_foff, header->r_voff, PAGE_READONLY}, {header->x_foff, header->x_voff, PAGE_EXECUTE}, {file_size, 0, PAGE_READONLY}, }; U64 region_count = (sizeof(boundaries)/sizeof(boundaries[0]) - 1); // calculate vsize for all regions U64 vdata_size = 0; for(U64 idx = 0; idx x_voff; ((void (*)())vdata_x)(); This makes the assumption that the first instruction stored is our entry point. If we ever wanted that to not be the case—as “real” executable image formats do—then we can simply store a virtual offset for the desired entry point within the image’s header. But in the case of dynamic loading, our loader’s job is not to merely begin executing at a single point in some code. Our load instead must load the image, and prepare for dynamic lookups of potentially many named entry points. On Windows, the usage code for this looks something like: HMODULE foo_library = LoadLibraryA(\"foo.dll\"); void (*foo_function)(void) = GetProcAddress(foo_library, \"foo_function\"); foo_function(); To facilitate this path, our executable image format must associate a number of names—like foo_function—with specific virtual offsets in the executable data section. This concept is known as an executable image’s exports, and it can be straightforwardly encoded as a set of pairs of names and virtual offsets. There’s a symmetric concept known as imports, which function as a fast path for the manual lookup of functions from a loaded executable image like the above code. On Windows, the usage code for that looks something like: __declspec(dllimport) void foo_function(void); #pragma comment(lib, \"foo.lib\") foo_function(); Both explicitly loaded (via GetProcAddress on Windows, or dlsym on Linux), and implicitly loaded (via __declspec(dllimport) on Windows, which is more automatic on Linux toolchains) functions are called through a double indirection. To perform an actual function call, first the CPU must follow the address of the pointer in which the loaded address is stored, then it can use whatever the value of that pointer as the address of the function to call. In the above example with explicit loading, the address of the loaded function is stored in the explicit foo_function function pointer variable. In the above example with implicit loading, the address of the loaded function is implicitly stored in hidden state, as an implementation detail. When an executable image has imports, in order for the loading of that image to succeed, the associated imports must be dynamically linked. This will be done automatically, as opposed to the program code manually calling—for instance—LoadLibrary or GetProcAddress. Address Stability & Relocations Machine code contained in an executable image can be hardcoded to refer to specific addresses. But as you’ll notice, in our toy loader, we don’t control which address at which our module for an image is placed in memory. We call VirtualAlloc to allocate memory for our module data, and whatever address it returns, we use that. Of course, we can request that VirtualAlloc place our allocation at a specific address, but that is not necessarily guaranteed to succeed. This means that if we, for instance, had an image with instructions which referred to a global variable’s absolute address, they would only be valid given that the image is loaded at a particular address. In principle, a loader could guarantee a fixed virtual address for a program’s initially loaded executable image. They don’t. But in any case, that cannot generally be true, because images can be loaded or unloaded dynamically, and they are not built to be aware of which other images are loaded simultaneously. Thus, they must be dynamically arranged—each image’s code should be able to operate correctly, irrespective of where its loaded module equivalent is placed in memory. In many cases, especially nowadays, addresses are encoded as relative to some offset into code, in which case they’re always valid, irrespective of which runtime address at which the module is loaded. But, nevertheless, there still exist mechanisms for code to be hardwired to refer to specific addresses. In such cases, an executable image also contains relocations, which encode locations within the executable image which must be reencoded after the base address of the loaded image is determined at runtime. It is the loader’s job to iterate these relocations, and patch in the appropriate addresses given the only-then-available knowledge of where the image’s loaded data is actually stored. And Finally, A Process An instance of a live, running program. Instantiated by the platform’s loader using the initial executable image to determine how it’s initialized, and what code is initially loaded. The granularity at which operating systems assign virtual address spaces. The container of several modules, and threads of execution. We’ve covered everything we need to sketch out a definition of a process—a running program. Each process is the owner of some number of threads, and some number of modules. It is the owner of a single virtual address space. Threads and virtual address spaces are, in a sense, orthogonal concepts—threads are used to virtualize CPU cores, virtual address spaces are used to virtualize physical storage—the process is the concept which binds them together. When a program is launched, a process is created, an executable image is loaded to produce an initial module, and an initial thread is spawned. An operating system’s scheduler then considers that process’ main thread as a viable candidate for scheduling. When it’s scheduled, the program executes. When a debugger is used to analyze another program, it does so at process granularity. It is registered by the operating system as being attached to a process. When a debugger is attached to a process, the operating system enables additional codepaths, which report information about that process’ execution to the debugger’s process. If that information includes addresses, they’re reported as virtual addresses, within the address space of the process to which the debugger is attached. But that’s enough for now! We’ll dig into exactly what kind of information an operating system reports to a debugger process, how it can do so, and how the debugger can interact with the debugged process, next time. If you enjoyed this post, please consider subscribing. Thanks for reading. -Ryan Subscribe 29 Share this post Digital Grove Demystifying Debuggers, Part 2: The Anatomy Of A Running Program Copy link Facebook Email Notes More 3 3 Share",
    "commentLink": "https://news.ycombinator.com/item?id=42498982",
    "commentBody": "Demystifying Debuggers, Part 2: The Anatomy of a Running Program (rfleury.com)141 points by robenkleene 17 hours agohidepastfavorite19 comments emeryberger 3 hours agoDebuggers are great, and even better with LLMs! https://github.com/plasma-umass/ChatDBG https://arxiv.org/abs/2403.16354 reply ffsm8 1 hour agoparentI've only read the introduction, but it's uncanny how everything they described is kinda in the feature bullet list of various static code analysis software that's been around for decades now, i.e. sonar Don't get me wrong, I'm sure a great LLM integration has the potential to improve a debugger in your ide. The chat interface in which a developer writes prompts would be absolutely garbage though, of that I'm certain But if it's able to trace everything until the error occured and then output an analysis for that error, that'd be great. Not sure how realistic that'd be though. Definitely seems way beyond the current capabilities, but let's see how the ecosystem will look in 10 years. reply righthand 15 hours agoprev [18 more] [flagged] ehnto 13 hours agoparentOne of the reasons I am not yet worried about LLMs is that this is not actually possible, at sufficient levels of domain complexity, and contextual complexity. The domain complexity that LLMs start failing is pretty low at the moment. As well, as domain complexity increases so does the required amount of precision, and again I find LLMs start getting less precise when put in the deep end. You don't tend to find this kind of software in your \"programmers programming\". You tend to find this kind of software in industry, where large businesses consist of wild business rules built up over decades, and that has been half-codified into software. I just don't see LLMs swooping in to take that over. reply RobRivera 14 hours agoparentprevWhat is the best diet for trolls these days? reply metadat 14 hours agorootparentThis is a fair question, though it's probably still mostly the same as pre-LLM: spoiled data, heinous cackling laughter, various darkweb fungi, and occasional raw flesh. reply righthand 3 hours agorootparentAh yes the petty responses of the “bigger” “smarter” person. reply metadat 2 hours agorootparentYou're good in my book. Happy holidays. :) reply righthand 14 hours agorootparentprevI’m sorry, but if LLMs are going to change the way we code, and the way we code is not even print debugging for junior developers. I know very few senior developers that use a debugger at all. I don’t understand why my question is trolling. What is the best diet for those easily triggered? reply rfleury 14 hours agorootparentAre you out of your mind? LLMs will require much more debugging, not less. Separately, I have no idea which “senior developers” you know, but with or without LLMs, debugging is an immensely important part of development across the industry. reply draven 10 hours agorootparentprevSenior here, I started to use LLMs recently but I still use a debugger. Working with complex code it helps me find the difference between what I think should be happening and what really happens. I don't think a LLM could do better for this task. Also, I use debuggers for print debugging: insert a breakpoint which doesn't suspend execution but prints something (perhaps conditionnaly.) That way no need to recompile the whole thing and restart it after adding a print statement. reply jemmyw 14 hours agorootparentprevI don't think LLMs have anything to do with it. I've rarely seen devs reach for a debugger until they really really need to. In web dev it can be pretty hard, you need a reproducible bug or production server access. I can probably count on one hand the number of times a bug occurred that stumped me to that point. If anything, I can see LLMs making debuggers more used because they'll help you with getting setup and interpreting what is going on. reply ehnto 13 hours agorootparentI am not trying to sell anyone on debugging, but honestly if you have a good debugger and it's easy to drop into, you'll find it way easier and faster than print debugging. I don't know why people put themselves through the pain of writing print all over the place when you can just, pause the program and see literally everything, all the way up the stack. reply jemmyw 13 hours agorootparentThere are different levels of debugger. I use language ones all the time. But this article is talking process debugging. If you dev native compiled apps then that's probably second nature. But if you do dev on a scripted or runtime language then it is, as I said in my previous comment, rarely applicable but occasionally very helpful. reply troupo 13 hours agorootparentprevJohn Carmack: It still boggles my mind how hostile to debuggers and IDEs Silicon Valley VC- backed companies are https://youtu.be/tzr7hRXcwkw?si=LX7LhGYUHmPAWVNJ reply troupo 11 hours agorootparentprev> if LLMs are going to change the way we code They are not > the way we code is not even print debugging for junior developers. what > I know very few senior developers that use a debugger at all I know very few, too, and it's their loss. Debugging is immeasurably more useful to understand and get to the root of the problem in a complex system than printf'ing and logging. > What is the best diet for those easily triggered? The best diet for you is to unplug from the hype trains on Twitter and HN. reply norir 1 hour agorootparent> Debugging is immeasurably more useful to understand and get to the root of the problem in a complex system I try to write systems that aren't so complex that I need a debugger but get the same job done. And I choose not to work on systems already so complex that their usage is mandatory. I do not consider this a loss at all. Systems that I write can almost uniformly rebuild and rerun their tests in under a second. I can add (pretty) prints and see the results faster than I can switch to a different tab to run a debugger. I have never introduced or encountered a bug that required advanced debugging techniques besides getting a stacktrace from a crashing problem. YMMV. reply righthand 3 hours agorootparentprevThey did, as never before were people talking to a chat bot to have code generated. Seems different to me, but please more talking down and pettiness. reply hun3 5 hours agoparentprev [–] Have fun brute forcing heisenbugs and LLM hallucinations reply righthand 3 hours agorootparent [–] I use a debugger, tell that to all the people using LLMs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article \"Demystifying Debuggers, Part 2: The Anatomy Of A Running Program\" explains the components involved in program execution, such as virtual address spaces, threads, and processes.- It details how operating systems use virtual memory and scheduling to manage multiple programs running simultaneously.- The post is part of a series focused on explaining how debuggers interact with these processes to analyze program execution."
    ],
    "commentSummary": [
      "Debuggers are crucial for analyzing running programs, and integrating them with Large Language Models (LLMs) could potentially enhance their functionality.",
      "There is a debate among developers about the potential of LLMs in debugging, with some optimistic about their error-tracing abilities and others skeptical of their current effectiveness.",
      "The discussion underscores varying perspectives on debugging practices and the evolving role of LLMs in software development."
    ],
    "points": 141,
    "commentCount": 19,
    "retryCount": 0,
    "time": 1735004011
  },
  {
    "id": 42499332,
    "title": "Automating the search for artificial life with foundation models",
    "originLink": "https://sakana.ai/asal/",
    "originBody": "Automating the Search for Artificial Life with Foundation Models December 24, 2024 Examples of discovered artificial lifeforms. We show examples of the simulations discovered by our algorithm, Automated Search for Artificial Life (Paper, Website). In Lenia (top-left), ASAL discovers a diverse set of dynamic self-organizing patterns reminiscent of real cells. In Boids (top-right), ASAL discovers exotic emergent flocking behavior. In Particle Life and Particle Life++ (bottom-left), ASAL discovers dynamic open-ended ecosystems of agentic patterns. In Game of Life (bottom-right), ASAL identifies novel cellular automata rules that are more open-ended and expressive than the original Conway’s Game of Life. Introduction For the past 300,000 years, Earth has had only one form of advanced intelligence on it: humans. With the recent advent of AI foundation models, some believe we are at the dawn of a new kind of intelligence. As AI continues to evolve, we may witness the proliferation of diverse intelligent lifeforms coexisting with us. But how did we get here in the first place? What fundamental principles govern the emergence of all life and intelligence, whether biological or artificial? What might the open-ended evolution of the ecosystem of our AI agents look like in the future? Though we don’t yet have the definitive answers to these questions, we can gain insight by returning to the scientific field that laid the groundwork for exploring these questions: Artificial Life (ALife). ALife offers the tools and framework to study the dynamics of artificial lifeforms, fostering insights into their potential behaviors, interactions, and trajectories. So what is ALife? At its core, ALife is the ambitious quest to recreate and understand the phenomena of life itself—how it emerges, evolves, and thrives. It’s not just about mimicking Earth’s biology but going beyond and creating completely alien worlds to understand the principles that underlie all possible life. ALife researchers craft virtual ecosystems, robotic organisms, self-replicating programs, and biochemical simulations to uncover the deep mechanisms of complexity, evolution, and intelligence. Sakana AI has previously drawn ideas from ALife to develop better foundation models, resulting in our works on evolutionary model merging, LLM self-play, and autonomous open-ended discovery. But now we want to go the other way: can foundation models help the study of ALife? Bridging this two-way road will be essential to getting more capable, natural systems and for understanding them as well. Regardless of whether or not you think foundation models will lead to the next generation of artificial lifeforms, they have already started revolutionizing various scientific fields. In fact, the recent Nobel Prize was awarded for radical advances in protein discovery, driven by a foundation model. They are also being used to predict the climate, do AI research itself, and prove mathematical theorems, so why not apply them to help in the search for artificial lifeforms? In collaboration with MIT, OpenAI, The Swiss AI Lab IDSIA, and Ken Stanley, we are excited to release our new paper, Automating the Search for Artificial Life with Foundation Models (website). In our paper, we propose a new algorithm called Automated Search for Artificial Life (“ASAL”) to automate the discovery of artificial life using vision-language foundation models! Our proposed approach aims to (1) find simulations that produce a specific target behavior, (2) discover simulations that keep generating novelty forever as you run it, and (3) illuminate all the different simulations that are possible. Finding lifeforms that produce a specific target behavior: Here, we present examples of Artificial Life simulations, discovered by ASAL, which were found only with specified target prompts. Read on to see more results below in this blog post! Because of the generality of foundation models, ASAL can discover new lifeforms across a diverse range of seminal ALife simulations, including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. ASAL even discovered novel cellular automata rules that are more open-ended and expressive than the original Conway’s Game of Life. Additionally, the use of foundation models allows us to quantifiably measure previously qualitative phenomena, in a human-aligned way. We believe this new paradigm may reignite ALife research by overcoming the bottleneck of manually designed simulations, thus advancing beyond the limits of human ingenuity. What is Artificial Life? ChatGPT-generated image with the prompt “the open-endedness of evolution on earth producing all the diversity of life”. Natural evolution has produced the vast diversity of life on Earth, from bacteria to whales and humans. Artificial Life (ALife) seeks to understand this open-ended process of life by recreating it through computer simulations. While inspired by biology, ALife’s ambitions extend far beyond, exploring not only “life as we know it” but also “life as it could be.” Over time, ALife has grown to study the general emergence of complex behaviors from simple components, encompassing phenomena such as self-organization, collective intelligence, and open-ended evolution. Using ALife, we can study some intriguing questions: What exactly is “life”? What lifeforms can exist in a hyper-realistic 3D world simulation? What does life even look like in a cellular automata? How does life emerge in a digital soup of self-replicating computer programs? If we slightly change the environment’s rules, is life still possible? When is it inevitable? What is required to ignite a “never-ending” algorithm similar to natural evolution? The last question touches on perhaps the most fascinating property of natural evolution: open-endedness. The pursuit of truly open-ended systems capable of discovering interesting artifacts indefinitely remains so elusive that it is widely regarded as a grand challenge. John Conway’s Game of Life (CGoL) is a simulation of a 2D grid with rules like “a dead cell with 3 alive neighbors becomes alive in the next timestep”. With only these simple rules, CGoL is able to simulate entire self-replicating “spaceships”. It can even simulate CGoL within itself! Karl Sim’s Evolved Virtual Creatures is a masterclass in discovering artificial lifeforms in a 3D virtual world. Similar to real animals, these artificial animals have DNA that encodes the morphology and the brain of the animal. Then, the DNA can be evolved with a genetic algorithm to produce phenotypes capable of swimming, walking, and jumping. Other ALife projects: Many more ALife simulations have been developed. Some model ecosystems of cells, while others simulate the evolutionary arms race of predators and prey as they forage for resources. Self-play, which bootstraps capabilities from scratch, is inspired by these evolutionary dynamics. Our Method: Automated Search for Artificial Life (ASAL) The most compelling ALife simulations took months, if not years, to hand-design. This is because the emergent behavior of complex systems is often impossible to predict in advance. Imagine the challenge of designing the periodic table from scratch, manually specifying the pairwise interactions of all elements. Finding the exact configuration that leads to interesting outcomes would be a nearly endless task. Now, imagine instead that you could simply define the number of elements—say, 100—and let an algorithm automatically discover the interaction rules that produce fascinating emergent simulations. ASAL: Our proposed framework, ASAL, uses vision-language foundation models to discover ALife simulations by formulating the processes as three search problems. Supervised Target: To find target simulations, ASAL searches for a simulation which produces a trajectory in the foundation model space that aligns with a given sequence of prompts. Open-Endedness: To find open-ended simulations, ASAL searches for a simulation which produces a trajectory that has high historical novelty during each timestep. Illumination: To illuminate the set of simulations, ASAL searches for a set of diverse simulations which are far from their nearest neighbor. This is the new paradigm we propose for ALife research. Once the researcher defines a space of simulations, or “substrate,” to search over, ASAL automates the search for interesting simulations that, when run, produce videos matching desired criteria, as evaluated by a vision-language foundation model. The criteria include: Supervised Target: Searching for a simulation that produces a specified target event or sequence of events, facilitating the discovery of arbitrary worlds or those similar to our own. Open-Endedness: Searching for a simulation that itself produces novelty in the foundation model representation space as you run it, thereby discovering worlds that are persistently interesting to a human observer. Illumination: Searching for a set of interestingly diverse simulations, enabling the illumination of all the possible alien worlds. Here are some simulations ASAL discovered after specifying a single prompt, which is a text description of the image the simulation should create. In Lenia, prompts like “a self-replicating pattern” revealed dynamic structures mimicking biological processes, while Boids captured emergent behaviors such as “collective intelligence” and “a Fibonacci spiral in nature”. Particle Life produced visually compelling patterns like “cell division” and “a diverse ecosystem of cells”, highlighting ASAL’s ability to turn abstract concepts into concrete simulations that evoke both scientific and artistic intrigue. Multiple prompts can be applied sequentially to find simulations producing a desired sequence of events. In the first simulation, ASAL discovered an update rule which allows a cell to split into two. The second simulation showcases ASAL’s level of control and its potential to eventually discover simulations displaying long and complex evolutionary trajectories. However, our work has not yet achieved this ultimate vision, and more progress is needed before truly fascinating worlds can be discovered solely through specified text descriptions. For Open-endedness, ASAL discovered several simulations which are more open-ended than the famous Conway’s Game of Life (CGoL). If people have found spaceships and computers in CGoL, imagine what we could discover in these new cellular automata worlds! ASAL can also illuminate the entire substrate to find a set of interestingly different simulations as shown here. This provides researchers with a general overview of what may be possible in a given substrate and serves as a step toward taxonomizing all potential life forms within the computational universe. What Now? ASAL is an exciting achievement, but there’s lots more to be done…by you! We open source our code on our GitHub. Try your own substrate We encourage you to apply ASAL to your own custom substrates you find interesting and explore what happens! Here, we develop a novel substrate “Particle Life++” based on Particle Life, but which allows the colors to change as part of the dynamics rule, allowing for a combinatorial explosion in interaction dynamics. Out of the box, ASAL is able to find open-ended simulations in this substrate: It would be very interesting to see ASAL applied to other substrates like ALIEN and JaxLife! Room for creative exploration After discovering some cool simulations, there is a lot of room for creativity and exploration. For example, here, we take the many “species” of Boids creatures discovered, and allow them to enter each other’s universes after a period of time. We can see some symbiotic relationships emerge, while some relationships are more invasive and destructive: Similarly, we can also compare different cellular automata by pitting them against each other to see which one dominates the most territory: Bigger picture Overall, our ASAL framework will allow for the automated discovery of many new kinds of artificial lifeforms. In turn, this will help us understand the general principles of life and all complex systems, adding to our knowledge of concepts like emergence, computational irreducibility, assembly theory, and open-endedness. In the bigger picture, we believe ALife is worth researching because there are a lot of important ideas from ALife that can be and should be incorporated into AI. For instance, the next generation of AI algorithms will likely incorporate concepts like open-endedness, self-organization, and collective intelligence in order to be more adaptive, creative, and continually learn. By bridging ALife and AI, we as a field have the unique opportunity to unlock a new era of natural AI systems. Sakana AI Interested in joining us? Please see our career opportunities for more information.",
    "commentLink": "https://news.ycombinator.com/item?id=42499332",
    "commentBody": "Automating the search for artificial life with foundation models (sakana.ai)134 points by hardmaru 16 hours agohidepastfavorite30 comments upghost 5 hours agoWow, it is really interesting the difference in comments between ALife and AI stories on HN. For some of you out there, there's a great book that really hasn't gotten enough attention called \"The Self-Assembling Brain\" [1] that explores intelligence (artificial or otherwise) from the perspectives of AI, ALife, robotics, genetics, and neuroscience. I hadn't realized the divide was a sharp as it is until I saw the difference in comments. i.e. this one[2] about GPT-5 has over 1000 comments of emotional intensity while comments on OP story are significantly less \"intense\". The thing is, if you compare the fields, you would quickly realize that which we call AI has very little in common which intelligence. It can't even habituate to stimuli. A little more cross disciplinary study would help is get better AI sooner. Happy this story made it to the front page. [1]: https://a.co/d/hF2UJKF [2]: https://news.ycombinator.com/item?id=42485938 reply Y_Y 1 hour agoparentApart from the obvious distinction that many of us on HN are making (or trying to make) money on LLMs I think you've also hit a broader point. There appears to be a class of article that have a relatively ratio of votes to comments, and concerns such topics as, e.g. Programming Language Theory or high-level physics. These are of broad interest and probably are widely read, but are difficult to make a substantial comment on. I don't think there are knee-jerk responses to be made on Quantum Loop Gravity, so even asking an intelligent question requires background and thought and reading the fine article. (Unless you're complaining about the website design.) The opposite is the sort of topic that generates bikeshedding and \"political\" discussion, along with genuine worthwhile contributions. AI safety, libertarian economics, and Californian infrastructure fall into this bucket. This is all based on vibes from decades of reading HN and its forerunners like /. but I would be surprised if someone hasn't done some statical analyses that support the broad point. In fact I half remember dang saying that the comments-to-votes ratio is used as an indicator of topics getting too noisy and veering away from the site's goals. reply rbanffy 5 minutes agorootparent> many of us on HN are making (or trying to make) money on LLMs I’d also highlight the misalignment between creating better AI and working towards AGI and extracting value right now from LLMs (and money investors). reply fedeb95 1 hour agoparentprevthanks for your resources. I am myself concerned with the question of artificial life, and I wonder if it is even possible to search for it, or rather it will emerge on its own. Perhaps, in a sense, it is already emerging, and we humans are its substrate... reply upghost 51 minutes agorootparentI'm not even sure that the goal of Artificial Life is actually \"life\", although that may be the AGI equivalent of ALife -- AGL or \"Artificial General Life\"?. In practice I think the discipline is much closer to the current LLM hype around \"Agentic AI\", but with more of a focus around the environment in which the agents are situated and the interactions between communities of agents. Much like the term \"Artificial Intelligence\", the term ALife is somewhat misleading in terms of the actual discipline. The overlap between \"agentic AI\" and ALife is so strong it's amazing to me that there is so little discussion between the fields. In fact it's closer to borderline disdain! reply Avicebron 34 minutes agorootparentI think it's a pretty natural reaction for a lot of people who have spent a significant of time and energy in the \"artificial life\"/\"AI\" space, I think my first introduction to artificial life was Steven Levy's book (1992) which captivated me over decade ago when I was much younger. Fast forward to now after grinding using \"AI\" in various capacities well before LLMs were what they are today, suddenly everyone and their brother is an \"AI expert\". I guess the best parallel would be (assuming you are a professional full time SWE/IC) if for some reason you were talking to a elementary school students and you find out that that they are all being handed PhDs in software engineering at graduation because no one stopped them and now everyone is an engineer. It's super bizarre. reply mvkel 12 hours agoprevFun fact: Sakana AI is founded by some of the authors of the original transformer paper, \"Attention Is All You Need\" reply sourcepluck 8 hours agoparentWhere's the fun part? I can't exactly imagine throwing this out as an anecdote to entertain a few friends during a sophisticated little soiree. reply jazzyjackson 6 hours agorootparentSome people have a low bar for fun, for example, learning something new that connects to something they already knew, and saying to themselves, \"Neat!\" reply diggan 2 hours agorootparentprev> to entertain a few friends during a sophisticated little soiree Isn't this basically what (we'd like to think) HN is? reply rbanffy 3 minutes agorootparentMinus food and drinks, and in-person interaction. reply vintagedave 7 hours agorootparentprevNo need to poke fun. I found it interesting. Among friends who are interested in AI it’s the kind of random fact you’d throw into conversation. reply fedeb95 1 hour agorootparentprevtry harder reply exe34 1 hour agorootparentprevyou have friends? reply hamburga 12 hours agoprevI actually found artificial life. Crocs. They keep on reproducing effectively and walking around (symbiotically with humans), with some mutation though the polysexual recombination process of Product Manager design reviews. reply vintermann 3 hours agoparentI think that it's a bit silly to call something life just because it resembles stuff you see under a microscope. But I can't deny that it's beautiful. Unlike crocs. reply jeroenvlek 11 hours agoprevIn this context I can also highly recommend the Sara Walker episodes on Lex Fridman: https://youtu.be/-tDQ74I3Ovs?si=1m0JV8gZEl4WFedG https://youtu.be/SFxIazwNP_0?si=R7yZroSNbw5Jjc0H https://youtu.be/wwhTfyX9J34?si=ceXh_aehsjQPklUT reply j0hnb 2 hours agoprevBefore I read the article all I could think about was what if AI was used with SETI's data, would we find something there? reply rrr_oh_man 2 hours agoparentWhat has prompted you to comment, after almost 10 years? reply j0hnb 55 minutes agorootparentNo specific reason, i'm here multiple times a day but I rarely comment. reply nextworddev 9 hours agoprevCurious - what’s the intended product direction of Sakana AI? Is it mainly a research lab or is it doing commercialization? reply diggan 2 hours agoparentIt's a incorporated for-profit company with VC investments, so somewhere/somehow there needs to be commercialization. reply rbanffy 2 minutes agorootparentCan always be a pure-play IP house. reply ribadeo 8 hours agoprevThe name of this company has real meaning in Português which I reckon is unintended. reply jazzyjackson 6 hours agoparentI mean it's Japanese for Fish, but yeah, perhaps we need a database of false cognates sorted by number-of-languages-that-consider-it-vulgar As for Portuguese, GPTo3 tells me \"depending on context it can mean “bastard,” “scumbag,” “dirty-minded jerk,” or imply that someone is a lecherous creep. It’s essentially an insult calling someone sleazy or untrustworthy.\" Would you say that's about right? reply diggan 2 hours agorootparent> perhaps we need a database of false cognates sorted by number-of-languages-that-consider-it-vulgar Or, like most people, we can assume the intent from the context and if someone says \"Use git\", we know they're not telling us to use a bum/rat/scum/whatever but the SCM :) reply theGnuMe 8 hours agoprevAre these cellular automata or Something more? reply readyplayernull 2 hours agoparent2025 prediction: Wolfram declares agentic cellular automaton supremacy. reply fedeb95 1 hour agorootparenthe's slowly building his army that will conquer all the computers in the world. reply wintercarver 14 hours agoprev [–] Congrats David & the whole team! Really enjoy everything Sakana AI produces and always look forward to your research results. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Automated Search for Artificial Life (ASAL) algorithm utilizes AI vision-language foundation models to discover artificial lifeforms in simulations, enhancing the field of Artificial Life (ALife) research.- ASAL has successfully identified new lifeforms in simulations such as Lenia, Boids, Particle Life, and Game of Life, showcasing dynamic patterns and novel cellular automata rules.- This approach automates the discovery process, overcoming the limitations of manually designed simulations, and encourages further exploration and collaboration to integrate ALife concepts into AI for more adaptive and creative systems."
    ],
    "commentSummary": [
      "The integration of foundation models in automating the search for artificial life (ALife) is becoming a topic of interest, highlighting the distinction between artificial intelligence (AI) and ALife.- \"The Self-Assembling Brain\" is a book that examines intelligence from multiple viewpoints, contributing to the ongoing discourse.- Sakana AI, a company founded by the authors of the influential \"Attention Is All You Need\" paper, is focusing on the commercialization of AI, backed by venture capital investments, and is part of discussions on AI's role in analyzing Search for Extraterrestrial Intelligence (SETI) data."
    ],
    "points": 134,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1735008467
  },
  {
    "id": 42498514,
    "title": "Parsing millions of URLs per Second (2023)",
    "originLink": "https://onlinelibrary.wiley.com/doi/10.1002/spe.3296",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"onlinelibrary.wiley.com\",cType: 'managed',cRay: '8f72d825d9202516',cH: 'ZphyRQm6mx__bfiMa7QP5HkJ3mFFSxLUcnDT.kagrPI-1735066932-1.2.1.1-2UBq11oo1K5XwKxb.fEnlvdLJ_tjqBtOmJBPSu7WHE4XdJ_gg9L_85OZqumEGTgx',cUPMDTk: \"\\/doi\\/10.1002\\/spe.3296?__cf_chl_tk=qXIvoSp72u68IKu4QiAtTO18dtxblgOMB.wCpZ0h67U-1735066932-1.0.1.1-3NVMTL2Q2P_uiP.7Zqsm8ZRRikQzoVBIOGZ0mYVLaY0\",cFPWv: 'b',cITimeS: '1735066932',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/doi\\/10.1002\\/spe.3296?__cf_chl_f_tk=qXIvoSp72u68IKu4QiAtTO18dtxblgOMB.wCpZ0h67U-1735066932-1.0.1.1-3NVMTL2Q2P_uiP.7Zqsm8ZRRikQzoVBIOGZ0mYVLaY0\",md: \"EmEIkX3C6qUD7.ptf_CFNBE9kOC7j7jK6SMy8NUl4wg-1735066932-1.2.1.1-dduR_BAqPBDOhbsb4_aIJHaeAjRGDOHH9czlkniiaoh3SOnLIqxf9C.AH9iKNNif81L0UeirPmyfKc06ZeuYD3j6yBRoGL1LqRgU3ySDOQqXCo3NFfd..wiBdWZ9oQNEaqjSRV74.Ydt1Ynv4CgpVmgFZLo3jvQe1GnMgRsmA4QL.GHCU7BIWhhkOdj86oG5TvOcSHVTHhDQtpNK94FhqInBRRXMEIMOoSn5XG.idzG7sPot0M6O0gHvjgGSjuf9PjLR2YFm40TpVGtLpZXqTvFSh8TaoDsW5rMjE3ePnZa1JRTxjyDX2s5V0ekBp22.IEDmoAspLe5q.i.yPm0RJNXQcbc5IPfoUbhfqNXJeRX7aF85rOgSPte2zBU73.or882JJMV6sY9WoaQacULSMwJD3ZjItiI1csdnzfU8T91u4jCmY_n9ZVvp7p3zRT6J988foktfEwAJM1am7wIgs1Y19Y4AgRG1EYAb4zsVFVnA2a6pHTjpsgg5PyyLALQ96GyTHw.XHVofCyeGuvZYwO9SF9q7MYbCK6dwldVJuK2W4X6K3L.l9rviBykbPLqXHNOkr9NEF6OthDFoAEibEn1uye74y.7RjLcIkDyRieZMCGZF1c0NUlEqtsYFV2NpUDYglHBFR9EiuDGjwDQp5LtQKiYYWY_jbo.a2CW8Qv4c9gC3Le8lpij_SvrfqzhDHZQF3fBryelQeiYiWDKAIQ0yHR._pSnsVhoe74K7BdeixWtYfUDtn_ttDb2KEX3TWlx0pfLaZ12BGzQIeB5CNug0aFMlaQk0nCC5XwV832nCfrFQqI6pbiC4SiVRm8Yx8l14BwZTnbuDf_13PPGIm.n6liQc6hEEBpaa.SreAZEM7H5kAZvS2_sau4Rl0GIzEknEPXIvqiiTFSfBSOY7AreEw.MqX.6qHyjqpJ_fVlO2.awHUs2R6sqpNJhMux4R0YQjZ3_A5vtdjyv0zQK4De6FttaNKTc8Y9cv9WX6FwTU4x.goi_KpSE4JDj7rL3DS3UaznMXP2zx8xC48O.wX3Kz5TrQ7WptNaUd82VzfMW7Sn4KTcW8RXHNWnYGJevdvr5k2tedrPOHJxULMiOKIbYPXlPKHNTZa5GSmaQJExLDkmABEh5XJi2hmPp4OGgJUU7C_k.xSm53Bi4oTQkBv8lqsj8lponqA8j3iLFxjpzjIcrwRotDOSG5v9Fst5DNcPzvh.WAVGLmuc8aiT1iarKOZryeTtzQTjD_ADZnfMY76CP2EiZld1ui6O70mhZMw4JvxTqyvmZEu3Xsnow1NQ5HlsVUnynfTiSZFfbzhLS5w7sDOEwRq0yae4hAYb4lG6VoQYYvqsYMgcVPGIp_MlIpKWvATvfui_2ycUSWJPWbVRVbm00O8wC_3eOEqeCvXHb7yxdM3WZrbGfeohkBIQtMJLD6dAfrvgxWoMBZxZjdfhVNHFRayKJ5SuY4pzruZZjyCNtOEqeZv.eg3p3pNccCBCCCRfSfA6c_smh2JrVk_.GhteYUQ.u4OT2uD8aWrn7dIazZH022DsN.ZYkVGoNH4AddKvnBxC6tzY8dPdCdXJgTOd2a2wM3moOU8rdO9eQub5Oi8g5yq7oUh8nt.9uGQ6cgC2oXWtS_qaLihe_5iJfA7cXigLizjXpXfneXRM88uvlA1vzNvt6Wtl1SqDVBhFXMITYXITbsSqJrKi_hIHBPPx4.gfBZpqNby9Bw.Sfet0cObZ3kuqWtG.yjmU7EZutpzdAwt4xNEy3SUIvunnkOVqh18wodG5De26pwkAXWcmCQgWp02bpMJ.rOkVEhjVNL5_dI4zFu7I1gEhQcbEeSU0dg5coTj0cp1vNab7MffjpYpqSsPAbRrkwD2zXDfIG60beOhyzI.xmNmEmfIgGJRaJSf64ENt82IAwBI5n3G5Cim.SGODf7S_JR3TUaY0AJySVUH4w5bmzuAZXSTVoZL0HmezYlUaYwfTVuujU26aPCtrLlnlm0ZQHzJ0nztgFCwukDNNVUZQKRI8RC17ylmrLe0h4KMRB_fgcmDba1a37xigdxaHdt.tY.MZSlW7uP.IxFmqPUnLtV7rNxYNB0sYvfJ5lAyMs4sQ4uKgzwMccy77ZFUFqnwD7c_ENRAx9Kr3pJdOg1mgBirUwtd.FMMrqwBeQ_hZ7vIAjUivPx3rt1bOEo0pfCiU5OCweKRfVXl8iUwyEUock5gdWrPXYPZkNzifbnb0B2hjsLNMwtkIQAuUuabxAgMSzJ32IEzuOu4q.8oJL7JupEU7OpXrSSwiPAJdWh8P_rQ4UPFx0EHMPRwi7qGNwmIggzYgdTK4X0IimwZz4GNCFucGD4MADwfjiS9CYA3JHECi4NJf8p_DT.S9nH1CXwI5DeXo9RhF0JjnrV.g5xON8vCfNFbaxOdEih8y8EfQbqM5mcAOGuwHB9FZeM1Zn5GfrUiB66lOEV2i6BIb515i4dt.Jje5fQI2hexNur_Twyd88sg9dOCjscz7o3D7u4Ypc.AW_ZsGd4el5XB6okeKbISvJ3qAwiYWtJctKwodbBlcwuxpiSfREAjbtiFHW.UMPT1g\",mdrd: \"8IaD0qwB_r6QPYZTWid0exwLVUPLkOIiurhxEiBpG18-1735066932-1.2.1.1-3n1xtXsM6VZ.UHnZ1Gisbba9onwHxrToKtW2A4f8ilu4UKNcazksPBJR5k6liVlWXRIVHAydkti938xnwT0Wp7Mu11TRkUe7vEyN3ClkAIOHYLfZkvDuLglZcjZVEKYZ3LZ24U23SG8q_wGxqo8yWiW4RCmSxBvIX3WyY881Zty6qupJYtVxBbYPfcIaSVLycOeKGr1Hx_utq7auTkhMVNcRNWZbe9XGO4qrIGwW6kK6WD5UDeMR3FnldMr_do6DKn9BQ.XLU9SWeO1rCwYVmEZVqzx6PeSI3Pi4lT8yZ_NvgBo7ilxuZJZrEuxwu_lW0QVWoT8DPXG9bcpqgh6n5n6M3vXN.dY6b22ZgwVvngw20hGWfftz2v6.4Bfqsc60LwEfvddUa4duvT8pNI.j151Ty4rXaLCwQHse9p9jG4weARsHvEPns556cO3pNmlCWtxHAjL.nvJaTzeIbfNmj3qcdSBQrdKq809w57xq7R.jvDiTDF.3HMIopymX4Pd2dUtWIy.Uk070Cv9HSzZ1pNLksupdWLaA9iQH9IK5kWYoNxJZ3GJYemrtkfZG5BJv_lvGiy.VhP4zVTmjYLuU.HfEAGYV_Y_PsDiJ6uEeRktqB6FYhyA0ZubgN79FSOJKzwrD3VvrQtKvRclB__s9Kz9Z0ZO11Bkc2Iz7k0obfvH_dCjd.FlgP47WsuWC2Mz55TN7LXNxcxd7jredbeM9FDgQe6pb4vz5jBDGR0l_UaVGNfWBvg_bNbf7M8c7AfeQWS0WXmzBVQrmgYEEnOukQh5.mKzuL9cvT3sIuL9sptq8AMtHTVz.yLdsBG5en8HRC6YqMJKiMT6D6jiz1ULZOP2Cs569QhWNO.FVVVBwHqrOZYsWjeyycNmb8QbmoEmagj1JqEGh4nP9wJLnNkjoEEX8zl34C_wKg49tCklqZxutTrcVLX_fgRrsyUvMtdXqiLW1EwVbUh2sQPTlTXlMZ.CbrUXP72eWIikguvfyYbjxK_a57Lwsw0ahOVEphn5zj_cuR1hQsbFxmwG2UMOEPJwJsQFf1zN0dmJiTJ3bVLjGyc2myGFfmp0Cy3twZISGmXFGjS7.NGzyQ7qWxsQNZm77Wr2TRGqJe_HauLnS4ZUBidXc1qKgxRolVD8lRLIyVa613U1jqofNnXZ5iIPHMjXXba6FQV5erYPbTtlzPdDSoV.uR28y97ePyM3bN69DCKbzgH9mVlojds44C85dQVQqPzlzbvy5m8GBC5bO0QBor9RyhB3dofN6wb6rQ9yea8P_fzP7YSxwwH2Y1zUBK0QFShjIjy0cVvk0XiEgM1sHxEAco5QlaeNYikKgrWc7DJyRU.jj4pNUBMJnTHvbFT4hx6oSkdTm5_boms0h.x510YSAnVRqWQjiTdM4t6qSyQh6PxmRgtKQIz8BsArsHkNKfyKuqH9.RA4JReSD7us0S8Yge6ujmtrvnKBctUssNXK7zTuyVHKOAjFgDrhEpYAjWlSiaQK2yldonpIFLteHZgq1vCCznoZm6Ok7SRayBHccCKSoO4CsEixJab.TypnBCHSIhvmw9s.A8qT6QxqBr7c1W2XUHIvRpPlQYpq1fXKgcFn71nfoNyUJYOAvlsmB2VHXGTKACWj7ZgUlutSbXLX3y1nU5_zL04tRT.jvOVVMo_4jsxYAQTSz9LG0Umi5yzZd5f0vaTQlr6ywKekjrYnWdOgL3N56lRWVN7.zpFRNMNQSbKkkH6CEDnDl5XY8Wtoc9NqDEiGDXVOvKpAvaTZCR2HNR.9oUp3HUF5ALHBrxLK6wjNNbgPg7oI1zsBvyYCBe.xuqWYFMZdhdS6Z5g5dP4w0MgxGWt_Oq.lphZecYpo3rC27OdeSUt40ytYpLMzgIrJ0U4cSZibwP9ae0UgUHnN_TKoZ328ifZ8lALginRbszICcBodhNpXMBK5pcNuw625TFUWih_BaGiRlT1X4vd4xquQqDuTjDgniqqpVaGhkN73gp3DXV0T2R5kKC9.CWlygTbpvj1lV6Hwtxy86.MoyBmyESqVNYLW6I9HY6wq7rS6DuJKtBOAS.lQP2IeuFbXesLrfsnG34Oc694RI49aF71YVHRwM2bYa0hSckiRD3HDoIEOm3DmGEVyMdIQDlSr0posS14hfkcTTXMN4w2Q3qIZKk9H3mrGizpVDZLMomP.6VQg63YJ39Am17zkc.JvgiY3b6qx0wz.dUFqbTF6WVJOLZ_qRcc1ELWdVSFMCZMGnGgfVtblOAxbHWk_BBQ4z483brrprRkgG3D301a5fV9qB72rGAKCoRZvyroxWBEJYMt.QMJuuafpuonXSIMGU1V3XSHjLxrMVyFRMo.u6OsYe6D3dlxqEA3rbh8HXq94HGt7PdFk1QVpb9gTGN32oNQ3xJM3voPX1BX7_Ph4tr_5u3bovIlq4\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8f72d825d9202516';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/doi\\/10.1002\\/spe.3296?__cf_chl_rt_tk=qXIvoSp72u68IKu4QiAtTO18dtxblgOMB.wCpZ0h67U-1735066932-1.0.1.1-3NVMTL2Q2P_uiP.7Zqsm8ZRRikQzoVBIOGZ0mYVLaY0\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42498514",
    "commentBody": "Parsing millions of URLs per Second (2023) (wiley.com)131 points by PaulHoule 19 hours agohidepastfavorite34 comments Thorrez 9 hours ago> For example, the input string http://xn--6qqa088eba.xn--3ds443g/./a/../b/./c should be normalized to the string https://xn--xn6qqa088eba-l19f.xn--xn3ds-zu3b/b/c Why would normalization change http:// to https:// ? reply chrismorgan 7 hours agoparentThere’s got to be some accidental mangling there. Somewhere. Because of that error, and still more because of the blatant error in the next sentence: > For example, given the base string http://example.org/foo/bar, the relative string http://example.com/ leads to the final URL http://example.org/example.com/. That’s just… no. I do not believe I have ever encountered any software which would parse it in that way, and I refuse to believe such software ever existed. It would be . But the PDF matches the HTML. I dunno, something weird is going on. Look at the hyperlinks there, too, “http://xn--ivg but not the rest of the URL that follows, and how the -- has been changed to –. Something went wrong somewhere in the editing or publication. reply silvestrov 3 hours agorootparentMy guess is that the html formatter changed the text \"example.com\" into \"http://example.com\" to make it a valid absolute URL. reply ramon156 7 hours agoparentprevBecause its 2024 reply marginalia_nu 7 hours agorootparenthttp:// is not a typo for https://. There's still a fairly large amount of web servers that do not talk https, and you simply cannot assume that they do. That will leave you with a lot of dead links. Besides, most that accept both will auto-renegotiate to https. reply TacticalCoder 7 hours agorootparent> There's still a fairly large amount of web servers that do not talk https, and you simply cannot assume that they do. OTOH I'm browsing since years forcing HTTPS only and life goes on fine. If the absolute worse comes to worse, I can use archive.is or archive.org but it's very rare that I need that. Basically: if a link is HTTP to me it's not worth opening. The one exception would be Debian packages URLs: but these are signed and the signatures are verified. User _apt is the only one allowed to emit HTTP traffic. This prevents my ISP or anyone else injecting nasty stuff. reply forgotmypw17 5 hours agorootparentJust because it is accessible to you does not mean it is accessible to everyone else. HTTPS has many failure modes which make it unreliable for essential access, such as time mismatches, certificate expirations, ssl version mismatches, etc. Security and privacy are important, and they are also not absolute. Sometimes the risk is outweighed by the importance of being able to access essential resources and reading material. reply Analemma_ 5 hours agorootparentprevUser preferences should not be encoded into parser behavior, that’s nuts. You wouldn’t just arbitrarily change an ftp:// link to an imap:// link, so why would you accept it here? That exists at a whole other layer of the stack. reply dmd 4 hours agorootparentThey would arbitrarily change an ftp:// link to an sftp:// link and then complain that it didn't work. reply grayhatter 3 hours agoprevI wonder how much time was spent promoting this parser, vs time spent on writing it? I've seen a lot of spam for this one, and I'm not the only one. https://daniel.haxx.se/blog/2023/11/21/url-parser-performanc... reply Cicero22 17 hours agoprevThis sort of work is something I wouldn't be able to do, but I can't help but point out at least one potential issue with the paper. It's a lot easier to find problems than solutions I guess. Are the benchmarks comparing node versions valid to conclude a real world performance increase? one possible confounder is the version of V8. https://github.com/nodejs/node/blob/v18.x/deps/v8/include/v8... https://github.com/nodejs/node/blob/v20.x/deps/v8/include/v8... ideally, they would've patched Node 18.15 with their changes directly and test their patch against 18.15. reply maartenscholl 11 hours agoprevI had a lot of fun writing low latency parsers for various message standards C++. There are a lot of fun things you can do when you can take ownership of the read buffer and you can figure out how to parse in-situ (modifying the data in place as you move along) reply beached_whale 16 hours agoprevFound the easier to read/download from Arxiv link https://arxiv.org/abs/2311.10533 reply notamy 18 hours agoprevThe title seems to have a few words missing. Original title: > Parsing millions of URLs per second reply HL33tibCe7 18 hours agoparentHN’s stupid/arrogant automatic title rewriter strikes again reply kristianp 14 hours agorootparentI've never noticed a title being rewritten automatically when posting an article. Are you sure that's really a thing? reply pests 12 hours agorootparentThere are some auto rewrite rules. Off the top of my head: numbers in the beginning are stripped, [pdf] or [video] can be added to the end, and one more I can't remember that gets stripped off beginning and can cause confusion. A pdf link to \"5 Reasons To Do Things\" will be \"Reasons To Do Things [pdf]\" for example. reply Tomte 9 hours agorootparent„How“ at the beginning is stripped, leading to all these strange sounding „I “ submissions. reply Tomte 9 hours agorootparentprevYes. And the algorithm is really incredibly stupid, but dang is opposed to even small improvements (like showing the changed title on submission beforehand, like the „x characters to long“ message). reply PaulHoule 18 hours agorootparentprevFixed reply ignoramous 8 hours agorootparentSo, suprassing 80k karma, one gets title edit rights? reply PaulHoule 5 hours agorootparentI think anybody can edit a title within a short time of posting something. Or if there is a karma threshold it is way less than 80k. I caught that one manually but YOShInOn's tail end needs some love and could be updated so it that it fixes up titles that get mashed automatically or adds a comment sometimes to editorialize or provide an archive link. reply youngtaff 8 hours agoprevLemire’s blog is well worth a read if you’re interested in this sort of thing https://lemire.me/blog/ reply TZubiri 16 hours agoprev [–] The title doesn't sound impressive at all to make me want to read this a regex can parse millions of URLS on a home computer (say 4GHz, you get 4000 cycles per URL.) These things were desgined to be parsed. reply wiseowise 9 hours agoparentMaybe you should’ve spent 2 minutes reading the article instead of arrogantly dismissing it with layman knowledge. reply fabrice_d 16 hours agoparentprevThe article explains optimizations to spend less cycles parsing URLs than other libraries. Very interesting work, there's no reason not to do things efficiently when it's possible. Also, good luck using regex to write a RFC or WHATWG conformant URL parser. reply TZubiri 16 hours agorootparent2 minutes reading an rfc about uris and I find a regex literally used in the specs: https://www.rfc-editor.org/rfc/rfc3986#page-7 >> The following line is the regular expression for breaking-down a well-formed URI reference into its components. >> ^(([^:/?#]+):)?(//([^/?#]))?([^?#])(\\?([^#]))?(#(.))? Did Chomsky die for nothing? I guess there are reasons to do things efficiently when possible, but a million URLs is not it, the adage about the root of all evil comes to mind. A billion URLs per second and it's almost interesting, but not really. reply yagiznizipli 15 hours agorootparentRFC 3986 is a lot simpler than WHATWG spec. You can literally write a zero copy 3986 parser whereas you can’t with WHATWG. (And Ada is still faster than 3986 parsers) reply runlevel1 1 hour agorootparentprevThat doesn't normalize the URL nor does it handle relative URL joining logic. It also doesn't handle URLs like: `file:///foo.txt` reply switchbak 15 hours agorootparentprevLast I heard Noam Chomsky was still alive, and a quick Google doesn’t contradict that. Or is this some kind of high brow joke that went over my head? reply FroshKiller 15 hours agorootparentprevThe spec provides an expression for \"well-formed\" URIs. Good luck with real-world input. reply paulddraper 16 hours agoparentprev [–] That will get you only one million per second. And depending on the length of your URL, 4000 will not be trivial, depending on the output format. reply metadat 14 hours agorootparentCorrect, URLs can be something like 4,000 characters long in 15 year old Firefox. I wonder what the current maximum length is? Today, Chrome supports 32,768 characters.. good luck processing that in 4,000 cycles! It'd require SIMD or some other fanciness. reply yjftsjthsd-h 15 hours agorootparentprev [–] > That will get you only one million per second. But per core, right? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussion focuses on the challenges of parsing millions of URLs per second, particularly issues with URL normalization and parsing errors.",
      "There is debate over the necessity of changing \"http://\" to \"https://\" and the reliability of HTTPS, with some skepticism about the parser's performance claims.",
      "Participants also discuss the difficulties of writing efficient parsers, the limitations of using regular expressions (regex) for URL parsing, and the complexities of adhering to various URL specifications."
    ],
    "points": 131,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1734997688
  },
  {
    "id": 42500704,
    "title": "Paris to Berlin by train is now faster by five hours",
    "originLink": "https://www.theguardian.com/travel/2024/dec/24/paris-to-berlin-by-train-faster-service-via-strasbourg",
    "originBody": "View image in fullscreen The new ICE service arrives at Strasbourg. Photograph: Hidden Europe Rail travel Paris to Berlin by train is now faster by five hours. We try out the new service The new direct daytime route, via Strasbourg, sweeps through enchanting landscapes from vineyards to forests. But some say it needs to be quicker still if the train is to be more attractive than flying Nicky Gardner Tue 24 Dec 2024 02.00 EST Share The Guardian’s journalism is independent. We will earn a commission if you buy something through an affiliate link. Learn more. U ntil last week, my most recent trip from Paris to Berlin on a direct daytime train was way back in early 2015 with RZD Russian Railways. The train had a Polish restaurant car with a quirky menu featuring caviar, kangaroo steak and South African wines. The journey between the two capital cities was scheduled to take 13 hours, and despite dawdling along the way, we arrived in Berlin bang on time. It was a day with breakfast by the Seine, lunch (without caviar) on the train as we crossed the Rhine and nightcaps by the Spree in Berlin. More recently, the only direct service between Paris and Berlin has been a thrice-weekly overnight train. Branded as a Nightjet, there is nothing jet-like about it. It takes even longer than the Russian daytime train did 10 years ago. But as of last week, there is now a faster direct option. Germany’s national rail operator Deutsche Bahn and its French counterpart SNCF have just launched a much faster direct daytime Paris to Berlin train. View image in fullscreen Passengers prepare to board the first Paris-Berlin high speed train line that started on 16 December. Photograph: Aurélien Morissard/AP I join the debut service from Paris on Monday 16 December, which clocks a travel time of 7hrs 59mins, symbolically important in ushering in a new era of rail travel between the capitals of the EU’s two most populous states. Fast, but some say not fast enough. On board are assorted media and rail industry officials, plus some regular travellers who unwittingly find themselves bit-part players in a piece of railway history. An Australian family on the train is surprised to find garlands, ribbons and gifts. “It’s our first train in Europe. Will we get this on every train?” they ask. This new direct Intercity Express (ICE) train from Paris to Berlin leaves from the Gare de l’Est, one-time departure station for such illustrious trains as the Orient Express. On Platform 29 a German ICE train, handsomely turned out for the occasion, is ready for boarding. Along the way, travellers experience a fine sweep of European geography as they travel from the Seine to the Spree The new train serves just three intermediate cities en route to Berlin: Strasbourg, Karlsruhe and Frankfurt am Main, in the latter case serving a secondary station (Frankfurt Süd) in the city’s trendy Sachsenhausen district. Departure from Paris is at 09.55 with arrival in Berlin Hauptbahnhof scheduled for 17.58. Along the way, travellers experience a fine sweep of European geography as they travel from the Seine to the Spree, crossing six other major rivers along the way: the Moselle, Meuse, Rhine, Neckar, Main and Elbe. I am struck by the engaging mix of landscapes. On a fast run east from Paris, the train speeds by Champagne vineyards then crests the Vosges hills in the Saverne tunnel before dropping down to Strasbourg, reached in just 1hr 45mins from the French capital. The Russian train in 2015 took over four hours merely to reach Strasbourg. The city is keen to promote its position as Capitale de Noël, but on this particular Monday, Santa is sidelined by an enthusiastic group of flag-waving Strasbourg députés delighted to see this first direct daytime train to Berlin for nine years. A few minutes beyond Strasbourg we cross the Rhine, arriving in Germany at Kehl. Back before the days of Schengen, even the Orient Express was forced to stop at Kehl for customs and immigration formalities. Our ICE to Berlin glides through without slowing. Once in Germany, progress is more measured than on the initial fast leg from Paris to Strasbourg. As we slip through reedy flatlands towards Karlsruhe, with misty views of the Black Forest away to the east, I repair to the restaurant car for that most quintessential of German delicacies: currywurst and chips. View image in fullscreen The new service reaches Strasbourg from Paris in less than two hours. Photograph: Hemis/Alamy Karlsruhe comes and goes, and now we are sedately running along the flank of the Odenwald, a range of forested hills dotted with attractive villages and vineyards on their western slopes. The Odenwald is one of those blocky uplands, not particularly mountainous in character, which dominate central Germany. Beyond Frankfurt, our train follows main rail routes north-east towards the Fulda Gap, taking advantage of a lowland route between the much higher Vogelsberg (to our left) and the Rhön uplands (to our right). Fine views of lovely hill country, then at Kassel we pick up a high-speed line to Hanover, where we swing off to the east and leave the hills behind as we make tracks across the North European Plain towards Berlin. The beauty of this journey is that you get a real feel for changing landscapes. How long will it take before an hour or two can be trimmed from the journey, so this new link can offer real competition to air travel? Paris to Berlin as the crow flies is about 550 miles. The route taken by the new train runs to 770 miles. So our train averages 96 mph from Paris to Berlin, which is not particularly “high speed” by the standards of French TGV services. View image in fullscreen The service reaches Berlin at about 6pm. Photograph: Andrey Denisyuk/Getty Images Winter darkness falls before Wolfsburg where our speed is tempered by a slower train on the track ahead. We slip by the striking Volkswagen factory in Wolfsburg, partly festooned with Christmas lights and a huge Santa Claus beside the railway. A few moments later we cross the former frontier between the two German states and dash through sparsely populated terrain to Berlin. Upon arrival at Berlin Hauptbahnhof, happily a few minutes early, there is a low-key but joyous celebration of all things European. Two leading capitals now linked by a direct and very comfortable daytime ICE train gives good cause for many smiles and handshakes. But many will still ask just why it has taken so very long to launch this service. And how many years will it take before an hour or two can be trimmed from the journey time, so that this new link can offer real competition to air travel. Winter rail adventures in Europe: three itineraries for the colder months Read more Way to go The new direct ICE leaves the Gare de l’Est in Paris daily at 09.55 and Berlin Hauptbahnhof at 11.54. One-way fares from €59.99 second class or €69.99 first class. On such a long daytime journey the first-class upgrade, if not excessive, is probably money well spent. Purchase online at int.bahn.de or raileurope.com. Interrail passes are valid, but pass holders need to pay a supplement (usually €19) on journeys to or from Paris or Strasbourg. This can be bought on Rail Europe. Nicky Gardner is lead author of Europe by Rail: The Definitive Guide (18th edition, Hidden Europe, £20.99), available from guardianbookshop.com Explore more on these topics Rail travel Berlin holidays Rail industry Public transport trips Paris holidays Germany holidays Strasbourg holidays features Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=42500704",
    "commentBody": "Paris to Berlin by train is now faster by five hours (theguardian.com)108 points by teleforce 9 hours agohidepastfavorite101 comments kidk 8 hours agoFrom city center to city center by train vs airplane Rough estimate for a flight: - 36 min train from Gare de l’Est to Paris Charles de Gaulle Airport - 90 min arrival before your flight (security, check-in, gate closes before flight leaves, ..) - 100 min flight time - 20 min getting out of the airport - 30 min taxi to Berlin Hbf (assuming no traffic) I think if you have large bags you can add an extra hour (max) even as check-in will take longer, and you'll have to wait for your bag. Which makes it 8,5 hours vs 4.6 hours, or 5.6 hours with bags. (I added 30 minutes to the train as you probably need to be there a little early.) Having done this a couple of times (Amsterdam to Berlin, which is about the same) with both modes of transport. I have to say I prefer the train if I have the time. Much more relaxed way of transportation. You can work the whole way there, and there are ok lunch options on the train. reply chii 7 hours agoparentThe majority of the air travel time is spent in the airport, doing \"administrative\" things. If air travel could be more streamlined (aka, no need for things like security, check-in, like how a train would be), then trains are absolutely uncompetitive on speed. The only advantage of a train would be that you could more easily sleep in one - so an overnight trip is less tiring, and you save on cost of an extra night of hotel stay. reply prawn 6 hours agorootparentI also find all the interruptions while you're on the plane itself tedious. Waiting for all the luggage wrangling, the safety briefing, the welcome from the crew, the message from the captain, the announcement about food or drinks, another message from the captain, various seatbelt-related announcements, something about duty free, and on and on. And they're all in that verbose style, like ChatGPT. Makes using any in-flight entertainment miserable. reply Scarblac 7 hours agorootparentprevAnd there's more to see out of the window, and you can take breaks (leave the train for a couple of hours to explore a city along the way, then continue on the next). And of course it's much better on CO2 emissions. But trains need a lot of infrastructure and long delays are frequent. reply nh2 6 hours agorootparentOn most train tickets you cannot hop off for a couple hours. That would require a new ticket. reply GJim 6 hours agorootparent[citation needed] This is quite common in Blighty; there is no need for another ticket (outside some time restricted fares where the ability to stop is limited). Indeed, it can be nice hopping off at a new city for a mooch around if you have time to do so. (Only slightly off-topic; We have the phenomenon of 'split tickets', where it can be cheaper to buy two tickets from A --> B --> C rather than one ticket from A --> C even when the train stops at B anyway! You don't even need to get out of the train at B if you don't wish to do so. Our ticketing system is a mess.) reply nh2 5 hours agorootparent> [citation needed] All saver tickets in German (which is what most people use), and most TGV tickets (even non saver) are bound to specific trains at specific times. https://www.bahn.de/faq/was-bedeutet-zugbindung The UK is the odd exception here. But it also doesn't have real high speed trains, except from the one that leaves the country, where hopping off is impossible (unless you want to end up in a dark tunnel under the sea and be arrested afterwards). reply switch007 4 hours agorootparentprevAdvances are also extremely common in/to/from England and sales are on the up. The government is working hard to replace off peak singles with advances. Flexible tickets are becoming unaffordable for certain journeys such as those on the ECML with LNER (aka The Government) reply athrowaway3z 6 hours agorootparentprevDoesn't this kind of argument work the other way around as well? If we get rid of other considerations, and train travel could be more streamlined (strap a jet engine on a train, no need to gain altitude), then airplanes are absolutely uncompetitive on speed. reply jules 6 hours agorootparentThere is a difference between changing administrative things versus fighting physics. reply chii 6 hours agorootparentprev> strap a jet engine on a train well, hyperloop tried and failed. It's just not practical. reply marcosscriven 6 hours agorootparentprevThis was always the upsetting thing about Eurostar/Eurotunnel between UK and France. In my naive teens I dreamt of how wonderful it would be if we joined Shengen and could literally just hop on the train to Paris. But sadly we went the other way and completely left - and now it’s as bad or worse than any airport. reply nh2 6 hours agorootparentWhile the Eurostar pre-boarding times are not great, they are much better that an airport. I usually arrive 40 minutes before the train starts moving, including large luggage (30 minutes before is the limit at which the ticket gates close). There is no waiting for you luggage on arrival because you always have it in your hand. This is also possible because the distance between check-in gates and the train is only around 50 meters in all 3 stops (London, Brussels, Paris). On those 50 meters you have the luggage scanners, passport control, and the waiting hall. Compare that to kilometers of walking at Heathrow Airport. reply snapplebobapple 2 hours agorootparentprevthat experience still exists flying private. You submit a passport photo the day before for prechecking then get a quick verify at the terminal. Its max 45 minutes from arrival to walking to plane and going. There is no reason we couldnt be doing that for all flights combined with marginally better pilot security. We arent because government has been (n the path of stupid for so long that it cant seem to change reply rf15 7 hours agorootparentprevfor frequent flyers, radiation exposure is also a factor. reply hyhconito 7 hours agorootparentThis is usually a misunderstanding of risk. I think the last stats I read were that per 18,000,000 miles of air travel for 200 people, you'd expect 1 extra person to end up with cancer. Over a baseline risk of 25% of people getting cancer, that's negligible. reply rand0m4r 7 hours agoparentprevTotally agree. I used to go to Holland and Germany from France back when I was living there and even though at the time it would take longer (slower and less trains) the trip was totally worth it: I even met people who eventually became friends. Now, when flying from Spain to France (every couple of years), even though the flight is less than two hours, the entire trip is more than four, and much less fun or enjoyable as far as I'm concerned. For the moment, it's just easier to take the plane but that's supposed to change... We'll see. reply prmoustache 8 hours agoparentprevAlso, if you don't want to go to the bar in the train, you can totally picnic without having to pay airport level of extortion for drink and food. You can bring your own food on a plane but not drinks and if you are having lunch in the plane but it is just not that comfortable, better doing that in the terminal. reply pantalaimon 8 hours agoparentprevThere is also a night train, which I find even more convenient for long distances. Sure it’s slower, but you’ll spend most of the time sleeping and it saves you one night at a hotel. https://www.nightjet.com/de/reiseziele/frankreich reply martijnarts 8 hours agorootparentThe night trains are great. It's quite comfortable, you get there in the morning allowing for a full day (after dropping off your bags at the hotel) of work/tourism, and I especially hope Nightjet and other night train companies converts all of their trains to the modern mini cabins[0]. They're incredible. [0]: https://www.seat61.com/trains-and-routes/nightjet-new-genera... reply usr1106 7 hours agorootparentComfortable and comfortable. It's certainly much better than spending a night on a seat. But most people won't sleep well. It's noisy and either too hot or too cold. The bed is hard, narrow and often too short for tall people. I have spent many nights on trains in various countries, seats, floor and berth. I will prefer a couchette or sleeper if the price is somewhat reasonable. But I won't live in the expectation to get something comfortable. Just less uncomfortable. reply orwin 7 hours agorootparentprevNight ferrys and night trains are my favourite way of travelling when on vacation. reply hyhconito 6 hours agorootparentprevDoesn't work for all of us. I can peacefully fall asleep from the noise of a CFM on an Airbus. On a train, rickety being chucked around all the time and always too hot, nope. I can do 10h on a plane fine. 3h on a train does me in every time. reply rplnt 7 hours agorootparentprevUnfortunately more expensive than a plane+hotel and has to be booked well in advance. It's worth the experience, at least for me, but it's not a real alternative. reply pantalaimon 7 hours agorootparentWhen I click on a date on that website, e.g. 16.01. I see tickets for 69.90€ on the coach wagon from Berlin to Paris. reply rplnt 4 hours agorootparentWhen you said night train you can sleep on I assumed sleeper cars. But to be fair, I always looked at trains to/from Vienna, maybe that's the exception in prices. reply pantalaimon 4 hours agorootparentCoach is a sleeper car, you just share the cabin with up to 5 people on bunk beds. Although on newer carriages (which I think is the one that goes to Paris) those are sleeping capsules where you have more privacy. I took the trip to Vienna in one of the old bunk bed cars for 60€ and it was fine. I liked the experience of chatting with the fellow passengers in the cabin that you usually don’t have on a normal train ride. reply weinzierl 7 hours agoparentprev\"I have to say I prefer the train if I have the time. Much more relaxed way of transportation.\" Me too, but I worry a bit that it might change. A big part of the inconvenience of flying is the security checks[1]. I only see these ramped up for trains as well. Look at the EuroStar for a glimpse into the future. I am afraid it will only take one bigger attack and we will turn train stations into airports security-wise. [1] The other big one is airports being far from centres. reply Symbiote 1 hour agorootparentThe Channel Tunnel is unique in Europe for being a long, undersea tunnel. No other train in Britain or France has security checks. reply jltsiren 6 hours agorootparentprevThat won't happen outside a handful of high-profile routes. Railway stations are far too small for security checks with the passenger volume they are handling. Terrorist attacks on trains and railway stations are not that rare. The last major one was in November in Pakistan. reply em-bee 5 hours agorootparentdon't think that will stop them. if china can do security checks with xray of any bags in the subway, then trains in europe can do it too. reply noprocrasted 6 hours agorootparentprev> Look at the EuroStar for a glimpse into the future This bullshit also happens in Barcelona Sants for the long-distance trains. reply teekert 6 hours agoparentprevSame for Rotterdam - London, the planes don't fly to the city center + the Train is a premium experience (power, wifi, second class seats on train similar to first class on plane, little cafe to walk to and eat drink, arrives in the city center). All in all this make the time similar but the experience by train much better. reply hyhconito 7 hours agoparentprevIndeed that is the case there but you have to evaluate every journey based on a number of factors. Recently I had to get a train 250km in Italy and it would have been quicker, cheaper and less stressful to fly to London and back due to the problems. reply thefz 5 hours agoparentprevNot to add the comparatively smaller CO2 footprint reply tgv 7 hours agoparentprevThere is a lot less capacity, though. And I don't see how that can be improved without massive investments. reply jotaen 8 hours agoprev> One-way fares from €59.99 second class or €69.99 first class. The “from” bit is worth to stress here: with DB (German railway company), one-way fares may be up to €233.00 for second class or €384.00 for first class, depending on demand, the cancellation policy you choose, and how long you book in advance. Seat reservations are extra. reply barrkel 6 hours agoparentI travel from Zurich to Hannover fairly regularly (about 7 hour train journey) and it generally costs 59 to 89 EUR each way first class, booked supersaver (fixed train) a week or two in advance with a 1st class Bahncard 25. The return is usually in the region of 140 to 160 EUR. From my point of view, DB trains are cheap, and first class is reasonably comfortable with a power point and meals ordered to your seats. Punctuality is a different matter of course. (I just checked the prices for a late January return trip I'll be doing, and right now it's 100 EUR return, 53 / 47 for the legs.) reply em-bee 5 hours agorootparentas a frequent and spontaneous traveler, i was never able to book fixed trains so far in advance. this inflexibility adds stress that i really would like to avoid. it ruins the experience because it takes away the spontaneity reply RandomThoughts3 8 hours agoparentprevI pay more than that to go back to my hometown by TGV which is less than 200km away from Paris and that’s with a subscription giving access to preferential prices. There is no way Paris-Berlin is going to be this cheap. From experience I expect the train to be at least twice more expensive than flying. Having worked a few years with SNCF, it can’t be otherwise. It’s the most mismanaged company I have ever worked for, gangrened by unions which fight tooth and nails to preserve advantages which reasons to exist disappeared decades ago. Unions review the full trains planning with management before it’s validated and veto any optimisations which would cut overtime or might impact compensations without any regards for customers. It’s revolting. reply rand0m4r 7 hours agorootparentBefore leaving France I often travelled by train and felt exactly the same way: a total ripoff for the service they offer...In Spain you'll also get trains that arrive late but they're usually cleaner and the price is much more affordable, and they don't go on strike as often as in strike land. reply noprocrasted 7 hours agorootparentAt least French trains actually work most of the time. UK is just as bad pricing wise but the service is so much worse. reply simgt 8 hours agorootparentprev> gangrened by unions which fight tooth and nails to preserve advantages which reasons to exist disappeared decades ago. Unions review the full trains planning with management before it’s validated and veto any optimisations which would cut overtime or might impact compensations without any regards for customers. It’s revolting I've heard that before, but usually from the crowd that wants to privatise the whole country anyway. I understand that you're claiming to have witnessed it, but do you have some sources? Surely, the amount of money they pour onto Cap Gemini and many other consulting firms to rebuild existing systems instead of having teams in-house who could maintain them must have an impact on ticket costs... reply RandomThoughts3 7 hours agorootparent> I understand that you're claiming to have witnessed it, but do you have some sources? I have witnessed that and more like optimisation software being disabled to avoid cutting into overtime. Just go take a look at how drivers are paid and where their comp comes from. It’s all public knowledge. Take a look at the recurring reports from the court of auditors or talk with anyone working there. We are talking about a company in which the two main unions are openly Trotskyist and which has a permanent strike warning by rotating between union to open one in defiance of French law. This is not drum beating for privatisation by the way (even if all the formerly public French companies I had the misfortune to work for are fairly mismanaged but nothing as bad as SNCF thankfully). I am sure with enough reforms it could work while being public. I mean Singapore manages so it’s not impossible. It’s just impossible to wonder why France is in such a sorry state after having working for anything publicly managed in France. The French state carves exception for itself in all the labour laws because they know their administration is so poor they can’t respect them. reply inferiorhuman 6 hours agorootparentAs an American reading this I feel like there's some nuance being lost in translation. Taken to their logical extremes we've seen what things like optimization software (East Palestine), banning strikes (PATCO and current ATC staffing woes), privatization (ATC again — see any video on youtube about the KSQL controller), and weak labor laws (pretty much every fatigue related crash e.g. Colgan Air) bring about. And it's not pretty. Certainly privatization can work. JR seems to have a decent reputation (obviously not without fault). But gaming things like overtime rules is a cultural problem, one that you're not going to solve with mandates or privatization. For quite a while the drivers at San Francisco's public transit agency were forbidden to strike. Instead you'd get sickouts and work slowdowns. reply RandomThoughts3 4 hours agorootparent> As an American reading this The working conditions at SNCF are not even imaginable for an American. We are taking unlimited sick days, between 28 and 38 days off a year excluding sick days, 10 bank holidays, retirement between 50 and 60 with pensions calculated on the last few years of work, subvention on train tickets for family members. That’s while working less than 40 hours a week and despite that they still manage to strike at least a week a year generally when it’s the most annoying for people actually working. reply simgt 3 hours agorootparent> unlimited sick days Everyone with a French employment contract does > between 28 and 38 days off a year excluding sick days, 10 bank holidays 25 days is the legal minimum, more is common in many industries, including most engineering fields. Of course we all have the same bank holidays in the country. > retirement between 50 and 60 with pensions calculated on the last few years of work Yep, that's a good one. A relic from when their life expectancy was much shorter due to coal. Indeed that's hard to justify. > subvention on train tickets for family members Annoying too... but I'm not sure it's significant. > That’s while working less than 40 hours a week The legal working time in France is 35h/week, if you do more than that your employer must compensate in a way or another. That SNCF abides by that law isn't shocking, what is shocking is that our NHS doesn't. > they still manage to strike at least a week a year generally when it’s the most annoying for people actually working. The latest strike was about the freight branch that is being sold to competitors to please the EU commission... Not exactly a request for more champagne next to the coffee machine of the drivers. [0] I, too, dislike the SNCF because my trains are unreliable and expensive, and their customer support is absolute rubbish, but you clearly are arguing in bad faith and repeating whatever is in our current wave of reactionary media, all without having provided a single source yet. [0] https://www.lemonde.fr/les-decodeurs/article/2024/11/21/dema... reply RandomThoughts3 2 hours agorootparentYou do realise that I’m not arguing in bad faith but pointing things which are not obvious to our American friends who often have 10 days of leaves including sick days. I know that the working conditions in France are insane for everyone. Still they are even better for cheminots. Even you have to acknowledge it reading my comment (and no advantages for family members are not negligible). I have given sources: I told you to go read the reports for the national court of auditors. Nothing of what I wrote is reactionary by the way. I have actually worked for the damn company which is a lot more than you can say. > The latest strike was about the freight branch that is being sold to competitors to please the EU commission The heart of the issue is that the drivers are not going to be cheminots anymore. It’s entirely about champagne and coffee machines. They are worried that it’s going to end like Geodis, which is profitable while being owned by SNCF, because it’s out of the circus and operated like an actual private company, which, Sud and CGT being good communists, is the worst thing imaginable. reply jcmp 8 hours agorootparentprevThe price really depends how much in advance you are booking, DB price are outrages on the same day but unbelivable cheap three weeks before, i really dont get the business logic behind that, the spread is like 19.99 to 240 reply rcMgD2BwE72F 6 hours agorootparentprevWhere's your hometown and when do you travel? That's not my experience at all. I live in France with no car and I don't fly domestic (obviously). I'll do Paris to Lyon (400 km) during peak holidays in next few days. That's €192 back and forth, for 2 persons. And I don't have any preferential prices. I'll go further than Lyon, right to a ski station actually. That's another €100 for 2 persons, back and forth. So that's about 1300 km at €150 /pax at one of the most expensive time of the year. reply jotaen 8 hours agorootparentprevI forgot to mention that the price ranges I cited refer to DB (German railway company); I’ve edited my initial post to clarify. I don’t know the pricing at SNCF (French railway company). reply vinni2 8 hours agoprevHere in Norway unfortunately flying is still the fastest mode of transportation between major cities. It takes 7-8 hours between Bergen and Oslo for example. For frequent business travelers it’s not a feasible option. I wish the government was prioritizing improving trains and rail infrastructure but instead they are investing heavily on highways and electric flights. reply laurencerowe 3 hours agoparentIt is an absolutely stunning train journey though. reply pyrale 9 hours agoprevYou could get from Paris to Berlin by train with one change, and it was about the same train time, plus maybe 1/2h of wait at frankfurt. This title is extremely misleading. reply plantain 9 hours agoparentThe number 1 rule of European (esp. German) train travel is - minimize changes as much as possible. Timetables are so unreliable that booking a long trip with a change is just too much risk. reply ant6n 8 hours agorootparentGoing Paris to Berlin implies a change in Frankfurt. If you miss your connection, but you have a through-ticket, you can pick the next train to Berlin. There’s a train from Frankfurt to Berlin like twice an hour or so. Sure it sucks missing a connection, but in this direction is not so bad. In the other direction on the other hand… reply superjan 7 hours agorootparentDoes this also apply to “supersparpreis” tickets? i occasionally use DB but I find the pricing options and rules confusing. reply nh2 6 hours agorootparentYes, if your delay is caused by DB, and the next train that continues your journey is an hour later, you can take that even with the super saver ticket. The DB Navigator app can now \"even\" show that your you are no longer bound to specific trains when that happens, with a little banner in the top. reply inglor_cz 8 hours agoparentprevProvided that you didn't miss the connection due to a delay. German railways are not an etalon of punctuality anymore, the infrastructure has rotten due to years of neglect. reply generic92034 8 hours agorootparent> German railways are not an etalon of punctuality anymore, the infrastructure has rotten due to years of neglect. But as compensation the holy debt brake (Schuldenbremse) persists! That should make future generations very happy, while driving their horse teams over clay paths. /s reply inglor_cz 7 hours agorootparentGermany had major budget surpluses until 2019 [0]. But that money wasn't spent on investments. It is not just question of raw money itself; Germany needs to change its very attitude to spending, preferring investments into the future, otherwise all that borrowed money once the Schuldenbremse is lifted will be wasted too. [0] https://tradingeconomics.com/germany/government-budget reply tarkin2 9 hours agoparentprevSomeone add \"direct\" before train? reply jcmp 8 hours agoprevIts cool that there is another train connections, saving time and has no need to switch trains! Also kind a funny to see in the comments that a lot of people counter the good news with some kind of bad takes about the german rail way. Just be happy about the new connection :) reply alismayilov 8 hours agoprevDoes anybody have experience in China? I heard a lot that their train system is much better at this point. So, for example, the same distance will take almost two times less in China. reply alisonatwork 4 hours agoparentIn my experience (primarily in the south), high speed rail in China is fast if you only consider station to station time, but the amount of bureaucracy at either end is comparable to flying. You can't buy tickets without ID, you can rarely find same day tickets, there's no option for standing carriage, there's numerous security checkpoints and lots of lining up. It's best to arrive at the station an hour plus beforehand with prebooked tickets. Meanwhile the stations themselves are often way outside the urban centers you probably wanted to visit and there's not much nearby to do that isn't overpriced shopping mall/chain store stuff. You also can't leave and reenter the stations at will, you have to pass through security each time, just like an airport. I found traveling by slow train or long distance bus much more pleasant in China because you can just walk in, buy a ticket to wherever and head out the same day. They take ages and there are all the usual delays, but the experience is much less stressful and more comparable to the train experience people from other countries might expect. All that said, I'd still pick Chinese high speed rail over flying just for emissions reasons if nothing else. reply mykowebhn 4 hours agorootparentThank you for all of that useful information, and especially for thinking about our planet. Cheers to you! reply csomar 8 hours agoparentprevThey have lots of options (fast/slow/sleeper/premium/etc..): https://www.travelchinaguide.com/china-trains/display.aspx?t... and also pricing is more predictable. In general, it's only a bit cheaper than flying but China rail stations are usually in the center of their cities, so you save 30minx2 vs the airport. reply em-bee 4 hours agorootparentin beijing and in changsha the primary high speed railway stations are almost as far out from the city as the airports. shanghai is also at one of the airport (the one that is closer to the city at least). i believe several other cities are similar. the problem is that chinese cities are huge, so looking at the map it feels like the trainstations are in the city, but then the same is true for many airports. reply RandomThoughts3 8 hours agoparentprevIt’s brand new and they have put in place modern technologies everywhere. So it works very well. Amusingly, China uses pieces of technology which were developed in Europe for its train system. Signalling is a good exemple (CTCS is basically ETCS). Europe is as usual deploying at a snail pace while China put it everywhere. It was obvious from the start it would end up this way. China is an actual country while Europe is a loose collection of countries which don’t really like each other supposedly spearheaded by Germany which actually only cares about pushing policies in its own interest and actively hinder anything else, and France which remains stuck thirty years in the past and is actively being sabotaged by most of the things the EU forces it to do regarding infrastructure (more true in energy than in rail that being said). reply nh2 6 hours agorootparentIt doesn't have much to do with \"China being an actual country\". Germany alone is also an actual country and the trains inside it's borders are super slow. In China and Japan, many long distance trains get 320 km/h average speed! The German ICE \"machine\" could theoretically also do that speed but there are barely any tracks where this is possible, so the average speed is around 3x slower. In France and Italy it is much better. TGV and Frecciarossa trains usually operate much closer to their specced speeds. reply em-bee 4 hours agorootparentgermany is making the big mistake of mixed use tracks. in china high speed tracks are dedicated to high speed trains, and a high speed connection means dedicated high speed tracks for the whole trip. germany is creating a patchwork of high speed routes thinking that this is enough to make high speed trains work. reply RandomThoughts3 4 hours agorootparentprevThe slowness regarding rolling out things like ETCS, ATS and standardised European infrastructure as everything to do with Europe not being an actual united political entity. As you rightfully pointed, some EU members rail strategy is very dubious. reply oceanplexian 7 hours agorootparentprevThe New York Central 999 broke 160km/hr in the 19th century. The US had commercial express train service running at similar speeds in the early 1900s. Outside the TGV Europe isn’t a few years behind they are like a century behind. reply RandomThoughts3 7 hours agorootparentThat’s non sense. TGV speed record is 320km/h by the way and 160km/h is a pedestrian daily occurrence on the French rail system. Meanwhile the US has somehow decided trains are for freight and as no decent rail system in place for travellers. The discussion makes sense with China which is pushing forward quickly using new tech. The US is not even part of it. reply maartenscholl 7 hours agorootparentprevI guess it is a century behind because Europe hasn't scrapped their commuter rail for roads. I took a train from Manhattan today and its theoretical max speed is 110 mile/h, so the US has fallen behind Europe by your logic. reply physicles 5 hours agoparentprevThe trains in China are amazing. Most of the current track has been built in the last ten years. They’re done connecting major cities, so now we’re seeing high speed lines to places like Beijing suburbs, which turn multiple-hour car rides into 40 minute train rides. For trips up to about 1500km I’ll favor the train because it’s just more comfortable — security checks are sane, no luggage check, and more leg room. Beijing to Shanghai could be faster by train or by plane depending on which airport you fly into, and where your destination is within the city. Beijing to HK is a different story. The fastest trains to Shenzhen (right across from HK) are currently 7h50m, compared to a 3.5 hr flight. An overnight sleeper is a good option, especially if you have 3 friends to travel with. reply em-bee 4 hours agorootparentwhen did they remove luggage checks? or do you mean check-in? reply onetokeoverthe 8 hours agoparentprevA hypothetical US equivalent of China's high speed 300km/hour rail could do US coast to coast in under 24 hours. reply pjmlp 7 hours agoprevIn optimal case, where trains don't get cancelled, or stop endless times in the middle of nowhere. reply RamblingCTO 9 hours agoprevI'm wondering if these tracks have digital signal boxes. That's one of the biggest problems with the German rail system: analogue signal boxes where an awful lot still needs personell to operate. Right now is a sick season, so you get reduced connections. reply usr1106 8 hours agoparentThere are at least 3 generations in use: Purely mechanical ones, relay based (which is digital), semi-conductor based computers of various age. I don't think analog computers have been used in any phase. I would not be surprised if purely mechanical ones achieve higher reliability than some generations of digital ones. But many small signal boxes are vulnerable if there is not enough staff, which has been a common problem for a couple of years. Naturally mechanical signal boxes are very limited in their range. reply fl0id 8 hours agoparentprevYou mean etcs? These are not new tracks, so they will or will not same as before. reply fl0id 8 hours agorootparentLast I heard, retrofitting etc was also put on hold due to austerity measures. reply Propelloni 8 hours agorootparentIt depends, but not on TEN-T [1] routes, which this one is. [1] https://transport.ec.europa.eu/transport-themes/infrastructu... reply coldtea 7 hours agoprevFaster by five hours? It shouldn't even be 3-4 hours total, with the speeds modern trains can achieve... I guess that's the improvement (from some slow train to a faster version). reply maxnoe 9 hours agoprevThe headline is BS. At least completely misleading. They only compare to some strange, Russian direct train. The reference should be the current best connection, which would have one train switch. The difference between this new connection and the fastest connection with one switch is ~15 minutes. reply orloffm 8 hours agoparentIt's not strange, there was a Moscow-Paris train before Covid-19. reply MoreMoore 8 hours agoparentprevI wonder how often that one switch is plagued by delays which turn those 15 minutes into significantly longer times, and cause propagating delays throughout the system. reply fl0id 8 hours agorootparentNot that often probably, as there was a 30 min switch time. reply ivan_gammel 7 hours agorootparentIf you have tickets for a family with seat reservation, any problem with the switch can be very annoying, especially if the second train is full. It’s definitely not the same as direct train. reply ivan_gammel 7 hours agoparentprevThat Russian train was optimized for long distance experience and had sleeper cars. It was pretty good and could have been better if it was traveling the same speed as the new train. We need more of such trains in Europe. reply thrance 7 hours agoprevUntil we factor in the cost of a plane trip to the community into the ticket price (ie carbon tax), trains will not be able to compete with air travel for transeuropean trips. reply badgersnake 8 hours agoprevInfernal will be happy. I heard their heart is hungry for love. reply oceanplexian 8 hours agoprev [–] 8 hours to go 1000km? You could fly from NYC to Berlin which is 6x the distance and the American would get there at the same time as their counterpart leaving Paris. For people traveling there rail networks is it like a tourist thing for train buffs? At least with a maglev train you can make the case that the it might be on par with an equivalent flight. reply pbsds 8 hours agoparentTrains are more comfy, almost silent and way more convenient. You don't have to start traveling to the airport 3 hours in advance. There is also this little silly thing people talk about called global warming reply kidk 8 hours agoparentprevYou can't just compare flight time vs time on train: https://news.ycombinator.com/item?id=42500989 reply mykowebhn 8 hours agoparentprevThis is not really an apples-to-apples comparison. I just googled it and the flight time alone from EWR to BER is 8 hours. You'll need to add several hours for getting to the airport, security, immigration, checkin, waiting for luggage, etc. reply maartenscholl 7 hours agoparentprevYes but the train from Manhattan to Newark is an additional hour and is terribly slow, taking almost as long as the subway to JFK. Going by car is as slow or slower if you get caught in traffic. reply prmoustache 8 hours agoparentprevBuy you'll have spent half of that flight time in traffic going in and out of airports, as well as in the actual terminal. Taking a flight means losing a minimum of half a day, regardless of the actual flight time. reply nh2 5 hours agoparentprevYou are right that 8 hours for 1000 km is rubbish for a train. Modern trains can do that in 3 hours, on conventional rail (no maglev needed). The trains can do that already for 40 years (the ICE could to 400 km/h in 1988): https://en.m.wikipedia.org/wiki/Intercity_Express The problem is that in contrast to China, Japan, France, and Italy, Germany does not build straight tracks that allow that speed. If you put a Chinese straight train track between Berlin and Paris, the travel time would be 3 hours, without sacrificing comfort. reply paganel 8 hours agoparentprev [–] Of course we do value our travelling time greatly here in Europe, hence why RyanAir is a big thing, i.e. even though we're treated as worst than cattle people still choose it because of the (low) prices and because of those much shorter travel times. News items like this one are just propaganda pieces pushed from top to down by the mainstream media in a futile (imo) attempt to convince the uncivilised and not-environmentally friendly masses that there is a good-enough alternative to cheap flights. There isn't, cause these train-rides are both a lot more expensive and they take a lot more time compared to taking RyanAir. Basically the well-off middle-classes and higher have become a little scared that their summer properties located just a few meters from the seashore might get damaged in any one way (rising sea levels, stronger storms etc), ditto for their other properties located in the middle of forest somewhere close to a mountain (like forest fires), and hence why they try (through the mainstream media that they fully control) to tie us down to where we currently happen to live, no more tourist-ing to Cyprus, the Cyclades in Greece or to Southern Spain in order to have a cheaper glass of beer (or several more), and, God forbid, trying to have some (short-lived) proletarian fun in this stressful life. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Deutsche Bahn and SNCF have launched a new direct daytime train service from Paris to Berlin, reducing travel time by five hours to 7 hours and 59 minutes.",
      "The route includes scenic views and stops in Strasbourg, Karlsruhe, and Frankfurt, with tickets starting at €59.99 for second class.",
      "While the service offers a more leisurely travel experience, some argue it needs to be faster to effectively compete with air travel."
    ],
    "commentSummary": [
      "The new direct train connection between Paris and Berlin reduces travel time by five hours, offering a more relaxed journey compared to air travel.",
      "Trains are environmentally friendly, producing lower CO2 emissions, but require substantial infrastructure investment.",
      "While train travel can be more costly and less flexible than flying, many travelers favor it for its comfort and convenience."
    ],
    "points": 108,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1735031834
  }
]
