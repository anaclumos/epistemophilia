[
  {
    "id": 42485124,
    "title": "We use our own hardware at Fastmail",
    "originLink": "https://www.fastmail.com/blog/why-we-use-our-own-hardware/",
    "originBody": "December 22, 2024 Dec 22: Why we use our own hardware at Fastmail Post categories Technical Rob Mueller Founder & CTO This is the twenty-second post in the Fastmail Advent 2024 series. The previous post was Dec 21: Fastmail In A Box. Check back tomorrow for another post. Why we use our own hardware There has recently been talk of cloud repatriation where companies are moving from the cloud to on premises, with some particularly noisy examples. Fastmail has a long history of using our own hardware. We have over two decades of experience running and optimising our systems to use our own bare metal servers efficiently. We get way better cost optimisation compared to moving everything to the cloud because: We understand our short, medium and long term usage patterns, requirements and growth very well. This means we can plan our hardware purchases ahead of time and don’t need the fast dynamic scaling that cloud provides. We have in house operations experience installing, configuring and running our own hardware and networking. These are skills we’ve had to maintain and grow in house since we’ve been doing this for 25 years. We are able to use our hardware for long periods. We find our hardware can provide useful life for anywhere from 5-10 years depending on what it is and when in the global technology cycle it was bought, meaning we can amortise and depreciate the cost of any hardware over many years. Yes, that means we have to do more ourselves, including planning, choosing, buying, installing, etc, but the tradeoff for us has and we believe continues to be significantly worth it. Hardware over the years Of course over the 25 years we’ve been running Fastmail we’ve been through a number of hardware changes. For many years, our IMAP server storage platform was a combination of spinning rust drives and ARECA RAID controllers. We tended to use faster 15k RPM SAS drives in RAID1 for our hot meta data, and 7.2k RPM SATA drives in RAID6 for our main email blob data. In fact it was slightly more complex than this. Email blobs were written to the fast RAID1 SAS volumes on delivery, but then a separate archiving process would move them to the SATA volumes at low server activity times. Support for all of this had been added into cyrus and our tooling over the years in the form of separate “meta”, “data” and “archive” partitions. Moving to NVMe SSDs A few years ago however we made our biggest hardware upgrade ever. We moved all our email servers to a new 2U AMD platform with pure NVMe SSDs. The density increase (24 x 2.5\" NVMe drives vs 12 x 3.5\" SATA drives per 2U) and performance increase was enormous. We found that these new servers performed even better than our initial expectations. At the time we upgraded however NVMe RAID controllers weren’t widely available. So we had to decide on how to handle redundancy. We considered a RAID-less setup using raw SSDs drives on each machine with synchronous application level replication to other machines, but the software changes required were going to be more complex than expected. We were looking at using classic Linux mdadm RAID, but the write hole was a concern and the write cache didn’t seem well tested at the time. We decided to have a look at ZFS and at least test it out. Despite some of the cyrus on disk database structures being fairly hostile to ZFS Copy-on-write semantics, they were still incredibly fast at all the IO we threw at them. And there were some other wins as well. ZFS compression and tuning When we rolled out ZFS for our email servers we also enabled transparent Zstandard compression. This has worked very well for us, saving about 40% space on all our email data. We’ve also recently done some additional calculations to see if we could tune some of the parameters better. We sampled 1 million emails at random and calculated how many blocks would be required to store those emails uncompressed, and then with ZFS record sizes of 32k, 128k or 512k and zstd-3 or zstd-9 compression options. Although ZFS RAIDz2 seems conceptually similar to classic RAID6, the way it actually stores blocks of data is quite different and so you have to take into account volblocksize, how files are split into logical recordsize blocks, and number of drives when doing calculations. Emails: 1,026,000 Raw blocks: 34,140,142 32k & zstd-3, blocks: 23,004,447 = 32.6% saving 32k & zstd-9, blocks: 22,721,178 = 33.4% saving 128k & zstd-3, blocks: 20,512,759 = 39.9% saving 128k & zstd-9, blocks: 20,261,445 = 40.7% saving 512k & zstd-3, blocks: 19,917,418 = 41.7% saving 512k & zstd-9, blocks: 19,666,970 = 42.4% saving This showed that the defaults of 128k record size and zstd-3 were already pretty good. Moving to a record size of 512k improved compression over 128k by a bit over 4%. Given all meta data is cached separately, this seems a worthwhile improvement with no significant downside. Moving to zstd-9 improved compression over zstd-3 by about 2%. Given the CPU cost of compression at zstd-9 is about 4x zstd-3, even though emails are immutable and tend to be kept for a long time, we’ve decided not to implement this change. ZFS encryption We always enable encryption at rest on all of our drives. This was usually done with LUKS. But with ZFS this was built in. Again, this reduces overall system complexity. Going all in on ZFS So after the success of our initial testing, we decided to go all in on ZFS for all our large data storage needs. We’ve now been using ZFS for all our email servers for over 3 years and have been very happy with it. We’ve also moved over all our database, log and backup servers to using ZFS on NVMe SSDs as well with equally good results. SSD lifetimes The flash memory in SSDs has a finite life and finite number of times it can be written to. SSDs employ increasingly complex wear levelling algorithms to spread out writes and increase drive lifetime. You’ll often see the quoted endurance of an enterprise SSD as either an absolute figure of “Lifetime Writes”/“Total bytes written” like 65 PBW (petabytes written) or a relative per-day figure of “Drive writes per day” like 0.3, which you can convert to lifetime figure by multiplying by the drive size and the drive expected lifetime which is often assumed to be 5 years. Although we could calculate IO rates for existing HDD systems, we were making a significant number of changes moving to the new systems. Switching to a COW filesystem like ZFS, removing the special casing meta/data/archive partitions, and the massive latency reduction and performance improvements mean that things that might have taken extra time previously and ended up batching IO together, are now so fast it actually causes additional separated IO actions. So one big unknown question we had was how fast would the SSDs wear in our actual production environment? After several years, we now have some clear data. From one server at random but this is fairly consistent across the fleet of our oldest servers: # smartctl -a /dev/nvme14 ... Percentage Used: 4% At this rate, we’ll replace these drives due to increased drive sizes, or entirely new physical drive formats (such E3.S which appears to finally be gaining traction) long before they get close to their rated write capacity. We’ve also anecdotally found SSDs just to be much more reliable compared to HDDs for us. Although we’ve only ever used datacenter class SSDs and HDDs failures and replacements every few weeks were a regular occurrence on the old fleet of servers. Over the last 3+ years, we’ve only seen a couple of SSD failures in total across the entire upgraded fleet of servers. This is easily less than one tenth the failure rate we used to have with HDDs. Storage cost calculation After converting all our email storage to NVMe SSDs, we were recently looking at our data backup solution. At the time it consisted of a number of older 2U servers with 12 x 3.5\" SATA drive bays and we decided to do some cost calculations on: Move to cloud storage. Upgrade the HD drives in existing servers. Upgrade to SSD NVMe machines. 1. Cloud storage: Looking at various providers, the per TB per month price, and then a yearly price for 1000Tb/1Pb (prices as at Dec 2024) Amazon S3 - $21 -> $252,000/y Cloudflare R2 - $15 -> $180,000/y Wasabi - $6.99 -> $83,880/y Backblaze B2 - $6 -> $72,000/y Amazon S3 Glacier Instant Retrieval - $4 -> $48,000/y Amazon S3 Glacier Deep Archive (12 hour retrieval time) - $0.99 -> $11,880/y Some of these (e.g. Amazon) have potentially significant bandwidth fees as well. It’s interesting seeing the spread of prices here. Some also have a bunch of weird edge cases as well. e.g. “The S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage classes require an additional 32 KB of data per object”. Given the large retrieval time and extra overhead per-object, you’d probably want to store small incremental backups in regular S3, then when you’ve gathered enough, build a biggish object to push down to Glacier. This adds implementation complexity. Pros: No limit to amount we store. Assuming we use S3 compatible API, can choose between multiple providers. Cons: Implementation cost of converting existing backup system that assumes local POSIX files to S3 style object API is uncertain and possibly significant. Lowest cost options require extra careful consideration around implementation details and special limitations. Ongoing monthly cost that will only increase as amount of data we store increases. Uncertain if prices will go down or not, or even go up. Possible significant bandwidth costs depending on provider. 2. Upgrade HDDs Seagate Exos 24 HDs are 3.5\" 24T HDDs. This would allow us to triple the storage on existing servers. Each HDD is about $500, so upgrading one 2U machine would be about $6,000 and have storage of 220T or so. Pros: Reuses existing hardware we already have. Upgrades can be done a machine at a time. Fairly low price Cons: Will existing units handle 24T drives? What’s the rebuild time on drive failure look like? It’s almost a day for 8T drives already, so possibly nearly a week for a failed 24T drive? Is there enough IO performance to handle daily backups at capacity? 3. Upgrade to new hardware As we know, SSDs are denser (2.5\" -> 24 per 2U vs 3.5\" -> 12 per 2U), more reliable, and now higher capacity - up to 61T per 2.5\" drive. A single 2U server with 24 x 61T drives with 2 x 12 RAIDz2 = 1220T. Each drive is about $7k right now, prices fluctuate. So all up 24 x $7k = $168k + ~$20k server =~ $190k for > 1000T storage one-time cost. Pros: Much higher sequential and random IO than HDDs will ever have. Price < 1 year of standard S3 storage. Internal to our WAN, no bandwidth costs and very low latency. No new development required, existing backup system will just work. Consolidate on single 2U platform for all storage (cyrus, db, backups) and SSD for all storage. Significant space and power savings over existing HDD based servers Cons: Greater up front cost. Still need to predict and buy more servers as backups grow. One thing you don’t see in this calculation is datacenter space, power, cooling, etc. The reason is that compared to the amortised yearly cost of a storage server like this, these are actually reasonably minimal these days, on the order of $3000/2U/year. Calculating person time is harder. We have a lot of home built automation systems that mean installing and running one more server has minimal marginal cost. Result We ended up going with the the new 2U servers option: The 2U AMD NVMe platform with ZFS is a platform we have experience with already SSDs are much more reliable and much higher IO compared to HDDs No uncertainty around super large HDDs, RAID controllers, rebuild times, shuffling data around, etc. Significant space and power saving over existing HDD based servers No new development required, can use existing backup system and code Long expected hardware lifetime, controlled upfront cost, can depreciate hardware cost So far this has worked out very well. The machines have bonded 25Gbps networks and when filling them from scratch we were able to saturate the network links streaming around 5Gbytes/second of data from our IMAP servers, compressing and writing it all down to a RAIDz2 zstd-3 compressed ZFS dataset. Conclusion Running your own hardware might not be for everyone and has distinct tradeoffs. But when you have the experience and the knowledge of how you expect to scale, the cost improvements can be significant.",
    "commentLink": "https://news.ycombinator.com/item?id=42485124",
    "commentBody": "We use our own hardware at Fastmail (fastmail.com)594 points by nmjenkins 10 hours agohidepastfavorite314 comments johnklos 4 hours agoThe whole push to the cloud has always fascinated me. I get it - most people aren't interested in babysitting their own hardware. On the other hand, a business of just about any size that has any reasonable amount of hosting is better off with their own systems when it comes purely to cost. All the pro-cloud talking points are just that - talking points that don't persuade anyone with any real technical understanding, but serve to introduce doubt to non-technical people and to trick people who don't examine what they're told. What's particularly fascinating to me, though, is how some people are so pro-cloud that they'd argue with a writeup like this with silly cloud talking points. They don't seem to care much about data or facts, just that they love cloud and want everyone else to be in cloud, too. This happens much more often on sites like Reddit (r/sysadmin, even), but I wouldn't be surprised to see a little of it here. It makes me wonder: how do people get so sold on a thing that they'll go online and fight about it, even when they lack facts or often even basic understanding? I can clearly state why I advocate for avoiding cloud: cost, privacy, security, a desire to not centralize the Internet. The reason people advocate for cloud for others? It puzzles me. \"You'll save money,\" \"you can't secure your own machines,\" \"it's simpler\" all have worlds of assumptions that those people can't possibly know are correct. So when I read something like this from Fastmail which was written without taking an emotional stance, I respect it. If I didn't already self-host email, I'd consider using Fastmail. There used to be so much push for cloud everything that an article like this would get fanatical responses. I hope that it's a sign of progress that that fanaticism is waning and people aren't afraid to openly discuss how cloud isn't right for many things. reply UltraSane 2 hours agoparent\"All the pro-cloud talking points are just that - talking points that don't persuade anyone with any real technical understanding,\" This is false. AWS infrastructure is vastly more secure than almost all company data centers. AWS has a rule that the same person cannot have logical access and physical access to the same storage device. Very few companies have enough IT people to have this rule. The AWS KMS is vastly more secure than what almost all companies are doing. The AWS network is vastly better designed and operated than almost all corporate networks. AWS S3 is more reliable and scalable than anything almost any company could create on their own. To create something even close to it you would need to implement something like MinIO using 3 separate data centers. reply fulafel 2 hours agorootparentOTOH: 1. big clouds are very lucrative targets for spooks, your data seem pretty likely to be hoovered up as \"bycatch\" (or maybe main catch depending on your luck) by various agencies and then traded around as currency 2. you never hear about security probems (incidents or exposure) in the platforms, there's no transparency 3. better than most coporate stuff is a low bar reply sfilmeyer 1 hour agorootparent>3. better than most corporate stuff is a low bar I think it's a very relevant bar, though. The top level commenter made points about \"a business of just about any size\", which seems pretty exactly aligned with \"most corporate stuff\". reply mardifoufs 26 minutes agorootparentprevMost corporations (which is the vast majority of cloud users) absolutely don't care about spooks, sadly enough. If that's the threat model, then it's a very very rare case to care about it. Most datacenters/corporations won't even fight or care about sharing data with local spooks/cops/three letter agencies. The actual threat is data leaks, security breaches, etc. reply stefan_ 39 minutes agorootparentprev4. we keep hitting hypervisor bugs and having to work around the fact that your software coexists on the same machine with 3rdparty untrusted software who might in fact be actively trying to attack you. All this silliness with encrypted memory buses and the various debilitating workarounds for silicon bugs. So yes, the cloud is very secure, except for the very thing that makes it the cloud that is not secure at all and has just been papered over because questioning it means the business model is bust. reply likeabatterycar 43 minutes agorootparentprev> you never hear about security probems (incidents or exposure) in the platforms Except that one time... https://www.seattlemet.com/news-and-city-life/2023/04/how-a-... reply noprocrasted 28 minutes agorootparentIf I remember right, the attacker’s AWS employment is irrelevant - no privileged AWS access was used in that case. The attacker working for AWS was a pure coincidence, it could’ve been anyone. reply noprocrasted 2 hours agorootparentprev> AWS infrastructure is vastly more secure than almost all company data centers Secure in what terms? Security is always about a threat model and trade-offs. There's no absolute, objective term of \"security\". > AWS has a rule that the same person cannot have logical access and physical access to the same storage device. Any promises they make aren't worth anything unless there's contractually-stipulated damages that AWS should pay in case of breach, those damages actually corresponding to the costs of said breach for the customer, and a history of actually paying out said damages without shenanigans. They've already got a track record of lying on their status pages, so it doesn't bode well. But I'm actually wondering what this specific rule even tries to defend against? You presumably care about data protection, so logical access is what matters. Physical access seems completely irrelevant no? > Very few companies have enough IT people to have this rule Maybe, but that doesn't actually mitigate anything from the company's perspective? The company itself would still be in the same position, aka not enough people to reliably separate responsibilities. Just that instead of those responsibilities being physical, they now happen inside the AWS console. > The AWS KMS is vastly more secure than what almost all companies are doing. See first point about security. Secure against what - what's the threat model you're trying to protect against by using KMS? But I'm not necessarily denying that (at least some) AWS services are very good. Question is, is that \"goodness\" required for your use-case, is it enough to overcome its associated downsides, and is the overall cost worth it? A pragmatic approach would be to evaluate every component on its merits and fitness to the problem at hand instead of going all in, one way or another. reply cyberax 38 minutes agorootparent> They've already got a track record of lying on their status pages, so it doesn't bode well. ??? reply gauravphoenix 1 hour agorootparentprevone of my greatest learnings in life is to differentiate between facts and opinions- sometimes opinions are presented as facts and vice-versa. if you think about it- the statement \"this is false\" is a response to an opinion (presented as a fact) but not a fact. there is no way one can objectively define and defend what does \"real technical understanding\" means. the cloud space is vast with millions of people having varied understanding and thus opinions. so let's not fight the battle that will never be won. there is no point in convincing pro-cloud people that cloud isn't the right choice and vice-versa. let people share stories where it made sense and where it didn't. as someone who has lived in cloud security space since 2009 (and was founder of redlock - one of the first CSPMs), in my opinion, there is no doubt that AWS is indeed superiorly designed than most corp. networks- but is that you really need? if you run entire corp and LOB apps on aws but have poor security practices, will it be right decision? what if you have the best security engineers in the world but they are best at Cisco type of security - configuring VLANS and managing endpoints but are not good at detecting someone using IMDSv1 in ec2 exposed to the internet and running a vulnerable (to csrf) app? when the scope of discussion is as vast as cloud vs on-prem, imo, it is a bad idea to make absolute statements. reply fulafel 1 hour agorootparentGreat points. Also if you end up building your apps as rube goldberg machines living up to \"AWS Well Architected\" criteria (indoctrinated by staff lots of AWS certifications, leading to a lot of AWS certified staff whose paycheck now depends on following AWS recommended practices) the complexity will kill your security, as nobody will understand the systems anymore. reply rmbyrro 2 hours agorootparentprevabout security, most businesses using AWS invest little to nothing in securing their software, or even adopt basic security practices for their employees having the most secure data center doesn't matter if you load your secrets as env vars in a system that can be easily compromised by a motivated attacker so i don't buy this argument as a general reason pro-cloud reply dajonker 1 hour agorootparentThis exactly, most leaks don't involve any physical access. Why bother with something hard when you can just get in through an unmaintained Wordpress/SharePoint/other legacy product that some department can't live without. reply Aachen 2 hours agorootparentprevAWS is so complicated, we usually find more impactful permission problems than in any company using their own hardware reply dehrmann 1 hour agorootparentprevThe other part is that when us-east-1 goes down, you can blame AWS, and a third of your customer's vendors will be doing the same. When you unplug the power to your colo rack while installing a new server, that's on you. reply throwawaysxcd0 52 minutes agorootparentOTOH, when your company's web site is down you can do something about it. When the CEO asks about it, you can explain why its offline and more importantly what is being done to bring it back. The equivalent situation for those who took a cloud based approach is often... ¯\\_(ツ)_/¯ reply evantbyrne 2 hours agorootparentprevMaking API calls from a VM on shared hardware to KMS is vastly more secure than doing AES locally? I'm skeptical to say the least. reply UltraSane 1 hour agorootparentEncrypting data is easy, securely managing keys is the hard part. KMS is the Key Management Service. And AWS put a lot of thought and work into it. https://docs.aws.amazon.com/kms/latest/cryptographic-details... reply evantbyrne 1 hour agorootparentKMS access is granted by either environment variables or by authorizing the instance itself. Either way, if the instance is compromised, then so is access to KMS. So unless your threat model involves preventing the government from looking at your data through some theoretical sophisticated physical attack, then your primary concerns are likely the same as running a box in another physically secure location. So the same rules of needing to design your encryption scheme to minimize blowout from a complete hostile takeover still apply. reply AtlasBarfed 20 minutes agorootparentprevIt's now been two years since I used KMS, but at the time it seemed little more than S3 API interface with Twitter size limitations Fundamentally why would KMS be more secure than S3 anyway? Both ultimately have the same fundamental security requirements and do the same thing. So the big whirlydoo is KMS has hardware keygen. im sorry, that sounds like something almost guaranteed to have nsa backdoor, or has so much nsa attention it has been compromised. reply gooosle 2 hours agorootparentprevnext [–]reply j45 2 hours agorootparentprevThe cloud is someone else’s computer. It’s like putting something in someone’s desk drawer under the guise of convenience at the expense of security. Why? Too often, someone other than the data owner has or can get access to the drawer directly or indirectly. Also, Cloud vs self hosted to me is a pendulum that has swung back and forth for a number of reasons. The benefits of the cloud outlined here are often a lot of open source tech packaged up and sold as manageable from a web browser, or a command line. One of the major reasons the cloud became popular was networking issues in Linux to manage volume at scale. At the time the cloud became very attractive for that reason, plus being able to virtualize bare metal servers to put into any combination of local to cloud hosting. Self-hosting has become easier by an order of magnitude or two for anyone who knew how to do it, except it’s something people who haven’t done both self-hosting and cloud can really discuss. Cloud has abstracted away the cost of horsepower, and converted it to transactions. People are discovering a fraction of the horsepower is needed to service their workloads than they thought. At some point the horsepower got way beyond what they needed and it wasn’t noticed. But paying for a cloud is convenient and standardized. Company data centres can be reasonably secured using a number of PaaS or IaaS solutions readily available off the shelf. Tools from VMware, Proxmox and others are tremendous. It may seem like there’s a lot to learn, except most problems they are new to someone have often been thought of a ton by both people with and without experience that is beyond cloud only. reply UltraSane 1 hour agorootparent> The cloud is someone else’s computer. And in the case of AWS it is someone else's extremely well designed and managed computer and network. reply j45 1 hour agorootparentGenerally I look to people who could build an AWS on the value of it or doing it themselves because they can do both. Happy to hear more. reply the_arun 2 hours agorootparentprev> The cloud is someone else’s computer Isn’t it more like leasing in a public property? Meaning it is yours as long as you are paying the lease? Analogous to renting an apartment instead of owning a condo? reply adamtulinius 2 hours agorootparentNot at all. You can inspect the apartment you rent. The cloud is totally opaque in that regard. reply j45 1 hour agorootparentTotally opaque is a really nice way to describe it. reply j45 1 hour agorootparentprevNope. It's literally putting private data in a shared drawer in someone else's desk where you have your area of the drawer. reply jameshart 1 hour agorootparentLiterally? I would just like to point out that most of us who have ever had a job at an office, attended an academic institution, or lived in rented accommodation have kept stuff in someone else’s desk drawer from time to time. Often a leased desk in a building rented from a random landlord. Keeping things in someone else’s desk drawer can be convenient and offer a sufficient level of privacy for many purposes. And your proposed alternative to using ‘someone else’s desk drawer’ is, what, make your own desk? I guess, since I’m not a carpenter, I can buy a flatpack desk from ikea and assemble it and keep my stuff in that. I’m not sure that’s an improvement to my privacy posture in any meaningful sense though. reply AtlasBarfed 18 minutes agorootparentprevOne of the ways the NSA and security services get so much intelligence on targets isn't by direct decryption of what they are storing in data or listening in. A great deal with their intelligence is simply metadata intelligence. They watch what you do. They watch the amount of data you transport. They watch your patterns of movement. So even if eight of us is providing direct security and encryption in the sense of what most security professionals are concerned with key strength etc etc etc, Eddie of us still has a great deal about of information about what you do, because they get to watch how much data moves from where to where and other information about what those machines are reply wslh 1 hour agorootparentprevFrom a critical perspective, your comment made me think about the risks posed by rogue IT personnel, especially at scale in the cloud. For example, Fastmail is a single point of failure as a DoS target, whereas attacking an entire datacenter can impact multiple clients simultaneously. It all comes down to understanding the attack vectors. reply UltraSane 1 hour agorootparentCloud providers are very big targets but have enormous economic incentive to be secure and thus have very large teams of very competent security experts. reply wslh 12 minutes agorootparentYou can have full security competence but be a rogue actor at the same time. reply jandrewrogers 1 hour agoparentprevThis trivializes some real issues. The biggest problem the cloud solves is hardware supply chain management. To realize the full benefits of doing your own build at any kind of non-trivial scale you will need to become an expert in designing, sourcing, and assembling your hardware. Getting hardware delivered when and where you need it is not entirely trivial -- components are delayed, bigger customers are given priority allocation, etc. The technical parts are relatively straightforward; managing hardware vendors, logistics, and delivery dates on an ongoing basis is a giant time suck. When you use the cloud, you are outsourcing this part of the work. If you do this well and correctly then yes, you will reduce costs several-fold. But most people that build their own data infrastructure do a half-ass job of it because they (understandably) don't want to be bothered with any of these details and much of the nominal cost savings evaporate. Very few companies do security as well as the major cloud vendors. This isn't even arguable. On the other hand, you will need roughly the same number of people for operations support whether it is private data infrastructure or the cloud, there is little or no savings to be had here. The fixed operations people overhead scales to such a huge number of servers that it is inconsequential as a practical matter. It also depends on your workload. The types of workloads that benefit most from private data infrastructure are large-scale data-intensive workloads. If your day-to-day is sling tens or hundreds of PB of data for analytics, the economics of private data infrastructure is extremely compelling. reply cookiengineer 41 minutes agoparentprevMy take on this whole cloud fatigue is that system maintenance got overly complex over the last couple years/decades. So much that management people now think that it's too expensive in terms of hiring people that can do it compared to the higher managed hosting costs. DevOps and kubernetes come to mind. A lot of people using kubernetes don't know what they're getting into, and k0s or another single machine solution would have been enough for 99% of SMEs. In terms of cyber security (my field) everything got so ridiculously complex that even the folks that use 3 different dashboards in parallel will guess the answers as to whether or not they're affected by a bug/RCE/security flaw/weakness because all of the data sources (even the expensively paid for ones) are human-edited text databases. They're so buggy that they even have Chinese idiom symbols instead of a dot character in the version fields without anyone ever fixing it upstream in the NVD/CVE process. I started to build my EDR agent for POSIX systems specifically, because I hope that at some point this can help companies to ditch the cloud and allows them to selfhost again - which in return would indirectly prevent 13 year old kids like from LAPSUS to pwn major infrastructure via simple tech support hotline calls. When I think of it in terms of hosting, the vertical scalability of EPYC machines is so high that most of the time when you need its resources you are either doing something completely wrong and you should refactor your code or you are a video streaming service. reply necovek 3 hours agoparentprevBut isn't using Fastmail akin to using a cloud provider (managed email vs managed everything else)? They are similarly a service provider, and as a customer, you don't really care \"who their ISP is?\" The discussion matters when we are talking about building things: whether you self-host or use managed services is a set of interesting trade-offs. reply citrin_ru 2 hours agorootparentYes, FastMail is a SAAS. But there adepts of a religion which would tell you that companies like FastMail should be built on top of AWS and it is the only true way. It is good to have some counter narrative to this. reply j45 2 hours agorootparentBeing cloud compatible (packaged well) can be as important as being cloud-agnostic (work on any cloud). Too many projects become beholden to one cloud. reply tomrod 3 hours agoparentprevnext [–]Dunno man, it's really really easy to set up an S3 and use it to share datasets for users authorized with IAM.... And IAM and other cloud security and management considerations is where the opex/capex and capability argument can start to break down. Turns out, the \"cloud\" savings comes from not having capabilities in house to manage hardware. Sometimes, for most businesses, you want some of that lovely reliability. (In short, I agree with you, substantially). Like code. It is easy to get something basic up, but substantially more resources are needed for non-trivial things. reply hamandcheese 2 hours agorootparentI feel like IAM may be the sleeper killer-app of cloud. I self-host a lot of things, but boy oh boy if I were running a company it would be a helluvalotta work to get IAM properly set up. reply sanderjd 2 hours agorootparentI strongly agree with this and also strongly lament it. I find IAM to be a terrible implementation of a foundationally necessary system. It feels tacked on to me, except now it's tacked onto thousands of other things and there's no way out. reply andrewfromx 2 hours agorootparentlike terraform! isn't pulumi 100% better but there's no way out of terraform. reply pphysch 2 hours agorootparentprevThat's essentially why \"platform engineering\" is a hot topic. There are great FOSS tools for this, largely in the Kubernetes ecosystem. To be clear, authentication could still be outsourced, but authorizing access to (on-prem) resources in a multi-tenant environment is something that \"platforms\" are frequently designed for. reply swiftcoder 2 hours agoparentprev> All the pro-cloud talking points... don't persuade anyone with any real technical understanding This is a very engineer-centric take. The cloud has some big advantages that are entirely non-technical: - You don't need to pay for hardware upfront. This is critical for many early-stage startups, who have no real ability to predict CapEx until they find product/market fit. - You have someone else to point the SOC2/HIPAA/etc auditors at. For anyone launching a company in a regulated space, being able to checkbox your entire infrastructure based on AWS/Azure/etc existing certifications is huge. reply shortsunblack 1 hour agorootparentYou can over-provision your own baremetal resources 20x and it will be still cheaper than cloud. The capex talking point is just that, a talking point. reply motorest 1 hour agoparentprev> All the pro-cloud talking points are just that - talking points that don't persuade anyone with any real technical understanding,(...) This is where you lose all credibility. I'm going to focus on a single aspect: performance. If you're serving a global user base and your business, like practically all online businesses, is greatly impacted by performance problems, the only solution to a physics problem is to deploy your application closer to your users. With any cloud provider that's done with a few clicks and an invoice of a few hundred bucks a month. If you're running your hardware... What solution do you have to show for? Do you hope to create a corporate structure to rent a place to host your hardware manned by a dedicated team? What options f you have? reply noprocrasted 7 minutes agorootparentThe complexity of scaling out an application to be closer to the users has never been about getting the hardware closer. It's always about how do you get the data there and dealing with the CAP theorem, which requires hard tradeoffs to be decided on when designing the application and can't be just tacked on - there is no magic button to do this, in the AWS console or otherwise. Getting the hardware closer to the users has always been trivial - call up any of the many hosting providers out there and get a dedicated server, or a colo and ship them some hardware (directly from the vendor if needed). reply johnklos 19 minutes agorootparentprev> This is where you lose all credibility. People who write that, well... If you're greatly impacted by performance problems, how does that become a physics problem that has as a solution which is being closer to your users? I think you're mixing up your sales points. One, how do you scale hardware? Simple: you buy some more, and/or you plan for more from the beginning. How do you deal with network latency for users on the other side of the planet? Either you plan for and design for long tail networking, and/or you colocate in multiple places, and/or you host in multiple places. Being aware of cloud costs, problems and limitations doesn't mean you can't or shouldn't use cloud at all - it just means to do it where it makes sense. You're making my point for me - you've got emotional generalizations (\"you lose all credibility\"), you're using examples that people use often but that don't even go together, plus you seem to forget that hardly anyone advocates for all one or all the other, without some kind of sensible mix. Thank you for making a good example of exactly what I'm talking about. reply stefan_ 36 minutes agorootparentprevIs everyone running online FPS gaming servers now? If you want your page to load faster, tell your shitty frontend engineers to use less of the latest frameworks. You are not limited by physics, 99% aren't. I ping HN, it's 150ms away, it still renders in the same time that the Google frontpage does and that one has a 130ms advantage. reply pixelesque 9 minutes agorootparentErm, 99%'s clearly wrong and I think you know it, even if you are falling into the typical trap of \"only Americans matter\"... As someone in New Zealand, latency does really matter sometimes, and is painfully obvious at times. HN's ping for me is around: 330 ms. Anyway, ping doesn't really describe the latency of the full DNS lookup propogation, TCP connection establishment and TLS handshake: full responses for HN are around 900 ms for me till last byte. reply zosima 4 hours agoparentprevCloud expands the capabilities of what one team can manage by themselves, enabling them to avoid a huge amount of internal politics. This is worth astronomical amounts of money in big corps. reply sgarland 3 hours agorootparentI’m not convinced this is entirely true. The upfront cost if you don’t have the skills, sure – it takes time to learn Linux administration, not to mention management tooling like Ansible, Puppet, etc. But once those are set up, how is it different? AWS is quite clear with their responsibility model that you still have to tune your DB, for example. And for the setup, just as there are Terraform modules to do everything under the sun, there are Ansible (or Chef, or Salt…) playbooks to do the same. For both, you _should_ know what all of the options are doing. The only way I see this sentiment being true is that a dev team, with no infrastructure experience, can more easily spin up a lot of infra – likely in a sub-optimal fashion – to run their application. When it inevitably breaks, they can then throw money at the problem via vertical scaling, rather than addressing the root cause. reply tylerchurch 3 hours agorootparentI think this is only true for teams and apps of a certain size. I've worked on plenty of teams with relatively small apps, and the difference between: 1. Cloud: \"open up the cloud console and start a VM\" 2. Owned hardware: \"price out a server, order it, find a suitable datacenter, sign a contract, get it racked, etc.\" Is quite large. #1 is 15 minutes for a single team lead. #2 requires the team to agree on hardware specs, get management approval, finance approval, executives signing contracts. And through all this you don't have anything online yet for... weeks? If your team or your app is large, this probably all averages out in favor of #2. But small teams often don't have the bandwidth or the budget. reply maccard 2 hours agorootparentI work for a 50 person subsidiary of a 30k person organisation. I needed a domain name. I put in the purchase request and 6 months later eventually gave up, bought it myself and expensed it. Our AWS account is managed by an SRE team. It’s a 3 day turnaround process to get any resources provisioned, and if you don’t get the exact spec right (you forgot to specify the iops on the volume? Oops) 3 day turnaround. Already started work when you request an adjustment? Better hope as part of your initial request you specified backups correctly or you’re starting again. The overhead is absolutely enormous, and I actually don’t even have billing access to the AWS account that I’m responsible for. reply mbesto 9 minutes agorootparent> 3 day turnaround process to get any resources provisioned Now imagine having to deal with procurement to purchase hardware for your needs. 6 months later you have a server. Oh you need a SAN for object storage? There goes another 6 months. reply maccard 3 minutes agorootparentAt a previous job we had some decent on prem resources for internal services. The SRE guys had a bunch of extra compute and you would put in a ticket for a certain amount of resources (2 cpu, SSD, 8GB memory x2 on different hosts). There wasn’t a massive amount of variability between the hardware, and you just requested resources to be allocated from a bunch of hypervisors. Turnaround time was about 3 days too. Except, you were t required to be self sufficient in AWS terminology to request exactly what you needed . cyberax 28 minutes agorootparentprev> Our AWS account is managed by an SRE team. That's an anti-pattern (we call it \"the account\") in the AWS architecture. AWS internally just uses multiple accounts, so a team can get their own account with centrally-enforced guardrails. It also greatly simplifies billing. reply maccard 7 minutes agorootparentThat’s not something that I have control over or influence over. j45 1 hour agorootparentprevManageability of cloud without a dedicated resource is a form of resource creep, and shadow labour costs that aren’t factored in. How many things don’t end up happening because of this? When they need a sliver of resources in the start? reply amluto 2 hours agorootparentprevI’ve never worked at a company with these particular problems, but: #1: A cloud VM comes with an obligation for someone at the company to maintain it. The cloud does not excuse anyone from doing this. #2: Sounds like a dysfunctional system. Sure, it may be common, but a medium sized org could easily have some datacenter space and allow any team to rent a server or an instance, or to buy a server and pay some nominal price for the IT team to keep it working. This isn’t actually rocket science. Sure, keeping a fifteen year old server working safely is a chore, but so is maintaining a fifteen-year-old VM instance! reply icedchai 1 hour agorootparentObligation? Far from it. I've worked at some poorly staffed companies. Nobody is maintaining old VMs or container images. If it works, nobody touches it. I worked at a supposedly properly staffed company that had raised 100's of millions in investment, and it was the same thing. VMs running 5 year old distros that hadn't been updated in years. 600 day uptimes, no kernel patches, ancient versions of Postgres, Python 2.7 code everywhere, etc. This wasn't 10 years ago. This was 2 years ago! reply j45 1 hour agorootparentprevThe cloud is someone else’s computer. Having redirected of a vm provider or installing a hyper visor on equipment is another thing. reply layer8 2 hours agorootparentprevThe SMB I work for runs a small on-premise data center that is shared between teams and projects, with maybe 3-4 FTEs managing it (the respective employees also do dev and other work). This includes self-hosting email, storage, databases, authentication, source control, CI, ticketing, company wiki, chat, and other services. The current infrastructure didn’t start out that way and developed over many years, so it’s not necessarily something a small startup can start out with, but beyond a certain company size (a couple dozen employees or more) it shouldn’t really be a problem to develop that, if management shares the philosophy. I certainly find it preferable culturally, if not technically, to maximize independence in that way, have the local expertise and much better control over everything. One (the only?) indisputable benefit of cloud is the ability to scale up faster (elasticity), but most companies don’t really need that. And if you do end up needing it after all, then it’s a good problem to have, as they say. reply SoftTalker 54 minutes agorootparentYour last paragraph identifies the reason that running their own hardware makes sense for Fastmail. The demand for email is pretty constant. Everyone does roughly the same amount of emailing every day. Daily load is predictable, and growth is predictable. If your load is very spiky, it might make more sense to use cloud. You pay more for the baseline, but if your spikes are big enough it can still be cheaper than provisioning your own hardware to handle the highest loads. Of course there's also possibly a hybrid approach, you run your own hardware for base load and augment with cloud for spikes. But that's more complicated. reply AnthonyMouse 2 hours agorootparentprevYou're assuming that hosting something in-house implies that each application gets its own physical server. You buy a couple of beastly things with dozens of cores. You can buy twice as much capacity as you actually use and still be well under the cost of cloud VMs. Then it's still VMs and adding one is just as fast. When the load gets above 80% someone goes through the running VMs and decides if it's time to do some house cleaning or it's time to buy another host, but no one is ever waiting on approval because you can use the reserve capacity immediately while sorting it out. reply xorcist 2 hours agorootparentprevThere is a large gap between \"own the hardware\" and \"use cloud hosting\". Many people rent the hardware, for example, and you can use managed databases which is one step up than \"starting a vm\". But your comparison isn't fair. The difference between running your own hardware and using the cloud (which is perhaps not even the relevant comparison but let's run with it) is the difference between: 1. Open up the cloud console, and 2. You already have the hardware so you just run \"virsh\" or, more likely, do nothing at all because you own the API so you have already included this in your Ansible or Salt or whatever you use for setting up a server. Because ordering a new physical box isn't really comparable to starting a new VM, is it? reply sanderjd 1 hour agorootparentI've always liked the theory of #2, I just haven't worked anywhere yet that has executed it well. reply necovek 2 hours agorootparentprevBefore the cloud, you could get a VM provisioned (virtual servers) or a couple of apps set up (LAMP stack on a shared host ;)) in a few minutes over a web interface already. \"Cloud\" has changed that by providing an API to do this, thus enabling IaC approach to building combined hardware and software architectures. reply noprocrasted 2 hours agorootparentprev3. \"Dedicated server\" at any hosting provider Open their management console, press order now, 15 mins later get your server's IP address. reply zbentley 2 hours agorootparentFor purposes of this discussion, isn't AWS just a very large hosting provider? I.e. most hosting providers give you the option for virtual or dedicated hardware. So does Amazon (metal instances). Like, \"cloud\" was always an ill-defined term, but in the case of \"how do I provision full servers\" I think there's no qualitative difference between Amazon and other hosting providers. Quantitative, sure. reply noprocrasted 2 hours agorootparent> Amazon (metal instances) But you still get nickel & dimed and pay insane costs, including on bandwidth (which is free in most conventional hosting providers, and overages are 90x cheaper than AWS' costs). reply irunmyownemail 1 hour agorootparentprevQualitatively, AWS is greedy and nickle and dime you to death. Their Route53 service doesn't even have all the standard DNS options I need and I can get everywhere else or even on my own running bind9. I do not use IPv6 for several reasons, when AWS decided charge for IPv4, I went looking elsewhere to get my VM's. I can't even imagine how much the US Federal Government is charging American taxpayers to pay AWS for hosting there, it has to be astronomical. reply everfrustrated 1 hour agorootparentOut of curiosity, which DNS record types do you need that Route53 doesn't support? reply goodpoint 2 hours agorootparentprevMore like 15 seconds. reply warner25 1 hour agorootparentprevYou gave me flashbacks to a far worse bureaucratic nightmare with #2 in my last job. I supported an application with a team of about three people for a regional headquarters in the DoD. We had one stack of aging hardware that was racked, on a handshake agreement with another team, in a nearby facility under that other team's control. We had to periodically request physical access for maintenance tasks and the facility routinely lost power, suffered local network outages, etc. So we decided that we needed new hardware and more of it spread across the region to avoid the shaky single-point-of-failure. That began a three year process of: waiting for budget to be available for the hardware / license / support purchases; pitching PowerPoints to senior management to argue for that budget (and getting updated quotes every time from the vendors); working out agreements with other teams at new facilities to rack the hardware; traveling to those sites to install stuff; and working through the cybersecurity compliance stuff for each site. I left before everything was finished, so I don't know how they ultimately dealt with needing, say, someone to physically reseat a cable in Japan (an international flight away). reply Symbiote 2 hours agorootparentprevYou have omitted the option between the two, which is renting a server. No hardware to purchase, maintain or set up. Easily available in 15 minutes. reply tylerchurch 1 hour agorootparentWhile I did say \"VM\" in my original comment, to me this counts as \"cloud\" because the UI is functionally the same. reply j45 1 hour agorootparentprevThere is. Middle ground between the extremes of those pendulums of all cloud or physical metal. You can start with using a cloud only for VMs and only run services on it using IaaS or PaaS. Very serviceable. reply bonoboTP 3 hours agorootparentprevYou can get pretty far without any of that fancy stuff. You can get plenty done by using parallel-ssh and then focusing on the actual thing you develop instead of endless tooling and docker and terraform and kubernetes and salt and puppet and ansible. Sure, if you know why you need them and know what value you get from them OK. But many people just do it because it's the thing to do... reply sanderjd 2 hours agorootparentprevI have never ever worked somewhere with one of these \"cloud-like but custom on our own infrastructure\" setups that didn't leak infrastructure concerns through the abstraction, to a significantly larger degree than AWS. I believe it can work, so maybe there are really successful implementations of this out there, I just haven't seen it myself yet! reply the__alchemist 3 hours agorootparentprevDo you need those tools? It seems that for fundamental web hosting, you need your application server, nginx or similar, postgres or similar, and a CLI. (And an interpreter etc if your application is in an interpreted lang) reply sgarland 3 hours agorootparentI suppose that depends on your RTO. With cloud providers, even on a bare VM, you can to some extent get away with having no IaC, since your data (and therefore config) is almost certainly on networked storage which is redundant by design. If an EC2 fails, or even if one of the drives in your EBS drive fails, it'll probably come back up as it was. If it's your own hardware, if you don't have IaC of some kind – even something as crude as a shell script – then a failure may well mean you need to manually set everything up again. reply noprocrasted 2 hours agorootparentGet two servers (or three, etc)? reply sgarland 2 hours agorootparentWell, sure – I was trying to do a comparison in favor of cloud, because the fact that EBS Volumes can magically detach and attach is admittedly a neat trick. You can of course accomplish the same (to a certain scale) with distributed storage systems like Ceph, Longhorn, etc. but then you have to have multiple servers, and if you have multiple servers, you probably also have your application load balanced with failover. reply zbentley 2 hours agorootparentprevFor fundamentals, that list is missing: - Some sort of firewall or network access control. Being able to say \"allow http/s from the world (optionally minus some abuser IPs that cause problems), and allow SSH from developers (by IP, key, or both)\" at a separate layer from nginx is prudent. Can be ip/tables config on servers or a separate firewall appliance. - Some mechanism of managing storage persistence for the database, e.g. backups, RAID, data files stored on fast network-attached storage, db-level replication. Not losing all user data if you lose the DB server is table stakes. - Something watching external logging or telemetry to let administrators know when errors (e.g. server failures, overload events, spikes in 500s returned) occur. This could be as simple as Pingdom or as involved as automated alerting based on load balancer metrics. Relying on users to report downtime events is not a good approach. - Some sort of CDN, for applications with a frontend component. This isn't required for fundamental web hosting, but for sites with a frontend and even moderate (10s/sec) hit rates, it can become required for cost/performance; CDNs help with egress congestion (and fees, if you're paying for metered bandwidth). - Some means of replacing infrastructure from nothing. If the server catches fire or the hosting provider nukes it, having a way to get back to where you were is important. Written procedures are fine if you can handle long downtime while replacing things, but even for a handful of application components those procedures get pretty lengthy, so you start wishing for automation. - Some mechanism for deploying new code, replacing infrastructure, or migrating data. Again, written procedures are OK, but start to become unwieldy very early on ('stop app, stop postgres, upgrade the postgres version, start postgres, then apply application migrations to ensure compatibility with new version of postgres, then start app--oops, forgot to take a postgres backup/forgot that upgrading postgres would break the replication stream, gotta write that down for net time...'). ...and that's just for a very, very basic web hosting application--one that doesn't need caches, blob stores, the ability to quickly scale out application server or database capacity. Each of those things can be accomplished the traditional way--and you're right, that sometimes that way is easier for a given item in the list (especially if your maintainers have expertise in that item)! But in aggregate, having a cloud provider handle each of those concerns tends to be easier overall and not require nearly as much in-house expertise. reply zosima 3 hours agorootparentprevYou are focusing on technology. And sure of course you can get most of the benefits of AWS a lot cheaper when self-hosting. But when you start factoring internal processes and incompetent IT departments, suddenly that's not actually a viable option in many real-world scenarios. reply jeffbee 3 hours agorootparentExactly. With the cloud you can suddenly do all the things your tyrannical Windows IT admin has been saying are impossible for the last 30 years. reply the_arun 2 hours agorootparentIt is similar to cooking at home vs ordering cooked food everyday. If some guarantees the taste & quality people would happy to outsource it. reply marcosdumay 3 hours agorootparentprevAll of that is... completely unrelated to the GP's post. Did you reply to the right comment? Do you think \"politics\" is something you solve with Ansible? reply sgarland 2 hours agorootparent> Cloud expands the capabilities of what one team can manage by themselves, enabling them to avoid a huge amount of internal politics. It's related to the first part. Re: the second, IME if you let dev teams run wild with \"managing their own infra,\" the org as a whole eventually pays for that when the dozen bespoke stacks all hit various bottlenecks, and no one actually understands how they work, or how to troubleshoot them. I keep being told that \"reducing friction\" and \"increasing velocity\" are good things; I vehemently disagree. It might be good for short-term profits, but it is poison for long-term success. reply daemonologist 2 hours agorootparentprevOur big company locked all cloud resources behind a floating/company-wide DevOps team (git and CI too). We have an old on-prem server that we jealously guard because it allows us to create remotes for new git repos and deploy prototypes without consulting anyone. (To be fair, I can see why they did it - a lot of deployments were an absolute mess before.) reply mark242 1 hour agorootparentprevThis is absolutely spot on. What do you mean, I can't scale up because I've used my hardware capex budget for the year? reply acedTrex 3 hours agorootparentprevI have said for years the value of cloud is mainly its api, thats the selling point in large enterprise. reply sgarland 3 hours agorootparentSelf-hosted software also has APIs, and Terraform libraries, and Ansible playbooks, etc. It’s just that you have to know what it is you’re trying to do, instead of asking AWS what collection of XaaS you should use. reply dan-robertson 3 hours agoparentprevWell cloud providers often give more than just VMs in a data enter somewhere. You may not be able to find good equivalents if you aren’t using the cloud. Some third-party products are also only available on clouds. How much of a difference those things make will depend on what you’re trying to do. I think there are accounting reasons for companies to prefer paying opex to run things on the cloud instead of more capex-intensive self-hosting, but I don’t understand the dynamics well. It’s certainly the case that clouds tend to be more expensive than self-hosting, even when taking account of the discounts that moderately sized customers can get, and some of the promises around elastic scaling don’t really apply when you are bigger. To some of your other points: the main customers of companies like AWS are businesses. Businesses generally don’t care about the centralisation of the internet. Businesses are capable of reading the contracts they are signing and not signing them if privacy (or, typically more relevant to businesses, their IP) cannot be sufficiently protected. It’s not really clear to me that using a cloud is going to be less secure than doing things on-prem. reply hnthrowaway6543 28 minutes agoparentprev> a desire to not centralize the Internet > If I didn't already self-host email this really says all that needs to be said about your perspective. you have an engineer and OSS advocate's mindset. which is fine, but most business leaders (including technical leaders like CTOs) have a business mindset, and their goal is to build a business that makes money, not avoid contributing to the centralization of the internet reply cpursley 4 hours agoparentprevThe fact is, managing your own hardware is a pita and a distraction from focusing on the core product. I loathe messing with servers and even opt for \"overpriced\" paas like fly, render, vercel. Because every minute messing with and monitoring servers is time not spent on product. My tune might change past a certain size and a massive cloud bill and there's room for full time ops people, but to offset their salary, it would have to be huge. reply noprocrasted 4 hours agorootparentThat argument makes sense for PaaS services like the ones you mention. But for bare \"cloud\" like AWS, I'm not convinced it is saving any effort, it's merely swapping one kind of complexity with another. Every place I've been in had full-time people messing with YAML files or doing \"something\" with the infrastructure - generally trying to work around the (self-inflicted) problems introduced by their cloud provider - whether it's the fact you get 2010s-era hardware or that you get nickel & dimed on absolutely arbitrary actions that have no relationship to real-world costs. reply jeffbee 3 hours agorootparentIn what sense is AWS \"bare cloud\"? S3, DynamoDB, Lambda, ECS? reply noprocrasted 2 hours agorootparentHow do you configure S3 access control? You need to learn & understand how their IAM works. How do you even point a pretty URL to a lambda? Last time I looked you need to stick an \"API gateway\" in front (which I'm sure you also get nickel & dimed for). How do you go from \"here's my git repo, deploy this on Fargate\" with AWS? You need a CI pipeline which will run a bunch of awscli commands. And I'm not even talking about VPCs, security groups, etc. Somewhat different skillsets than old-school sysadmin (although once you know sysadmin basics, you realize a lot of these are just the same concepts under a branded name and arbitrary nickel & diming sprinkled on top), but equivalent in complexity. reply inemesitaffia 3 hours agorootparentprevEC2 reply bsder 1 hour agorootparentI would actually argue that EC2 is a \"cloud smell\"--if you're using EC2 you're doing it wrong. reply sgarland 3 hours agorootparentprevCounterpoint: if you’re never “messing with servers,” you probably don’t have a great understanding of how their metrics map to those of your application’s, and so if you bottleneck on something, it can be difficult to figure out what to fix. The result is usually that you just pay more money to vertically scale. To be fair, you did say “my tune might change past a certain size.” At small scale, nothing you do within reason really matters. World’s worst schema, but your DB is only seeing 100 QPS? Yeah, it doesn’t care. reply tokioyoyo 3 hours agorootparentI don’t think you’re correct. I’ve watched junior/mid-level engineers figure things out solely by working on the cloud and scaling things to a dramatic degree. It’s really not a rocket science. reply sgarland 2 hours agorootparentI didn't say it's rocket science, nor that it's impossible to do without having practical server experience, only that it's more difficult. Take disks, for example. Most cloud-native devs I've worked with have no clue what IOPS are. If you saturate your disk, that's likely to cause knock-on effects like increased CPU utilization from IOWAIT, and since \"CPU is high\" is pretty easy to understand for anyone, the seemingly obvious solution is to get a bigger instance, which depending on the application, may inadvertently solve the problem. For RDBMS, a larger instance means a bigger buffer pool / shared buffers, which means fewer disk reads. Problem solved, even though actually solving the root cause would've cost 1/10th or less the cost of bumping up the entire instance. reply icedchai 3 hours agorootparentprevWriting piles of IaC code like Terraform and CloudFormation is also a PITA and a distraction from focusing on your core product. PaaS is probably the way to go for small apps. reply sgarland 1 hour agorootparentA small app (or a larger one, for that matter) can quite easily run on infra that's instantiated from canned IaC, like TF AWS Modules [0]. If you can read docs, you should be able to quite trivially get some basic infra up in a day, even with zero prior experience managing it. [0]: https://github.com/terraform-aws-modules reply icedchai 1 hour agorootparentYes, I've used several of these modules myself. They save tons of time! Unfortunately, for legacy projects, I inherited a bunch of code from individuals that built everything \"by hand\" then copy-pasted everything. No re-usability. reply UltraSane 2 hours agorootparentprevBut that effort has a huge payoff in that it can be used to disaster recovery in a new region and to spin up testing environments. reply fhd2 4 hours agorootparentprevI'm with you there, with stuff like fly.io, there's really no reason to worry about infrastructure. AWS, on the other hand, seems about as time consuming and hard as using root servers. You're at a higher level of abstraction, but the complexity is about the same I'd say. At least that's my experience. reply cpursley 3 hours agorootparentI agree with this position and actively avoid AWS complexity. reply cpursley 4 hours agorootparentprevAnecdotal - but I once worked for a company where the product line I built for them after acquisition was delayed by 5 months because that's how long it took to get the hardware ordered and installed in the datacenter. Getting it up on AWS would have been a days work, maybe two. reply stubish 3 hours agorootparentYes, it is death by 1000 cuts. Speccing, negotiating with hardware vendors, data center selection and negotiating, DC engineer/remote hands, managing security cage access, designing your network, network gear, IP address ranges, BGP, secure remote console access, cables, shipping, negotiating with bandwidth providers (multiple, for redundancy), redundant hardware, redundant power sources, UPS. And then you get to plug your server in. Now duplicate other stuff your cloud might provide, like offsite backups, recovery procedures, HA storage, geographic redundancy. And do it again when you outgrown your initial DC. Or build your own DC (power, climate, fire protection, security, fiber, flooring, racks) reply sgarland 2 hours agorootparentMuch of this is still required in cloud. Also, I think you're missing the middle ground where 99.99% of companies could happily exist indefinitely: colo. It makes little to no financial or practical sense for most to run their own data centers. reply sroussey 2 hours agorootparentprevOh, absolutely, with your own hardware you need planning. Time to deployment is definitely a thing. Really, the one major thing that bites on cloud providers in there 99.9% margin on egress. The markup is insane. reply xorcist 2 hours agorootparentprev> every minute messing with and monitoring servers You're not monitoring your deployments because \"cloud\"? reply awholescammy 2 hours agoparentprevThere is a whole ecosystem that pushes cloud to ignorant/fresh graduates/developers. Just take a look at the sponsors for all the most popular frameworks. When your system is super complex and depends on the cloud they make more money. Just look at the PHP ecosystem, Laravel needs 4 times the servers to server something that a pure PHP system would need. Most projects don't need the cloud. Only around 10% of projects actually need what the cloud provides. But they were able to brainwash a whole generation of developers/managers to think that they do. And so it goes. reply gjsman-1000 27 minutes agorootparentHaving worked with Laravel, this is absolutely bull. reply browningstreet 2 hours agoparentprevMost companies severely understaff ops, infra, and security. Your talking points might be good but, in practice, won’t apply in many cases because of the intractability of that management mindset. Even when they should know better. I’ve worked at tech companies with hundreds of developers and single digit ops staff. Those people will struggle to build and maintain mature infra. By going cloud, you get access to mature infra just by including it in build scripts. Devops is an effective way to move infra back to project teams and cut out infra orgs (this isn’t great but I see it happen everywhere). Companies will pay cloud bills but not staffing salaries. reply j45 2 hours agorootparentUsing a commercial cloud provider only cements understaffing in, in too many cases. reply jhwhite 57 minutes agoparentprev> It makes me wonder: how do people get so sold on a thing that they'll go online and fight about it, even when they lack facts or often even basic understanding? I feel like this can be applied to anything. I had a manager take one SAFe for Leaders class then came back wanting to implement it. They had no previous AGILE classes or experience. And the Enterprise Agile Office was saying DON'T USE SAFe!! But they had one class and that was the only way they would agree to structure their group. reply mmcwilliams 2 hours agoparentprevIt seems that the preference is less about understanding or misunderstanding the technical requirements but more that it moves a capital expenditure with some recurring operational expenditure entirely into the opex column. reply onli 4 hours agoparentprevThe one convincing argument from technical people I saw, that would be repeated to your comment, is that by now, you dont find enough experienced engineers to reliably setup some really big systems. Because so much went to the cloud, a lot of the knowledge is buried there. That came from technical people who I didn't perceive as being dogmatically pro-cloud. reply mjburgess 4 hours agoparentprev1. People are credulous 2. People therefore repeat talking points which seem in their interest 3. With enough repetition these become their beliefs 4. People will defend their beliefs as theirs against attack 5. Goto 1 reply twoparachute45 1 hour agoparentprev>What's particularly fascinating to me, though, is how some people are so pro-cloud that they'd argue with a writeup like this with silly cloud talking points. They don't seem to care much about data or facts, just that they love cloud and want everyone else to be in cloud, too. The irony is absolutely dripping off this comment, wow. Commenter makes emotionally charge comment with no data or facts and decries anyone who disagrees with them as \"silly talking points\" for not caring about data and facts. Your comment is entirely talking about itself. reply glitchc 4 hours agoparentprevCloud solves one problem quite well: Geographic redundancy. It's extremely costly with on-prem. reply sgarland 3 hours agorootparentOnly if you’re literally running your own datacenters, which is in no way required for the majority of companies. Colo giants like Equinix already have the infrastructure in place, with a proven track record. If you enable Multi-AZ for RDS, your bill doubles until you cancel. If you set up two servers in two DCs, your initial bill doubles from the CapEx, and then a very small percentage of your OpEx goes up every month for the hosting. You very, very quickly make this back compared to cloud. reply Cyph0n 2 hours agorootparentBut reliable connectivity between regions/datacenters remains a challenge, right? Compute is only one part of the equation. Disclaimer: I work on a cloud networking product. reply sgarland 1 hour agorootparentIt depends on how deep you want to go. Equinix for one (I'm sure others as well, but I'm most familiar with them) offers managed cross-DC fiber. You will probably need to manage the networking, to be fair, and I will readily admit that's not trivial. reply irunmyownemail 1 hour agorootparentprevI use Wireguard, pretty simple, where's the challenge? reply Cyph0n 1 hour agorootparentI am referring to the layer 3 connectivity that Wireguard is running on top of. Depending on your use case and reliability and bandwidth requirements, routing everything over the “public” internet won’t cut it. Not to mention setting up and maintaining your physical network as the number of physical hosts you’re running scales. reply icedchai 3 hours agorootparentprevExcept, almost nobody, outside of very large players, does cross region redundancy. us-east-1 is like a SPOF for the entire Internet. reply liontwist 2 hours agorootparentprevCloud noob here. But if I have a central database what can I distribute across geographic regions? Static assets? Maybe a cache? reply sgarland 1 hour agorootparentYep. Cross-region RDBMS is a hard problem, even when you're using a managed service – you practically always have to deal with eventual consistency, or increased latency for writes. reply dietr1ch 3 hours agorootparentprevDoes it? I've seen outages around \"Sorry, us-west_carolina-3 is down\". AWS is particularly good at keeping you aware of their datacenters. reply toast0 3 hours agorootparentIt can be useful. I run a latency sensitive service with global users. A cloud lets me run it in 35 locations dealing with one company only. Most of those locations only have traffic to justify a single, smallish, instance. In the locations where there's more traffic, and we need more servers, there are more cost effective providers, but there's value in consistency. Elasticity is nice too, we doubled our instance count for the holidays, and will return to normal in January. And our deployment style starts a whole new cluster, moves traffic, then shuts down the old cluster. If we were on owned hardware, adding extra capacity for the holidays would be trickier, and we'd have to have a more sensible deployment method. And the minimum service deployment size would probably not be a little quad processor box with 2GB ram. Using cloud for the lower traffic locations and a cost effective service for the high traffic locations would probably save a bunch of money, but add a lot of deployment pain. And a) it's not my decision and b) the cost difference doesn't seem to be quite enough to justify the pain at our traffic levels. But if someone wants to make a much lower margin, much simpler service with lots of locations and good connectivity, be sure to post about it. But, I think the big clouds have an advantage in geographic expansion, because their other businesses can provide capital and justification to build out, and high margins at other locations help cross subsidize new locations when they start. reply dietr1ch 1 hour agorootparentI agree it can be useful (latency, availability, using off-peak resources), but running globally should be a default and people should opt-in into fine-grained control and responsibility. From outside it seems that either AWS picked the wrong default to present their customers, or that it's unreasonably expensive and it drives everyone into the in-depth handling to try to keep cloud costs down. reply bdangubic 3 hours agorootparentprevif you see that you are doing it wrong :) reply sgarland 3 hours agorootparentAWS has had multiple outages which were caused by a single AZ failing. reply dietr1ch 7 minutes agorootparentYup, I was referring to, I guess, one of these, - https://news.ycombinator.com/item?id=29473630: (2021-12-07) AWS us-east-1 outage - https://news.ycombinator.com/item?id=29648286: (2021-12-22) AWS appears to be down again Maybe things are better now, but it became apparent that people might be misusing cloud providers or betting that things work flawlessly even if they completely ignore AZs. reply ayuhito 2 hours agorootparentprevMy company used to do everything on-prem. Until a literal earthquake and tsunami took down a bunch of systems. After that, yeah we’ll let AWS do the hard work of enabling redundancy for us. reply tyingq 3 hours agoparentprevI think part of it was a way for dev teams to get an infra team that was not empowered to say no. Plus organizational theory, empire building, etc. reply sgarland 3 hours agorootparentYep. I had someone tell me last week that they didn't want a more rigid schema because other teams rely on it, and anything adding \"friction\" to using it would be poorly received. As an industry, we are largely trading correctness and performance for convenience, and this is not seen as a negative by most. What kills me is that at every cloud-native place I've worked at, the infra teams were both responsible for maintaining and fixing the infra that product teams demanded, but were not empowered to push back on unreasonable requests or usage patterns. It's usually not until either the limits of vertical scaling are reached, or a SEV0 occurs where these decisions were the root cause does leadership even begin to consider changes. reply ants_everywhere 22 minutes agoparentprev...but your post reads like you do have an emotional reaction to this question and you're ready to believe someone who shares your views. There's not nearly enough in here to make a judgment about things like security or privacy. They have the bare minimum encryption enabled. That's better than nothing. But how is key access handled? Can they recover your email if the entire cluster goes down? If so, then someone has access to the encryption keys. If not, then how do they meet reliability guarantees? Three letter agencies and cyber spies like to own switches and firewalls with zero days. What hardware are they using, and how do they mitigate against backdoors? If you really cared about this you would have to roll your own networking hardware down to the chips. Some companies do this, but you need to have a whole lot of servers to make it economical. It's really about trade-offs. I think the big trade-offs favoring staying off cloud are cost (in some applications), distrust of the cloud providers,and avoiding the US Government. The last two are arguably judgment calls that have some inherent emotional content. The first is calculable in principle, but people may not be using the same metrics. For example if you don't care that much about security breaches or you don't have to provide top tier reliability, then you can save a ton of money. But if you do have to provide those guarantees, it would be hard to beat Cloud prices. reply lelanthran 2 hours agoparentprev> On the other hand, a business of just about any size that has any reasonable amount of hosting is better off with their own systems when it comes purely to cost From a cost PoV, sure, but when you're taking money out of capex it represents a big hit to the cash flow, while taking out twice that amount from opex has a lower impact on the company finances. reply anotherhue 4 hours agoparentprevThey spent time and career points learning cloud things and dammit it's going to matter! You can't even blame them too much, the amount of cash poured into cloud marketing is astonishing. reply sgarland 3 hours agorootparentThe thing that frustrates me is it’s possible to know how to do both. I have worked with multiple people who are quite proficient in both areas. Cloud has definite advantages in some circumstances, but so does self-hosting; moreover, understanding the latter makes the former much, much easier to reason about. It’s silly to limit your career options. reply noworriesnate 2 hours agorootparentBeing good at both is twice the work, because even if some concepts translate well, IME people won't hire someone based on that. \"Oh you have experience with deploying RabbitMQ but not AWS SQS? Sorry, we're looking for someone more qualified.\" reply sgarland 2 hours agorootparentThat's a great filter for places I don't want to work at, then. reply tzs 3 hours agoparentprevThere was a time when cloud was significantly cheaper then owning. I'd expect that there are people who moved to the cloud then, and over time started using services offered by their cloud provider (e.g., load balancers, secret management, databases, storage, backup) instead of running those services themselves on virtual machines, and now even if it would be cheaper to run everything on owned servers they find it would be too much effort to add all those services back to their own servers. reply toomuchtodo 3 hours agorootparentThe cloud wasn’t about cheap, it was about fast. If you’re VC funded, time is everything, and developer velocity above all else to hyperscale and exit. That time has passed (ZIRP), and the public cloud margin just doesn’t make sense when you can own and operate (their margin is your opportunity) on prem with similar cloud primitives around storage and compute. Elasticity is a component, but has always been from a batch job bin packing scheduling perspective, not much new there. Before k8s and Nomad, there was Globus.org. (Infra/DevOps in a previous life at a unicorn, large worker cluster for a physics experiment prior, etc; what is old is a new again, you’re just riding hype cycle waves from junior to retirement [mainframe->COTS on prem->cloud->on prem cloud, and so on]) reply dboreham 3 hours agorootparentprevThat was never true except in the case that the required hardware resources were significantly smaller than a typical physical machine. reply dehrmann 1 hour agoparentprevThe real cost wins of self-hosted are that anything using new hardware becomes an ordeal, and engineers won't use high-cost, value-added services. I agree that there's often too little restraint in cloud architectures, but if a business truly believes in a project, it shouldn't be held up for six months waiting for server budget with engineers spending doing ops work to get three nines of DB reliability. There is a size where self-hosting makes sense, but it's much larger than you think. reply bluedino 2 hours agoparentprevI want to see an article like this, but written from a Fortune 500 CTO perspective It seems like they all abandoned their VMware farms or physical server farms for Azure (they love Microsoft). Are they actually saving money? Are things faster? How's performance? What was the re-training/hiring like? In one case I know we got rid of our old database greybeards and replaced them with \"DevOps\" people that knew nothing about performance etc And the developers (and many of the admins) we had knew nothing about hardware or anything so keeping the physical hardware around probably wouldn't have made sense anyways reply ndriscoll 1 hour agorootparentComplicating this analysis is that computers have still been making exponential improvements in capability as clouds became popular (e.g. disks are 1000-10000x faster than they were 15 years ago), so you'd naturally expect things to become easier to manage over time as you need fewer machines, assuming of course that your developers focus on e.g. learning how to use a database well instead of how to scale to use massive clusters. That is, even if things became cheaper/faster, they might have been even better without cloud infrastructure. reply jrs235 52 minutes agorootparentprev>we got rid of our old database greybeards and replaced them with \"DevOps\" people that knew nothing about performance etc Seems a lot of those DevOps people just see Azures recommendations for adding indexes and either just allow auto applying them or just adding them without actually reviewing it understanding what use loads require them and why. This also lands a bit on developers/product that don't critically think about and communicate what queries are common and should have some forethought on what indexes should be beneficial and created. (Yes followup monitoring of actual index usage and possible missing indexes is still needed.) Too many times I've seen dozens of indexes on tables in the cloud where one could cover all of them. Yes, there still might be worthwhile reasons to keep some narrower/smaller indexes but again DBA and critical query analysis seems to be a forgotten and neglected skill. No one owns monitoring and analysing db queries and it only comes up after a fire has already broken out. reply mark242 1 hour agoparentprevI'm curious about what \"reasonable amount of hosting\" means to you, because from my experience, as your internal network's complexity goes up, it's far better for your to move systems to a hyperscaler. The current estimate is >90% of Fortune 500 companies are cloud-based. What is it that you know that they don't? reply moltar 3 hours agoparentprevCloud is more than instances. If all you need is a bunch of boxes, then cloud is a terrible fit. I use AWS cloud a lot, and almost never use any VMs or instances. Most instances I use are along the lines of a simple anemic box for a bastion host or some such. I use higher level abstractions (services) to simplify solutions and outsource maintenance of these services to AWS. reply JOnAgain 2 hours agoparentprevAs someone who ran a startup with 100’s of hosts. As soon as I start to count the salaries, hiring, desk space, etc of the people needed to manage the hosts AWS would look cheap again. Yea, hardware costs they are aggressively expensive. But TCO wise, they’re cheap for any decent sized company. Add in compliance, auditing, etc. all things that you can set up out of the box (PCI, HIPPA, lawsuit retention). Gets even cheaper. reply sanderjd 2 hours agoparentprevAlso, by the way, I found it interesting that you framed your side of this disagreement as the technically correct one, but then included this: > a desire to not centralize the Internet This is an ideological stance! I happen to share this desire. But you should be aware of your own non-technical - \"emotional\" - biases when dismissing the arguments of others on the grounds that they are \"emotional\" and+l \"fanatical\". reply swozey 1 hour agoparentprevI have about 30 years as a linux eng, starting with openbsd and have spent a LOT of time with hardware building webhosts and CDNs until about 2020 where my last few roles have been 100% aws/gcloud/heroku. I love building the cool edge network stuff with expensive bleeding edge hardware, smartnics, nvmeOF, etc but its infinitely more complicated and stressful than terraforming an AWS infra. Every cluster I set up I had to interact with multiple teams like networking, security, storage sometimes maintenance/electrical, etc. You've got some random tech you have to rely on across the country in one of your POPs with a blown server. Every single hardware infra person has had a NOC tech kick/unplug a server at least once if they've been in long enough. And then when I get the hardware sometimes you have different people doing different parts of setup, like NOC does the boot, maybe boostraps the hardware with something that works over ssh before an agent is installed (ansible, etc), then your linux eng invokes their magic with a ton of bash or perl, then your k8s person sets up the k8s clusters with usually something like terraform/puppet/chef/salt probably calling helm charts. Then your monitoring person gets it into OTEL/grafana, etc. This all organically becomes more automated as time goes on, but I've seen it from a brand new infra where you've got no automation many times. Now you're automating 90% of this via scripts and IAC, etc, but you're still doing a lot of tedious work. You also have a much more difficult time hiring good engineers. The markets gone so heavily AWS (I'm no help) that its rare that I come across an ops resume that's ever touched hardware, especially not at the CDN distributed systems level. So.. aws is the chill infra that stays online and you can basically rely on 99.99something%. Get some terraform blueprints going and your own developers can self serve. Don't need hardware or ops involved. And none of this is even getting into supporting the clusters. Failing clusters. Dealing with maintenance, zero downtime kernel upgrades, rollbacks, yaddayadda. reply kevin_thibedeau 1 hour agoparentprevCapital expenditures are kryptonite to financial engineers. The cloud selling point was to trade those costs for operational expenses and profit in phase 3. reply sgarland 3 hours agoparentprev> What's particularly fascinating to me, though, is how some people are so pro-cloud that they'd argue with a writeup like this with silly cloud talking points. I’m sure I’ll be downvoted to hell for this, but I’m convinced that it’s largely their insecurities being projected. Running your own hardware isn’t tremendously difficult, as anyone who’s done it can attest, but it does require a much deeper understanding of Linux (and of course, any services which previously would have been XaaS), and that’s a vanishing trait these days. So for someone who may well be quite skilled at K8s administration, serverless (lol) architectures, etc. it probably is seen as an affront to suggest that their skill set is lacking something fundamental. reply bulatb 0 minutes agorootparentSomeone from the arctic visits the tropics and yells at all the stupids wearing shorts, because they'll freeze to death. The locals roll their eyes and go swimming. The visitor concludes the locals must be hypothermic and their judgement is already too far gone for them to see. reply TacticalCoder 3 hours agorootparentprev> So for someone who may well be quite skilled at K8s administration ... And running your own hardware is not incompatible with Kubernetes: on the contrary. You can fully well have your infra spin up VMs and then do container orchestration if that's your thing. And part your hardware monitoring and reporting tool can work perfectly fine from containers. Bare metal -> Hypervisor -> VM -> container orchestration -> a container running a \"stateless\" hardware monitoring service. And VMs themselves are \"orchestrated\" too. Everything can be automated. Anyway say a harddisk being to show errors? Notifications being sent (email/SMS/Telegram/whatever) by another service in another container, dashboard shall show it too (dashboards are cool). Go to the machine once the spare disk as already been resilvered, move it where the failed disk was, plug in a new disk that becomes the new spare. Boom, done. I'm not saying all self-hosted hardware should do container orchestration: there are valid use cases for bare metal too. But something as to be said about controlling everything on your own infra: from the bare metal to the VMs to container orchestration. To even potentially your own IP address space. This is all within reach of an individual, both skill-wise and price-wise (including obtaining your own IP address space). People who drank the cloud kool-aid should ponder this and wonder how good their skills truly are if they cannot get this up and working. reply sgarland 3 hours agorootparentFully agree. And if you want to take it to the next level (and have a large budget), Oxide [0] seems to have neatly packaged this into a single coherent product. They don't quite have K8s fully running, last I checked, but there are of course other container orchestration systems. > Go to the machine once the spare disk as already been resilvered Hi, fellow ZFS enthusiast :-) [0]: https://oxide.computer reply noprocrasted 2 hours agorootparentprev> And running your own hardware is not incompatible with Kubernetes: on the contrary Kubernetes actually makes so much more sense on bare-metal hardware. On the cloud, I think the value prop is dubious - your cloud provider is already giving you VMs, why would you need to subdivide them further and add yet another layer of orchestration? Not to mention that you're getting 2010s-era performance on those VMs, so subdividing them is terrible from a performance point of view too. reply sgarland 1 hour agorootparent> Not to mention that you're getting 2010s-era performance on those VMs, so subdividing them is terrible from a performance point of view too. I was trying in vain to explain to our infra team a couple of weeks ago why giving my team a dedicated node of a newer instance family with DDR5 RAM would be beneficial for an application which is heavily constrained by RAM speed. People seem to assume that compute is homogenous. reply irunmyownemail 1 hour agorootparentprevI agree, I run PROD, TEST and DEV kube clusters all in VM's, works great. reply irunmyownemail 2 hours agoparentprev> If I didn't already self-host email, I'd consider using Fastmail. Same sentiment all of what you said. reply luplex 3 hours agoparentprevIn the public sector, cloud solves the procurement problem. You just need to go through the yearlong process once to use a cloud service, instead of for each purchase > 1000€. reply TacticalCoder 3 hours agoparentprev> All the pro-cloud talking points are just that - talking points that don't persuade anyone with any real technical understanding ... And moreover most of the actual interesting things, like having VM templates and stateless containers, orchestration, etc. is very easy to run yourself and gets you 99.9% of the benefits of the cloud. About just any and every service is available as container file already written for you. And if it doesn't exist, it's not hard to plumb up. A friend of mine runs more than 700 containers (yup, seven hundreds), split over his own rack at home (half of them) and the other half on dedicated servers (he runs stuff like FlightRadar, AI models, etc.). He'll soon get his own IP addresses space. Complete \"chaos monkey\" ready infra where you can cut any cable and the thing shall keep working: everything is duplicated, can be spun up on demand, etc. Someone could still his entire rack and all his dedicated server, he'd still be back operational in no time. If an individual can do that, a company, no matter its size, can do it too. And arguably 99.9% of all the companies out there don't have the need for an infra as powerful as the one most homelab enthusiast have. And another thing: there's even two in-betweens between \"cloud\" and \"our own hardware located at our company\". First is colocating your own hardware but in a datacenter. Second is renting dedicated servers from a datacenter. They're often ready to accept cloud-init directly. And it's not hard. I'd say learning to configure hypervisors on bare metal, then spin VMs from templates, then running containers inside the VMs is actually much easier than learning all the idiosyncrasies of all the different cloud vendors APIs and whatnots. Funnily enough when the pendulum swung way too far on the \"cloud all the things\" side, those saying at some point we'd read story about repatriation were being made fun of. reply sgarland 2 hours agorootparent> If an individual can do that, a company, no matter its size, can do it too. Fully agreed. I don't have physical HA – if someone stole my rack, I would be SOL – but I can easily ride out a power outage for as long as I want to be hauling cans of gasoline to my house. The rack's UPS can keep it up at full load for at least 30 minutes, and I can get my generator running and hooked up in under 10. I've done it multiple times. I can lose a single server without issue. My only SPOF is internet, and that's only by choice, since I can get both AT&T and Spectrum here, and my router supports dual-WAN with auto-failover. > And arguably 99.9% of all the companies out there don't have the need for an infra as powerful as the one most homelab enthusiast have. THIS. So many people have no idea how tremendously fast computers are, and how much of an impact latency has on speed. I've benchmarked my 12-year old Dells against the newest and shiniest RDS and Aurora instances on both MySQL and Postgres, and the only ones that kept up were the ones with local NVMe disks. Mine don't even technically have _local_ disks; they're NVMe via Ceph over Infiniband. Does that scale? Of course not; as soon as you want geo-redundant, consistent writes, you _will_ have additional latency. But most smaller and medium companies don't _need_ that. reply sanderjd 2 hours agoparentprev> All the pro-cloud talking points are just that - talking points that don't persuade anyone with any real technical understanding, but serve to introduce doubt to non-technical people and to trick people who don't examine what they're told. This feels like \"no true scotsman\" to me. I've been building software for close to two decades, but I guess I don't have \"any real technical understanding\" because I think there's a compelling case for using \"cloud\" services for many (honestly I would say most) businesses. Nobody is \"afraid to openly discuss how cloud isn't right for many things\". This is extremely commonly discussed. We're discussing it right now! I truly cannot stand this modern innovation in discourse of yelling \"nobody can talk about XYZ thing!\" while noisily talking about XYZ thing on the lowest-friction publishing platforms ever devised by humanity. Nobody is afraid to talk about your thing! People just disagree with you about it! That's ok, differing opinions are normal! Your comment focuses a lot on cost. But that's just not really what this is all about. Everyone knows that on a long enough timescale with a relatively stable business, the total cost of having your own infrastructure is usually lower than cloud hosting. But cost is simply not the only thing businesses care about. Many businesses, especially new ones, care more about time to market and flexibility. Questions like \"how many servers do we need? with what specs? and where should we put them?\" are a giant distraction for a startup, or even for a new product inside a mature firm. Cloud providers provide the service of \"don't worry about all that, figure it out after you have customers and know what you actually need\". It is also true that this (purposefully) creates lock-in that is expensive either to leave in place or unwind later, and it definitely behooves every company to keep that in mind when making architecture decisions, but lots of products never make it to that point, and very few of those teams regret the time they didn't spend building up their own infrastructure in order to save money later. reply slothtrop 1 hour agoparentprevThe bottom line > babysitting hardware. Businesses are transitioning to cloud because it's better for business. reply irunmyownemail 1 hour agorootparentActually, there's been a reversal trend going on, for many companies, better is often on premises or hybrid now. reply jeffbee 4 hours agoparentprevThe problem with your claims here is they can only be right if the entire industry is experiencing mass psychosis. I reject a theory that requires that, because my ego just isn't that large. I once worked for several years at a publicly traded firm well-known for their return-to-on-prem stance, and honestly it was a complete disaster. The first-party hardware designs didn't work right because they didn't have the hardware designs staffing levels to have de-risked to possibility that AMD would fumble the performance of Zen 1, leaving them with a generation of useless hardware they nonetheless paid for. The OEM hardware didn't work right because they didn't have the chops to qualify it either, leaving them scratching their heads for months over a cohort of servers they eventually discovered were contaminated with metal chips. And, most crucially, for all the years I worked there, the only thing they wanted to accomplish was failover from West Coast to East Coast, which never worked, not even once. When I left that company they were negotiating with the data center owner who wanted to triple the rent. These experiences tell me that cloud skeptics are sometimes missing a few terms in their equations. reply marcosdumay 3 hours agorootparent> The problem with your claims here is they can only be right if the entire industry is experiencing mass psychosis. Yes. Mass psychosis explains an incredible number of different and apparently unrelated problems with the industry. reply floating-io 3 hours agorootparentprev\"Vendor problems\" is a red herring, IMO; you can have those in the cloud, too. It's been my experience that those who can build good, reliable, high-quality systems, can do so either in the cloud or on-prem, generally with equal ability. It's just another platform to such people, and they will use it appropriately and as needed. Those who can only make it work in the cloud are either building very simple systems (which is one place where the cloud can be appropriate), or are building a house of cards that will eventually collapse (or just cost them obscene amounts of money to keep on life support). Engineering is engineering. Not everyone in the business does it, unfortunately. Like everything, the cloud has its place -- but don't underestimate the number of decisions that get taken out of the hands of technical people by the business people who went golfing with their buddy yesterday. He just switched to Azure, and it made his accountants really happy! The whole CapEx vs. OpEx issue drives me batty; it's the number one cause of cloud migrations in my career. For someone who feels like spent money should count as spent money regardless of the bucket it comes out of, this twists my brain in knots. I'm clearly not a finance guy... reply sgarland 2 hours agorootparent> or are building a house of cards that will eventually collapse (or just cost them obscene amounts of money to keep on life support) Ding ding ding. It's this. > The whole CapEx vs. OpEx issue drives me batty Seconded. I can't help but feel like it's not just a \"I don't understand money\" thing, but more of a \"the way Wall Street assigns value is fundamentally broken.\" Spending $100K now, once, vs. spending $25K/month indefinitely does not take a genius to figure out. reply krsgjerahj 2 hours agorootparentprevyou forgot cogs it's all about painting the right picture for your investors, so you make up shit and classify as cogs or opex depending on what is most beneficial for you in the moment reply johnklos 4 hours agorootparentprev> The problem with your claims here is they can only be right if the entire industry is experiencing mass psychosis. What's the market share of Windows again? ;) reply mardifoufs 24 minutes agorootparentYou're proving their point though. Considering that there are tons of reasons to use windows, some people just don't see them and think that everyone else is crazy :^) (I know you're joking but some people actually unironically have the same sentiment) reply noprocrasted 4 hours agorootparentprevThere's however a middle-ground between run your own colocated hardware and cloud. It's called \"dedicated\" servers and many hosting providers (from budget bottom-of-the-barrel to \"contact us\" pricing) offer it. Those take on the liability of sourcing, managing and maintaining the hardware for a flat monthly fee, and would take on such risk. If they make a bad bet purchasing hardware, you won't be on the hook for it. This seems like a point many pro-cloud people (intentionally?) overlook. reply cyberax 39 minutes agoparentprev> The whole push to the cloud has always fascinated me. I get it - most people aren't interested in babysitting their own hardware. For businesses, it's a very typical lease-or-own decision. There's really nothing too special about cloud. > On the other hand, a business of just about any size that has any reasonable amount of hosting is better off with their own systems when it comes purely to cost. Nope. Not if you factor-in 24/7 support, geographic redundancy, and uptime guarantees. With EC2 you can break even at about $2-5m a year of cloud spending if you want your own hardware. reply TheFlyingFish 11 minutes agoprevLots of people here mentioning reasons to both use and avoid the cloud. I'll just chip in one more on the pro-cloud side: reliability at low scale. To expand: At $dayjob we use AWS, and we have no plans to switch because we're tiny, like ~5000 DAU last I checked. Our AWS bill isIf its an outage at AWS its their fault. Well, still your fault, but easy to judo the risk into clients saying supporting multi-cloud is expensive and not a priority. reply graemep 3 hours agorootparentManagement in many places will not even know what multi-cloud is (or even multi-region). As Cloudstrike showed, if you follow the crowd and tick the right boxes you will not be blamed. reply bobnamob 21 minutes agorootparentnit: Crowdstrike Unless the incident is now being referred to as “Cloudstrike”, in which case, eww reply ghaff 5 hours agorootparentprevThere are other, if often at least tangentially related, reasons but more than I can give justice to in a comment. Many people largely got a lot of things wrong about cloud that I've been meaning to write about for a while. I'll get to it after the holidays. But probably none more than the idea that massive centralized computing (which was wrongly characterized as a utility like the electric grid) would have economics with which more local computing options could never compete. reply oftenwrong 4 hours agorootparentprevIn small companies, cloud also provides the ability to work around technical debt and to reduce risk. For example, I have seen several cases where poorly designed systems that unexpectedly used too much memory, and there was no time to fix it, so the company increased the memory on all instances with a few clicks. When you need to do this immediately to avoid a botched release that has already been called \"successful\" and announced as such to stakeholders, that is a capability that saves the day. An example of de-risking is using a cloud filesystem like EFS to provide a pseudo-infinite volume. No risk of an outage due to an unexpectedly full disk. Another example would be using a managed database system like RDS vs self-managing the same RDBMS: using the managed version saves on labor and reduces risk for things like upgrades. What would ordinarily be a significant effort for a small company becomes automatic, and RDS includes various sanity checks to help prevent you from making mistakes. The reality of the industry is that many companies are just trying to hit the next milestone of their business by a deadline, and the cloud can help despite the downsides. reply sgarland 2 hours agorootparent> For example, I have seen several cases where poorly designed systems that unexpectedly used too much memory > using a managed database system like RDS vs self-managing the same RDBMS: using the managed version saves on labor As a DBRE / SRE, I can confidently assert that belief in the latter is often directly responsible for the former. AWS is quite clear in their shared responsibility model [0] that you are still responsible for making sound decisions, tuning various configurations, etc. Having staff that knows how to do these things often prevents the poor decisions from being made in the first place. [0]: https://aws.amazon.com/compliance/shared-responsibility-mode... reply graemep 1 hour agorootparentNot a DB admin, but I do install and manage DBs for small clients. My experience is that AWS makes the easy things easy and the difficult things difficult, and the knowledge is not transferable. With a CLI or non-cloud management tools I can create, admin and upgrade a database (or anything else) exactly the same way, locally, on a local VM, and on a cloud VM from any provider (including AWS). Doing it with a managed database means learning how the provider does it - which takes longer and I personally find it more difficult (and stressful). What I cannot do as well as a real DB admin could do is things like tuning. Its not really an issue for small clients (a few generic changes to scale settings to available resources is enough - and cheaper than paying someone to tune it). Come to think of it, I do not even know how to make those changes on AWS and just hope the defaults match the size of RDS you are paying for (and change when you scale up?). having written the above I am now doubting whether I have done the right thing in the past. reply Winsaucerer 4 hours agorootparentprevI'm very interested in approaches that avoid cloud, so please don't read this as me saying cloud is superior. I can think of some other advantages of cloud: - easy to setup different permissions for users (authorisation considerations). - able to transfer assets to another owner (e.g., if there's a sale of a business) without needing to move physical hardware. - other outsiders (consultants, auditors, whatever) can come in and verify the security (or other) of your setup, because it's using a standard well known cloud platform. reply wongarsu 1 hour agorootparentThose are valid reasons, but not always as straight forward: > easy to setup different permissions for users (authorisation considerations) Centralized permission management is an advantage of the cloud. At the same time it's easy to do wrong. Without the cloud you usually have more piecemeal solutions depending on segmenting network access and using the permission systems of each service > able to transfer assets to another owner (e.g., if there's a sale of a business) without needing to move physical hardware The obvious solution here is to not own your hardware but to rent dedicated servers. Removes some of the maintenance burden, and the servers can be moved between entities as you like. The cloud does give you more granularity though > other outsiders (consultants, auditors, whatever) can come in and verify the security (or other) of your setup, because it's using a standard well known cloud platform There is a huge cottage industry of software trying to scan for security issues in your cloud setups. On the one hand that's an advantage of a unified interface, on the other hand a lot of those issues wouldn't occur outside the cloud. In any case, verifying security isn't easy in or out of the cloud. But if you have an auditor that is used to cloud deployments it will be easier to satisfy them there, that's certainly true reply baxtr 5 hours agorootparentprevI predict a slow but unstoppable comeback of the sysadmin job over the next 5-10 years. reply homebrewer 5 hours agorootparentIt never disappeared in some places. In my region there's been zero interest in \"the cloud\" because of physical remoteness from all major GCP/AWS/Azure datacenters (resulting in high latency), for compliance reasons, and because it's easier and faster to solve problems by dealing with a local company than pleading with a global giant that gives zero shits about you because you're less than a rounding error in its books. reply graemep 5 hours agorootparentprev> it becomes just another arcane skill set Its an arcane skill set with a GUI. It makes it look much easier to learn. reply kwillets 48 minutes agoparentprevSSD's are also a bit of an achilles heel for AWS -- they have their own Nitro firmware for wear levelling and key rotations, due to the hazards of multitenant. It's possible for one EC2 tenant to use up all the write cycles and then pass it to another, and encryption with key rotation is required to keep data from leaking across tenant changes. It's also slower. We had one outage where key rotation had been enabled on reboot, so data partitions were lost after what should have been a routine crash. Overall, for data warehousing, our failure rate on on-prem (DC-hosted) hardware was lower IME. reply edward28 6 hours agoparentprevThe power of Moore's law. reply jeffbee 3 hours agoparentprevI don't see how point 2 could have come as a surprise to anyone. reply kayson 4 minutes agoprevAny ideas how they manage the ZFS encryption key? I've always wondered what you'd do in an enterprise production setting. Typing the password in at a prompt as any seem scalable (but maybe they have few enough servers that it's manageable) and keeping it in a file on disk or on removable storage would seem to defeat the purpose... reply akpa1 3 hours agoprevThe fact that Fastmail work like this, are transparent about what they're up to and how they're storing my email and the fact",
    "originSummary": [
      "Fastmail opts for using its own hardware over cloud services, focusing on cost optimization and long-term planning, leveraging 25 years of experience.",
      "They have upgraded to NVMe SSDs, enhancing performance and reliability compared to traditional Hard Disk Drives (HDDs).",
      "Fastmail uses ZFS for storage, which offers advantages like compression and encryption, and has chosen new 2U servers with SSDs for improved input/output, reliability, and cost-effectiveness."
    ],
    "commentSummary": [
      "Fastmail opts for using its own hardware over cloud services, citing cost-effectiveness for businesses with substantial hosting requirements.",
      "The discussion between cloud and self-hosting is often clouded by misconceptions, with some cloud advocates lacking technical insight.",
      "Fastmail's strategy underscores the feasibility of businesses managing their own systems, questioning the assumption that cloud solutions are universally superior."
    ],
    "points": 594,
    "commentCount": 314,
    "retryCount": 0,
    "time": 1734856591
  },
  {
    "id": 42483895,
    "title": "Rosetta 2 creator leaves Apple to work on Lean full-time",
    "originLink": "https://www.linkedin.com/posts/leonardo-de-moura-26a27b5_leanlang-leanprover-leanfro-activity-7274523099394400256-0F0x",
    "originBody": "Leonardo de Moura Senior Principal Applied Scientist at AWS, and Chief Architect at Lean FRO (non-profit) 5d Report this post I am thrilled to welcome Cameron Zwarich to the Lean FRO! As the brilliant creator of Rosetta 2 and an exceptional software developer with over 15 years of experience at Apple specializing in low-level systems software, Cameron will focus on enhancing Lean's code generator. I can’t wait to see the incredible impact his expertise will have on the Lean ecosystem! #LeanLang #LeanProver #LeanFRO 206 1 Comment Like Comment Share Copy LinkedIn Facebook Twitter Sean Jensen-Grey Engineer @ SUBSECRET 15h Report this comment This is phenomenal! I am so happy for everyone including Cameron. This makes my week. Like Reply 1 Reaction 2 Reactions To view or add a comment, sign in",
    "commentLink": "https://news.ycombinator.com/item?id=42483895",
    "commentBody": "Rosetta 2 creator leaves Apple to work on Lean full-time (linkedin.com)316 points by ladberg 16 hours agohidepastfavorite96 comments cwzwarich 16 hours agoThis is me! Didn’t expect to see this on here, but I’m looking forward to working with everyone else at the Lean FRO and the wider Lean community to help make Lean even better. My background is in mathematics and I’ve had an interest in interactive theorem provers since before I was ever a professional software engineer, so it’s a bit of a dream come true to be able to pursue this full-time. reply mattgreenrocks 3 hours agoparentRosetta 2 is easily one of the most technically impressive things I've seen in my life. I've done some fairly intense work applying binary translation (DynamoRIO) and Rosetta 2 still feels totally magical to me. reply markus_zhang 4 hours agoparentprevThank you! My work laptop is a M4 Macbook Pro so I really appreciate the beauty of Rosetta. Thank you for the effort! I just checked your LinkedIn and realized you joined Apple since 2009 (with one year of detour to Mozilla). You also graduated from Waterloo as a Pure Math Graduate student (I absolutely love Waterloo, the best Math/CS school IMO in my country - at the age of 40+ I'd go without doubt if they accept me). May I ask, what is the path that leads you to the Rosetta 2 project? I even checked your graduate paper: ( https://uwspace.uwaterloo.ca/items/4bc518ca-a846-43ce-92f0-8... ), but it doesn't look like it's related to compiler theory. (I myself studied Mathematics back in the day, but I was not a good student and I studied Statistics, which I joked that was NOT part of Mathematics, so I didn't take any serious Algebra classes and understand nothing of your paper) reply cwzwarich 2 hours agorootparent> May I ask, what is the path that leads you to the Rosetta 2 project? The member of senior management who was best poised to suggest who should work on it already knew me and thought I would be the best choice. Getting opportunities in large companies is a combination of nurturing relationships and luck. reply markus_zhang 2 hours agorootparentThank you for the information! I'm sure your skills are well trusted. reply flkenosad 3 hours agorootparentprevWaterloo really is the best CS school in the world. reply markus_zhang 2 hours agorootparentI have never been there, what do you consider to be its speciality comparing to say MIT and Berkeley? reply mixmastamyk 1 hour agorootparentprevBelgium, London, ABBA, Canada, or San Dimas? Why better than others? reply Insanity 26 minutes agorootparentlol, pretty sure it is Waterloo Canada / Ontario. People like to “idolize” their Alma Mater. reply cookiengineer 11 hours agoparentprevDo you have book recommendations in regards to disassembly, syscalls, x86/64 assembler etc? What do I need to know to be able to build something as advanced as rosetta? I am assuming that you reimplemented the syscalls for each host/guest system as a reliable abstraction layer to test against. But so many things are way beyond my level of understanding. Did you build your own assembler debugger? What kind of tools did you use along the way? Were reversing tools useful at all (like ghidra, binaryninja etc)? reply peterkelly 8 hours agorootparent\"Virtual Machines: Versatile Platforms for Systems and Processes\" by Jim Smith and Ravi Nair is a great book on the topic. reply markus_zhang 2 hours agorootparentThank you. Other than papers, I think this is one of the rare books that talk extensively about dynamic recompilation. I was hoping to learn more about the PPC M68K emulator (early version interpreter style and later version dynamic recompilation style) and definitely will read it. reply lenkite 7 minutes agoparentprevCan Lean can do what TLA+ does - model check thorny concurrency problems ? reply croemer 14 hours agoparentprevWe're you really _the_ creator of Rosetta 2? How big was the team, what was your role in it? Rosetta 2 is amazing, I'm genuinely surprised this is the work of just one person! reply cwzwarich 14 hours agorootparentI was the only person working on it for ~2 years, and I wrote the majority of the code in the first version that shipped. That said, I’m definitely glad that I eventually found someone else (and later a whole team) to work on it with me, and it wouldn’t have been as successful without that. When people think of a binary translator, they usually just think of the ISA aspects, as opposed to the complicated interactions with the OS etc. that can consume just as much (or even more) engineering effort overall. reply zmb_ 12 hours agorootparentAs someone frustrated in a team of 10+ that is struggling to ship even seemingly trivial things due to processes and overheads and inefficiencies, I would really appreciate some insights on how do you organize the work to allow a single developer to achieve this. How do you communicate with the rest of the organization? What is the lifecycle and release process like? Do you write requirements and specs for others (like validation or integration) to base their work on? Basically, what does the day to day work look like? reply cwzwarich 2 hours agorootparentWell, the first thing to realize about scaling codebases with developers is that an N developer team will usually produce a codebase that requires N developers to maintain. So by starting small and staying small until you reach a certain critical mass of fundamental decisions, you can avoid some of the problems that you get from having too many developers too early. You can easily also fall into the reverse trap: a historical core with pieces that fit too well together, but most of the developers on the team don’t intuitively understand the reasons behind all of the past decisions (because they weren’t there when they happened). This can lead to poorly affixed additions to a system in response to new features or requirements. As far as Rosetta in particular was concerned, I think I was just in the right environment to consistently be in a flow state. I have had fleeting moments of depression upon the realization that I will probably never be this productive for an extended period of time ever again. reply tonyedgecombe 10 hours agorootparentprev>How do you communicate with the rest of the organization? I wonder if Apple's renowned secrecy is a help with this. If nobody outside your small team knows what you are doing then it is hard for them to stick their oar in. reply ladberg 3 hours agorootparentFor the record I was interning on Cameron's team while he worked on Rosetta 2 and didn't even know myself what he worked on (the rest of the team and I were working on something else). I only found out later after it was released! reply iwontberude 1 hour agorootparentApple is like this, I have seen plenty of instances where you have one person carrying a team of 5 or more on their back. I always wonder how they manage to compensate them when it’s clear they are getting 10x more done. Hopefully they get paid 10x, but something tells me that isn’t true. reply croemer 14 hours agorootparentprevThat's super impressive. I remember being astonished that the x86 executable of Python running through Rosetta 2 on my M1 was just a factor of 2 slower than the native version. QEMU was something like a factor of 5-10x slower than native, IIRC. reply lostmsu 13 hours agorootparentQEMU probably had to account for differences in memory models. A fork with that stuff removed might be able to easily catch up. reply bonzini 12 hours agorootparentQEMU loses a bit from being a generic translator instead of being specialized for x86->ARM like Rosetta 2, Box64 or FEXEmu. It does a lot of spilling for example even though x86 has a lot fewer registers than aarch64. Flags are also tricky, though they're pretty well optimized. In the end the main issue with them is also the spilling, but QEMU's generic architecture makes it expensive to handle consecutive jump instructions for example. reply croemer 10 hours agorootparentI found this blog post reverse engineering Rosetta 2 translated code: https://dougallj.wordpress.com/2022/11/09/why-is-rosetta-2-f... reply bonzini 10 hours agorootparentInteresting. Yeah, being able to use Arm flags always is probably a big thing, since they even added hardware support for that. It's a huge achievement for a single person to have written most of that. reply RandomThoughts3 7 hours agorootparent> It's a huge achievement for a single person to have written most of that. Qemu was mostly Fabrice Bellard by himself at the beginning and plenty of emulators are single person project. It’s a field which lends itself well to single person development. How to properly architecture compiler/interpreter/emulator has been studied to death and everyone mostly uses the same core principles so there is little guess work as how to start (provided you have taken the time to study the field). If you are ready to do the work, you can reach a working translator from hard work alone. Then, the interesting work of optimising it starts. Don’t get me wrong, Rosetta 2 is a very impressive achievement because the performances are really good. I tip my metaphorical hat to whoever did it. My post is more in the spirit of you can do something in the same ballpark too if that’s your kick. reply sitkack 13 hours agorootparentprevThat is fascinating that this amazing system was the work of largely one person. You mentioned that interacting with the OS was super difficult. What were the most enjoyable aspects of building Rosetta? reply porphyra 12 hours agorootparentI am also amazed that this was the work of largely one person. Having seamless and performant Rosetta 2 was a major factor why the Apple transition from Intel to Apple Silicon was viable in the first place! reply archagon 13 hours agorootparentprevIt's a shame that Apple's stated intent is to throw the project away after a while. Personally, I really hope it sticks around forever, though I'm not optimistic. reply wtallis 11 hours agorootparentRosetta 2 can't go away until Apple is ready to also retire Game Porting Toolkit. At most, they might drop support for running regular x86 macOS applications while keeping it around for Linux VMs and Windows applications, but that would be pretty weird. reply nubinetwork 10 hours agorootparent> game porting toolkit I don't understand why Apple even bothers these days, I wouldn't be surprised if Apple's gaming market is a quarter of what the Linux gaming market currently is (thanks to Valve and their work on proton and by extension wine)... reply drexlspivey 9 hours agorootparentBecause people want to use their fancy new hardware to play games? Linux market share wouldnt be increasing so fast if Valve didn’t do the work so why shouldn’t Apple do the same? reply nubinetwork 9 hours agorootparent> so why shouldn't Apple do the same? If Apple truly cared, they would stop blocking older games from being run on newer versions of OSX... reply bzzzt 6 hours agorootparentThey don't really block games though. It's more like they don't want to maintain the roads the games need to run on. Transitioning to ARM wasn't possible if they had to support 2 x86 ABI's and an extra ARM 32 bits ABI. Throw in another migration and you have an untestable number of legacy combinations. reply duskwuff 11 hours agorootparentprevIn principle, the Linux Rosetta binaries should remain usable well into the future. Even if Apple discontinues support for Rosetta in their VMs, there's very little (beyond a simple, easily removed runtime check) preventing them from being used standalone. reply vbezhenar 9 hours agorootparentAFAIK Linux Rosetta does not work standalone but uses some channels to exchange x86 and arm binary code between Linux guest and macOS host. Actual translation happens in the macOS. reply duskwuff 41 minutes agorootparentYou'd think so, but no. With a patch to remove the runtime check, Rosetta works on Asahi Linux, with no macOS kernel present at all. reply saagarjha 9 hours agorootparentprevThe kernel could drop support. reply K7PJP 12 hours agorootparentprevWhere did Apple state that Rosetta 2 was to be deprecated? reply larusso 10 hours agorootparentI think they assuming from the past that this will happen. When Apple moved from powerPC to x86 there was Rosetta 1. It got deprecated as well. reply skissane 9 hours agorootparentI think it is different this time. A lot of developers use Rosetta 2 for Linux to run x86-64 Linux Docker containers under macOS (including me). They'll be upset if Apple discontinues Rosetta 2 for Linux. By contrast, once the PPC-to-Intel transition was under way, Rosetta was only used for running old software, and as time went by that software became increasingly outdated and use of it declined. While I think Rosetta 2 for macOS usage will likely decline over time too, I think Rosetta 2 for Linux usage is going to be much more stable and Apple will likely maintain it for a lot longer. Maybe if we eventually see a gradual migration of data centres from x86-64 to ARM, Rosetta 2 for Linux usage might begin to also decline, and then Apple may actually kill it. But, if such a migration happens, it is going to take a decade or more for us to get there. reply larusso 7 hours agorootparentI just pointed out what happened in the past. I have no clue if Apple will deprecate it and what reason they put forward doing so. I personally like the fact that I can run both arm and x86 binaries. But I think judging Apple that if they don’t have a personal reason to support Linux (they also use it for their services) they will remove it. But deprecated dons’t mean it will be removed anytime soon. Apple keeps APIs and frameworks as long as they don’t interfere with something else. reply CodesInChaos 7 hours agorootparentprevWhat's the advantage of running x86-64 Linux Docker containers over running ARM Linux Docker containers? Aren't most distributionss and packages available for both platforms? reply 0x0 1 hour agorootparentMicrosoft SQL Server is only available as an x86-64 docker container binary. They actually had a native(?) arm64 docker container under the name \"azure-sql-edge\", which was (and still is) super useful as you can run it \"natively\" in an arm64 qemu linux for example, but alas that version was not long lived, as Microsoft decided to stop developing it again, which feels like a huge step backwards. https://techcommunity.microsoft.com/blog/sqlserver/azure-sql... There's probably other closed-source linux software being distributed as amd64-only binaries (rosetta 2 for linux VMs isn't limited to docker containers). reply spockz 6 hours agorootparentprevSome images are only available for amd64 still. Like oracle databases. Even if there is an arm64 of a recent version of the app, it may not exist for older versions that you want to test against. reply xienze 6 hours agorootparentprevThe advantage is the fact that they exist. Not every Docker container is built for multiple platforms. reply GeekyBear 2 hours agorootparentprevThe first Rosetta was based on licensed technology, used at a time when Apple was still pinching pennies. It made financial sense to stop paying the licensing fee to include it in each new version of the OS as quickly as possible. There is no financial incentive to remove the current version of Rosetta, since it was developed in-house. reply spockz 11 hours agorootparentprevIt is my experience that it is easier to create good quality things as an individual than as a team. Especially for the core of a product. Also look at Asahi. However, to really finish/polish a product you need a larger group of people. To get the UI just right, to get the documentation right, to advocate the product, to support it. It is easily possible to have 10 people working on the team and only having a single core person. Then find someone to act as product manager while as the core person you can focus on the core of the product while still setting the direction without having to chase all the other work. It is possible, but not easy to set up in most organisations. You need a lot of individual credit/authority and/or the business case needs to be very evident. reply steego 14 hours agoparentprevThis is exciting! Given your experience with Rosetta 2 and your deep understanding of code translation and optimization, what specific areas in Lean’s code generation pipeline do you see as ‘low-hanging fruit’ for improvement? Additionally, which unique features or capabilities of Lean do you find most promising or exciting to leverage in pushing the boundaries of efficient and high-quality code generation? reply singularity2001 10 hours agoparentprevsorry to hijack the discussion but do you see any chance of consolidating the theoretical framework of real numbers with practical calculations of floats? That is if I proof the correctness of some theorem for real numbers ideally I would just use that as the algorithm to compute things with floats. also I was shocked to learn that the simple general comparison of (the equality of) two real numbers is not decidable, which is very logical if you think about it but an enormous hindrance for practical applications. Is there any work around for that? reply zozbot234 10 hours agorootparentYou can use floats to accelerate interval arithmetic (which is \"exact\" in the sense of constructive real numbers) but that requires setting the correct rounding modes, and being aware of quirks in existing hardware floating point implementations, some of which may e.g. introduce non-exact outputs in several of the least significant digits, or even flush \"small\" (for unclear definitions of \"small\", not always restricted to FP-denormal numbers) results to zero. Equality is not computable in the general case, but apartness can be stated exactly. For some practical cases, one may also be able to prove that two real numbers are indeed equal. reply adamnemecek 15 hours agoparentprevWhat was the tipping point that made you want to work on Lean? reply cwzwarich 13 hours agorootparentI don't think there was a single tipping point, just a growing accumulation of factors: - the release of Lean 4 slightly over a year ago, which impressed me both as a proof assistant and a programming language - the rapid progress in formalization of mathematics from 2017 onward, almost all of which was happening in Lean - the growing relevance of formal reasoning in the wake of improvements in AI - seeing Lean's potential (a lot of which is not yet realized) for SW verification (especially of SW itself written in Lean) - the establishment of the Lean FRO at the right time, intersecting all of the above reply a1o 9 hours agorootparentHow does Lean compares with Coq? (I am not familiar with Lean but am familiar with Coq) reply denotational 9 hours agorootparentMario Carneiro’s MS Thesis has a good overview of the type theory and how it compares to Coq: https://github.com/digama0/lean-type-theory/releases/downloa... reply brcmthrowaway 16 hours agoparentprevSurprised you didnt go into something AI adjacent reply fantod 15 hours agorootparentI don't know what his reasons are but it makes sense to me. Yes, there are incredible results coming out of the AI world but the methods aren't necessarily that interesting (i.e. intellectually stimulating) and it can be frustrating working in a field with this much noise. reply uoaei 15 hours agorootparentI don't want to come across as too harsh but having studied machine learning since 2015 I find the most recent crop of people excited about working on AI are deep in Dunning-Kruger. I think I conflate this a bit with the fascination of results over process (I suppose that befuddlement is what led me to physics over engineering) but working in ML research for so long it's hard to gin up a perspective that these things are actually teleologically useful, and not just randomly good enough most of the time to keep up the illusion. reply croemer 14 hours agorootparentWhat do you mean by \"things that are actually teleologically useful\"? Fellow physicist here by the way reply Baeocystin 14 hours agorootparentNot OP, but I'm assuming he means that they are maddeningly black-boxy, if you want to know how the sausage is made. reply uoaei 13 hours agorootparentprevLike useful in an intentional way: purpose-built and achieves success via accurate, parsimonious models. The telos here being the stated goal of a structurally sound agent that can emulate a human being, as opposed to the accidental, max-entropy implementations we have today. reply calf 12 hours agorootparentIs a guide dog teleologically useful? reply jrflowers 11 hours agorootparentNot if you’re taste testing ceviche reply croemer 11 hours agorootparentprevI see, so humans are also not usefully intelligent in an intentional way, because they also follow the 2nd law of thermodynamics and maximize entropy and aren't deterministic? reply throw646577 8 hours agorootparentPure, refined “but humans also”. reply deet 13 hours agorootparentprevI feel that way sometimes too. But then I think about how maddeningly unpredictable human thought and perception is, with phenomena like optical illusions, cognitive biases, a limited working memory. Yet it is still produces incredibly powerful results. Not saying ML is anywhere near humans yet, despite all the recent advances, but perhaps a fully explainable AI system, with precise logic, 100% predictable, isn’t actually needed to get most of what we need out of AI. And given the “analog” nature of the universe maybe it’s not even possible to have something perfect. reply hn_throwaway_99 12 hours agorootparent> But then I think about how maddeningly unpredictable human thought and perception is, with phenomena like optical illusions, cognitive biases, a limited working memory. I agree with your general point (I think), but I think that \"unpredictable\" is really the wrong word here. Optical illusions, cognitive biases and limited working memory are mostly extremely predictable, and make perfect sense if you look at the role that evolution played in developing the human mind. E.g. many optical illusions are due to the fact that the brain needs to recreate a 3-D model from a 2-D image, and it has to do this by doing what is statistically most likely in the world we live in (or, really, the world of African savannahs where humans first evolved and walked upright). This, it's possible to \"tricks\" this system by creating a 2D image from a 3D set of objects that is statistically unlikely in the natural world. FWIW Stephen Pinker's book \"How the Mind Works\" has a lot of good examples of optical illusions and cognitive biases and the theorized evolutionary bases for these things. reply adamnemecek 15 hours agorootparentprevLean is AI adjacent. reply saagarjha 15 hours agorootparentOnly because the AI people find it interesting. It's not really AI in itself. reply cwzwarich 15 hours agorootparentIf you’re interested in applications of AI to mathematics, you’re faced with the problem of what to do when the ratio of plausible proofs to humans that can check them radically changes. There are definitely some in the AI world who feel that the existing highly social construct of informal mathematical proof will remain intact, just with humans replaced by agents, but amongst mathematicians there is a growing realization that formalization is the best way to deal with this epistemological crisis. It helps that work done in Lean (on Mathlib and other developments) is reaching an inflection point just as these questions become practically relevant from AI. reply mkl 14 hours agorootparentprevIt's not AI in itself, but it's one of the best possibilities for enabling AI systems to generate mathematical proofs that can be automatically verified to be correct, which is needed at the scale they can potentially operate. Of course it has many non-AI uses too. reply zozbot234 11 hours agorootparentprevProof automation definitely counts as AI. Not all AI is based on machine learning or statistical methods, GOFAI is a thing too. reply fspeech 12 hours agorootparentprevIf you want to have superhuman performance like AlphaZero series you need a verifier (valuation network) to tell you if you are on the right track. Lean (proof checker) in general can act as a trusted critic. reply ofrzeta 12 hours agorootparentprevThey do have AI on their roadmap, though: https://lean-fro.org/about/roadmap-y2/ reply trenchgun 10 hours agorootparentprevIt's not ML, but it is AI reply evaneykelen 10 hours agoprevIn a previous discussion the name of Gary Davidian is mentioned who also — initialy single-handed — did amazing work on architecture changes at Apple. There’s an interview with him in the Computer History Museum archive. https://news.ycombinator.com/item?id=28914208 https://youtu.be/MVEKt_H3FsI?si=BbRRV51ql1V6DD4r reply markus_zhang 3 hours agoparentFrom wiki it looks like David's emulator is perhaps uses interpreting as wiki says Eric's uses dynamical recompilation and Connectix' is even faster so maybe more optimization. I tried to find the source code of any without any success. reply brcmthrowaway 16 hours agoprevWhat is Lean FRO? reply cwzwarich 16 hours agoparenthttps://lean-fro.org/about/ reply hinkley 59 minutes agorootparentYeah that really doesn’t help. reply threeseed 15 hours agoparentprevhttps://lean-lang.org/lean4/doc/ reply jemmyw 14 hours agorootparentThere are a lot of broken links in the docs. Like most of the feature links. reply kmill 11 hours agorootparentThere's a completely new language reference in the process of being written: https://lean-lang.org/doc/reference/latest/ (by David Thrane Christiansen, co-author of The Little Typer, and Lean FRO member) Some links here seem to be broken at the moment — and David's currently on vacation so they likely won't be fixed until January — but if you see for example https://lean-lang.org/basic-types/strings/ it's supposed to be https://lean-lang.org/doc/reference/latest/basic-types/strin... reply croemer 15 hours agorootparentprevThat answers the Lean part, FRO stands for Focused Research Organization reply thih9 8 hours agoparentprevBackground about the organization: https://en.m.wikipedia.org/wiki/Convergent_Research Their proof assistant / programming language: https://en.m.wikipedia.org/wiki/Lean_(proof_assistant) reply bagels 12 hours agoparentprev\"we aim to tackle the challenges of scalability, usability, and proof (Mathematics) automation in the Lean proof assistant.\" reply revskill 8 hours agoprevThe linkedin back button is weird. Instead of coming back to hn after back button, it goes to its homepage. reply Yujf 5 hours agoparentIts not weird its just disgusting. The back button should go back reply ein0p 9 hours agoprevApple just seems to be bleeding talent left and right. I wonder what's going on over there to cause people to leave when the job market is as uncertain as it is right now. reply blitzar 9 hours agoparentThe 20th million doesn't hit as hard as the 19th and when you make 2x your salary on the dividends on your stock you start to wonder why not just do something more interesting. reply tchbnl 6 hours agoparentprevSometimes people just want to work on cool stuff and have the luxury of being able to do that. Rosetta 2 is shipped and done. reply turnsout 2 hours agoparentprevYou could have posted this in 1985 and been right. Talented people have options. reply raverbashing 9 hours agoparentprevCitation needed? I mean, there will always be long tenured people leaving, even without offers on the table Some jobs get old eventually reply danielktdoranie 9 hours agoprev [–] I am pretty sure “lean” is that codeine cough syrup rappers drink reply dilsmatchanov 6 hours agoparent [–] https://youtu.be/4Or-5OLCNDA?si=mzd_o0573HPgCVrl&t=51 reply hinkley 44 minutes agorootparent [–] The audio on this is about the worst I’ve ever heard on YouTube. I fast forwarded and at least he stops playing that loud music over his quiet voice, but damn. He gets off topic a lot (bullies, amphetamine salts??) and spends the entire time talking to the commenters not the video recording. Surely, there’s a better video out there than this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Leonardo de Moura, a key figure at AWS and Lean FRO, announced that Cameron Zwarich, known for creating Rosetta 2 at Apple, has joined Lean FRO.",
      "Cameron Zwarich will focus on improving Lean's code generator, bringing his extensive software development experience to the team."
    ],
    "commentSummary": [
      "The developer behind Rosetta 2 has left Apple to focus on Lean, a proof assistant and programming language, highlighting a shift towards formal reasoning and software verification.",
      "Rosetta 2 is recognized for its technical prowess, with significant contributions from this developer before a team was established.",
      "The transition to Lean is motivated by its potential in AI advancements, with goals to enhance scalability, usability, and proof automation."
    ],
    "points": 316,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1734832911
  },
  {
    "id": 42482829,
    "title": "A data table thousands of years old (2020)",
    "originLink": "https://www.datafix.com.au/BASHing/2020-08-12.html",
    "originBody": "For a full list of BASHing data blog posts see the index page. A data table thousands of years old I knew that data tables had been around a long time, but I didn't appreciate how long until I read recently about account-keeping in ancient Mesopotamia. The accounts were written on clay tablets, sometimes with impressed lines to mark off rows and columns. Here's a drawing of the front and back sides of an example: This tablet was found on the site of the old city of Larsa near the mouth of the Euphrates in Iraq. Someone wrote on the tablet in the Old Babylonian Period, ca 3600-4000 years ago. The cuneiform text was transliterated and translated by Eleanor Robson, who currently heads the History Department at University College London. You can see her efforts at this website by clicking on the tablet's British Museum catalog number, BM 085232. Below I've put Prof. Robson's translation into a spreadsheet. Items in square brackets are guessed fill-ins. The \"s.\" stands for shekels and \"m.\" is mūšar, equal to 60 shekels. A = \"Troops, available assets\" B = \"Those who were sent from Sin's dyke\" C = \"Earth, the (daily) work assignment of 1 man\" D = \"Its earth\" E = \"Check made\" F = \"Arrears\" G = \"Its name\" It looks to me like a payroll summary from a construction project, but that's a modern-day view. What's clear is that the person who inscribed the clay was organising similar data items neatly in rows and columns (with column headers), and operating on those items in ways familiar to us moderns. Column A, for example, looks like the sum of columns B (or E) and F. Column C is a base rate of pay, and column D is the base rate multiplied by the quantity in column B (or E). Each non-header row corresponds to a person or persons in column G, except the last row with column totals. This remarkable record is evidence that data tables (as we understand them) were used for record-keeping more than 3500 years ago. It's tempting to think that civilisation progresses step-wise, and that you could trace the use of data tables in an unbroken path from clay tablets to today's spreadsheets. I don't think that's a realistic view. Inventions get lost or forgotten, then re-invented. What was obvious to that account-keeper in ancient Mesopotamia might not have been obvious at later times and in other places. I'm pretty confident, though, that in another thousand years there will still be ancient data tables \"archived\" underground in Iraq, while todays' billions of spreadsheets in digital form and on non-archival paper will have long since disappeared. Last update: 2020-08-12 The blog posts on this website are licensed under a Creative Commons Attribution-NonCommercial 4.0 International License",
    "commentLink": "https://news.ycombinator.com/item?id=42482829",
    "commentBody": "A data table thousands of years old (2020) (datafix.com.au)227 points by rickcarlino 20 hours agohidepastfavorite98 comments jbkcc 19 hours agoThis is amazing. I’ve been collecting images of tables in an are.na album for a while, trying to get a handle on all the ways they show up in visual culture. This one is by far the oldest I’ve ever seen! If you’re interested in this you might enjoy the album, too. It’s https://www.are.na/joshua-kopin/tabular-presentation reply mvkel 13 minutes agoparentProjects like this are marvelous. Without it, we will be re-learning so many things that we should already know. reply Micoloth 9 hours agoparentprevHa. What an amazing collection! It hits so many right sposts. Thanks for sharing it reply GnarfGnarf 3 hours agoprevI'm working on a project to 3D-print tablets of text, press them onto clay slabs, and fire the latter in a kiln. Should preserve the information, such as biographies, for as long as Babylonian tablets. reply jvm___ 1 hour agoparentI've wondered if you could stamp them into an aluminum can. Like with a typewriter (obviously too weak) or some vintage typesetting press device. Not sure if the aluminum would last it probably would. reply yurishimo 25 minutes agorootparentDepends on how thick the metal is. I’ve seen aluminum cans be eaten away by time. Something a few millimeters should suffice. reply eieio 18 hours agoprevIs there a good word for \"obvious\" that doesn't have negative connotations? When I see something like this it makes me think about how a spreadsheet structure is \"obvious\" - but I mean it positively! It's a beautiful, intuitive, almost inevitable way to lay out data, and I'm delighted that folks came up with something like this so long ago. I feel this way about a lot of my favorite posts on HN, whether they're a bit of history, a totally new invention, or something different entirely. And I certainly feel it here. reply Hackbraten 15 hours agoparentThere's a German word \"naheliegend\" (pronounced nuh-her-lee-guend), whose literal translation would be \"lying nearby\". I think we typically use it as a mixture of \"sensible\", \"seemingly natural\" and \"obvious\" without that confrontational subtone. reply philipswood 11 hours agorootparentIn Afrikaans we have the (slightly old fashioned): Voor die hand liggend. Something like: It is right in front of your hands. reply Insanity 35 minutes agorootparentStemming from the equivalent Dutch “voor de hand liggend”. And I would not have a negative connotation with it in the right context. (E.g “Een tabel is een voor de hand liggende structuur om data te representeren” - a table is an ‘obvious’ manner to represent data) reply eieio 14 hours agorootparentprevI think plenty of other comments have made good suggestions but that this clearly takes the cake for me! I suppose I shouldn't be surprised that German has a great word for this, although I admit when I started reading your comment I expected it to be a compound word. I quite like the literal translation too! reply codetrotter 14 hours agorootparentWe have the same compound word in Norwegian, with same kind of meaning: Nærliggende > 2. som naturlig faller en i tanken ; som det er naturlig å gripe til (Aside from also having a literal meaning of being in physical proximity.) Translated: “Which naturally comes to mind; which it is natural to resort to.” https://naob.no/ordbok/n%C3%A6rliggende This Norwegian word would not have naturally come to mind for me though, if it wasn’t for GP mentioning the German equivalent of it. It is not a word I usually use myself. But I do hear others use it now and then. reply dpassens 3 hours agorootparentprevIt is a compound word. Nahe (close by) liegend (lying). reply dyauspitr 12 hours agorootparentprevThat word sounds like you’re saying “near the ground”. reply RedNifre 4 hours agorootparentIt's more like \"nearby, on the ground\". reply gitaarik 3 hours agorootparentprevYeah, I think saying \"near lying\" or \"close to lying\" would be less confusing. Also that is actually the order in the German word also! Because it consists of 2 words written as 1. reply kijin 12 hours agorootparentprevA similar English expression might be \"low-hanging fruit\", but again for some reason we've attached negative connotations to it. I don't know why English keeps doing that. It feels so cynical. reply t-3 4 hours agorootparentIt's not a fault of the language, it's the culture. \"Average\" and \"mediocre\" both have negative connotations in vernacular use as well, even though they're normal and should be expected. If we expect excellence and world-shaking performance as the standard, good enough will not be good enough. reply coldtea 5 hours agorootparentprevI don't think \"low hanging fruit\" has negative connotations attached to it. The only negative sentiment tangentially associated with it is that when it's exhausted, further progress slows down. reply emmelaich 6 hours agorootparentprevOr \"right in front of your face\". Though that's used with and without negative connotations. reply noduerme 16 hours agoparentprevI think once someone wrote a list (a 1D array), it was pretty inevitable it would turn into a 2D array within a week or a month. But it took what, another 4000 years for people to start writing arrays of 3 dimensions or more? And then within a couple centuries we got tensors, and the arrays are too big to check. reply thaumasiotes 14 hours agorootparent> But it took what, another 4000 years for people to start writing arrays of 3 dimensions or more? Paper is two-dimensional. reply coldtea 5 hours agorootparentSo? You can write arrays of whatever dimensions on paper, it's a matter of notation, not of the substrate. reply d0mine 12 hours agorootparentprevbook is 3D reply HappMacDonald 9 hours agorootparentTrilogy is 4Dnine nineDnine depending where you pick it up reply hammock 18 hours agoparentprevColumn headers as well, as per modern convention, as opposed to row headers. reply notorandit 11 hours agorootparentUsually we put \"properties\" as column headers while rows represent entities whose those properties are assigned or recorded. It would be interesting to understand why it's not been the other way around or whether Sumerians used both orientations. reply HappMacDonald 9 hours agorootparentI'd imagine it's down to our convention of writing left to right, so more-related data points (such as properties of the same item) get arranged left to right. A quick test to that hypothesis (which I'm too lazy to try to perform but offer to anyone who might be interested in looking or who might already know) would be looking at ancient Chinese table layouts. :) reply 6510 7 hours agorootparentprevthe last col is the header. I see the use of row-span too! Something we are still struggling to figure out. reply DamonHD 9 hours agoparentprevMy uncle, who was a top UK lawyer but not really into tech, basically reinvented a spreadsheet on paper spread over his office floor, while working on a hige planning case. Yes, I think that basic structure will pop out of a number of problem types, eg Gaussian elimination. reply MomsAVoxell 10 hours agoparentprevI had the thought that the columns served different levels of literacy - that there is a hierarchy of competence in the columns themselves, or at least that each column could be assigned to a different person for action. For example, the purpose of the columns containing sums could be the assignment to an individual (or eventual role) which is responsible exclusively for the paying-out of the sums indicated - whereas the prior columns were to be used by roles responsible for setting the amounts to be paid, and a role perhaps for assaying the land/works. Each column could be for an individual role, and thus the table indicates not only figures and amounts, but also organizational structure. If one flows from left to right, one can see different identities involved in filling in the cells, eventually terminating in the actual recipients of the funds being distributed. reply begueradj 12 hours agoparentprevIt's obvious nowadays in the era of a sofa and Netflix, not 4000 years ago where 9 out of 10 new born kids die and those who survived generally didn't make it until 25 years old, and where the primary issue of people was what to eat the following day and in case no tribe attacks them in the middle of the night if they would survive to the bite of that scorpion. reply defrost 12 hours agorootparentAlmost half of all births ended in death before the age of 5, greatly lowering the average. When infant mortality is removed, evidence seem to show averages of life expectancy for 3000 years ago to be around 52, give or take 15 years. https://pmc.ncbi.nlm.nih.gov/articles/PMC2625386/ reply noselasd 8 hours agorootparentprevWhen you're cities of 20-40k people 4000 years ago, there's quite a bit of admin work that has to be done - it's not all small farmer villages or hunter-gatherers. Ancient Sumeria was quite advanced. reply Cthulhu_ 9 hours agorootparentprevThat's a lot of assumptions about life 4000 years ago when the article provides evidence of someone doing admin work instead of worrying about food, \"tribe\" attacks and scorpion bites. reply coldtea 5 hours agorootparentprev>and those who survived generally didn't make it until 25 years old That's a myth. reply yard2010 7 hours agorootparentprevYou make the dystopian world we live in sounds like a disney utopia. Which is very nice. reply ziotom78 4 hours agoparentprevI'm not a native English speaker, but could \"natural\" be appropriate for this context? reply moffers 16 hours agoparentprevYou said it already! “Intuitive”. reply zipping1549 9 hours agorootparentYou can both be unintuitive at first and be obvious at the same time. Double entry accounting, for example. reply jasdi 14 hours agoparentprevAn artist once told me some people enjoy Making Contact with Beauty. In the Simplest of things. And that can become a goal or a guiding philosophy. It's like when you look at a facial expression in a frame of Calvin & Hobbes or Tintin or Miyazaki it is extremely SIMPLE. The fewest of dots, dashes and squiggles basically. Change them even a little and you get total shit. It captures Reality in such a fantastic way, exciting the exact same neurons in your head that something real does, that people have to come up with words for it like - Beauty. reply fifilura 10 hours agoparentprevHaving worked a lot with columnar data, I often have to tell the object oriented crowd that \"It's the rows and columns, stupid!\". (And that last sentence was a paraphrase. They are far from stupid, just differently wired). I think managers should be emboldened to do that too. They often work out their solutions in Excel. And then the developers turn those fine rows and columns into an object oriented soup. reply mrkeen 8 hours agorootparentThe problem is my rows typically don't have the same columns. A 'userCreated' row has 10 columns (for now), but a 'userDeleted' row overlaps on only two of those (let's say 'Datetime' and 'userId'). And userBanned brings in a new column 'reason' which isn't in the schema, so I have to store it in some catch-all json 'data' column which kills my db's size & performance. I persevere with the format, but always wish we were using the right tool for the job (nosql). reply 6510 7 hours agorootparentAs I started out in a time when you had to coin your own format for everything I passionately hate it when the data has to facilitate to the tool. I'm no db wizard so I feel terrible using the json cell unsure about the level of sin involved. I also adopted comma separated fields. Don't tell anyone reply CalRobert 10 hours agorootparentprevIt is indeed! But it fails when you need more dimensionality. reply shalmanese 6 hours agoparentprevCarcinization. All software inevitably evolves into an Excel that can read email given a long enough timeline. reply geor9e 16 hours agoparentprevemergent? natural? a 2D surface has two orthogonal directions, so if you're using lines, so your choices are either grid, slanted grid, or godawful mess reply highwind 18 hours agoparentprevHow about self-evident? reply hammock 18 hours agorootparentInnate, instinctive, intuitive, natural, automatic. I don’t think obvious is a bad word though. Descartes did not invent x-y coordinates until the 1600s, yet a table of columns and rows is totally natural and emergent given a two-dimensional recordkeeping medium reply HappMacDonald 9 hours agorootparentI'm never not going to be gobsmacked that Euclid didn't ever try using a coordinate grid as a tool. 8I reply mncharity 15 hours agoparentprevHmm, and 2D sort-into-piles is done even in kindergarten. Including one axis being ordered. Especially 2x2 sorts. Oddly, ordering both axes is very rare - size-vs-color yes, and color-vs-numberOfHoles, but not size-vs-numberOfHoles. Which was a puzzle when considering xkcd-ish discrete Ashby charts for K. Sort-within-cell is also uncommon. reply jiggawatts 9 hours agoparentprevAnd yet when I got bored during the COVID lockdown and decided to analyse the published data sets against infection spreading models such as SIR, to my horror I discovered that every published data set had something Stupid about it with a capital S. Most commonly it was transposed data, published with each day's data in columns instead of rows. I remember one official announcement from a state government health department that was investing significant money into developing a \"scalable solution\" because... they hit the 16K Excel maximum column count. Of course, they could have simply put their data into rows and \"scaled\" their existing solution to 1M data points, but they'd much rather pay Deloitte, Accenture, or whomever a couple of million dollars for a real enterprise system instead. Next time I come across idiocy like this, I'm going refer back to this article and point to the four thousand year old tablet and say: \"Those people got it! They understood how to do this! Why haven't you caught up to technology that was around before widespread adoption of the wheel!?\" reply nuancebydefault 4 hours agorootparentThe problem is mostly that some structure that looks good at start, looks bad after a while of using. Maybe the first data was on postit notes. As the pandamic kept returning in waves, they thought they could use data in excel with new dates per row. Then new beta, delta,... variants emerged and they ran out of horizontal screen real estate. reply dr_dshiv 9 hours agoparentprev“Natural” reply numpy-thagoras 16 hours agoprevSumerian Spreadsheets. This means only one thing: the History channel will find a way to attribute the creation of spreadsheets to aliens. reply Cthulhu_ 8 hours agoparentOr maybe it was a time traveling accountant? Either way, the truth is out there... reply closed 14 hours agoprevIt's neat to see tablets discussed in the context of modern tools. I recently helped edit an article for Great Tables[1] that discusses the history of tables like this, and recently Hannes mentioned a protocuniform tablet in his duckdb keynote at posit::conf()[2]. There's something really inspiring from realizing how far back tables go. [1]: https://posit-dev.github.io/great-tables/blog/design-philoso... [2]: https://youtu.be/GELhdezYmP0?si=bSISmFjeRpKxfLWq reply kijin 12 hours agoparent\"Table\" and \"tablet\" literally have the same root. It's flat surface, a two-dimensional blank space that is perfect for laying out data, dinner, or anything else you'd like to display. reply TZubiri 19 hours agoprevThe advantages of tables, are that you can visually or geometrically read the contents easily, whether it is reading a row and only a row, or wether it's reading the contents of a column sequentally. While we had spreadsheets since the 90s, which visually allow the user to create tables. Relational database take this concept to the very architecture in both the storage format and as in the data retrieval mechanisms. Relational databases define schemas with fixed length fields, and by extension each row has a fixed length. This is equivalent to the horizontal length of a column, but in terms of bytes. This allows for quickly finding the nth row of a table, or the ith field of a column. Query languages formalize the algorithm for reading a traditional table. Going row by row checking the description of each transaction (Select * from table), comparing it to our searched term (where description = salary), then going to the column with the destination account, and looking for that in another table with a similar process. Just that, interesting how the same metaphor lead to 2 very different types of accounting software. reply gerdesj 18 hours agoparent\"interesting how the same metaphor lead to 2 very different types of accounting software.\" The tablets are tabulated lists which is how anyone might do a shopping list or list of income and expenditure. Double entry book keeping is only around 600 years old (I'd have to look it up). That method requires an in from somewhere corresponding to an out from somewhere else. It enables or enhances all sorts of funny business and also cross checking and auditing. Then we move on to the full Nominal/Sales/Purchase ledgers with Cashbook and all the rest. Perhaps we might instead go for the personal version. Anyway, my point is that accounting does not depend on IT related metaphors. The tablets in OP are tabulated tallies of works and how they were generated - it is like a spreadsheet where the human is the computer. Funnily enough, we call them tablets instinctively. Computer originally meant a person who computed things. No need for metaphors at all 8) reply notorandit 11 hours agorootparentDEBK is also tabular. And it's a perfect solution when you cannot (or don't want) delete or update older data. Just like when you write on clay tablets. I wouldn't be surprised if we recover Sumerians example of DEBK tablets. reply TZubiri 16 hours agorootparentprevNot sure how double entry book keeping relates here. Not relevant to tablets, to excel, nor rel dbs. Is the argument here that single entry bookkeeping is not real accounting? reply zitterbewegung 18 hours agoparentprevLANPAR, available in 1969, was the first electronic spreadsheet but was on mainframes https://en.wikipedia.org/wiki/Spreadsheet?wprov=sfti1# reply gerdesj 19 hours agoparentprev\"While we had spreadsheets since the 90s\" I was using SuperCalc in the '80s. reply thristian 16 hours agorootparentVisiCalc, the first computerised spreadsheet, was released in 1979. Presumably there were non-computerised spreadsheets, actual large sheets of paper, used for calculations before that. reply TZubiri 16 hours agorootparentThe tab character, along with record separators were present in the OG ascii block too, so they were probably always there. reply TZubiri 16 hours agorootparentprevMy bad. Also lotus 123. reply gerdesj 19 hours agoparentprev\"Relational databases define schemas with fixed length fields\" What is a varchar or a blob? Even a .csv allows for a variable length field (by default). I think you missed out the word: \"can\". Fixed field width is an optimisation strategy not a requirement. reply TZubiri 16 hours agorootparentNot strong on db internals, but those are 100% the exception, late additions, and not recommended for performance. The table is stored as a fixed length structure and var length fields are pointers to some other place. In the same manner that a traditional table might point to some other book for more details. Csv is also exclusively variable length, and it's nevet fixed length. Another example of fixed length structures are arrays. I'm not postulating a novel breakthrough. reply panstromek 11 hours agorootparentSqlite stores everything as variable length I believe. They have their own varint type for storing integers. reply notorandit 11 hours agoprevWhat it's not obvious it the amount of technical and cultural advancements Sumerians did. We don't know enough about them as their history has been mostly lost and only crumbles and leftovers can be recovered from the dust of the millennia. Besides a bunch of words still in use, in some form, in modern languages, the writing itself seems not to be the greatest invention, while bringing humanity from prehistory silence to history chatter. I wouldn't be surprised if we found evidence of more technical and social advancements we have given for granted in the past thousand years. reply rezmason 11 hours agoprevFor years I've wondered what the first, earliest color lookup table was. Like any mapping from an index to a color value. Like a design for a Roman mosaic that indexes tesserae, or a declaration of which parts of a statue or mural would receive which color paint. Or even the inventory of someone who traded in pigments. reply uncomplexity_ 2 hours agoprevthat's a heavy ass ipad to bring around reply smpx7 10 hours agoprevExcel -2k reply psd1 9 hours agoparentOh god. Debugging macros was horrible before VB.Cuneiform. You had to sprinkle your code with because there was no support for reply nuancebydefault 4 hours agorootparentStill trying to debug those 2 sentences... reply BeefWellington 17 hours agoprevThanks for sharing this. Pretty awesome to see how old aspects of technology are, especially as relates to clear and concise communication. reply niobe 19 hours agoprevExcel is in our DNA and will never die reply jon_richards 18 hours agoparentUnfortunately our DNA is also in excel. Several genes had to be renamed because they kept being identified as dates. reply smcin 9 hours agorootparentExcellent reply TZubiri 19 hours agoparentprevFunnily enough our DNA does not use a fixed-length offset mechanism. It uses null termination sequences (and start sequences too, for some reason.) Which is closer to the storage mechanism of excel (XML), and not to it's visualization interface (tables). reply klabb3 17 hours agorootparentInteresting. Well yeah null termination seems better if (a) you don’t have an integer encoding and (b) you have random ”bit” flips. reply TZubiri 16 hours agorootparentI don't think you need integer encoding to process fixed lengths. They do it just fine at the word level for codons. You would need a specific mechanic processors for each different schema length pattern though. I think bit flips have no effect on the appropriateness of either fixed length or null termed. But omissions and comissions are probably why anything fixed length doesn't work. reply mcphage 19 hours agoprev> I'm pretty confident, though, that in another thousand years there will still be ancient data tables \"archived\" underground in Iraq, while todays' billions of spreadsheets in digital form and on non-archival paper will have long since disappeared. Probably, but you never know. The Mesopotamians didn’t intend their tablets to last this long, either—but they often got burned in fires, which hardened them so they lasted. So some of our artifacts might get accidentally preserved as well. reply TZubiri 19 hours agoparentThe older the stuff you read is, the stronger the selection bias. There must be a huge amount of civilizations that were writing on paper or papyrus around that era, but they just didn't survive. The success of purposeful creation of monuments is usually attributed to their size, like pyramids. Turns out making it big is a pretty good strategy if you want something to last and (not loose it). I'm sure in mileniums we will have both purposefully long lasting small and big monuments, as well as unintentional long lasting records. reply Cthulhu_ 8 hours agorootparentThis is what I don't really understand about modern-day rich / famous people; they'll build big houses and yachts, and some governments even build government seats and palaces which might be preserved for the ages. But it doesn't feel like they're building \"monuments\" per se. Then again, survivorship / selection bias like you said; we don't yet know what the Wonders of the World built today will be in 2000-4000 years, because we don't know what will remain or what will be considered significant. I mean there's huge skyscrapers, ostentatious buildings built in the richer cities. There's a giant clock in Mecca, the Venetian and Grand Lisboa in Maccau, the New Century Global Complex in China, etc. But few or none built to just exist, like the pyramids that were sealed off. After solving world hunger etc, if I were stupidly rich, I'd have a monument built. Sealed off containing the world's knowledge in redundant and multiple mediums. And with a visitor center / museum because people will be curious, of course. reply micromodel 15 hours agorootparentprev> There must be a huge amount of civilizations that were writing on paper or papyrus around that era, but they just didn't survive. I don't think this part is true. Papyrus wasn't cheap. reply mncharity 16 hours agoparentprev> some of our artifacts might get accidentally preserved as well IIRC (not likely these decades later), when recovering old MIT AI Lab backups (9-track tape goes slowly by a read head, yielding bits, plastic backing, and a pile of magnetic dust), one lisp machine backup contained a core dump file, which included the screen buffer. A single moment of someone's long-ago day, with assorted windows, including the cause of the dump. And a bit of graphics fun - a critter crawling across the screen - frozen in time. reply runevault 14 hours agoparentprevWhat are the odds any electronic data store, tape or SSD or anything in between, could last that long? I guess some random store keeps getting moved from one storage device to another by accident, but beyond that I'm not sure if it is reasonably possible. reply panstromek 11 hours agorootparentMicrosoft is has been developing one for quite some time. Glass structure that should last thousands of years. reply ggm 19 hours agoparentprev\"Memoirs found in a bathtub\" by Stanislaw Lem. Printouts preserved in mud deep in a fictitious pentagon basement for thousands of years after nuclear holocaust wipes computer memories. reply mcphage 16 hours agorootparentThat book is wonderfully unsettling. The ending was perfect. reply ggm 13 hours agorootparentThe star diaries are my favourite, more whimsical. I'm very fond of his robot stories. I've never re-read the Memoirs, they were .. very unsettling. As was the futurological congress. Solaris is too overlayed by the Tarkovsky film for me now, i used to make shredded paper to hang in my office airvents as a homage. reply psd1 9 hours agorootparentPeace On Earth is my favourite Lem by a long way. The English translation has aged better than Memoirs, too. reply OrvalWintermute 14 hours agoprevNotice how it includes the Igigi (lesser gods of their pantheon) and mention great weapons of An[u], Enlil & Enki, the Ruling gods of their pantheon, associated with city destruction reply 29athrowaway 17 hours agoprev [–] If you like history and you like tables, these are some of the most historically relevant tables: https://en.wikipedia.org/wiki/Alfonsine_tables reply ecocentrik 13 hours agoparent [–] Not quite as ancient but still very cool. \"Nicolaus Copernicus bought a copy while at the University of Cracow, and cared about it enough to have it professionally bound with pieces of wood and leather.[9] Alexander Bogdanov maintained that these tables formed the basis for Copernicus's development of a heliocentric understanding in astronomy.\" reply Cthulhu_ 8 hours agorootparent [–] That's a neat example of how \"boring\" statistical data / record keeping can lead to great scientific results. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ancient Mesopotamian account-keeping utilized clay tablets with rows and columns, akin to modern data tables, as evidenced by a tablet from Larsa, Iraq, translated by Eleanor Robson.- The tablet, cataloged at the British Museum, resembles a payroll summary for a construction project, demonstrating organized data with headers and calculations over 3500 years ago.- This historical insight highlights that while data tables have ancient origins, their continuous use isn't guaranteed, as inventions can be lost and rediscovered over time."
    ],
    "commentSummary": [
      "The discussion emphasizes the timeless relevance of tabular data representation, despite the data table being from 2020, which is considered outdated by current standards.",
      "Participants explore the historical significance and intuitive design of tables, noting their evolution into modern spreadsheets and databases.",
      "The conversation also considers cultural and technical advancements in data preservation, suggesting that modern data might be preserved similarly to ancient data."
    ],
    "points": 227,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1734819937
  },
  {
    "id": 42484139,
    "title": "Slow deployment causes meetings (2015)",
    "originLink": "https://tidyfirst.substack.com/p/slow-deployment-causes-meetings",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"tidyfirst.substack.com\",cType: 'non-interactive',cRay: '8f625d748a5deae0',cH: 'nP0ILLr41p0HBAckahJkAgrGQrnZ9o7lb146Lme62RY-1734894134-1.2.1.1-Xoi94ZFxzlSpo0y22o4LRUejuYScHk7iPtecYKVvYYWYYhvfvFm8AppSgp6UBaLw',cUPMDTk: \"\\/p\\/slow-deployment-causes-meetings?__cf_chl_tk=ozspGqqD80uG4G7j3VIi9nYEwdVu0nK7Yv1P8cgijEo-1734894134-1.0.1.1-.OpWF9oU7U9O7is0rhh3uDXm5Vr.I8LdJJWMIATq36A\",cFPWv: 'b',cITimeS: '1734894134',cTTimeMs: '1000',cMTimeMs: '120000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/slow-deployment-causes-meetings?__cf_chl_f_tk=ozspGqqD80uG4G7j3VIi9nYEwdVu0nK7Yv1P8cgijEo-1734894134-1.0.1.1-.OpWF9oU7U9O7is0rhh3uDXm5Vr.I8LdJJWMIATq36A\",md: \"KrKzJHhTgx7_wMCO7L129QXn8J3UHr4vWtzXdF5xDLg-1734894134-1.2.1.1-FDFITwkSjATp0a58C8rs5_vLsLGaolQSGZEufJ6mU1yZolNMBR.60j2qsqCQgeIL5tEB9mYp8y07RnI3EGDayesc5.CMibwAoRInKT3nJxMb57nrABnnpDBkPWi9vN4fdhNd6bqL.bC9IKxk3vEUjKe42rkgnO5ZlO1LkbU_imCNtfOEUcb71sBQNK6GPlIdoH3ceEoXnr8Z8sADjmd4V.rKGN8l8Yq9U1y.HyK_OCgxEJxM102YNU7HipKBxN91cMRsc30sU1cHNIQlPGmVsiu1sninTtmgxk2xqYzmcc5czE3aLsbtL5H8OogDKfQSBeajtCCCSBvs_GAGqTFehMsAK3Qs5AejVURBgMY9PyvNnwq_N.mtu2E5cQaQbiKah9bC2Z249P2Az5hKtBUpOcmUB7boL3MsH3cXzIFtWgsH9kXtk26N_5H.RJsOEFHHoUyUxXTSEi_GkwfcPLAoejuMn67D_pgxRY7Yc3cRBdZ5Jmfz1n7o9z_eU7QxRGr3_dAhs1AHnsxl73vq_pEaDapVAFqxkr8w0I97_6.IUTgWXp3PTubwRwv8ip7SNfh.5bcnyteaaqb7c1hkWUVLRtSgRnE0qIcv52b4xbVIwCuHIrki3lY1E.mg68IMkA1OYsjUNlHrmbLPrmuFjzuxIBb3.yxqIWtQvnm2yn5NjCs6x9GjEr8mWb_VKsSpAF033NGm1806IfGT3pcowZ7.mGiNKdCCIm7cq953YqmxxV2z38inUiZ.DSYHFTDeJxcP9t1gp1HOZsAcXnxQxydsT8L4VbToGDvJME3_PG_T989PNbfgRjmnimv_erXcjp_RfZSvr7viOfXsQn9FAJrm3D_OdptWJGDx69b1rPzaxPIikBhcQpf8vRV2pas7_F.4BqtfqWMjUPlFglpSnexNhznNghpZRynMC3Iq26x_hADGniR67fX2O6RC05_AEeRKjpufKhlp4vus51vzix47r8DvR98kSv5l3LBjYr8klOXYGLbLmhKt0A9U1RJB7VAOfGs4Uzjy_D5rFaVSJzy04u_zJn0kG.gcmgP1LulUpb2ZeBsBKs.463PYWOaSwtHKOqMELFkxM7jFp8GJMfr8wUQ3Aaru76u24CAtgUlgLEVmpgFb8J44lyPbi_SvoPC.vwHE1LOy0SlDaf4jOeVv30DUSoN.GwHc.JY3lzrHr4ehauuvTeOWF_GPiws7evvRhzPkWMxgWt4wQ2SukLvHmaBYY4ZNIGirG7xBb8.wG8VB1Fx3.h.UnlF5SUXd8uCFBgQYLTwAkiORauuuJBwJtFW0KNqK0c7V6TtEIyq87P4mpcypTj1jYzI6Og5j0o7wXQ1GXRLk8Fe25HNB6ZJrNgVlwZ1qQ8n7EhfecKeF.XaEaW2GB.O0w5b1kfc5MKQ6dEYyxw1ipdT5HKx3bQxWUlrLXBUFG_KOtaycnZs8QE8Y51KlW8kleEX5IlJqXusHLV3PO28gVk98C7bqo.vFl4Ti3W_rsZMRfE2DP.RpLKyDFr5qgdyDtylwoO9GezfU3zm4.YF9Iu3CtJTOfK2qYAbA9.lNAkmwiGFrBLpvtau6n_cvQdrACZQcreQgRif9mYC7kYS2yY_3LtDggRJg2W5SGyEDS4ikyq5evgpPxTTAzbGOVPCGe5RXy2xH.S9Ak3Et9N431JXzaAR4Up1OsBM4Mp10QhTOiIYQBBY7hC25cEz3PB8.988IpanLhl_gDndzqistxkmpHrRMavEpypXqnAi.MWkbicn0pUGo1inrzqnHRAZnxzpfBEODnbla2AFkcY3W8bDfwewJLEXxMKdBqQ8vtH84R93UgQTHQlvV3HwyzUawJWvNmpr3ePHu5YA2gzEaiJG4v9.XZriPJo5ZMbqsufhXwyj9hojoiMA\",mdrd: \"jqKUXnl9FuYg0o2bYTA_krY5Ghs8UY4Tg0_lnqZArTI-1734894134-1.2.1.1-.hSu4Spec4HhvtRdEmxcNTEz66STuSFpX5SXXbmkQrQ1AQhcRChq7zCJTDPTvvmtFigNjf5J6BeoN3h9DuhZnlYowrzO11kb6Cl4pIv_KXFs2aZga0njEZ5HpfiJ2bWHq7l2p0DDYLiTMMRXqd7XlyBc9o3j7IXvSB.w9AIphF8R_vEScSPqS_J_5EnlLBRt1ICza3dkaO5vJU_zidFctlhuXMhIXDI9DD3BZPwa49m32Fkg_PVUiwJpy4M2x8roQeEeS94PGXmbvsWYn8C22Z6RP47PdSWCIOD2JUifFAc0hKrPLjdpnGjOvOYp5F8p.Hw9zgrXempwg3SmKhdZ..wiaX68h6QpDqBi49UtAderr2HaauZStTHMogQ8POGl28BYRJmaBUpxym4462dcs2dJxWdDa36uspRTh76l76yTWRp6dy9dpf1DZZpPTxR9c06p1EAWACMp_cVGHVSe8TT7Yh3Nk_aXXTLJLkVh3mTubWVxkCM085V1lYsQM4.5wSYcvvE.PWMFVaIM_A4qr9HGQ7l0VkF5w_Eghx9km2497_lKP0tB0K4SmLjDuuWQlvjgDzorNvdeOGBjUVWo6M.XcmeaNzAxa8YSFSGYdE8ZkciCTN2DhL_r8r9WgSsC4iVpDd8c5kVR8rXTGe4pMNhS00Qs1qWTRqMrgI6QV2gkbRY_5dnw92PTr3h6FMsN38wGUUvnvS_rhfMJCiOFxOdo7kOatsfvEFOruMU1aLDzMZBkXiytwg49NuvfeVGmR1q2LQYxVhUgnTe7V6rg1YOPPCaKQto31gUMkCa5Vwto1xqP5fpyf5WCl5.YrZ8GwCTf_K.fjnJacM0YR.3BcqnTZYUUMz_ym16CIVac9rxZ0eS4EMX5Ih8ldog5akixOM3GLWBG.Eyf2Ksqpzo0MfilQJ5P7HS.SA7ViDqw7_xqsiX1H._YXu0CyClvzlSaWUlgnM494Ngi3d7BBmfClr9vo8r2b.YsGseEuiWHzq6J2m9Xj2F.yVZY8GK76FZXEd5Hckh4JtyH.ZcGmo6exghlhIC9EqRaY19UZtkeC35ePNFLg3mZYLdhmN8CUqK66w7w7I4MlxlM49timYPBF8Y_4NIJWVXx0s657uWTaJyTCOLwr.BB3JHVM9KMLhDgeP93vYkg.bbcdHtMDpYbtqu0KQWvtO6XAT_7x7Xs2Ic5ftrLx0CDHxwBmtCsOxlVDq3dUDKWssUBuY_y88gdbtMOOuEkYzzwm1N8cW2gpKKxIX8GUtXFLLn9UhRPBOioArTTHMB7Tq2OZeJKNvzQVdetmdwiulmpxm8wzpgy1BDB1xSYcufyEB.6Yv02tTx3iLcnWaTOGqCtnsGRVGUi29WoixYqcgNpZPU1WnR5UIu2VI8wk8FahofwlQJUDOq9G3We8ylxMFM3NWEZ2yk0ZGMXKHblPW38ndnMHPh3Vn2BsCACTCOcA0biS2nfk8U6n2ow7vtYr78kL3qfCCAV8j2kNfei_8jVs0jm_ngpR9OtCogbkl38JFqXJboiIGpw5blhGfMf4YYUlKUjtUvrGWh8P4mX52iT2JMjiYTm4pU96VIad26agyzmr1jTZAwnqCpIYh7mNG4mc7NsR8BKSky2yzHXEi3cUv7VZ0G.3PDFh1Y2KXGJHrr.wWQIoJ1PJLILskfipnQZYWCOhglaW2fRb6ryyvEeHr8b1CjVrCDXpSNLT2uVfH.sUvVZyGqpIkivNTiuhLvSxDKvvdQcwBdF2fOggmGNLipqJBwkUi07Kj9ZGdKAVSZap72KTcRgGe5.TFYE0RErM.yUsAy7pn9F2dBO2Q8PaBtToa0trnH2f734arXYUVNmcGovo_f.UqaWJIbR1nHzBXUVdC2sDNOpzrumMIyR8N4dlzWflBid12n8a73K59S6m6yR0WsfIJdOTlau1NNSoQ12PKTHRgoPiolH8ES1sXY0cYTXW5BRBIj8JnxNhbdWRS7nNtknMwBDQHy7QawpMRaY3KWwRTschFd2IDdK4R95NGV73ro50PwGh8m7XTb6A9mDAhNVRB2Y6FVv8lAT96jZgL1eMv.00k.s2LR6SSDlGPZy09Kp_Ouvn7pCSVMcsD00.2cf18V9F2S8rDqbHTJCXchcwEjVlyejNTIhhu0Mo.sLAf.keSjdA9SBl8l3kX_NmM_x4b17GydzoOqQPpcoGXKH6g8J8XwA7F19N8J8kisrMVUqx3ecIdf6u_KcwsR04NrW8oTz0VIJhNQ9koODrOmiPBDXfuZwLCuKGgjzXQoWKNxQTBrZwVxWARPQutvdZkYDD6H2owZL702AU_5XCHpzgm3Avqq9G8cj92xDdsuc79cibgBwzftfUSSdnHe.Ejky3JDfD5vwrKAwHY6xOS6gS9HIH.GqOEqYmBQ2rtEw3qlAX4dp1pQTwWok1_Du6X8X\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8f625d748a5deae0';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/slow-deployment-causes-meetings?__cf_chl_rt_tk=ozspGqqD80uG4G7j3VIi9nYEwdVu0nK7Yv1P8cgijEo-1734894134-1.0.1.1-.OpWF9oU7U9O7is0rhh3uDXm5Vr.I8LdJJWMIATq36A\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42484139",
    "commentBody": "Slow deployment causes meetings (2015) (tidyfirst.substack.com)164 points by fagnerbrack 15 hours agohidepastfavorite60 comments sourceless 10 hours agoI think unfortunately the conclusion here is a bit backwards; de-risking deployments by improving testing and organisational properties is important, but is not the only approach that works. The author notes that there appears to be a fixed number of changes per deployment and that it is hard to increase - I think the 'Reversie Thinkie' here (as the author puts it) is actually to decrease the number of changes per deployment. The reason those meetings exist is because of risk! The more changes in a deployment, the higher the risk that one of them is going to introduce a bug or operational issue. By deploying small changes often, you get deliver value much sooner and fail smaller. Combine this with techniques such as canarying and gradual rollout, and you enter a world where deployments are no longer flipping a switch and either breaking or not breaking - you get to turn outages into degradations. This approach is corroborated by the DORA research[0], and covered well in Accelerate[1]. It also features centrally in The Phoenix Project[2] and its spiritual ancestor, The Goal[3]. [0] https://dora.dev/ [1] https://www.amazon.co.uk/Accelerate-Software-Performing-Tech... [2] https://www.amazon.co.uk/Phoenix-Project-Helping-Business-An... [3] https://www.amazon.co.uk/Goal-Process-Ongoing-Improvement/dp... reply motorest 9 hours agoparent> The reason those meetings exist is because of risk! The more changes in a deployment, the higher the risk that one of them is going to introduce a bug or operational issue. Having worked on projects that were perfectly full CD and also projects that had biweekly releases with meetings with release engineers, I can state with full confidence that risk management is correlated but an indirect and secondary factor. The main factor is quite clearly how much time and resources an organization invests in automated testing. If an organization has the misfortune of having test engineers who lack the technical background to do automation, they risk never breaking free of these meetings. The reason why organizations need release meetings is that they lack the infrastructure to test deployments before and after rollouts, and they lack the infrastructure to roll back changes that fail once deployed. So they make up this lack of investment by adding all these ad-hoc manual checks to compensate for lack of automated checks. If QA teams lack any technical skills, they will push for manual processes as self-preservation. To make matters worse, there is also the propensity to pretend that having to go through these meetings is a sign of excellence and best practices, because if you're paid to mitigate a problem obviously you have absolutely no incentive to fix it. If a bug leaks into production, that's a problem introduced by the developer that wasn't caught by QAs because reasons. If the organization has automated tests, it's even hard to not catch it at the PR level. Meetings exist not because of risk, but because organizations employ a subset of roles that require risk to justify their existence and lack skills to mitigate it. If a team organizes it's efforts to add the bare minimum checks to verify a change runs and works once deployed, and can automatically roll back if it doesn't, you do not need meetings anymore. reply vegetablepotpie 3 hours agorootparentThis is very well said and succinctly summarizes my frustrations with QA. My experience has been that non-technical staff in technical organizations create meetings to justify their existence. I’m curious if you have advice on how to shift non-technical QA towards adopting automated testing and fewer meetings. reply blackjack_ 2 hours agorootparentHi, senior SRE here who was a QA, then QA lead, then lead automation / devops engineer. QA engineers with little coding experience should be given simple automation tasks with similar tests and documentation/ people to ask questions to. I.e. setup a pytest framework that has a few automated test examples, and then have them write similar tests. The automated tests are just TAC (tests as code) versions of the manual test cases they should already write, so they should have some idea of what they need to do, and then google / ChatGPT/ automation engineers should be able to help them start to translate that to code. People with growth mindsets and ambitions will grow from the support and being given the chance to do the things, while some small number will balk and not want anything to do with it. You can lead a horse to water and all that. reply sourceless 3 hours agorootparentprevI think we may be violently agreeing - I certainly agree with everything you have said here. reply tomxor 9 hours agoparentprevI tend to agree. Whenever I've removed artificial technical friction, or made a fundamental change to an approach, the processes that grew around them tend to evaporate, and not be replaced. I think many of these processes are a rational albeit non-technical response to making the best of a bad situation in the absence of a more fundamental solution. But that doesn't mean they are entirely harmless. I've come across some scenarios where the people driving decisions continued to reach for human processes as the solution rather than a workaround, for both new projects and projects designated specifically to remove existing inefficiencies. They either lacked the technical imagination, or were too stuck in the existing framing of the problem, and this is where people who do have that imagination need to speak up and point out that human processes need to be minimised with technical changes where possible. Not all human processes can be obviated through technical changes, but we don't want to spread ourselves thin on unnecessary ones. reply lifeisstillgood 1 hour agoparentprevSo this seems quantifiable as well - there must be a number of processes / components that a business is made up of, and those presumably are also weighted (payment processing has weight 100, HR holiday requests weight 5 etc). I would conjecture that changing more than 2% of processes in any given period is “too much” - but one can certainly adjust that. And I suspect that this modifies based on area (ie the payment processing code has a different team than the HR code) - so it would be sensible to rotate releases (or possibly teams) - this period this team is working on the hard stuff, but once that goes live the team is rotated back out to tackle easier stuff - either payment processing or HR The same principle applies to attacking a trench, moving battalions forward and combined arms operations. Now that is of course a “management” problem - but one can easily see how to automate a lot of it - and how other “sensory” inputs are useful (ie which teams have committed code to these sensitive modules recently One last point is it makes nonsense of “sprints” in Agile/Scrum - we know you cannot sprint a whole marathon, so how do you prepare the sprints for rotation? reply vasco 6 hours agoparentprevI agree entirely - I use the same references, I just think it's bordering on sacrilege what you did to Mr. Goldratt. He has been writing about flow and translating the Toyota Production System principles and applying physics to business processes way before someone decided to write The Phoenix Project. I loved the Phoenix Project don't get me wrong, but compared to The Goal it's a like a cheaply produced adaptation of a \"real\" book so that people in the IT industry don't get scared when they read about production lines and run away saying \"but I'm a PrOgrAmmEr, and creATIVE woRK can't be OPtiMizEd like a FactOry\". So The Phoenix Project if anything is the spiritual successor to The Goal, not the other way around. reply grncdr 6 hours agorootparentThat’s exactly what the GP wrote: The Goal is the spiritual ancestor of The Phoenix Project. reply vasco 5 hours agorootparentWell now I can't tell if it was edited or if I just misread and decided to correct my own mistake. I'll leave it be so I remember next time, thanks. reply sourceless 3 hours agorootparentThat's indeed how I wrote it, but I could have worded it better. Very much agree that the insights in The Goal go far beyond the scope of The Phoenix Project. reply mrbluecoat 4 hours agorootparentprevI totally read it as successor as well. Interesting how the brain fills in what we expect to see :) reply ozim 9 hours agoparentprevI am really interested in organizations capacity of soaking the changes. I live in B2B SaaS space and as much as development goes we could release daily. But on the receiving side we get pushback. Of course there can be feature flags but then it would cause “not enabled feature backlog”. In the end features are mostly consumed by people and people need training on the changes. reply ajmurmann 5 hours agorootparentI think that really depends on the product. I worked on a on-prem data product for years and it was crucial to document all changes well and give customers time to prepare. OTOH I also worked on a home inspection app and there users gave us pushback on training because the app was seen as intuitive reply paulryanrogers 4 hours agorootparent> ...there users gave us pushback on training because the app was seen as intuitive I would weep with joy to receive such feedback! Too often the services I work on have long histories with accidental UIs, built to address immediate needs over and over. reply ricardobeat 7 hours agoparentprev> By deploying small changes often, you get deliver value much sooner and fail smaller. Which increases the number of changes per deployment, feeding the overhead cycle. He is describing an emergent pattern here, not something that requires intentional culture change (like writing smaller changes). You’re not disagreeing but paraphrasing the article’s conclusion: > or the harder way, by increasing the number of changes per deployment (better tests, better monitoring, better isolation between elements, better social relationships on the team) reply sourceless 3 hours agorootparentI am disagreeing with the conclusion of the article, and asserting that more and smaller deployments are the better way to go. reply ricardobeat 22 minutes agorootparentYou are not. The conclusion of the article is the same, you \"need to expand the far end of the hose\" by increasing deployment rate or making more, smaller changes. What was your interpretation? reply manvillej 38 minutes agoparentprevthis isn't even a software things. Its any production process. The greater amount of work in progress items, the longer the work in progress items, the greater risk, the greater amount of work. Shrink the batch, shorten the release window window. It infuriates me that software engineering has had to rediscover these facts when the Toyota production system was developed between 1948-1975 and knew all these things 50 years ago. reply lifeisstillgood 3 hours agoprevI am trying to expound a concept I call “software literacy” - where a business can be run via code just as much as today a company can be run by English words (policy documents, emails etc). This leads to a few corollaries - things like “If GPUs do the work then coders are the new managers” or we need whole-org-test-rigs to be clear about the impacts of chnages. This seems directly related to this excellent article - to my mind if all the decision makers are not looking at the code as the first class object in a chnage process (is opposed to Jiras or project plans) then not all decision makers are (software) literate - and this comes up a lot in the threads here (“how do I discuss with non-technical management”) - the answer is you cannot - that management must be changed. This is an enormous generational road block that I thought was a problem thirty years ago but naively assumed would disappear as coders grew up. Of course the problem is that to “run” a company one does not need to code - so until not coding is something embarrassing like not writing is for a newspaper editor we won’t get past it. The main point is that we need companies that can be run with the new set of self-reinforcing concepts - sops, testing, not meetings but systems as communication. I will try and rewrite this comment later - it needs work reply andy_ppp 7 hours agoprevThe organisation will actively prevent you from trying to improve deployments though, they will say things like “Jenkins shouldn’t be near production” or “we can’t possibly put things live without QA being involved” or “we need this time to make sure the quality of the software is high enough”. All with a straight face while having millions of production bugs and a product that barely meets any user requirements (if there are any). In the end fighting the bureaucracy is actually impossible in most organisations, especially if you’re not part of the 200 layers of management that create these meetings. I would sack everyone but programmers and maybe two designers and let everyone fight it out without any agile coaches and product owners and scrum master and product experts. Slow deployment is a problem but it’s not the problem. reply gleenn 1 hour agoparentYou sound very defeatist about fighting bureaucracy. If you work at an org with too much management, you can slowly push to move it in the direction you hope for or leave. If you keep ending up at places that seem impossible to change, perhaps you should ask more questions about this during the interview. I've worked at many small companies where there wasn't crazy bureaucracy because that's definitely what I preferred. I also currently work at a megacorp and yes there is difficulty, but being consistent and persuasive has lead to many things slowly heading in the right direction. Things take time. You have to realize why people have made things some way and then find convincing arguments to make things better. Sometimes places do just suck so don't stick around. But being hopeless doesn't seem helpful. reply gavmor 15 minutes agoparentprev> Jenkins shouldn’t be near production > we can’t possibly put things live without QA being involved > we need this time to make sure the quality of the software is high enough I've only developed software professionally since 2012, but in that time not only have I never encountered such sentiments, but (and, perhaps, because) it has always been a top priority of leadership to emphatically insist on the very opposite: day one of any initiative is Jenkins to production—often directly via trunk-based development—and quality is every developer's responsibility. At the IC level, there was no \"fighting bureaucracy,\" although I don't doubt leadership debated these things vigorously, from time to time, especially as external partners and stakeholders were often intimately involved. > I would sack everyone but programmers and maybe two designers and let everyone fight it out That works for me! But it doesn't scale. We definitely have to keep at least one product \"owner\" or \"expert\" or \"manager\" to enqueue stakeholder priorities and, while this can be a \"hat\" that devs and designers trade off, it's also a skill at which some individuals uniquely excel. All that being said, I don't want to come across as pearl-clutching, shocked Pikachu face about this. I understand that many organizations don't operate this way. The way I've helped firms make this change is via the introduction of a single, experimental team of volunteers dedicated to these practices—one protected (but not dictated to) by a mandate from on high. But, then again, this is California. reply braza 1 hour agoprevA marginally related point but I do not know if others faced the following situation: I worked in a place with a CI pipeline room ~25 minutes with the unit/integration tests (3000+) taking 18 minutes. When something happens in production we ended up placing more tests; and of course when things goes south at least 50 minutes were necessary to recover. After a lot of consideration we decided to focus on the recovery and relax and simply some tests and focus on recovery (i.e. have the full thing in less than 5 minutes) combined with a canary as deployment strategy (instead rolling updates). At least for us was a so refreshing experience but sounded wrong in some ways. reply vegetablepotpie 5 hours agoprevI have personal experience with this in my professional career. Before Christmas break I had a big change, and there was fear. My org responded by increasing testing (regression testing, which increased overhead). This increased the risk that changes on dev would break changes on my branch (not a code merging way, but in a complex adaptive system way). I responded to this risk by making a meeting. I presented our project schedule, and told my colleagues about their expectations, I.e. if they drop code style comments on the PRs they will be deferred to a future PR (and then ignored and never done). What we needed is fine grained testing with better isolation between components. The problem is is that our management is at a high level, they don’t see meetings as a means to an end, they see meetings as a worthy goal in and of itself self to achieve. More meetings means more collaboration, means good. I’d love to see advice on how to lead technical changes with non-technical management. reply qaq 13 hours agoprevA bit tangential but why is CloudFormation so slowww? reply Aeolun 11 hours agoparentThe reason by boss tends to give is that it’s made by AWS, so it cannot possibly be bad. Also, it’s free. Which is never given as anything more than a tangentially related reason, but… reply justin_oaks 12 hours agoparentprevI figure it's because AWS can get away with it. reply shepherdjerred 2 hours agorootparentAWS deploys using cfn internally reply hk1337 10 hours agoparentprevThis is just anecdotal but I have found anytime a network interface is involved, it can slow down the deployment. I had a case where I was deleting lambdas in a VPC, and connected to EFS, that the deployment was rather quick but it took ~20 minutes for cloudformation to cleanup and finish. reply motorest 7 hours agoparentprev> A bit tangential but why is CloudFormation so slowww? It's not that CloudFormation is slow. It's that the whole concept of infrastructure-as code-as-codd is slow by nature. Each time you deploy a change to a state as a transaction, you need to assert preconditions and post-conditions at each step. If you have to roll out a set of changes that have any semblance of interdependence, you have no option other than to deploy each change as sequential steps. Each step requires many network calls to apply changes, go through auth, poll state, each one taking somewhere between 50-200ms. That quickly adds up. If you deploy the same app on a different cloud provider with Terraform or Ansible, you get the same result. If you deploy the same changes manually you turn a few minutes into a day-long ordeal. The biggest problem with IaC is that it is so high-level and does so much under the hood that some people have no idea what changes they are actually applying or what they are doing. Then they complain it takes so long. reply qaq 6 hours agorootparentThing is Terraform is faster reply austin-cheney 7 hours agoprevWhile this is mostly correct it’s also just as irrelevant. TLDR; software performance, thus human performance, is all that matters. Risk management/acceptance can be measured with numbers. In software this is actually far more straightforward than in many other careers, because software engineers can only accept risk within the restrictions of their known operating constraints and everything else is deferred. If you want to go faster you need to maximize the frequency of human iteration above absolutely everything else. If a person cannot iterate, such as waiting on permissions, they are blocked. If they are waiting on a build or screen refresh they are slowed. This can also be measured with numbers. If person A can iterate 100x faster than person B correctness becomes irrelevant. Person B must maximize upon correctness because they are slow. To be faster and more correct person A has extreme flexibility to learn, fail, and improve beyond what person B can deliver. Part of iterating faster AND reducing risk is fast test automation. If person A can execute 90+% test coverage in time of 4 human iterations then that test automation is still 25x faster than one person B iteration with a 90+% lower risk of regression. reply dang 14 hours agoprevRelated: Slow Deployment Causes Meetings - https://news.ycombinator.com/item?id=10622834 - Nov 2015 (26 comments) reply lizzas 13 hours agoprevMicroservices lets you horizontally scale deployment frequency too. reply fulafel 11 hours agoparentI think this was the meme before moduliths[1][2] where people conflated the operational and code change aspects of microservices. But it's just additional incidental complexity that you should resist. IOW you can do as many deploys without microservices if you organize your monolithic app as independent modules, while keeping out the main disadvantages of the microservice (infra/cicd/etc complexity, and turning your app's function calls into a unreliable distributed system communication problem). [1] https://www.fearofoblivion.com/build-a-modular-monolith-firs... [2] https://ardalis.com/introducing-modular-monoliths-goldilocks... reply trog 11 hours agorootparentAn old monolithic PHP application I worked on for over a decade wasn't set up with independent modules and the average deploy probably took a couple seconds, because it was an svn up which only updated changed files. I frequently think about this when I watch my current workplace's node application go through a huge build process, spitting out a 70mb artifact which is then copied multiple times around the entire universe as a whole chonk before finally ending up where it needs to be several tens of minutes later. reply withinboredom 10 hours agorootparentEven watching how php applications get deployed these days, where it goes through this huge thing and takes about the same amount of time to replace all the docker containers. reply fulafel 11 hours agorootparentprevYeah, if something even simpler works, that's of course even better. I'd argue the difference between that PHP app and the Node app wasn't the lack of modularity, you could have a modulith with the same fast deploy. (But of course modulith is too just extra complexity if you don't need it) reply motorest 9 hours agorootparentprev> I think this was the meme before moduliths[1][2] where people conflated the operational and code change aspects of microservices. People conflate the operational and code change aspects of microservices just like people conflate that the sky is blue and water is wet. It's a statement of fact that doesn't go away with buzzwords. > IOW you can do as many deploys without microservices if you organize your monolithic app as independent modules, while keeping out the main disadvantages of the microservice (infra/cicd/etc complexity, and turning your app's function calls into a unreliable distributed system communication problem). This personal opinion is deep within \"not even false\" territory. You can also deploy as many times as you'd like with any monolith, regardless of what buzzwords you tack on that. What you're completely missing from your remark is the loosely coupled nature of running things on a separate service, how trivial it is to do blue-green deployments, and how you can do gradual rollouts that you absolutely cannot do with a patch to a monolith, no matter what buzzwords you tack on it. That is the whole point of mentioning microservices: you can do all that without a single meeting. reply fulafel 6 hours agorootparentI seem to have struck a nerve! While there may be some things that can come for free with microservices (and not moduliths), your mentioned ones don't sound convincing. Blue-green deployments and gradual rollouts can be done with modulith and can't think of any reason that would be harder than with microservices (part of your running instances can run with a different version of module X). The coupling can be just as loose as with microservices. reply jmulho 2 hours agorootparentprevBlue-green deployments is a buzzword no matter what color you tack on it. reply theptip 13 hours agoparentprevNot a silver bullet; you increase api versioning overhead between services for example. reply motorest 11 hours agorootparent> Not a silver bullet; you increase api versioning overhead between services for example. That's actually a good thing. That ensures clients remain backwards compatible in case of a rollback. The only people who don't notice the need for API versionin are those who are oblivious to the outages they create. reply whateveracct 12 hours agorootparentprevTrue but your API won't be changing that rapidly especially in a backwards-incompatible way. reply dhfuuvyvtt 12 hours agorootparentWhat's that got to do with microservices? Edit, because you can avoid those things in a monolith. reply faizshah 11 hours agoparentprevIt’s a monkey’s paw solution, now you have 15 kinda slow pipelines instead of 3 slow deployment pipelines. And you get to have the fun new problem of deployment planning and synchronizing feature deployments. reply motorest 8 hours agorootparent> It’s a monkey’s paw solution, now you have 15 kinda slow pipelines instead of 3 slow deployment pipelines. Not a problem. In fact, they are a solution to a problem. > And you get to have the fun new problem of deployment planning and synchronizing feature deployments. Not a problem too. You don't need to synchronize anything if you're consuming changes that are already deployed and running. You also do not need to synchronize feature deployment if you know the very basics of your job. Worst case scenario, you have to move features behind a feature flag, which requires zero synchronization. This sort of discussion feels like people complaining about perceived problems they never bothers to think about, let alone tackle. reply devjab 10 hours agoparentprevYou can do this with a monolith architecture as others point out. It always comes down to governance. With monoliths you risk slowing yourself down in a huge mess of SOLID, DRY and other “clean code” nonsense which means nobody can change anything without it breaking something. Not because any of the OOP principles are wrong on face value, but because they are so extremely vague that nobody ever gets them right. It’s always hilarious to watch Uncle Bob dismiss any criticism with a “they misunderstood the principles” because he’s always completely right. Maybe the principles are just bad when so many people get them wrong? Anyway, microservices don’t protect you from poor governance it just shows up as different problems. I would argue that it’s both extremely easy and common to build a bunch of micro services where nobody knows what effect a change has on others. It comes down to team management, and this is where our industry sucks the most in my experience. It’ll be better once the newer generations of “Team Topologies” enter, but it’ll be a struggle for decades to come if it’ll ever really end. Often it’s completely out of the hands of whatever digitalisation department you have because the organisation views any “IT” as a cost center and never requests things in a way that can be incorporated in any sort of SWE best practice process. One of the reasons I like Go as a general purpose language is that it often leads to code bases which are easy to change by its simplicity by design. I’ve seen an online bank and a couple of landlord systems (sorry I can’t find the English word for asset and tenant management in a single platform) explode in growth. Largely because switching to Go has made it possible for them to actually deliver what the business needs. Mean while their competition remains stuck with unruly Java or C# code bases where they may be capable of rolling out buggy additions every half year if their organisation is lucky. Which has nothing to do with Go, Java or C# by the way, it has to do with old fashioned OOP architecture and design being way too easy to fuck up. In one shop I worked they had over a thousand C# interfaces which were never consumed by more than one class… Every single one of their tens of thousands of interfaces was in the same folder and namespace… good luck finding the one you need. You could do that with Go, or any language, but chances are you won’t do it if you’re not rolling with one of those older OOP clean code languages. Not doing it with especially C# is harder because abstraction by default is such an ingrained part of the culture around it. Personally I have a secret affection for Python shops because they are always fast to deliver and terrible in the code. Love it! reply punnerud 11 hours agoparentprevAs long as every team managing the different APIs/services don’t have to be consulted for others to get access. You then get both the problems of distributed data and even more levels of complexity (more meetings than with a monolith) reply motorest 9 hours agorootparent> As long as every team managing the different APIs/services don’t have to be consulted for others to get access. Worst-case scenario, those meetings take place only when a new consumer starts consuming a producer managed by an external team well outside your org. Once that rolls out, you don't need any meeting anymore beyond hypothetical SEVs. reply jojobas 10 hours agoprevFast deployment causes incident war rooms. reply boxed 1 hour agoparentI was on a team that went from every 3 weeks to multiple times per day. The number of incidents in production dropped drastically. But much more important than that drop, was that when things went wrong is was MUCH MUCH faster to find the problem. It was also much safer and easier to roll back, since there were so few changes that would be rolled back. No one wants to back off 3 weeks of work. That's chaos. reply Trasmatta 4 hours agoparentprevIn my experience, there's very little correlation. I've been on projects with 1 deployment every six weeks, and there were just as many production incidents as projects with daily deployments. reply DougBTX 9 hours agoparentprevMaybe the opposite, slow rollbacks cause escalating incidents. reply yarg 14 hours agoprevI had a boss who actually acknowledged that he was deliberately holding up my development process - this was a man who refused to allow me a four day working week. reply Sparkyte 13 hours agoprev [–] Sounds like a process problem. 2024 development cycles should be able to handle multiple lanes of development and deployments. Also why things moved to microservices so you can deploy with minimal impact as long as you don't tightly couple your dependencies. reply m00x 13 hours agoparent [–] You don't need microservices to do this. It's actually easier deploying a monolith with internal dependencies than deploying microservices that depend on each other. reply adrianpike 12 hours agorootparent [–] This is very accurate - microservices can be great as a forcing function to revisit your architectural boundaries, but if all you do is add a network hop and multiple components to update when you tweak a data model, all you'll get is headcount sprawl and deadlock to the moon. I'm a huge fan of migrating to microservices as a secondary outcome of revisiting your component boundaries, but just moving to separate repos & artifacts so we can all deploy independently is a recipe for pain. reply jrs235 6 hours agorootparent [–] and a recipe for \"career\" driven managers and directors to grow department head count, budget oversight, and self importance. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Slow deployments increase risk and lead to more meetings, while frequent, smaller deployments reduce risk and enhance value delivery.",
      "Automated testing and infrastructure improvements can decrease the need for meetings and improve deployment efficiency, but organizations often face challenges like bureaucracy and lack of technical skills.",
      "Encouraging non-technical staff to embrace automated testing and reducing manual processes, along with adopting microservices, can improve deployment frequency, though it requires organizational change and better governance."
    ],
    "points": 164,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1734837143
  },
  {
    "id": 42484527,
    "title": "The essays of Michel de Montaigne online",
    "originLink": "https://hyperessays.net/",
    "originBody": "HyperEssays is a project to create a modern and accessible online edition of the Essays of Michel de Montaigne. HyperEssays.net hosts four editions of the Essays: A 1598 edition, in middle French, edited by Marie de Gournay. This is a slightly revised version of Gournay’s original edition published in 1595. A complete and searchable edition of John Florio’s 1603 translation of the Essays, in early modern English. A 1685 translation by Charles Cotton, also in early modern English. Only some chapters of this edition have been copyedited and posted. A complete and searchable modern edition of the Essays based on W. Carew Hazlitt’s 1877 update of Charles Cotton’s translation. I am slowly replacing the Cotton/Hazlitt translation with a contemporary one and adding new notes. My goals with HyperEssays are to provide context and tools for first-time readers of the Essays and to design a lasting resource for all interested in Montaigne’s work. To that end, I copyedit, update, and annotate the original text and its translations. I tag them for indexing and searching, and format them them for easy reading on smartphones, desktop computers, and tablets. In addition, I prepare and provide free chapter PDFs for offline reading. You can help make HyperEssays a reliable online resource by supporting this project. With your contribution, this site can continue to grow and remain free and accessible to all. What are the Essays about? The Essays is not a single, cohesive book but a collection of short and long pieces on various subjects such as religion, horses, friendship, sleep, law, or suicide, which Montaigne wrote over more than twenty years. His goals for the book and the circumstances under which he worked on it changed over time. The first edition, published in 1580, comprised two books. Eight years later, an updated edition included hundreds of revisions and a new, third book. By the time of his death, in 1592, Montaigne had planned many more changes, which were in­cor­po­rat­ed in the first posthumous edition of 1595. So, while you can read the Essays from beginning to end, starting with Montaigne’s address To the Reader, you can also follow John Cage’s advice and “begin any­where.” Pick from a selection of some of the most well-known chapters: To Philosophize Is to Learn to Die, On the Education of Children, On Friendship, On Cannibals, On Books, Apology for Raymond Sebond, On Some Verses of Virgil, On Coaches, On Experience. Or look at the table of contents and let your curiosity guide you. Who was Michel de Montaigne? Michel de Montaigne, the author of the Essays, is often described as a sixteenth-century French philosopher. But was Montaigne actually a philosopher? And did he really retire from the world to write in solitude for years, as is commonly believed? In On Montaigne, I address these questions and provide biographical context to better understand the Essays. The companion timeline provides a chronological overview of his life. If you want to learn more about him, I recommend these four biographies of Montaigne (along with two modern translations of the Essays). Each one is engaging but written with a different audience in mind. Recent updates Copyediting, translating, writing notes, updating metadata … the work never ends. This is HyperEssays’s work log, a list of the chapters I’ve been working on: Dec 22, 2024 · De la gloire Dec 22, 2024 · De la presomption Dec 21, 2024 · On Repentance Dec 21, 2024 · On Pedantry Dec 20, 2024 · Let Others Judge of Our Happiness after Our Death Dec 18, 2024 · Que nostre desir s’accroist par la malaisance Dec 15, 2024 · Comme nostre esprit s’empesche soy-mesmes Dec 15, 2024 · On Repentance Dec 15, 2024 · De juger de la mort d’autruy Dec 14, 2024 · Apologie de Raimond de Sebonde Dec 10, 2024 · A Consideration on Cicero Dec 9, 2024 · Apology for Raymond Sebond Work on HyperEssays started on January 17, 2020 and likely won’t be completed for many years.",
    "commentLink": "https://news.ycombinator.com/item?id=42484527",
    "commentBody": "The essays of Michel de Montaigne online (hyperessays.net)148 points by octed 13 hours agohidepastfavorite38 comments tfolbrecht 12 hours agoThank you for your efforts. One of my favorite quotes of all time: \"’Tis an absolute and, as it were, a divine perfection, for a man to know how loyally to enjoy his being. We seek other conditions, by reason we do not understand the use of our own; and go out of ourselves, because we know not how there to reside. ’Tis to much purpose to go upon stilts, for, when upon stilts, we must yet walk with our legs; and, when seated upon the most elevated throne in the world, we are but seated upon our breech.\" — Michel de Montaigne, Essays, \"Of Experience\" I like the contemporary translations floating around the web \"even on the highest throne in the world, we still sit on our ass\" reply octed 2 hours agoparentJust to clarify this isn't my own work, I just found it online by accident. If you wish to thank/support this project and it's creator you should check out the support page: https://hyperessays.net/support/ reply wazoox 9 hours agoparentprevNotice that the original does not mince words : \"Et au plus eslevé throne du monde, si ne sommes assis que sus notre cul\". reply phtrivier 9 hours agoprevGreat effort ! The most striking things to me when I started reading the Essays, is how much it reads like... A blog. Variety of topics, general consistency of theme, a tone that is surprisingly conversationnal... And the bombardment of references, in jokes, quotes, etc... that you use to create connivence with your reader. Also, in the end, if you were asked what Montaigne was famous for, what he actually did, _beyond writing his blog_, you would be... hard pressed to answer. Still, I would probably lurk his substack, and watch his stand up on Instagram. reply kergonath 3 hours agoparent> The most striking things to me when I started reading the Essays, is how much it reads like... A blog. This is somewhat deceptive. The Essais were very personal, but not spontaneous at all. He spent a lot of time polishing them and rewriting them right until he died. Just like Rabelais, the apparent casual tone of the language is actually quite a lot of work. In comparison, blog posts are quick to post and then just left as they are. They are closer to letters in that respect. reply jchanimal 4 hours agoparentprevI had the pleasure of reading Montaigne before blogs were invented. When I started reading blogs, the format reminded me of his essays. reply melvinmelih 11 hours agoprevOne of the most life altering essays I’ve ever read is Montaigne’s To Philosophize Is To Die (https://hyperessays.net/essays/to-philosophize-is-to-learn-t...) where he lays out the principle of memento mori (“remember to die”). Fear of death is often very debilitating, and a topic we all like to avoid but we all have to deal with it, sooner or later. The sooner you accept it, the freer (and happier) you’ll feel. reply JodieBenitez 3 hours agoprevhttp://xtf.bvh.univ-tours.fr/xtf/view?docId=tei/B330636101_S... reply ZacnyLos 8 hours agoprevHere are his works on Wikisources (in 7 languages, public domain): https://en.wikisource.org/wiki/Author:Michel_de_Montaigne reply bloak 10 hours agoprevA translation into modern French might be an interesting addition. reply edweis 8 hours agoparentHere it is, 1907: https://fr.wikisource.org/wiki/Essais/%C3%A9dition_Michaud,_... EDIT: my bad, this is not modern French reply RandomThoughts3 1 hour agorootparentThis is very much modern French. Anything written after 1650 is easy to understand by the average French person and anything written after 1800 is indistinguishable from how French is written nowadays. reply bambax 8 hours agoprevThis book How to Live: A Life of Montaigne in one question and twenty, by Sarah Bakewell (2011) is an incredible introduction to Montaigne. I greatly enjoyed it and recommend it fondly to anyone's interested in the man or what he had to say. reply tbcj 5 hours agoparentAgreed - how it contextualizes the time and place for Montaigne when writing the essays is invaluable to understanding the essays and how he changes over time. reply ninalanyon 7 hours agoprevW. Carew Hazlitt’s 1877 update of Charles Cotton’s translation is on Project Gutenberg if you prefer, as I do, an epub copy. https://www.gutenberg.org/ebooks/3600 reply benreesman 11 hours agoprevMonsieur de Montaigne’s observations are highly above the average as concerns wealthy people advising everyone what to do with a nobleman’s time and resources. As a new magistrate or nobleman it’s a decent place to start winding up with something better than a Tesla and a Substack advising people how to inherit a monopoly. But for us plebs, it’s about as compelling as any other bunch of dandies on a tennis court. reply aubanel 1 hour agoparentHard disagree. His thoughts are so rich and varied that it's harsh to classify them under \"blogs for wealthy people\". He speaks about death, self worth, many other things that speak to anyone. reply benreesman 26 minutes agorootparentI myself said that de Montaigne is pretty good stuff as this sort of thing goes. But the kind of agency attached to being quasi-Royal wealthy in the mid-sixteenth century France is not terribly useful to anyone under crushing debt peonage then, nor it’s resurgent beginning comeback now. For truly catholic stoicism there are better sources. If I want to hear someone talk about inner will from atop a mountain I’ll go all the way back to Marcus Aurelius. It’s good to see that Randian Objectivists are diversifying out of such a shitty brand, but it’s all boomers and their bootstraps to me, and I’ve read fucking ALL of it. Twice. reply lukan 7 hours agoparentprevI think philosophy works when you have money, as well when you don't. Or rather, only good philosophy works also when you don't have money and I enjoyed Montaigne a lot, when I was backpacking without money. reply benreesman 10 hours agoparentprevYou want some carcinogenic French thought? May I introduce you to Julien Offray de La Mettrie. reply afpx 1 hour agoparentprevI’m almost done with Book 1, and so far at least 85% applies to anyone: don't be idle, don't lie, don't make hasty decisions, build up your willpower, be courageous, be present, do what you say you will do, challenge customs (because people tend to choose custom over reason), learn through experience, be present, life is a delicate balance, and about 50 more reply benreesman 17 minutes agorootparentIt’s a lot of very bright folks on HN, and very well read. But this stuff at this stage of the game is somewhere between propaganda and Stockholm’s Syndrome. Hackers have never been paid less, or had worse job security, or been more subject to being fucked with than in decades. Getting funding or traction or press or customers has never been more insular and nepotistic and corrupt and numb in decades. There is a season for de Montaigne and there is a season for Thomas Paine. reply youssefabdelm 8 hours agoprevPro tip: html { -webkit-font-smoothing: antialiased; } For a better reading experience. If you're on Arc and have Boosts, I also recommend a darker background. reply billfruit 8 hours agoprevThere is already the excellent Screech translation and Frame translation(which seems popular with Americans) available, how is this different apart from being online. reply m3kw9 1 hour agoprevWhat if a boxer learns some sort of visual prompt injections to increase the hit rate. Like punching past people a heads reply rramadass 4 hours agoprevA good edition to own is the beautiful hardcover edition published by Everyman's Library titled Michel de Montaigne The Complete Works Essays Travel journal Letters and translated by Donald Frame - http://www.everymanslibrary.co.uk/classics-author.aspx?lette... reply loughnane 3 hours agoparentThe Donald frame translation is lovely. It’s a shame it’s not in the public domain. reply rramadass 3 hours agorootparentAsk and ye shall receive ;-) https://archive.org/details/MontaigneCompleteFrame/mode/2up https://archive.org/stream/MontaigneCompleteFrame/Montaigne%... reply octed 2 hours agorootparentThis is the translation I was referring to in a previous comment! Interesting how it has been brought up twice in a single thread. reply selimthegrim 3 hours agorootparentprevI wonder which joker tagged the language as Norwegian. reply grozmovoi 12 hours agoprevI've been working on writing essays to process my thinking for the last few months. Glad to see this be on #1 of `new`. reply seizethecheese 13 hours agoprev [–] Okay I’ll bite. The essay on raising children piqued my interest. The first two paragraphs: > I never yet saw that father, but let his son be never so decrepit or deformed, would not, notwithstanding, own him: not, nevertheless, if he were not totally besotted, and blinded with his paternal affection, that he did not well enough discern his defects: but that with all defaults, he was still his. Just so, I see better than any other, that all I write here are but the idle reveries of a man that has only nibbled upon the outward crust of sciences in his nonage, and only retained a general and formless image of them; who has got a little snatch of everything and nothing of the whole, à la Françoise. This does not seem “updated” or “modern”. Updating these old texts seems like a perfect use case for AI. Let’s give GPT 4o a shot: > I have never seen a father, no matter how frail or deformed his son may be, who would not still claim him as his own. Yet, unless completely blinded by paternal affection, the father is fully aware of his son’s flaws. Despite those shortcomings, the son remains his child. In the same way, I am more aware than anyone else that what I write here is nothing more than the idle musings of someone who, in his youth, only skimmed the surface of knowledge. I have retained only a vague and incomplete impression of the sciences, having dabbled a little in everything but mastered nothing—true to the French way. Much better! reply octed 12 hours agoparentThe author of the website has mentioned that > I am slowly replacing the Cotton/Hazlitt translation with a contemporary one and adding new notes So I would assume that the essay you're talking about is from the earlier Cotton translation and has still not been replaced. This is the first time I've seen AI being used to \"modernize\" old texts, and it works wonderfully in this case; though a bit of the old-timey charm is lost imo. I used to read a translation that I'd found in my university library which I enjoyed a lot. Very readable but still retained the \"feel\" of a 16th century book. I don't recall the translator unfortunately. reply RobPfeifer 12 hours agorootparentTotally right. The art and science of translation is an age-old debate and where AI isn’t super well suited. We’re not at a point where it ends up more than a summary but the point is the proper “translation” of the tone, subtle intent and idiosyncrasies of the author. That said, most human translators take license (e.g. The Bible) and how do we counterweight against their flaws, so there’s not a great answer here. Except I hope the guy works through it and does a good job cause the original is a bit of a slog! reply a2800276 10 hours agoparentprevDid you translate the original French to modern English or modernize the new translation? I wonder how those would compare. Translating the original would probably be the 'correct' thing to do, from a literary point of view. Poses some interesting question, though. reply l3x4ur1n 10 hours agoparentprevFor a non native English speaker the \"translation\" is much more readable and can convey more information to me. The old text is kind of comprehensible to me, but I have to read really slow, re-read parts and think a lot to understand. reply alldayhaterdude 12 hours agoparentprev [–] I disagree. This sucks. reply baudaux 1 hour agorootparent [–] It is like repainting La Joconde reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "HyperEssays is an online project providing modern editions of Michel de Montaigne's Essays, featuring four versions from different historical translations and updates.- The project offers tools and context for new readers, with free chapter PDFs and formatting for easy reading on various devices, covering topics like religion, friendship, and law.- HyperEssays is a continually evolving resource, welcoming contributions to maintain its accessibility and free availability."
    ],
    "commentSummary": [
      "Michel de Montaigne's essays, available online, are often likened to blogs due to their personal yet carefully crafted nature, covering diverse topics such as the acceptance of death.",
      "A notable quote from his essay \"Of Experience\" emphasizes the human condition, stating that even on the highest throne, we remain human.",
      "Modern translations and AI-assisted updates aim to make Montaigne's writings more accessible, preserving their original charm for both wealthy and ordinary readers."
    ],
    "points": 148,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1734844597
  },
  {
    "id": 42481612,
    "title": "Dividing unsigned 8-bit numbers",
    "originLink": "http://0x80.pl/notesen/2024-12-21-uint8-division.html",
    "originBody": "Dividing unsigned 8-bit numbers Author: Wojciech Muła Added on: 2024-12-21 Updated on: 2024-12-22 (fixed a typo, thanks John Rinehart, clarify introduction based on HN comments, add more AVX-512 variants) Contents Introduction Floating-point point operations Division with rounding Division without rounding Using approximate reciprocal Long division Vectorization SSE & AVX2 Step 1: Updating reminder Step 2: Comparison Step 3: Conditional operations Implementation (SSE) AVX-512 Step 1: Updating reminder Step 2: Comparison Step 3: Conditional operations Implementation Experiment results Ryzen 7 Skylake-X IceLake Source code Introduction Division is quite an expensive operation. For instance, latency of the 32-bit division varies between 10 and 15 cycles on the Cannon Lake CPU, and for Zen4 this range is from 9 to 14 cycles. The latency of 32-bit multiplication is 3 or 4 cycles on both CPU models. None of commonly used SIMD ISAs (SSE, AVX, AVX-512, ARM Neon, ARM SVE) provides the integer division, only RISC-V Vector Extension does. However, all these ISAs have floating point division. In this text we present two approaches to achieve a SIMD-ized division of 8-bit unsigned numbers: using floating point division, using the long division algorithm. We try to vectorize the following C++ procedure. The procedure cannot assume anything about dividends, especially if they are all equal. Thus, it is not possible to employ division by a constant. void scalar_div_u8(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n) { for (size_t i=0; i2b45: 45 31 c0 xor %r8d,%r8d 2b60: 42 0f b6 04 07 movzbl (%rdi,%r8,1),%eax 2b65: 42 f6 34 06 divb (%rsi,%r8,1) 2b69: 42 88 04 02 mov %al,(%rdx,%r8,1) 2b6d: 49 ff c0 inc %r8 2b70: 4c 39 c1 cmp %r8,%rcx 2b73: 75 eb jne 2b602b75: c3 ret Floating-point point operations An 8-bit number can be converted into single precision floating point number without any precision loss. The generic outline of division consist the following steps: cast 8-bit dividend and divisor into 32-bit integers, convert unsigned integers into floating-point numbers, perform floating-point division, convert floating-point result into 32-bit integers, cast back 32-bit integer into 8-bit final result. Division with rounding Here is the actual implementation of SSE procedure. Note that we need to explicitly truncate the floating point number before converting back into integer. By default that conversion rounds the argument, so we would get wrong results (off by 1). Load four 8-bit dividends. uint32_t buf_a; memcpy(&buf_a, &a[i], 4); And four 8-bit divisors. uint32_t buf_b; memcpy(&buf_b, &b[i], 4); Transfer them to SSE register and cast to 32-bit numbers. const __m128i a_u8 = _mm_cvtsi32_si128(buf_a); const __m128i a_u32 = _mm_cvtepu8_epi32(a_u8); const __m128i b_u8 = _mm_cvtsi32_si128(buf_b); const __m128i b_u32 = _mm_cvtepu8_epi32(b_u8); Cast 32-bit integers into floats. const __m128 a_f32 = _mm_cvtepi32_ps(a_u32); const __m128 b_f32 = _mm_cvtepi32_ps(b_u32); Perform division and then truncation. const __m128 c_f32 = _mm_div_ps(a_f32, b_f32); const __m128 c_f32_2 = _mm_round_ps(c_f32, _MM_FROUND_TO_ZERO_MM_FROUND_NO_EXC); Convert floats back into integers. const __m128i c_i32 = _mm_cvtps_epi32(c_f32_2); Cast 32-bit into 8-bit numbers: gather lowest 8-bit numbers into single 32-bit word and save this word to the output array. const __m128i c_u8 = _mm_shuffle_epi8(c_i32, _mm_setr_epi8( 0, 4, 8, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 )); const uint32_t buf = _mm_cvtsi128_si32(c_u8); memcpy(&out[i], &buf, 4); Division without rounding Rounding instruction ROUNDPS has quite big latency, at least on Intel CPUs. On IceLake it is 8 cycles, while Zen4 has only 3 cycles. We can avoid floating point rounding by multiplying the dividend 256 (shift left by 8 bits) and shifting right by 8 the final result. The shift right can be done at no cost, because we anyway use shuffling to gather individual bytes, so it's only matter of a constant. Shifting left by 8 is suitable only for SSE code — we can use byte shuffle to shift-and-extend integers. In the case of AVX2 code, byte shuffling is done on 128-bit lanes, thus we would need more work to prepare input for that operation. The SSE procedure is almost the same as in the previous section: Load four dividends. uint32_t buf_a; memcpy(&buf_a, &a[i], 4); const __m128i a_u8 = _mm_cvtsi32_si128(buf_a); Convert dividend > 8 into 8-bit numbers: gather bit #1 of each 32-bit word. const __m128i c_u8 = _mm_shuffle_epi8(c_i32, _mm_setr_epi8( 0 + 1, 4 + 1, 8 + 1, 12 + 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1 )); const uint32_t buf = _mm_cvtsi128_si32(c_u8); memcpy(&out[i], &buf, 4); Using approximate reciprocal SSE comes with instruction RCPPS that calculates the approximate inversion of its argument: 1/x. This would allow us to use expression dividend ⋅ approx(1/divisor). The specification says relative error does not exceed 1.5 ⋅ 2 − 12. But for our needs, the absolute error is too big to use the instruction result directly. The following table shows the results for initial x values, for which the error is significant. However, by trial-and-error search, we found that after multiplying the dividend by value 1.00025, the result of RCPPS can be used. To be precise, any multiplier between 1.00024 and 1.00199 works. x 1 / x approx 1 / x error float hex float hex 1 1.000000 3f800000 0.999756 3f7ff000 0.000244 2 0.500000 3f000000 0.499878 3efff000 0.000122 3 0.333333 3eaaaaab 0.333313 3eaaa800 0.000020 4 0.250000 3e800000 0.249939 3e7ff000 0.000061 5 0.200000 3e4ccccd 0.199951 3e4cc000 0.000049 6 0.166667 3e2aaaab 0.166656 3e2aa800 0.000010 7 0.142857 3e124925 0.142822 3e124000 0.000035 8 0.125000 3e000000 0.124969 3dfff000 0.000031 9 0.111111 3de38e39 0.111084 3de38000 0.000027 10 0.100000 3dcccccd 0.099976 3dccc000 0.000024 11 0.090909 3dba2e8c 0.090897 3dba2800 0.000012 12 0.083333 3daaaaab 0.083328 3daaa800 0.000005 13 0.076923 3d9d89d9 0.076920 3d9d8800 0.000004 14 0.071429 3d924925 0.071411 3d924000 0.000017 15 0.066667 3d888889 0.066650 3d888000 0.000016 16 0.062500 3d800000 0.062485 3d7ff000 0.000015 17 0.058824 3d70f0f1 0.058807 3d70e000 0.000016 18 0.055556 3d638e39 0.055542 3d638000 0.000014 19 0.052632 3d579436 0.052620 3d578800 0.000012 20 0.050000 3d4ccccd 0.049988 3d4cc000 0.000012 ... 255 0.003922 3b808081 0.003922 3b808000 0.000000 Long division The long division algorithm is the one we know from school. We start with the reminder and quotient equals zero. Then, in each step, we extend the reminder with the next digit of the dividend, starting from most significant digits. If the remainder becomes bigger than the divisor, we calculate q := reminder/divisor. This number, a digit, is in range [0, 9] for decimal division, or is either 0 or 1 for binary division. We decrement reminder by q ⋅ divisor. We prepend the digit q to the quotient. If there are still digits in dividend, go to point #2. A nice property of the algorithm is calculating both the quotient and reminder. The cons of algorithm are: Each step depends on the predecessor, it makes hard to utilize out-of-order CPUs. The number of iterations equal the number of significant digits in dividend minus # of digits in divisor plus 1. In the worst case, it equals # of digits in dividend. Since we're going to divide 8-bit numbers, it means that the basic step of algorithm has to be repeated eight times. Below is a reference implementation. uint8_t long_div_u8(uint8_t dividend, uint8_t divisor) { uint8_t reminder = 0; uint8_t quotient = 0; for (int i=7; i >= 0; i--) { // make room for i-th bit of dividend at 0-th position reminder > i) & 1; if (reminder >= divisor) { // set i-th bit in quotient quotient |= 1 > i) & 1); compare the reminder and divisor; conditionally set i-th bit in the quotient and adjust the reminder: quotient |= 1 = 0; i--) { // msb => 0 or -1 const uint8_t msb = (int8_t)dividend = (int8_t)(divisor_xored) { // set i-th bit in quotient quotient |= bit; // adjust reminder reminder -= divisor; } bit >>= 1; dividend > i; reminder = reminder(t0 & 1); // ternary operation Step 2: Comparison AVX512 supports unsigned byte comparison, and returns a mask. Step 3: Conditional operations This is straightforward use of masked operations. Implementation The actual AVX512 implementation is shown below. Unlike SSE code, the inner loop is manually unrolled. Also, there's no explictly use of the ternary logic intrinsic function — but examing the assembly code revelas that a compiler nicely fuses binary operation. void avx512_long_div_u8(const uint8_t* A, const uint8_t* B, uint8_t* C, size_t n) { const __m512i one = _mm512_set1_epi8(1); for (size_t i=0; i < n; i += 64) { const __m512i dividend = _mm512_loadu_si512((const __m512*)(&A[i])); const __m512i divisor = _mm512_loadu_si512((const __m512*)(&B[i])); const __m512i dividend_bit7 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 7), one); const __m512i dividend_bit6 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 6), one); const __m512i dividend_bit5 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 5), one); const __m512i dividend_bit4 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 4), one); const __m512i dividend_bit3 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 3), one); const __m512i dividend_bit2 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 2), one); const __m512i dividend_bit1 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 1), one); const __m512i dividend_bit0 = _mm512_and_epi32(_mm512_srli_epi32(dividend, 0), one); __m512i quotient = _mm512_set1_epi32(0); __m512i reminder = dividend_bit7; { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit6); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi32(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit5); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi32(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit4); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi32(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit3); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi32(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit2); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi32(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit1); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi32(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } reminder = _mm512_add_epi32(reminder, reminder); reminder = _mm512_or_epi32(reminder, dividend_bit0); { const __mmask64 ge = _mm512_cmpge_epu8_mask(reminder, divisor); reminder = _mm512_mask_sub_epi8(reminder, ge, reminder, divisor); quotient = _mm512_add_epi8(quotient, quotient); quotient = _mm512_mask_add_epi8(quotient, ge, quotient, one); } _mm512_storeu_si512((__m512*)&C[i], quotient); } } Experiment results Short summary: AVX-512 implementation of long division is the fastests on all Intel CPUs. AVX2 using approximate reciprocal is the fastests on Ryzen. It's worth noting that GCC autovectorized better or comparable code to hand-written AVX2 variants. All benchmark programs were compiled with -O3 -march=native options on each machine separately. Tested procedures Procedure Comments scalar plain 8-bit division scalar (unrolled x 4) division unrolled manually 4 times scalar (long division) scalar implementation of long division, with disabled autovectorization scalar (long div, autovect) scalar implementation of long division, with autovectorization SSE division with rounding SSE (no rounding) division without rounding (divident multiplied by 256) SSE (cvtt) division followed by casting with truncation (CVTTPS2DQ) SSE (rcp) multiplication by approximate reciprocal SSE long div long division implemented with SSE instructions AVX2 division with rounding AVX2 (cvtt) division followed by casting with truncation (CVTTPS2DQ) AVX2 (rcp) multiplication by approximate reciprocal AVX2 long div long division implemented with AVX2 instructions AVX512 (cvtt) division followed by casting with truncation (CVTTPS2DQ) AVX512 (rcp) multiplication by approximate reciprocal AVX512 long div long division implemented with AVX-512 instructions Ryzen 7 Compiler: gcc (Debian 14.1.0-5) 14.1.0 CPU: AMD Ryzen 7 7730U with Radeon Graphics procedure time in cycles per byte speed-upaverage bestscalar 1.777 1.759 1.0 ████▊ scalar (unrolled x 4) 1.881 1.869 0.9 ████▌ scalar (long div) 5.548 5.523 0.3 █▌ scalar (long div, autovect) 0.418 0.414 4.2 ████████████████████▍ SSE 0.372 0.367 4.8 ███████████████████████ SSE (no rounding) 0.334 0.331 5.3 █████████████████████████▌ SSE (rcp) 0.330 0.327 5.4 █████████████████████████▉ SSE long div 0.751 0.741 2.4 ███████████▍ AVX2 0.220 0.218 8.1 ██████████████████████████████████████▉ AVX2 (rcp) 0.215 0.212 8.3 ████████████████████████████████████████ AVX2 long div 0.376 0.372 4.7 ██████████████████████▊ Skylake-X CPU: Intel(R) Xeon(R) W-2104 CPU @ 3.20GHz Compiler: gcc (GCC) 11.2.0 procedure time in cycles per byte speed-upaverage bestscalar 8.044 8.018 1.0 ███▌ scalar (unrolled x 4) 7.020 7.016 1.1 ████ scalar (long div) 18.988 18.176 0.4 █▌ scalar (long div, autovect) 1.003 0.991 8.1 ████████████████████████████▎ SSE 1.168 1.163 6.9 ████████████████████████▏ SSE (no rounding) 0.820 0.814 9.9 ██████████████████████████████████▍ SSE (cvtt) 0.831 0.826 9.7 █████████████████████████████████▉ SSE (rcp) 0.907 0.903 8.9 ███████████████████████████████ SSE long div 2.096 2.090 3.8 █████████████▍ AVX2 1.062 1.057 7.6 ██████████████████████████▌ AVX2 (cvtt) 0.839 0.835 9.6 █████████████████████████████████▋ AVX2 (rcp) 0.971 0.967 8.3 █████████████████████████████ AVX2 long div 1.076 1.069 7.5 ██████████████████████████▎ AVX512 (cvtt) 1.483 1.474 5.4 ███████████████████ AVX512 (rcp) 1.191 1.186 6.8 ███████████████████████▋ AVX512 long div 0.709 0.702 11.4 ████████████████████████████████████████ IceLake Compiler: gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2) CPU: Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz procedure time in cycles per byte speed-upaverage bestscalar 6.036 6.011 1.0 ██▋ scalar (unrolled x 4) 6.016 6.012 1.0 ██▋ scalar (long div) 9.865 8.397 0.7 █▉ scalar (long div, autovect) 0.581 0.578 10.4 ███████████████████████████▍ SSE 0.593 0.586 10.3 ███████████████████████████ SSE (no rounding) 0.476 0.473 12.7 █████████████████████████████████▍ SSE (cvtt) 0.487 0.484 12.4 ████████████████████████████████▋ SSE (rcp) 0.504 0.498 12.1 ███████████████████████████████▊ SSE long div 1.247 1.240 4.8 ████████████▊ AVX2 0.546 0.518 11.6 ██████████████████████████████▌ AVX2 (cvtt) 0.447 0.442 13.6 ███████████████████████████████████▊ AVX2 (rcp) 0.445 0.431 13.9 ████████████████████████████████████▊ AVX2 long div 0.641 0.635 9.5 ████████████████████████▉ AVX512 (cvtt) 0.827 0.814 7.4 ███████████████████▍ AVX512 (rcp) 0.507 0.503 12.0 ███████████████████████████████▍ AVX512 long div 0.398 0.396 15.2 ████████████████████████████████████████ Source code Sample implementation is available at GitHub.",
    "commentLink": "https://news.ycombinator.com/item?id=42481612",
    "commentBody": "Dividing unsigned 8-bit numbers (0x80.pl)148 points by mfiguiere 23 hours agohidepastfavorite61 comments orlp 22 hours agoWhat's not mentioned is that in most cases you have a constant divisor which lets you replace division by multiplication with the reciprocal. The reciprocal can be rounded to a nearby dyadic rational, letting you do the division with a right-shift. For example, 8-bit division by 3 is equivalent to widening multiplication by 171 followed by a right-shift of 9, as 171/2^9 = 0.333984375 which is close enough to 1/3 that the results match exactly. reply titzer 42 minutes agoparentThe article might have been updated since you read it, but this is in there: > We try to vectorize the following C++ procedure. The procedure cannot assume anything about dividends, especially if they are all equal. Thus, it is not possible to employ division by a constant. reply edflsafoiewq 22 hours agoparentprevA shift of 16 is enough for every 8-bit numerator, ie. x/a is (u32(x)*b)>>16 for some b depending only on a. You could precompute b for each a and store it a lookup table. The largest b is b=65536 for a=1 and the smallest is b=258 for a=255, so b fits in a u16 if stored with a 1-offset. Not sure it's worth it unless you reuse the denominator many times though. reply 15155 19 hours agoparentprevThese methods are especially useful in hardware/FPGA implementations where it's infeasible to have a ton of fully pipelined dividers. reply andrepd 18 hours agorootparentThey are actually useful for optimising compilers too! Mul or mul+shifts is often faster than div reply uticus 4 hours agoparentprevDoes this approach still support modulo? reply dzaima 4 hours agorootparentWorst-case, you just multiply the result by the divisor and subtract that from the dividend. reply kevinventullo 21 hours agoparentprevAlso, if you know ahead of time that it’s exact division, there is a similar approach that doesn’t even need widening multiplication! reply orlp 20 hours agorootparentYes, if you know something is an exact multiple of n = r*2^k where r is odd, you can divide out the multiple by right-shifting k followed by modular multiplication by the modular multiplicative inverse of r. reply thaumasiotes 15 hours agorootparentTheoretically, you could also take advantage of two's complement arithmetic: 00011011 (27) x 01010101 (-0.333...) ------------------------ 11110111 (-9) invert and add one: 00001001 (9, like we wanted) I'd be interested in extending this to non-exact multiples, but if that's possible I don't know how. reply Footkerchief 19 hours agorootparentprevCan you provide an example with details? Thanks! reply kevinventullo 19 hours agorootparentIn 8-bit arithmetic (i.e. mod 256), the multiplicative inverse of 11 is 163. So, if you take some multiple of 11, say 154, then you can compute 154/11 instead as 154*163. Indeed, 154*163 = 25102, and 25102 = 14 (mod 256). reply thaumasiotes 21 hours agoparentprev> For example, 8-bit division by 3 is equivalent to widening multiplication by 171 followed by a right-shift of 9, as 171/2^9 = 0.333984375 which is close enough to 1/3 that the results match exactly. Is this related to the fact that 171 is the multiplicative inverse of 3 (mod 256), or is that a coincidence? reply zahlman 19 hours agorootparentSort of. (After all, a \"reciprocal\" for our purposes is just a multiplicative inverse in the reals, so it makes sense that it would be related to the multiplicative inverse in other domains.) 3 times 171 is 513. So to divide by 3, we could multiply by 171 and then divide by 513. Dividing by 513... isn't any easier, but 513 is close to 512, so we hope that dividing by 512 (which is trivial - just a right-shift) gets us close enough. Suppose for a moment we try dividing 3 by 3 using this trick. First we'll multiply by 171 to get 513. Consider that value in binary: 1000000001 ^^^^^^^^^ ~~~~~~~~ Dividing by 512 will shift away the ^ bits. For floor division, we therefore want the ^ underlined value to be close to zero. (That way, when we divide 255 (say) by 3, the error won't be big enough to overflow into the result bits.) The multiplicative inverse fact is equivalent to telling us that the ~ underlined bits are exactly 1. Conveniently, that's close to 0 - but we didn't account for all the ^ underlined bits. For example, the multiplicative inverse of 7 (mod 256) is 183, but 7 times 183 is 1281. That's close to 1280, but that doesn't really help us - we could right-shift by 8 but then we still have to divide by 5. If we just ignore the problem and divide by 1024 (right-shift by 10), of course we get a lot of wrong results. (Even 6 / 7 would give us 1 instead of 0.) It also turns out that we'll need more bits for accurate results in the general case. I think it's possible without overflowing 16-bit numbers, but it definitely requires a bit more trickery for problematic divisors like (from my testing) 195. I thought I remembered the details here but proved myself wrong at the Python REPL :( reply orlp 18 hours agorootparentDivision by 195 is trivial. The answer is simply uint8_t div195(uint8_t x) { return x >= 195; } reply zahlman 18 hours agorootparentYes, but you can't incorporate that into an algorithm that allows the divisor to vary and avoids branching. 195 is problematic for the multiply-shift strategy. reply orlp 20 hours agorootparentprevIt's not entirely a coincidence but also not a general result that one should use the modular inverse as multiplier. 171 * 3 = 2^9 + 1, which is not surprising as we know that 171 * 3 = 1 (mod 2^8). So rearranged we have 171 / 2^9 = 1/3 + 1/(2^9*3) which shows it's a close approximation of 1/3. reply RaisingSpear 7 hours agoprevFor CPUs that support AVX-512 VBMI, there's a faster reciprocal-based approach: https://avereniect.github.io/2023/04/29/uint8_division_using... A VBMI2 example implementation can be found in the function named divide_with_lookup_16b: https://godbolt.org/z/xdE1dx5Pj reply mlochbaum 22 hours agoprevWe implemented something like this in CBQN last year (mainly for modulus, as floor division isn't a primitive). Commit is https://github.com/dzaima/CBQN/commit/d333902, some proofs of when and why it works at https://mlochbaum.github.io/BQN/implementation/primitive/ari.... reply AlotOfReading 23 hours agoprevI'm not certain it'll actually be faster, but you should be able to merge the reciprocal, multiplication and rounding adjustment into a single fma in log space via evil floating point bit hacks. Then you'd just be paying the cost of converting to and from the float representation of the integer. reply ashvardanian 22 hours agoparentI'd also love to see such comparisons. In SimSIMD, if AVX-512FP16 is available, I use FP16 for most I8/U8 operations, but can't speak about division. Overall, using F64 directly for I32 division is a well known trick, and it should hold for smaller representations as well. reply AlotOfReading 22 hours agorootparentDon't have time to figure out the actual numbers myself, but here's an example doing the same for various transcendental functions: https://github.com/J-Montgomery/hacky_float_math/blob/623ee9... a*(1.xxx/b) = a*(-1*1.xxx*log2(b)), which means you should be able to do a*(fma(b, magic, constant)) with appropriate conversions on either side. Should work in 32 bits for u8s. reply dzaima 20 hours agoprevHere's a version of the vrcpps idea, doing whole vectors of 32 u8's, bypassing lane crossing and somewhat simplifying the packing/unpacking, that's ~1.5x faster on Haswell: https://godbolt.org/z/TExGahv3z reply Cold_Miserable 18 hours agoparentThis is ~9.6x faster than \"scalar\". ASM_TestDiv proc ;rcx out, rdx A, r8 B mov rax,05555555555555555H kmovq k1,rax vmovdqu8 zmm0,zmmword ptr [rdx] vmovdqu8 zmm4,zmmword ptr [r8] vpbroadcastw zmm3,word ptr [FLOAT16_F8] vmovdqu8 zmm2{k1},zmm0 ;lower 8-bit vmovdqu8 zmm16{k1},zmm4 ;lower 8-bit vpsrlw zmm1,zmm0,8 ;higher 8-bit vpsrlw zmm5,zmm4,8 ;higher 8-bit vpord zmm1,zmm1,zmm3 vpord zmm2,zmm2,zmm3 vpord zmm5,zmm5,zmm3 vpord zmm16,zmm16,zmm3 vsubph zmm1,zmm1,zmm3{rd-sae} ;fast conv 16FP vsubph zmm2,zmm2,zmm3{rd-sae} vsubph zmm5,zmm5,zmm3{ru-sae} vsubph zmm16,zmm16,zmm3{ru-sae} vrcpph zmm5,zmm5 vrcpph zmm16,zmm16 vfmadd213ph zmm1,zmm5,zmm3{rd-sae} vfmadd213ph zmm2,zmm16,zmm3{rd-sae} vxorpd zmm1,zmm1,zmm3 vxorpd zmm2,zmm2,zmm3 vpsllw zmm1,zmm1,8 vpord zmm1,zmm1,zmm2 vmovdqu8 zmmword ptr [rcx],zmm1 ;16 8-bit unsigned int ret reply foundry27 22 hours agoprevI think the approximate reciprocal approach is interesting here. The doc mentions multiplying the dividend by ~1.00025 in the math to avoid FP error so you don’t end up off-by-one after truncation, but I think this hack is still incomplete! On some inputs (like 255, or other unlucky divisors near powers of two), you might get borderline rounding behaviour that flips a bit of the final integer. It’s easy to forget that single-precision floats don’t line up neatly with every 8bit integer ratio in real code, and a single off-by-one can break pixel ops or feed subtle bugs into a bigger pipeline. I suspect a hybrid scheme like using approximate reciprocals for most values but punting to scalar for unlucky ones could handle these corner cases without killing performance. That’d be interesting to benchmark reply LegionMammal978 22 hours agoparentThere are only 65280 possible inputs, that's easily small enough to test every value for correctness. reply jonhohle 16 hours agoprevI do a fair amount of decompiling and the list of constants is awesome! I usually can make a reasonable guess or have intuition, but frequently end up stepping through several integers to find the right value. Having a handy lookup table will be great. reply synthos 21 hours agoprevNewton Raphson could be used to calculate a reciprocal. (in a bit width larger than 8). If starting from a good reciprocal approximation, convergence to bit accurate reciprocal should not take many iterations. Then multiply the reciprocal with the numerator to perform the divide reply ryao 22 hours agoprevWhy not use libdivide? https://libdivide.com/ reply LegionMammal978 22 hours agoparentlibdivide is optimized for the case of a common divisor used many times, not for a long array of distinct divisors. reply ack_complete 22 hours agoprevThink the SSE2 implementation could be tightened up by using the same register for the dividend and the quotient, shifting the quotient bits into the dividend as the dividend bits are shifted out. This was common practice in software CPU division routines. reply xpe 15 hours agoprevThe bit-division burns my eyes and makes a mockery of my puny intellect, my liege. Mine fellow vassals possess skills most frightful in their potency. reply purplesyringa 18 hours agoprevIt's odd that the only reason AVX-512 long division wins in the end is that it's compared to AVX2 reciprocal. Would it be possible to compare it to AVX-512 reciprocal computation? reply Cold_Miserable 21 hours agoprevHeh? Surely fast convert 8-bit int to 16-bit FP,rcp+mul/div is a no-brainer? edit make that fast convert,rcp,fma (float 16 constant 1.0) and xor (same constant) reply bremac 16 hours agoparentUnfortunately none of the hardware used for testing supports FP16 arithmetic. Between Intel and AMD, the only platform that supports AVX512-FP16 is currently Sapphire Rapids. reply purplesyringa 16 hours agoparentprevI tried a similar approach with 32-bit FP before, and the problem here is that fast conversion is only fast in the sense of latency. Throughput-wise, it takes 2 uops instead of one, so in the end, a plain floatint conversion wins. reply abcd_f 23 hours agoprevSince it's 8bit by 8bit, a precomputed lookup table (64K in size) will be another option. reply ack_complete 22 hours agoparentA lookup table in memory can only be accessed an element at a time, so it gets bottlenecked in the execution units on the load and address generation traffic. This algorithm uses 8-bit integers, so the vectorized version is processing between 16-64 elements per operation depending on the vector width. It's even worse if the 8-bit divide is integrated with other vector operations as then the lookup table method also has insert/extract overhead to exchange the values between the vector and integer units. A hybrid approach using small in-register lookup tables in the vector unit (pshufb/tbl) can be lucrative, but are very limited in table size. reply mithametacs 21 hours agorootparentI've never thought of a micro-lookup-table! That's cool. What have you used those for? reply anonymoushn 11 hours agorootparentYou can test whether a register full of bytes belong to a class of bytes with the high bit unset and distinct lower nibbles in 2 instructions (each with 1 cycle latency). For example the characters that commonly occur in text and must be escaped in json are \"\\\"\\\\\\r\\t\" (5 different bytes). Their lower nibbles are: \\\": 0010 \\t: 1001 : 1010 \\\\: 1100 \\r: 1101 Since there are no duplicate lower nibbles, we can test for membership in this set in 2 instructions. If we want to get the value into a general-purpose register and branch on whether it's 0 or not, that's 2 more instructions. For larger sets or sets determined at runtime, see here: http://0x80.pl/notesen/2018-10-18-simd-byte-lookup.html reply glangdale 11 hours agorootparentprevNot OP, but we use these tables all the time in Hyperscan (for string and character class matching) and it's a long-standing technique to use it for things like SIMD population count (obsoleted now if you have the right AVX-512 ISA, ofc). reply ack_complete 21 hours agorootparentprevTypically just like conventional lookup tables, where you can get the table size down small enough. Indexed palette / quantization coding is a case where this can often work. It's pretty niche given the limitations, but if you can make it work it's often a major speedup since you're able to do 16/32/64 lookups in parallel. reply mmastrac 23 hours agoparentprevLookup tables aren't necessarily faster these days for a lot of things when using low-level languages, but it would have been interesting to see the comparison here. reply adhoc32 23 hours agorootparentpretty sure a memory access is faster than the methods presented in the article. reply PhilipRoman 22 hours agorootparentDepends also heavily on the context. You pay for each cache miss twice - once for the miss itself, and next time when you access whatever was evicted during the first miss. This is why LUTs often shine in microbenchmarks, but drag down performance in real world scenarios when mixed with other cache bound code. reply retrac 22 hours agorootparentprevAccess to main memory can be many many cycles; a short routine already in cache may be able to recompute a value more quickly than pulling it from main memory. reply ryao 22 hours agorootparentprevAn uncached random memory access is around 100 cycles. reply Sesse__ 20 hours agorootparent100 cycles would be very low. Many systems have more than 100 ns! reply Retr0id 22 hours agorootparentprev64K is enough to fill L1 on many systems reply dist-epoch 22 hours agorootparentprevHitting L2 is more than 3-4 cycles reply ashvardanian 23 hours agoparentprev64 KB is a pretty significant budget for such a small operation. I've had a variant that uses 768 bytes with some extra logic, but will deprecate that kernel soon. https://github.com/ashvardanian/StringZilla/blob/0d47be212c5... reply sroussey 22 hours agorootparentThis seems like something that could be in hardware to allow native execution of the instruction. Is that not the case anywhere? reply afstr 16 hours agoprevM I’m looking I’m I’m O ; r reply dataflow 22 hours agoprev [–] > SIMD ISAs usually do not provide the integer division; the only known exception is RISC-V Vector Extension It's kind of funny to read \"the only known exception is...\" in this context. What would an unknown exception be - an ISA that accidentally implements this but that the author believes nobody is aware of yet? More seriously, I actually don't understand the intended meaning here. I assume the author means \"out of all the ISAs I know\"? What is that set of ISAs? reply dzaima 21 hours agoparentSome SIMD ISAs with integer division: - Arm SVE, though only for 32-bit and 64-bit element types: https://developer.arm.com/architectures/instruction-sets/int... - loongarch64 LSX/LASX: https://jia.je/unofficial-loongarch-intrinsics-guide/viewer/... - MRISC32 (though that's somewhat obvious as basically everything in it is shared between scalar and vector). reply bee_rider 22 hours agoparentprevPractically, could the expression “only know exception” mean anything other than “known by me?” I mean, it is clearly possible for an exception to exist, on account of the existing known exception, so they can’t know that more exceptions don’t exist out there somewhere. I dunno. I think it is a more meaningful statement if we know more about the author; if we assume that they are very well informed, I guess we would assume that the fact that they don’t know about something is really meaningful. In the case of a blog post where most of us don’t know the author, it is hard to infer much. But at least it tells us why they decided to do the thing. reply zahlman 18 hours agorootparentIf a SIMD ISA exists, someone must know that it exists, because definitionally we only apply the term \"SIMD ISA\" to things that were consciously created to be such. So we could simply check every such example. Saying \"only known example\" is indeed silly. But e.g. in mathematics, if we say that \"almost every member of set X has property Y; the only known exception is Z\" then there absolutely could be more exceptions, even if we pool the knowledge of every mathematician. It isn't necessary that X is finite, or even enumerable. It could be possible for exceptions other than Z to exist even though every other member of the set that we know about has the property. It could be possible to prove that there are at most finitely many exceptions in an infinite set, and only know of Z but not be able to rule out the possibility of more exceptions than that. We don't even need to appeal to infinities. For example, there are problems in discrete math where nobody has found the exact answer (which necessarily is integer, by the construction of the problem) but we can prove upper and lower bounds. Suppose we find a problem where the known bounds are very tight (but not exact) and the bounded value is positive. Now, construct a set of integers ranging from 0 up to (proven upper bound + 1) inclusive... you can probably see where this is going. The latter doesn't apply to SIMD ISAs, because we know all the interesting (big hand-wave!) properties of all of them rather precisely - since they're designed to have those properties. reply camel-cdr 21 hours agoparentprevThe Convex C1 [0] and for a newer example NEC SX Aurora [1] both also support vector integer division. [0] https://bitsavers.org/pdf/convex/080-000120-000_CONVEX_Archi... [1] https://ftp.libre-soc.org/NEC_SX_Aurora_TSUBASA_VectorEngine... reply michaelhoffman 22 hours agoparentprev> I assume the author means \"out of all the ISAs I know\"? Out of all the ISAs where they know whether it provides integer division or not. reply dataflow 22 hours agorootparentYeah but my point is that as a reader I'm trying to figure out which ISAs actually don't provide this (vs. which ISAs the author lacks knowledge of), and I still don't know what those are. The sentence looks like it's supposed to tell me, but it doesn't. reply perching_aix 22 hours agoparentprev [–] You could come up with an ISA that implements it and it wouldn't be \"known\". Maybe that helps? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text explores methods for dividing unsigned 8-bit numbers using SIMD (Single Instruction, Multiple Data) instructions, focusing on floating-point division and the long division algorithm.",
      "It highlights the implementation of these methods using SSE, AVX2, and AVX-512 instructions, with performance comparisons on CPUs like Ryzen 7, Skylake-X, and IceLake.",
      "Notably, the AVX-512 implementation of long division is the fastest on Intel CPUs, while AVX2 using approximate reciprocal is the fastest on Ryzen, with source code available on GitHub."
    ],
    "commentSummary": [
      "Division of unsigned 8-bit numbers can be optimized by using multiplication with a reciprocal, approximated by a dyadic rational, such as multiplying by 171 and right-shifting by 9 for division by 3.",
      "This optimization is beneficial in hardware and optimizing compilers, with additional techniques like modular multiplication, lookup tables, and SIMD (Single Instruction, Multiple Data) instructions, though each has limitations.",
      "Some Instruction Set Architectures (ISAs), like RISC-V Vector Extension, support SIMD integer division, but such support is uncommon, and various techniques, including floating-point arithmetic and micro-lookup tables, are explored for optimization."
    ],
    "points": 148,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1734809113
  },
  {
    "id": 42481813,
    "title": "JEP 483: Ahead-of-Time Class Loading and Linking",
    "originLink": "https://openjdk.org/jeps/483",
    "originBody": "OpenJDK ErrorHTML { font-size: 18px; line-height: 1.3; } BODY { display: grid; grid-template-columns: 1fr; grid-template-rows: 100vh; align-items: center; } DIV.root { align-self: center; justify-self: center; } DIV.img { display: flex; justify-content: center; } IMG { margin-bottom: 2rem; align: center; } P { width: 30em; } P.incident { text-align: center; } A { text-decoration: none; } A:link { color: #437291; } A:visited { color: #666666; } A[href]:hover { color: #e76f00; } The site is experiencing technical difficulty. We are aware of the problem and are working to correct the issue. We apologize for any inconvenience.If you need assistance, please email ops@openjdk.org and provide this incident number: 0.15192117.1734894137.b98e13e3",
    "commentLink": "https://news.ycombinator.com/item?id=42481813",
    "commentBody": "JEP 483: Ahead-of-Time Class Loading and Linking (openjdk.org)118 points by ptx 23 hours agohidepastfavorite30 comments fulafel 10 hours agoWhat does this mean for Clojure? At least loading the Clojure runtime should benefit, but what about app code loading. reply pixelmonkey 19 minutes agoparentI don't know how this JEP affects Clojure, but if you want to use Clojure for fast-loading CLI apps, a good thing to look at is babashka (bb). I wrote about it here: \"Learning about babashka (bb), a minimalist Clojure for building CLI tools\" https://amontalenti.com/2020/07/11/babashka reply diggan 8 hours agoparentprevI feel like for the Clojure applications where you need it to start really fast, like tiny CLI utilities that don't do a lot of work, the improvements would be so marginal to not matter much. The example they use in the JEP seems to have gone from a ~4 second startup to ~2 seconds, which for a tiny CLI, still would make it seem pretty slow. You're better off trying to use Babashka, ClojureScript or any of the other solutions that give a fast startup. And for the bigger applications (like web services and alike), you don't really care that it takes 5 seconds or 10 seconds to start it, you only restart the server during deployment anyways, so why would startup time matter so much? reply dig1 6 hours agorootparentBig apps where startup time matters are desktop/mobile GUI apps. These aren't heavily emphasized in the Clojure community (excluding ClojureScript), but they are feasible to build - and I do build some of them. If startup time is reduced by 40%, the end user will definitely notice it. IMHO, while optimizations in the JVM are always welcome, they primarily address surface-level issues and don't tackle Clojure's core limitation: the lack of a proper tree shaker that understands Clojure semantics. Graalvm offers help here by doing whole-program optimization at the bytecode level, but a Clojure-specific tree shaker could take things further: it could eliminate unused vars before/during Clojure AOT, thereby reducing both program size and startup time. These improvements would happen before the JVM optimizations kick in, making everything that follows a nice extra bonus. reply fulafel 6 hours agorootparentInteresting thought, I wonder if there's a way to reason about the magnitude of effect this would have. reply misja111 6 hours agorootparentprevLoad balanced web services on e.g. K8S could need to start and stop quite a lot if load varies. Any speed up will be welcome. Also, I guess Java-based desktop applications like IntelliJ and DBeaver will benefit. reply dtech 8 hours agorootparentprevThe 4 second application is a web server. They also give a basic example starting in 0.031s, fine for a CLI. One of the use cases for startup time is AWS lambda and similar. reply diggan 7 hours agorootparent> The 4 second application is a web server. They also give a basic example starting in 0.031s, fine for a CLI. Sure, my comment was more about the relative improvement. In the case of the 0.031s example (which is the number without the improvement), it gets down to 0.018s with this new AOT class loading. What value do you get from something starting in 0.018s instead of 0.031s? The difference is so marginal for that particular use case. > One of the use cases for startup time is AWS lambda and similar. I suppose that's one use case where it does make sense to really focus on startup times. But again, I'd rather use something that fast startup already exists (Babashka, ClojureScript) instead of having to add yet another build-step into the process. reply Tostino 3 hours agorootparentThere are plenty of CLI applications that need to be low overhead. E.g. postgres can call a wal archive command for backup purposes, and I specifically remember work being done to reduce the startup overhead for backup tools like pgbackrest / wal-e. reply kuschku 4 hours agorootparentprevIf you're building e.g. a PS1 prompt replacement, you'll want to start, gather data, output the PS1 prompt and exit in less than 0.016s at most. Any slower and the user will see a visible delay. If you're on higher FPS monitors, the budget shrinks accordingly. At 60fps you'll have 16ms, at 480fps you'll have 2ms. The same applies for any app that should feel like it starts instantly. reply bobnamob 7 hours agorootparentprevPrebuilding a cache through a training run will be difficult between lambda invocations though and snapstart[1] already \"solves\" a lot of the issues a class cache might address. [1] https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html Of course, I wouldn't be surprised if the boffins at lambda add some integration between snapstart and class caching once their leadership can get it funded reply funcDropShadow 8 hours agoparentprevIt should benefit, if namespaces are AOT-compiled by Clojure. reply o11c 15 hours agoprevThe concern that jumps out at me is: what about flags that affect code generation? Some are tied to the subarch (e.g. \"does this amd64 have avx2?\" - relevant if the cache is backed up and restored to a slightly different machine, or sometimes even if it reboots with a different kernel config), others to java's own flags (does compressed pointers affect codegen? disabling intrinsics?). reply lxgr 15 hours agoparentI don’t see any mention that code is actually going to be stored in a JITted form, so possibly it’s just architecture-independent loading and linking data being cached? reply ignoramous 22 minutes agorootparentFrom a related JEP (on AOT): https://openjdk.org/jeps/8335368 As another possible mismatch, suppose an AOT code asset is compiled to use a specific level of ISA, such as Intel’s AVX-512, but the production run takes place on a machine that does not support that ISA level. In that case the AOT code asset must not be adopted. Just as with the previous case of a devirtualized method, the presence of AVX-512 is a dependency attached to the AOT asset which prevents it from being adopted into the running VM. Compare this with the parallel case with static compilers: A miscompiled method would probably lead to a crash. But with Java, there is absolutely no change to program execution as a result of the mismatch in ISA level in the CDS archive. Future improvements are possible, where the training run may generate more than one AOT code asset, for a method that is vectorized, so as to cover various possibilities of ISA level support in production. Also: https://openjdk.org/projects/leyden/ reply MBCook 14 hours agorootparentprevMy impression from reading this was it was about knowing which classes reference which other classes when and which jars everything is in. So I think you’re right. So a bit more linker style optimization than compiler related caching stuff. reply brabel 10 hours agorootparentThe JEP explains what this does: \"The AOT cache builds upon CDS by not only reading and parsing class files ahead-of-time but also loading and linking them.\" While CDS (which has been available for years now) only caches a parsed form of the class files that got loaded by the application, the AOT cache will also \"load and link\" the classes. The ClassLoader.load method docs explain what loading means: https://docs.oracle.com/en/java/javase/21/docs/api/java.base... 1. find the class (usually by looking at the file-index of the jar, which is just a zip archive, but ClassLoaders can implement this in many ways). 2. link the class, which is done by the resolveClass method: https://docs.oracle.com/en/java/javase/21/docs/api/java.base... and explained in the Java Language Specification: https://docs.oracle.com/javase/specs/jls/se21/html/jls-12.ht... \"Three different activities are involved in linking: verification, preparation, and resolution of symbolic references.\" Hence, I assume the AOT cache will somehow keep even symbolic references between classes, which is quite interesting. reply petesoper 17 hours agoprevSweet! reply s6af7ygt 8 hours agoprevI'm a dunce reply dtech 8 hours agoparentRead the article, this doesn't reduce JIT capabilities at all. reply foolfoolz 16 hours agoprevi’m curious if any of this was inspired from aws lambda snapstart reply layer8 16 hours agoparentMaybe read the History section. reply layer8 16 hours agoprev [–] > [example hello world] program runs in 0.031 seconds on JDK 23. After doing the small amount of additional work required to create an AOT cache it runs in in 0.018 seconds on JDK NN — an improvement of 42%. The AOT cache occupies 11.4 megabytes. That’s not immediately convincing that it will be worth it. It is a start I guess. reply dgfitz 13 hours agoparent [–] How so? RAM is almost free if you’re not on embedded, and embedded could run Java sure, but it isn’t common. reply pdpi 12 hours agorootparentThat’s not an in-memory cache either. AIUI it’s storing those artefacts to disk reply lmz 10 hours agorootparentContainer sizes may be affected though. reply pdpi 3 hours agorootparentIf you’re deploying Java applications, container size isn’t exactly your first priority anyhow, and this is O(n) additional space. If image size is a concern, I imagine a native binary using GraalVM would’ve been a better way out anyhow, and you’ll bypass this cache entirely. reply bobnamob 7 hours agorootparentprevSo you're now weighing the increased container pull time (due to size) vs the class load time you're saving through the cache. It's nice to at least have the option of making that tradeoff (And I suspect for plenty of applications, the class cache will be worth more time than (an also probably cached) image pull) reply imtringued 7 hours agorootparentprev [–] RAM might be inexpensive, but this hasn't stopped cloud providers from being stingy with RAM and price gouging. At current RAM prices you'd expect the smallest instances to have 2GB, yet they still charge $4/month for 512MB, which isn't enough to run the average JVM web server. reply zokier 5 hours agorootparent [–] That is pretty ridiculous complaint. Your problem is that they allow configuring instances smaller than your arbitrary baseline? Especially as AWS allows you to pick 2/4/8 GB per vCPU for general purpose instances. And the smallest of these (c7g.medium) is 2GB/1vCPU. The .5 GB t4g.nano has actually more generous ratio because it also has only .1 vCPU, putting it at 5GB/vCPU. I'd assume they are very aware of demand levels for different types and would be adjusting the configurations if needed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "JEP 483 proposes Ahead-of-Time (AOT) class loading and linking for Java, aiming to enhance startup times, particularly beneficial for command-line interface (CLI) applications.",
      "The proposal focuses on caching class loading and linking data, not Just-In-Time (JIT) compiled code, which could be advantageous in serverless environments like AWS Lambda.",
      "A trade-off exists between increased container size and improved class loading speed, which needs careful evaluation, especially for larger applications."
    ],
    "points": 118,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1734810806
  },
  {
    "id": 42485795,
    "title": "Decoding the telephony signals in Pink Floyd's 'The Wall'",
    "originLink": "https://corelatus.com/blog/Decoding_the_telephony_signals_in_Pink_Floyd_s__The_Wall_.html",
    "originBody": "The Corelatus Blog E1/T1 and SDH/SONET telecommunications ABOUT I'm one of the founders at Corelatus. Our niche is telecom hardware for the E1/T1 and SDH/SONET interfaces. I work with layer 2 signalling, i.e SS7 MTP-2, ATM and frame relay, and the Erlang control system for our embedded hardware. You can reach me at matthias@corelatus.com CATEGORY: All 2024 Decoding the telephony signals in Pink Floyd's 'The Wall' 2023 How far can I place a PMP from the tap-in point? (update) 2019 Reading SS7 packets from a PCap file and replaying Replaying bit-exact E1/T1 timeslot recordings 2017 Figuring out a Corelatus module's (forgotten) IP address Finding the bad link in an SDH network using BIP counters 2016 What does an improperly placed monitor point do to a signal? Debugging a PDH frequency offset 2015 Decoding random data I have 47 E1s? How much hardware do I need to monitor them? 2014 MTP-2 Annex A (High Speed Link) signalling Daisy-chaining SDH/SONET interfaces Getting hex dumps of E1/T1 timeslots 2013 save_to_pcap now supports PCap-NG for Wireshark Dimensioning SDH/SONET monitoring Configuring SDH/SONET 2012 Retroactive debugging: recording signalling for later analysis Upgrading firmware on GTH Decoding signalling on the Abis interface with Wireshark 2011 Controlling a GTH from Amazon EC2 Live Wireshark captures on Windows 2010 Porting C socket code to Windows Decoding UMTS (3G) interfaces with Wireshark Decoding the Gb Interface with Wireshark GTH 'C' API 2009 Audio power levels on E1/T1 timeslots: the digital milliwatt How does TCP behave on an interrupted network? Capturing SS7 with wireshark or tshark Generating DTMF using a 'player' on GTH Perl example code for GTH: SS7 ISUP decoding and playback/record Decoding MTP-3 and ISUP The timestamp field in signalling headers GTH audio streaming: why stream over TCP? SOX parameters for downsampling to 8kHz Alaw Erlang example code for GTH CATEGORIES All articles(35) C(5) GTH(34) SDH and SONET(5) Windows(3) erlang(2) python(3) questions-from-customers(5) telecom-signalling(24) wireshark(7) Decoding the telephony signals in Pink Floyd's 'The Wall' Posted December 4th 2024 I like puzzles. Recently, someone asked me to identify the telephone network signalling in The Wall, a 1982 film featuring Pink Floyd. The signalling is audible when the main character, Pink, calls London from a payphone in Los Angeles, in this scene (Youtube). Here's a five second audio clip from when Pink calls: What's in the clip? The clip starts with some speech overlapping a dial-tone which in turn overlaps some rapid tone combinations, a ring tone and some pops, clicks and music. It ends with an answer tone. The most characteristic part is the telephone number encoded in the rapid tone combinations. Around 1980, when the film was made, different parts of the world used similar, but incompatible, tone-based signalling schemes. They were all based on the same idea: there are six or eight possible tones, and each digit is represented by a combination of two tones. Let's examine a spectrogram SoX, an audio editing tool for PCs, can make charts that show the spectral components of the audio over time. The horizontal axis represents time, the vertical axis frequency, and darker sections show more audio power, and lighter sections less. Signalling tones appear as dark horizontal lines in the spectrogram, with the digit signalling visible from 0.7 to 1.8 seconds. That part of the signalling has tones at roughly 700, 900, 1100, 1300, 1500 and 1700 Hz. Which signalling standards were in common use? DTMF (ITU-T Q.23 and Q.24) Everyone's heard DTMF (Dual Tone Multi Frequency). It's the sound your phone makes when you interact with one of those \"Press 1 if you are a new customer. Press 2 if you have a billing enquiry. Press 3...\" systems. DTMF is still used by many fixed-line telephones to set up a call. In DTMF, each digit is encoded by playing a \"high\" tone and a \"low\" tone. The low ones can be 697, 770, 852 or 941 Hz. The high ones 1209, 1336, 1477 and 1633 Hz. None of the pairs in the audio match this, so it's not DTMF. Here's an audio clip of what it would sound like if we used DTMF signalling for the same number, with about the same speed of tones: CAS R2 (ITU-T Q.400—490) CAS R2 uses a two-out-of-six tone scheme with the frequencies 1380, 1500, 1620, 1740, 1860 and 1980 Hz for one call direction and 1140, 1020, 900, 780, 660 and 540 Hz for the other. None of these are a good match for the tones we heard. Besides, Pink is in the USA, and the USA did not use CAS R2, so it's not CAS. This is what the digit signalling would have sounded like if CAS were used: SS5 (ITU-T Q.153 and Q.154) SS5 also uses a two-out-of-six scheme with the frequencies 700, 900, 1100, 1300, 1500 and 1700 Hz. This matches most of what we can hear, and SS5 is the signalling system most likely used for a call from the USA to the UK in the early 1980s. This is what the digit signalling sounds like in SS5, when re-generated to get rid of all the other sounds: SS7 (ITU-T Q.703—) It can't be SS7. Signalling system No. 7 (SS7) doesn't use tones at all; it's all digital. SS7 is carried separately from the audio channel, so it can't be heard by callers. SS7 wasn't in common use until later in the 1980s. Comparing spectrograms I made a spectrogram which combines all three signalling types on the same chart. The difference between DTMF and SS5 is subtle, but recognisable. CAS is obviously different. Let's feed the audio to some telecom hardware I injected the audio file into a timeslot of an E1 line, connected it to Corelatus' hardware and started an ss5_registersig_monitor. The input audio has a lot of noise in addition to the signalling, but these protocols are robust enough for the digital filters in the hardware to be able to decode and timestamp the dialled digits anyway. Now, we know that the number signalling we hear was 044 1831. The next step is to analyse the frequencies present at the start time for each tone. I re-analysed the audio file with SoX, which did an FFT on snippets of the audio to find the actual tone frequencies at the times there were tones, like this: sox input.wav -n trim 0.700 0.060 stat -freq The results are: Time Frequencies Interpretation 0—1200 ms 483 Hz dial tone 729 1105 + 1710 KP1 (start) 891 1304 + 1507 0 999 1306 + 703 4 1107 1306 + 701 4 1215 703 + 888 1 1269 902 + 1503 8 1377 902 + 1101 3 1566 701 + 900 1 1674 1501 + 1705 KF (stop) 3800 2418 Answer tone At this point, I'm certain the signalling is SS5. It uses the correct frequencies to transmit digits. It uses the correct digit timing. It obeys the SS5 rules for having KP1 before the digits and KF after the digits. It uses a tone close to 2400 Hz to indicate that the call was answered. I've also listed the dial tone at the beginning, and the 2400 Hz seizing tone at the end. SS5 also uses a 2600 Hz tone, which is infamous for its use in blue box phreaking (telephone fraud) in the 1980s. How was the film's audio made? My best guess is that, at the time the film was made, callers could hear the inter-exchange signalling during operator-assisted calls in the US. That would have allowed the sound engineer to record a real telephone in the US and accurately capture the feeling of a long-distance call. The number itself was probably made-up: it's too short and the area code doesn't seem valid. The audio was then cut and mixed to make the dial tone overlap the signalling. It sounds better that way and fits the scene's timing. Addendum, 18. December 2024: the audio also appears in 'Young Lust' It turns out that an extended version of the same phone call appears near the end of 'Young Lust', a track on the album 'The Wall'. Other engineers with actual experience of 1970s telephone networks have also analysed the signalling in an interesting article with a host of details and background I didn't know about, including the likely names of the people in the call. It's nice to know that I got the digit decoding right, we both concluded it was 044 1831. One surprise is that the number called is probably a shortened real number in London, rather than a completely fabricated one as I suspected earlier. Most likely, several digits between the '1' and the '8' are cut out. Keith Monahan's analysis noted a very ugly splice point there, whereas I only briefly wondered why the digit start times are fairly regular for all digits except that the '8' starts early and the final '1' starts late. PermalinkTags: GTH, telecom-signalling",
    "commentLink": "https://news.ycombinator.com/item?id=42485795",
    "commentBody": "Decoding the telephony signals in Pink Floyd's 'The Wall' (corelatus.com)117 points by matthiasl 3 hours agohidepastfavorite24 comments cassiepaper 2 hours agoFrom: James Guthrie interview > Another piece that worked better than expected was the telephone operator. Roger was keen to illustrate the personal disconnect of being on the road. We were in L.A. at Producer’s Workshop so I phoned my neighbour, Chris Fitzmorris in London. He had the keys to my flat and I asked him to go there and said that I would call him through an operator. “No matter how many times I call”, I said, “just pick up the phone, say ‘Hello’, let the operator speak and then hang up”. I placed a telephone in a soundproof area, got on to an extension phone and started recording to ¼” tape. It took a couple of operators – the first 2 were a bit abrupt, but the 3rd was perfect. I told her that I wanted to make a collect call to Mrs. Floyd. “Who’s calling?” she asked. “Mr. Floyd”, I replied. Chris’s timing was terrific, over and over he would hang up just at the right moment and she became genuinely concerned. “Is there supposed to be someone there besides your wife?” I was playing her along saying things like “No! I don’t know who that is!” “What’s going on?” and she would try the call again. Unwittingly, she was helping to tell the story. Afterwards I went through the ¼” and edited my voice out, just leaving her and Chris. I sometimes wonder if she ever heard herself on the record. Source: https://www.brain-damage.co.uk/other-related-interviews/jame... reply raywu 1 hour agoparent> Initially, I was shocked at how slowly everything moved! I was used to working really quickly when producing and engineering albums. Suddenly it was like the brakes were on and often it was difficult to get the momentum going. Eventually, I adapted to the Floyd pace. One of the great things about working with this band is that you are allowed time to be creative, to pursue an idea even if it takes some time. The Floyd had a production deal to make their records and the record label never heard anything until it was done. The record was made purely and only by the people in the studio. The creative freedom without commercial intervention - this is very cool. I can almost hear it in The Wall - how grand and elongated the songs are. What a great interview. Thank you for linking reply hinkley 31 minutes agorootparent“The Floyd”? Ironic that Have a Cigar was released four years before The Wall: The band is just fantastic, that is really what I think Oh, by the way, which one's Pink? reply mrandish 1 hour agoparentprevIt's cool to hear how that came together as an improvisation. It recalls a simpler time when a major album (or movie, TV show, etc) could just feature your neighbor and a random telephone operator without signing releases and clearing rights. It also gave Chris Fitzmorris (the neighbor) one of the greatest \"random cool thing that happened to me\" stories ever. reply wahnfrieden 8 minutes agorootparentMusicians still release samples without clearing them reply codazoda 9 minutes agoprevYoung Lust is the song where this operator is heard. Not being an audiophile, it took me some time to figure out the specific song. My brother had The Wall album, and I enjoyed it, but I never listened to it on my own. I went back and listened to it again for the context. I really enjoy music but I don't listen to it as often I'd like. I think part of the reason is that I have difficulty concentrating when there is audio in the background. Some of my software engineer co-workers can turn on music or a video while they work, but I'm more productive in silence. reply defaultcompany 1 hour agoprevOn the cassette tape version of The Wall I had if you flipped the cassette over during this phone call sequence it would end up being right in the middle of another song (can't remember which one) which has this recording playing as part of the background. I feel like it couldn't have been intentional but who knows. reply hinkley 24 minutes agoparentI’m betting subconscious but intentional. I’ve heard a couple artists talk about how they organize an album and there’s a vibe they’re going for but I didn’t get the impression any of them had it down to anything like a science. Dark Side of the Moon and the Wizard of Oz. It’s just two artists putting a story arc together by feel and getting the same shape. A bit birthday paradox, but a bit shared vibe. In the case of The Wall, I would bet a certain degree of symmetry was being reached for. Few artists want to leave or start an album on a sour note, but there will be songs in the middle that are. One of the things I miss from the pre-streaming era is that “nobody” listens to whole albums at a time anymore, and I find that a shame. I used to start humming the next song on the album when I would hear things on the radio. Makes it worse when they trim the intro or outro for radio play though. I prefer the album version of Wish You Were Here, for instance. reply joey_spaztard 24 minutes agoprevThis brings back memories of being a clueless script kid in the 1990s. I knew those tones as CCITT5 tones. In the days of blueboxing I had a 486 laptop that I acquired because the harddrive died and booted from floppys, a DOS program called 'The Little Operator' that played tones and a photocopy of a book about telephone switching. reply qwertox 24 minutes agoprev> https://telephoneworld.org/landline-telephone-history/pink-f... Doesn't the internet still have some pretty places? reply ChrisArchitect 1 hour agoprevRelated: Pink Floyd's 'The Wall': A Complete Analysis https://news.ycombinator.com/item?id=42402981 reply parpfish 1 hour agoprevA while back I tracked down the video clip from the show Gomer Pyle that was used for the “But there's somebody else that needs taking care of in Washington” background audio. Seeing that in its original context was jarring reply peutetre 3 hours agoprev> The number itself was probably made-up: it's too short and the area code doesn't seem valid. 44 is the country code for the UK. reply matthiasl 2 hours agoparentAuthor here. Yep, 44 is the UK country code. The problem I got stuck on is that the rest of the number, 1831, didn't make sense. I assumed the number was complete, since it had the right start and stop signalling (KP1/KF). It's not long enough to be a London telephone number, and, today, I think London numbers start with 020. The UK numbering plan has changed several times since 1980, but I couldn't find a time between 1980 and now where part of 1831 was a London number. Later on (in the addendum), it turns out that others took a look at the signal in the time domain and spotted a splice, i.e. digits are chopped out of the middle of the number, so the area code probably isn't there at all. It could be that the area code starts with 1, and then the phone number ends with 831. reply cf100clunk 1 hour agorootparentHiya, I see that you updated your blog in regards to this previous HN discussion: https://news.ycombinator.com/item?id=28858285 It references a Telephone World article called ''Pink Floyd's Young Lust – explained and demystified'' with great analysis of the sequences heard. reply matthiasl 1 hour agorootparentThe discussion is new to me. I started analysing the audio because someone sent me a link to the film (The Wall) on youtube and asked me about the signalling. Once I'd decoded the telephone number, I tried googling it, to see if someone else had already figured out what it was (a US local number? the number to a US operator? the number a US operator called to talk to a UK operator? the number a UK operator dialled to get a London number?), but nothing came up. There's quite a bit of good discussion about that in the comments here. A week or two later, I tried googling 'Pink Floyd Telephone Call', and found that the audio actually comes from the album, i.e. it's not just in the film, and a bit more information about how it was made, and put that in the addendum. reply graemep 2 hours agorootparentprev> I couldn't find a time between 1980 and now where part of 1831 was a London number. What about when outer London numbers started 081? Dialed from another country with the 00 international call prefix the number if could be a fragment starting from the second digit (i.e. 0044 1831xxxx)? the change form 01 was a big deal, and BT ran some very amusing ads playing on snobbishness about where in London one lived. reply robthebrew 2 hours agorootparentprevI remember London area codes being 01 (or 0441 from USA) followed by 6 digits, so maybe 3 are missing. It would be interesting to see if that landline is still available, but with 1000 possibilities (01ABC831) maybe a google search might be the way to go. reply wdfx 2 hours agorootparentThis is correct. In the 90's I recall London area prefix changed from 01 to first 071 (central) and 081 (greater) and then 0171 and 0181. Later still, those codes became 0207 and 0208. I still notice old shop signs up with the old prefixes. reply matthiasl 2 hours agorootparentprevInteresting. Normally, films use deliberately fictious numbers, e.g. in the US it's always xxx555xxxxxx. Wikipedia says the UK uses various area codes for the same thing, including 011x and 01x1. The Pink Floyd number is a bit unusual---it's not made up. According to a previous analysis, the call was the album's \"Chief Engineer James Guthrie who called his own London apartment\", with a neighbour answering the phone. Someone probably knows roughly where James Guthrie lived in 1979/1980 and what the area code there was. But I don't. reply abdullahkhalids 12 minutes agorootparentThere are some hits in this old phonebook [1], but one needs to pay to view the records. Maybe some one else can do this. [1] https://www.ancestry.co.uk/search/collections/1025/?name=Jam... reply Polizeiposaune 1 hour agorootparentprevPer https://en.wikipedia.org/wiki/555_(telephone_number)#Fiction..., the officially reserved block under the NANP has shrunk to XXX-555-01XX, but outside of XXX-555-1212 (directory assistance), and toll-free 800-555-XXXX there is little use of the range. reply aardvark179 2 hours agorootparentprevOkay. If the call was from within the UK then I think the 0441 code would have been Swansea, but that would have required 3 numbers being cut from the middle. reply codevark 2 hours agoprev [–] Odd. This and other PF audio figured in a dream last night. Then I came downstairs and saw this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Corelatus Blog delves into telecom hardware, specifically E1/T1 and SDH/SONET interfaces, and recently analyzed telephony signals in Pink Floyd's 'The Wall'.- The analysis identified SS5 signalling, a system used in the early 1980s, by examining audio spectrograms and comparing them to known standards like DTMF and CAS R2.- The study concluded that the audio likely originated from a real call, providing an authentic long-distance connection feel, with an extended version appearing in the song 'Young Lust'."
    ],
    "commentSummary": [
      "James Guthrie revealed that a telephone operator's voice was recorded for Pink Floyd's 'The Wall' to symbolize the isolation experienced while touring.",
      "The recording process for the album was marked by artistic freedom, free from commercial constraints, leading to its expansive and intricate tracks.",
      "The interview also highlighted the difficulty in deciphering the phone number used in the recording due to changes in London's area codes over the years."
    ],
    "points": 117,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1734868152
  },
  {
    "id": 42481659,
    "title": "Singlefile: A web extension to save a complete web page into a single HTML file",
    "originLink": "https://github.com/gildas-lormeau/SingleFile",
    "originBody": "SingleFile SingleFile is a Web Extension (and a CLI tool) compatible with Chrome, Firefox (Desktop and Mobile), Microsoft Edge, Safari, Vivaldi, Brave, Waterfox, Yandex browser, and Opera. It helps you to save a complete web page into a single HTML file. Table of Contents SingleFile Table of Contents Demo Install Getting started Additional notes FAQ Release notes Known Issues Troubleshooting unknown issues Command Line Interface (SingleFile CLI) Integration with user scripts File format comparison Projects using/compatible with SingleFile Privacy Policy Contributors Code derived from third party projects Icons License Demo Demo.SingleFile.mp4 Install SingleFile can be installed from the store of: Firefox: https://addons.mozilla.org/firefox/addon/single-file Firefox for Android: https://addons.mozilla.org/android/addon/single-file Chrome: https://chrome.google.com/extensions/detail/mpiodijhokgodhhofbcjdecpffjipkle Safari (macOS and iOS): https://apps.apple.com/us/app/singlefile-for-safari/id6444322545 Microsoft Edge: https://microsoftedge.microsoft.com/addons/detail/efnbkdcfmcmnhlkaijjjmhjjgladedno You can also download the zip file (https://github.com/gildas-lormeau/SingleFile/archive/master.zip) of the project and install it manually by unzipping it somewhere on your disk and following these instructions: Firefox: https://extensionworkshop.com/documentation/develop/temporary-installation-in-firefox Chrome and Microsoft Edge: https://github.com/gildas-lormeau/SingleFile-MV3 Safari: https://github.com/gildas-lormeau/SingleFile-Safari-Extension Getting started Click on the SingleFile button in the extension toolbar to save the page. You can click again on the button to cancel the action when processing a page. Additional notes Open the context menu by right-clicking the SingleFile button in the extension toolbar or on the webpage. It allows you to save: the current tab, the selected content, the selected frame. You can also process multiple tabs in one click and save: the selected tabs, the unpinned tabs, all the tabs. Select \"Annotate and save the page...\" in the context menu to: highlight text, add notes, remove content. The context menu also allows you to activate the auto-save of: the current tab, the unpinned tabs, all the tabs. With auto-save active, pages are automatically saved every time after being loaded (or before being unloaded if not). Right-click on the SingleFile button and select \"Manage extension\" (Firefox) / \"Options\" (Chrome) to open the options page. Enable the option \"Destination > save to Google Drive\" or \"Destination > upload to GitHub\" to upload pages to Google Drive or GitHub respectively. Enable the option \"Misc. > add proof of existence\" to prove the existence of saved pages by linking the SHA256 of the pages into the blockchain. You can use the customizable shortkey Ctrl+Shift+Y to save the current tab or the selected tabs. Go to about:addons and select \"Manage extension shortcuts\" in the cogwheel menu to change it in Firefox. Go to chrome://extensions/shortcuts to change it in Chrome. The default save folder is the download folder configured in your browser, cf. about:addons in Firefox and chrome://settings in Chrome. See the extension help in the options page for more detailed information about the options and technical notes. FAQ See https://github.com/gildas-lormeau/SingleFile/blob/master/faq.md Release notes See https://addons.mozilla.org/firefox/addon/single-file/versions/ Known Issues All browsers: For security reasons, you cannot save pages hosted on https://chrome.google.com, https://addons.mozilla.org and some other Mozilla domains. When this happens, 🛇 is displayed on top of the SingleFile icon. For security reasons, SingleFile is sometimes unable to save the image representation of canvas and snapshots of video elements. The last saved path cannot be remembered by default. To circumvent this limitation, disable the option \"Misc > save pages in background\". The following characters are replaced by their full-width equivalent symbols in file names: ~, +, ?, %, *, :, |, \", , . The replacement characters are respectively: ～, ＋, ？, ％, ＊, ：, ｜, ＂, ＜, ＞, ＼. Other invalid charcaters are replaced by _. This is done to maintain compatibility with various OSs and file systems. If you don't need that level of compatibility and know what you are doing, you can change the list of forbidden characters and the replacement characters in the Hidden options(https://github.com/gildas-lormeau/SingleFile/wiki/Hidden-options). Chromium-based browsers: You must enable the option \"Allow access to file URLs\" in the extension page to display the infobar when viewing a saved page, and to save or to annotate a page stored on the filesystem. If the file name of a saved page looks like \"56833935-156b-4d8c-a00f-19599c6513d3.html\", disable the option \"Misc > save pages in background\". Reinstalling the browser may also fix this issue. You can find more info about this bug here. Disabling the option \"File name > open the \"Save as\" dialog to confirm the file name\" will work if and only if the option \"Ask where to save each file before downloading\" is disabled in chrome://settings/downloads. Firefox: The \"File name > file name conflict resolution\" option does not work if set to \"prompt for a name\" Sometimes, SingleFile is unable to save the contents of sandboxed iframes because of this bug. When processing a page from the filesystem, external resources (e.g. images, stylesheets, fonts etc.) will not be embedded into the saved page. You can find more info about this bug here. This bug has been closed by Mozilla as \"WontFix\". But there is a simple workaround proposed here. Waterfox Classic User interface elements displayed in the page (progress bar, logs panel) won't be displayed unless dom.webcomponents.enabled is enabled in about:config. When opening pages saved with the option \"Images > group duplicate images together\" enabled, some duplicate images might not displayed. It is recommended to disable this option. Troubleshooting unknown issues Please follow these steps if you find an unknown issue: Save the page in incognito. If saving page in incognito did not fix the issue, reset SingleFile options. If resetting options did not fix the issue, restart the browser. If restarting the browser did not fix the issue, try to disable all other extensions to see if there is a conflict. If there is a conflict then try to determine against which extension(s). Please report the issue with a short description on how to reproduce it here: https://github.com/gildas-lormeau/SingleFile/issues. Command Line Interface (SingleFile CLI) You can save web pages to HTML from the command line interface. See here for more info: https://github.com/gildas-lormeau/single-file-cli. Integration with user scripts You can execute a user script just before (and after) SingleFile saves a page. For more info, see https://github.com/gildas-lormeau/SingleFile/wiki/How-to-execute-a-user-script-before-a-page-is-saved. File format comparisonHTML Self-extracting ZIP MHTML Webarchive (Safari) HTML+folder Pages are saved as a single file ✓ ✓ ✓ ✓HTML and styles are minified ✓ ✓Unused HTML and styles are removed from files ✓ ✓Binary resources are not encoded in base 64✓✓ ✓ Files are compressed✓Files can be viewed without installing any extension ✓ ✓¹ ✓² ✓³ ✓ Files can be viewed without running JavaScript ✓✓ ✓ ✓ Files can be unzipped to extract page resources✓ n/a Files contains the text of the page (plain or formatted) which can be indexed ✓ ✓⁴ ✓ ✓ ✓ Footnotes: ¹ When using the \"universal\" self-extracting file format ² Only in Chromium-based browsers, and Internet Explorer ³ Only in Safari ⁴ An option must be enabled in the extension Projects using/compatible with SingleFile ArchiveBox - Open-source self-hosted web archiving: https://github.com/ArchiveBox/ArchiveBox htmls-to-datasette - Tool to index HTML files into a Sqlite database: https://github.com/pjamar/htmls-to-datasette linkding - Bookmark manager that you can host yourself. It's designed be to be minimal, fast, and easy to set up using Docker: https://github.com/sissbruecker/linkding obsidian-html-plugin - Plugin for reading HTML pages in Obsidian: https://github.com/nuthrash/obsidian-html-plugin Petal Cite Web Importer - Browser extension to save PDFs and capture web pages in Petal Cite: https://github.com/ks-collab/cite-extension singlefile2trilium - Tool to save faithful copy of a web page as a Trilium note with SingleFile: https://github.com/nil0x42/singlefile2trilium SingleFileMac - Integration of SingleFile in a swift application using webkit: https://github.com/david-littlefield/SingleFileMac Trilium-SingleFile - An addon for Trilium Notes to easily import SingleFile html pages: https://github.com/rauenzi/Trilium-SingleFile Trilium Simple SingleFile Renderer - A plugin to add a new Template note to Trilium for render file created by SingleFile: https://github.com/xnohat/trilium-simple-singlefile-renderer web document - Browser extension for saving web documents locally, allowing you to access them offline and quickly search for webpage: https://github.com/wvit/web-document Zotero Connector - Browser extension for Zotero, a tool to help you collect, organize, cite, and share your research sources: https://github.com/zotero/zotero-connectors Privacy Policy See https://github.com/gildas-lormeau/SingleFile/blob/master/privacy.md Contributors Chinese translation done by yfdyh000 (https://github.com/yfdyh000), Liu8Can (https://github.com/Liu8Can), KrasnayaPloshchad (https://github.com/KrasnayaPloshchad), frostblazergit (https://github.com/frostblazergit), dnknn (https://github.com/dnknn), lqzhgood (https://github.com/lqzhgood) Traditional Chinese translation done by frostblazergit (https://github.com/frostblazergit), lqzhgood (https://github.com/lqzhgood) Dutch translation done by jooleer (https://github.com/jooleer) German translation done by womotroll (https://github.com/womotroll), bannmann (https://github.com/bannmann) Italian translation done by Fastbyte01 (https://github.com/Fastbyte01) Japanese translation done by Shitennouji(四天王寺) (https://github.com/Shitennouji) Polish translation done by xesarni (https://github.com/xesarni) Portuguese translation done by Blackspirits (https://github.com/Blackspirits) Portuguese-Brazilian translation done by @mezysinc, Blackspirits (https://github.com/Blackspirits) Russian translation done by rstp14, kramola-RU (https://github.com/kramola-RU), solokot (https://github.com/solokot), TotalCaesar659 (https://github.com/TotalCaesar659) Spanish translation done by strel (https://github.com/strel) Turkish translation done by hbaklan943 (https://github.com/hbaklan943) Ukrainian translation done by perdolka (https://github.com/perdolka), gildas-lormeau Code derived from third party projects csstree: https://github.com/csstree/csstree postcss-media-query-parser: https://github.com/dryoma/postcss-media-query-parser postcss-selector-parser: https://github.com/postcss/postcss-selector-parser UglifyCSS: https://github.com/fmarcia/UglifyCSS parse-srcset: https://github.com/albell/parse-srcset parse-css-font: https://github.com/jedmao/parse-css-font Readability: https://github.com/mozilla/readability whatwg-mimetype: https://github.com/jsdom/whatwg-mimetype Icons Icon made by Kiranshastry from Flaticon is licensed by CC 3.0 BY License SingleFile is licensed under AGPL. Code derived from third-party projects is licensed under MIT. Please contact me at gildas.lormeaugmail.com if you are interested in licensing the SingleFile code for a commercial service or product. Suggestions are welcome :)",
    "commentLink": "https://news.ycombinator.com/item?id=42481659",
    "commentBody": "Singlefile: A web extension to save a complete web page into a single HTML file (github.com/gildas-lormeau)112 points by throw0101a 23 hours agohidepastfavorite44 comments Dwedit 21 hours agoI was working on a trick to save files in a more compact way by taking advantage of UTF-16 encoded HTML files. I have an example HTML files on my website here: https://www.dwedit.org/files/test.html (Tip: type \"codeText\" into the JS console to see the second-stage loader. First stage-loader is in clear text in the source) It's storing a 64479 byte JPEG into 64680 bytes of UTF-16 text, for around 0.2985% expansion. The decoder isn't size-optimized right now (there's even comments in the code), so it takes about 10KB more. This is doing two things: First stage is a basic kind of UTF-16 packing, where you use a simple Javascript string literal, and anything that needs escaping gets escaped. This keeps most 8-bit data the same size, but byte pairs that turn into a forbidden string character get escaped. Then you split your result string into bytes. Forbidden characters (requiring escaping) in a Javascript string: 0x00, 0x0A, 0x0D, 0x22, 0x5C, 0x2028, 0x2029, 0xD800-0xDFFF --- Second stage involves using very large integers to pack 335 bits into every 336 actual bits. This avoids all escaping completely in the JS string, as you can avoid the forbidden characters. During decoding, my code is using a handmade 48-bit BigInt (actually stores 7 48-bit words for 336 total bits), allowing support on web browsers that predate native bigints, and it runs faster too. Let me know if I should make an article about this. (I also made a WASM version of the decoder to see if it would run any faster, but it didn't.) reply gildas 21 hours agoparentI took another approach in SingleFile by offering a way to save pages as self-extracting pages (i.e. ZIP/HTML polyglot files), see [1] for more info. [1] https://github.com/gildas-lormeau/Polyglot-HTML-ZIP-PNG reply Dwedit 20 hours agorootparentReally cool stuff there, how did you read the bytes back? Normally you get a CORS error if you try to use a network request to read back yourself. reply gildas 20 hours agorootparentThe saved page is encoded in windows-1252. It includes \"consolidation data\" to read the ZIP data as text from the DOM and recover the replacements of \\r and \\r occurrences (this is the only data loss and it represents approx. 1% of ZIP data), see the links below for more info. https://gildas-lormeau.github.io/Polyglot-HTML-ZIP-PNG/en-EN... https://gildas-lormeau.github.io/Polyglot-HTML-ZIP-PNG/en-EN... https://gildas-lormeau.github.io/Polyglot-HTML-ZIP-PNG/en-EN... reply jclarkcom 20 hours agoparentprevNeat idea, how does it compare to just using base64 when gzip is in the middle? reply Dwedit 17 hours agorootparentYou won't be using gzip on compressed binary files (images, videos, audio, etc). reply KolenCh 17 hours agorootparentBut they are talking about gzipping base64 encoded data though. reply mdaniel 21 hours agoprevseems there was some robust discussion on the 2022 submission https://news.ycombinator.com/item?id=30527999 but I didn't dig into it to discover \"but, why?\" as compared to the \"save\" built into the browsers having said that, this here is an instant \"nope\" for me https://github.com/gildas-lormeau/SingleFile/blob/master/faq... > By default, SingleFile removes scripts because they can alter the rendering and there is no guarantee they will work offline. so, not complete; got it reply gildas 21 hours agoparentSee https://github.com/gildas-lormeau/SingleFile/blob/master/faq... reply freehorse 21 hours agoparentprevThe very next sentence says > However, you can save them by unchecking the option \"Network > blocked resources > scripts\" [..] reply mdaniel 21 hours agorootparentYeah, understood, there seems to be a bunch of \"yes, well, monkey with this thing\" options, but if the default behavior is not to preserve all the resources because of some pearl-clutching paranoia or whatever, then it's not a complete web page now is it? reply varelaseb 21 hours agorootparentThis is the worst gotcha I've ever seen reply lynndotpy 18 hours agorootparentprevIt's not paranoia, it's a feature that I consider to be useful and desirable. When I want to download the JavaScript, I use the built-in \"save\" feature. When I don't, I use SingleFile. reply wakawaka28 20 hours agorootparentprevIt depends what you want... Lots of JS will not work offline and I don't think a plugin can know what will work or not. If the page is dynamically populated from an API, saving the page to rely on JS later could be basically wrong. reply maxloh 21 hours agoparentprevAn alternative approach would be saving all outbound requests as a HAR file using your browser's DevTools. This captures fetched API responses as well. HAR is a JSON-based format, making it straightforward to inspect. Unfortunately, there aren't any tools currently for opening a HAR archive directly as a webpage. Perhaps someone could develop one using Electron? (Hold your downvotes – we actually need a full browser environment to render HTML and execute JavaScript. Isn't it?) reply stuffoverflow 17 minutes agorootparentI'm pretty sure I've used some tool in the past to convert a HAR in to WARC which can then be browsed with https://replayweb.page/ reply smittywerben 10 hours agorootparentprevRecording all of your SSL-decrypted network logs and reconstructing them later is one step further. I'm sure there's a chain of custody issue not much further. But what I like about SingleFile is that printing the SingleFile page tends to print to PDF better than an actual dynamic page (annoying modals etc show up in the print that are not normally visible). I have a python service running that organizes my SingleFile html files into folders by each domain and stored my google drive. It's like a confused mix of wget's -x directories and a postscript printer so you can avoid chrome's print to skia pdf pipeline until a brighter day. reply maxloh 21 hours agoprevChromium-based browsers (Chrome, Edge, Brave, etc.) already have great MHTML support. Just right click on a page > \"Save as...\", and select \"Webpage, Single File\" in the dialog. reply freehorse 21 hours agoparentBecause having just an html file can just be simpler and html files are more ubiquitously rendered. Personally having a single html file with base64 encoded images embedded into it serves 99.9% of what I usually want to do when wanting to have a webpage saved offline. Such a file can be eg be put in my dropbox and rendered in dropbox web interface which makes it easy to share with others. I have actually made a script that looks for image tags in an html file on my computer, looks in the local directory location for them and base64 embeds them in the html file itself, so this seems to simplify the process. reply hahn-kev 14 hours agorootparentIf that's all you want then couldn't you just print to PDF? reply freehorse 8 hours agorootparentBecause I do not want a page structure in the document. Images are arranged better in an html file because a pdf would have to arrange things into pages and thus create undesirable gaps or put images in weird ways. Plus html is just a text file I can further edit if I want. reply eviks 13 hours agorootparentprevIt's not responsive anymore reply thekevan 21 hours agoparentprevI was just going to ask why this was better than just saving an MHTML file in the browser. reply maxloh 21 hours agorootparentIt seems that Firefox doesn't support saving a page as MHTML yet. Maybe that's the reason. https://connect.mozilla.org/t5/ideas/add-support-of-mhtml-fo... reply xeonmc 12 hours agorootparentAlso it doesn’t quite fully save all of the assets, like fonts if it were referenced in CSS files reply gildas 21 hours agorootparentprevBecause the MHTML format produced by Chromium can almost be considered as proprietary. reply maxloh 21 hours agorootparentThat's not correct. MHTML has been defined as internet standard since 1999. MHTML has been an internet standard since 1999, with support dating back to IE 5.0. https://datatracker.ietf.org/doc/html/rfc2557 reply gildas 21 hours agorootparentI'm actually referring to the \"set of modifications\" described here [1] and the fact that only Chromium can open (these) MHTML files today. [1] https://docs.google.com/document/d/1FvmYUC0S0BkdkR7wZsg0hLdK... reply maxloh 20 hours agorootparentGood to know that! Are you meaning that these modifications cause the generated MHTML files to become non-standards-compliant? I wouldn't consider them proprietary though. At least the primary software used to open these files, Chromium, is open-source software. reply gildas 20 hours agorootparentI agree that it's debatable, which is why I used the word “almost” ;). The fact remains that this info isn't in RFCs and is hard to find on the web. reply n8henrie 22 hours agoprevIf anyone wants to help me update the nix package for the CLI, I'd appreciate it. The whole Deno thing is foreign territory for me. https://github.com/NixOS/nixpkgs/blob/81629effd3f7e0cea0c1cf... reply mdaniel 21 hours agoparentYou linked to an invocation of grep without any further context. What, specifically, do you need help with? reply n8henrie 15 hours agorootparent> You linked to an invocation of grep without any further context. I linked to an invocation of grep in the package in question. Sorry, on mobile so I just copied the first link to the package from my search results. > What, specifically, do you need help with? As I mentioned, I don't know how deno works. To elaborate, now that single-file-cli is using deno, I don't know how to package something that uses deno for nix. reply dannyobrien 21 hours agoprevIf today's Hacker News trend is \"ways to archive web pages for permanent storage\", let me also point folks to https://webrecorder.net/archivewebpage/ and the WebRecorder suite of tools. reply dang 21 hours agoparentWhat other thread(s) from today are you thinking of? (maybe https://news.ycombinator.com/item?id=42441609?) We try to avoid repetition on the frontpage—people tend to post related/follow-up links as submissions (bad) instead of in the comments (good, as you did here!) https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply crgk 20 hours agorootparentFWIW, the discussion you linked came to my mind when I saw this one. reply glenstein 21 hours agoparentprevThanks for this recommendation, this is exactly a kind of tool that I think I had hoped might exist out there somewhere. I don't suppose there's a Firefox version? reply dang 21 hours agoprevRelated. Others? How SingleFile Transformed My Obsidian Workflow - https://news.ycombinator.com/item?id=39147615 - Jan 2024 (1 comment) New Feature: Self-Extracting Zip Files Added to SingleFile - https://news.ycombinator.com/item?id=37791838 - Oct 2023 (1 comment) SingleFile is finally available on Safari (macOS/iOS) - https://news.ycombinator.com/item?id=33643192 - Nov 2022 (50 comments) SingleFile version compatible with Manifest V3 - https://news.ycombinator.com/item?id=33063619 - Oct 2022 (274 comments) SingleFile: Save a complete web page into a single HTML file - https://news.ycombinator.com/item?id=30527999 - March 2022 (240 comments) SingleFile Lite, new version of SingleFile compatible with Manifest V3 - https://news.ycombinator.com/item?id=29331038 - Nov 2021 (2 comments) Save web pages as self-extracting HTML/ZIP files from the CLI - https://news.ycombinator.com/item?id=25218947 - Nov 2020 (3 comments) Browser extension and CLI tool to save a complete webpage as single HTML file - https://news.ycombinator.com/item?id=22449602 - Feb 2020 (1 comment) Store the proof of a webpage saved with SingleFile in Bitcoin - https://news.ycombinator.com/item?id=21970779 - Jan 2020 (77 comments) SingleFileZ, a web extension for saving pages as HTML/ZIP hybrid files - https://news.ycombinator.com/item?id=21426056 - Nov 2019 (36 comments) SingleFileZ – Save a web page in a HTML file which is also a zip file - https://news.ycombinator.com/item?id=19933196 - May 2019 (18 comments) SingleFile can now save a web page from the command line - https://news.ycombinator.com/item?id=18974357 - Jan 2019 (2 comments) SingleFile 1.0 is out - https://news.ycombinator.com/item?id=17457943 - July 2018 (1 comment) reply jftuga 19 hours agoprevI use this extension to locally save my Claude chats. It does a great job preserving code blocks. reply betaby 19 hours agoprevFor command line I can recommend `monolith` https://github.com/Y2Z/monolith reply create-username 21 hours agoprevStill no way to make a backup of xml books or BlinkLearning websites reply xrd 19 hours agoprevJust saying, my static blog tool does this: https://www.npmjs.com/package/svekyll-cli https://extrastatic.dev/svekyll/svekyll-cli There is something really exciting about creating a communication medium that is in a single file. It feels like you can then easily publish to any web server, but also to things like IPFS. reply pentagrama 20 hours agoprev [–] Tried it, and it worked perfectly. Neat! But should I uninstall it? I noticed that this extension (at least the Firefox version) requires \"Access your data for all websites\" and there's no option to grant access only when needed, clinking on the extension button. I’m not comfortable with extensions having access to all my browsing data unless it’s absolutely necessary (e.g., uBlock Origin). reply gildas 20 hours agoparent [–] SingleFile respects your privacy, you can find the privacy policy here [1]. [1] https://github.com/gildas-lormeau/SingleFile/blob/master/pri... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SingleFile is a versatile web extension and command-line tool that enables users to save entire web pages as single HTML files, compatible with major browsers such as Chrome, Firefox, Edge, and Safari.",
      "It offers features like saving selected content, multiple tabs, annotations, auto-save, and integration with Google Drive and GitHub, although it has some limitations with certain domains and file name characters.",
      "The tool is open-source, licensed under AGPL, and is utilized in projects like ArchiveBox and Zotero Connector, with contributions from various translators."
    ],
    "commentSummary": [
      "SingleFile is a web extension designed to save entire web pages as single HTML files, offering a compact saving method using UTF-16 encoding.- The extension provides an alternative by saving pages as self-extracting ZIP/HTML files, addressing limitations of standard browser save functions and enhancing PDF printing.- Privacy concerns were discussed, but the developer confirmed that SingleFile respects user privacy, making it a valuable tool for offline access and sharing."
    ],
    "points": 112,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1734809595
  }
]
