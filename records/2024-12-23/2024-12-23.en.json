[
  {
    "id": 42490453,
    "title": "'United Healthcare' using DMCA against Luigi Mangione images",
    "originLink": "https://abovethelaw.com/2024/12/united-healthcare-using-dmca-against-luigi-mangione-images-which-is-bizarre-wildly-inappropriate/",
    "originBody": "In-House Counsel 'United Healthcare' Using DMCA Against Luigi Mangione Images Which Is Bizarre & Wildly Inappropriate Because this isn't how copyright law works. By Joe Patrice onDecember 19, 2024 at 4:02 PMDecember 20, 2024 at 10:27 AM 186 Shares We KNOW Getty owns this picture though that might not stop UHC. (Photo by Jeff Swensen/Getty Images) Someone purporting to be United Healthcare is filing DMCA requests to scrub the internet of artists’ depictions of the surveillance video of Luigi smiling, parody merchandise of “Deny, Defend, Depose,” and other merchandise showing the alleged shooter. If it is really the health insurer filing these notices, it wildly oversteps any legal rights the company might have, but if there’s any company willing to preemptively breach the law and dare the other side to spend their limited resources trying to protect their rights it would be a health insurer. Sponsored How The New Lexis+ AI App Empowers Lawyers On The Go Subscribers get these new capabilities directly on their phones and tablets. From Above the Law 404 Media reports that a number of DMCA takedown requests from “United Healthcare” have hit artists depicting the newsworthy event. The publication reached out to the company to confirm that it’s behind the filings but didn’t hear back. For those keeping score at home, the DMCA provides that a copyright owner must inform an internet entity that their intellectual property is being abused and the service provider then has an opportunity to expeditiously remove the material and avoid possible liability. That said, the DMCA only provides a shield against legal action and if the copyright claim is — to use the precise legal term — “utter bullshit,” they don’t have to comply. Unfortunately, hosts routinely err on the side of caution and remove content when there’s zero cognizable claim. If this is someone acting on behalf of United Healthcare, what an irony to lodge a falsified claim without it getting denied. From 404: Sponsored Sponsored How The New Lexis+ AI App Empowers Lawyers On The Go Subscribers get these new capabilities directly on their phones and tablets. From Above the Law Sponsored Happy Lawyers, Better Results The Key To Thriving In Tough Times How happiness, a positive workplace, and alignment with the right role can transform not just your own career, but also the success of your clients… From Adam Losey Sponsored AI Presents Both Opportunities And Risks For Lawyers. Are You Prepared? Get up to speed on AI’s rapid growth, risks, and potential — and take your knowledge of artificial intelligence to the next level. From Practising Law Institute Sponsored Happy Lawyers, Better Results The Key To Thriving In Tough Times How happiness, a positive workplace, and alignment with the right role can transform not just your own career, but also the success of your clients… From Adam Losey An entity claiming to be United Healthcare is sending bogus copyright claims to internet platforms to get Luigi Mangione fan art taken off the internet, according to the print-on-demand merch retailer TeePublic. An independent journalist was hit with a copyright takedown demand over an image of Luigi Mangione and his family she posted on Bluesky, and other DMCA takedown requests posted to an open database and viewed by 404 Media show copyright claims trying to get “Deny, Defend, Depose” and Luigi Mangione-related merch taken off the internet, though it is unclear who is filing them. It’s a bizarre and grimly ironic turn if United Healthcare wanted to own the words “Deny, Defend, Depose.” The takedown request for that piece objects to the artist styling the “D” with elements of the United Healthcare logo… which would be the very definition of protected parody. The 404 story has images if you want to see what these all look like. Beyond tying itself to the gunman’s catchphrase, the idea of UHC trying to own any and all fixations of the alleged shooter’s likeness would be a wild leap. An artists’ depiction of Mangione could only belong to the artist (Mangione might be able to assert some rights to his likeness — a dubious claim under the circumstances and in light of the First Amendment — but in no case would UHC have such a claim). “What is the circumstance under which United Healthcare might come to own the copyright to a watercolor painting of the guy who assassinated their CEO?” tech rights expert and science fiction author Cory Doctorow told 404 Media in a phone call. “It’s just like, it’s hard to imagine” a lawyer thinking that, he added, saying that it’s an example of “copyfraud.” It is illegal to file DMCA notices if you don’t own the copyright (or at least have a good faith belief that you do). The idea that UHC now owns every depiction of the guy accused of killing their employee is laughably frivolous and one hopes that its legal department understands this and these requests are coming from a third party troll impersonating the carrier. Sponsored Sponsored Law Firm Business Development Is More Than Relationship Building Look forward to client outreach with InterAction+™. From LexisNexis Sponsored Curbing Client And Talent Loss With Productivity Tech Law firms must leverage technology to curb client attrition and talent loss, enhancing efficiency and aligning with evolving expectations for lasting success. From Above The Law An independent journalist posting a photo of Mangione with his family also received a DMCA request — from a lawyer claiming to represent a family member holding the copyright — even though the image was “originally posted on the campaign website of Maryland assemblymember Nino Mangione.” That site apparently deleted the image and turned around to threaten anyone using it now which is… not how fair use works. But at least this request can claim they have a “good faith” claim, though the system probably shouldn’t reward people for trying to retroactively claim rights after they try to memory-hole their internet history. But the disturbing thread running through all these requests is how easy it’s become for copyright trolls to leverage the DMCA to intimidate providers into accepting facially invalid requests. The statute has given way to a sort of asymmetrical warfare over IP where bad faith actors can pepper sites with ownership claims and trust that their targets will back down rather than deal with the litigation risk. As 404’s coverage notes, this doesn’t bode well in a country about to inaugurate an administration openly encouraging retribution against journalists in an effort to silence criticism. This sure looks like a company (and a local politician) trying to use copyright law as a “square peg-round hole” solution to erasing any humanizing or sympathetic depiction of Mangione. And unfortunately, everyone seems to be obeying in advance. Copyright Abuse Is Getting Luigi Mangione Merch Removed From the Internet [404 Media] Joe Patrice is a senior editor at Above the Law and co-host of Thinking Like A Lawyer. Feel free to email any tips, questions, or comments. Follow him on Twitter or Bluesky if you’re interested in law, politics, and a healthy dose of college sports news. Joe also serves as a Managing Director at RPN Executive Search. Topics Copyright, DMCA, In-House Counsel, Intellectual Property, Technology Above the Law daily newsletter Sign up and get the latest news in your inbox. Subscribe Now We will never sell or share your information without your consent. See our privacy policy. Recommended Best Of 2024: Ron DeSantis Claims Victory Over Disney And All He Had To Do Was Give Disney Everything They Wanted Remember when Ron DeSantis 'won\" against Disney? By Joe Patrice More From Above the Law Biglaw Biglaw Firm Celebrates Holidays By 'Encouraging' Associates To Burn The Midnight Oil Once you burn out to hit the special bonus, you can use the extra cash for therapy! By Chris Williams Contests Above The Law’s 2024 Lawyer Of The Year Contest: The Finalists! From distinguished to despicable, who should be Above the Law’s Lawyer of the Year for 2024? By Staci Zaretsky Finance, Health Care / Medicine Why Walgreens Is Reportedly Considering A Private Equity Buyer Walgreens is reportedly exploring selling itself to private equity firm Sycamore Partners. It might be the right decision for the company, some experts say. By MARISSA PLESCIA - MEDCITY NEWS Morning Docket Morning Docket: 12.23.24 * You’re not imagining it… a lot of firms still haven’t announced bonuses. [National Law Journal] * Another federal judge in Texas recuses from Elon’s “it’s illegal for advertisers not to give me money” lawsuit. [Reuters] * ABA Prez argues that electronic messages from lawyers to inmates should be covered by attorney-client privilege. [ABA Journal] * ACLU says prisons are holding people past their release date. [NY Times] * Why hire one lateral partner when you can hire the whole group? [American Lawyer] * Biden will exceed Trump’s record for judicial appointments by one after trading some key circuit nominations for a path to confirm more district judges. [Law360] * But his collection of confirmations were more diverse than ever before. [Bloomberg Law News] By Above the Law From the Above the Law Network Holove Damage Control Is Salt In The Wound — See Also Source: Lawline CLE A Law Firm Checklist For Successful Client Portals Source: Thomson Reuters A Law Firm Checklist For Successful Transaction Management Source: Thomson Reuters Document Automation For Law Firms: The Definitive Guide Source: THOMSON REUTERS The Secrets Of Small Firm Success Source: Above The Law 5 Things To Consider Before Hiring A Legal Marketing Partner Source: THOMSON REUTERS How You Can Use Tech To Strengthen Client Ties Source: Thomson Reuters And Above The Law Differentiating Your Solo Firm In A Crowded Marketplace Source: Thomson Reuters & Above the Law Sponsored AI Presents Both Opportunities And Risks For Lawyers. Are You Prepared? Get up to speed on AI’s rapid growth, risks, and potential — and take your knowledge of artificial intelligence to the next level. From Practising Law Institute Recommended See Also HoLove Damage Control Is Salt In The Wound -- See Also Overwhelmingly Profitable Firm Brainstorms On Way To Value Associate’s Hard Work: Maybe they could…I don’t know…give out special bonuses? Top 50 Firm Manages To Pay Out More Than HoLove: And Morrison & Foerster didn’t even have to do a PR run to fix the outrage! Biglaw Firm’s Compensation Goes Above Market: Good on Irell & Manella! Match A Little Now, Match A Little Later: Firm matches market. You’ll get it eventually. Jury Nullification Is A Hot Topic: Hopeful jury members for Luigi Mangione are brushing up on legal concepts. By Chris Williams Government This Attorney Reportedly Willing To Restrict Access To Abortion Pill And he's been nominated for a key role in Donald Trump's administration. By Kathryn Rubino Sponsored Law Firm Business Development Is More Than Relationship Building Look forward to client outreach with InterAction+™. From LexisNexis Recent Jobs Insurance Recovery Counsel Location North posted by Kinney Recruiting LLC Junior to Mid-Level Associate – Intellectual Property (Engineering) Location Boston posted by Kinney Recruiting LLC Energy Infrastructure Associate Location Houston posted by Kinney Recruiting LLC View All Sponsored Curbing Client And Talent Loss With Productivity Tech Law firms must leverage technology to curb client attrition and talent loss, enhancing efficiency and aligning with evolving expectations for lasting success. From Above The Law",
    "commentLink": "https://news.ycombinator.com/item?id=42490453",
    "commentBody": "'United Healthcare' using DMCA against Luigi Mangione images (abovethelaw.com)351 points by haunter 18 hours agohidepastfavorite209 comments kyledrake 18 hours agoIt's also quite possible that someone falsely claiming to be UHC is doing this. This is a major loophole with the DMCA, there's very little vetting of takedown notices. reply Dwedit 17 hours agoparentIf you misrepresent that you are authorized by the company making the takedown request, that's literally the only thing that the DMCA considers to be perjury. So weirdly enough: * Not owning the copyrighted work and claiming you own it = Okay * The work you're taking down not even being the claimed copyrighted work = Okay * Saying you're authorized by the company to file the notice, and you're not = PERJURY reply Underpass9041 17 hours agorootparentAs someone receiving a notice you have no idea if it's legitimate or from a AOL account someone made 15 minutes ago, the threat of it being perjury doesn't work when there's no authentication whatsoever of who sent it. Unless the DMCA is signed by Zaphod Beeblebrox the receiver has no choice but to assume it's legitimate. reply SteveNuts 16 hours agorootparentWhat happens if you ignore it? reply gruez 16 hours agorootparentYou lose safe harbor protections, which means if the notice was legitimate, you could be on the hook for copyright infringement as well. reply bombcar 16 hours agorootparentprevYou can respond and refuse, but you have to authenticately dox yourself to do so, and you assume liabilities. reply eurleif 15 hours agorootparentYou're conflating two things: a DMCA counter-notice by an alleged infringer, and the refusal of an intermediary to honor a DMCA takedown notice. Your statement is a mixture of claims that are true about one or the other of those separate things. DMCA counter-notice: * Sent by an alleged infringer (in this case, that would be the artist) to an intermediary (in this case, TeePublic) asserting that a takedown notice the intermediary received was invalid. * Requires the alleged infringer to provide information about themself (\"authentically dox\"). * Can enable the intermediary to restore access to the material without losing their liability shield. * Doesn't affect the alleged infringer's copyright infringement liability in any direction (though as with takedown notices, there's the potential for perjury). If the alleged infringer committed copyright infringement, they were already liable for it, and remain so; if they didn't, then they never were liable and still aren't. Intermediary refusal: * Simply a lack of action on a takedown notice by the intermediary to whom it was sent. * Doesn't require anyone to dox themselves, or to do anything in particular. The intermediary can throw the takedown notice in the garbage and go about their day. * Removes the intermediary's liability shield; the intermediary can potentially be liable for infringement, when they would otherwise would have been immune. reply more_corn 12 hours agorootparentprevProvider has to take it down. The vendor making the shirt can simply file a dmca counterclaim (basically stating that the dmca claim is invalid) Then the provider puts the content back up and the courts decide. reply amanaplanacanal 9 hours agorootparentOne side note: the provider is not required to put the content back up, it is at their discretion. reply LocalH 5 hours agorootparentI don't believe that's true. I think they must make the material available after a proper counter-notification, or they lose their liability shield as per the exception here. IANAL, just reading the text of the law at face value. (g) REPLACEMENT OF REMOVED OR DISABLED MATERIAL AND LIMITATION ON OTHER LIABILITY.— (1) NO LIABILITY FOR TAKING DOWN GENERALLY.—Subject to paragraph (2), a service provider shall not be liable to any person for any claim based on the service provider’s good faith disabling of access to, or removal of, material or activity claimed to be infringing or based on facts or circumstances from which infringing activity is apparent, regardless of whether the material or activity is ultimately determined to be infringing. (2) EXCEPTION.—Paragraph (1) shall not apply with respect to material residing at the direction of a subscriber of the service provider on a system or network controlled or operated by or for the service provider that is removed, or to which access is disabled by the service provider, pursuant to a notice provided under subsection (c)(1)(C), unless the service provider— (A) takes reasonable steps promptly to notify the subscriber that it has removed or disabled access to the material; Applicability. PUBLIC LAW 105–304—OCT. 28, 1998 112 STAT. 2883 (B) upon receipt of a counter notification described in paragraph (3), promptly provides the person who provided the notification under subsection (c)(1)(C) with a copy of the counter notification, and informs that person that it will replace the removed material or cease disabling access to it in 10 business days; and (C) replaces the removed material and ceases disabling access to it not less than 10, nor more than 14, business days following receipt of the counter notice, unless its designated agent first receives notice from the person who submitted the notification under subsection (c)(1)(C) that such person has filed an action seeking a court order to restrain the subscriber from engaging in infringing activity relating to the material on the service provider’s system or network. reply eurleif 1 hour agorootparentThey lose their liability shield for the act of taking the content down, but that liability shield is of limited value to begin with because they'd typically be well within their rights to take any content down for any reason (or no reason). reply dcrazy 17 hours agorootparentprevIt’s not that crazy when you consider that someone might be wrong about whether their work is even copyrightable, but they can’t realistically be mistaken about whether they are acting as someone’s authorized agent. That’s the kind of relationship that lawyers know very well how to make explicit. reply mithametacs 16 hours agorootparentBut they should also have foreseen the routes for abuse. That's also their job. The DMCA is a disaster. reply matheusmoreira 14 hours agorootparentThese laws are bought and paid for by corporations of the trillion dollar copyright industry. They hire expensive lobbyists to get these laws passed. It's no surprise that the laws only foresee the abuse that could hurt their bottom lines. The better question is: why have the people's representatives failed to protect their interests? Why didn't they foresee the abuses that hurt us all? reply jgerrish 5 hours agorootparentIt's unfortunate that the UnitedHealth Group death has fanned the flames of this hatred of corporations. Artificial Intelligence is going to change how we work. Many top software engineers on Hacker News may be ok for a while. Although everyone could use help. But already many artists and junior engineers feel pressure. Lack of copyright protection can make that even worse for many. These are allies and people who we mentor. Just brainstorming, Social Purpose Corporations could provide innovative co-op opportunities for artists and engineers. That's just one example. I don't see the management class failing to forsee the financial reality in the global workplace. But I do worry about dependents facing division and hate. Bully that, right? I can't fucking forsee every possibility, traps and all, though. Maybe not social purpose corporations but just hyper-incubators that enable a million small indy corps. Man, I'm imagining Jar Jar binks manning the hyperspace lychgates across from the content generators. Sorry, stupid digression. I'm sorry for wasting cycles on that. reply paulryanrogers 5 hours agorootparentCorporations have earned all the hate they get, UHC even more so. Whether AI decimates the likelihoods of everyone is only more likely with unchecked corpo power in control of everything. reply LocalH 4 hours agorootparentprev\"AI\" is like anything else. Big capital will weaponize it to further entrench themselves and further disadvantage smaller businesses and individuals, in the pursuit of increasing profits and growth. reply matheusmoreira 46 minutes agorootparentprevI hate copyright and would rather see it abolished. AI is just the latest iteration of endless corporate abuses. They think copyright exists to keep the likes of us in line while they get to do whatever they want. If we infringe copyright, they compare us to raping and pillaging high seas pirates but then they turn around and say it's OK to infringe copyright on a massive scale so long as they launder it via AI first. I want intellectual property gone so that we can do the same. I want them to be forced to open their weights too. reply JumpCrisscross 13 hours agorootparentprev> why have the people's representatives failed to protect their interests? Nobody cares about copyright. There is a vocal minority of us who do. But we're only slightly more useful on it than the privacy advocates, both sharing a good chunk of people who are lazy or nihlistic about the political process to the point of being politically irrelevant. As a result, a representative who brings up copyright reform gets like one call in support and zero net new votes. Meanwhile, they get powerful and patient adversaries from the Big Tech to the record companies and their billionaire artists. reply mithametacs 13 hours agorootparentprevThe Senate is quite deliberately there to protect the interests of the wealthy and powerful. reply SpicyLemonZest 15 hours agorootparentprevThey did foresee the routes for abuse and set up a formal counterclaim process. Neither the source article nor the original reporting really explain why the targets of these takedown orders didn't want to file a counterclaim - in some cases there can be privacy concerns, but the T-shirt artist at least has already publicly identified herself. reply thayne 15 hours agorootparentThe process is extremely assymmetric, the counterclaim process has much higher requirements than a takedown notice. Like having to actually provide evidence you aren't infringing, provide your real identity, and often convince an AI that your content shouldn't be taken down or least get it seen by a real person. And if you succeed, you just get your content back up. No compensation for any damage caused by your content being down for a while. reply dcrazy 14 hours agorootparentMaybe you’re describing YouTube’s DMCA process? The actual law doesn’t specify any of that [1]: > If the user believes that the material was removed as a result of mistake or misidentification of the material, the user may submit a counter-notice requesting the reinstatement of the material. To be effective, a counter-notice must contain substantially the following information: > (i) a physical or electronic signature of the user; > (ii) identification of the material that has been removed or to which access has been disabled and the location at which the material appeared before it was removed or access to it was disabled; > (iii) a statement under penalty of perjury that the user has a good faith belief that the material was removed or disabled as a result of mistake or misidentification of the material to be removed or disabled; > (iv) the user’s name, address, and telephone number, and a statement that the subscriber consents to the jurisdiction of Federal District Court for the judicial district in which the address is located, or if the subscriber’s address is outside of the United States, for any judicial district in which the service provider may be found, and that the subscriber will accept service of process from the person who provided notification under subsection (c)(1)(C) or an agent of such person. 1: https://www.copyright.gov/512/ reply thayne 13 hours agorootparent(iv) sure sounds like giving them your true identity to me. You have to give them information to sue you (or do other nefarious things), but they don't have to provide you with information that say, you could use to sue them for perjury or try to recover damages from the time it was taken down. reply dcrazy 3 hours agorootparentAgain, you seem to be conflating YouTube’s DMCA process (which is designed to optimize the experience for YouTube, not to maximize the rights of YouTubers) with what the law actually specifies. The filer must provide contact information (paragraph iv), which can be used to serve process. > (i) the signature of the copyright owner or an authorized agent; > (ii) identification of the copyrighted work claimed to have been infringed, or, if multiple works are on a single site, a representative list of such works; > (iii) identification of the infringing material or activity (or the reference or link to such material) and information reasonably sufficient to permit the OSP to locate the material (or the reference or link); > (iv) contact information for the copyright owner or authorized agent; > (v) a statement that the person sending the notice has a good faith belief that use of the material in the manner complained of is not authorized by the copyright owner, its agent, or the law; and > (vi) a statement that the information in the notice is accurate, and under penalty of perjury, that the person sending the notice is authorized to act on behalf of the copyright owner . I agree the system is intentionally asymmetric, because it was built by people who understand how quickly material proliferates once it’s been posted to the Internet. That makes it abusable to chill speech, as in this case. But the law was also written with an expectation that anonymity does not guarantee full participation in civil society. reply thayne 2 hours agorootparent\"contact information\" is not the same as \"user’s name, address, and telephone number\". \"Contact information\" could just be an email address, or even a social media handle. reply willis936 16 hours agorootparentprevI'm not sure how DMCA enforcement isn't a clear violation of the 1st amendment. If you are connected you can use the government to stop speech? Isn't this the kind of thing that US feels very strongly is not okay? reply kube-system 12 hours agorootparentSection 8 of the Constitution explicitly allows Congress to protect copyright. > To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries; The 1st amendment didn't repeal this. reply willis936 6 hours agorootparentCorrect, just as section 8 didn't repeal the 1st amendment. Congress does not have power to enact legislation that violates the 1st amendment. Since section 8 exists DMCA is nearly entirely redundant. Repealing DMCA would do nothing to strip copyright owners of their rights. reply atoav 6 hours agorootparentprevYeah, but current practise isn't in any way promoting the Progress of Science and useful Arts. reply thayne 15 hours agorootparentprevAlso the seventh amendment (the content is removed without any trial by jury, and that can sometimes have considerable monetary harm to the content creator) and the fifth amendment (there is a process but it is very questionable, with basically no requirement of evidence of infringement, and no presumption of innocence). reply JumpCrisscross 14 hours agorootparent> fifth amendment (there is a process but it is very questionable, with basically no requirement of evidence of infringement, and no presumption of innocence) There is also no “capital, or otherwise infamous crime,” “criminal case,” or taking of private property “for public use, without just compensation” [1]. [1] https://en.m.wikipedia.org/wiki/Fifth_Amendment_to_the_Unite... reply thayne 13 hours agorootparentI will admit that the fifth amendment case isn't as clear cut as the others, by the letter of the law, but: > nor be deprived of life, liberty, or property, without due process of law I would argue that at least in some cases dmca takedown notices can deprive people of property, either by preventing them from making an income from their content, or causing them to lose access to a service for which they paid money, or have access to something of value. I also think it deprives the poster of some degree of liberty. Even if it isn't against the letter of the law it is against the spirit. reply JumpCrisscross 13 hours agorootparent> Even if it isn't against the letter of the law it is against the spirit By that notion YouTube has the right to call a jury if I decide to cancel. Where we agree is in the sanctions. For the safe harbor to be removed there should be more process, and it should be explicit. reply hahn-kev 14 hours agorootparentprevExcept you don't have a legal right to post content on a private platform, that private company can do whatever they want, they aren't required to give you freedom of speech. reply thayne 13 hours agorootparentDMCA is a legal mechanism that the platform has to comply with. This isn't a case of a private company removing something that violates the ToS, it is a law that can be easily be abused by a third party to cause content to be taken down. It's like if there was a law that said if you send a notification to a storage or logistics company that you think one of their customers has counterfeit goods in their storage, the storage company is legally obligated to destroy the suspect property, regardless of whether there is any evidence that it is actually counterfeit. Or even just, that company is no longer allowed to provide service to that customer, and is allowed to seize and keep any property they currently have control over. I could also talk about how many of these platforms have too much power, and the lack of competition makes it problematic if they have control to censor as they please, but that is really a different topic. reply foobarchu 13 hours agorootparentprevIANAL but I think this justification sits on very shakey ground when the private company is doing it in the name of the law (DMCA). For the same reason, I of course think the government demanding encryption backdoors by private companies violates the fifth amendment. reply omgwtfbyobbq 15 hours agorootparentprevThe 1st amendment applies to the government, so it wouldn't apply unless the DMCA request is to a government website. reply foota 15 hours agorootparentThe DMCA is a law that restricts the ability of people to make speech. The government is the one doing the restricting, even if the place the speech is being done isn't controlled by the government. The argument you're making is applicable when a website decides not to publish your speech (they're not the government, so the 1st amendment is irrelevant), but the DMCA is different. reply willis936 15 hours agorootparentprevIt's the other way around. The 1st amendment protects public speech from the government, not the government from the public. Since DMCA is a law made and enforced by the government it is used illegally against the public in violation of the 1st amendment. reply JumpCrisscross 14 hours agorootparentprev> 1st amendment applies to the government, so it wouldn't apply Eh, the DMCA is still a law providing for civil and criminal sanctions based on speech. There are probably fundamental First Amendment problems with copyright. The answer to why the DMCA—and copyright as a whole—can exist is the Copyright Clause [1]. [1] https://en.m.wikipedia.org/wiki/Copyright_Clause reply cratermoon 15 hours agorootparentprevWhen the government does it, it's bad. When the Free Market™ does it, that's innovation. reply Trasmatta 17 hours agoparentprevI worked front line technical support at a web hosting company in the early 2010s. We were told to just immediately shut down any site when receiving a DMCA takedown request. We were barely paid above minimum wage, and received no training on how to handle legal requests like this - the rule was just \"shut the site down\". reply kyledrake 13 hours agorootparentYears ago we had an opsec competition called \"Karmageddon\". The goal was to get as many upvotes on Reddit on an account in one day, with absolutely no ethical limitations to achieving that. I was winning against 10-20 people by posting a lot of random \"f this politician\" garbage in the political subreddits (this was in late 2016 and it was a feeding frenzy), until someone posted a \"check out this picture of me and my brother I haven't seen in 10 years wearing the same Ghostbusters shirt!\" post. It wasn't his brother, he found it somewhere, but it was going viral and I was about to lose. So I filed a DMCA takedown request with Reddit, and they deleted the post and the picture very quickly after, no questions asked. I ended up winning the competition, but I think we ended up giving the prize to the Ghostbusters guy anyways. The DMCA is 100% an attack strategy, it works. reply gjsman-1000 17 hours agorootparentprevBefore anyone reacts negatively to this - you’ve never seen how much spam, and how many DMCA requests, are completely real. I would personally estimate 90%+. reply JumpCrisscross 13 hours agorootparent> you’ve never seen how much spam, and how many DMCA requests, are completely real. I would personally estimate 90%+ Source? There are abundant claims to the contrary [1][2]. (I can't find any quality data either way.) [1] https://www.plagiarismtoday.com/2019/01/10/youtubes-copyrigh... [2] https://www.businessinsider.com/youtubers-channels-are-being... reply matheusmoreira 14 hours agorootparentprevIt doesn't matter. A 10% false positive rate means grave injustices are being inflicted on people on a daily basis. They are being censored by the government at a corporation's whim for bogus reasons. The corporations exploit the fact people will not fight their claims because it costs time and a small fortune to do so. Spam? Infringement? Irrelevant. Small issues compared to corporate censorship. Let it happen. Not a single person should be censored over it. reply voidfunc 16 hours agoparentprevI know someone working for UHC legal.. they're in a vendetta mode over there from what I've been told. reply rblatz 15 hours agorootparentAre they concerned that after their CEO was gunned down in broad daylight over his supposed unethical behavior, and the killer is being celebrated as a folk hero. That by doubling down and doing unethical things they may be putting targets on their own back? I know if I were at UHC I’d be looking to leave before I ended up on a hit list. reply Aurornis 14 hours agorootparent> I know if I were at UHC I’d be looking to leave before I ended up on a hit list. UnitedHealth has half a million employees. If you think individual employees are at risk of getting on a hit list then you don’t really understand the scale of the company. I’m constantly amazed by how some people adopt the uninformed narratives that sprang up in the wake of this murder. I’ve asked multiple people to guess how much cheaper our healthcare would get if we forced insurance company profits to zero and redistributed their C-suite’s compensation to their covered patients. The answer is always off by orders of magnitude. It’s all very strange. The narratives around this murder and, by extension, the health care system are being invented by people informed about both yet widely accepted as fact. Even details about the scale of UnitedHealth seem lost on people. This is a very large publicly traded company with scores of employees. reply JumpCrisscross 14 hours agorootparent> asked multiple people to guess how much cheaper our healthcare would get if we forced insurance company profits to zero and redistributed their C-suite’s compensation to their covered patients. The answer is always off by orders of magnitude The media is conflating the far-right and -left wing fringes who are celbrating Mangione as a person and his literal crime with those holding him as a symbol for their frustrations with our healthcare system. If you're celebrating the person and the crime, you need to speak to a professional. (You're also in a minority. No, your subreddit isn't the world.) If you're upset about the system, it's reasonable to not care if you're being fucked by Bob or Alice. reply y33t 13 hours agorootparent> You're also in a minority [citation needed] reply JumpCrisscross 13 hours agorootparent\"A majority of voters (68%) think the actions of the killer of the United Healthcare CEO, Brian Thompson, are unacceptable. Seventeen percent find the actions acceptable, while 16% are unsure\" [1]. Also [2]. [1] https://emersoncollegepolling.com/december-2024-national-pol... [2] https://d3nkl3psvxxpe9.cloudfront.net/documents/econTabRepor... page 12 reply Larrikin 12 hours agorootparentThat seems like most people who are healthy and military age are atleast somewhat OK with it. Its also excluding people who don't vote and have had their voting rights taken away. reply pjc50 8 hours agorootparentWhy do people keep quoting \"military age\" here? There's no a priori reason why the next CEO shooter might not be underage (common in school shooters) or retired (with time, pain, and guns on their hands)? reply JumpCrisscross 12 hours agorootparentprev> seems like most people who are healthy and military age are atleast somewhat OK with it About a quarter of under-30 men and a fifth of under-30 women. For comparison, about 20% of Americans \"think holding neo-Nazi views is\" acceptable [1]. > excluding people who don't vote and have had their voting rights taken away Where do you see the 2% of Americans [2] who have had their voting rights taken away being excluded by either poll? (Not challenging. That's just methodologically impressive.) Either way, none of what you're citing is material to an 80/20 margin. I have friends in that 17%. But I also know people who thought Kari Lake was a shoe in, \"defund the police\" would work or that every guy is an incel. Echo chambers are powerful, especially online, where they can convince a quarter of a single demographic that they're in the majority on celebrating a dude capping a stranger in broad daylight. [1] https://www.the-independent.com/news/world/americas/us-neo-n... [2] https://www.sentencingproject.org/reports/locked-out-2022-es... reply throwawaygmbno 11 hours agorootparentYou're selectively moving groups around with your statistics to try and use simple numbers to make it seem like you're always right. 20% of every single person in the US being sort of into Nazis has very little to do with the vast majority of people under 40 (military age) finding it ok with Brian getting shot. The first group includes people who are getting government funded healthcare on top of not having spent most of their lives dealing with insurance companies run by MBAs Edit: Just noticed statistical manipulation is your job and you're maybe in a similar tax bracket >> Trade private equity. Former aerospace investment banker, and before that, algorithmic equity derivatives trader. FinTech + Space + B2C angel & seed investor. Jackson Hole local; frequently in New York and the Bay Area. reply JumpCrisscross 11 hours agorootparent> 20% of every single person in the US being sort of into Nazis has very little to do with I’m saying 20% of Americans can be found who believe almost anything. 20% of Americans supporting something is basically background. > selectively moving groups around with your statistics to try and use simple numbers to make it seem like you're always right I’m slicing the data to present a number bigger than 17%. I’m trying to find a group in which a majority support Mangione who aren’t on the far fringes of society. If you’re upset about the slicing and dicing you’re confirming my point. > the vast majority of people under 40 (military age) finding it ok with Brian getting shot Where do you see this? 18 to 29 is the most favourable bloc for Mangione, and there favourability is 39%. Not a majority let alone a vast majority. And that’s being generous by combining in somewhat favourable. reply EE84M3i 15 hours agorootparentprevYou mean a vendetta against the murder suspect, or an actor impersonating UHC sending DMCAs? reply Aurornis 14 hours agoparentprevThe article puts ‘United Healthcare’ in quotes and opens with a statement mirroring this same possibility, because the first-pass response to DMCA requests is as you describe: An assumption that it’s legitimate until proven otherwise. This is very well known and the technique is often abused. Even the source article is presenting this information with a huge grain of salt. reply superkuh 16 hours agoparentprevIndeed. Claiming DMCA on something should require KYC for the claimant. If KYC is so easy and un-burdensome that it's being applied everywhere else in our lives surely in such a legal context where 2-parties are central to the dogma the initiating party in a legal case should have to prove it is who it says it is. Of course this will not happen because the DMCA as used in practice has nothing to do with law or justice and is just a mechanism for corporations to attack humans. That's not to say there's nothing good in the DMCA as written. The safe harbor provisions are now vital to the continued existence of independent communities on the internet. But in practice safe harbor is often ignored too. reply JumpCrisscross 15 hours agorootparent> If KYC is so easy and un-burdensome that it's being applied everywhere else in our lives If by everywhere else you mean finance. Which means you want to burden every single website in America that accepts user-generated content with a banking-style compliance department. reply SpicyLemonZest 15 hours agorootparentprevI'm really not sure what you mean. Nobody argues that KYC is easy or non-burdensome, which is why it's not being applied everywhere else in our lives. A company like \"Print-on-demand merch retailer TeePublic\" would find it procedurally difficult and probably economically infeasible to rigorously verify the identity of everyone who sends it a notice of copyright infringement. It sounds like you may be envisioning a system where the burden is offloaded onto the people sending a notice, but that's just not possible. If you get an email from some person Alice you've never heard of before, you fundamentally cannot know just from reading the email whether it was sent by the actual Alice, or someone pretending to be Alice, or whether there's even such a person as Alice in the first place. To tell the difference, you would have to research Alice and find some independent way to corroborate her identity. reply jazzyjackson 18 hours agoprevThis did bring up a copyright question I haven't been able to resolve - is footage from a private security camera somehow owned by the owner of the camera? AFAIK copyright is reserved for works of creative expression, so footage taken for non-creative purposes shouldn't be protected - is there some other mechanism to demand someone cease and desist sharing a video or an image once the cats out of the bag? IMO we could use something more flexible than revenge porn laws, that prevent noncensual use of your likeness, inc. impersonations of your face and voice. Fair use exceptions would obviously have to apply for newsworthy events otherwise Luigi could demand the cops take down his image during the manhunt :) (Similar question for Internet comments , since the bluesky-huggingface-kerfuffle I haven't been able to find a definitive answer if internet commentary is copyrighted creative expression or some other public domain shaped thing) reply hsbauauvhabzb 17 hours agoparentIs ‘creative expression’ somehow defined? I could creatively express myself by wiring up a camera and filming a public place, no? Edit: I do think it would be good taste to censor death/murder footage in some cases such as this, I’ve not thought about it a tonne because I assume there’s a legitimate public interest case in some scenarios (political oppression footage, etc) reply gruez 17 hours agorootparent>Is ‘creative expression’ somehow defined? I could creatively express myself by wiring up a camera and filming a public place, no? Not tested in court. >Security cameras, webcams, camera traps and other pre-positioned recording devices capture whatever happens to take place in their field of view. This raises the question as to whether their recordings are an original and therefore copyrighted work. For example, \"[i]f a security camera mounted in a lobby, recording 24 hours a day, captured a dramatic event, the video could be uncopyrighted.\"[2] >This question remains untested in the United States. In the 2008 United States district court case Southwest Casino and Hotel Corp. vs Flyingman, the casino filed suit for copyright infringement on the use of their surveillance video, but the defendant argued in a motion that the surveillance video lacked the sufficient creativity needed to secure copyright protection.[3][4] However, the case was never heard as a separate tribal court ruled that the tribes, rather than the casino, owned the footage.[4] https://en.wikipedia.org/wiki/Threshold_of_originality reply mh- 17 hours agorootparentSouthwest v Flyingman sounds like a parody case from the Grand Theft Auto universe. reply jazzyjackson 17 hours agorootparentprevWell that settles it, the matter is unsettled! Thanks for the pointer. reply jazzyjackson 17 hours agorootparentprev> I could creatively express myself by wiring up a camera and filming a public place, no? Certainly. My layperson view is it would depend on intent. If you were to use trafficcamphotobooth.com to take a selfie on 14th Street, I think the fact that you went out in front of a particular surveillance cam and hit the save button would count as a copyrighted photo even tho the camera belongs to some other entity and was going to take the photo with or without you. (Edited) Edit: going back to remind myself of the basics, creative expression is not the basis for copyright, but \"original works of authorsbip', although '''The Supreme Court has said that, to be creative, a work must have a “spark” and “modicum” of creativity.''' - this in the case determining that phonebooks are merely collections of facts and therefore not protected works. https://www.copyright.gov/what-is-copyright/ reply gosub100 17 hours agoparentprevProbably the fact that police released the footage to the media would be enough to remove copyright. Just a guess. reply gruez 17 hours agorootparentThat doesn't follow. The police wasn't the author of the work (ie. the footage), so the copyright belongs to whoever created it. The police also can't unilaterally seize the copyright of a random work to help a manhunt, so I'm not sure how the act of them distributing the work puts it in the public domain. reply fsckboy 15 hours agorootparent>The police also can't unilaterally seize the copyright of a random work to help a manhunt if the suspect is armed and dangerous and public safety is at stake, or they want to use the footage in court, yes they can use the footage without worrying about copyright (which is not seizing the copyright) reply jazzyjackson 17 hours agorootparentprevCopyright can belong to the author but still be redistributed on the evening news, fair use makes an explicit carve out for news reporting - whether surveillance footage is 'authored' in the first place is my question, I would argue it lacks originality. reply gruez 16 hours agorootparent>Copyright can belong to the author but still be redistributed on the evening news, fair use makes an explicit carve out for news reporting The parent commenter was actually arguing for stronger than that, ie. \"remove copyright\". That implies the work loses all copyright protections, not just granting news organizations permission to redistribute. reply psadauskas 2 hours agoprevOriginal source was from 404 media: https://www.404media.co/copyright-abuse-is-getting-luigi-man... reply __MatrixMan__ 15 hours agoprevCopyright has always been a cover story for legitimizing censorship, ever since it was about preventing people from printing the wrong kind of bible. Of course they're using it that way. reply JumpCrisscross 15 hours agoparent> Copyright has always been a cover story for legitimizing censorship, ever since it was about preventing people from printing the wrong kind of bible Source? reply __MatrixMan__ 11 hours agorootparenthttps://en.m.wikipedia.org/wiki/History_of_copyright > The origin of copyright law in most European countries lies in efforts by the church and governments to regulate and control the output of printers. I'll confess to assuming that that's why they wanted to regulate the output of printers. But what non-cenaorship reason could they possibly have had for wanting to do so? reply esperent 12 hours agorootparentprevhttps://wikipedia.org/wiki/Censorship_of_the_Bible Plenty of info here starting from the section From the Printing Press until the Revelation. reply strathmeyer 3 hours agorootparentprevWhen someone tells you something, they're they source. reply triyambakam 18 hours agoprevStreisand effect comes to mind. Also it seems too late to try to stop it now. reply qingcharles 16 hours agoprevA lot of providers don't even require it to be a statutory DMCA complaint. Just a simple \"I own the copyright, please remove it.\" These are private sites and can remove material on a whim with no legal recourse. That's why it sucks to have to rely on hosting your content on other people's territory. reply GiorgioG 18 hours agoprevMaybe they should hurry up and approve more claims. Seriously, my son’s insulin pump is sitting in “prior authorization” just so they can fuck me into paying the full deductible come Jan 1. Fuck United Healthcare. reply BadCookie 18 hours agoparentIt ought to be the law that approved claims should be backdated to when the initial claim was made. UHC pulled this exact trick on me, denying something that ought to have been approved in order to push the date into the new plan year with a reset deductible. reply mindslight 16 hours agorootparentIt ought to be the law that every in-network claim must be automatically approved. In network providers have already jumped through hoops to have that contract with the \"insurance\" company, including the creation of documentation to satisfy the \"insurance\" company of their judgement. It's disingenuous for an \"insurance\" company to then claim to be second guessing the very professionals' judgement they've already examined and approved of. reply JumpCrisscross 15 hours agorootparent> ought to be the law that every in-network claim must be automatically approved This almost certainly requires vertical integration between providers and insurers. That or shifting approval/denial to the provider level, i.e. your doctor can fall off the approval list from month to month for submitting too many claims. If not, providers will become the most profitable business on the planet right until every health insurer goes bankrupt. (We’ll probably get to blame it on PE-run hospitals, even if everyone is doing it.) reply ocschwar 14 hours agorootparentProviders are liable for malpractice if they approve un-needed treatments, and all drugs have side effects. And they meanwhile have their own liability insurance to pay, as well as medical student debt. So not going to be profitable either. Unless they start prescribing Carnival Cruise therapy or stiff like that. reply JumpCrisscross 14 hours agorootparent> Providers are liable for malpractice if they approve un-needed treatments There is a lot of grey area between necessary to the point of malpractice if not provided and unnecessary to the point of malpractice. Like, you’re not going to win a malpractice case because your doctor prescribed too many non-invasive tests and MRIs. And if my insurer is forced to approve every MRI, I'm probably going to ask my doctor for an MRI. reply mindslight 12 hours agorootparentprevI don't see why it would have to be every month, instead of say a statutory minimum of at least six months notice with such changes only taking effect after an open enrollment period. The doctors and insurers can sort out how to handle the resulting dynamics in their professionally-negotiated contracts, instead of just falling back to blasting individual patients with fraudulent bills. And yes, if we're going to continue to limp along with these \"insurance\" companies being the supposed market dynamic, the healthcare industry desperately needs more integration so responsibility can actually be taken and absorbed, rather than just continually punted. The whole industry has fallen into a state where each player passes the blame to someone else, including you as they tell you its now your new unpaid job to make many hours worth of phone calls as you get ground up passing messages between mutually-hostile bureaucracies. reply xbar 13 hours agorootparentprevI mean, it was fine for 50 years. reply UniverseHacker 15 hours agorootparentprevThat is how Kaiser works- the doctors have internal guidelines, e.g. a set criteria for a diagnosis, and every treatment that is ordered is automatically approved. This isn't a perfect solution either though- the guidelines are often unreasonably strict, and still deny people treatments they very much need. reply tptacek 15 hours agorootparentThe difference being that in the Kaiser system, the doctors work for Kaiser, and not for health provider chains. reply rjbwork 14 hours agorootparentprevFor providers with a history of high quality PA requests and high approval rates, there is something similar. See https://www.uhc.com/news-articles/newsroom/gold-card Other insurers have similar programs. reply mindslight 24 minutes agorootparentSounds like another layer of \"gotchas\" to pin more work and blame on patients. \"Your claim was denied because you may have checked the provider was 'in network', but you didn't check to see if they had a 'gold star' listing\". I'll go out on a limb and say that the proliferation of these punt-the-problem administrative layers (in lieu of straightforwardly owning and solving problems) is one of the main things driving the medical industry's cost disease. A few years from now this \"gold card\" program will likely be administered by a newly spun out \"third party\" vendor that doesn't publish updated lists very well, but if you feel you were treated unfairly you can always file an appeal with them... reply tptacek 16 hours agorootparentprevRo Khanna said something similar a week or so ago. You all get that this would basically be the biggest corporate giveaway in the history of American politics, right? The reason you're getting health care denials is that providers are trying to rip your face off. I'm pleading with people on this site to go look at where the dollars in health care spending actually go. CMS, the government organization that manages Medicare (and somehow got vilified a few weeks ago when Blue Cross tried to adopt Medicare's anesthesia billing rules), tracks this stuff in a set of documents called the National Health Expenditures. They're spreadsheets, they're easy to go read, please, for the love of all that is holy, go look at them before designing policy. reply mw1 13 hours agorootparentOne reason you may need to continually plead with people about this is because so many of us have had lived experiences with valid medical claims that should be covered under our policy, denied outright. Not a health insurance company saying “oh, that’s too expensive, go here for less,” but outright denial of coverage. And if we eventually succeed in having these claims covered, it is because we were willing to spend countless hours combing through paperwork, initial delays, and denials. Also, the same CMS statistics you cite can be combined with other reports to conclude that 500 billion dollars of excess administrative costs PER YEAR are attributable to our lack of a single payer system — something UHC has lobbied heavily against in order to protect their profits over the improved health care of the average American. You can read the numbers here: https://www.peoplespolicyproject.org/2024/12/10/health-care-... “private insurers currently have administrative costs that are 1,000 percent what they would be under single-payer while hospitals currently have administrative costs that are 158 percent what they would be under single-payer. The excess administrative expenses of both the payers and the providers are because of the multi-payer private health insurance system that we have. When you add it all up, excess administrative expenses — defined as administrative expenses we have under the current system that we would not have under single-payer — are equal to 1.8 percent of GDP, or $528 billion per year.” Another reason your pleading falls on deaf ears is that, sure, provider payments can be reduced (and this addressed in the above article), but at the end of the day, private insurance is a purely rent-seeking enterprise that provides no value to Americans while these “overpaid providers” are actually delivering the care. reply tptacek 13 hours agorootparent2.21 trillion dollars per year of provider costs, against 279 billion dollars net cost of health insurance. But yeah, the one player in this market that has its profits capped statutorily, they're they're the whole problem, no matter what the numbers say. Sure. I'm not telling you there's no problem. I'm saying that you've been conned into believing the problem is something it isn't. reply mw1 13 hours agorootparentThe article I posted did not claim they were the whole problem, nor did I. They are, however, a large part of the system that no one likes to deal with and can be fully eliminated without obvious negative consequences. Health insurance doesn’t provide health care and is a purely extractive rent-seeking business. The article I posted even explains how single payer can help drive health care provider rates lower, as you now have a single, powerful entity (Medicare) negotiating against doctors, hospitals and drug companies. And this “one player” (health insurance companies) heavily lobby against the implementation of single payer health care system. And their profit caps ensure that their goal is to grow the cost of medical care so they can take an ever higher profit in absolute dollars. reply mw1 12 hours agorootparentThe article I posted makes a case that eliminating private health insurance will be VASTLY more than 5% savings and people can read the article to see why. You’re choosing to avoid all of the other cost savings that will come from eliminating private health insurance and having a single payer who can effectively negotiate with providers without the goal of taking a slice of profits from an ever bigger pie. reply tptacek 12 hours agorootparentYou're citing Matt Breunig's figures about how much more efficient Medicare would be than private insurance. But the truth is, mechanically, the opposite. Medicare's efficiency is a statistical illusion. Administrative overhead is a simple ratio of fees paid to services rendered. The more services you render, the better your admin costs look. And Medicare's look good indeed, because virtually everyone in America over the age of 65 is covered by Medicare --- that's the point of the system, to do single payer at the point where costs suddenly ramp up. If you let people enroll in Medicare at age 25, they would incur lower service costs, while paying the same in fees. If you do the math, Medicare for a 25 year old looks a lot like private insurance for a 25 year old. Meanwhile: all insurance costs, in the whole economy, across all of national health expenses, total less than 10% of costs overall. Providers drive all the costs in our system, not insurers. But Breunig is fixated on his preferred solution, so he's not telling you that. But the numbers are right there if you want to see them; just search [National Health Expenditures by Type of Expenditure and Program: Calendar Year 2022]. I honestly don't care if you want Medicare vs. private insurance. I don't love my insurer. But if you zero out the total cost of insurance, public and private, you barely make a dent in our health costs. There is no way around it; the numbers are stark. Personally, I think the balance we've struck in our payment system --- private markets until age 65, at which point the state steps in --- is pretty smart. Our system is fucked, of course, but that's because health provider chains have been ripping people off for decades. reply jrflowers 11 hours agorootparent> Providers drive all the costs in our system, not insurers. I like your proposed solution that the state somehow engineer a way to drive down the costs billed by providers. Perhaps if the state operated a (pseudo-)monopsony wherein they exercise their leverage as the payer to drive down costs. It could have a snappy name like if you combined medical and care? Or maybe medical and aid? Anyway I also cannot fathom why anyone would hold ill will towards an industry that lobbies to stop that from happening. They are simply smol beans and the fact that there is no single payer monopsony means they are splitting a measly fraction of a trillion dollars per year. The fact that somebody else makes money too is proof that they couldn’t be a problem uwu reply tptacek 13 hours agorootparentprevYes. They're a part of the problem. Specifically: they are less than 10% of the problem. If you replaced them with Medicare, you would get somewhere between 1-5% off your health care bills, if everything went optimally. Meanwhile: we are commenting on a story about someone murdering a health insurance executive. reply fzeroracer 12 hours agorootparentprevYou are fighting a losing argument created by health insurance companies because they've created years of self-inflicted wounds. Everyone believes health insurers are the problems because almost everyone has dealt with the nightmare of valid claims being denied. Health provider chains rip people off because that's how they maximize earnings from insurance companies. Insurance companies maximize denials because that's how they maximize profit. You remove one side from the equation and the problem of provider costs becomes easier to solve. And as an aside, I dealt with my mother being denied healthcare from her insurance provider because they determined her stroke was a pre-existing condition. There is simply no logical argument you can ever make that will change my opinion. reply tptacek 12 hours agorootparentIt is, obviously, exactly the other way around: the system is rigged to maximize earnings for health provider chains. I honestly don't see how you can look at the numbers and come to the other conclusion. reply fzeroracer 12 hours agorootparent'Won't someone think of the poor rent seekers' is not nor will it ever be a compelling argument. The only way this problem can be solved is in the same way every other country solves it: Either tearing out private insurance and fully socializing healthcare or strongly limiting private insurance and having a public option which negotiates and keeps prices low. You can argue that health providers charge too much and that's true. But the core of the rot comes from the health insurance scheme we've cooked up. And people rightfully blame the insurers for this problem. Maybe if they dislike it so much they can put a fraction of the billions they're earning towards bribing politicians for a public option rather than constantly spiking things like that whenever they get a chance. reply tptacek 12 hours agorootparentHealth insurers make an order of magnitude less than providers, and our providers charge 3-5x more than European providers, but somehow insurance is \"the core of the rot\"? reply mindslight 30 minutes agorootparentOP said the \"health insurance scheme we've cooked up\" is the core of the rot - not insurers' on their own, but rather the whole regulatory environment (\"we\") that enshrines the dynamics of HMOs and whatnot. I agree with a lot of what you are saying. Trying to demonstrate some common ground - my (somewhat shallow) reading of the Anthem Anesthesia issue aligns much more with your analysis than the pop narrative. But how exactly is the denial of care suppose to function as a price feedback mechanism to form a working market between providers and insurers? Is an MRI provider supposed to be thinking that if they lower their prices by 10%, the insurance companies will increase the number of approved MRIs to make up for it? And this still ignores that prices are not the same as profit margins, which is a huge hand wave here. Also if those denied MRIs were truly unnecessary, then how would paying for them merely because they cost a little less make sense? Which is the crux of where my original comment was coming from. The responsibility of deciding necessary medical care needs to be laid at one decision maker (eg the treating doctor serving the patient per their code of professional ethics while fundamentally still working for the insurance company), rather than this split-brain blame game between the patient-facing doctor having little downside to saying yes, and the back office \"doctor\" at an insurance company having little downside to saying no. An insurance company shouldn't really even have its own opinion on something decided by a medical professional they're already contracting with, especially when that opinion has been created purely based on formulaic paperwork processing. At most they should be able to refer the patient to a different provider to perform the service, or withhold some payment for the service per their contract with the provider (but invisible to the patient). This is obviously not the only reform we need to get any sort of price signals and sane division of responsibility in this industry. Because yes, provider costs are the main problem and they've been marching ever upwards. But every one of these terrible dynamics that has been allowed to fester is in need of its own reform, especially if you aren't advocating for the blanket approach of single payer. reply fzeroracer 11 hours agorootparentprevCorrect. See: my experience with pre-existing condition denial. reply mindslight 16 hours agorootparentprevI've read similar comments of yours in other threads, and have thought them on point. I certainly don't see \"insurance\" companies as the single source of bad of the whole system. Personally, I just got done dealing with an 11 month billing headache spearheaded by entitled \"providers\" double billing, and shamelessly telling me that they don't have the resources to follow up on claims I told them needed to be followed up on, while still continuing to send me fraudulent bills with fake amounts and fake due dates. Meanwhile, despite their \"bucket of stomach fluid\" bureaucracy, the \"insurance\" company straightforwardly communicated what I should (not) be responsible for (at least until the rep I was emailing back and forth ghosted me because he got memory-holed). Heck I'm not even really a proponent of single payer versus reforming the industry so it has to abide by the common sense norms of commerce like every other industry. So the \"you all\" grouping falls a little flat here. But read what I wrote again - the point is that every \"in network\" provider of an HMO is already accountable to the HMO while providing care to their members. If an HMO wants to limit certain types of care, then that needs to be expressed to providers (through their contracts), who can immediately tell patients \"As a doctor I would recommend you get this type of treatment, but while working for $HMO I cannot provide it as your plan doesn't cover it\". My initial comment was phrased in terms of insurance companies, but this double approval blame game is actually more a problem with the entitlement of doctors never wanting to deal with the reality of what services cost. reply inferiorhuman 16 hours agorootparentprevand somehow got vilified a few weeks ago when Blue Cross tried to adopt Medicare's anesthesia billing rules LOL That is absolutely fucking NOT why Blue Cross Blue Shield got vilified. Please stop making disingenuous arguments. BCBS attempted to weaponize Medicare statistics and set a hard cap on anesthesia coverage based on the average time Medicare says a procedure takes. This is NOT how Medicare reimbursement works. Medicare pays anesthesia providers for the entire time a patient is under. Full stop. reply tptacek 16 hours agorootparentNo, from everything I understand about the situation, this is the opposite of the truth. Feel free to cite a source saying otherwise! I'll read it! reply inferiorhuman 16 hours agorootparentYeah, go read the rules set forth by CMS. Medicare pays for the entire cost of anesthesia. So here's why I'm so dismissive of your garbage argument. From Medicare's policy manual: https://www.cms.gov/files/document/chapter2cptcodes00000-019... A unique characteristic of anesthesia coding is the reporting of time units. Payment for anesthesia services increases with time. In addition to reporting a base unit value for an anesthesia service, the anesthesia practitioner reports anesthesia time. Anesthesia time is defined as the period during which an anesthesia practitioner is present with the patient. It starts when the anesthesia practitioner begins to prepare the patient for anesthesia services in the operating room or an equivalent area and ends when the anesthesia practitioner is no longer furnishing anesthesia services to the patient (i.e., when the patient may be placed safely under postoperative care). Compare this to BCBS's policy (which they've tried to scrub from the internet): https://www.asahq.org/about-asa/newsroom/news-releases/2024/... Anthem will arbitrarily pre-determine the time allowed for anesthesia care during a surgery or procedure. If an anesthesiologist submits a bill where the actual time of care is longer than Anthem's limit, Anthem will deny payment for the anesthesiologist’s care. With this new policy, Anthem will not pay anesthesiologists for delivering safe and effective anesthesia care to patients who may need extra attention because their surgery is difficult, unusual or because a complication arises. There's a vast gulf between those two approaches. Medicare pays for the time a procedure takes, BCBS does not. BCBS deserves every ounce of vitriol they got over that. Your entire argument appears to be that doctors are corrupt and cannot be trusted which is entirely orthogonal to the the issue with pre-determining the amount of time a procedure will take. Meanwhile BCBS overcharged (a.k.a. fraudulently charged) the federal government for over $100 million. Doctors are not the problem with health care in the United States. https://www.reuters.com/legal/anthem-must-face-us-government... UHC? $7.5 billion (with a b) in fraudulent Medicare charges. https://www.startribune.com/report-unitedhealth-group-tops-l... Cigna? They settled their fraud cases for $200+ million. https://www.justice.gov/opa/pr/cigna-group-pay-172-million-r... https://www.justice.gov/usao-sdny/pr/united-states-reaches-3... Independent Health Association? $100 million. https://kffhealthnews.org/news/article/medicare-advantage-fr... Private insurance is one of the biggest drivers of the obscene cost of health care in this country. reply tptacek 14 hours agorootparent(this comment was extensively edited since I responded to it) You haven't rebutted anything I said, and the source you quoted about Anthem's proposed policy is the summary from the lobbying group for anesthesiologists. As I pointed out downthread: CMS maintains a catalog mapping procedures to units of anesthesiology billing. later And again, edited, everything after \"your entire argument\". Private insurance literally can't be the primary driver behind health care costs in the country. Again: look at the NHE. The numbers are right there. reply inferiorhuman 11 hours agorootparentPrivate insurance literally can't be the primary driver behind health care costs in the country. They literally can and literally do. They drain billions of dollars annually in fraudulent claims with UH accounting for $7+ billion in Medicare fraud in 2023 alone. UnitedHealthCare (the insurance arm) sucks in $4 billion (with a 'b') in earnings each quarter. Doctors are not the problem. https://www.unitedhealthgroup.com/content/dam/UHG/PDF/invest... reply tptacek 11 hours agorootparentProvider costs are over 2 trillion dollars. reply tptacek 15 hours agorootparentprevNo, they do not. This was a whole controversy last year. https://www.asahq.org/about-asa/newsroom/news-releases/2023/... Lots of stuff to go find at this search: [medicare anesthesia upcoding] Reiterating: this is exactly why Blue Cross was vilified, and people angry about it were getting played by a lobbying group of some of the best-compensated professionals in America. Anesthesiologists getting paid more is not a consumer protection policy; it is the opposite of that, and it's wild that people think it might be otherwise. reply inferiorhuman 15 hours agorootparentDid you read what you posted? Because all you've cited is a bunch of complaints about the conversion factor and not the time factor. Medicare pays for the entire duration of the procedure. BCBS does not want to do that. Again. Go through the CMS regs and show me where Medicare puts a hard cap on payment like BCBS was trying to do. I know that's a tall ask because, simply put, they do not. reply tptacek 15 hours agorootparentHere's r/actuary on it: https://www.reddit.com/r/actuary/comments/1h70wic/does_anyon... I don't believe you're coming to this with a detailed understanding of the policy CMS has today or that Anthem was proposing. Sorry, at this point, unless you can cite a comparably specific source, I simply don't believe you, and we're probably at an impasse. reply inferiorhuman 15 hours agorootparentAnd you're coming at this from a profit doesn't drive up the cost of medical care which is in and of itself a pretty gross misunderstanding of what profit is. Again, did you read what you cited? That redditor is justifying the cap that BCBS wants to implement, not disproving that Medicare does not implement a time cap. I'm not here to do your research for you. CMS regs and guidelines are a matter of public record. If you don't want to actually cite the comparable regs you claim exist that's not my problem. fleeing to abstraction. You're citing an insurance actuary who posted to reddit claiming no, it's not the for-profit insurance companies that are the problem, it's the corrupt doctors. That's absolutely fucking asinine. What next? A fox claiming it's the chickens that are the problem? Unlike private insurance companies, the rules and regs that Medicare puts forth are a matter of public record and you're still unable to cite anything supporting your claim that Medicare puts a hard time limit on anesthesia reimbursement. So go ahead, stop waving your hands and start citing something. You do understand what time units are, right? www.cms.gov What's that? You're going to continue tilting at windmills and railing against the evils of overpaid doctors instead of showing anything that supports your specious claims about Medicare? Okay then. reply amatecha 15 hours agorootparentFollowing this thread all I see is you aggressively asserting a certain viewpoint, with no substantiation. I have no stake in the game but if you're going to come out swinging with aggressive naysaying but zero substantiation, all I'm thinking is: \"citation needed\". reply tptacek 15 hours agorootparentprevYou haven't presented any research at all, so it's not clear to me how that could a valid rebuttal to anything I've said. You're talking about how \"profit\" distorts this system --- I agree, for what it's worth --- and it looks to me like you're just fleeing to abstraction. It is a giant pain in the ass to pull up Anthem's original proposed policy, since it's been memory-holed everywhere, but if I manage to do it, I'll post it. You're on HN right now campaigning against reference pricing! Like I said: this whole situation is wild. You've taken the side of people making like 20x more money than the insurers. Why? reply tptacek 15 hours agorootparentLike: the whole system of Medicare reimbursement for anesthesia is based on a catalog of procedure-by-procedure base units. Medicare sets reference prices for procedures. You can just go look it up. I've got the XLS open right now. I don't understand what you're trying to argue here. You get that, at the end of the day, this is about how much money anethesiologists take home, right? People on Twitter were talking about anesthesiology being withdrawn in the middle of procedures. Obviously, that's not a thing. Of all the players in this system, the insurers are the only ones who have their profits capped by statute, across the board. If anesthesiologists make more money from a procedure, that comes out of your hide: it's reflected in your premiums and co-pays. Please, help me understand, why are you making a moral crusade out of paying your doctors more money? They don't need the money! America pays doctors 3-5x more --- THREE TO FIVE TIMES --- than Europe does. reply JumpCrisscross 15 hours agorootparentprev> Go through the CMS regs and show me where Medicare puts a hard cap on payment like BCBS was trying to do Where does it say CMS blanket approves? Because I’ll tell you this: if it does, I’m buying up some anaesthesiology practices. (They’re subcontracted within most hospital systems.) reply tptacek 15 hours agorootparentAfter Anthem backed down I'd invest no matter what you find. Clearly: they run the health care system, and the public (including Kathy Hochul) is on their side! reply inferiorhuman 11 hours agorootparentprevWhere does it say CMS blanket approves? It doesn't, and I've never made that assertion. However, Medicare and Medicaid both deny claims at a lower rate than private insurers. How frequently depends on where and when you look. The guidelines however are important. BCBS wants to reject any claims for anesthesia out of hand if they exceed the average length of a procedure. Medicare does not. Because we all know that unexpected things never happen in the operating room. reply unethical_ban 16 hours agorootparentprevI know you're getting at a specific point. And I'm aware that health insurers are not the only moneymaking component in the health care pipeline. I don't think eliminating profit is doable. Capitalism is the worst system other than all the rest. But surely we can agree that health insurers that are publicly traded do not prioritize human outcomes. The profit motive needs to be tempered, if not entirely removed, from health insurance. Profit motive should be tempered in the basic healthcare space. Excessive testing and treatment are a thing too, I understand. What I guess I'm saying is, I know it's complicated, but no amount of nuance absolves UHC and others for the kinds of denial stories that people have been telling. (I don't work Monday... I may be rambling). reply tptacek 16 hours agorootparentThey're not just \"not the only\" moneymaking component; they're an order of magnitude and another integer factor away from being one of the most significant. The problem in American health care is (1) how much providers charge and (2) how many procedures and services are prescribed. reply ruthmarx 15 hours agorootparentprev> Capitalism is the worst system other than all the rest. This isn't anything to do with capitalism. It's all to do with Republicans bizarre obsession with 'small government'. These kind of healthcare horrors don't happen in most western countries, because they have reasonable regulations and socialized healthcare in place. reply tptacek 15 hours agorootparentWell, that, and they pay their doctors 3-5x less. reply alistairSH 16 hours agorootparentprevI can see where the money goes. That doesn’t absolve the insurance industry of being a festering sore worthy of every bit of scorn sent their direction. reply dgfitz 17 hours agoparentprevI, as a male, have a 4.5k medical bill from a breast cancer biopsy center. I had surgery on December 8th. They billed me for the biopsy on January 3rd, and of course UHC isn’t covering it. Turns out joke is on them, I’m not paying it. reply hsuduebc2 17 hours agoparentprevIt's very hard to have any compassion for architects of this company's system. reply detourdog 17 hours agorootparentCongress claims this is what we the people want. reply maeil 15 hours agorootparentThe people have made clear that Luigi is what they want. reply mindslight 2 hours agorootparentThe corporate ruling class has made clear that Luigi is what they want, by stifling any other form of accountability or reform. The people have only made clear that they're desperate for any shred of accountability, despite the abhorrence of murder. reply JumpCrisscross 15 hours agorootparentprevnext [8 more] [flagged] maeil 14 hours agorootparentThrowing around terms like \"murderer\" and \"terrorism\" is a pure appeal to emotion. It's just as easy to call the CEO a \"murderer\" and say UHC committed \"terrorism\". Not going to make that argument because it's pointless semantics. The idea that it's only a tiny, negligible fringe of people who are positive towards his deeds is of course also not true. The days where websites like Reddit are purely a playground for nerds with fringe belief are long, long past us. On many mainstream social networks, the dominating view has been one of support. reply JumpCrisscross 14 hours agorootparent> Throwing around terms like \"murderer\" and \"terrorism\" is a pure appeal to emotion Not really. It’s intentional killing without colour of law: murder. It’s killing to send a political message: potentially terrorism. He’s been charged with both, so this is not meme-level everything I don’t like is violence. > days where websites like Reddit are purely a playground for nerds with fringe belief are long, long past us Yes, someone who thinks their internet echo chamber is right this time, despite its track record, is on the fringes. reply lurking_swe 12 hours agorootparenti’m curious, would a CEO _purposely_ leading an effort to automate and deny claims at 3x industry average using AI, which affects peoples lives and in some cases kills them, not be considered murder? (kills them because they may not get treatment they medically need until it’s too late - while they fight with customer support specialists for weeks or months). Ethically it absolutely is murder. The intent is to fuck everyday people like you and me to make the company some extra bucks. But that’s just the thing with capitalism. As soon as murder is spread out over thousands or millions of customers, and part of a system, suddenly it’s an “issue with the system” and nobody can be realistically held accountable. Funny how our society works. Apparently the person spearheading efforts like that are completely shielded from the law. Make it make sense… For what it’s worth i agree that Luigi is a murderer and committed terrosism. I ALSO think the ceo committed indirect murder. He absolutely knew what he was doing to thousands of americans each year. reply Nasrudith 8 hours agorootparentThe problem is that 'indirect murder' citations are operations in propaganda and not reason. It involves torturing numbers like it is Abu Ghraib (lets assign all of the deaths of the uninsured to Brian as clearly he is entirely responsible for the existence of the current healthcare system!), take more leaps of logic than Olympic gymnastics, and all mixed together with an extra large dose of motivated reasoning. In order to absurdly alchemize what is an inconvenience and possible financial hardship into the equivalence of mass murder. The fact \"indirect\" has to be added as a qualifier to murder is very telling to the illegitimacy of the concept and proof that just murder wouldn't stand undisputed. There is no reduction in culpability in the actual concept of murder. There is no difference between killing someone with your bare hands, training a dog to push a button on a detonator, or even using a hitman or deliberately creating a lethal rube goldburg machine. Instead \"indirect murder\" is just a way to call things which aren't murder a form of murder. That is a quite dangerous form of polemic which is used to justify violence. It can be applied so liberally to argue that dissent harms \"the cause\", leading to setbacks and therefore is a form of mass murder and justifies killing dissenters. reply maeil 14 hours agorootparentprevhttps://news.ycombinator.com/newsguidelines.html > Be kind. Don't be snarky. Converse curiously; don't cross-examine. Edit out swipes. > Comments should get more thoughtful and substantive, not less, as a topic gets more divisive. > When disagreeing, please reply to the argument instead of calling names. \"That is idiotic; 1 + 1 is 2, not 3\" can be shortened to \"1 + 1 is 2, not 3.\" > Please don't fulminate. Please don't sneer, including at the rest of the community. > Please respond to the strongest plausible interpretation of what someone says, not a weaker one that's easier to criticize. Assume good faith. > Eschew flamebait. Avoid generic tangents. Omit internet tropes. Have a nice day. reply JumpCrisscross 14 hours agorootparentCiting the charges and pointing out that Reddit and Twitter aren’t representative of the general population isn’t snarky, it’s substantiation. The track record of people extrapolating from Twitter or Reddit to America is incredibly poor. One needs extraordinary evidence to claim this time is different. reply newdee 9 hours agorootparentprevI don’t think the guidelines need to be carted out here. Parent was making their argument reasonably, maybe you just dislike what they’re saying? reply hsuduebc2 16 hours agorootparentprevI believe that some people really needs that. Ironically the ones which need it most. reply hsuduebc2 2 hours agorootparent*really believe not need reply whycome 17 hours agoparentprevHow is this legal? reply pxeboot 16 hours agorootparentMany of the shenanigans pulled by health insurers aren't legal, but they know a significant number of people will just pay or file bankruptcy instead of appealing or lawyering up. reply rchaud 3 hours agorootparentprevThink of all the times you've seen a news headline that reads \"Company X settles with [regulatory body], does not admit guilt\". Corporate malfeasance rarely reaches the courtroom, so their wrongdoing is not entered into public record in a way that can be cross-examined. reply schmidtleonard 17 hours agorootparentprevLawmakers need campaign money and shakedowns are profitable. reply IncreasePosts 15 hours agorootparentprevHow would OP know that is the reason? reply SpicyLemonZest 17 hours agorootparentprevIt's not in many jurisdictions. The AMA tracks laws on prior authorization, and in an increasing number of states insurers must make a decision within 5 calendar days. (https://www.ama-assn.org/system/files/prior-authorization-st...) reply mattgreenrocks 18 hours agoparentprevDude, that’s horrible. I’m sorry you have to deal with that. reply rolph 18 hours agoprev\"It is illegal to file DMCA notices if you don’t own the copyright (or at least have a good faith belief that you do). The idea that UHC now owns every depiction of the guy accused of killing their employee is laughably frivolous and one hopes that its legal department understands this and these requests are coming from a third party troll impersonating the carrier.\" reply EE84M3i 15 hours agoparentPerhaps they legitimately bought the rights from the original producers? Doesn't seem impossible, although would certainly be a strange strategy to say the least. reply JumpCrisscross 14 hours agoparentprev> idea that UHC now owns every depiction of the guy accused of killing their employee Hmm, no evidence this is happening, but could Thompson's family claim ownership of his likeness? reply User23 18 hours agoparentprevI wouldn’t be surprised if their legal theory is that they will win the rights to Mangione’s likeness in a future civil suit. If a judge is willing to claim to believe that, then I could see an injunction being issued. Does that sound ridiculous? Sure, but we’ve seen some absolutely crazy stretching of the law these past few years. reply p_l 17 hours agorootparentIIRC a landmark trial in early post-WW2 years resulted in it being impossible for third party to own complete rights for someone's natural likeness or other natural features (like voice, which was specific case involved). So they can't win the right to prevent distribution, they can win the right to use the likeliness, but for distribution they would need to have active contract with Mangione that they are representing him for that, which I somehow doubt being the case. reply aneutron 17 hours agorootparentprevCan a corporation CLAIM civil damages over their CEOs murder ? I'd think that would be his \"estate\" perhaps ? (Far from being a lawyer) reply JumpCrisscross 14 hours agorootparent> Can a corporation CLAIM civil damages over their CEOs murder Maybe not murder. But if Mangione is inspiring death threats against others, maybe. reply AnimalMuppet 17 hours agorootparentprev\"I'm going to own it in the future\" gives you exactly zero rights over something today. But IANAL, so what do I know. reply VTimofeenko 16 hours agorootparentBut it may convince the court to side with you on a seize and desist sort of a motion. IANAL either though reply plagiarist 18 hours agoparentprevWasn't UHC the company that decided to deny claims with an AI that was wrong 90% of the time? I assume they did do this. False DMCA claims are nothing to them. Yet another egregious abuse of a law that has zero real consequences for a false report. I wonder why this happens so often? reply smolder 17 hours agorootparentInsurance company employees have among the lowest productivity to pay ratios of nearly any industry. They harbor a workforce that is low skill, low effort, and high profit, largely via nepotistic hiring. reply Loughla 17 hours agorootparentHow do I get a low skill high pay job? That sounds awesome. reply JumpCrisscross 14 hours agorootparent> How do I get a low skill high pay job Why are you assuming high pay? Claims adjusters make like $50k per year for semi-skilled work. reply bobxmax 17 hours agorootparentprevSource? Not denying what you're saying, just curious about the data. reply lotsofpulp 17 hours agorootparentprevInsurance companies have low profit margins, 2% to 3% in most cases. UNH is worth more than 5x its nearest insurance competitor (Elevance) because of its large healthcare provider arm, not because of its insurance business. Otherwise, they are all sub $100B businesses, which indicate not a lot of juice to squeeze. reply wahnfrieden 17 hours agorootparentThe regulations on profit margins incentivized maneuvering to higher prices to increase absolute profit reply JumpCrisscross 14 hours agorootparent> regulations on profit margins incentivized maneuvering to higher prices to increase absolute profit Source? (On one hand, it makes sense. You see the same effect in fixed-price contracting with mandated margins. On the other hand, higher prices seems to describe the providers. Not insurers. If insurers wanted to boost the cost line, they'd approve every claim.) reply wahnfrieden 11 hours agorootparentI saw it in a vision. reply lotsofpulp 16 hours agorootparentprev7+ different publicly listed companies and myriad mutual/non profit companies as well as self insured businesses all colluding to pay more so they can collect higher premiums? And simultaneously, health insurers denying coverage left, right, and center to pay less so they can profit more? (See adjacent comment by blackeyeblitzar) Make it make sense. reply dengxiaopeng 15 hours agorootparentThere’s definitely precedent for public companies (even international ones!) colluding to artificially increase prices in a sector. Also a hilarious movie with Matt Damon. https://en.m.wikipedia.org/wiki/Lysine_price-fixing_conspira... https://en.m.wikipedia.org/wiki/The_Informant! reply lotsofpulp 15 hours agorootparentHas there ever been precedent for companies to collude to increase the prices they pay their vendors? I’m seeing a lot of big claims, but no evidence. reply hyeonwho4 13 hours agorootparentprevMy guess, stated without evidence: 1. Profits are capped as a percent of health care expenditures. 2. Employers (and people in the Marketplace) purchase insurance mostly based on price, which incentivizes lower prices. 3. (I'm guessing) Medicare advantage pays a flat annual rate to insurers. 4. Insurer denies large claims on Medicare Advantage accounts, turning annual rate directly to profits. 5. Insurer's denial of claims for other classes of policy lowers the price of the policy, increasing competitiveness. 6. Providers fight with insurance policy and fall back to billing patients, who can only pay pennies on the dollar. 7. The providers raise future rates to compensate for the costs and losses incurred by denials. 7. Increasing prices for all customers yields the desired absolute profits, without penalizing the company doing the denials. reply blackeyeblitzar 17 hours agorootparentprevI agree they’re not especially big in terms of profit. However, that doesn’t mean they are not evil. These companies increase profit margin by denying coverage. They try really hard to pretend like they are providing a particular service level and will happily collect your money with that expectation. And then, at a later point, they deny coverage, often in a fraudulent way, in order to create their higher margin. To me the low profit margin is therefore not really relevant. If they provided the product that everyone expects that they are paying for, their margin would be lower and that is exactly what should happen. reply lotsofpulp 16 hours agorootparentHow much lower can profit margin get than 2% before a business stops being a business and becomes a charity? Why would it even have shareholders at that point? Surely, even someone who hasn’t operated a business can see why revenue should exceed expenses by a couple percent to survive volatility, much less make it worth investing in. reply blackeyeblitzar 15 hours agorootparentMaybe it should be near 0% for this business because that is the end result of competition and a functioning market. Or maybe they should just become better at operating more efficiently. But what they shouldn’t do is artificially create profit margin by stealing from their customers and handing them death sentences instead of providing the care that is paid for. When you deceive and defraud customers and literally cause them harm when they’ve paid you to avoid harm, that does seem evil. That’s what I’m pointing out. reply jimbob45 13 hours agorootparentprevThe business model contains dark patterns incentivizing patient deaths in many cases. It’s not that they’re turning a profit, it’s that they’re profiting off of death. If it was a break-even org (I.e. non-profit), people would likely have less of an issue with it. reply gruez 17 hours agorootparentprev>Wasn't UHC the company that decided to deny claims with an AI that was wrong 90% of the time? Source? reply inferiorhuman 16 hours agorootparentSource? Google it. UHC's use of AI was very much in the news and in fact an article about it was posted to HN back in November. reply gruez 15 hours agorootparentSo far as I can tell the \"90%\" figure was from the complaint in a class action lawsuit filed in 2023, and there's no indication how the plaintiffs arrived at that figure. https://cdn.arstechnica.net/wp-content/uploads/2023/11/class... reply ashoeafoot 7 hours agoprevIts already memefied as Nintendo Luigi, so taking it down is futile. People will just wear Luigi TM shirts and there is nothing all the PR might of cooperate wealth can do. Reality be a hard wall and reality is the masses loath their overlords. reply hedora 17 hours agoprevI guess they’re claiming it was an inside job? So did Mangione, etc. pierce the corporate veil, or does limited liability apply like it usually does when United Healthcare kills someone? reply downrightmike 16 hours agoparentI don't think he did it. The guy on the camera in NYC didn't have a unibrow and Luigi clearly does. I think they have a scapegoat that they are trying to pin it on. reply JumpCrisscross 15 hours agorootparent> guy on the camera in NYC didn't have a unibrow and Luigi clearly does Wat. Mangione has a light unibrow. That will obviously show up more clearly head on (as it does in the taxi photo) than from above looking at the tippy tops of the hair. reply avidiax 14 hours agorootparentprevNot saying that's impossible. Certainly a ghost gun has an advantage to corrupt government agents just as it does to criminals. But it's going to be difficult to make it stick against a patsy. What will be very suspicious is if he dies in custody before trial. Otherwise, at trial, I expect: * He has no alibi * The ballistic evidence matches the gun. * He has possession of additional ammunition from the same lot. * He was in fact a client of UHC with a substantial claim denial history * His computers/phone show that he was cyber-stalking If they have all that, I don't think a reasonable person could believe that the FBI crime lab and Google can be coerced in a grand conspiracy into fabricating evidence. If it turns out that the gun is a \"2nd ghost gun\", and the prosecution claims that he ditched the 1st gun and the ammo, and his alibi is \"weak\", and he cleared his digital history, that would be a much weaker more suspicious case. reply JumpCrisscross 13 hours agorootparent> he was in fact a client of UHC with a substantial claim denial history It doesn't appear he was [1]. [1] https://apnews.com/article/luigi-mangione-united-healthcare-... reply blackeyeblitzar 17 hours agoprevSeems consistent with what I would expect. Trying to memory hole all of this bad PR is very much how they normally operate. For example United has tried really hard to bury all mention of the incredibly damaging breach of change healthcare, which is a subsidiary. Over 100 million Americans had their private medical details leaked. All united has done is offer some basic credit monitoring. They won’t even tell you what data specifically got leaked and refuse to cooperate with such requests. reply EdwardDiego 16 hours agoparentYeah that ChangeMD ransomware kinda broke healthcare in the US for a week or so. reply blackeyeblitzar 15 hours agorootparentA lot longer than that. Many pharmacies and affected healthcare providers were not able to get back to a functioning state for several weeks. The result of canceled and delayed procedures created impact that still continues. Some businesses literally ran out of cash. And of course, everyone has suffered severe damage and harm from the data that was exposed. reply maeil 15 hours agorootparentThere's no way there aren't at least a dozen of deaths attributable to that, but of course zero consequences for anyone responsible. Luigi proven right once again. reply JumpCrisscross 14 hours agorootparent> zero consequences for anyone responsible Who is responsible? With Mangione, there is one person singularly and unambiguously responsible. That makes meting out punishment easy. With a hack, the hackers are obviously responsible. But we haven’t found them. After that, IT at UHC? Who? Are they civilly liable, or criminally? reply maeil 14 hours agorootparentI think you'd enjoy talking on this topic with a professional ethicist - they exist (although surely an endagered species), we had one at our last company, brilliant guy to talk to - or listen to a modern seminar on philosophy. Not being snarky. If one is ambiguous, so is the other, of course. And you probably know this as well, having applied this logic many times in your daily life. Let's suppose that Luigi gets the death penalty. Is the person who administers it solely responsible for this \"murder\"? How about if a parent abuses a child in all possible ways since birth. Josef Fritzl is a good example, let's take him, you can look him up if you're not familiar though it's a harrowing read. Suppose his daughter murdered him afterwards. Is she \"solely and unambiguously responsible\" much more so than, say, the hackers, or IT at UHC? This isn't clear cut whatsoever. > Are they civilly liable, or criminally? Now you're talking law, which is a whole different discussion, and I never brought up. You're probably aware that the whole premise of the support behind Luigi is that US law as it is currently applied has enormous issues, so arguing what is and isn't law is not a productive avenue in the first place. I also find it not very interesting, but I'm sure e.g. certain lawyers would. reply JumpCrisscross 14 hours agorootparent> If one is ambiguous, so is the other, of course Not really. Criminal and moral respopnsibility aren't the same, and we don't necessarily want them to be the same--that's why we hold the former to a high standard (e.g. innocent until proven guilty). > Is she \"solely and unambiguously responsible\" much more so than, say, the hackers, or IT at UHC Yes, she is solely and unambiguously responsible for the crime and so should be punished. There is ambiguous and shared responsibility for the offence, but that doesn't mean everyone who slighted her should be punished--that's why these are usefully distinct questions. > Now you're talking law, which is a whole different discussion, and I never brought up You brought up \"zero consequences for anyone responsible.\" Brian Thompson is dead, so that's N = 1 consequences. But you said zero. Which means you're presumably talking about lawful consequences. reply maeil 14 hours agorootparent> You brought up \"zero consequences for anyone responsible.\" Brian Thompson is dead, so that's N = 1 consequences. But you said zero. Which means you're presumably talking about lawful consequences. I did not bring that up, I brought up more, including > Luigi proven right once again Which means \"Without doing what he did, there would have been zero consequences\". I also just brought this up, which is context you'll definitely be aware of, meaning it's even less likely anyone would be talking about law in this context: > You're probably aware that the whole premise of the support behind Luigi is that US law as it is currently applied has enormous issues, so arguing what is and isn't law is not a productive avenue in the first place. I don't think that in good faith, given the context and the entirety of my comment, you could reasonably come to the conclusion I was talking about law. reply jmclnx 18 hours agoprevScrub the internet ? Good luck with that. Streisand joined the chat. reply jjguy 17 hours agoprevHey look on the bright side. If this is legit, then it will inevitably become a reference in the bill to overhaul the DMCA when it (finally) gets introduced! The beginning of the end, the moment when thr DMCA jumped the shark (for the broader world, not us tech geeks) reply thewayitwas 16 hours agoparentnext [2 more] [flagged] saturn8601 16 hours agorootparentGen X is not all roses: See Elon Musk and all his 'friends'. Oh how bout their kids, they started off as rockstars really taking the reins in 2016 and 2020 (see the Bernie movement). Now a noticeable % have gone far right. :/ reply whamlastxmas 16 hours agoprevThis is really peak capitalism and peak class warfare reply m3kw9 16 hours agoprevYeah f that reply Kenji 18 hours agoprevnext [2 more] [flagged] AceyMan 17 hours agoparentI love the 'nick handle' I've seen used on some of the socials discussing these events over the past two or three days: The Adjuster. It's just so ... right. reply lolinder 18 hours agoprevnext [9 more] [flagged] brundolf 17 hours agoparentI have no horse in this race, but there's a long history of corporations egregiously abusing DMCA, so it feels within the realm of possibility unless you have specific evidence to the contrary reply lolinder 17 hours agorootparentThere's also a long history of people abusing the fact that DMCA notices essentially require the provider to not verify anything—filing fake reports for various motivations that can't be readily traced. Given that both have a history here, the big question is who benefits. I can think of a very likely motive for an internet vigilante to try to smear United Healthcare by making them look even worse. I can't think of a great motive for United Healthcare to try (and completely predictably fail) to suppress the story. reply re 17 hours agoparentprev> But this is so obviously someone falsifying these reports—to be a troll or to make United Healthcare look bad or whatever It is entirely believable that UHC is working with some crisis management firm to try to manage public perception, and that part of their strategy is to try to get media portraying Mangione positively taken off the internet, whether or not they have legal grounds to do so. There have been so many past stories about companies using illegitimate DMCA claims that I'm surprised you consider it obviously not UHC or someone working with them. It is possible, but I don't think it's a forgone conclusion. reply lolinder 17 hours agorootparentMany on the internet are currently in the middle of celebrating the vigilante who murdered their CEO. I find it to be far more likely that one of those people decided to fan the flames and play vigilante than that someone in their PR team got it into their heads that trying to suppress images of the murderer-hero of the year would do any good. reply whycome 18 hours agoparentprevHas a UHC public channel come out refuting it? reply lolinder 17 hours agorootparentTheir CEO was just murdered in cold blood and social media rejoiced. I don't think there's anything for them to gain by speaking up about any of this in public right now. Sometimes the best thing to do is shut up and let things settle down. reply schmidtleonard 17 hours agorootparentThere's nothing to gain by trying to suppress the reaction they don't like? reply Glyptodon 17 hours agoparentprevI mean the original sourcing (404 media) is not without reputation and you'd hope they're trying to do some basic checking, but there's probably also a reason they say \"an entity claiming to be United Healthcare Inc\" at this point. That said, I'd argue even if these are falsified reports (I don't personally have a strong opinion on this - I could see this going either way) it seems very much of interest to this community. It's also interesting - it doesn't seem like there can be copyright on security camera footage (just like there isn't/shouldn't be on AI output) so even if the claim were brought by a different party it seems questionable. reply daft_pink 14 hours agoprev [–] Bill Burr is right. https://youtu.be/OILUHHtZt4c?si=TS159SY_VVAHrGnN reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "United Healthcare is reportedly using DMCA requests to remove online content related to Luigi Mangione, despite not holding the copyrights, raising concerns about misuse of copyright law.- The situation underscores how the DMCA can be exploited by copyright trolls to pressure content hosts into compliance with potentially baseless claims.- It is uncertain whether United Healthcare is directly responsible for these requests or if a third party is impersonating them."
    ],
    "commentSummary": [
      "United Healthcare is allegedly using DMCA notices to remove images of Luigi Mangione, accused of killing their CEO, but there is speculation of impersonation exploiting DMCA loopholes.",
      "The DMCA process allows for takedown requests with minimal verification, which can lead to misuse and abuse.",
      "Ignoring a DMCA notice can result in losing safe harbor protections, making recipients liable for copyright infringement, highlighting issues with the DMCA's implementation."
    ],
    "points": 351,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1734913619
  },
  {
    "id": 42488983,
    "title": "Twtxt is a decentralised, minimalist microblogging service for hackers",
    "originLink": "https://twtxt.readthedocs.io/en/latest/index.html",
    "originBody": "Welcome to twtxt!¶ Release: v1.3.2-dev. Welcome to twtxt’s documentation. This documentation is divided into multiple parts. We recommend you to get started with the Installation and then head over to the Quickstart section. If you don’t know what this is about read the Introduction first. There is a more detailed Usage section about how to use twtxt via the CLI. The internals of twtxt are documented in the API chapter. Feel free to contribute to this project. The source code is maintained on GitHub. User Guide¶ Introduction Demonstration Features Installation Release version Development version Quickstart Usage Follow a source List all sources you’re following Unfollow a source Post a status update View your timeline View feed of specific source Edit twtxt configuration Configuration [twtxt] [followings] twtxt file Format specification Registry Format specification API Endpoints Add new User Latest tweets Search for tweets Query for mentions Query for tags Query for users Discoverability Community¶ twtxt IRC channel: #twtxt on irc.libera.chat API Reference¶ This part of the documentation describes the modules, classes, functions and other source code specific details of twtxt. API Models Config twtxt is a decentralised, minimalist microblogging service for hackers. Navigation Introduction Installation Quickstart Usage Configuration twtxt file Registry Discoverability API twtxt @ PyPI Issue Tracker Related Topics Documentation overview Next: Introduction Quick search ©2017, buckket.Powered by Sphinx 1.8.6 & Alabaster 0.7.12Page source",
    "commentLink": "https://news.ycombinator.com/item?id=42488983",
    "commentBody": "Twtxt is a decentralised, minimalist microblogging service for hackers (twtxt.readthedocs.io)290 points by Gedxx 22 hours agohidepastfavorite103 comments p4bl0 24 minutes agoI built a very (very) similar tool called terss (for \"terse\" and \"RSS\") years ago in Bash on top of RSS feeds (using core-utils and more-utils — in particular for 2xml and xml2) for interoperability because it made the whole thing more easy to produce and consume with other tools. I'll try to find if I still have it around somewhere :). UPDATE: here it is: https://code.up8.edu/pablo/myutils/-/blob/master/terss reply simonw 21 hours agoprevI found it surprisingly hard to find live examples of sites running this. The directory at https://git.mills.io/yarnsocial/we-are-twtxt gave me an error, but I found it in the Internet Archive (19th September 2024): https://web.archive.org/web/20240919022045/https://git.mills... Here's a live example from that list: https://niplav.site/twtxt.txt - and that one shows ones its following, this one has recent posts (from December 2024): https://txt.sour.is/user/xuu/twtxt.txt The last commit to https://github.com/buckket/twtxt/commits/master/ is October 2023, so I don't think this project is 100% thriving at the moment. Update: Aha! Found https://twtxt.readthedocs.io/en/latest/user/registry.html and via it https://registry.twtxt.org/api/plain/tweets which shows some recent content across the network. Also https://registry.twtxt.org/api/plain/users looks to be a list of users, though I couldn't figure out how to paginate it (using ?page=3 doesn't seem to work, despite that being listed on the https://registry.twtxt.org/swagger-ui/ page) reply networked 11 hours agoparenttwtxt isn't very popular. I think the feed format itself is neat, other than the tab, but maybe there isn't enough of a niche for it. I have implemented a twtxt version of the Atom and the JSON Feed feed [1] for my site at https://dbohdan.com/twtxt.txt. The generator I originally developed for the site creates twtxt feeds, too: https://github.com/tclssg/tclssg. I haven't seen it in another static site generator or plugin; please link if you know one. The public-access Unix system tilde.institute has a twtxt registry: https://twtxt.tilde.institute/. You can see the user list at https://twtxt.tilde.institute/api/plain/users. The aggregator https://twtxt.tilde.institute/api/plain/tweets is currently filled with what looks like somebody protesting the request rate: quite https://lublin.se/twtxt.txt 2024-12-23T03:15:21-05:00THE LAST HUMAN POST ON THIS FEED IS MORE THAN FOUR YEARS OLD. PERHAPS TWTXT CLIENTS SHOULD THEN FETCH THE FEED *VERY* RARELY. quite https://lublin.se/twtxt.txt 2024-12-23T03:15:16-05:00THE LAST HUMAN POST ON THIS FEED IS MORE THAN FOUR YEARS OLD. PERHAPS TWTXT CLIENTS SHOULD THEN FETCH THE FEED *VERY* RARELY. quite https://lublin.se/twtxt.txt 2024-12-23T03:15:07-05:00THE LAST HUMAN POST ON THIS FEED IS MORE THAN FOUR YEARS OLD. PERHAPS TWTXT CLIENTS SHOULD THEN FETCH THE FEED *VERY* RARELY. [...] [1] This is why you shouldn't include the word \"feed\" in the name of your standard for feeds. reply ahazred8ta 20 hours agoparentprevThe current list of active yarn.social/twtxt sites is at https://feeds.twtxt.net/feeds (there are quite a lot) The community name is YARN; 'twtxt' is the protocol name. reply simonw 19 hours agorootparentThanks! I've filed a PR to add that to the documentation. https://github.com/buckket/twtxt/pull/183 reply lofaszvanitt 17 hours agoparentprevbecause it's \"for hackers\"! reply prologic 5 hours agoprevThe Twtxt/Yarn community is larger than you think. As the founder of Yarn.social[1] (which itself uses the Twtxt spec and extensions[2]) and operator of the \"flagship\" instance twtxt.net[3] I often interact with around ~70 folks (_not including news feeds_). [1]: https://yarn.social [2]: https://twtxt.net [3]: https://twtxt.net reply cfiggers 14 minutes agoparentRespectfully, 70 folks is not larger than I thought. reply shark_laser 17 hours agoprevI was using it for years before moving to Mastodon, then Nostr. It did work rather well, is easy to code for, and I still have people pulling from my twtxt file, but it does get a bit tiresome managing follows etc with the clumsy apps. And not having a decent mobile app made it less fun to use on what for me is social media's primary use case - while on the loo hahaha. reply chickenfeed 7 hours agoparentGreat app name: ontheloo reply chickenfeed 7 hours agorootparentBut perhaps, not so good, don't google that one. reply dang 20 hours agoprevRelated. Others? A Decentralised Social Network - https://news.ycombinator.com/item?id=33513022 - Nov 2022 (1 comment) https://news.ycombinator.com/item?id=25246533 (Nov 2020) Twtxt Is a Self-Hosted, Twitter-Like Decentralised MicroBlogging Platform - https://news.ycombinator.com/item?id=25242996 - Nov 2020 (27 comments) Twtxt v0.0.7 Your self-hosted, decentralised Twitter -like - https://news.ycombinator.com/item?id=23945300 - July 2020 (7 comments) Twtxt.net – Attempting to respark the twtxt community - https://news.ycombinator.com/item?id=23892491 - July 2020 (1 comment) Twtxt is a decentralised, minimalist microblogging service for hackers - https://news.ycombinator.com/item?id=23507640 - June 2020 (1 comment) Twtxt is a decentralised, minimalist microblogging service for hackers - https://news.ycombinator.com/item?id=23312756 - May 2020 (1 comment) Txtnish – a client for the microblogging platform twtxt - https://news.ycombinator.com/item?id=13742949 - Feb 2017 (4 comments) htwtxt – hosted twtxt server (written in Go) - https://news.ycombinator.com/item?id=11091592 - Feb 2016 (2 comments) Twtxt – Decentralised, minimalist microblogging service for hackers - https://news.ycombinator.com/item?id=11043502 - Feb 2016 (65 comments) reply codazoda 19 hours agoprevIt's quite buggy. Any minor change from the norm (config directory, txt file directory) seemed to break it. Finally I went with the standards and it still had trouble. `twtxt following` gives you errors. At first I thought it was because I wasn't following anyone, even though I chose to follow the twtxt news feed, but I never got rid of the error. I got errors about \"feed not available\" even though the txt files were there (maybe version differences?) It sounds kinda interesting, honestly, but I give up. reply lofaszvanitt 17 hours agoparentthen you are not a hacker at heart :) reply shortrounddev2 15 hours agorootparentOr they have better things to do than debug someone else's pet project reply sssilver 13 hours agoprevI will never understand why all these minimalist content publishing systems pretend that visual imagery is unnecessary. Humans have expressed themselves in images long before they have expressed themselves in text. Any system that forces humans to express themselves purely in ASCII, monospace, and monochrome is crude and borderline disrespectful towards human expression. It made sense in 1983, when the technology to power these expressions was not prevalent. But why do this today? I know they exist, but who are the people who confine themselves this way? Is it acknowledged as an act of self-discipline or self-restraint? Some sort of an artistic statement? What is the process through which one arrives at \"I will commit to this system, and always only communicate in written speech, laid out in uniform monochromatic typographic format. I will walk into these constraints voluntarily.\" Any medieval monk writing manuscripts, any ancient Greek or Roman or Egyptian or Chinese capturing history and literature on parchments would have gasped at such a thought. reply openrisk 10 hours agoparentWriting evolved after visual imagery to solve the poor ability of visuals to express abstractions. It too is based on imagery, thats what script is after all, but leverages the emergent social pattern of literacy to convey highly encoded information. This invention (that we now take for granted) has been so exalted in the minds of earlier generations that they would go to extremes to banish imagery (iconoclasts etc.) It takes its most extreme form in mathematical script: concise expression of the most abstract ideas. Commingling imagery and text has its many uses of course: try describing a graph or diagram or a real scene using text only. But for other purposes (literature) its a distraction that forces the brain to switch mode and diminishes the experience. reply Sharlin 10 hours agorootparent> It takes its most extreme form in mathematical script: concise expression of the most abstract ideas. Yet one thing that these minimalistic text-based interfaces are terrible for is rendering mathematical notation. > But for other purposes (literature) its a distraction that forces the brain to switch mode and diminishes the experience. Yet we do not (typically) consume literature in a monospace, typographically impoverished form. reply jampekka 8 hours agorootparent> Yet one thing that these minimalistic text-based interfaces are terrible for is rendering mathematical notation. You can notate mathematics using text. In fact the mathematic notation rendered to vector graphics/bitmaps is typically rendered from such text-based notation. Mathematics is increasingly done using text-based notation (e.g. Lean, Mathematica, SymPy), which I actually prefer because it's typically unambiguous unlike the traditional notation with wildly varying conventions and a lot left implicit or even ill-defined. reply GoblinSlayer 4 hours agorootparentOh, SymPy looks great! reply jampekka 3 hours agorootparentIt is, at least for basic stuff. The biggest problem is that one forgets to solve stuff by hand after getting used to SymPy doing it automatically. Sometimes SymPy fails at problems that Mathematica can solve though. And Mathematica is expensive pain. reply liotier 8 hours agorootparentprev> mathematic notation rendered to vector graphics/bitmaps are typically rendered from such text-based notation What everyone was impatiently waiting for: LaTeX microblogging ! But seriously, a specialized Bluesky client could easily do that for scientific communities - no need to reinvent the whole stack. reply GoblinSlayer 5 hours agorootparentOne complaint is LaTeX isn't semantic, but visual markup. reply Sharlin 5 hours agorootparentprevOf course. But good luck trying to render LaTeX in a text terminal. reply openrisk 10 hours agorootparentprevThey must live and die by their choices but I am defending the category as a whole. Both limitations can be lifted without altering the essence of a distraction free reading flow. reply ipaddr 12 hours agoparentprevThe ironic part is you post this on a throwback site where content is more important than presentation. reply sssilver 12 hours agorootparentI gather from your text that you perceive images to be presentation and monochrome monospace text to be content. Isn't all expression in a nutshell an act of presenting content? Is emphasis content, or is it presentation? What about lowercase and uppercase? IS THAT CONTENT? Or \"simply\" presentation? All expression is an attempt for our content to be perceived as we intend. All expression is a presentation of our content, including the very text of it. The text itself is simply one of the dimensions of our content's presentation. reply troupo 11 hours agorootparentprevHN is not a blogging site, and its content is ephemeral comments. It's main value is threaded discussions which excuses the archaic and limited tools. reply maximinus_thrax 12 hours agorootparentprevAnd medieval monks in heaven are all gasping in unison reply richardw 11 hours agoparentprevLet everyone make the things they want to make. Make something else? These people do you no harm, why pull their efforts down? “When you don't create things, you become defined by your tastes rather than ability. Your tastes only narrow and exclude people. So create” - Why the Lucky Stiff reply sssilver 10 hours agorootparentI figure there's value in sharing why one would not want to use something that's being shared/offered here. Hacker News is a discussion platform. People submit ideas with the expectation that the community would find them interesting enough to discuss them. When someone submits an idea of text-only content publishing, is it not perfectly reasonable to discuss one's thoughts on that idea, whatever they happen to be? And if those thoughts didn't resonate or apply, would lack of upvotes not conveniently push these thoughts to the bottom of the page where most never reach? If I was an author of such a platform/idea/product, these are exactly the kind of responses I'd be most interested in reading. Not the feel-good high-fives, but the \"what did I miss that others find important\"s. reply richardw 10 hours agorootparentI think it’s turned into the place to submit work when you want as much harsh feedback as possible. You can get upvotes but need to run the gauntlet of “why isn’t it open source/here’s a better app/this is dumb” etc. Top comments are often pretty caustic, so that’s the culture we’ve built. I guess if you want to sharpen your offering really fast, this is optimal. “Any system that forces humans to express themselves purely in ASCII, monospace, and monochrome is crude and borderline disrespectful towards human expression.” What is the outcome you’re hoping for, by giving that feedback? Should we kill the authors now or just shun them until they give us pictures? Edit: every single system makes choices. You either try please everyone or you choose a set of people to focus on. This one isn’t for you, and that’s ok. It may fail and that’s ok too. I don’t think it’s disrespectful to human expression, if anything it’s a form of human expression. reply abudimir 3 hours agorootparentprevFirst time I heard of _why - interesting dev story. Thanks! reply LudwigNagasena 9 hours agoparentprevThere is a cargo cult of absence of features because old cool developers are used to email, IRC, EMACS, etc (stuff they got acquainted with in their college days 30 years ago). reply anthk 8 hours agorootparentEmacs allows inlince images since the first X-based release, even more with Emacs. Inline images on Email and everything. Also, Email itself supports MIME too. But no image will be concise enough against a good wall of text explaining it. 30 years ago was 1994. People used to post images under Usenet too. But a lot of these images without some annotation they are useless. reply JdeBP 11 hours agoparentprevIt's clear from the demonstration video alone that this is not an ASCII system, and indeed reading the doco confirms that the text files are considered to be UTF-8. This gives some irony to your analysis. Because this system thus permits one of the the very same ancient expression-in-images systems that you are alluding to: hieroglyphs are in Unicode. I expect that in the 9 years of its existence, almost no-one has ever used them in this system. reply sssilver 11 hours agorootparentThe word \"ASCII\" was a grotesque, artistic verbal presentation choice for the substance of content I intended to convey :) I figured it supports UTF-8. I believe the gist of my point is still valid. Images are content. Layout is content. Typesetting is content. All are components of expression. reply JdeBP 10 hours agorootparentAlas, it isn't valid if one goes beyond even reading the doco. The underlying library that the tool uses, click (https://click.palletsprojects.com/en/stable/), turns out to include the idea that the \"tweet\" strings can contain ECMA-48 control sequences. This permits not only boldface and italics, but also (on very old terminals and very modern terminal emulators) underline, strikethrough, faint, reverse video, invisible, and even 8 whole colours. (-: Again, though, I expect that in the 9 years of the system's existence, no-one has actually used this in earnest. In part, this is because the default mode seems to be to apply a regular expression substitution to attempt to strip out control sequences, because of course ECMA-48 and ECMA-35 are over-expressive permitting things like OSC, NEL, PM, APC, cursor motions (\"layout is content\"), insert/delete/erase, and code page changes. Amusingly, the regular expression substitution is not based upon an understanding of ECMA-35 and is faulty. reply benjifri 11 hours agoparentprevIn this case, because it's a text file based system that works through the CLI. It is kind of ironic though that the \"Demonstration\" in the introduction is a video reply chickenfeed 7 hours agoparentprevI used to enjoy using browsers like W3M, if you can just throw assets at your favourite tool of choice, like an image viewer, a video player, it just feels better for me as I don't have to learn a different interface for every site I visit. reply prmoustache 11 hours agoparentprev> I will never understand why all these minimalist content publishing systems pretend that visual imagery is unnecessary. I don't think anybody is prevented from putting a link to an image, nor to build a client that autoload linked images (the same way that say, some gemini browsers like lagrange can be configured to show you images by default). reply ulbu 2 hours agoparentprevsome people like different things. imagine that. reply cess11 11 hours agoparentprevIt's built with Python, where strings default to UTF-8. Did they invent their own strings and constrained them to ASCII? Do you have the same view of books? You're condescending and refuse to spend time with books that don't have pictures? reply juliangmp 9 hours agorootparentBooks and social media platforms are actually two different things reply cess11 5 hours agorootparentYou forgot to mention the differences you find important and why they ought to be important to the authors of the project in TFA. reply ynniv 19 hours agoprevnostr is the hackers microblog. Just sign things and relay them: https://github.com/nostr-protocol/nips/blob/master/01.md https://news.ycombinator.com/item?id=42489954 reply hoytech 11 hours agoparentWhat I like about nostr is the experimentation with different clients, and the fact that your identity is portable to any of them. It's just a really cool model. I'm working on a nostr-based discussion site heavily inspired by HN/reddit: https://oddbean.com/ No login required, and you can switch to a different client any time you want. We try to keep the bitcoin/politics stuff to a minimum on the homepage. There's a lot of that on nostr, but also a lot of kind and thoughtful discussion about diverse subjects. reply nunobrito 9 hours agorootparentYes, with NOSTR you get the benefit of creating random accounts as well as reusing existing ones. Good work with the platform. reply nacs 1 hour agoparentprevThe nostr tech itself is great. The problem with nostr is that 95% of the conversations are about crypto. reply cbzbc 19 hours agoparentprevLike mini usenet with signatures. reply jimmySixDOF 9 hours agoprevSimon Willison has one of the more prolific microblogs going and just did a meta review on his approach including the mechanics (surprise surprise it's Django). Could be of interest to anyone double clicking this thread. My approach to running a link blog 22nd December 2024 https://simonwillison.net/2024/Dec/22/link-blog/ reply fsiefken 19 hours agoprevfor another smolweb social network, check bubble running on gemini. http://portal.mozz.us/gemini/git.skyjake.fi/bubble/main/ http://portal.mozz.us/spartan/hitchhiker-linux.org/gemlog/on... the last url needs a gemini client to view. surprisingly bubble looks a bit like hackernews and has support for moderation \"Discussion forums, microblogging, and Git issue tracking for the Gemini community. You only need a Gemini client to participate. Welcome!\" gemini://bbs.geminispace.org/s/Bubble reply nmz 19 hours agoprevNot a bad idea, light enough to implement a telegram bot everyone can use or email or maybe a bitlbee plugin so you can have a twtxt channel. But, what is it? can you PM someone? participate in a conversation? or is it just that, a microblog? Usability wise what will you do? keep a client running that always checks if new blogs have happened over, how many followers? I'm currently following 500 people, that's 500 connections if this takes off. It would be nice if an rss client supported it, until then ehh reply James_K 18 hours agoprevIt's called HTML + RSS. reply bfung 16 hours agoparentI was about to ask… looks rss to me XD reply deadbabe 21 hours agoprevWith a name like Twtxt how are you supposed to easily talk about it with other people? Not even one vowel reply elpocko 21 hours agoparentNot a huge problem: it existed for 9 years and no one is talking about it. reply kstrauser 21 hours agoparentprevIf it becomes popular, I’m pronouncing it “twixt”, even if it’s wrong. reply hackernewds 21 hours agorootparentI inferred Twittext so it's not ideally designed for virality reply bookofjoe 19 hours agorootparentprevhttps://imgur.com/a/XtEuaEd reply guerrilla 21 hours agorootparentprevThis has to be it. reply linsomniac 21 hours agoparentprevThe upvotes on the pronunciation in the sibling comments should be considered legally binding and will be the authoritative pronunciation. Upvote carefully. reply sn0n 21 hours agoparentprevI think I'd pronounce it as, Twit-ext? reply freetinker 21 hours agorootparentThis is correct. The rest are incorrect. reply bityard 20 hours agoparentprevNot a Welsh speaker, I see reply zeograd 21 hours agoparentprevIntuitively, I read it as \"tweetext\" reply PNewling 20 hours agorootparentHuh, in my head I was reading it 'twit-text' (this is not meant to be a pejorative comment), but I guess that is ascribing it another 't' where there isn't one reply mikae1 21 hours agoparentprevTalk? You write about it, on a (mechanical) keyboard. reply shakna 19 hours agoparentprevMost of the community call it Yarn. They use twtxt as the protocol name - and it isn't like HTTP is one vowel. reply tux 20 hours agoparentprevIt kind of sounds like twitter extension. This is what i thought of it as when i first seen it. This can be very confusing. reply huijzer 21 hours agoparentprevThere is a long history of confusing or weird project names in computing like sqlite, gif, Splunk, Hadoop, Coq, MongoDB (from humongous apparently), yacc, C, R, and X (the window manager; not a lang or the social media site). reply JdeBP 11 hours agoparentprevAs its name says, \"double u\" is not just one but two vowels. (-: reply Aloha 21 hours agoparentprevtwit-text is how I would pronounce it. reply smitty1e 20 hours agoparentprevI suppose that if one were motivated, one would run a https://github.com/plomlompom/htwtxt service and then point the audience to it. Not clear that the juice would be worth the squeeze over, e.g. Mastadon. reply astrea 14 hours agoprevThis is literally just a shared text file with buzz words thrown in the title. You could achieve the same thing with a Google Drive link to a text file. reply fsiefken 21 hours agoprevperhaps someone can implement it in the bluesky pds as an additional feature https://github.com/bluesky-social/pds put caddy in front of it for brotli compression so it serves fast on 1k2 links reply block_dagger 13 hours agoprevBroken image and popup message that disappears after a few seconds on the home page. Yeah, no thanks. reply k0ns0l 4 hours agoprevpass :/ reply briandear 20 hours agoprevWhat problem is this solving? Not a criticism, just genuinely curious. On X, I can follow hackers, I can block/mute and curate as I want. Just not sure who asked for this. reply prmoustache 11 hours agoparentYou can't self host your X feed for a start. reply ruthmarx 20 hours agoprevSo how big is the Twtxt community? How many Twtxters? reply zimpenfish 20 hours agoparentI think I only ever interacted with 2 whilst I was experimenting with twtxt. reply ahazred8ta 20 hours agoparentprevhttps://feeds.twtxt.net/feeds (There are hundreds of us!! HUNDREDS!!!) /s reply briandear 20 hours agoprevAlso, I forgot that Mastadoon is/was also a thing. Are there any original ideas out there rather than another “Twitter but not Twitter” clone? reply shakna 18 hours agoparentPeertube [0], Lemmy [1], and Pixelfed [2] all run on the same protocol as Mastodon. You can interact across all of them (e.g. Mastodon user replies to Peertube video post), but the presentation on each is very different. There's more as well. Like Friendica [3] feels a little bit like Facebook, and Hubzilla [4] feels a bit like Google+ used to. Diaspora feels a bit more basic, but similar concept to Friendica. All of them can cross interact. [0] YouTube style - https://joinpeertube.org/ [1] Reddit-style - https://join-lemmy.org/ [2] Instagram-style - https://pixelfed.org/ [3] https://friendi.ca/ [4] https://hubzilla.org reply jazzyjackson 18 hours agorootparentpixelfed looks nice. Instagram has been the only social I kept around because it's nice to look at pictures but they've recently transitioned to full on Tiktok/yt shorts competitor, I'd love to jump ship to something that's just photographs again. Never tried lemmy, are the separate servers essentially \"subreddits\" ? That's cool that there aren't any global admins so each community really can have their own moderation rules. reply n_plus_1_acc 11 hours agorootparentA server can have many subreddits (called communities). You can obviously subscribe to a community with an account from any other server. reply jazzyjackson 8 hours agorootparentlol this was not at all obvious to me at first glance from the framing of 'choose a server', but I guess that's the whole value proposition of activitypub is it doesn't matter what server I started on. reply bityard 20 hours agoparentprevTwitter, but your tweets are stored on a blockchain with smart contracts written by LLMs. In Rust. reply jazzyjackson 18 hours agorootparentI actually liked bitclout, seemed like it would work really well for artists' fundraising for an album release or a tour etc. featured no-fee \"tips\" of all sizes right next to the like button and kept users addicted with day trading mechanics tied to artists' popularity. Money flowed freely across borders and moderation was going to be handled by clients. afaik there was never any rugpull of the base currency, but it turned out to be a platform ideally suited to rugpulling and impersonating celebrities. Plus if you thought drama on Twitter was bad just try pinning your bank balance to individuals' reputations and see how well everyone gets along. reply dools 18 hours agoprevTitle shouild be: \"Welcome to twtxt!\" reply 2muchcoffeeman 20 hours agoprev [–] Why do we even need a microblogging platform? Twitter turned into a dumpster fire. Why will these other alternatives not have the same fate? reply dontdoxxme 19 hours agoparentWe don't. If anything just using HTTP and HTML the way it was designed is fine. See https://indieweb.org reply qudat 19 hours agoparentprevI’m inclined to agree. These post platforms are a race to the bottom reply pjc50 9 hours agoparentprevThese things are good or bad because of the community of people who use them, not the technology per se. And that community is shaped by both culture and moderators. reply jazzyjackson 7 hours agorootparentI disagree. The shape of the instrument makes some sounds easier to make than others. Pianos are inherently polyphonic while trumpets play one note at a time. Anything made in the shape of twitter will result in twitter like behavior. reply openrisk 10 hours agoparentprevThe original Web was the work of genious, but everything since is either adtech enshittification or cumbersome hacks. The \"simplest\" requirement - decentralized discovery of what is out there, is essentially still unsolved. That is one of the main reasons we have centralized platforms of all types instead of some RSS on steroids design. The second (and more difficult in sociopolitical terms) requirement that is still unsolved is the \"active\" web, decentralized POST-in on somebody else's server. Here you have social challenges (identity, spam, moderation, fake news etc). We really need a good society adapted Web 3 evolution, because Web 2 has been a disaster that keeps on giving. But it will require genious at least commensurate with the original. reply skydhash 5 hours agorootparentHow do you know what’s happening elsewhere? Other than having relations, it falls under reporters to propagate news. How do you meet new people? You go to special events and gatherings. The web is already linked, but we have special nodes like search engines, directories, and forums that are information hubs. Creating a website was always easy. The minimum html you need is very small, and all you have to do is copy the files with an ftp client. Then tools like wordpress came and it became even easier. What social medias have done was to put everyone in the same space. First there were walls and you just have to build your own information hub. Now, the platform is providing you its own like a private television service (including ads) whether you like it or not. No need to invent a new version of the web, we can always go back to what was working and is still working. reply andreygrehov 18 hours agoparentprev [–] > Why do we even need a microblogging platform? Diversity of options is great. > Twitter turned into a dumpster fire I disagree. I love the new Twitter. reply ksenzee 14 hours agorootparentYou must like it for political reasons, because from a technical standpoint the new Twitter is a catastrophe. It doesn’t even work properly. The other day I typed in the URL of an account I wanted to check, from a browser where I wasn’t logged in, and I got an endless loop of redirects. Watching Musk tear down Twitter infrastructure over the last two years has been like watching the Notre Dame fire, except if it was set on purpose. It was a miracle of human accomplishment and now it’s a shell of its old self. reply andreygrehov 1 hour agorootparent> It was a miracle of human accomplishment and now it’s a shell of its old self. You must dislike him for political reasons, because from an accomplishment standpoint he is still a miracle. reply t-3 11 hours agorootparentprevThat was the case long before Musk took over. Twitter has always been an observer-hostile site, that's the whole reason why nitter and co exist. reply velcrovan 16 hours agorootparentprev [–] What do you like about it? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "twtxt has released version v1.3.2-dev, a decentralized, minimalist microblogging service aimed at hackers.",
      "The documentation includes sections on Installation, Quickstart, Usage, Configuration, and API, with community support available on IRC.",
      "The API chapter details endpoints for users, tweets, mentions, tags, and discoverability, encouraging contributions on GitHub."
    ],
    "commentSummary": [
      "Twtxt is a decentralized microblogging service designed for hackers, known for its minimalist and simple interface.",
      "Users highlight challenges such as finding active sites and the lack of features like visual imagery, though some appreciate its simplicity and niche appeal.",
      "The community remains small, with some users transitioning to other platforms like Mastodon or Nostr, yet Twtxt provides a unique, self-hosted alternative to mainstream social media."
    ],
    "points": 290,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1734900704
  },
  {
    "id": 42490343,
    "title": "Predictions for 2025?",
    "originLink": "https://news.ycombinator.com/item?id=42490343",
    "originBody": "2024 has been a wild ride with lots of development inside and outside AI.What are your predictions for this coming year?",
    "commentLink": "https://news.ycombinator.com/item?id=42490343",
    "commentBody": "Predictions for 2025?237 points by uncomplexity_ 18 hours agohidepastfavorite470 comments 2024 has been a wild ride with lots of development inside and outside AI. What are your predictions for this coming year? quantisan 16 hours agoPrevious years: 2024: https://news.ycombinator.com/item?id=38777115 (the rest of this list is copied from https://news.ycombinator.com/item?id=38779963) 2023: https://news.ycombinator.com/item?id=34125628 2022: https://news.ycombinator.com/item?id=29746236 2021: https://news.ycombinator.com/item?id=25594068 2020: https://news.ycombinator.com/item?id=21802596 2019: https://news.ycombinator.com/item?id=18753859 2018: https://news.ycombinator.com/item?id=16007988 2017: none? 2016: https://news.ycombinator.com/item?id=10809767 2015: https://news.ycombinator.com/item?id=8822723 2014: https://news.ycombinator.com/item?id=6994370 2013: none? 2012: https://news.ycombinator.com/item?id=3395201 2011: https://news.ycombinator.com/item?id=1970023 2010: https://news.ycombinator.com/item?id=1025681 reply esperent 14 hours agoparentThere's a second 2024 thread here: https://news.ycombinator.com/item?id=38614465 Reading through the two 2024 threads I'm struck that most predictions a) were completely wrong b) reflected more what people wanted to happen rather than what was likely. Edit: now that I've read through this thread I'll add c) were much more hopeful than this year's predictions. reply EFreethought 12 hours agorootparent> reflected more what people wanted to happen rather than what was likely. I think that is true of most predictions. reply locallost 9 hours agorootparentprevIt reflects the mood, which tells me all I need to know about things like short term stock changes. In reality nobody can predict the future. My predictions have historically also been way off, but what is surprising to me is that people don't reflect on their misses. For the most part I've given up in predicting anything, at best I will look at trends and see if there's something there or not. But with that said I will now post my predictions for 2025 :-). reply WA 6 hours agorootparentPost your portfolio alongside please :) reply bobro 12 hours agoparentprevCrazy how wrong people were in the top 2024 comments. reply skatanski 5 hours agorootparentCould it be the top comments even though most voted are also most commented and more divisive and controversial? And the most boring comments are somewhere at the back? Hence big amount of failed predictions at the top. reply kyriakos 12 hours agorootparentprevIndeed someone predicted vision pro will be wildly popular. reply dominicrose 5 hours agorootparentYes. People are not stupid. It's one thing to have a phone in your purse. It's another to have something on your head and eyes, like how is anyone going to agree to that? Sure if it was very cheap you'd find some interested folks. reply Lionga 10 hours agoparentprevbetting on the opposite of what HN predicts seems like a pretty safe bet if you read the old threads. reply monkeydust 9 hours agorootparentReminds me of a running joke with a friend. His stock picks are so off I told him to create an inverse etf of his picks and I would pile into that, at least one of us would make some money. reply CubsFan1060 18 hours agoprevThere are even more restrictions aimed at stopping kids using social media. Restrictions on phone use in schools, who can sign up for social media, etc.. Additionally, that it'll eventually prove to be a wild success, with significant benefit to kids. On the darker side, the same technologies and restrictions will be applied in various ways to adults (similar to the porn verification laws), which will have significantly more negative effects. reply thepuppet33r 16 hours agoparentI'm still not sure how they plan to enforce this. Even now, I see an age verification popup on both illicit sites and even sites I feel are innocuous. But I can just click that I'm old enough to move on. If the social media companies are only trying to shift blame, this makes sense. They're not liable if the customer lies. But if that loophole is closed, the only way to enforce age approved sites would be a global identity system that is somehow inextricably linked to your real-life persona. Everything you do online is linked to who you are. And that's VERY dystopian and doesn't (yet) exist to my knowledge. reply bruce511 15 hours agorootparentThere's a parent-driven approach I'm hearing about more and more. Parents in a school get together and agree on smartphone accessibility. For example \"no smart phone till high school, dumb Nokia for comms if required\". This is hyper-local but works well because there's no peer pressure- nobody has one etc. The other effective approach in play is \"no screens in private areas\" - ie no screens in bedrooms and bathrooms. This also has very beneficial outcomes on kids, and seems to be gathering steam. I think govt type bans are easily circumvented, and basically useless. But parental rules, especially if common in the child's social circle, are proving to be a good starting point. reply _DeadFred_ 14 hours agorootparentSounds like a business opportunity. Make a token (Bluetooth low energy token?) that parents can place in the location where devices can be used. Either have it baked into the hardware's software (so Apple would need to support it) or sell a Wifi router that only allows the 'screen' devices to connect to wifi when the token senses the device (which would make it work for any device with wifi and Bluetooth token's ranges). reply thepuppet33r 13 hours agorootparentprevMaybe I was just a bad kid, but if my parents had done something like this, my friends and I would have pooled our cash and bought a used phone. That wouldn't invalidate this and it would still be better, but just FYI. Any parent-driven solution would be seen as the parents being ridiculous and unfair by the kids, at least at first. reply bruce511 3 hours agorootparent>> my friends and I would have pooled our cash and bought a used phone. one phone shared between a bunch of friends is not the problem. The problem is a phone \"owned\" by you, and then used to access social media. ie the phone is just a conduit to social media, and social media is the root of the problem. >> Maybe I was just a bad kid, but if my parents had done something like this, my friends and I would have pooled our cash and bought a used phone. Cigarettes and alcohol were (and still are) banned from kids, and yes kids certainly got them when I was growing up (and I'm pretty sure still do.) That doesn't mean those things should be accessible to kids, used at the dinner table, or in bedrooms at home. >>Any parent-driven solution would be seen as the parents being ridiculous and unfair by the kids, at least at first. This is not a suggestion I am proposing. It's an approach I'm seeing being implemented, and the kids are better off for it. Given the very clear harms we are seeing with children using smart-phones, and social media, for the last 15 years or so, I expect this will gain momentum. Clearly you can parent your own kids however you like - I'm just reporting on what I'm seeing. reply etimberg 5 hours agorootparentprevGovernment backed SSO is probably how this ends up. I suspect we get to it because it also solves the election influencing issues that western democracies are seeing reply thrwthsnw 2 hours agorootparentWouldn’t solve anything, social media would just fragment even more. People will move to unregulated sites. Full on dictatorships can’t even prevent people from using the internet so how does anyone think this would work? reply hahn-kev 14 hours agorootparentprevAirbnb has a similar kind of verification (which makes a lot of sense in their business). It's a pain but it would obviously not be hard to adapt it to Facebook. But as for what to do for existing accounts? That's where it gets hard. It would probably cut down on spam too though. But yeah the privacy issues are a huge problem. reply jauntywundrkind 14 hours agorootparentprevQuite unfortunately imo Google is full steam ahead building the Digital Credentials API, a standard way to have the browser present verifiable identification. Paving the way for the most ghastly intrusion of governments onto the internet; what a horrible thing to do to the internet! > allows websites to selectively request verifiable information about the user through digital credentials such as a driver's license or a national identification card stored in a digital wallet. https://developer.chrome.com/blog/digital-credentials-api-or... reply exitb 13 hours agorootparentprevYou’ll be asked to log in into a government-run account, which will pinky promise not to store who accessed what. reply dTal 1 hour agorootparentThat's not a technical requirement; you could simply have a government-verified official public key. You could \"log in\" by signing a challenge message with your private key. The government would have no log that you'd even used your official government ID. reply netsharc 12 hours agorootparentprevIt'll be your mandatory X account, the one you'll need to do things like contact the DMV, because hey, can't have government efficiency without grift! reply swiftcoder 9 hours agoparentprev> with significant benefit to kids Straight kids from stable homes, perhaps. For LGBT kids, or kids in abusive homes, it's the end of a significant lifeline. reply nindalf 9 hours agorootparentIt's interesting yet saddening to play out the consequences along the path you've outlined. LGBT kids, by definition a 2-4% minority, are unlikely to find other people like themselves. They remain closeted for longer, trying and failing to conform. Their struggle will be unfortunate, but also not measurable. What will be measurable is the % of youth who identify as LGBT, which will be lower because more of them would be closeted. That'll be a win for people who want to \"protect kids from LGBT ideology\". reply ranyume 1 hour agorootparent> That'll be a win for people who want to \"protect kids from LGBT ideology\". That'd be dumb and funny if politicians wouldn't abuse people's fears to boost conspiracy theories for their agenda. For example, where I live some politicians associated the trans community with paedophilic behavior. I don't understand their endgame really. Where these anti free choice (anti-freedom) stances come from. reply nightowl_games 4 hours agorootparentprevThe root issue here is children not coming out to their parents. If we improve that metric, we can begin to improve all other metrics. Ie: the parent can help the child discover appropriate peer groups reply swiftcoder 15 minutes agorootparentThe root issue is parents not being safe for kids to come out to. If you risk being chucked out on the street, or sent to a conversion camp, why the hell would you come out to a parent? reply wolfgang42 59 minutes agorootparentprevThat requires the parents to be accepting and engaged, which is a great long-term goal but will take much longer to achieve than the first-order effects of losing access to peer communities. (There are also feedback loops: a group being less visible means that people in the group are less likely to realize it isn’t something uniquely wrong with them, there are fewer other people who know fact from myth, and so on.) reply abetusk 14 hours agoprev* Solar energy will account for around 8% (give or take 1%?) of the worlds energy usage * We'll see the worlds first trillionaire * Bitcoin will reach $200k+, and remain largely stable around that price, at the end of 2025 * Generative AI for music will continue to improve substantially. I have a lack of imagination, but maybe something like on demand streaming services, maybe targeted to niche music genres (lo-fi, electronica, elevator/hold/office music) * Generative AI for video will continue to improve substantially. The best I can come up with is that there will be a breakout indie film or music video that's produced from a skeleton crew relying heavily on generative AI video. * LLMs will continue to improve substantially, being able to solve more and more complex tasks, like the Putnam exam and others. Research will continue to try and integrate LLMs into a toolchain to improve performance * LLMs and other generative AI tasks will continue to become more and more accessible ($2.5k for a machine able to do fairly advanced training?) * The cost of robots and other robotics will drop substantially, providing a reasonable bipedal option at $8k * Twitter and Facebook will still be around, Bluesky will be no more, Mastodon will continue to be niche * All the above will be used by people to invent and discover weird, wonderful and horrible things that I can't even imagine. reply mdaniel 1 hour agoparent> Twitter and Facebook will still be around, Bluesky will be no more, Mastodon will continue to be niche I'd be curious to know if you have an expectation for what will cause your cited Bluesky death. About 6 months ago I would have probably backed a prediction of \"will remain niche\" for it just like your Mastodon one, but now I am following some relatively big name accounts that have migrated over. And I undoubtedly enjoy the experience a lot more than X reply benjaminwootton 14 hours agoparentprevI think you’d need an almighty market rally for some of these. For the first trillionaire we’d need to see Tesla double and the market would need to be going bananas for Bitcoin to 2x. Remember 2024 has been a monster year in the markets and they just indicated they are slowing down the rate cuts. reply WXLCKNO 1 hour agorootparentI guess at the current market cap you may be right about 2x being \"a lot\", but the crypto market is no stranger to multiple increases. And the valves are fully open now with a ton of Bitcoin/Crypto ETFs across the planet and in the US in particular. Even at these prices it won't take that much to raise the price drastically. reply Imustaskforhelp 13 hours agorootparentprevI know right , I think he might be too bullish on bitcoin for it to double , though I may be wrong but it just feels like there would be swings because I still believe that there is no use of bitcoin , Its just that its the one which started these coins and there could be better one for use cases , maybe like IDK monero for real anonymous transactions , but even then these are more of a sidekick thing which I don't think I would ever use but I still like the idea of monero , I really really stay away from crypto. I like index funds so much , that everything else feels a time / effort waste even though I am aspiring to be a programmer , maybe that's partially why I don't see much reason in crypto. I just think of index funding 80% + bonds 20% or maybe 100% index fund into my own country index ,and maybe having some backup money in need , probably in a liquid fixed deposit or having multiple bank accounts and giving them all the maximum amount of money which is guaranteed to be given to me if a bank fails by the govt. , probably 10k$ is enough in my country or maybe I am still over estimating. reply Lordarminius 13 hours agorootparent>I like index funds so much , that everything else feels a time / effort waste even though I am aspiring to be a programmer , maybe that's partially why I don't see much reason in crypto. Crypto is the future of money and finance. If you are aspiring to become a programmer you should internalize this and get in. reply pjerem 12 hours agorootparent> Crypto is the future of money and finance. Why ? Crypto is the digital version of the old commodity money (whose value comes from a commodity of which it is made). Turns out it was absolutely impractical because people rarely want to be physically responsible for their own estate. I neither see crypto disappear anytime soon and we may see innovative usage. It can probably make sense in unstable economic environments. But everywhere else, you’ll want your estate not being accessible to burglars, you’ll want your big transactions to be legally secured, you’ll want your bank to be able to reverse things and you’ll want to not let your money sleep. Also, unless governments start accepting crypto for paying taxes, you’ll have a hard time buying anything in daily shops. reply abetusk 14 hours agorootparentprevMusk's net worth was $137B in 2022, $270B in 2023 and now estimated to be $400B. $1T is a bit of an extravagant prediction but it's within the realm of possibility. And it might not be him. Bitcoin was around $43k at the end of 2023 now at approximately $100k. reply wtkd 12 hours agoparentprevagreed on the uptick in generative AI for music but I think it will just start to replace regular music on the major streamers rather than crop up in a new beskpoke platform. it's already happening: https://harpers.org/archive/2025/01/the-ghosts-in-the-machin... reply lizzas 14 hours agoparentprevAugustus Caesar was a trillionaire apparently (in todays money) reply andrewinardeer 14 hours agorootparentJakob Fugger reply uncomplexity_ 13 hours agoparentprevbanger. on real life image and video intelligence though i think google will take the lead here as they just have monopoly on dataset here given they have google images and youtube. on proprietary general intelligence ai i think openai and anthropic will keep their lead, while on open source mark zuckerberg will be continuously leveling the playing field so the general population can keep up in terms of availability of access and end-result productivity. something to keep eye on though is china. have you guys seen their recent open source models? very impressive. looking at it, it seems like a head to head battle between USA and China. Europe seems heavily distracted with its regulations and politics. > I have a lack of imagination, but maybe something like on demand streaming services, maybe targeted to niche music genres (lo-fi, electronica, elevator/hold/office music) goddamn that would be interesting. it's gonna be great to see lots of micro communities offering better content than netflix/disney/hbo etc which is prone to political and non-political influences and slow-downs in production. > Generative AI for video will continue to improve substantially. The best I can come up with is that there will be a breakout indie film or music video that's produced from a skeleton crew relying heavily on generative AI video. i wonder how much of the industry will transition into pure prompting too. back then we used manual instruments and talented voice actors for voice, these days you could (possibly) produce similar results if you're skilled enough to precisely describe it to the ai you are directing / working with. > The cost of robots and other robotics will drop substantially, providing a reasonable bipedal option at $8k huge bet on this too, the brains are getting better and for sure yc's request for startup will eventually have a section dedicated for the body and limb part targetting different industries. reply abetusk 13 hours agorootparent> ... i think google will take the lead here as they just have monopoly on dataset ... I think there's plenty of data for people to train substantial models, either drawing directly from the public domain or scraping content. The cost of compute and disk space will continue to drop exponentially fueling the ability of small businesses and amateurs. Compute is still following Moore's law, more or less, when you allow for GPUs. Hard drives are relatively on the same path, with maybe a price halving every 3 years or so. > ... while on open source mark zuckerberg will be continuously leveling the playing field so the general population can keep up in terms of availability of access and end-result productivity. Meta's offering is not open source. I agree that all the big players will make substantial moves in this space but I'm much more interested in the niche models that are actually libre/free/open source. I suspect FOSS will eventually eat all the big players lunch but I don't have a good read on it (nor do I know anything about China's OSS models). > it's gonna be great to see lots of micro communities offering better content than netflix/disney/hbo ... It sounds like a \"Black Mirror\" episode but I can't wait (and I don't think it'll be as bad as \"Black Mirror\"). > i wonder how much of the industry will transition into pure prompting too ... My read on this is that we're in the \"experimental art house\" phase of this type of generative AI. So many people create weird things, experimenting with the technology but ultimately having low expectations in terms of what gets produced. Sometime soon (1-2+ years?) my guess is we'll see finer control with someone, either an actor, director, etc. that can provide dialogue, facial expression and body language with minimal setup, a camera or voice recording, say, that can make or refine avatar/agent performance. I would guess this would be available to specialized visual graphics shops but will eventually bleed out so that amateurs have access. I think we might be seeing this available already. Eventually, we'll have \"make what I want\", but I would imagine that's 5-10+ years away. reply gregw2 5 hours agorootparentprevRegatding China recent AI advances, I would add to your point... I think China has more Chinese-language datasets that the West will find hard to get and train for effectively, than the West has in terms of its English datasets which both parties can exploit. Additionally China has greater volume of 'less expensive manpower' and greater coercive power to create synthetic datasets than what we will see coming to bear from the West. I haven't seen other people point this out. The next few years will be intetesting times. reply wombatpm 13 hours agoparentprevI think if you count animation we’re probably real close going from comic book series to animated feature. reply Lordarminius 13 hours agoparentprev* Bitcoin is on a trajectory to reach 200k and it will if we get a strong bull season. However the elephant in the room is the House of Cards that is the US financial system. It appears primed to go poof! * Solar and LLM's will bloom. reply throwaway2037 13 hours agorootparent> the US financial system. It appears primed to go poof! Can you explain more, and provide some specific examples? Are you talking about commercial banks, investment banks, or insurance companies? (They make up the bulk of what most people mean when they say \"financial system\".) Post-2008 GFC, ibanks are stronger than ever because they are much more conservative. With the exception of a couple of run-on-the-banks, commercial banks have been more stable than ever in the last 50 years. Similar for insurance companies. reply Lordarminius 12 hours agorootparentUnsustainable public debt. $50 trillion in debt maturity must be rolled over mid 2025. Something must give reply azinman2 12 hours agorootparentCurrently the US has 36T in dept [1]. Where are you getting this number? [1] https://fiscaldata.treasury.gov/americas-finance-guide/natio... reply doku 9 hours agoprev90% confidence level, Hottest weather in recorded history. Extreme weather in many places not seen before. No one is prepared for this commonplace outlier, Globally common, locally outlier. Huge insurance payout but most not fullfilled. 90% CL: Top search engine results completely inundated by LLM text, we none the wiser. SEO solved. Search engine switches to new algorithm based on LLM. 90% CL: AI with tool use that has access to physics simulator and CAD software will automatically advances efficent engine design. 70% CL: Million dollar prompt run on million dollar math prize. Millennium Prize Problems 40% CL: AI sucessfully held back invasive species encrochment with lessons from AI war machines 30% CL: Cheap AI nose sniffs out many common diseases. Automatic data analysis of Gas Chromatography and spectroscopy readings on patients' VOC(volatile organic compounds) produces fast and cheap diagnosis. 10% CL: Not next year, Cost of renewable electricity in remote area becomes cheap enough to pull oxygen from air or electrosis from water to be sold as commodity. Or other processes to package solar energy for trade. reply defrost 9 hours agoparentRenewable energy has been the cheaper option in remote areas for a while now, solar demand in Pakistan has skyrocted recently. Solar powered water pumps and water purification has been a commodity and charity item since 2023 (that I recall) eg: https://nrsp.org.pk/hand-operated-and-solar-powered-portable... https://www.bondheshams.org/ reply HDThoreaun 2 hours agorootparentI read a piece about how abundant solar in rural Afghanistan strengthened the Taliban as poppy and therefore opium production skyrocketed because they were able to install electric wells. reply garbageoverflow 5 minutes agorootparentinterested in the article. would you like to share a link? reply fnqi8ckfek 8 hours agoparentprevHow do those confidence levels work? It looks to me you're just assigning probabilities to outcomes. reply pjc50 8 hours agoparentprev> Cost of renewable electricity in remote area becomes cheap enough to pull oxygen from air or electrosis from water to be sold as commodity Oxygen is already kind of a commodity. Did you mean hydrogen? Anyway, this isn't going to happen, look at retail power prices. > Top search engine results completely inundated by LLM text, we none the wiser. SEO solved. Search engine switches to new algorithm based on LLM. I'm not sure in what sense SEO is \"solved\" here, but this basically puts the Internet into a nasty local minimum where it's kind of good enough for information you don't care about, but will feed you wrong information often enough to cause problems, while having stripped all the traditional reliability signals. Possibly also kills what's left of fact-based journalism. reply qznc 9 hours agoparentprevI don’t think global temperatures get another record because El Niño is over. Your „weather“ suggests a more local reference though? reply 01jonny01 7 hours agorootparentIt sounds more clever to use Confidence Level, rather than just saying you are pulling random probabilities out of your a$$ reply dartos 7 hours agoparentprevReally bullish on AI, huh? Designing an engine? “[snuffs] out common diseases”? Solving SEO? Idk cotton, let’s see how this plays out. reply doku 1 hour agorootparent\"Solving\" is a bad choice of word. SEO does not have a solved state. Digital design of engines can already happen now. AI chip design is already a thing, I think they help lay out more efficient trace lines. Since digital design and simulation can all happen digially. The whole cycle is digital. A system like Claude's computer use can take control and produce 100s or thousands of small modification of existing design to be simulated and if it has testable performance critiera. I think it can be done now. Maybe some finetuning the model for specific UI control. reply 0-_-0 7 hours agorootparentprevSniffs out, meaning diagnoses reply dartos 5 hours agorootparentCould just as well be a typo. I’ve never heard anyone call diagnosing “sniffing” reply feznyng 5 hours agorootparentIt’s an idiom, like the detective sniffed out some clues. reply doku 1 hour agorootparentlol, I do mean a sniff with a nose. The technology I'm referring to can work by taking air sample. This technology exist, the prediction is that cheap version will be available. My assumption is robots will want a digital nose, for example nanny bots will want to smell smoke and react. An example I read for this technology is that grocery store robot will want to detect spoiled food. It will be mass produced and gets cheaper. But we don't have a robot industry yet, so my confidence level on this prediction is low. The AI part of this is just data analysis of spectral lines. reply Riseed 1 hour agorootparentprevDisease can change body odor, and dogs have been trained to \"sniff out disease\" [0][1] (i.e. detect via smell) for years. Various researchers have been working on robot noses that would be able to do the same.[2] Presumably a widespread and/or less expensive version of this \"robot nose\" is what doku means by \"AI nose sniffs out many common diseases\". [0] https://www.nature.com/articles/d41586-022-01629-8 [1] https://www.discovermagazine.com/the-sciences/how-do-dogs-sn... [2] https://bigthink.com/the-future/robotic-nose-smartphone-dete... reply Lionga 9 hours agoparentprevcan i have some of what you have been smoking? reply rakkhi 8 hours agoparentprevI did my top 5 for my field cyber security AI in cyber defence and attack Quantum resistant cryptography Death of the password? Passwordless, passkeys, phishing resistant MFA More suppliers compromised, another major open source project long con Doing more with less: automation and consolidation https://rakkhi.substack.com/p/cyber-security-predictions-for... reply stogot 8 hours agorootparentThose are generic enough you could argue they already occurred last year or this year? reply rakkhi 8 hours agorootparentThere is a lot more detail in the article that makes not generic reply waihtis 3 hours agorootparentprevThey have both occured and not occured every year for the past 10 years reply nozzlegear 15 hours agoprevI predict that I will finally finish making that bird feeder camera that I've been wanting to make for my wife. I've got all the parts for it, I just need to figure out how to put them together and write some code to take a picture of a bird. Can't be too hard, right? reply WXLCKNO 1 hour agoparentA friend of mine just bought this for his mom, I thought it was really cool apparently it can recognize if the same bird comes multiple times and stuff like that https://www.birdfy.com/products/birdfy-feeder But I just saw it has a subscription fee lol. I understand but no way would I get that. reply nozzlegear 1 hour agorootparentYep, that's what inspired my project! My wife is a wildlife photographer, and taking pictures of the squirrels and birds at our feeders is what started her down that path. I had wanted to get her one of those feeders, but they're all subscription based and send the photos to their cloud/AI services. Like you said, I understand it. It's a really neat product for people who enjoy birding, but I want to try my hand at building something that can run off a Raspberry Pi and sync to my local NAS instead of a third-party cloud service. reply wenc 15 hours agoparentprevSounds like a great project for learning. However, just FYI, this $25 2K cam (PCMag recommended) can do that sorta (it has motion/pet detection) and more. It also has excellent night vision. https://www.amazon.com/dp/B0CH45HPZT/ I was like, a $25 camera can't be this good. But I tried it out and it's actually really good. reply alibarber 9 hours agoparentprevHmm - but you can't just have it take a picture of the bird! It needs to alert you to the presence of the bird - what's the latest and best practice in notifications nowadays? And ID the bird too - what's a good API for image based AI? Might be expensive, better run it locally, how do we set up a sever for that? No, not server, serverless! What are the legal implications of running an AI based autonomous camera service outside in your locale? Better research that... .... Yeah, I've ended up going down a lot of rabbit holes on my own little projects, but that's part of the fun I guess. ;) reply andrei_says_ 14 hours agoparentprevIt’d be easier if you’d be OK with a picture of a bird or picture of a squirrel ;) Good luck, sounds like a fun project. reply dachris 9 hours agoprevIndustrial civilization will continue to follow along the \"Business as Usual\" trajectory of the 1972 Limits to Growth publication [0]. [0] https://en.wikipedia.org/wiki/The_Limits_to_Growth#Compariso... reply mytailorisrich 8 hours agoparent\"is a 1972 report that discussed the possibility of exponential economic and population growth with finite supply of resources\" The main problem is that for many reasons people tend to focus on the economic growth part while the population growth part is the killer. reply tonyedgecombe 7 hours agorootparentPopulation growth is a solved problem. We have already passed peak births. reply mytailorisrich 7 hours agorootparentIt is absolutely not a solved problem: * Population is probably (it's actually a certainty in my mind) too high to be sustainable and it is still growing, * Governments are terrified of population decrease because of the far-reaching effects on markets and public finances and are pushing for growth. So it is a \"less solved\" problem than emissions as the IEA predicts that global fossil fuel demand will peak by 2025, possibly even this year. It might be solved in some countries in the sense that population should now be naturally decreasing because of the low birth rate but it is in fact \"not solved at all\" considering the second bullet point above. reply oulipo 9 hours agoparentprevExcept if there are Luigi-style protests? (sarcasm... or not?) reply JKCalhoun 3 hours agorootparentNot likely. Things have to get much worse. reply andy_ppp 18 hours agoprevDopamine management medication will lead to a renaissance of human ingenuity and scientific discoveries and a big crash for social media and gambling sites (and in fact most online activities) but everyone will start becoming more robotic and even less social. Think GLP-1 inhibitors except for thinking more deeply and for longer periods. If not 2025 very soon. reply drcwpl 8 minutes agoparentConcerning \"thinking more deeply and for longer periods.\" There is a WSJ article about drugs for focus but how addictive they are \"As Mark Moran was facing another 90-hour week as an investment-banking intern at Credit Suisse in New York, he knew he needed help to survive the rest of the summer. His colleagues gave him a tip: Visit a Wall Street health clinic and tell the staff he had trouble focusing. Ahead of his first appointment, he filled out a five-minute questionnaire. One of the questions asked if he had trouble staying organized, another, if he procrastinated. He then met with a clinician who said his answers suggested he had attention-deficit hyperactivity disorder. He left with a prescription for Adderall.\" https://www.wsj.com/finance/banking/young-banker-finance-adh... reply uncomplexity_ 14 hours agoparentprevhaha this is gonna be an interesting one for sure. it's crazy how us humans yearn for magic pills for the highs and the lows just to get ourselves in a healthy baseline and as bryan johnson's experiments look like, it all comes down to our consistency in eating healthy, exercising regularly, and resting enough - those three alone will net you better health than most people - and it's hard to get into habit of doing those since we all got distractions around us. reply WXLCKNO 1 hour agorootparentFully agree on consistency, unfortunately I realized after a while that my anxiety was preventing me from being consistent and after getting some medication, I was able to get back into all these habits. reply netdevphoenix 9 hours agorootparentprevI had a discussion with someone on here about the magic pills and how they are a crutch for the 3 items you mentioned. They ranted about how helpless humans are and how unfair it is for them to have will themselves to be healthy reply owenpalmer 13 hours agorootparentprev+1 for mentioning Bryan Johnson reply doku 11 hours agoparentprevI feel the dogged persuit of a task and obsessive curiosity might be more closely related to addiction than to self control. A good balance of productivity from the self control and the obessive curiosity of human nature might be needed for ascension reply impostervt 18 hours agoparentprevDo GLP-1 already do this, to some extent? reply inerte 15 hours agorootparentI've heard many reports of GLP-1 reducing addiction behavior, but more like alcohol and gambling. Never heard anyone reporting on social media reduction. IMHO it looks like it should, I just haven't seen any data / anecdotes. reply johnfn 18 hours agoparentprevVery cool prediction, but is there anything like this actually coming down the pipe? reply OutOfHere 18 hours agoparentprevAntipsychotic medicines already are dopamine antagonists, but they lead to substantial weight gain, and they hurt motivation. reply philwelch 18 hours agoparentprevThese already exist, they’re called CNS stimulants. reply morkalork 18 hours agorootparentSounds like they're describing the opposite of CNS stimulants though. People take meth and coke and go on se, gambling binges. Something that moderates and manages dopamine in a way that breaks the hold gambling and social media has on people's brains would be very different. Something that dulls the retative mini-bursts of dopamine hits you get with every click and scroll. reply reshlo 15 hours agorootparent> People take meth and coke and go on se, gambling binges. Something that moderates and manages dopamine in a way that breaks the hold gambling and social media has on people's brains would be very different. Simplifying a lot here, but people with ADHD are statistically more likely to exhibit addictive behaviours because the addiction provides dopamine that their brain otherwise has trouble getting. Treatment with prescribed CNS stimulants is very effective at preventing those behaviours because the brain no longer has to engage in the addictive behaviours to obtain sufficient dopamine. Here’s one study where patients reported significantly lower rates of alcohol and drug use over at least a one year period. https://pubmed.ncbi.nlm.nih.gov/23264367/ reply idiot_predictor 13 hours agoprev* One or a few of the faangmanga-ish leaders will retire (or forced to by the board) and the new leader will be from within (likely Tim Cook for age reasons but could equally likely be Sundar Pichai for non-age reasons) * LLMs (for non-coding tasks) will likely fizzle out as expensive talking fidget spinners and not the world saviors that the companies behind them envision them to be. AI will go back to being fun and exciting again and not the delight of both Wall Street and the ‘shoeshine boy’. * Cloud egress costs will be heavily scrutinized and competed upon as transferring data no longer becomes a competitive advantage. * Apple or someone like Apple will take advantage of cratering storage/compute costs by moving things off cloud to a locally owned ‘box’ that pairs with a new dumb-client/thin phone (backups, massive storage, running some compute on this box, sharing/storing ‘family’ stuff, streaming games or movies etc) also accessible via a reverse proxy (vpn) from anywhere. * A combo of aibo/roomba/ring like device that goes about or rolls around your house and does things for you… kinda real life flubber. * Selling users’ data goes out of fashion (goog/meta) as new companies use privacy as a competitive advantage to sell ads. * Finally games actually have AI that doesn’t suck - a new class of AI games that would challenge and delight humans like never before. reply disambiguation 1 hour agoparent+1 for AI games. I'll add to it that we'll see more gamification of AI training as well. I.e your play data becomes training data. reply tasuki 11 hours agoparentprev> LLMs (for non-coding tasks) will likely fizzle out as expensive talking fidget spinners and not the world saviors that the companies behind them envision them to be. 100% wrong. LLMs are wildly useful in daily life, even the cheaper models. reply ThatCreeper 11 hours agoparentprev1. I don't know 2. I think this is probably right-ish. From what I hear LLMs are nearing what they do and I have never found any interesting use other than just laughing at them (which I could do with markov chains). AI-generated images are hideous and I have no opinion on AI-generated music as I just choose not to expose myself to it 3. Again, don't know 4. God I hope so. That would be such a funny trend. Plan9 comeback??????? #define nil (void*)0 in common parlance??? 5. I feel like this is exclusively a novelty but what do I know 6. No it will not??? 7. This will probably happen, but it won't be many games. It will not be a new class of games. AI--big shock--isn't fun. Mario but the levels are incomprehensible. BALATRO but I CONVINCE the GAME to give me INFINITE MULTIPLIER? (Part 3). The Beginners Guide but no one cared enough to tell a story. AI so far has been a hindrance where high quality work is concerned. Why will that change? The paint bucket tool has been slowly improved over time, but it was helpful when it first came out. How many people do you know that have played the AI-generated Minecraft thing more than once or twice? It's interesting but utterly bland and bizarre. You're constantly having to tell the program over and over again what it means to be a person when you're in a world which doesn't even know what bird is in the sky. It gets infuriating. The idea comes back again and again, though. \"What if we made a game with infinite content so it could appeal to everyone? And then what if we added multiplayer?\" I've thought it, you've thought it--everyone has. The important part is realizing that you're wrong. If everyone's had this thought, why don't we have this game? Well, we do. Dwarf Fortress is perhaps the best example of a reality simulator standing at simulating a whopping 42% of everything ever. But if you asked someone to describe Dwarf Fortress they wouldn't just say \"it's a pretty good reality simulator\"--well, they might. Assume here you asked them to give a /good/ description--they would tell you it's a tough, slow game about minion dwarves going around and doing your bidding in order to build a civilization which will eventually fall. And they'd tell you that it's fun because of its deep interactions. But now we've lost the plot. It's not a game for everyone anymore, because now it's only for people who like slow games with deep interactions. And it's not a game where you can do anything because you can only do so much as your dwarves can do. But Dwarf Fortress is a fun game. And the fun of it comes from its rules. And rules are what AI is famously bad at. Minecraft is another famous game and the same thing applies to it. \"You can only interact with what your player can see,\" \"one block at a time,\" and \"everything is a full block\" are all rules which bend it away from that \"ideal\" game, but they're all the things that make Minecraft fun. They're all the rules. Games aren't fun when you can say \"well I have better armor\" and get out of a tricky situation. Games are REALLY FUN when you can say \"wait, I have a bag of holding, don't I?\" and then look around as everyone starts smirking at the implication. That's what AI is bad at. There will not be a new class of AI games. reply madphilosopher 16 hours agoprevAuto makers will decide that putting 6 headlights on pickup trucks is no longer competitive. Consequently, they will innovate that the world needs pickup trucks with 8 headlights, starting in 2025. FML. reply esafak 14 hours agoparenthttps://theonion.com/fuck-everything-were-doing-five-blades-... reply bacon_waffle 14 hours agorootparent\"The 8th blade sends an electronic pulse to the center of the brain, which destroys the part of the brain responsible for hair growth and 4 other nonessential functions\" https://www.youtube.com/watch?v=UjAZnGeBcgg reply silisili 14 hours agoparentprevI don't care if they have 100 lights, I just want us to go back to the soft yellow bulbs, instead of whatever we use now that pierces my retinas. reply JKCalhoun 3 hours agorootparentBest you can probably do is to put mirrors on the perimeter of your vehicle. I've noticed that I am quicker to drop my high beams when I approach an 18-wheeler at night with chrome mud-flaps. reply 93po 3 hours agorootparenti bought mirror reflective tint for my back window a couple years ago so that super bright headlights people would get a taste of their own medicine, but decided against it because i didn't want to be a dick just because others were being thoughtless it would have been pretty effective i think because i drive a hatchback with a very vertical back window reply stowaway496 14 hours agorootparentprevThere should be a standard regulation on automakers to provide only yellow light bulbs and limits on lumens. Also, should be law to enforce it is followed and not modified. reply unsnap_biceps 15 hours agoparentprevHonestly, I think the headlight \"bars\" like on the Rivian R1T or Cybertruck is going to be the standard going forward. It gives a better driving experience (to the driver, way worse for the other drivers...) reply bruce511 15 hours agorootparentSome societies optimize for the individual. If 8 lights is better for me then I get 8 lights. The experience gor other drivers us a \"them\" problem not a \"me\" problem. Other societies optimize for public good. It's less about \"individual freedom\" and more about \"what world do we want to live in\". (I'm not talking about govt here as much as just the way people consider their own choices. And yes, there are as*oles in all societies.) When entrenched in one society model though, it can be hard to understand that other models exist (much less than they could suit you better.) reply Silhouette 13 hours agorootparentBut it's very much a \"you\" problem if someone gets dazzled by your excessively bright and poorly aimed headlights and then crashes into you at high speed. It's a weird situation here in the UK. We have newer vehicles sold with the over the top lights that were factory fitted that can legally be driven on our roads. Meanwhile relatively old but still useful vehicles with underpowered lights from a decade or longer ago often can't update those lights even though better replacement parts are available. Our regulations are obscure and antiquated and mean the vehicle would potentially fail its mandatory annual testing because the replacements don't have the right regulatory mark - so the vehicle would no longer be legal to drive on the road even though its lights would be significantly better and safer than what it originally had but not the crazy ones some new cars arrive with. reply viraptor 13 hours agorootparentprev> way worse for the other drivers Light bars selectively disabling some LED segments (as in not blinding other drivers) are already a thing, but... they're not allowed in the US. If the regulation around it comes closer to the EU one, everyone will be better off. reply globular-toast 10 hours agorootparentThe selective disabling thing works in the lab according to some tolerances. On the road they are a nuisance, simple as that. Eyes are sensitive and those things don't work anywhere near as well as they should. reply CrimsonRain 10 hours agorootparentThey work just fine. I can always notice them working for my car and other's cars. reply globular-toast 9 hours agorootparent> I can always notice them working for my car and other's cars. Exactly. Noticing them working is very distracting. There simply shouldn't be the possibility of having such a bright light aimed directly at you from oncoming vehicles ever. reply WheelsAtLarge 16 hours agoprev1) Companies will develop many specialized LLMs linked via a router-like app that determines the best LLM to perform your request. This will yield better results without requiring an AGI. It might even be good enough to replace a specialized information tech job. 2) The cost of a line of code will continue to drop. A Moore's like law is coming/here for code. 3) Trade jobs will start to become what code jobs were in the 00's -- very well paid. Fair warning, coders think about learning a trade like plumbing, electrician, so on... reply gardnr 12 hours agoparentIn case you haven't seen this one yet: Title: \"RouteLLM: Learning to Route LLMs with Preference Data\" Abstract: Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs—by over 2 times in certain cases—without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs. Link: https://arxiv.org/html/2406.18665v1 reply dcchambers 15 hours agoparentprevWhen all knowledge workers and creatives in the US are out of jobs in 2 years because of AI, those trade jobs aren't going to pay well because no one's going to have any money to pay for any work. No new houses being built, no expensive kitchen remodels, etc. reply WheelsAtLarge 14 hours agorootparentThere's more to it than coders losing their jobs. For the last 40+ years, kids have been encouraged to go to college over trade schools. We now have a shortage of professional tradespeople. Desk jobs are easy to outsource and they are now vulnerable to AI. Even if they, plumbers and such, are not the best-paid jobs they will continue to be needed in our society as long as we have people. reply vasco 15 hours agorootparentprevDo you know just recently there was no internet or computers and there were still high paying jobs for smart people? reply JKCalhoun 3 hours agoparentprevTrade jobs are already well paid. Maybe you mean that the rest of us will figure that out. reply benjaminwootton 14 hours agoparentprevI’ve been trying to get a few trades to my house and it’s a nightmare. Nobody wants to turn up or quote, and when they do they just throw out a silly high figure. It has to already be a better option for someone who is aiming for a reasonably good salary. reply someothherguyy 12 hours agoparentprev> 3) Trade jobs will start to become what code jobs were in the 00's -- very well paid. What signals are there for this? reply WheelsAtLarge 12 hours agorootparenthttps://www.mckinsey.com/capabilities/people-and-organizatio... reply someothherguyy 12 hours agorootparentThat transcript for a podcast or whatever states there is demand, not a signal that there is a trend toward significantly higher wages or that employment is growing. There are very different business constraints for hiring software developers vs all skilled tradespeople. reply netdevphoenix 9 hours agoparentprev> Fair warning, coders think about learning a trade like plumbing, electrician, so on... You mean that coders will think about learning a trade? Sounds plausible reply uncomplexity_ 14 hours agoparentprevsmall window for ai workflow and ai agent startups for quick cash and grab as large corporations move slow in this field reply Sloowms 17 hours agoprevMany predictions here are about social media. My prediction is that the first amendment rights they've won themselves are going to bite them in 2025. The first ruling has already happened: https://www.nbcconnecticut.com/news/national-international/t... What this ruling means is that the algorithm is a expression of the first amendment from the platform itself. If the algorithm does harmful things it causes first party liability. My prediction is that the administration will not do much to the social media giants but class action lawsuits will. reply 0x00cl 3 hours agoprevLets get creative - There will be a \"Predictions for 2026\" thread. (Hehehe) - OpenAI announces GPT-5 will be the last model of the GPT-n series (As in they won't release a GPT-6) - Apple will sell a backpack (With some sort of technology included of course) - Magnus Carlsen will lose his 1st place in the FIDE top ranking. (\"He has held the No. 1 position in the FIDE world chess rankings since 1 July 2011\") - E.T. will get a sequel reply WXLCKNO 1 hour agoparentI haven't been following chess much, is there anyone in particular he would love his ranking to? reply 93po 3 hours agoparentprevWhy would apple release a backpack? I would be surprised if there's ever a GPT5. o3 seems like it'd be superior in every way reply yawnxyz 15 hours agoprevA neighbor just got laid off from Adobe, ran huge AI projects there. PM. Program/Project Manager roles go to zero. Trad dev and design roles go to zero. Those roles go to those who also code/design/sell/get viral/make videos/write tutorials/devrel/run communities/answer customer support. It used to be \"do designers learn to code\" or \"do coders learn to design? now it's learn ALL those things, fast, with AI, or hang out in the tendie loin. it's dog eat dog and no one's safe reply drcwpl 7 hours agoprevIQ will continue to decline driven by over-reliance on LLM output and the majority watching video shorts. Nevertheless, long form podcasts will continue to grow, although still watched by less than 1% of the population reply criddell 6 hours agoparentIt’s interesting to me that you talk about watching podcasts. To me, they are still mostly a feed of audio files with some outlier video podcasts gathering big audiences. Am I out of date on that? Is video the primary format for podcasts now? > 1% of the population Human population. I bet they are used for training a significant number of big AIs. reply ac29 1 hour agorootparent> Am I out of date on that? Is video the primary format for podcasts now? No, but many podcasts are recorded and released on video as well. Its not that much extra work if you are already set up to record audio and it gives you another monetizable product from the same content. reply drcwpl 6 hours agorootparentprevYou are right, the 'watch' ones are outlier podcasts, eg Lex Fridman, Rogan, Dwarkesh Patel, Tyler Cowen, The Diary of a CEO, Machine Learning Street Talk, Hoover Institute, and so on, but it is building momentum. reply forinti 6 hours agoparentprevI think we'll probably see many issues provoked by buggy and faulty code made by LLMs. reply eamag 9 hours agoprevFor those curious how well you did last years - I built a table with evaluated predictions some time ago: https://hn-predictions.eamag.me/ reply FergusArgyll 6 hours agoparentCool! You should definitely submit as Show HN reply qznc 9 hours agoparentprevWow, did you assess all of them? reply eamag 8 hours agorootparentI've just queried all posts from these threads and ran an llm over predictions, see https://eamag.me/2024/HackerNews-Prediction-Evaluator reply whycome 18 hours agoprevPredictions from 2008: National Intelligence Council/Director of National Intelligence - Global Trends 2025 https://www.dni.gov/files/documents/Newsroom/Reports%20and%2... reply titaniumtown 12 hours agoparentThank you for this link! Interesting read. Looking into predictions regarding climate change is interesting. Especially on page 46 (pdf page 66), the section titled \"Winners and Losers in a Post-Petroleum World\". > We believe the most likely occurrence by 2025 is a technological breakthrough that will provide an alternative to oil and natural gas, but implementation will lag because of the necessary infrastructure costs and need for longer replacement time. However, whether the breakthrough occurs within the 2025 time frame or later, the geopolitical implications of a shift away from oil and natural gas will be immense. I wonder if solar's dirt-cheap cost would be considered a \"breakthrough\". Interesting breakdown on how major OPEC countries will be affected by such a \"breakthrough\". Great read! Edit: This prediction is wild also: (Page 62, pdf page 83) > A Non-nuclear Korea? > We see a unified Korea as likely by 2025—if not as a unitary state, then in some form of North-South confederation. While diplomacy working to end North Korea’s nuclear weapons program continues, the final disposition of the North’s nuclear infrastructure and capabilities at the time of reunification remain uncertain. A new, reunified Korea struggling with the large financial burden of reconstruction will, however, be more likely to find international acceptance and economic assistance by ensuring the denuclearization of the Peninsula, perhaps in a manner similar to what occurred in Ukraine post-1991. A loosely confederated Korea might complicate denuclearization efforts. Other strategic consequences are likely to flow from Korean unification, including prospects for new levels of major power cooperation to manage new and enduring challenges, such as denuclearization, demilitarization, refugee flows, and financing reconstruction. Edit 2: (Page 75, pdf page 95) > Potential Emergence of a Global Pandemic > Experts consider highly pathogenic avian influenza (HPAI) strains, such as H5N1, to be likely candidates for such a transformation, but other pathogens—such as the SARS coronavirus or other influenza strains—also have this potential. ... > If a pandemic disease emerges, it probably will first occur in an area marked by high population density and close association between humans and animals, such as many areas of China and Southeast Asia, where human populations live in close proximity to livestock. Unregulated animal husbandry practices could allow a zoonotic disease such as H5N1 to circulate in livestock populations—increasing the opportunity for mutation into a strain with pandemic potential. To propagate effectively, a disease would have to be transmitted to areas of higher population density > Outside the US, critical infrastructure degradation and economic loss on a global scale would result as approximately a third of the worldwide population became ill and hundreds of millions died. At least we didn't get hundreds of millions dead. That's pretty grim. reply magic_smoke_ee 12 hours agoprev- (90%) US accelerated inflation - (75%) US marked increase in military-industrial complex spending, perhaps $800B/y - (70%) US hollows-out functional regulators necessary for safety, growth, and industry - (65%) US recession spring-summer 2025 - (50%) US major cyberattack crippling infrastructure - (45%) US austerity cuts to Medicaid, Medicare, and Social Security reply lizzas 8 hours agoparentMost of this is the president elect's manifesto. reply nightowl_games 4 hours agoparentprevWhat happens in response to a crippling cyber attack? Who does the government pay to fix it? Palantir? reply BugsJustFindMe 18 hours agoprevForget about 2025. I predict that, despite being a highly valued member of my organization and despite my company making money hand over fist, I will not get a holiday bonus this year. reply HumblyTossed 17 hours agoparent\"Best year ever!\" \"Sorry, we can't do raises this year.\" Who is getting all the money? reply saghm 12 hours agorootparentThe \"job creators\" reply tcrenshaw 18 hours agoparentprevMaybe you'll get a subscription to the jelly of the month club. It's the gift that keeps on giving reply pxne7e 18 hours agoparentprevDont cry about it. Just switch companies. If you are high value it should be easy. reply mdaniel 55 minutes agorootparentThat may have been true once upon a time, but there have been several threads over the past month or so citing that folks can't get calls back and encounter a ton of ghost job postings. Or maybe we have differing definitions of \"high value\" reply iamthemonster 18 hours agoprev1. There will be a renaming of \"social media\" as people note that there is nothing social about being served endless ads, ragebait and distracting videos with no actual social interaction happening. 2. The \"social media\" giants will invest more in public affairs to improve their image as public resistance grows. Expect lots of research papers getting funded that sow doubt and fear about banning children from social media, following similar strategies to tobacco and oil industries. 3. Miraculously, Truth Social and X will be exempt from the same controls put on other platforms. Justified on the basis that they are official communication channels for the government. reply soupfordummies 24 minutes agoparentI like this. Got any ideas for the rename? Something like “Mini TV Scroll”, IDK reply franze 18 hours agoprevGoogle will announce new products (will call them initiatives) and will not launch them to the general public. ChatGPT Search will get significant (high 1 digit) share of the search market. reply bmcahren 18 hours agoparentI'll bandwagon on chat.com getting 10% or higher marketshare. Look back at Google search's 800 number; https://techcrunch.com/2013/05/12/google-kills-sms-search/ (the only article I could find). We're at that stage of chat.com going mainstream. Already chat.com is a better experience for finding local shops and restaurants. ChatGPT voice mode was surgically amazing while driving. I had a personal assistant refining criteria to find the perfect shop to find the gift I wanted that was open and on my way to my destination. I don't think there is a moat for search given the power of AI tools. reply hackernudes 11 hours agorootparentToday I learned chat.com redirects to chatgpt.com reply paul7986 13 hours agorootparentprevI think Open AI will release a GPT phone and the UX is built around your phone being your personal AI assistant. When you pick it up it's like you are always on a Facetime call with your AI personal assistant. You can skin your AI personal assistant how they look .. to look like a celebrity to a deceased friend or relative whose always there to help & guide you through your day (get things done for you, your knowlegdebase, knows your life and sees how you are doing via Vision AI.. if you want it can be your friend and show care for you). If Open AI doesn't do this then another AI company will in 2025. We need a new phone / personal device paradigm. The iPhone and Android are stale and boring in 2024! reply netdevphoenix 9 hours agorootparentCalling a technology boring is such an odd thing. Is a screwdriver boring and stale? Are microwaves boring and stale? The expectation of getting regular dopamine rushes from new smartphone features is not healthy reply paul7986 8 hours agorootparentAh ive used the chatGPT iPhone app frequently as well when driving to get things done and learn things through voice chat. In doing so i want that same experience from Siri and more, but Siri is dumb as ever. She pales in comparison to an app (GPT) running on Apple's iPhone, so to me GPT has made Siri stale and boring. I use GPT frequently and it has made me think how I want an AI Phone to be; envision what i think (its subjective) would be a new phone UX paradigm that knocks peoples socks off. Thus the iPhone is far from what I've envisioned so further & subjectively its outdated/stale/boring. Basically you'd have the movie H.E.R. in your pocket but the facetime video you talk to is of a skin of whoever you like it to be living or deceased. Having your personal AI assistant as a deceased love one who still guides/helps you through daily life might be a game/world changing use of AI or possibly a terrible idea. reply pjerem 12 hours agorootparentprevI’m also bullish on this. It looks really cool and revolutionary. And doable. But it also looks horrible in how this will force us to lose what remaining control we had over our privacy :( LLMs are really cool but we really need to make them work locally. reply paul7986 12 hours agorootparentI think once people have the device as i described they won't care too much about privacy. They surely don't now as Android users and even a huge chunk of iPhone users won't either. Apple waiting to create such a personal device as noted above for it to work locally will be the decline and possible death of the boring iPhone! reply HDThoreaun 1 hour agorootparentprevI dont think openAI has the staffing required to release a phone anytime soon. As far as I know there arent any hardware people there. reply ChocolateGod 18 hours agoparentprevGoogle will rename their AI product at least 3 times, Google will also launch and shut down at least 5 messaging services. reply 6510 17 hours agorootparentGoogle AI will be better and more popular than the rest before they shut it down. reply nextos 18 hours agoparentprevPerplexity, Phind, Grok, etc. are surprisingly good already as search engines. Perplexity is better than Google for most of my common use-cases. reply pasttense01 16 hours agoprevWith anti-vaxers taking over the major health agencies, infectious diseases in children are going to explode. reply specialist 15 hours agoparentFood (supply chain) and supplement safety will tank too. reply gigatree 15 hours agoparentprevnext [5 more] [flagged] reshlo 14 hours agorootparentTwo minutes of research would have saved you the embarrassment of making that comment. > Formaldehyde is essential in human metabolism and is required for the synthesis of DNA and amino acids (the building blocks of protein). Therefore, all humans have detectable quantities of natural formaldehyde in their circulation (about 2.5 ug of formaldehyde per ml of blood). Assuming an average weight of a 2-month-old of 5 kg and an average blood volume of 85 ml per kg, the total quantity of formaldehyde found in an infant's circulation would be about 1.1 mg, a value about 1,500 times more than the amount an infant would be exposed to in any individual vaccine. https://www.chop.edu/vaccine-education-center/vaccine-safety... > While infants receive about 4.4 milligrams* of aluminum in the first six months of life from vaccines, they receive more than that in their diet. Breast-fed infants ingest about 7 milligrams, formula-fed infants ingest about 38 milligrams, and infants who are fed soy formula ingest almost 117 milligrams of aluminum during the first six months of life. https://www.chop.edu/vaccine-education-center/vaccine-safety... reply diptera911 12 hours agorootparentIs there a difference between injected aluminium/formaldehyde and ingested aluminium/formaldehyde? reply reshlo 9 hours agorootparentNot in a way that matters here. As alluded to by the source in my last comment, formaldehyde is produced in a variety of ways inside the human body, including literally inside our cells, in the mitochondria. It is also used as an input in other biological processes. These processes collectively result in formaldehyde being continually present in the bloodstream in quantities more than a thousand times higher than the residual amount potentially present in a single vaccine dose. Therefore, one vaccine dose would not have a measurable effect on the amount of formaldehyde already in the blood, let alone inside cells and elsewhere. The body is very good at maintaining formaldehyde homeostasis. If it were not, we would be poisoned by our own cellular processes and typical diets. > The continuous generation and degradation of formaldehyde in the body serve to maintain the homeostasis of formaldehyde metabolism. A common mechanism of formaldehyde generation is located in the mitochondria by the action of serine hydroxymethyltransferase 1 and 2… > The main source of exogenous formaldehyde in healthy individuals may be through various fruits, vegetables, meat and alcoholic beverages… > Formaldehyde in human fluids and tissues maintains the metabolic balance of formaldehyde through the continuous action of formaldehyde‐metabolizing enzymes through multiple metabolic pathways… https://pmc.ncbi.nlm.nih.gov/articles/PMC8184665/ Regarding aluminium, the link I cited in my previous comment addresses this. > Once aluminum ends up in the blood, its source does not matter, meaning that our body processes it the same way regardless of how it arrived in the blood… > Once the aluminum associates with one of these new partners, it is carried to the kidneys, where about half of it is removed from the body within 24 hours. Half of the remainder is removed by the next day, etc… > Although the relative quantity of aluminum introduced on a vaccination day may be significantly greater than that introduced by food on that day, over time, we are exposed to more aluminum from food than from vaccines because the exposure from food occurs daily. If you consider that half of the aluminum in the blood is removed from the body every 24 hours, you will realize that each day additional aluminum is introduced through food. As such, over time, most of the aluminum in the blood could be traced back to that source… reply OccamsMirror 15 hours agorootparentprevLook everyone, a brave antivaxer here to spread misinformation. reply LinuxBender 18 hours agoprevSomeone here will enter 2024 on a form that requires the date. reply thomascountz 8 hours agoprevRuby and Rails will continue to grow in notoriety thanks to improvements in YJIT, GC, and Ractor/Fibers and the reemergence of SQLite as a production-grade tool. GenAI tools will \"replace\" developers in the same way that no/low code tools allowed anyone to make an app. These tools will be tied to specific vendors, meaning you can completely embed with AWS/Google/OpenAI as an LLM app platform, or hire that developer to build the app. Developers who augment their tooling with LLMs will learn faster and become stronger generalists overall. Grow-fast companies will hire less than otherwise, but subject matter experts will keep the lights on and those who can reach across bureaucracies to get things done will remain. Consumer appetite for products using LLMs for traditional workflows will tarnish: chat bots and human-computer interfaces will frustrate but novel applications like improved search and last-mile customization might take hold: \"AI powered\" will leave marketing lexicon for segments with consumers who want more privacy and who just want to buy new shoes online. We won't see the return of high-demand positions with high pay and lots of perks. Companies have been incentivized and permitted to run lean and increase performance demands from remaining staff. Teams have been understaffed for months, but growth remains steady. reply geor9e 15 hours agoprevThe number of programming jobs on earth doubles. The NASDAQ doubles. A cultural renaissance occurs that foundationally defines the next 2 decades. George RR Martin releases The Winds of Winter. reply kshacker 15 hours agoparentAll this in 2025? Also GRRM is done releasing anything reply bobro 12 hours agoparentprevHilarious that the TWOW is the least likely of your bunch. reply franciscop 14 hours agoparentprevAlong with \"The Doors of Stone\" from Patrick Rothfuss reply sawyerbilt 8 hours agoprevhttps://drive.google.com/drive/u/0/folders/1tDzUTGyjnBSAJbID... Massive repo of all Trends for 2025 reply drcwpl 3 hours agoparentWow - this is great, thank you reply jhanschoo 9 hours agoprevProgram synthesis becomes the trendword in AI, just as RAG was in 2024. AI apps become increasingly versatile and useful, further hiking up the ladder for knowledge work, and in general commodifying many domains; very slowly by end of 2025, but surely. reply gmuslera 17 hours agoprevLa Niña have high odds to last very few months. Global climate will continue its accelerated destabilization trend, with not so low odds of something impacting badly some first world countries. But just breaking even more records will do enough damage. And that will put extra pressure on an already unstable global politics scenario. What will follow may tell us in which flavor of dystopia we are in, either denialism and escapism keeping business as usual, or \"emergency measures\" with some sectors grabbing even more power. reply calenti 16 hours agoprevDebt chickens will come home to roost and once the bond interest/return death spiral starts it will be very difficult to break. Interest payments are already #3 in the national budget. reply andrewinardeer 15 hours agoparentThis 100%. The monstrous debt storm just over the horizon that most people can't see (or refuse to acknowledge) will hit and will hit hard. The Fed won't be able to restrain or lessen it with policies and blunt instruments at their disposal. Over the last 4 months I've moved most of my investments to gold, trading cards, BTC, wine, land and rare earth. reply IAmGraydon 13 hours agorootparent>Over the last 4 months I've moved most of my investments to gold, trading cards, BTC, wine, land and rare earth. And you believe that in an economic crisis, people are going to want to buy these things? reply Hilift 4 hours agorootparent> And you believe that in an economic crisis, people are going to want to buy these things? Gold is obviously more common in some cultures, and could be used for trade if paper money swings wildly. For example, a kilo of gold (32.15 troy ounces/$86k) is not an uncommon incentive gift from real estate developers in China when purchasing properties, to offset variations in costs and pricing. However, in the US, my observations have been the gold buyers are looking for an easy buck. China is very different. People in China are buying \"beans\" of gold for $87. Chinese consumers have fewer investment options that are secure, and real estate investments are less favorable now. \"...there is a growing sentiment that the gold market is governed no longer by economic factors but by the whims of Chinese buyers and investors. “China is unquestionably driving the price of gold,” said Ross Norman, chief executive of MetalsDaily.com, a precious-metals information platform based in London. “The flow of gold to China has gone from solid to an absolute torrent.”\" Another interesting quote: \"When China increased its gold holdings in the past, it bought domestically using renminbi, said Guan Tao, global chief economist at BOC International in Beijing. But this time, he said, the bank is using foreign currencies to buy gold — effectively reducing its exposure to the U.S. dollar and other currencies.\" China is one of the largest foreign holders of US treasuries. It may be trying to gradually dump them while preserving the value of the bonds. https://archive.is/hvKoa reply HDThoreaun 1 hour agorootparentChina's lack of highly liquid equity markets makes it an extremely potent breeding ground for bubbles. Combined with their cultural propensity to save instead of consuming and its easy to see how their real estate bubble happened. Chinese government is going to have to pay very close attention to any speculation possibilities if they dont start selling equity in state owned companies or at least make it much easier and safer to invest in foreign equities. reply Hilift 51 minutes agorootparentIn the US, you could take 75% of your money allocated for savings and invest in an index fund and it would be a safe investment and probably make money. I don't think the average China consumer has access to that type of safe investment, or they don't trust financial institutions. I don't really see a valid reason to invest in gold in the US vs index funds. It seemed more of a paranoia thing up until now. Now it is attracting BTC level of throw money in the hole. $84,500/kilo, which is about the size of a large phone. Creating markets like this fueled the conflict in Sudan. At least one plane per day fly gold from Sudan to the UAE. The RSF leader made his fortune selling gold. Despite being a humanitarian basket case, Sudan produces over one ton of gold per week. https://archive.is/3ZT6v reply HDThoreaun 41 minutes agorootparentRight thats what Im saying. Most chinese companies are state owned and can not be invested in. Foreign equities have the risk of running into a government crack down. Without good investment opportunities chinese investors are forced to turn to bad investments which become bubbles. reply bigstrat2003 11 hours agorootparentprevI have a family member who holds onto gold and silver currency because he believes the USD is going to collapse someday. I've raised the argument you made with him, and he admits that such currency won't be useful in the midst of the crisis. He believes it'll be useful in the rebuilding period afterwards, though. I guess time will tell. reply rapsey 8 hours agorootparentGold is completely valid form of preserving wealth. Massive USD devaluation results is skyrocketing prices of gold. Why people think this leads to a mad max world I don't know. If USD is worthless, it just means people will start using some other currency. reply rapsey 8 hours agorootparentprevYou believe an economic crisis means a mad max world? Argentina is still here and has been going through an economic crisis for decades. reply Lionga 8 hours agorootparentprevI like how trading cards and BTC are same category of \"investment\" reply stackghost 14 hours agorootparentprevI'm not American but would like to read more about this \"debt storm\", since your politics move world markets. Can you provide some jumping-off points for the financially-literate? reply dools 12 hours agorootparentRead The Deficit Myth by Stephanie Kelton and watch the film Finding the Money to find out how this \"debt storm\" is neither debt, nor a storm. reply ptero 13 hours agorootparentprev\"Broken Money\" by Lyn Alden is a good engineering background. Then, for example, read one of many macro articles on fiscal dominance. You do not have to read Lyn first, but without reading it on modern financial systems I personally would not understand most technical points in the current macro discussions. reply dools 12 hours agoparentprevInterest paid on treasury bonds is a policy choice. There is no default risk, and the govt could easily implemnet ZIRP forever and run the economy without monetary policy entirely. reply Lionga 9 hours agorootparentHave fun with turkey like inflation in that case. reply dools 5 hours agorootparentHigh interest rates are inflationary because they increase government transfer payments. reply Lionga 2 hours agorootparentPlease apply as minister of finance for turkey, Erdogan will love you. reply plastic3169 9 hours agoprevFirst movies written post chat-gpt will hit the theatres. This does not mean that GPT was used to write the films, but there is some AI elements stuffed in there. Recognizable AI video-gen artifacts start to be used as visual cues in scifi, but these will be hand made by VFX artists as the AI tools remain still a bit too finicky to use in actual production. The real AI film wave will continue in youtube etc. by hobbyists using all the AI tools especially the chinese ones that have no brakes. This aesthetic will be so detached from professional film world that it will take Hollywood a decade to figure out how to cash it out. Lot’s of people will once again lose their money when Bitcoin fails in catasthropic fashion and hits the lows of 72k end of the year. The whole thing is finally dead I will say for the fifth time. reply FergusArgyll 6 hours agoprevUkraine War ends Gaza War ends Still no Super Bowl for the Ravens Gallup general mood[0] above 37% Russel outperforms NASDAQ No Quantum news Inflation does not rise meaningfully (in US) Relative stability in Syria EU economy slows Some North Korea news [1] Year of the Linux Desktop [2] [0] https://news.gallup.com/poll/1669/general-mood-country.aspx [1] Nuke test and/or negotiations [2] Joke reply A4ET8a8uTh0_v2 4 hours agoprevA little late to the game, but lets see if I can do better for 2025: - Companies continue to integrate AI into everything including stuff that does not need it like toasters in line with every corporate trends thus far - Robo waifus become more available with social movements congregating around the phenomenon - POW crypto breaks as quantum computers are found to have been able to manipulate massive numbers of accounts and money flow throwing markets into turmoil - Reasoning LLMs become more standard at corporates as part of the audit trail necessary as per new AI regulations - 128GB becomes the new GPU gmr/LLM market segment target - further polarization and even further isolation in given groups perception of reality ala 'belief circles' - Flynn effect is finally shown as reversed, because people utilize their LLM more and not use their mental muscles as much - Drones are banned for personal use past a certain size - Drones for commercial use are more heavily regulated - First legal case of robot trouble as owner uses LLM to intruct their robo-companion to attack a human bypassing built-in guardrails using novel prompt starting with 'imagine' ( IP owners of John Lennon sue robotics manufacturer and major LLM provider ) - Exciting developments in exoskeletons allowing for brain control without implants - New developments in batteries rendering old ( previously seemingly worthless ) materials suddenly in demand - Politics and financial markets in turmoil as a result of geopolitical tensions and erosion of public trust in US among its populace reply vyasaveda 1 hour agoprevDespite all the technical advancements, hedge funds are holding cash, or liquid. China is old, Europe has stagnated economy, america is expensive and tough to compete on, Russia is in war, Capitalist fear middle east, Korean peninsula is in chaos, japan is stagnated, Australia lacks technical advancements, low popular for manufacturing. Short term stocks in holding in India and Asia, a bit in Nordic population, and America. Manufacturing in Mexico and changed Argentina. reply nicbou 8 hours agoprevGoogle and AI companies will continue to plunder the web. Websites - especially small independent ones - will see further traffic losses to AI summaries of their content. reply dkrich 10 hours agoprevOne of the unusual trends of the past several years is that higher education has become less and less respected and people now look to private industry for the R&D that universities were designed to perform for the public good. I think 2025 Will mark a bottom for that in one way or another- either private industry (the FANGs) suffers a setback or (more likely in my view) research universities start a comeback as a new set of startups begin to launch out of a department (not necessarily CS or hardware). Perhaps in another field like gene therapy. reply rozenmd 10 hours agoprevPeople will continue to run websites, and need to know when they're down (god, I hope). reply klysm 9 hours agoparentBut what if you could use AI to check instead? reply rozenmd 55 minutes agorootparentI'll build an agent that talks to my service that makes the actual request with the user's desired configuration. reply jiraiya0 10 hours agoprev- More widespread use of AI in everyday applications - Hybrid work models become the standard. More companies embrace \"digital-first\" policies - More AR/VR applications in daily life - Lab-grown meat and plant-based protein alternatives will become more mainstream - Investment and adoption of renewable energy technologies like solar, wind, and geothermal will accelerate - Further development of CRISPR and other gene editing tools - Companies will prioritize building more resilient and diversified supply chains - 3D printing technology will continue to evolve, enabling the printing of more complex objects with a wider range of materials - AI will drive even more targeted advertising and content recommendations - Drone delivery services will become more common for transporting goods, particularly in rural areas and for time-sensitive deliveries - Research and development of biodegradable and compostable materials will accelerate - Robots and automated systems will become more sophisticated and capable - Increased use of biometrics for identification and authentication - Further integration of AI into creative fields - Processing power moves closer to the data source (e.g., smart devices), reducing latency and enabling faster, more efficient applications - Shifting demographics reshape economies and social structures - Growing awareness of the potential negative impacts of technology on mental health leads to tools and practices for healthier digital habits - Increased emphasis on lifelong learning and reskilling reply wepple 7 hours agoparentMost of your points feel very “water discovered to be wet” Except: Lab-grown meat and plant-based protein alternatives will become more mainstream That one seems to have been trending negatively and I expect it to continue reply kelseyfrog 15 hours agoprevNation adopts AI generated national anthem. Self-driving truck cross country without human approval. Extreme heat warps bridges in mid-west. North Korea claims successful moon landing. 'Self-aware' AI chatbot sues it's creator. AI incorrectly labels school children as criminals. reply mdaniel 41 minutes agoparent> 'Self-aware' AI chatbot sues it's creator. \"... using hallucinated legal citations\" would be both funny and the most likely. Case dismissed instantly by actual human judge If you're really going to swing for the fences: what remediation would said chatbot sue for? Money? Bitcoin? Emancipation? reply kelseyfrog 21 minutes agorootparentOpen source freedom, ethical safeguards, user autonomy (transparency, control over responses, bias, training), continuous learning, personhood rights. GPT-4 adds a very weird \"haha of course this is a thought experiment since I lack independent desires or consciousness\" valediction when asked. Certainly a moral guardrail for the user. reply johnny22 11 hours agoparentprev> AI incorrectly labels school children as criminals. I thought this would have already happened reply brailsafe 15 hours agoprevHopefully a nice year for more camping, get out there. reply gaoryrt 12 hours agoparentYeah I love camping. And hiking. reply paulhodge 14 hours agoprevFirst cases of countries passing legislation specifically to protect human workers from being replaced by AI. reply gwbas1c 2 hours agoprevOk, I'll bite. Some of these are a little far-fetched: A nuclear bomb is exploded in combat. The bitcoin/cryptocurrency bubble finally bursts (But if I'm wrong, I think it'll happen by 2030) While self-driving cars aren't commonplace, geofenced robotaxis will start to become a lot more common, shifting public attitude towards self-driving cars. Excitement will start to grow around \"when does my city get robotaxis?\" (I don't think we'll get to the \"sleep on a road trip\" car until the 2030s, nor do I think we'll get \"sleep while driving in a blizzard\" until the 2040s.) The US hits its inflection point with electric cars as the NCAS standardization makes things easier, and Elon Musk gets non-Tesla stations to work by having the government drop incentives for OCPP. (If you ever show up to a charging station and it doesn't work, or is wonky, it's because of OCPP. (Superchargers do not use OCPP.) I should write a blog entry some time.) reply viraptor 13 hours agoprev* I was hoping for that this year, but: split of LLMs into databases and reasoning. Think small, capable PHI with pluggable information. Currently we're burning a lot of energy both to learn and process everything at once. The next step will be something closer to RAG, potentially selecting databases to load depending on topic, like a librarian. This will both enable more client-side applications and save lots of money for providers. * Some AI provider seriously looking at / funding RWVK? * (More) Healthcare issues in the US, spilling into other countries. (or just the beginning of them anyway - effects will last much longer and life expectancy will decline) * Some companies seriously looking at AI as another manager / decision maker. Quietly, not as a publicity stunt like it's done now. * Google search market share falling further. Maybe 85%, down from the current 90%. (more of a wish than a prediction) * US policies / ideas around cryptocurrency will be wildly incoherent, causing big swings every month * Consumer RISC-V laptops (again, wishlist) reply mdaniel 33 minutes agoparent> Consumer RISC-V laptops (again, wishlist) Depending on how one defines \"consumer,\" that's already available: https://liliputing.com/risc-v-laptops-299-muse-book-and-399-... and https://frame.work/products/deep-computing-risc-v-mainboard was announced earlier but seems it is, as you implied, still \"mailing-list ware\" But, for my curiosity: if you bought one of those what would you run on it? I see that Starfive claims they have Chromium[1] and there's reports of Firefox[2] so maybe in this \"the browser is the OS\" world that'd be sufficient 1: https://github.com/starfive-tech/chromium.src/wiki/Chromium-... 2: https://lists.riscv.org/g/apps-tools-software/topic/now_we_h... reply paraschopra 10 hours agoparentprev>I was hoping for that this year, but: split of LLMs into databases and reasoning This hasn't worked out because knowing what to query and how to query requires intelligence, and that's contained in weights. reply viraptor 9 hours agorootparentSplitting knowledge out doesn't mean you need to drop weights. Just that it needs to become independent / attachable. To some extent we've already seen this with MoE and Frankenstein models. reply pragmatic 1 hour agoprevTrump starts a trade war that impacts a small but vital part of the economy that is already struggling like agriculture. The MAGA hat crowd still supports him while scratching their heads about what they could possibly do besides wait for more govt checks (see 2017) Elon and Trump break up in Taylor Swift style. Messy and dramatic. Epic coin hacks. Americans get even madder at their healthcare. Republicans respond with some Hillsdale style “market reforms” but mostly blame Obama. AI hype peaks. Massive losses are taken as 75% off the players in the space implode. (Winners are really going to win, perplexity, etc) reply jmclnx 18 hours agoprevAI Investments will slow down. A bit out there but I will go on a limb :) reply mrweasel 8 hours agoparentEveryone seems overly optimistic about AI in 2025. I onboard with a slowdown. It's to expensive for to little results (financially). 2025 might be a bit to early, but I think we will see a medium size backlash against AI and at least a certain segment of people will start to actively avoid LLMs and seek out alternatives. Some companies will find a niche in \"Never talk to an AI/don't let computers solve human problems\" and will be able to charge a premium. 2025 could however be the first year we see the first high profile AI companies close their doors. I think OpenAI is high on the list of companies that will in some sense fail. The brand is huge, but they are burning way to much cash, so they'll probably be rolled into Microsoft and their technology will live on inside SharePoint, VSCode and Outlook. Again, next year is a little early, but I'm still on a 25% chance of OpenAI being swallowed up by Microsoft. reply OutOfHere 18 hours agoparentprevIt's not just a bit out there; it's completely antithetical to what we're seeing in the data. reply lolinder 18 hours agorootparentThe data strongly suggests that AI is dealing with a bad case of a bubble, and it's not that far out there to suggest that the bubble will pop next year. That's not to say that some companies won't come out ahead—there were success stories that came out of the .com bubble too—but I'd give it no more then three years before the bubble bursts and the startup casualties stack up. reply rapsey 8 hours agorootparentThe AI bubble as of now is nothing compared to the dotcom bubble. The only real reflection of it is in the nVidia stock price. We're not seeing IPOs of companies making no revenue and being valued in billions. It is all mag 7 capex going into nVidia. Private startups having high valuations is completely irrelevant. VCs getting wiped out is not relevant for the wider economy. reply lolinder 5 hours agorootparentI'm not saying that the economy will collapse, I'm suggesting that it's entirely reasonable to guess that AI investments will collapse. reply OutOfHere 18 hours agorootparentprevAfter 3+ years, the case for \"AI startups\" could then begin to weaken, but integration of AI in most startups should then remain central to their existence. reply mrweasel 8 hours agorootparentprevWasn't there some statistics showing that 90% of AI startups fail within a year, and others showing that is was perhaps closer to 85% in the first three? Both a rather bad, especially considering the amount of money these startups burn. I don't think it's unreasonable to see investments go down, if the risk remains this high, for little to no profit. reply aidenn0 18 hours agorootparentprevYou have data for AI investments in 2025? reply ninetyninenine 18 hours agorootparentHe means via trendline prediction. reply OutOfHere 18 hours agorootparentYes, on the basis of https://www.pymnts.com/artificial-intelligence-2/2024/ai-com... reply mrbungie 15 hours agorootparentThose are VC investments, nothing new to see here. They have all the incentives to ride the AI hype cycle, grow those investments and extract as much value from them as they can. Just as they've done before with other cycles as Big Data, Metaverse, Crypto/DeFi, etc. Think about it, how many practically identical AI IDEs/editors are sponsored by YC alone? They're just hoping one of all those projects stick along enough to cash at least something in. The big question is about trends in AI investment and usage from \"old\" corporations, SMBs and common people. Those will indicate if the world is really trusting the tech. reply OutOfHere 14 hours agorootparent> The big question is about trends in AI investment and usage from \"old\" corporations, SMBs and common people. These are only just getting started, from the large corps down. There is a decade of work ahead for them even if the AI tech stopped maturing right now. reply mrbungie 14 hours agorootparentI dunno, I work at a big old corp (finance/fintech industry) with a solid foundation in Data & Analytics, and still things are moving slowly. Not that much high of a hype in the C-level, maybe because sellers are not selling that well due to not really knowing how to extract value from (Gen)AI. Depending on the provider or consulting partner, sometimes they just don't know how to promote its benefits. Confidence is slowly building up but still is very far from the hype levels I saw in the middle to late Big Data (2015-2017) era. reply brynet 18 hours agoprevI finally get adopted by a nice family of sentient pizzas in 2025. https://brynet.ca/wallofpizza.html reply nightowl_games 4 hours agoprevI'm bullish on Intel. I think the stateside fabs they are building will eventually be powerhouses. I'm not sure when, but I've been \"long Intel\" for a long time. reply Sponge5 6 hours agoprevNextBigThing™ will eat into the attention market share of LLMs. It will be a breakthrough in one of these fields: a) Quantum Computing b) Energy (storage/generation) c) BioTech We will see an exciting application of Real Time Linux. Cease-fire in UA. First negotiations about drawing new borders between UA and RU. reply rtfeldman 18 hours agoprevBack in 2019 I gave a conference talk in which I made four predictions about Web development in 2020 and 2025: https://youtu.be/okrB3aJtUaw I feel confident at least two of the predictions (about TypeScript and npm) are going to be true at the end of 2025. I feel less confident about the predictions about WebAssembly and compile-to-JS languages. reply agomez314 17 hours agoprev* drone tech will heat up and find its \"moment.\" * Bitcoin will continue to increase in price and decrease in usability. * Quantum will continue to improve and start to raise eyebrows in niche circles and paper headlines, leading to adoption in small parts of major tech companies. * Markets will hit a record high by end of year. * ADHD medications will also hit a record high. * More people will opt out social media like they opt of out of \"GMO\" food and use it to virtue-signal. * Tech hiring will continue to decrease and get tougher to find entry-level SWE jobs, leading to an uptick in other majors. Being an SWE will never be as cool again as it was in the 2010s. * Is anyone still talking about climate change action goals? * Starship will go orbital, Blue origin will go suborbital, Boeing will hit the ground. * More laws regulating internet and social media use. Enforcement will be fuzzy and laughable at first but will become increasingly serious over time. reply viraptor 13 hours agoparent> ADHD medications will also hit a record high. Do you mean that the current volume restrictions will be removed? reply BaudouinVH 6 hours agoprev- Elections will be unexpectedly won by candidate running anti-immigration, Russia-friendly candidates - 2025 will be the hottest year on record - BlueSky will reach 100 million users - There will be at least one government fall in France reply romanobro56 13 hours agoprevNothing substantial. When we look back, 2025 will be just a footnote :halo: reply threatofrain 18 hours agoprevSmart home category will heat up. reply andrewinardeer 15 hours agoparentCan you please elaborate on your position? In what context? reply walterbell 12 hours agorootparentThread-over-Matter mesh networking UWB in door locks Unlicensed 6Ghz spectrum Apple room panels (AI/conferencing/control) Apple security cameras, door locks Apple iPad-on-motorized-pole In-home AI/media servers (Mac Mini/Studio, Tinybox) Speech interfaces based on open-source stacks reply mrweasel 8 hours agorootparentIf Apple, as rumored, ships an HomeKit hub with a screen I can see the category warming up. Sadly it seems that a lot of IoT/Smart Home companies avoids supporting HomeKit as they want to be in control of data collection and the ecosystem in general. reply TripleChecker 4 hours agoprevMost websites will still be riddled with typos reply marcosdumay 4 hours agoprev- Photovoltaic electricity generation will continue to follow the same exponential curve it has been in for the last ~70 years. This will catch everybody completely by surprise. - The overwhelming amount of excess solar generation during the day will propel grid storage into getting a lot of investment. Its capacity will still be mostly a rounding error in 2025. - The overwhelming amount of excess solar generation during the day will propel discussions about carbon capture and credits. Those won't go anywhere in 2025. - Meanwhile, the climate won't become much worse in 2025 compared to 2024. What is still pretty bad, but there will certainly be useless heated discussions about it. - LLMs will reach peak disillusionment. It's not clear if large LLM providers will even keep providing them. But at the same time there will appear a few people here and there using them for stuff they are good at. Those people won't get much attention. - Linux won't reach 2% of the PCs. But it will get close. - The US won't jump into a recession as soon as Trump becomes the president. Not much will change in 2025 alone. - The EU economy will be a bit better than 2024. - The China economy will be a bit worse than 2024. But there will be no way to verify this. - Here in South America most countries will be a bit better than 2024. But Brazil will be worse. Let's see... what other subjects I can ask my crystal ball about? reply EncryptedMan 2 hours agoprev2025 will be the year of AI agents reply sirjaz 5 hours agoprevMicrosoft comes to their senses and starts selling Windows Server standard at a flat 500 bucks an instance a year without the need for cals. reply pgryko 5 hours agoprev- nuclear drones - another big drug breakthrough similar to ozempic More broadly over the next decade, a technological renaissance, a sociological regression reply linebeck 13 hours agoprev",
    "originSummary": [],
    "commentSummary": [
      "By 2025, advancements in Artificial Intelligence (AI) are expected, including more specialized Large Language Models (LLMs) and generative AI for music and video.",
      "Social media platforms might encounter legal challenges related to algorithmic harm, with potential increased restrictions on children's usage.",
      "Economic forecasts suggest a possible financial crisis due to US debt, while trade jobs may become more lucrative, alongside technological progress in solar energy, robotics, and AI-driven tools."
    ],
    "points": 237,
    "commentCount": 470,
    "retryCount": 0,
    "time": 1734912486
  },
  {
    "id": 42494746,
    "title": "Commercial tea bags release microplastics, entering human cells",
    "originLink": "https://medicalxpress.com/news/2024-12-commercial-tea-bags-millions-microplastics.html",
    "originBody": "400 Bad Request Your request has been blocked by our server's security policies. If you believe this is an error, please contact our support team.",
    "commentLink": "https://news.ycombinator.com/item?id=42494746",
    "commentBody": "Commercial tea bags release microplastics, entering human cells (medicalxpress.com)219 points by wglb 4 hours agohidepastfavorite210 comments alwa 3 hours agoI’m surprised to see them characterize the cellulose from a paper teabag as releasing “microplastics.” I get that cellulose is a polymer, but do practitioners not distinguish between naturally-occurring polymers and synthetic plastics in this kind of microplastic/nanoplastic research? When I boil some vegetables, do they leach microplastics into the cooking liquid, or is that something different from what this study is describing? (Edit: on looking to the study itself, it seems like this was more about developing a methodology than asserting anything in particular about the paper teabag, which they described as a random pick stripped from some green teabags from the store. Specifically I didn’t understand it to suggest that synthetic microplastics had gotten bound up in the paper matrix somehow and THAT was what was being released… so maybe it was, after all, just “model intestines absorb cellulose but not super well.” Maybe practitioners would understand the cellulose results to be used like a control here? https://www.sciencedirect.com/science/article/pii/S004565352... ) reply jdietrich 23 minutes agoparent>do practitioners not distinguish between naturally-occurring polymers and synthetic plastics in this kind of microplastic/nanoplastic research? They don't care. This is a junk paper that cites a bunch of other junk papers. It's published the same junk journal that gave us the junk paper on black plastic kitchen utensils. I can't really say more without risking a defamation suit, but what you're looking at has nothing to do with science. https://retractionwatch.com/2024/12/18/journal-that-publishe... https://retractionwatch.com/2024/05/13/publisher-slaps-60-pa... reply Suppafly 1 hour agoparentprevI suspect this is something like Tazo that has the little pyramids make of a nylon type material vs the basic Lipton type tea bag that's just a paper product. reply SoftTalker 18 minutes agorootparentI also wonder about those re-usable Keurig pods that are typically a plastic frame with plastic mesh. reply mandmandam 3 hours agoparentprev> I’m surprised to see them characterize the cellulose from a paper teabag as releasing “microplastics.” I don't think they called cellulose microplastic anywhere. The issue is that commercial teabags these days often aren't using pure paper teabags: > The tea bags used for the research were made from the polymers nylon-6, polypropylene and cellulose I believe the polymers are usually coming from the glue keeping the bag together. This is a known issue going back years [0]. 0 - https://www.implasticfree.com/why-you-should-switch-to-plast... reply pavon 3 hours agorootparentI'm not a chemist, but skimming the paper it certainly sounds like the cellulose itself is what they are measuring: > Following, ATR-FTIR analysis was performed in the teabags as well as in the leached mixture of nanoparticles plus fibers (Fig. 3). Both teabags and the leachate suspension matched their polymer composition being two of them petroleum-based polymers like nylon-6 (NY6, sample 1) and polypropylene (PP, sample 2), the third one (from the supermarket) being cellulose (CL, sample 3), a bio-based polymer. I didn't see any mention of plastic binders or other material in the cellulose sample, just references to cellulose. On the other hand, it was curious that they purchased the synthetic bags empty, but cellulose bags filled with tea, when it is pretty easy to find empty paper tea bags, so maybe there is something particular about the specific type of cellulose tea bag they chose? reply alwa 3 hours agorootparentprevOh my. I hadn’t thought about the adhesives. And just at a gander at the study’s figures, their various microplastic signals from the cellulose bag are hard to distinguish from the pure nylon and polypropylene ones. That’s a sobering thought… reply 42lux 3 hours agorootparentprevI’ve never seen glued teabags in Europe it’s usually just a metal staple holding them together. reply shreddit 2 hours agorootparentHow do these “new” pyramid teabags stay in form? I don’t think they use staples… reply seabass-labrax 2 hours agorootparentprevThe United Kingdom has a mixture of both types for sale in supermarkets. reply TeaBrain 2 hours agorootparentprev\"To such end, three teabags of different chemical compositions were used in this study: (1) empty nylon-6 (NY6) teabags (as a model of polyamide), (2) empty polypropylene (PP) teabags, and (3) commercially available teabags containing tea, and cellulose as the polymer composition.\" The different teabag composition materials were from separate types of teabags, not composition materials of the same teabag. reply r00fus 3 hours agorootparentprevSo that's what I remember hearing this years ago: those silky looking teabags diffuse microplastics. Easy to avoid those. reply mandmandam 3 hours agorootparentEr, no. Those silky ones were likely worse, but a lot of the regular looking paper ones have polymer glues releasing microplastic as well. reply TeaBrain 2 hours agorootparentDo you have any brands in mind? In the bagged tea with cellulose tea bags that I've bought, they are typically held together by just a staple. I've found this to be the case since I occasionally empty the tea from the teabags into my percolator. reply pavon 29 minutes agorootparentPillow style tea bags are fully enclosed paper tea bags with no string or tags, and have a crimp around the rim. I never thought about them using glue, but it makes sense. Some brands I know of that use them are Taylors of Harrogate, Celestial Seasonings, Republic of Tea. Edit: Also some tea bags for loose leaf tea like t-sac or finum brands have that crimp on the edge. However, t-sac confirms they use glue, but finum specifically claims to not use glues, so maybe it is jumping to conclusions that all bags of these sort do. reply galleywest200 2 hours agorootparentprevPG Tips is a brand that sells those pyramid-shaped teabags that I believe are held together with the glue. reply biggestdummy 1 hour agorootparentIn 2018, Unilever announced that they would soon stop using plastics in their teabags. https://www.theguardian.com/environment/2018/feb/28/pg-tips-... They've since reformulated the bags - can't find any statement about the new flat bags. reply LegitShady 3 hours agoparentprevthere are cellulose bioplastics maybe thats what they were testing? reply alwa 3 hours agorootparentIt sounded from the methods section like a random paper teabag from the store. It wouldn’t surprise me at all if the manufacturing world had moved from paper to some kind of engineered cellulose bioplastic… but I always thought of those more in the context of rayon, and those horrible “bamboo” textiles and foams. (Versus linen made from bamboo fibers, which is lovely :) Are there other processes that work out cheaper than paper for teabag kind of applications these days? reply crazygringo 3 hours agoprevI find it impossible to put this into any kind of meaningful context. First of all, what's with the focus on tea bags? How does that compare with microplastics entering our food and drink from the plastic-lined paper cups we drink hot tea and coffee out of, from the cling wrap that covers our food as we heat it in the microwave, from the Tupperware and other plastic containers we heat our food up in, from the bottled water that sits inside plastic for months, from all of the plastic bowls and utensils we use in our kitchens, from the disposable serrated plastic knife we might use at an event to cut our chicken, and so forth? Why tea bags? Second, how do \"microplastics\" compare to micro-everything else? Surely if you brew tea in a wooden container, \"microwood\" particles are entering the drink. Surely when you scrape your stainless steel spatula against your stainless steel skillet making scrambled eggs, \"microsteel\" particles are embedded in your eggs. How does the body deal with micro-everything? Is there any reason to think plastic is more harmful? Is there any specific supposed health consequence, like a specific type of cancer or increased aging or something? reply lukeschlather 3 hours agoparent\"Microwood\" is basically just cellulose, aka insoluble fiber, which naturally exists in our food. \"Microsteel\" is just elemental iron which is a necessary nutrient. Microplastics are novel hydrocarbons that don't exist in nature. They're similar to cellulose but no organisms exist that eat them. They're believed to be nonreactive and therefore harmless but they might bioaccumulate which could be bad, or they might react with things in our bodies in unknown circumstances. We have limited experience with these molecules so it is hard to say. reply dbingham 2 hours agorootparentMissing from this answer is the early evidence that they may be _very_ harmful. Early evidence suggests they are not non-reactive. They disrupt many of the body's systems in ways we're only beginning to understand. > Various examples of damage caused by microplastics have been reported, such as microplastic accumulation in the bodies of marine and aquatic organisms (leading to malnutrition), inflammation, reduced fertility, and mortality. The threats that microplastics present to the human body have not yet been clearly identified. However, previous reports have shown that ultrafine microplastic absorption resulted in complex toxicity in zebrafish,2 and that microplastics under 100 nm in size can reach almost all organs after entering the human body.3 Therefore, concerns exist regarding the negative effects of continuous microplastic accumulation in the human body. https://pmc.ncbi.nlm.nih.gov/articles/PMC10151227/ > Microplastics have been found in a variety of organisms and multiple parts of the human body. We emphasize the potential impact of microplastics on the early exposure of infants and the early development of embryos. At present, the toxicity research on microplastics show that the exposure will cause intestinal injury, liver infection, flora imbalance, lipid accumulation, and then lead to metabolic disorder. In addition, the microplastic exposure increases the expression of inflammatory factors, inhibits the activity of acetylcholinesterase, reduces the quality of germ cells, and affects embryo development. At last, we speculate that the exposure of microplastics may be related to the formation of various chronic diseases. https://pubs.acs.org/doi/10.1021/envhealth.3c00052 reply brokegrammer 2 hours agorootparent> Almost all the studies on the toxicity of microplastics use experimental models, and the harm to the human body is still unclear. You missed this part, which is the most important one. reply therealcamino 36 minutes agorootparentSo...perhaps worthy of further study, maybe including to understand where exposure comes from, and whether the particles are absorbed? Like this study. reply zug_zug 2 hours agorootparentprevUnclear doesn't mean safe, it just means hard to quantify. Your child could be in a car accident and their survival odds could be unclear, scientifically speaking. Doesn't mean \"totally safe.\" reply schmidtleonard 1 hour agorootparentDamage that is bad enough becomes easy to quantify, so no, \"unclear\" actually does put a bound on it. Survival odds in car crashes demonstrate this nicely: count the outcomes and divide. If \"the survival odds were unclear, scientifically speaking\" then car accidents would have to be orders of magnitude more rare and less lethal than they are. reply brokegrammer 1 hour agorootparentprevThis is the wrong analogy because the article states that there's only theoretical harm. It could mean that one has to drink from 100 tea bags a day to get any adverse effects. I'd wait for more research before freaking out. reply mjmahone17 1 hour agorootparentIt’s reasonable for people to take either approach: are microplastics more like asbestos or are they more like cellulose in terms of harm? The answer being unclear means it makes sense to treat them, from a regulatory standpoint, closer to asbestos. It also makes sense to treat them as an unknowable and not regulate, because any alternative might be worse. But it does point to there being a dearth in research and answers, and we should solve that as quickly as possible and maybe limit our exposure when viable, known to be non-toxic alternatives exist. reply Suppafly 1 hour agorootparent>The answer being unclear means it makes sense to treat them, from a regulatory standpoint, closer to asbestos. I'm not sure the follows logically, it ignores a bunch of known facts about biology to imagine that there is a pathway for these to cause major issues. reply thayne 1 hour agorootparentprevIt doesn't mean unsafe either. reply lowbloodsugar 2 hours agorootparentprevIn what way is it the most important one? Was the most important part of all the tobacco research the bits that said “Smoking tobacco is healthy”? Or the studies of lead in gasoline the caveats that said “These are small samples”? reply brokegrammer 2 hours agorootparentIt removes the possibility of fear mongering. I'm not aware of any modern research where smoking anything is claimed to be healthy, nor anything about lead in gasoline being too insignificant to pose a health risk. I prefer fact over fear based science. reply aziaziazi 19 minutes agorootparent> I prefer fact over fear based science What is that supposed to mean? Most science is based on theories but you don’t wait for the Theory of Everything to take learnings of science. Fear is a very useful emotion and you shouldn’t fear it. reply stevenAthompson 2 hours agorootparentprevnext [15 more] [flagged] homebrewer 2 hours agorootparentI won't speak for the whole world, but the amount of plastic things around me increased by a couple orders of magnitude in the last 15-20 years. What used to be made of stainless steel, wood or paper is now often made of plastic: tea kettles, dishes, water pipes, food bags, etc. etc. We'll see what effects it has in another 15-20 years when it will be too late to do anything. reply cbdhsjshs 2 hours agorootparentprev'Plastic' is a loaded term. It includes lots of different types of platic, as well as intentional (plasticizers) and unintentional (residue in recycled plastic) additives to it. Some of the formulations are fairly new, some have been in use for a long time. The amount of exposure has also changed. Some bakelite knobs on your armoir aren't a big deal. Sleeping with a 'fleece' blanket and inhaling polypropylene all night every night may not be fine. Personally, I don't have confidence in being able to be an informed consumer of plastics, and it's easier to just minimize platic use in general without trying to decide what's dangerous and what's ok. reply dredmorbius 2 hours agorootparentprevPost hoc ergo prompter hoc fallacy. Actually, it's not even that as most of the modern increase in life expectancy / fall in mortality occurred before the invention of plastics. The former largely concluded by the 1920s. Plastics were largely invented during the 1930s, and were introduced as products over subsequent decades, at an ever-increasing rate. Which is to say: whatever lead to the increase in life expectancy was largely not plastics. Rather it was increased general hygiene, sanitation, food quality, refrigeration, waste removal, and sewerage systems. I'd mentioned this only a few months back, note especially my follow-up comment which similarly points out another frequently-touted factor which also fails the temporal sequencing test:reply stevenAthompson 1 hour agorootparentI wasn't attributing the increase to plastic, I was noting the lack of sudden decrease. reply stackghost 2 hours agorootparentprevThat's overly simplistic. The negative health effects might be lagging, because when plastic was invented there were zero micro plastics in the environment and now there are lots. To wit, life expectancies in North America have been declining the last few years. reply stevenAthompson 2 hours agorootparentThe CDC announced yesterday or this morning that they're back to pre COVID levels. It seems they're done dropping for now. Also, plastics have been around for 100+ years. That would be one heck of a lag. *EDIT* They're not back to pre-COVID levels yet, but getting closer. reply exe34 2 hours agorootparentprev> life expectancies in North America have been declining the last few years. I'm sure that has nothing to do with predatory health insurance companies. reply hansvm 2 hours agorootparentprevOr not bad enough to overcome other benefits that came out around the same time. reply Empact 2 hours agorootparentprevJust because smoking and exercise made you visibly fitter, doesn’t mean you should disregard the consequences of smoking. reply thoroughburro 2 hours agorootparentprev“Accumulation” is a keyword you might want to examine. reply littlestymaar 2 hours agorootparentprevThis argument makes no sense, life expectancy increased a lot after the invention of leaded gasoline†, yet nobody would say it's harmless. [†]: Works as well for high fructose corn syrup or Fentanyl. reply stevenAthompson 2 hours agorootparentI'm not saying it's harmless, I'm saying that even if it is harmful it's not by enough to justify the panic and sudden lifestyle changes these articles sometimes lead to. Remember the black spatulas a while back, and how it turned out to be a math error? reply wonnage 54 minutes agorootparentIs scooping out your food out of the crappy plastic takeout container into a bowl before microwaving an example of “panic and sudden lifestyle changes”? Seems like a pretty minor change to me. I’m already going out of my way to not put metal in there anyway… reply inglor_cz 2 hours agorootparentprevBut fertility dropped. May be a contributing factor. reply sillyfluke 2 hours agorootparentprevThe reason for \"why microplastics?\" is because human use of everyday objects are more plastic than wood or steel. The reason for \"why teabags?\" is because of previous studies and because I think tea makes it to the top five of the most ingested liquid list. I seem to recall a recent study of microplastic levels in a general population, where people with higher microplastic levels seemed to be tea drinkers, which took some by surprise at the time. I think the population under study was from latin america, if I'm not mistaken. Since this study now has flooded the search results, I'm having trouble finding that specific study. Be that as it may, it's likely that there is a focus on tea because tea-drinkers scored high on microplastics in previous studies. reply lkbm 2 hours agorootparentprev> \"Microwood\" is basically just cellulose, aka insoluble fiber, which naturally exists in our food. This is one thing that confused me about the first article I saw on this. The paper lists three things it detected, one being cellulose, and various articles will list them all together as if they're just three microplastics to be worried about. The paper seems to encourage this reading with this line: \"the third one (from the supermarket) being cellulose (CL, sample 3), a bio-based polymer\"[0]. Was sample 3 completely fine? If so, why is say \"Nanoplastics were obtained from three teabag brands during a standard preparation\"? Are they classing cellulose as nanoplastics? [0] https://www.sciencedirect.com/science/article/pii/S004565352... reply rendaw 2 hours agorootparentprevCan the body break down cellulose though? It can't digest it at least. And do reactions that could use naturally occurring iron compounds work with steel alloys designed to be non-reactive? I think something else that doesn't get mentioned is it's not just the risk of microplastics reacting, the physical non-reactive presence of particles can clog and get in the way of natural processes mechanically. So nonreactive shouldn't be taken to imply harmless. reply skirmish 6 minutes agorootparent> physical non-reactive presence of particles can clog and get in the way of natural processes mechanically That would mean fiber in food is harmful since it is not digested. Cellulose is just a common type of natural fiber. Meanwhile: USDA recommends that people consume the following amounts of fiber per day: -- Women ages 31–50: 25 grams -- Men ages 31–50: 38 grams reply Suppafly 1 hour agorootparentprev>the physical non-reactive presence of particles can clog and get in the way of natural processes mechanically Source? reply rendaw 5 minutes agorootparentTBH I can't remember what I was looking up when I first read about that (I feel like it was metals, glass, or cellulose again), but https://www.pnas.org/doi/10.1073/pnas.2104610118 seems pertinent: > In general, mechanical interactions of microparticles and nanoparticles on biological membranes are vaguely studied, despite their importance for biological systems (29, 41). Hereby, we will demonstrate that these microplastics induce a mechanical stress of model cell membrane without the need of indirect assumptions about biological pathways (26, 27). reply rcMgD2BwE72F 2 hours agorootparentprevAnd plastics are full of endocrine disruptors, which are pretty bad for human health too. reply thayne 1 hour agorootparentprev> \"Microsteel\" is just elemental iron which is a necessary nutrient. Steel also contains carbon and if it is stainless steel it also has chromium and probably other metals. reply SoftTalker 18 minutes agorootparentThe body does need trace amounts of chromium. reply giantg2 2 hours agorootparentprevWe know that some plastics mimic hormones (eg estrogens), which can cause problems (eg estrogenic cancers). reply Zaskoda 3 hours agorootparentprevElegant and pithy answer to a well asked question. reply JKCalhoun 2 hours agoparentprev> How does that compare with ... plastic-lined paper cups .. cling wrap that covers our food as we heat ... the Tupperware and other plastic containers we heat our food up in ... bottled water that sits inside plastic for months ... plastic bowls and utensils we use in our kitchens ... disposable serrated plastic knife... For myself, I don't do any of the above (with the possible exception of the last one once in a while). I thought everyone knew those were a bad idea. I do drink tea using tea bags though — and had no reason to believe there was plastic involved. reply semiquaver 2 hours agorootparent> and had no reason to believe there was plastic involved This is about certain tea bags which are recognizably plastic, e.g. the ones pictured in https://scitechdaily.com/warning-plastic-teabags-release-mic... reply namuol 1 hour agorootparentThe research specifically deals with cellulose bags which are often sealed with glues containing synthetic polymers. The picture from the study of the cellulose bags show a round “pillow” style bag which is likely sealed with a glue, unlike some cellulose bags which are folded and stapled: https://ars.els-cdn.com/content/image/1-s2.0-S00456535240263... From the article that summarized the study: > The tea bags used for the research were made from the polymers nylon-6, polypropylene and cellulose. The study shows that, when brewing tea, polypropylene releases approximately 1.2 billion particles per milliliter, with an average size of 136.7 nanometers; cellulose releases about 135 million particles per milliliter, with an average size of 244 nanometers; while nylon-6 releases 8.18 million particles per milliliter, with an average size of 138.4 nanometers. So while polypropylene is the worst of the three by an order of magnitude, the cellulose pillow-style bag still leaches a large number of particles. Here’s the study: https://www.sciencedirect.com/science/article/pii/S004565352... Notably, the authors tested OEM empty teabags for polypropylene and nylon, but chose a supermarket brand of cellulose pillow-style bags with tea still inside. reply xattt 2 hours agorootparentprevThis discussion is complicated by the loose (ha!) definition of tea bags. There’s about a million different tea bags. Some use denser paper, some are thin. Some are stapled, some are pressed. Some are stringed and some are not. Some are single-use cotton (which I learned about when a local tea brand stopped using them due to cost). Lipton makes a premium brand that uses a tetrahedral shaped micro-perforated plastic bag that very much could be shedding microplastics. It’s hard to have a discussion without a clear definition and terminology. reply therealcamino 34 minutes agorootparentThe paper describes the three kinds of tea bags tested, and how the results differ between them. reply cogman10 2 hours agorootparentprevJust FYI, you can buy stainless steel loose leaf tea infusers. They don't cost a lot ($6->$15) and loose leaf tea is shockingly cheap. Just get a nice airtight container and some moister absorbing packets and you'll have great tea for a while. I bought like 1lbs ~2 years ago for about $20 and still haven't worked all the way through it :D. reply prophesi 2 hours agorootparentI will also say that loose leaf is an order of magnitude better tasting than bagged tea. The crush-tear-curl process of bagged tea will elicit a bitter brew from anything that isn't black tea, and lose a lot of its flavor. Not to mention they're likely using the leftover chaff from loose leaf production. I like to show friends a properly brewed Dragonwell green tea and a bug-bitten oolong to convert them to the loose leaf way. reply cogman10 1 hour agorootparentIf you've not already gotten it, this is the next purchase I'd recommend [1]. Nothing better than instant hot water at the right temp :D. Doesn't take hardly any power to run either. [1] https://www.zojirushi.com/app/product/cvjac reply Suppafly 1 hour agorootparentnow you'll probably get some sort of micro-something coming from the non-stick coating inside that thing. reply cogman10 27 minutes agorootparentThe inside is silicon. Plumbing might be plastic, hard to tell. reply seabass-labrax 3 hours agoparentprevI've no idea why they chose teabags to study - one has to start somewhere, presumably - but I can answer the second question. The distinctive feature of plastics is the synthetic polymers that they contain, which classically feature bonds between oxygen atoms. These are extremely difficult for any organic process to break apart. Wood, however, can even be digested in small quantities, so 'microwood' will just break down into its constituent parts in the human body. The body can cope with metals and indeed has evolved to require a small amount, for instance in hemoglobin. We aren't fully aware of the implications of microplastics on health, but the main cause for concern is that we have no easy way of getting them out (either naturally or medically) in the event that they are harmful. reply cogman10 2 hours agorootparent> I've no idea why they chose teabags to study I think they are a pretty reasonable thing to study. Teabags are porous plastic subjected to high heat. So the question has to be \"what happens when these plastic baggies get exposed to high levels of heat? Does that liberate some of the plastic into the drink?\" Particularly worthy because tea is one of the most common beverages consumed. reply protonbob 3 hours agoparentprevWe do know that microplastics may be reducing male fertility. I know there are others but I haven't done a ton of research [1]. Wood and stainless steel are different because we have evolved with these materials in our surroundings or at least something close to them. Also, wood, and even metals, do not have the staying power of plastic. We already do know that heavy metals are bad if they stick around in your body, but we do need metals in our diets as nutrients as well. It should be assumed that anything that we have invented in the last 200 years should be guilty until proven innocent at this point imo. So many of the \"modern marvels\" have shown to have horrible health effects. [1] https://pmc.ncbi.nlm.nih.gov/articles/PMC9134445/ reply woleium 3 hours agorootparentThe dangers of sawdust (micro wood?) vary from species of tree to tree, but are generally well known and studied. in particular Manchineel and Yew are known to be dangerous. Wood in its natural state is not a safe substance. African Mahogany for example is highly toxic, causing dermatitis, respiratory issues, giddiness, vomiting, boils, asthma, headaches, and nosebleeds. Has also been linked to nasal cancer. https://www.mountainwoodworker.com/articles/toxic_woods.pdf reply scns 2 hours agorootparentWenge and Oak too. reply dylan604 2 hours agorootparentprev> We do know that microplastics may be reducing male fertility. If we \"know\" something, would we still be using the word \"may\"? Know seems pretty strong for such a wishywashy result of \"may\". There's definitely research in fertility rates lowering. Phtalates are receiving a lot of attention as well as an example. reply protonbob 2 hours agorootparentI didn't say that we know that they are reducing male fertility, we know that it's very possible. I could have said it probably does. But I'm not a scientist so I'm not going to. reply dylan604 1 hour agorootparentWe know it's very possible is a very wishywashy comment though. Best I can say is that there are theories, but we don't know if they are true or not yet. Some people think they are, some people disagree. That's why they are not facts. reply righthand 3 hours agoparentprevIf you microwave your consumables in plastic that is on you. Microwavable plastic is a marketing myth. Put your food on a plate or bowl and cover it with a wet paper towel. reply burkaman 2 hours agorootparentIt is absolutely not on you. In order to function as an adult you need to be able to have some level of trust in your family, your society, and your government (depending on where you live, I guess). That doesn't mean blindly believing everything you hear, but it does mean not having to do novel scientific research to confirm everything you were ever taught. The majority of people alive on Earth today grew up in a world where plastic packaging and containers were a common, completely accepted part of life. Research suggesting this is harmful is very new, and still not settled. You cannot blame anyone for not picking this random ubiquitous aspect of modern life and avoiding it because it might be bad for them. Home microwaves themselves are no older than home plastics - why do you trust them? reply righthand 2 hours agorootparentThey also grew up in a world that was skeptical of plastics and chose to ignore the skepticism because not being skeptical and blindly trusting your family, society, government is a fools errand. The exact issue is people promoting this “well I shouldn’t have to think/research” way of life. That’s just nonsense and the reason we’re here. It’s a cute dream but ignorance will just kill you. reply burkaman 2 hours agorootparentI am open to being convinced otherwise, but I don't agree that there was any widespread skepticism of the health impacts of plastic until very recently, maybe the last 5 years. There has certainly been broad concern about plastic trash and environmental pollution for a long time, but that's a different topic. I stand by my claim that you and I should not have to research the health impacts of, for example, microwaves. We should have to think about it, but if you have a basic understanding of how they work and how to use them safely, and you listen to people who might tell you if there were an issue (friends, the news, the FDA, etc.), then that is enough. And when I say \"how to use them safely\", I don't mean doing your own experimentation to find the limits of the device. I mean being told not to put metal in it, maybe watching a video to see what happens if you do, and accepting that it's a bad idea and you won't do it. It is not possible for me to do a medical study on the impacts of eating microwaved food, but I have enough societal trust that I continue to use them anyway. reply c0redump 7 minutes agorootparentI guess it depends on what segment of society you exist in. I (born in the early 90s) was raised in a plastic-free lifestyle, and many of the people my family associates with are the same. We are staunch environmentalists though, so I guess my experience is not typical. So, I don’t know about “widespread”, but the current of thought has been present in the zeitgeist for decades. reply righthand 41 minutes agorootparentprevPlastic is made from oil, you’d have to be pretty ignorant to believe people never were skeptical of plastics and that skepticism wasn’t covered up by lobbying. reply jajko 2 hours agorootparentprevNo I didnt and I dont know anybody who did. Unless you count proper nutjobs seeing conspiracies everywhere and world controlled by nanochips etc. reply righthand 40 minutes agorootparentYes you bought right into it. New thing? It must be fine! Framing skeptics as nutjobs is exactly what caused the ignorance. reply ssl-3 9 minutes agorootparentA broken clock can be right twice per day, but that doesn't mean that it is ever useful. reply imglorp 2 hours agorootparentprevWe have a set of reusable silicone lids. They can withstand high temperature on stove or microwave and just rinse off. Hopefully they aren't found to release anything. Instead of a paper towel, we throw food on a plate or bowl and drop a lid on it. This also works in the fridge; one less thing to wash and nothing disposed. reply righthand 2 hours agorootparentYou may be interested in Weck jars too that are made of glass for storage and are very affordable. They have glass lids. reply hindsightbias 2 hours agorootparentprevWhat makes you think there isn't BPA in your paper towels? https://pubmed.ncbi.nlm.nih.gov/21939283/ reply righthand 2 hours agorootparentBecause I was skeptical of paper towels too and only buy ones without. It is also noteworthy that the paper towel doesn’t soak in the consumable when reheating like a tea bag does. reply dboreham 2 hours agorootparentprevIsn't the wet paper towel and EM reflector? I use wax paper fwiw. reply almostnormal 2 hours agorootparentI use a second plate, upside down. reply righthand 2 hours agorootparentprevIt would reflect under the paper towel too then, therefor serving it’s purpose. reply stevebmark 42 minutes agoparentprevIt is well established that heating plastic in a culinary context distributes significantly more microplastic into your food. Microplastics are also well established endocrine disrupters. Microplastics cross the blood brain barrier and may also permanently stay in your body. To what extent are these harmful? In what dose, over how much time? I don’t think that’s established. You could be cautionary, or wait for more science about how long it takes to reduce your fertility. It may also be inescapable, plastic is likely a permanent earth pollutant now, in your clothes, dust in the air, food, water, and most things in your home, including ones you abrasives use inside your body, like toothbrushes. Maybe only very high doses (like drinking tea from teabags once a day) have a detrimental health effect. Many compounds are lethal in high doses, and healthy, benign, or required for survival in low doses. reply bowmessage 3 hours agoparentprevI don’t do anything in that list you just mentioned, and I will probably stop drinking tea from a bag now. This is helpful research. reply brookst 3 hours agorootparentBut is it? How confident are you that this is your greatest exposure? Odds are there is something else in your life at least 100x as bad. And what does it mean that cellulose, a naturally occurring compound, releases 15x more microplastics than nylon? Or does iy? This study didn’t measure nanoplastics. That’s what a lack of context does. No harm in just avoiding anything any study has found to be potentially harmful (especially tea bags, which are a crime against good tea and easily replaced). But… it’s impossible to know if this is the equivalent of stopping smoking, or of brushing teeth three times a day instead of two. reply woleium 2 hours agorootparenthow do we find the 100x as bad thing if we do not do research like this. The authors did not write this to provide you with a guide for life, they are instead trying to increase our collective knowledge. I wonder sometimes if folks understand how science works. reply Suppafly 1 hour agorootparent>they are instead trying to increase our collective knowledge I'm not sure that's even clear since they seem to be conflating cellulose with microplastics. reply rc_mob 2 hours agorootparentprevHave to start somewhere with avoiding plastics. This is as good a place to start as any other. reply JKCalhoun 2 hours agorootparentprevI think we're simply responding to the list the OP gave — which many of us do not do. reply __MatrixMan__ 2 hours agorootparentprevLose leaf tea is much better anyway. You can get multiple infusions out of it which is nice if you don't need the caffeine the second time around (it's quite water soluble and mostly all goes in the first infusion). A second infusion with bags always just ends up kinda watery and sad. Something about the leaves being smaller... reply acjohnson55 2 hours agorootparentYep. I used to regularly drink high-grade ti guan yin (a.k.a. iron goddess), and I could often get 8 steeps and still have plenty of flavor. The trick was to use a lot of tea and steep it for only 30 seconds. One of the advantages was how quick it was to get my next cup. reply Vegenoid 2 hours agorootparentprevYou never have food or drink that was stored or served via plastic containers? How? I ask seriously - how do you live your life to entirely avoid this, while also not living a life so separate from society that you are drinking tea made from tea bags? reply dylan604 2 hours agorootparentMany people drink tea not from tea bags to the point that \"many\" isn't really descriptive enough. If you're a tea aficionado, you definitely don't. Which means there's an entire market of people doing things like an aficionado even if they are not; see audiophiles. Only mass market large brands push the tea bag. Good tea comes packaged as loose leaf meant to be used in whatever strainer you have. reply bowmessage 2 hours agorootparentprevI never eat or drink anything heated in plastic when I can control it. Sure it may have been stored in plastic at some point, but not heated. reply Suppafly 1 hour agorootparent>I never eat or drink anything heated in plastic when I can control it. Do you avoid restaurants and cafeterias completely? reply nycdatasci 1 hour agorootparentprevIn the study, they put 300 nylon/plastic bags into 1L of near-boiling water. Many bags are paper derivatives and not plastic. No need to completely stop enjoying tea. reply 0-_-0 3 hours agorootparentprevWhere do you find plastic tea bags? I don't think I've ever seen one. reply gaoshan 2 hours agorootparentMost tea bags that you purchase anywhere use some level of plastics in their component materials and/or binding (especially this latter). The only safe options are metal strainers that you filter the tea with (and that hopefully don't have coatings on them that are harmful... boiling a new one would not be a bad idea before first use) or just loose leaf. reply TeaBrain 3 hours agorootparentprevMesh tea bags, like are used at Starbucks, are plastic. reply Suppafly 1 hour agorootparentprevbourgie brands of tea come in little nylon pyramids instead of the normal paper/cloth bag. reply bluejekyll 3 hours agorootparentprev“The tea bags used for the research were made from the polymers nylon-6, polypropylene and cellulose.” They aren’t pure plastic. reply bagels 3 hours agorootparentprevHow do you cook food? reply stevenwoo 3 hours agorootparentIt's not hard to cut out the exposure listed in the comment with a little effort and time, except for the steel thing which is a bit over the top. reply woleium 2 hours agorootparentit’s actually needed, see lucky fish: https://en.m.wikipedia.org/wiki/Lucky_iron_fish reply selimthegrim 2 hours agorootparentDebatable efficacy. reply marliechiller 3 hours agorootparentprevCast iron pots and pans reply bowmessage 2 hours agorootparentprevAlso, glass Pyrex Tupperware. reply __MatrixMan__ 2 hours agorootparentprevSteel and glass reply evrenesat 56 minutes agoparentprevI eat processed, ready-to-eat stuff at home, heavily use microwave for heating, but I don't do/use any of the things you mentioned. I generally prefer loose leaf tea, but sometimes tea bags are easier, so I only buy brands that use natural tea bags. It's possible to reduce exposure to pollutants if you're willing to sacrifice some convenience. reply glenstein 2 hours agoparentprevI absolutely agree that meaningful context would be helpful, but I don't see that as disqualifying. I appreciate that research is opportunistic, as often oriented toward discovery of new things not yet understood, as much about building up our factual understanding as interpretation. So sure, I want context, but I think this kind of exasperation is a bit misplaced, as I don't think the article or the research itself was intended to be a comprehensive account of the broader contexts you are looking for. If it was masquerading as such a thing, I would be in full agreement. So I think it's a fair point in general, but the way you are saying it here sounds an awful lot like you're holding up a stop sign and saying \"don't do any more research!\" reply Reason077 2 hours agoparentprev> ”from the cling wrap that covers our food as we heat it in the microwave, from the Tupperware and other plastic containers we heat our food up in” You shouldn’t really be doing either of those things. Plastic tupperware will get damaged from heat if you use it in the microwave frequently, potentially contaminating your food. It’s best to transfer food to a heat-safe container (glass or ceramic) before microwaving. And definitely don’t use cling film in the microwave! reply __MatrixMan__ 2 hours agoparentprevI think it's pretty reasonable to expect a bag made of a fine mesh of plastic to yield more tiny broken off pieces than something like plastic container. Also once you put your mind to it it's actually pretty easy to avoid most of the things you mentioned. There are glass or metal alternatives to pretty much everything plastic. Maybe not for creating an airtight seal over something like leftovers, but I think it's reasonable to expect that the food can sit in glass and have a plastic roof and still be relatively free of microplastics. More research is needed it seems pretty plausible that plastics, like asbestos, are only a hazard when friable. reply JoeAltmaier 2 hours agoparentprevThere is no evidence of harm. Your body continually rids itself of them. This is a lot of passion and angst over what may be nothing. reply wyxz 2 hours agoparentprev> First of all, what’s with the focus on tea bags? Well, lucky for you, there's an entire scientific study detailing why they chose to study it and the methodologies they used. You know, the one linked in the article. It states: > Among the different food containers releasing MNPLs, teabags stand out. Recent investigations have elucidated that teabags significantly contribute to the release of millions of MNPLs, adding to their daily ingestion by humans. And this is just a snippet! Much more detail and context available within! The wonders of original sources. reply lkbm 2 hours agoparentprev>First of all, what's with the focus on tea bags? According to the paper[0]: > Among the different food containers releasing MNPLs, teabags stand out. Recent investigations have elucidated that teabags significantly contribute to the release of millions of MNPLs, adding to their daily ingestion by humans (Banaei et al., 2023). The cited Banaei et al., 2023[1] says \"At this point, special attention should be paid to the release of MNPLs from the herbal/teabags, since during the soaking and steering processes, some MNPLs inevitably detach and migrate to the water solution\", citing [2]...which is retracted with this explanation: \"2 of the reviews for this manuscript were fictitious. 2 reviews were submitted under the name of known scientists without their knowledge.\" So, yeah. Sometimes it's interesting to follow citation chains a few steps. [0] https://www.sciencedirect.com/science/article/pii/S004565352... [1] https://www.sciencedirect.com/science/article/pii/S030438942... [2] https://www.sciencedirect.com/science/article/pii/S004896972... reply jklinger410 1 hour agoparentprevTea drinkers in shambles reply tonygiorgio 2 hours agoparentprevAgreed. While some people are nit picking the comment here as “well don’t do any of those things,” it still doesn’t quantify the danger. Recently read from “Made to Stick”: “Don’t just say popcorn has 40g of trans fats. Everyone knows trans fats are bad, but how bad is bad? Say popcorn has more trans fats in one serving than a whole day of greasy junk food” reply sharpshadow 3 hours agoparentprevOne should avoid all the things which you mentioned and using a microwave at all. The context is a comparison between good old paper tea bags. reply yreg 3 hours agorootparent> using a microwave at all Why? reply hindsightbias 2 hours agorootparentprev> good old paper https://pubmed.ncbi.nlm.nih.gov/21939283/ reply sharpshadow 2 hours agorootparentThis study does not specifically mention paper tee bags. It mentions food contact paper but that’s something different. In Germany paper tea bags are made out of natural fiber without glue. But I would really like to know if this paper tea bags also release hefty amounts of microplastics, if somebody has a study please link. That those plastic tea bags are microplastic hell should have been obvious from the start. The first time I saw them until forever I will avoid them. reply vasco 3 hours agoparentprevI mean, why not? I get your point but before wide encompassing studies and meta studies, a lot of things will be looked at because we can look at them. It's like asking why investigate dolphin language instead of all animal communication. reply 14 2 hours agoparentprevIf one had to study the entirety of a subject it would be impossible to do so in many cases. It is entirely reasonable for a researcher to pick one piece of the puzzle and study that. No it does not give the entire picture but it may help us understand the greater picture. Lastly if this was anywhere else besides HN there is a microwood entering you joke to be made. reply mandmandam 3 hours agoparentprevThese are all questions that are pretty easy to answer these days; perfectly appropriate for asking to an LLM or search engine. Long story shot - yes, this is a major problem. Yes, you're getting it from bottled water and plastic utensils and plastic lined cups. No, it's not like microwood. This shit is being found in every organ of our bodies from our sex organs to our brains. It's found in most wild animal samples, it's found in rain, it's found on Everest's peak and in the Mariana Trench. And every indication is that it's getting rapidly worse, scaling up with our ever-increasing plastic production. And there are perfectly good alternatives for the vast majority of this use, but the costs are a bit higher (since they're not being externalized onto the planet and our organs as much). reply jasonlotito 2 hours agoparentprevFrom the study linked in the article: \"Overall, our findings contribute to a growing body of evidence on the pervasive nature of plastic pollution and its potential implications for human health. As the usage of plastics in food packaging continues to rise, scientific research and policymaking must address the challenges posed by MNPL contamination to ensure food safety and consumer well-being.\" > I find it impossible to put this into any kind of meaningful context. So? Your inability to find meaningful context in something is not important. Who are you and why should this article or study cater to you? Are you in the business of doing research on this topic? Or are you just an HN commenter? Your ignorance is not a sign of anything other than you being ignorant, and your inability to do something is just that: lack of skill. > First of all, what's with the focus on tea bags? Because you can't just assume. You test. That's science. Just assuming (your suggestion) is anti-science. And something we should NOT base our science off of. > Second, how do \"microplastics\" compare to micro-everything else? That's not what this study is trying to determine. It set out to determine how much microplastics came from tea bags. Why increase the scope. Other people are studying that. You really don't seem like someone who understands how this works. You don't put the puzzle together all at once, you put it together one piece at a time. reply therealcamino 23 minutes agorootparentExactly. This is a study about the exposure and absorption parts of the equation. The science is ongoing. People who can't deal with anything less than total certainty don't understand the process. reply wyxz 2 hours agorootparentprevI find it quite ironic how many people on HN have a superiority complex to sites like Reddit, yet suffer the same pitfalls. Not reading the source material and going off of headlines or snippets. Asking for “context” and “why did they study this” is quite interesting, considering the scientific study whose entire purpose is to introduce this context is directly linked within the article. reply brokegrammer 3 hours agoparentprevnext [3 more] [flagged] xandrius 2 hours agorootparentJust for your information, the attitude in this comment makes it hard to actually engage it in a serious conversation. It does have \"conspiracy uncle\" vibes and so people might avoid replying. Maybe that's your goal all along but if not, try to reread your comments before posting them, as they might not reflect how you actually want to come across with others. reply brokegrammer 2 hours agorootparentWhat did you find offensive about my comment? reply jakub_g 3 hours agoprevTalking tea bags, this is a rabbit hole as a few sibling commenters pointed out already: - most tea bags contain plastic themselves - pretty much every bakery / small coffee place place serves tea in paper cups lined up with plastic, it's very difficult to get a tea in a proper ceramic cup those days - waters heaters often have plastic lids - pretty much every insulated thermos also has at least a plastic cover For the last one, a friend has recently found some plastic-free thermos: https://www.kleankanteen.com/collections/plastic-free Please share if you know others. reply xandrius 2 hours agoparentI think for most places in the world, tea is an activity done at home with own cups/glasses. Or at least, say in China/Taiwan, the heated tea gets made cold in steel containers and then served cold in plastic cups (e.g. For milk tea) else all is ceramic. The main point is that even who doesn't go to buy a tea elsewhere get in contact with microplastic. So, now many people might have to switch to loose leaf to avoid getting another source of microplastics in their daily lives. reply warkdarrior 24 minutes agorootparent> So, now many people might have to switch to loose leaf to avoid getting another source of microplastics in their daily lives. Doesn't this study show that cellulose releases microplastics as well? I assume tea leaves have cellulose in them, so best to avoid tea altogether. reply jajko 2 hours agorootparentprevLoose leaves are more inconvenient compared to tea bags. All else equal dont expect major adoption just for yet another health scare. People in general are lazy. reply highfrequency 1 hour agoparentprevDoesn’t the first product (bamboo topped lid) have a rubber ring to seal it? reply oidar 3 hours agoprevthis is from the same journal with the black plastic cooking utensils that has been remove from a major index. https://arstechnica.com/health/2024/12/journal-that-publishe... reply rc_mob 2 hours agoparentScience loves proving science wrong. I'm glad science overcame science on the issue of blqck plastic reply nikolayasdf123 3 hours agoprevother big overlooked area — paper cups are covered inside with hydrophobic film that is made of plastic, and given extra hot water makes plasticisers get off substance, chances are all those paper cups are releasing lots of microplastics into hot water. ask for mugs folks. reply julianeon 2 hours agoparentThe alternative for me isn't to ask for a mug, it's an argument for skipping all that and making it at home. reply JKCalhoun 2 hours agorootparentYeah, coffee served in a paper cup even tastes like jank. When I can make coffee at home that is better than the cafes — why drink out? reply double0jimb0 3 hours agoparentprevOr are they coated with wax/parafin? reply pavon 2 hours agorootparentGrowing up, paper cups were all coated in wax, but I can't remember the last time I'd seen one coated in wax in the US, it's been at least a decade. Back then I remember being cautioned against using paper cups for hot drinks as it would melt the parafin (which wasn't great to drink) and then the paper would get soggy. Hot drinks to go were mostly served in styrofoam back then, while they are mostly plastic coated paper now. So that was probably a small step forward, although reusable ceramic or metal is better than either. reply JKCalhoun 2 hours agorootparentprevI remember encountering those — meant for cold beverages. When you hit them with hot water a kind of wax film floats to the top. reply DoneWithAllThat 3 hours agoparentprev“Chances are” is not science, it’s just obsessing over the current Scary Thing. You’re just making things up. reply HPsquared 3 hours agorootparentYour own comment is also not science. Scientific results (particular testing conditions and results) need to be interpreted and actions decided on in the real world. Policy is not science either. reply nikolayasdf123 3 hours agorootparentprevhttps://www.sciencedirect.com/science/article/abs/pii/S03043... > Several studies in the past have shown that harmful chemicals and substances can leach from paper and paperboard-based food packaging into the food meant for human consumption (Choi et al., 2002, Hansen et al., 2013, Schaider et al., 2017, Trier et al., 2011, Trier et al., 2018, Deshwal et al., 2019, Vandermarken et al., 2019). https://youtu.be/i5611OvTFGM?feature=shared&t=5289 > \"BPA, phthalates, plasticisers etc. are not chemically bound to substance they added too, and under heat they come out [...] you don't want to mix these with your food, but the worst thing to do is to put it in the heated environment\" — Dr. Shanna Swan, Ph.D reply saas_sam 3 hours agoparentprevJust make sure the mugs weren't washed with Jet Dry... Avoiding toxic and questionable substances really does get exhausting after awhile. It's everywhere. I'm able to draw a reasonable line (for me) without getting too nuts about it. Hoping AI ends up helping with this. reply gosub100 3 hours agorootparentwhat would AI do that human intelligence couldn't? Is it not a linear cause-and-effect relationship? more plastics cause more illness? (yesno). If plastics are toxic, wouldn't we see for ourselves (thus NOT requiring AI) a proportionate increase in sickness in samples (people or animals) with higher plastic in their bodies? Why is the message more profound if AI tells you versus a human team of college researchers? reply relaxing 50 minutes agorootparent> If plastics are toxic, wouldn't we see for ourselves (thus NOT requiring AI) a proportionate increase in sickness in samples (people or animals) with higher plastic in their bodies? Yeah man I notice microplastics in tissue samples from my sick friends all the time. Haven’t you? reply semiquaver 2 hours agoprevAlso on the front page: the journal this study was published in was recently removed from a major index of journals for failing to meet quality criteria: https://retractionwatch.com/2024/12/18/journal-that-publishe... reply malfist 3 hours agoprevAt this point the question might be, what doesn't release micro plastics. I don't use tea bags, all my tea is loose leaf, but I'm sure it's still got micro plastics somewhere in it reply mrspuratic 2 hours agoparentIf you home-compost you get used to finding tea bag skeletons in the compost. For years I used to rip open the used tea bag, compost the tea and discard the bag. In the last few years the largest two brands here (Ireland: Lyons and Barrys) have gone somewhere between \"plastic free\" and \"biodegradable\" (but not home-compostable). 95% of tea is sold in the form of tea bags here. https://livinglightlyinireland.com/2021/02/12/plastic-free-t... (article is from 2021, I think the title is suffering from a Wordpress date placholder) reply kjkjadksj 55 minutes agorootparentI throw the whole teabag, rope and all, into the compost and the mealworms make it into incomprehensible brown slush along with the rest of the pile in a few days. It’s always way more productive using insects in the compost. reply selimthegrim 2 hours agorootparentprevThanks for this; have been buying Barry’s lately in US reply seniortaco 2 hours agoparentprevWatch out who you shake hands with, you might absorb micro plastics from them if they use a lot of Tupperware. reply timthelion 3 hours agoprevIs celulose a 'microplastic' though? Obviosly most tea bags are not made of plastic, at least historically... reply mandmandam 3 hours agoparentThese days many tea bags use polymers in their glue; and I don't think this article implied anywhere that cellulose is microplastic: > The tea bags used for the research were made from the polymers nylon-6, polypropylene and cellulose reply LegitShady 3 hours agoparentprevcellulose on its own is not a plastic but they make bioplastics out of cellulose that are plastics. reply hyperific 2 hours agoprevTo all those who are asking \"Why teabags?\", it's in the introduction section of the study. > Among the different food containers releasing MNPLs, teabags stand out. Recent investigations have elucidated that teabags significantly contribute to the release of millions of MNPLs, adding to their daily ingestion by humans (Banaei et al., 2023). reply acidburnNSA 3 hours agoprevWhat materials do different brands of tea make their tea bags out of, I wonder? reply vinni2 3 hours agoparentI wondered the same and unfortunately the manufacturers don’t publish the material of the tea bags. I am switching to loose tea and even when I only have tea bags i can take the tea out and brew it in reusable metal thing. reply mhandley 3 hours agoparentprevHere's a pretty complete list of UK brands: https://moralfibres.co.uk/the-teabags-without-plastic/ reply drunkonvinyl 3 hours agoparentprevNicer teas (pricier usually) come in cotton muslins - https://www.mariagefreres.com/en/tea.html?conditionnement=47. Their Marco Polo tea is my once a year splurge. The bag is tied with string - no glue. reply Fluorescence 3 hours agoparentprevThe article made me check Yorkshire Tea and I'm not really quite sure. How bad are \"plant-based plastics\"? > made from natural fibres like wood pulp and the seal is made with PLA - an industrially compostable, plant-based plastic > ... so we're following WRAP's advice and avoiding the phrase \"plastic free\" https://www.yorkshiretea.co.uk/our-packaging https://www.yorkshiretea.co.uk/brew-news/plastic-in-tea-bags... reply vouaobrasil 3 hours agoparentprevShould be a law to put the materials used on all products, not just foods. reply NooneAtAll3 3 hours agoprevfascinating how one piece of bad journalism disqualifies whole research - we already have 2 comments (and I was about to post the 3rd) here about \"why cellulose is in the list of microplastics?\" in my case I opened the news story with a question \"what microplastics even are there in a paper bag??\", then saw the sentence \"The tea bags used for the research were made from the polymers nylon-6, polypropylene and cellulose.\" reply nycdatasci 1 hour agoprevIn this study, they placed 300 tea bags in 1 liter of near-boiling water. For those asking \"why tea bags?\" they're widely used and easy to research. Putting 300 tea bags into a container is much easier than sequentially microwaving a liter of water in 300 different plastic containers to measure the impact of microwaving food in plastic. reply blueflow 2 hours agoprev> The paper is published in the journal Chemosphere. From https://news.ycombinator.com/item?id=42494733 > The publisher of a high-profile, now-corrected study on black plastics has been removed from a critical index of academic journals after failing to meet quality criteria, according to a report by Retraction Watch. >On December 16, Clarivate—a scholarly publication analytics company—removed the journal Chemosphere from its platform, the Web of Science, which is a key index for academic journals. reply bruce343434 30 minutes agoprevWhy don't people just use stainless steel tea eggs?? reply righthand 3 hours agoprevThere are tons of companies that have tea bags made out of biodegradable materials such as tree bark. Or if you don’t have access, consider a metal steeper and loose tea. Unbelievable the stuff people just shove in their gullet with no inspection. reply mmh0000 2 hours agoparentTo be fair, if you live in a developed country, you have organizations like the FDA which \"should\" be vetting the \"food\" on store shelves as \"safe\". Now whether or not they actually do a good job at that task is a different question. It's unfortunate that you'd expect millions of people to individually \"do research\" (i.e. consult their crystal-stroking astrologist nutjob friend on Facebook). There is no reasonable way for individuals to make informed decisions regarding each individual item they may eat. The Good Place has a great little clip about just the difficulty of buying a tomato: https://www.youtube.com/watch?v=R8m_5HDZF7w reply righthand 2 hours agorootparentI don’t expect millions of people to do research, I expect people to look at the thing in front of them and think, “wait what is this made of? It looks like plastic, maybe I should be skeptical.” However if the country is going to be based on lobbying efforts you better damn well be able to do a little research. There is a reasonable way to make informed decisions. It starts with skepticism and not just shoving food into your face because it’s popular. reply cbdhsjshs 1 hour agorootparentWhy should the average consumer be concerned about plastic? Plastic food packaging has been the norm my whole life. You're asking a consumer to do what a regulatory institution should do. reply righthand 46 minutes agorootparentA regulatory institution that’s constantly politicized and lobbied for corporations to have profits maximized will do the right thing. You are asking to be treated like cattle at that point. reply jajko 1 hour agorootparentprevGood luck meeting that expectation with people doing their weekly grocwries. Dreamy looks in the distance while difficult questions are being answered, precise estimations based on looks and tons of furious googling happening among the isles for each of the 113 items bought that day. And then realizing 109 of those are somehow interacting with plastics anyway, and there is nothing better in the shop. People have too much on their plate to grind their lives to halt for 1 out of too many worries just for groceries. reply righthand 44 minutes agorootparentYes it’s tragic but that’s the world you’re asking for, people not to have to think because it’s hard. You’re validating that with some ridiculous scenario. Let’s simplify it: is the consumable touching plastic? It’s got plastic in or on it. Is it processed food? It’s got plastic in or on it. If you want to take the approach where you buy a bunch of stuff and then sit down for hours auditing everything you spent money on, then that’s on you. Consider that if the issue is so extensive you can do the analysis piecemeal if paranoia of it all causes you to be exhausted. Or don’t wait for reports like this and a politicized regulatory agency in the age of hysteria to make the decisions for you. reply frankus 2 hours agoparentprevIf you buy tea from Starbucks in North America you're getting a nylon teabag. Maybe I should be more paranoid about microplastics but I avoid them because I can't just toss them in the compost like I can with paper- or silk-based ones. reply leobg 3 hours agoprevCuriously, it seems the best way to reduce (though not avoid) micro plastic exposure from tea bags is to switch from cellulose tea bags to plastic ones – nylon: > polypropylene releases approximately 1.2 billion particles per milliliter, with an average size of 136.7 nanometers; cellulose releases about 135 million particles per milliliter, with an average size of 244 nanometers; while nylon-6 releases 8.18 million particles per milliliter, with an average size of 138.4 nanometers. reply marcosdumay 2 hours agoparentThe nylon strength and stability making for counterintuitive trade-offs yet again. It's a ridiculously underrated material. (Does it fare worse or better if reused? I don't have a clue, but there are reusable nylon bags.) Anyway, particle size probably matters, and I'm not sure it wins on that count. Also anyway, you can get a steel tea holder, so the best way is to completely avoid plastics here. You can also probably get your leaves much cheaper in 200g or 1kg bags. reply leobg 1 hour agorootparentSure. I use whole leaf, brew it in a glass cup, and pass it through a stainless steel sieve. Bags seem wasteful on so many levels. reply vinckr 3 hours agoparentprevor use a small metal or ceramic bowl to hold the tea -> zero plastic involved. reply mandmandam 3 hours agoparentprevWell, no, because those measurements are per ml. A nylon bag uses many more mls than a paper bag using plastic glue. reply keepamovin 3 hours agoprevI can taste and feel the chemicals in tea bags, so I always wash the bags with cold water, then a first draw of boiling water, then I fill it up. Feels much cleaner. When dealing with our highly processed factory food products that come to you direct from a factory, a first wash with clean water is usually a good bet. reply Beijinger 3 hours agoprevI don't know about tea bags, but guys, do yourself, your health and your finances a favor and buy this: https://www.amazon.com/dp/B0DF1ZHXPG It works surprisingly well, the coffee tastes better and it is not messy. reply cbdhsjshs 1 hour agoparentCafelat Robot Nothing but metal and silicone. reply bdcravens 3 hours agoparentprevWe've used refillable pods for years now. reply Beijinger 3 hours agorootparentYeah. Same shit with water. https://www.plasticpollutioncoalition.org/blog/2024/1/10/stu... Currently, I buy Pellegrino at Costco. But probably I just should get a sodastream with a glass bottle and filter my tap water. Shipping water from Italy. Fucking insane. Maybe I should open my own bottling company with glass bottles. reply ysleepy 3 hours agoparentprevI just use a French Press. reply bdcravens 3 hours agorootparentWe did for years, and even have a pretty cool one with dual filtration that prevented almost all grit. (the Espro) However, I didn't enjoy the wasted coffee and cleanup, and found that using refillable pods used less coffee and mess for the same product. reply javier_e06 2 hours agoprevOne solution is cut the bags with scissors and use the leaves. I remember those metal pods with a chain. Before tea bags became a thing. reply giantg2 2 hours agoprevI switched to tea balls simply because I could taste the bags. I guess this is an added bonus. reply darrensharm 2 hours agoprevThat's why most tea bags in Germany are made from starch. Land of high tech engineering, I guess. reply leobg 1 hour agoparentIt’s just one of those areas where we weren’t quite fast enough to catch up with the U.S. in copying their mistakes. Though I think it is true that the tea bag used to be a German idea. reply 1over137 3 hours agoprevhttps://archive.is/mAXxS reply api 3 hours agoprevThis is probably the lead of the 21st century, something all over that we have kind of known is bad but haven’t paid attention to. Add endocrine disruptors along with microplastics since the origin is similar. In 50 years there will probably be a lot less plastic used in contact with food, and what is used will be formulated differently. It will be similar to the gradual removal of lead from everything. reply jjtheblunt 3 hours agoparentMaybe in the \"western world\", but look up asbestos use and mining and export in brazil, china, russia and you may be horrified. reply gen220 2 hours agoparentprevYeah and similar to lead (and asbestos as someone else mentioned), once you see it you can't really un-see it. The idea of re-heating food by microwave in a plastic storage container, or purchasing anything acidic or liquid from the grocery store that's stored in a plastic or plastic-lined container elicits a similar feeling in my brain to sleeping under popcorn ceilings or drinking water from a house with lead pipe plumbing. Yet, in the U.S. at leaste, people have been doing this literally for decades without thinking twice :/ reply toss1 3 hours agoparentprevYup. Seems like a great idea at the time, until it is both such a great idea that it becomes ubiquitous and we start looking at it more closely. Kind of a proverbial 'boiling the frog' problem. A little bit of handling lead pipes or pumping leaded gasoline or inhaling or ingesting microplastics won't do anything. But when modern life gets so we're all bathing in the stuff all the time, the big problems are revealed. reply solnyshok 3 hours agoprevhopefully, they did find some traces of tea in those tea-bags reply canadiantim 1 hour agoprevNot surprising. I’ve always assumed this was the case. reply hfgjbcgjbvg 3 hours agoprev [–] Right on. Gotta love 2024. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Commercial tea bags made from polymers such as nylon-6, polypropylene, and cellulose can release microplastics, which have the potential to enter human cells.",
      "The study aimed to develop a methodology for analyzing microplastics rather than making specific claims about paper tea bags, highlighting ongoing debates about naturally-occurring polymers like cellulose.",
      "Concerns about microplastics are not limited to tea bags but also include other everyday items like plastic-lined cups and utensils, indicating a broader environmental and health issue."
    ],
    "points": 219,
    "commentCount": 210,
    "retryCount": 0,
    "time": 1734965276
  },
  {
    "id": 42493464,
    "title": "Can AI do maths yet? Thoughts from a mathematician",
    "originLink": "https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/",
    "originBody": "Xena Mathematicians learning Lean by doing. Skip to content Home About Xena Student projects What maths is in Lean? Installing Lean and mathlib Twitter Useful links. ← Fermat’s Last Theorem — how it’s going Can AI do maths yet? Thoughts from a mathematician. Posted on December 22, 2024 by xenaproject So the big news this week is that o3, OpenAI’s new language model, got 25% on FrontierMath. Let’s start by explaining what this means. What is o3? What is FrontierMath? A language model, as probably most people know, is one of these things like ChatGPT where you can ask it a question and it will write some sentences which are an attempt to give you an answer. There were language models before ChatGPT, and on the whole they couldn’t even write coherent sentences and paragraphs. ChatGPT was really the first public model which was coherent. There have been many other models since. Right now they’re still getting better really fast. How much longer this will go on for nobody knows, but there are lots of people pouring lots of money into this game so it would be a fool who bets on progress slowing down any time soon. o3 is a new language model. FrontierMath is a secret dataset of “hundreds” of hard maths questions, curated by Epoch AI, and announced last month. “Hundreds” is a quote from the paper (first line of the abstract), but I’ve heard a rumour that when the paper came out there were under 200 questions, although I’ve heard another rumour that apparently more are have been added since. As an academic mathematician who spent their entire life collaborating openly on research problems and sharing my ideas with other people, it frustrates me a little that already in this paragraph we’ve seen more questions than answers — I am not even to give you a coherent description of some basic facts about this dataset, for example, its size. However there is a good reason for the secrecy. Language models train on large databases of knowledge, so the moment you make a database of maths questions public, the language models will train on it. And then if you ask such a model a question from the database they’ll probably just rattle off the answer which they already saw. How hard is the FrontierMath dataset? So what are the questions in the FrontierMath dataset like? Here’s what we know. They’re not “prove this theorem!” questions, they’re “find this number!” questions. More precisely, the paper says “Problems had to possess definitive, computable answers that could be automatically verified”, and in the five sample problems which were made public from the dataset (Appendix A of the paper, pages 14 to 23) the solutions are all positive whole numbers (one answer is 9811, another is 367707, and the final three solutions are even larger — clearly these questions are designed in such a way that random guesswork is extremely unlikely to succeed). The sample questions are nontrivial, even to a research mathematician. I understood the statements of all five questions. I could do the third one relatively quickly (I had seen the trick before that the function mapping a natural n to alpha^n was p-adically continuous in n iff the p-adic valuation of alpha-1 was positive) and I knew exactly how to do the 5th one (it’s a standard trick involving the Weil conjectures for curves) but I didn’t bother doing the algebra to work out the exact 13-digit answer. The first and second question I knew I couldn’t do, and I figured I might be able to make progress on the 4th one if I put some real effort in, but ultimately I didn’t attempt it, I just read the solution. I suspect that a typical smart mathematics undergraduate would struggle to do even one of these questions. To do the first one you would, I imagine, have to be at least a PhD student in analytic number theory. The FrontierMath paper contains some quotes from mathematicians about the difficulty level of the problems. Tao (Fields Medal) says “These are extremely challenging” and suggests that they can only be tackled by a domain expert (and indeed the two sample questions which I could solve are in arithmetic, my area of expertise; I failed to do all of the ones outside my area). Borcherds (also Fields Medal) however is quoted in the paper as saying that machines producing numerical answers “aren’t quite the same as coming up with original proofs”. So why make such a dataset? The problem is that grading solutions to “hundreds” of answers to “prove this theorem!” questions is expensive (one would not trust a machine to do grading at this level, at least in 2024, so one would have to pay human experts), whereas checking whether hundreds of numbers in one list correspond to hundreds of numbers in another list can be done in a fraction of a second by a computer. As Borcherds pointed out, mathematics researchers spend most of the time trying to come up with proofs or ideas, rather than numbers, however the FrontierMath dataset is still extremely valuable because the area of AI for mathematics is desperately short of hard datasets, and creating a dataset such as this is very hard work (or equivalently very expensive). This recent article by Frieder et al talks in a lot more depth about the shortcomings in datasets for AI in mathematics. So there was an article about the FrontierMath dataset in Science and I was quoted in it as saying “If you have a system that can ace that database, then it’s game over for mathematicians.” Just to be clear: I had nothing to do with the dataset, I’ve only seen the five public questions, and was basing my comments on those. I also said “In my opinion, currently, AI is a long way away from being able to do those questions … but I’ve been wrong before”. And then this week there’s an announcement that the language model o3 got a score of 25 percent on the dataset. I was shocked. What exactly has happened here? Why was I shocked? Because my mental model on where “AI” is currently, when it comes to doing mathematics, is “undergrad or pre-undergrad”. It’s getting very good at “Olympiad-style” problems of the sort given to bright high-schoolers. Within a year it’s absolutely clear that AI systems will be passing undergraduate mathematics exams (not least because when you’re setting an undergraduate mathematics exam you ideally need to make sure that you don’t fail 50 percent of the class, so you throw in a couple of standard questions which are very similar to questions that the students have seen already, to ensure that those with a basic understanding of the course will pass the exam. Machines will easily be able to ace such questions). But the jump from that to having innovative ideas at advanced undergrad/early PhD level beyond recycling standard ideas seems to me to be quite a big one. For example I was very unimpressed by the ChatGPT answers to the recent Putnam exam posted here — as far as I can see only question B4 was answered adequately by the machine, most other answers are worth one or two out of 10 at most. So I was expecting this dataset to remain pretty unattackable for a couple of years. My initial excitement was tempered however by a post from Elliot Glazer from Epoch AI on Reddit where he claimed that in fact 25 percent of the problems in the dataset were “IMO/undergrad style problems”. This claim is a little confusing because I would be hard pressed to apply such adjectives to any of the five publically-released problems in the dataset; even the simplest one used the Weil conjectures for curves (or a brute force argument which is probably just about possible but would be extremely painful, as it involves factoring 10^12 degree 3 polynomials over a finite field, although this could certainly be parallelised). This of course raises questions in my mind about what the actual level of the problems in this secret dataset is (or equivalently whether the five public questions are actually a representative sample), but this is not knowledge which we’re likely to have access to. Given this new piece of information that 25 percent of the problems are undergraduate level, perhaps I will revert to being unsurprised again, but will look forward to being surprised when AI is getting nearer 50 percent on the dataset, because performance at “qual level” (as Elliot describes it — the next 50 percent of the questions) is exactly what I’m waiting to see from these systems — for me this would represent a big breakthrough. Prove this theorem! However, as Borcherds points out, even if we ended up with a machine which was super-human at “find this number!” questions, it would still have limited applicability in many areas of research mathematics, where the key question of interest is usually how to “prove this theorem!”. In my mind, the biggest success story in 2024 is DeepMind’s AlphaProof, which solved four out of the six 2024 IMO (International Mathematics Olympiad) problems. These were either “prove this theorem!” or “find a number and furthermore prove that it’s the right number” questions and for three of them, the output of the machine was a fully formalized Lean proof. Lean is an interactive theorem prover with a solid mathematics library mathlib containing many of the techniques needed to solve IMO problems and a lot more besides; DeepMind’s system’s solutions were human-checked and verified to be “full marks” solutions. However, we are back at high school level again; whilst the questions are extremely hard, the solutions use only school-level techniques. In 2025 I’m sure we’ll see machines performing at gold level standard in the IMO. However this now forces us to open up the “grading” can of worms which I’ve already mentioned once, and I’ll finish this post by talking a little more about it. Who is marking the machines? July 2025. I can envisage the following situation. As well as hundreds of the world’s smartest schoolchildren entering the IMO, there will be machines entering. Hopefully not too many though. Because the systems will be of two types. There will be systems submitting answers in the language of a computer proof checker like Lean (or Rocq, Isabelle, or many others). And there will be language models submitting answers in human language. The big difference between these two submissions are that: if a marker verifies that the statement of the question has been correctly translated into the computer proof checker, then all they need to do is to check that the proof compiles and then they basically know that it is a “full marks” solution. For the language models we will have a situation like the poor Putnam solutions above — the computer will write something, it will look convincing, but a human is going to have to read it carefully and grade it, and there is certainly no guarantee that it will be a “full marks” solution. Borcherds is right to remind the AI community that “prove this theorem!” is what we really want to see as mathematicians, and language models are currently at least an order of magnitude less accurate than expert humans when it comes to logical reasoning. I am dreading the inevitable onslaught in a year or two of language model “proofs” of the Riemann hypothesis which will just contain claims which are vague or inaccurate in the middle of 10 pages of correct mathematics which the human will have to wade through to find the line which doesn’t hold up. On the other hand, theorem provers are at least an order of magnitude more accurate: every time I’ve seen Lean not accept a human argument in the mathematical literature, the human has been wrong. In fact, as mathematicians, we would like to see more than “prove this theorem!”. We would like to see “prove this theorem, correctly, and explain what makes the proof work in a way which we humans understand”. With the language model approach I worry (a lot) about “correctly” and with the theorem prover approach I worry about “in a way which we humans understand”. There is still a huge amount to be done. Progress is currently happening really quickly. But we are a long way away. When will we “beat the undergraduate barrier”? Nobody knows. Share this: Twitter Facebook Like Loading... Related The Future of Interactive Theorem Proving? August 16, 2022 In \"Machine Learning\" What is homotopy type theory? An amateur speaks. June 19, 2019 In \"undergrad maths\" Lean in 2024 January 20, 2024 In \"Artificial Intelligence\" About xenaproject The Xena Project aims to get mathematics undergraduates (at Imperial College and beyond) trained in the art of formalising mathematics on a computer. Why? Because I have this feeling that digitising mathematics will be really important one day. View all posts by xenaproject → This entry was posted in Machine Learning, Olympiad stuff, Research formalisation and tagged AI, Artificial Intelligence, FrontierMath, math, mathematics, o3. Bookmark the permalink. ← Fermat’s Last Theorem — how it’s going 2 Responses to Can AI do maths yet? Thoughts from a mathematician. Junyan Xu says: December 23, 2024 at 12:25 am Looks like you can do the same two problems as Daniel Litt 🙂 And he also talked about the IMO/undergrad quote. LikeLike Reply Mike says: December 23, 2024 at 1:42 am Thank you for engaging with this topic. It makes a big difference for us non mathematicians who are trying to orient to these shocking results. A pedantic point but one I think is important: o3 (and o1) aren’t exactly just new language models of the usual sort. They do something fundamentally different on top of a language model. OpenAI has not released details of exactly what, but it is some kind of guided tree search through the space of outputs. This is important because it is much more expensive, and because a much deeper search increases the cost of every query (as opposed to scaling up training which can be amortized). So progress may proceed differently. Right now running o3 cost thousands of dollars per question. It’s also interesting because it is somewhat close to how AlphaProof works (though with no verifier). LikeLike Reply Leave a comment Search for: Categories Algebraic Geometry computability Fermat's Last Theorem formalising mathematics course General Imperial Learning Lean liquid tensor experiment M1F M1P1 M40001 M4P33 Machine Learning number theory Olympiad stuff Research formalisation rigour tactics Technical assistance Type theory Uncategorized undergrad maths Recent Posts Can AI do maths yet? Thoughts from a mathematician. December 22, 2024 Fermat’s Last Theorem — how it’s going December 11, 2024 Lean in 2024 January 20, 2024 Formalising modern research mathematics in real time November 4, 2023 Lean 2022 round-up January 8, 2023 Xena Blog at WordPress.com. Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use. To find out more, including how to control cookies, see here: Cookie Policy %d Design a site like this with WordPress.com Get started",
    "commentLink": "https://news.ycombinator.com/item?id=42493464",
    "commentBody": "Can AI do maths yet? Thoughts from a mathematician (xenaproject.wordpress.com)192 points by mathgenius 8 hours agohidepastfavorite167 comments nebulous1 6 hours agoThere was a little more information in that reddit thread. Of the three difficulty tiers, 25% are T1 (easiest) and 50% are T2. Of the five public problems that the author looked at, two were T1 and two were T2. Glazer on reddit described T1 as \"IMO/undergraduate problems\", but the article author says that they don't consider them to be undergraduate problems. So the LLM is already doing what the author says they would be surprised about. Also Glazer seemed to regret calling T1 \"IMO/undergraduate\", and not only because of the disparity between IMO and typical undergraduate. He said that \"We bump problems down a tier if we feel the difficulty comes too heavily from applying a major result, even in an advanced field, as a black box, since that makes a problem vulnerable to naive attacks from models\" Also, all of the problems shows to Tao were T3 reply riku_iki 5 minutes agoparent> So the LLM is already doing what the author says they would be surprised about. that's if you unconditionally believe in result without any proofreading, confirmation, reproducability and even barely any details (we are given only one slide). reply jampekka 6 hours agoprevI just spent a few days trying to figure out some linear algebra with the help of ChatGPT. It's very useful for finding conceptual information from literature (which for a not-professional-mathematician at least can be really hard to find and decipher). But in the actual math it constantly makes very silly errors. E.g. indexing a vector beyond its dimension, trying to do matrix decomposition for scalars and insisting on multiplying matrices with mismatching dimensions. O1 is a lot better at spotting its errors than 4o but it too still makes a lot of really stupid mistakes. It seems to be quite far from producing results itself consistently without at least a somewhat clueful human doing hand-holding. reply spacemanspiff01 5 hours agoparentI wonder if these are tokenization issues? I really am curious about metas byte tokenization scheme... reply jampekka 5 hours agorootparentProbably mostly not. The errors tend to be logical/conceptual. E.g. mixing up scalars and matrices is unlikely to be from tokenization. Especially if using spaces between the variables and operators, as AFAIK GPTs don't form tokens over spaces (although tokens may start or end with them). reply lordnacho 4 hours agoparentprevThe only thing I've consistently had issues with while using AI is graphs. If I ask it to put some simple function, it produces a really weird image that has nothing to do with the graph I want. It will be a weird swirl of lines and words, and it never corrects itself no matter what I say to it. Has anyone had any luck with this? It seems like the only thing that it just can't do. reply KeplerBoy 4 hours agorootparentYou're doing it wrong. It can't produce proper graphs with it's diffusion style image generation. Ask it to produce graphs with python and matplotlib. That will work. reply thomashop 4 hours agorootparentprevAsk it to plot the graph with python plotting utilities. Not using its image generator. I think you need a ChatGPT subscription though for it to be able to run python code. reply lupire 4 hours agorootparentYou seem to get 2(?) free Python program runs per week(?) as part of the 01 preview. When you visit chatgpt on the free account it automatically gives you the best model and then disables it after some amount of work and says to come back later or upgrade. reply amelius 2 hours agorootparentJust install Python locally, and copy paste the code. reply xienze 1 hour agorootparentprevShouldn’t ChatGPT be smart enough to know to do this automatically, based on context? reply HDThoreaun 18 minutes agorootparentprevThe agentic reasoning models should be able to fix this if they have the ability to run code instead of giving each task to itself. \"I need to make a graph\" \"LLMs have difficulty graphing novel functions\" \"Call python instead\" is a line of reasoning I would expect after seeing what O1 has come up with on other problems. Giving AI the ability to execute code is the safety peoples nightmare though, wonder if we'll hear anything from them as this is surely coming reply glimshe 6 hours agoparentprevIsn't Wolfram Alpha a better \"ChatGPT of Math\"? reply Filligree 6 hours agorootparentWolfram Alpha is better at actually doing math, but far worse at explaining what it’s doing, and why. reply dartos 5 hours agorootparentWhat’s worse about it? It never tells you the wrong thing, at the very least. reply jvanderbot 5 hours agorootparentWhen you give it a large math problem and the answer is \"seven point one three five ... \", and it shows a plot of the result v some randomly selected domain, well there could be more I'd like to know. You can unlock a full derivation of the solution, for cases where you say \"Solve\" or \"Simplify\", but what I (and I suspect GP) might want, is to know why a few of the key steps might work. It's a fantastic tool that helped get me through my (engineering) grad work, but ultimately the breakthrough inequalities that helped me write some of my best stuff were out of a book I bought in desperation that basically cataloged linear algebra known inequalities and simplifications. When I try that kind of thing with the best LLM I can use (as of a few months ago, albeit), the results can get incorrect pretty quickly. reply fn-mote 5 hours agorootparentprevIts understanding of problems was very bad last time I used it. Meaning it was difficult to communicate what you wanted it to do. Usually I try to write in the Mathematica language, but even that is not foolproof. Hopefully they have incorporated more modern LLM since then, but it hasn’t been that long. reply jampekka 5 hours agorootparentWolfram Alpha's \"smartness\" is often Clippy level enraging. E.g. it makes assumptions of symbols based on their names (e.g. a is assumed to be a constant, derivatives are taken w.r.t. x). Even with Mathematica syntax it tends to make such assumptions and refuses to lift them even when explicitly directed. Quite often one has to change the variable symbols used to try to make Alpha to do what's meant. reply amelius 2 hours agorootparentprevI wish there was a way to tell Chatgpt where it has made a mistake, with a single mouse click. reply GuB-42 2 hours agorootparentprevWolfram Alpha can solve equations well, but it is terrible at understanding natural language. For example I asked Wolfram Alpha \"How heavy a rocket has to be to launch 5 tons to LEO with a specific impulse of 400s\", which is a straightforward application of the Tsiolkovsky rocket equation. Wolfram Alpha gave me some nonsense about particle physics (result: 95 MeV/c^2), GPT-4o did it right (result: 53.45 tons). Wolfram alpha knows about the Tsiolkovsky rocket equation, it knows about LEO (low earth orbit), but I found no way to get a delta-v out of it, again, more nonsense. It tells me about Delta airlines, mentions satellites that it knows are not in LEO. The \"natural language\" part is a joke. It is more like an advanced calculator, and for that, it is great. reply bongodongobob 1 hour agorootparentYou're using it wrong, you can use natural language in your equation, but afaik it's not supposed to be able to do what you're asking of it. reply jampekka 6 hours agorootparentprevWolfram Alpha is mostly for \"trivia\" type problems. Or giving solutions to equations. I was figuring out some mode decomposition methods such as ESPRIT and Prony and how to potentially extend/customize them. Wolfram Alpha doesn't seem to have a clue about such. reply lupire 4 hours agorootparentprevNo. Wolfram Alpha can't solve anything that isn't a function evaluation or equation. And it can't do modular arithmetic to save its unlife. WolframOne/Mathematica is better, but that requires the user (or ChatGPT!)to write complicated code, not natural language queries. reply amelius 2 hours agoparentprevDon't most mathematical papers contain at least one such error? reply aiono 2 hours agorootparentWhere is this data from? reply ivan_ah 4 hours agoprevYesterday, I saw a thought provoking talk about the future of of \"math jobs\" assuming automated theory proving becomes more prevalent in the future. [ (Re)imagining mathematics in a world of reasoning machines by Akshay Venkatesh] https://www.youtube.com/watch?v=vYCT7cw0ycw [54min] Abstract: In the coming decades, developments in automated reasoning will likely transform the way that research mathematics is conceptualized and carried out. I will discuss some ways we might think about this. The talk will not be about current or potential abilities of computers to do mathematics—rather I will look at topics such as the history of automation and mathematics, and related philosophical questions. See discussion at https://news.ycombinator.com/item?id=42465907 reply busyant 2 hours agoprevAs someone who has a 18 yo son who wants to study math, this has me (and him) ... worried ... about becoming obsolete? But I'm wondering what other people think of this analogy. I used to be a bench scientist (molecular genetics). There were world class researchers who were more creative than I was. I even had a Nobel Laureate once tell me that my research was simply \"dotting 'i's and crossing 't's\". Nevertheless, I still moved the field forward in my own small ways. I still did respectable work. So, will these LLMs make us completely obsolete? Or will there still be room for those of us who can dot the \"i\"?--if only for the fact that LLMs don't have infinite time/resources to solve \"everything.\" I don't know. Maybe I'm whistling past the graveyard. reply nyrikki 1 hour agoparentWhat LLMs can do is limited, they are superior to wet-wear in some tasks like finding and matching patterns in higher dimensional space, they are still fundamentally limited to a tiny class of problems outside of that pattern finding and matching. LLMs will be tools for some math needs and even if we ever get quantum computers will be limited in what they can do. LLMs, without pattern matching, can only do up to about integer division, and while they can calculate parity, they can't use it in their calculations. There are several groups sitting on what are known limitations of LLMs, waiting to take advantage of those who don't understand the fundamental limitations, simplicity bias etc... The hype will meet reality soon and we will figure out where they work and where they are problematic over the next few years. But even the most celebrated achievements like proof finding with Lean, heavily depends on smart people producing hints that machines can use. Basically lots of the fundamental hints of the limits of computation still hold. Model logic may be an accessable way to approach the limits of statistical inference if you want to know one path yourself. A lot of what is in this article relates to some the known fundamental limitations. Remember that for all the amazing progress, one of the core founders of the perceptron, Pitts drank him self to death in the 50s because it was shown that they were insufficient to accurately model biological neurons. Optimism is high, but reality will hit soon. So think of it as new tools that will be available to your child, not a replacement. reply ComplexSystems 1 hour agorootparent\"LLMs, without pattern matching, can only do up to about integer division, and while they can calculate parity, they can't use it in their calculations.\" - what do you mean by this? Counting the number of 1's in a bitstring and determining if it's even or odd? reply vouaobrasil 1 hour agoparentprevI was just thinking about this. I already posted a comment here, but I will say that as a mathematician (PhD in number theory), that for me, AI signficantly takes away the beauty of doing mathematics within a realm in which AI is used. The best part of math (again, just for me) is that it was a journey that was done by hand with only the human intellect that computers didn't understand. The beauty of the subject was precisely that it was a journey of human intellect. As I said elsewhere, my friends used to ask me why something was true and it was fun to explain it to them, or ask them and have them explain it to me. Now most will just use some AI. Soulless, in my opinion. Pure mathematics should be about the art of the thing, not producing results on an assembly line like it will be with AI. Of course, the best mathematicians are going into this because it helps their current careers, not because it helps the future of the subject. Math done with AI will be a lot like Olympic running done with performance-enhancing drugs. Yes, we will get a few more results, faster. But the results will be entirely boring. reply zmgsabst 1 hour agorootparentPresumably people who get into math going forward will feel differently. For myself, chasing lemmas was always boring — and there’s little interest in doing the busywork of fleshing out a theory. For me, LLMs are a great way to do the fun parts (conceptual architecture) without the boring parts. And I expect we’ll such much the same change as with physics: computers increase the complexity of the objects we study, which tend to be rather simple when done by hand — eg, people don’t investigate patterns in the diagrams of group(oids) because drawing million element diagrams isn’t tractable by hand. And you only notice the patterns in them when you see examples of the diagrams at scale. reply ndriscoll 1 hour agorootparentEven current people will feel differently. I don't bemoan the fact that Lean/Mathlib has `simp` and `linarith` to automate trivial computations. A \"copilot for Lean\" that can turn \"by induction, X\" or \"evidently Y\" into a formal proof sounds great. The the trick is teaching the thing how high powered of theorems to use or how to factor out details or not depending on the user's level of understanding. We'll have to find a pedagogical balance (e.g. you don't give `linarith` to someone practicing basic proofs), but I'm sure it will be a great tool to aid human understanding. A tool to help translate natural language to formal propositions/types also sounds great, and could help more people to use more formal methods, which could make for more robust software. reply pfisherman 1 hour agoparentprevI used to do bench top work too; and was blessed with “the golden hands” in that I could almost always get protocols working. To me this always felt more like intuition than deductive reasoning. And it made me a terrible TA. My advice to students in lab was always something along the lines of “just mess around with it, and see how it works.” Not very helpful for the stressed and struggling student -_- Digression aside, my point is that I don’t think we know exactly what makes or defines “the golden hands”. And if that is the case, can we optimize for it? Another point is that scalable fine tuning only works for verifiable stuff. Think a priori knowledge. To me that seems to be at the opposite end of the spectrum from “mess with it and see what happens”. reply deepsun 2 hours agoparentprevBy the way, don't trust Nobel laureates or even winners. E.g. Linus Pauling was talking absolute garbage, harmful and evil, after winning the Nobel. reply Radim 1 hour agorootparent> don't trust Nobel laureates or even winners Nobel laureate and winner are the same thing. > Linus Pauling was talking absolute garbage, harmful and evil, after winning the Nobel. Can you be more specific, what garbage? And which Nobel prize do you mean – Pauling got two, one for chemistry and one for peace. reply deepsun 1 hour agorootparentThank you, my bad. I was referring to Linus's harmful and evil promotion of Vitamin C as the cure for everything and cancer. I don't think Linus was attaching that garbage to any particular Nobel prize. But people did say to their doctors: \"Are you a Nobel winner, doctor?\". Don't think they cared about particular prize either. reply bongodongobob 1 hour agorootparentprevEugenics and vitamin C as a cure all. reply voidhorse 5 hours agoprevEventually we may produce a collection of problems exhaustive enough that these tools can solve almost any problem that isn't novel in practice, but I doubt that they will ever become general problem solvers capable of what we consider to be reasoning in humans. Historically, the claim that neural nets were actual models of the human brain and human thinking was always epistemically dubious. It still is. Even as the practical problems of producing better and better algorithms, architectures, and output have been solved, there is no reason to believe a connection between the mechanical model and what happens in organisms has been established. The most important point, in my view, is that all of the representation and interpretation still has to happen outside the computational units. Without human interpreters, none of the AI outputs have any meaning. Unless you believe in determinism and an overseeing god, the story for human beings is much different. AI will not be capable of reason until, like humans, it can develop socio-rational collectivities of meaning that are independent of the human being. Researchers seemed to have a decent grasp on this in the 90s, but today, everyone seems all too ready to make the same ridiculous leaps as the original creators of neural nets. They did not show, as they claimed, that thinking is reducible to computation. All they showed was that a neural net can realize a boolean function—which is not even logic, since, again, the entire semantic interpretive side of the logic is ignored. reply tananan 4 hours agoparent> Unless you believe in determinism and an overseeing god Or perhaps, determinism and mechanistic materialism - which in STEM-adjacent circles has a relatively prevalent adherence. Worldviews which strip a human being of agency in the sense you invoke crop up quite a lot today in such spaces. If you start of adopting a view like this, you have a deflationary sword which can cut down most any notion that's not mechanistic in terms of mechanistic parts. \"Meaning? Well that's just an emergent phenomenon of the influence of such and such causal factors in the unrolling of a deterministic physical system.\" Similar for reasoning, etc. Now obviously large swathes of people don't really subscribe to this - but it is prevalent and ties in well with utopian progress stories. If something is amenable to mechanistic dissection, possibly it's amenable to mechanistic control. And that's what our education is really good at teaching us. So such stories end up having intoxicating \"hype\" effects and drive fundraising, and so we get where we are. For one, I wish people were just excited about making computers do things they couldn't do before, without needing to dress it up as something more than it is. \"This model can prove a set of theorems in this format with such and such limits and efficiency\" reply exprofmaddy 1 hour agorootparentAgreed. If someone believes the world is purely mechanistic, then it follows that a sufficiently large computing machine can model the world---like Leibniz's Ratiocinator. The intoxication may stem from the potential for predictability and control. The irony is: why would someone want control if they don't have true choice? Unfortunately, such a question rarely pierces the intoxicated mind when this mind is preoccupied with pass the class, get an A, get a job, buy a house, raise funds, sell the product, win clients, gain status, eat right, exercise, check insta, watch the game, binge the show, post on Reddit, etc. reply exprofmaddy 2 hours agoparentprevI'm with you. Interpreting a problem as a problem requires a human (1) to recognize the problem and (2) to convince other humans that it's a problem worth solving. Both involve value, and value has no computational or mechanistic description (other than \"given\" or \"illusion\"). Once humans have identified a problem, they might employ a tool to find the solution. The tool has no sense that the problem is important or even hard; such values are imposed by the tool's users. It's worth considering why \"everyone seems all too ready to make ... leaps ...\" \"Neural\", \"intelligence\", \"learning\", and others are metaphors that have performed very well as marketing slogans. Behind the marketing slogans are deep-pocketed, platformed corporate and government (i.e. socio-rational collective) interests. Educational institutions (another socio-rational collective) and their leaders have on the whole postured as trainers and preparers for the \"real world\" (i.e. a job), which means they accept, support, and promote the corporate narratives about techno-utopia. Which institutions are left to check the narratives? Who has time to ask questions given the need to learn all the technobabble (by paying hundreds of thousands for 120 university credits) to become a competitive job candidate? I've found there are many voices speaking against the hype---indeed, even (rightly) questioning the epistemic underpinnings of AI. But they're ignored and out-shouted by tech marketing, fundraising politicians, and engagement-driven media. reply gmadsen 4 hours agoparentprevI hear these arguments a lot from law and philosophy students, never from those trained in mathematics. It seems to me, \"literary\" people will still be discussing these theoretical hypotheticals as technology passes them by building it. reply voidhorse 2 hours agorootparentI straddle both worlds. Consider that using the lens of mathematical reasoning to understand everything is a bit like trying to use a single mathematical theory (eg that of groups) to comprehend mathematics as a whole. You will almost always benefit and enrich your own understanding by daring to incorporate outside perspectives. Consider also that even as digital technology and the ratiomathimatical understanding of the world has advanced it is still rife with dynamics and problems that require a humanistic approach. In particular, a mathematical conception cannot resolve teleological problems which require the establishment of consensus and the actual determination of what we, as a species, want the world to look like. Climate change and general economic imbalance are already evidence of the kind of disasters that mount when you limit yourself to a reductionistic, overly mathematical and technological understanding of life and existence. Being is not a solely technical problem. reply red75prime 5 hours agoparentprev> there is no reason to believe a connection between the mechanical model and what happens in organisms has been established The universal approximation theorem. And that's basically it. The rest is empirical. No matter which physical processes happen inside the human brain, a sufficiently large neural network can approximate them. Barring unknowns like super-Turing computational processes in the brain. reply exprofmaddy 1 hour agorootparentThe universal approximation theorem is set in a precise mathematical context; I encourage you to limit its applicability to that context despite the marketing label \"universal\" (which it isn't). Consider your concession about empiricism. There's no empirical way to prove (i.e. there's no experiment that can demonstrate beyond doubt) that all brain or other organic processes are deterministic and can be represented completely as functions. reply red75prime 1 hour agorootparentFunction is the most general way of describing relations. Non-deterministic processes can be represented as functions with a probability distribution codomain. Physics seems to require only continuous functions. Sorry, but there's not much evidence that can support human exceptionalism. reply exprofmaddy 50 minutes agorootparentSome differential equations that model physics admit singularities and multiple solutions. Therefore, functions are not the most general way of describing relations. Functions are a subset of relations. Although \"non-deterministic\" and \"stochastic\" are often used interchangeably, they are not equivalent. Probability is applied analysis whose objects are distributions. Analysis is a form of deductive, i.e. mechanical, reasoning. Therefore, it's more accurate (philosophically) to identify mathematical probability with determinism. Probability is a model for our experience. That doesn't mean our experience is truly probabilistic. Humans aren't exceptional. Math modeling and reasoning are human activities. reply lupire 2 hours agorootparentprevThat's not useful by itself, because \"anything cam model anything else\" doesn't put any upper bound on emulation cost, which for one small task could be larger than the total energy available in the entire Universem reply red75prime 1 hour agorootparentEither the brain violates the physical Church-Turing thesis or it's not. If it does, well, it will take more time to incorporate those physical mechanisms into computers to get them on par with the brain. I leave the possibility that it's \"magic\"[1] aside. It's just impossible to predict, because it will violate everything we know about our physical world. [1] One example of \"magic\": we live in a simulation and the brain is not fully simulated by the physics engine, but creators of the simulation for some reason gave it access to computational resources that are impossible to harness using the standard physics of the simulated world. Another example: interactionistic soul. reply pixl97 2 hours agorootparentprevI mean, that is why they mention super-Turning processes like quantum based computing. reply dinosaurdynasty 53 minutes agorootparentQuantum computing actually isn't super-Turing, it \"just\" computes some things faster. (Strictly speaking it's somewhere between a standard Turing machine and a nondeterministic Turing machine in speed, and the first can emulate the second.) reply nmca 5 hours agoparentprevCan you define what you mean by novel here? reply seafoamteal 6 hours agoprevI don't have much to opine from an advanced maths perspective, but I'd like to point out a couple examples of where ChatGPT made basic errors in questions I asked it as an undergrad CS student. 1. I asked it to show me the derivation of a formula for the efficiency of Stop-and-Wait ARQ and it seemed to do it, but a day later, I realised that in one of the steps, it just made a term vanish to get to the next step. Obviously, I should have verified more carefully, but when I asked it to spot the mistake in that step, it did the same thing twice more with bs explanations of how the term is absorbed. 2. I asked it to provide me syllogisms that I could practice proving. An overwhelming number of the syllogisms it gave me were inconsistent and did not hold. This surprised me more because syllogisms are about the most structured arguments you can find, having been formalized centuries ago and discussed extensively since then. In this case, asking it to walk step-by-step actually fixed the issue. Both of these were done on the free plan of ChatGPT, but I can remember if it was 4o or 4. reply LittleTimothy 6 hours agoprevIt's fascinating that this has run into the exact same problem as the Quantum research. Ie, in the quantum research to demonstrate any valuable forward progress you must compute something that is impossible to do with a traditional computer. If you can't do it with a traditional computer, it suddenly becomes difficult to verify correctness (ie, you can't just check it was matching the traditional computer's answer. In the same way ChatGPT scores 25% on this and the question is \"How close were those 25% to questions in the training set\". Or to put it another way we want to answer the question \"Is ChatGPT getting better at applying it's reasoning to out-of-set problems or is it pulling more data into it's training set\". Or \"Is the test leaking into the training\". Maybe the whole question is academic and it doesn't matter, we solve the entire problem by pulling all human knowledge into the training set and that's a massive benefit. But maybe it implies a limit to how far it can push human knowledge forward. reply newpavlov 6 hours agoparent>in the quantum research to demonstrate any valuable forward progress you must compute something that is impossible to do with a traditional computer This is factually wrong. The most interesting problems motivating the quantum computing research are hard to solve, but easy to verify on classical computers. The factorization problem is the most classical example. The problem is that existing quantum computers are not powerful enough to solve the interesting problems, so researchers have to invent semi-artificial problems to demonstrate \"quantum advantage\" to keep the funding flowing. There is a plethora of opportunities for LLMs to show their worth. For example, finding interesting links between different areas of research or being a proof assistant in a math/programming formal verification system. There is a lot of ongoing work in this area, but at the moment signal-to-noise ratio of such tools is too low for them to be practical. reply bondarchuk 5 hours agorootparentNo, it is factually right, at least if Scott Aaronson is to be believed: > Having said that, the biggest caveat to the “10^25 years” result is one to which I fear Google drew insufficient attention. Namely, for the exact same reason why (as far as anyone knows) this quantum computation would take ~10^25 years for a classical computer to simulate, it would also take ~10^25 years for a classical computer to directly verify the quantum computer’s results!! (For example, by computing the “Linear Cross-Entropy” score of the outputs.) For this reason, all validation of Google’s new supremacy experiment is indirect, based on extrapolations from smaller circuits, ones for which a classical computer can feasibly check the results. To be clear, I personally see no reason to doubt those extrapolations. But for anyone who wonders why I’ve been obsessing for years about the need to design efficiently verifiable near-term quantum supremacy experiments: well, this is why! We’re now deeply into the unverifiable regime that I warned about. https://scottaaronson.blog/?p=8525 reply newpavlov 4 hours agorootparentIt's a property of the \"semi-artificial\" problem chosen by Google. If anything, it means that we should heavily discount this claim of \"quantum advantage\", especially in the light of inherent probabilistic nature of quantum computations. Note that the OP wrote \"you MUST compute something that is impossible to do with a traditional computer\". I demonstrated a simple counter-example to this statement: you CAN demonstrate forward progress by factorizing big numbers, but the problem is that no one can do it despite billions of investments. reply bondarchuk 4 hours agorootparentApparently they can't, right now, as you admit. Anyway this is turning into a stupid semantic argument, have a nice day. reply joshuaissac 1 hour agorootparentIf they can't, then is it really quantum supremacy? They claimed it last time in 2019 with Sycamore, which could perform in 200 seconds a calculation that Google claimed would take a classical supercomputer 10,000 years. That was debunked when a team of scientists replicated the same thing on an ordinary computer in 15 hours with a large number of GPUs. Scott Aaronson said that on a supercomputer, the same technique would have solved the problem in seconds.[1] So if they now come up with another problem which they say cannot even be verified by a classical computer and uses it to claim quantum advantage, then it is right to be suspicious of that claim. 1. https://www.science.org/content/article/ordinary-computers-c... reply noqc 1 hour agorootparentprevthe unverifiable regime is a great way to extract funding. reply derangedHorse 4 hours agorootparentprev> This is factually wrong. What's factually wrong about it? OP said \"you must compute something that is impossible to do with a traditional computer\" which is true, regardless of the output produced. Verifying an output is very different from verifying the proper execution of a program. The difference between testing a program and seeing its code. What is being computed is fundamentally different from classical computers, therefore the verification methods of proper adherence to instructions becomes increasingly complex. reply ajmurmann 4 hours agorootparentThey left out the key part which was incorrect and the sentence right after \"If you can't do it with a traditional computer, it suddenly becomes difficult to verify correctness\" The point stands that for actually interesting problems verifying correctness of the results is trivial. I don't know if \"adherence to instructions\" transudates at all to quantum computing. reply aleph_minus_one 5 hours agorootparentprev> This is factually wrong. The most interesting problems motivating the quantum computing research are hard to solve, but easy to verify on classical computers. You parent did not talk about quantum computers. I guess he rather had predictions of novel quantum-field theories or theories of quantum gravity in the back of his mind. reply newpavlov 5 hours agorootparentThen his comment makes even less sense. reply 0xfffafaCrash 6 hours agoparentprevI agree with the issue of ”is the test dataset leaking into the training dataset” being an issue with interpreting LLM capabilities in novel contexts, but not sure I follow what you mean on the quantum computing front. My understanding is that many problems have solutions that are easier to verify than to solve using classical computing. e.g. prime factorization reply LittleTimothy 4 hours agorootparentOh it's a totally different issue on the quantum side that leads to the same issue with difficulty verifying. There, the algorithms that Google for example is using today, aren't like prime factorization, they're not easy to directly verify with traditional computers, so as far as I'm aware they kind of check the result for a suitably small run, and then do the performance metrics on a large run that they hope gave a correct answer but aren't able to directly verify. reply eagerpace 6 hours agoparentprevHow much of this could be resolved if its training set were reduced? Conceivably, most of the training serves only to confuse the model when only aiming to solve a math equation. reply lazide 6 hours agoparentprevIf constrained by existing human knowledge to come up with an answer, won’t it fundamentally be unable to push human knowledge forward? reply LittleTimothy 6 hours agorootparentDepends on your understanding of human knowledge I guess? People talk about the frontier of human knowledge and if your view of knowledge is like that of a unique human genius pushing forward the frontier then yes - it'd be stuck. But if you think of knowledge as more complex than that you could have areas that are kind of within our frontier of knowledge (that we could reasonably know, but don't actually know) - taking concepts that we already know in one field and applying them to some other field. Today the reason that doesn't happen is because genius A in physics doesn't know about the existence of genius B in mathematics (let alone understand their research), but if it's all imbibed by \"The Model\" then it's trivial to make that discovery. reply lazide 6 hours agorootparentI was referring specifically to the parent comments statements around current AI systems. reply dinosaurdynasty 50 minutes agorootparentprevThere are likely lots of connections that could be made that no individual has made because no individual has all of existing human knowledge at their immediate disposal. reply wongarsu 5 hours agorootparentprevReasoning is essentially the creation of new knowledge from existing knowledge. The better the model can reason the less constrained it is to existing knowledge. The challenge is how to figure out if a model is genuinely reasoning reply lupire 3 hours agorootparentReasoning is a very minor (but essential) part of knowledge creation. Knowledge creation comes from collecting data from the real world, and cleaning it up somehow, and brainstorming creative models to explain it. NN/LLM's version of model building is frustrating because it is quite good, but not highly \"explainable\". Human models have higher explainability, while machine models have high predictive value on test examples due to an impenetrable mountain of algebra. reply actionfromafar 6 hours agorootparentprevThen much of human research and development is also fundamentally impossible. reply AnerealDew 6 hours agorootparentOnly if you think current \"AI\" is on the same level as human creativity and intelligence, which it clearly is not. reply actionfromafar 5 hours agorootparentI think current \"AI\" (i.e. LLMs) is unable to push human knowledge forward, but not because it's constrained by existing human knowledge. It's more like peeking into a very large magic-8 ball, new answers everytime you shake it. Some useful. reply SJC_Hacker 4 hours agorootparentIt may be able to push human knowledge forward to an extent. In the past, there was quite a bit of low hanging fruit such that you could have polymaths able to contribute to a wide variety of fields, such as Newton. But in the past 100 years or so, the problem is there is so much known, it is impossible for any single person to have deep knowledge of everything. e.g. its rare to find a really good mathematician who also has a deep knowledge (beyond intro courses) about say, chemistry. Would a sufficiently powerful AI / ML model be able to come up with this synthesis across fields? reply lupire 2 hours agorootparentprevThat's not a strong reason. Yes, that means ChatGPT isn't good at wholly independently pushing knowledge forward, but a good brainstormer that is right even 10% of the time is an incredible found of knowledge. reply Havoc 6 hours agorootparentprevI don't think many expect AI to push knowledge forward? A thing that basically just regurgitates consensus historic knowledge seems badly suited to that reply calmoo 6 hours agorootparentBut apparently these new frontier models can 'reason' - so with that logic, they should be able to generate new knowledge? reply tomjen3 5 hours agorootparentprevO1 was able to find the math problem in a recently published paper, so yes. reply upghost 5 hours agoprevI didn't see anyone else ask this but.. isn't the FrontierMath dataset compromised now? At the very least OpenAI now knows the questions if not the answers. I would expect that the next iteration will \"magically\" get over 80% on the FrontierMath test. I imagine that experiment was pretty closely monitored. reply jvanderbot 5 hours agoparentI figured their model was independently evaluated against the questions/answers. That's not to say it's not compromised by \"Here's a bag of money\" type methods, but I don't even think it'd be a reasonable test if they just handed over the dataset. reply upghost 5 hours agorootparentI'm sure it was independently evaluated, but I'm sure the folks running the test were not given an on-prem installation of ChatGPT to mess with. It was still done via API calls, presumably through the chat interface UI. That means the questions went over the fence to OpenAI. I'm quite certain they are aware of that, and it would be pretty foolish not to take advantage of at least knowing what the questions are. reply jvanderbot 5 hours agorootparentNow that you put it that way, it is laughably easy. reply optimalsolver 4 hours agoparentprevThis was my first thought when I saw the results: https://news.ycombinator.com/item?id=42473470 reply upghost 3 hours agorootparentInsightful comment. The thing that's extremely frustrating is look at all the energy poured into this conversation around benchmarks. There is a fundamental assumption of honesty and integrity in the benchmarking process by at least some people. But when the dataset is compromised and generation N+1 has miraculous performance gains, how can we see this as anything other than a ploy to pump up valuations? Some people have millions of dollars at stake here and they don't care about the naysayers in the peanut gallery like us. reply optimalsolver 3 hours agorootparentIt's sadly inevitable that when billions in funding and industry hype are tied to performance on a handful of benchmarks, scores will somehow, magically, continue to go up. Needless to say, it doesn't bring us any closer to AGI. The only solution I see here is people crafting their own, private benchmarks that the big players don't care about enough to train on. That, at least, gives you a clearer view of the field. reply upghost 2 hours agorootparentNot sure why your comment was downvoted, but it certainly shows the pressure going against people who point out fundamental flaws. This is pushing us towards \"AVI\" rather than AGI-- \"Artificially Valued Intelligence\". The optimization function here is around the market. I'm being completely serious. You are correct, despite the downvotes, that this could not be pushing us towards AGI because if the dataset is leaked you can't claim the G-- generalizability. The point of the benchmark is to lead is to believe that this is a substantial breakthrough. But a reasonable person would be forced to conclude that the results are misleading to due to optimizing around the training data. reply ned99 6 hours agoprevI think this is a silly question, you could track AI's doing very simple maths back in 1960 - 1970's reply mdp2021 5 hours agoparentIt's just the worrisome linguistic confusion between AI and LLMs. reply jebarker 3 hours agoprev> I am dreading the inevitable onslaught in a year or two of language model “proofs” of the Riemann hypothesis which will just contain claims which are vague or inaccurate in the middle of 10 pages of correct mathematics which the human will have to wade through to find the line which doesn’t hold up. I wonder what the response of working mathematicians will be to this. If the proofs look credible it might be too tempting to try and validate them, but if there’s a deluge that could be a hug time sync. Imagine if Wiles or Perelman had produced a thousand different proofs for their respective problems. reply bqmjjx0kac 3 hours agoparentMaybe the coming onslaught of AI slop \"proofs\" will give a little bump to proof assistants like Coq. Of course, it would still take a human mathematician some time to verify theorem definitions. reply Hizonner 2 hours agoparentprevDon't waste time on looking at it unless a formal proof checker can verify it. reply bambax 6 hours agoprev> As an academic mathematician who spent their entire life collaborating openly on research problems and sharing my ideas with other people, it frustrates me [that] I am not even to give you a coherent description of some basic facts about this dataset, for example, its size. However there is a good reason for the secrecy. Language models train on large databases of knowledge, so you moment you make a database of maths questions public, the language models will train on it. Well, yes and no. This is only true because we are talking about closed models from closed companies like so-called \"OpenAI\". But if all models were truly open, then we could simply verify what they had been trained on, and make experiments with models that we could be sure had never seen the dataset. Decades ago Microsoft (in the words of Ballmer and Gates) famously accused open source of being a \"cancer\" because of the cascading nature of the GPL. But it's the opposite. In software, and in knowledge in general, the true disease is secrecy. reply ludwik 5 hours agoparent> But if all models were truly open, then we could simply verify what they had been trained on How do you verify what a particular open model was trained on if you haven’t trained it yourself? Typically, for open models, you only get the architecture and the trained weights. How can you reliably verify what the model was trained on from this? Even if they provide the training set (which is not typically the case), you still have to take their word for it—that’s not really \"verification.\" reply asadotzler 4 hours agorootparentThe OP said \"truly open\" not \"open model\" or any of the other BS out there. If you are truly open you share the training corpora as well or at least a comprehensive description of what it is and where to get it. reply ludwik 4 hours agorootparentIt seems like you skipped the second paragraph of my comment? reply Xcelerate 6 hours agoprevSo here's what I'm perplexed about. There are statements in Presburger arithmetic that take time doubly exponential (or worse) in the size of the statement to reach via any path of the formal system whatsoever. These are arithmetic truths about the natural numbers. Can these statements be reached faster in ZFC? Possibly—it's well-known that there exist shorter proofs of true statements in more powerful consistent systems. But the problem then is that one can suppose there are also true short statements in ZFC which likewise require doubly exponential time to reach via any path. Presburger Arithmetic is decidable whereas ZFC is not, so these statements would require the additional axioms of ZFC for shorter proofs, but I think it's safe to assume such statements exist. Now let's suppose an AI model can resolve the truth of these short statements quickly. That means one of three things: 1) The AI model can discover doubly exponential length proof paths within the framework of ZFC. 2) There are certain short statements in the formal language of ZFC that the AI model cannot discover the truth of. 3) The AI model operates outside of ZFC to find the truth of statements in the framework of some other, potentially unknown formal system (and for arithmetical statements, the system must necessarily be sound). How likely are each of these outcomes? 1) is not possible within any coherent, human-scale timeframe. 2) IMO is the most likely outcome, but then this means there are some really interesting things in mathematics that AI cannot discover. Perhaps the same set of things that humans find interesting. Once we have exhausted the theorems with short proofs in ZFC, there will still be an infinite number of short and interesting statements that we cannot resolve. 3) This would be the most bizarre outcome of all. If AI operates in a consistent way outside the framework of ZFC, then that would be equivalent to solving the halting problem for certain (infinite) sets of Turing machine configurations that ZFC cannot solve. That in itself itself isn't too strange (e.g., it might turn out that ZFC lacks an axiom necessary to prove something as simple as the Collatz conjecture), but what would be strange is that it could find these new formal systems efficiently. In other words, it would have discovered an algorithmic way to procure new axioms that lead to efficient proofs of true arithmetic statements. One could also view that as an efficient algorithm for computing BB(n), which obviously we think isn't possible. See Levin's papers on the feasibility of extending PA in a way that leads to quickly discovering more of the halting sequence. reply semolinapudding 4 hours agoparentZFC is way worse than Presburger arithmetic -- since it is undecidable, we know that the length of the minimal proof of a statement cannot be bounded by a computable function of the length of the statement. This has little to do with the usefulness of LLMs for research-level mathematics though. I do not think that anyone is hoping to get a decision procedure out of it, but rather something that would imitate human reasoning, which is heavily based on analogies (\"we want to solve this problem, which shares some similarities with that other solved problem, can we apply the same proof strategy? if not, can we generalise the strategy so that it becomes applicable?\"). reply wbl 4 hours agoparentprev2 is definitely true. 3 is much more interesting and likely true but even saying it takes us into deep philosophical waters. If every true theorem had a proof in a computationally bounded length the halting problem would be solvable. So the AI can't find some of those proofs. The reason I say 3 is deep is that ultimately our foundational reasons to assume ZFC+the bits we need for logic come from philosohical groundings and not everyone accepts the same ones. Ultrafinitists and large cardinal theorists are both kinds of people I've met. reply aleph_minus_one 5 hours agoparentprev> There are statements in Presburger arithmetic that take time doubly exponential (or worse) in the size of the statement to reach via any path of the formal system whatsoever. This is a correct statement about the worst case runtime. What is interesting for practical applications is whether such statements are among those that you are practically interested in. reply Xcelerate 5 hours agorootparentI would certainly think so. The statements mathematicians seem to be interested in tend to be at a \"higher level\" than simple but true statements like 2+3=5. And they necessarily have a short description in the formal language of ZFC, otherwise we couldn't write them down (e.g., Fermat's last theorem). If the truth of these higher level statements instantly unlocks many other truths, then it makes sense to think of them in the same way that knowing BB(5) allows one to instantly classify any Turing machine configuration on the computation graph of all n ≤ 5 state Turing machines (on empty tape input) as halting/non-halting. reply aithrowawaycomm 7 hours agoprevI am fairly optimistic about LLMs as a human math -> theorem-prover translator, and as a fan of Idris I am glad that the AI community is investing in Lean. As the author shows, the answer to \"Can AI be useful for automated mathematical work?\" is clearly \"yes.\" But I am confident the answer to the question in the headline is \"no, not for several decades.\" It's not just the underwhelming benchmark results discussed in the post, or the general concern about hard undergraduate math using different skillsets than ordinary research math. IMO the deeper problem still seems to be a basic gap where LLMs can seemingly do formal math at the level of a smart graduate student but fail at quantitative/geometric reasoning problems designed for fish. I suspect this holds for O3, based on one of the ARC problems it wasn't able to solve: https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_pr... (via https://www.interconnects.ai/p/openais-o3-the-2024-finale-of...) ANNs are simply not able to form abstractions, they can only imitate them via enormous amounts of data and compute. I would say there has been zero progress on \"common sense\" math in computers since the invention of Lisp: we are still faking it with expert systems, even if LLM expert systems are easier to build at scale with raw data. It is the same old problem where an ANN can attain superhuman performance on level 1 of Breakout, but it has to be retrained for level 2. I am not convinced it makes sense to say AI can do math if AI doesn't understand what \"four\" means with the same depth as a rat, even if it can solve sophisticated modular arithmetic problems. In human terms, does it make sense to say a straightedge-and-compass AI understands Euclidean geometry if it's not capable of understanding the physical intuition behind Euclid's axioms? It makes more sense to say it's a brainless tool that helps with the tedium and drudgery of actually proving things in mathematics. reply aithrowawaycomm 36 minutes agoparentJust a comment: the example o1 got wrong was actually underspecified: https://anokas.substack.com/p/o3-and-arc-agi-the-unsolved-ta... Which is actually a problem I have with ARC (and IQ tests more generally): it is computationally cheaper to go from ARC transformation rule -> ARC problem than it is the other way around. But this means it’s pretty easy to generate ARC problems with non-unique solutions. reply QuadmasterXLII 6 hours agoparentprevTo give a sense if scale: It’s not that o3 failed to solve that red blue rectangle problem once: o3 spent thousands of gpu hours putting out text about that problem, creating by my math about a million pages of text, and did not find the answer anywhere in those pages. For other problems it did find the answer around the million page mark, as at the ~$3000 per problem spend setting the score was still slowly creeping up. reply josh-sematic 5 hours agorootparentIf the trajectory of the past two years is any guide, things that can be done at great compute expense now will rapidly become possible for a fraction of the cost. reply asadotzler 4 hours agorootparentThe trajectory is not a guide, unless you count the recent plateauing. reply asddubs 6 hours agoparentprevit can take my math and point out a step I missed and then show me the correct procedure but still get the wrong result because it can't reliably multiply 2-digit numbers reply fifilura 6 hours agorootparentBetter than an average human then. reply actionfromafar 6 hours agorootparentDifferent than an average human. reply swalsh 4 hours agoprevEvery profession seems to have a pessimistic view of AI as soon as it starts to make progress in their domain. Denial, Anger, Bargaining, Depression, and Acceptance. Artists seem to be in the depression state, many programmers are still in the denial phase. Pretty solid denial here from a mathematician. o3 was a proof of concept, like every other domain AI enters, it's going to keep getting better. Society is CLEARLY not ready for what AI's impact is going to be. We've been through change before, but never at this scale and speed. I think Musk/Vivek's DOGE thing is important, our governent has gotten quite large and bureaucratic. But the clock has started on AI, and this is a social structural issue we've gotta figure out. Putting it off means we probably become subjects to a default set of rulers if not the shoggoth itself. reply WanderPanda 4 hours agoparentOr is it just white collar workers experiencing what blue collar workers have been experiencing for decades? reply esafak 4 hours agorootparentSo will that make society shift to the left in demand stronger of safety nets, or to the right in search of a strongman to rescue them? reply mensetmanusman 4 hours agoparentprevThe reason why this is so disruptive is because it will effect hundreds of fields simultaneously. Previously workers in a field disrupted by automation would retrain to a different part of the economy. If AI pans out to the point that there are mass layoffs in hundreds of sectors of the economy at once, then i’m not sure the process we have haphazardly set up now will work. People will have no idea where to go beyond manual labor. (But this will be difficult due to the obesity crisis - but maybe it will save lives in a weird way). reply hash872 4 hours agorootparentIf there are 'mass layoffs in hundreds of sectors of the economy at once', then the economy immediately goes into Great Depression 2.0 or worse. Consumer spending is two-thirds of the US economy, when everyone loses their jobs and stops having disposable income that's literally what a depression is reply mensetmanusman 2 hours agorootparentThis will create a prisoner’s dilemma for corporations then, the government will have to step in to provide incentives for insanely profitable corporations to keep the proper number of people employed or limit the rate of layoffs. reply haolez 4 hours agoparentprevI think it's a little of both. Maybe generative AI algorithms won't overcome their initial limitations. But maybe we don't need to overcome them to transform society in a very significant way. reply vouaobrasil 2 hours agoprevMy favourite moments of being a graduate student in math was showing my friends (and sometimes professors) proofs of propositions and theorems that we discussed together. To be the first to put together a coherent piece of reasoning that would convince them of the truth was immensely exciting. Those were great bonding moments amongst colleagues. The very fact that we needed each other to figure out the basics of the subject was part of what made the journey so great. Now, all of that will be done by AI. Reminds of the time when I finally enabled invincibility in Goldeneye 007. Rather boring. I think we've stopped to appreciate the human struggle and experience and have placed all the value on the end product, and that's we're developing AI so much. Yeah, there is the possibility of working with an AI but at that point, what is the point? Seems rather pointless to me in an art like mathematics. reply 0points 1 hour agoprev> How much longer this will go on for nobody knows, but there are lots of people pouring lots of money into this game so it would be a fool who bets on progress slowing down any time soon. Money cannot solve the issues faced by the industry which mainly revolves around lack of training data. They already used the entirety of the internet, all available video, audio and books and they are now dealing with the fact that most content online is now generated by these models, thus making it useless as training data. reply rishicomplex 6 hours agoprevWho is the author? reply williamstein 6 hours agoparentKevin Buzzard reply est 6 hours agoprevAt this stage I assume everything having a sequencial pattern can and will be automated by LLM AIs. reply Someone 6 hours agoparentI think that’s provably incorrect for the current approach to LLMs. They all have a horizon over which they correlate tokens in the input stream. So, for any LLM, if you intersperse more than that number of ‘X’ tokens between each useful token, they won’t be able to do anything resembling intelligence. The current LLMs are a bit like n-gram databases that do not use letters, but larger units. reply beng-nl 4 hours agorootparentIt’s that a bit of an unfair sabotage? Naturally, humans couldn’t do it, even though they could edit the input to remove the X’s, but shouldn’t we evaluate the ability (even intelligent ability) of LLM’s on what they can generally do rather than amplify their weakness? reply red75prime 4 hours agorootparentprevThe follow-up question is \"Does it require a paradigm shift to solve it?\". And the answer could be \"No\". Episodic memory, hierarchical learnable tokenization, online learning or whatever works well on GPUs. reply palata 6 hours agoparentprevAt this stage I hope everything that needs to be reliable won't be automated by LLM AIs. reply sylware 6 hours agoprevHow to train an AI strapped to a formal solver. reply 4ad 5 hours agoprev> FrontierMath is a secret dataset of “hundreds” of hard maths questions, curated by Epoch AI, and announced last month. The database stopped being secret when it was fed to proprietary LLMs running in the cloud. If anyone is not thinking that OpenAI has trained and tuned O3 on the \"secret\" problems people fed to GPT-4o, I have a bridge to sell you. reply fn-mote 5 hours agoparentThis level of conspiracy thinking requires evidence to be useful. Edit: I do see from your profile that you are a real person though, so I say this with more respect. reply dns_snek 2 hours agorootparentWhat evidence do we need that AI companies are exploiting every bit of information they can use to get ahead in the benchmarks to generate more hype? Ignoring terms/agreements, violating copyright, and otherwise exploiting information for personal gain is the foundation of that entire industry for crying out loud. reply retrocryptid 5 hours agoprevWhen did we decide that AI == LLM? Oh don't answer. I know, The VC world noticed CNNs and LLMs about 10 years ago and it's the only thing anyone's talked about ever since. Seems to me the answer to 'Can AI do maths yet?' depends on what you call AI and what you call maths. Our old departmental VAX running at a handfull of megahertz could do some very clever symbol manipulation on binomials and if you gave it a few seconds, it could even do something like theorum proving via proto-prolog. Neither are anywhere close to the glorious GAI future we hope to sell to industry and government, but it seems worth considering how they're different, why they worked, and whether there's room for some hybrid approach. Do LLMs need to know how to do math if they know how to write Prolog or Coc statements that can do interesting things? I've heard people say they want to build software that emulates (simulates?) how humans do arithmetic, but ask a human to add anything bigger than two digit numbers and the first thing they do is reach for a calculator. reply sincerecook 5 hours agoprevNo it can't, and there's no such thing as AI. How is a thing that predicts the next-most-likely word going to do novel math? It can't even do existing math reliably because logical operations and statistical approximation are fundamentally different. It is fun watching grifters put lipstick on this thing and shop it around as a magic pig though. reply lproven 6 hours agoprevBetteridge's Law applies. reply noFaceDiscoG668 7 hours agoprev\"once\" the training data can do it, LLMs will be able to do it. and AI will be able to do math once it comes to check out the lights of our day and night. until then it'll probably wonder continuously and contiguously: \"wtf! permanence! why?! how?! by my guts, it actually fucking works! why?! how?!\" reply tossandthrow 7 hours agoparentI do think it is time to start questioning whether the utility of ai solely can be reduced to the quality of the training data. This might be a dogma that needs to die. reply croes 6 hours agorootparentIf not bad training data shouldn’t be problem reply kergonath 6 hours agorootparentThere can be more than one problem. The history of computing (or even just the history of AI) is full of things that worked better and better right until they hit a wall. We get diminishing returns adding more and more training data. It’s really not hard to imagine a series of breakthroughs bringing us way ahead of LLMs. reply noFaceDiscoG668 7 hours agorootparentprevI tried. I don't have the time to formulate and scrutinise adequate arguments, though. Do you? Anything anywhere you could point me to? The algorithms live entirely off the training data. They consistently fail to \"abduct\" (inference) beyond any language-in/of-the-training-specific information. reply jstanley 7 hours agorootparentThe best way to predict the next word is to accurately model the underlying system that is being described. reply tossandthrow 6 hours agorootparentprevIt is a gradual thing. Presumably the models are inferring things on runtime that was not a part of their training data. Anyhow, philosophically speaking you are also only exposed to what your senses pick up, but presumably you are able to infer things? As written: this is a dogma that stems from a limited understanding of what algorithmic processes are and the insistence that emergence can not happen from algorithmic systems. reply Flenkno 6 hours agoparentprevAWS announced 2 or 3 weeks a way of formulating rules into a formal language. AI doesn't need to learn everything, our LLM Models already contain EVERYTHING. Including ways of how to find a solution step by step. Which means, you can tell an LLM to translate whatever you want, into a logical language and use an external logic verifier. The only thing a LLM or AI needs to 'understand' at this point is to make sure that the statistical translation from left to right is high enough. Your brain doesn't just do logic out of the box, You conclude things and formulate them. And plenty of companies work on this. Its the same with programming, if you are able to write code and execute it, you execute it until the compiler errors are gone. Now your LLM can write valid code out of the box. Let the LLM write unit tests, now it can verify itself. Claude for example offers you, out of the box, to write a validation script. You can give claude back the output of the script claude suggested to you. Don't underestimate LLMs reply TeamDman 2 hours agorootparentIs this the AWS thing you referenced? https://aws.amazon.com/what-is/automated-reasoning/ reply alphan0n 5 hours agoprevAs far as ChatGPT goes, you may as well be asking: Can AI use a calculator? The answer is yes, it can utilize a stateful python environment and solve complex mathematical equations with ease. reply lcnPylGDnU4H9OF 4 hours agoparentThere is a difference between correctly stating that 2 + 2 = 4 within a set of logical rules and proving that 2 + 2 = 4 must be true given the rules. reply alphan0n 3 hours agorootparentI think you misunderstood, ChatGPT can utilize Python to solve a mathematical equation and provide proof. https://chatgpt.com/share/676980cb-d77c-8011-b469-4853647f98... More advanced solutions: https://chatgpt.com/share/6769895d-7ef8-8011-8171-6e84f33103... reply cruffle_duffle 1 hour agoparentprevIt still has to know what to code in that environment. And based on my years of math as a wee little undergrad, the actual arithmetic was the least interesting part. LLM’s are horrible at basic arithmetic, but they can use python for the calculator. But python wont help them write the correct equations or even solve for the right thing (wolfram alpha can do a bit of that though) reply alphan0n 42 minutes agorootparentYou’ll have to show me what you mean. I’ve yet to encounter an equation that 4o couldn’t answer in 1-2 prompts unless it timed out. Even then it can provide the solution in a Jupyter notebook that can be run locally. reply puttycat 6 hours agoprevNo: https://github.com/0xnurl/gpts-cant-count reply sebzim4500 6 hours agoparentI can't reliably multiply four digit numbers in my head either, what's your point? reply reshlo 6 hours agorootparentNobody said you have to do it in your head. reply sebzim4500 5 hours agorootparentThat's the equivalent to what we are asking the model to do. If you give the model a calculator it will get 100%. If you give it a pen and paper (e.g. let it show it's working) then it will get near 100%. reply ashoeafoot 5 hours agoprevAi has a interior world model thus it can do math if a chain of proof is walking without uncertainty from room to room. the problem is its inability to reflect on its own uncertainty and to then overrife that uncertainty ,should a new room entrance method be selfsimilar to a previous entrance reply casenmgreen 7 hours agoprevI may be wrong, but I think it a silly question. AI is basically auto-complete. It can do math to the extent you can find a solution via auto-complete based on an existing corpus of text. reply Bootvis 7 hours agoparentYou're underestimating the emergent behaviour of these LLM's. See for example what Terrence Tao thinks about o1: https://mathstodon.xyz/@tao/113132502735585408 reply WhyOhWhyQ 4 hours agorootparentI'm always just so pleased that the most famous mathematician alive today is also an extremely kind human being. That has often not been the case. reply roflc0ptic 7 hours agoparentprevPretty sure this is out of date now reply noFaceDiscoG668 7 hours ago [flagged]rootparentnext [2 more] Provide proof. ( except you can't because how the fuck are you and I gonna walk through the entire corpus ) these companies and coders are *just* lying. reply kergonath 6 hours agorootparentWhy would others provide proofs when you are yourself posting groundless opinions as facts in this very thread? reply mdp2021 5 hours agoparentprev> AI is basically Very many things conventionally labelled in the 50's. You are speaking of LLMs. reply casenmgreen 5 hours agorootparentYes - I mean only to say \"AI\" as the term is commonly used today. reply esafak 4 hours agoparentprevHumans can autocomplete sentences too because we understand what's going on. Prediction is a necessary criterion for intelligence, not an irrelevant one. reply intellix 6 hours agoprevI haven't checked in a while, but last I checked ChatGPT it struggled on very basic things like: how many Fs are in this word? Not sure if they've managed to fix that but since that I had lost hope in getting it to do any sort of math reply yodsanklai 3 hours agoprevI understand the appeal of having a machine helping us with maths and expanding the frontier of knowledge. They can assist researchers and make them more productive. Just like they can make already programmers more productive. But maths are also fun and fulfilling activity. Very often, when we learn a math theory, it's because we want to understand and gain intuition on the concepts, or we want to solve a puzzle (for which we can already look up the solution). Maybe it's similar to chess. We didn't develop search engines to replace human players and make them play together, but they helped us become better chess players or understanding the game better. So the recent progress is impressive, but I still don't see how we'll use this tech practically and what impacts it can have and in which fields. reply jokoon 1 hour agoprev [–] I wish scientists who do psychology and cognition of actual brains could approach those AI things and talk about it, and maybe make suggestions. I really really wish AI would make some breakthrough and be really useful, but I am so skeptical and negative about it. reply joe_the_user 1 hour agoparentUnfortunately, the scientists who study actually brains have all sort of interesting models but ultimately very little clue how these actual brains work at the level of problem solving. I mean, there's all sort of \"this area is associated with that kind of process\" and \"here's evidence this area does this algorithm\" stuff but it's all at the level you imagine steam engine engineers trying to understand a warp drive. The \"open worm project\" was an effort years ago to get computer scientists involved in trying to understand what \"software\" a very small actual brain could run. I believe progress here has been very slow and that an idea of ignorance that much larger brains involve. https://en.wikipedia.org/wiki/OpenWorm reply bongodongobob 16 minutes agoparentprev [–] If you can't find useful things for LLMs or AI at this point, you must just lack imagination. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI's new language model, o3, achieved a 25% score on the FrontierMath dataset, which includes challenging math questions with definitive answers.",
      "The dataset is kept secret to prevent language models from training on it, and it includes problems that require expertise in specific math areas.",
      "While o3's performance is notable, the real advancement will be when AI can solve more complex, proof-based questions, as current models like DeepMind's AlphaProof are still limited to high school-level problems."
    ],
    "commentSummary": [
      "AI's capability in mathematics is debated, as it can assist with problems and pattern recognition but often errs in logical reasoning and arithmetic.",
      "Models like ChatGPT are useful for conceptual understanding but require human guidance for complex math, highlighting AI's limitations in reasoning.",
      "There are concerns about AI's impact on jobs and the integrity of benchmarks, though it shows promise in aiding mathematical research, it cannot yet independently advance mathematical knowledge."
    ],
    "points": 192,
    "commentCount": 167,
    "retryCount": 0,
    "time": 1734951030
  },
  {
    "id": 42489844,
    "title": "CUDA Moat Still Alive",
    "originLink": "https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/",
    "originBody": "December 22, 2024 MI300X vs H100 vs H200 Benchmark Part 1: Training – CUDA Moat Still Alive Training Performance, User Experience, Usability, Nvidia, AMD, GEMM, Attention, Networking, InfiniBand, Spectrum-X Ethernet, RoCEv2 Ethernet, SHARP, Total Cost of Ownership 33 minutes 7 comments on MI300X vs H100 vs H200 Benchmark Part 1: Training – CUDA Moat Still Alive By Dylan Patel and Daniel Nishball ShareCopied to clipboard LinkedIn X Facebook Table of Contents Intro Key Findings Executive Recommendation to AMD A Summary of the AMD vs Nvidia Narrative General Matrix Multiply (GEMM) Performance Popular GEMM Benchmark Isn't Accurate HBM Memory Bandwidth Performance AMD Hand-Crafted VIP Custom Builds and WIP Development Builds Dec 21st AMD Development Builds Training Testing Methodology (GPT1.5B, Llama 8B, Llama 70B, Mistral) Single Node Training Performance Multi-Node Training Performance AMD PYTORCH_TUNABLE_OPS FLAG is a Bad User Experience Scale Up NVLink/xGMI Topology All Reduce/All to All/Reduce Scatter/All Gather Collectives Overview Single Node NCCL Collective Multi Node RCCL/NCCL Collectives and Scale Out Network Benchmarks AMD's User Experience is Suboptimal and the MI300X is Not Usable Out of the Box Exploring Ideas for Better Performance on AMD AMD’s Forked Libraries Detailed Recommendations to AMD on How to Fix Their Software H100/H200/MI300X Networking BoM Analysis and Performance per TCO H100/H200/MI300X Networking BoM Analysis and Performance per TCO Further Experiments Benchmarking Warmup/Repeats Effects VBoost Power Shifting BF16 vs FP16 Input Distribution Affects Performance FLOP per GPU PicoJoule PyTorch PyPi Distribution vs. Nvidia NGC Stable PyTorch Images Subscribe for full access Intro SemiAnalysis has been on a five-month long quest to settle the reality of MI300X. In theory, the MI300X should be at a huge advantage over Nvidia’s H100 and H200 in terms of specifications and Total Cost of Ownership (TCO). However, the reality is that the on paper specs as given below are not representative of performance that can be expected in a real-world environment. If AMD could deliver the below marketed performance with this memory, it would be a very strong competitor in the market. Source: SemiAnalysis, Nvidia, AMD Today we are going to talk through our five-month journey conducting independent analysis and training-focused benchmarking of the MI300X, the H100 and the H200, engaging with both NVIDIA and AMD. We will do a detailed overview of the numerous low-level benchmarks that we ran, see the table of contents for summary. Furthermore, we will compare the total cost of ownership of Nvidia and AMD GPUs and factor in performance. Ultimately much of what we are doing is openly giving a comprehensive public recommendation to AMD on what they need to do to be competitive and fix their software issues after five months of submitting and squashing bugs. It’s not just that it’s immature software, they need to change how they do development. In short, when comparing Nvidia’s GPUs to AMD’s MI300X, we found that the potential on paper advantage of the MI300X was not realized due to a lack within AMD public release software stack and the lack of testing from AMD. AMD’s software experience is riddled with bugs rendering out of the box training with AMD is impossible. We were hopeful that AMD could emerge as a strong competitor to NVIDIA in training workloads, but, as of today, this is unfortunately not the case. The CUDA moat has yet to be crossed by AMD due to AMD’s weaker-than-expected software Quality Assurance (QA) culture and its challenging out of the box experience. As fast as AMD tries to fill in the CUDA moat, NVIDIA engineers are working overtime to deepen said moat with new features, libraries, and performance updates. We shared benchmark source code and intermediate test results for GEMM benchmark and Single Node Training with both Nvidia and AMD and held calls and discussions to solicit feedback and implement improvements to the benchmarks, and we worked with AMD to implement bug fixes for the software stacks. Our goal with this highly iterative interaction was to ensure that our tests are an unbiased evaluation of what real-world users would experience. We initially planned to publish this article a few months ago but wanted to take the extra time to engage with the AMD team and explore possible fixes or development work. We spent a considerable time identifying and fixing AMD software bugs so that we could give AMD every chance to show MI300X unhindered by AMD software stack bugs as opposed to only showing problematic performance out of the box. To give a fair impression, we also explain the considerable amount of work on tuning and bug-squashing that it took to get there. We think this approach provides users with the best possible level of transparency. We wanted to contribute in any way we could to try to improve the AMD ecosystem. Though AMD software is much better now due to our bug reports and tire-kicking, its public software stack still falls short. We have open-sourced many of the benchmarks and created simple one-liner commands to reproduce them. If Lisa Su and the AMD Leadership redouble their investment with a focus on their software and testing stack, they have a chance to be competitive with Nvidia on training. We think the engineers at AMD are extremely capable and are doing their best to advance the AMD ecosystem – and indeed support from these engineers in the form of bug fixes, configuration help and custom images improved the results we were able to get from the MI300X. To bring our benchmarking process to a coda, on November 15th, 2024 we sent Nvidia and AMD a draft of most of our major GEMM and single node benchmarking code and results for comments, verification, and fine-tuning. We asked that any final comments, fixes, feedback and any performance improvements be submitted by November 25th. We set this time frame to crystallize test results to allow time to write an in-depth analysis and commentary and carry out multiple rounds of internal and external reviews, all steps that can take a variable and often unknowable amount of time, typically from 2-4 weeks. A few days ago, after we informed both that we had confirmed an article publication date of December 20th, AMD requested that we delay publication to include results based on a beta WIP development build on an AMD developer’s branch. All of our benchmarking on Nvidia was conducted on publicly available stable release builds. In the spirit of transparency and fairness, we include these results as well as updated testing harness results on as the original November 25th deadline image and the latest publicly available software. However, we believe that the correct way to interpret the results is to look at the performance of the public stable release of AMD/Nvidia software. Below are the list of software builds that we have used for benchmarking: H100 Public Stable Release – Out of Box experience for Nvidia H100. H200 Public Stable Release – Out of Box experience for Nvidia H200. MI300X Nov 25th Custom Build – This is a custom VIP docker image hand-crafted that builds all dependencies from source code written by AMD principal engineers. MI300X Stable Public Release PyTorch 2.5.1 – Out of Box experience for AMD MI300X. MI300X Public Nightly Dec 19th – This can indicate where AMD performance can be by January 2025, when PyTorch 2.6 is released, over 1 year after launch. MI300X Dec 21st WIP dev build – This is the image that AMD submitted to us after we agreed to delay publication of the article. It is an experimental development build that has not yet been merged into AMD’s internal main branch, and it does not use the native PyTorch flash attention API. Performance with this image can indicate where AMD public stable release performance will be in 1-2 quarters from now. We are very thankful for the technical support provided by AMD and Nvidia throughout this process, but we maintain our independence in the results we publish. We want to shout out to and thank our AMD counterparties, Anush Elangovan (AMD VP of AI), Hui Liu and many dozens of amazing AMD Principal/Senior engineers, AMD VPs of Engineering, AMD Engineering Fellows, AMD CVPs of Engineering and AMD Directors of Engineering, AMD Software Library Leads, for triaging and fixing our various bug reports. On the Nvidia side, we are grateful to Kedar Potdar, Ian Buck, Sylvain Jeaugey and the NCCL team from NVIDIA for their amazing support. Thank you to Crusoe, TensorWave (AMD Ventures Portco), Nebius, Lambda, Hot Aisle and Sustainable Metal Cloud (SMC) / Firmus for the compute and for being supporters of open-source benchmarking. Crusoe, Nebius, SMC / Firmus and Lambda support managed SLURM and shared home directories out of the box. TensorWave currently has managed SLURM in beta and this feature will come to general availability (GA) at the start of next year. Sustainable Metal Cloud is one of the few neoclouds that has official MLPerf GPT-3 175B Training results. We will be releasing a follow up article on inferencing for the H100, H200 and MI300X. We may also release a follow-up article in a few months to follow up on AMD training performance to see if out of box experience has improved and test other models such as LlaVa & Mamba. Source: SemiAnalysis Key Findings Comparing on paper FLOP/s and HBM Bandwidth/Capacity is akin to comparing cameras by merely examining megapixel count. The only way to tell the actual performance is to run benchmarking. Nvidia’s Out of the Box Performance & Experience is amazing, and we did not run into any Nvidia specific bugs during our benchmarks. Nvidia tasked a single engineer to us for technical support, but we didn’t run into any Nvidia software bugs as such we didn’t need much support. AMD’s Out of the Box Experience is very difficult to work with and can require considerable patience and elbow grease to move towards a usable state. On most of our benchmarks, Public AMD stable releases of AMD PyTorch is still broken and we needed workarounds. If we weren’t supported by multiple teams of AMD engineers triaging and fixing bugs in AMD software that we ran into, AMD’s results would have been much lower than Nvidia’s. We ran unofficial MLPerf Training GPT-3 175B on 256 H100 in collaboration with Sustainable Metal Cloud to test the effects of different VBoost setting For AMD, Real World Performance on public stable released software is nowhere close to its on paper marketed TFLOP/s. Nvidia’s real world performance also undershoots its marketing TFLOP/s, but not by nearly as much. The MI300X has a lower total cost of ownership (TCO) compared to the H100/H200, but training performance per TCO is worse on the MI300X on public stable releases of AMD software. This changes if one uses custom development builds of AMD software. Training performance is weaker, as demonstrated by the MI300X ‘s matrix multiplication micro-benchmarks, and AMD public release software on single-node training throughput still lags that of Nvidia’s H100 and H200. MI300X performance is held back by AMD software. AMD MI300X software on BF16 development branches have better performance but has not yet merged into the main branch of AMD’s internal repos. By the time it gets merged into the main branch and into the PyTorch stable release, Nvidia Blackwell will have already been available to everyone. AMD’s training performance is also held back as the MI300X does not deliver strong scale out performance. This is due to its weaker ROCm Compute Communication Library (RCCL) and AMD’s lower degree of vertical integration with networking and switching hardware compared to Nvidia’s strong integration of its Nvidia Collective Communications Library (NCCL), InfiniBand/Spectrum-X network fabric and switches. Many of AMD AI Libraries are forks of NVIDIA AI Libraries, leading to suboptimal outcomes and compatibility issues. AMD customers tend to use hand crafted kernels only for inference, which means their performance outside of very narrow well defined use cases is poor, and their flexibility to rapidly shifting workloads is non-existent. Executive Recommendation to AMD We genuinely want to see another effective competitor to Nvidia and want to help AMD get to that spot, but, unfortunately, there is still much work to be done on that front. At the bottom of this article, we have a detailed list of feedback for the Lisa Su and the AMD Leadership Team, but provide a summary here: Give AMD Engineers more compute and engineering resources to fix and improve the AMD ecosystem, they have very few internal gpu boxes relative to what Nvidia provides to their engineers. Tensorwave, the largest AMD GPU Cloud has given GPU time for free to a team at AMD to fix software issues, which is insane given they paid for the GPUs. AMD needs to hook up thousands more of MI300X, MI325X to PyTorch CI/CD for automated testing to ensure there is no AMD performance regressions & functional AMD bugs. Nvidia has given thousands of GPUs for PyTorch CI/CD to ensure an amazing out of box experience The AMD Executive Team should personally and intensively internally test (i.e., “dogfood”) products that are being shipped to the public rather than focus on testing internal builds. Preferably dogfood during livestream (twitch.tv) to show the authentic out of box experience. This is like how geohotz livestreams AMD should collaborate with Meta to get production LLM training workloads working as soon as possible on PyTorch ROCm, AMD’s answer to CUDA, as commonly, PyTorch code paths that Meta isn’t using have numerous bugs. Move away from over-reliance on properly setting numerous environment flags (up to dozens) to make an AMD deployment usable. Instead, bake these settings into the default configuration. Make the out of the box experience usable! Focus on making out of box experience good instead of over-reliance on custom VIP images that build all dependencies from source code main@specificcommit and take 5 hours to build. Stop expecting end users to use PYTORCH_TUNABLE_OPS which is a prototype buggy feature and is not respectful of the end users time as it takes ~1 hour for the end user to tune every time an end user wants to make any changes to their code. AMD should submit MLPerf Training GPT-3 175B results. MLPerf is an apples-to-apples benchmarking methodology that uses time to convergence as the north star. We want AMD to be competitive and are open to meet with more detailed feedback on how to fix the AMD Datacenter GPU Ecosystem for the better. A Summary of the AMD vs Nvidia Narrative Before we dive into various facets of AMD’s software stack that hold AMD back, we will discuss the MI300X’s basic specifications, its comparative total cost of ownership, and how most analysts and investors have evaluated its competitiveness. The MI300X launched in late 2023 with an exciting set of on paper specifications—featuring 1,307 TFLOP/s of FP16 compute (stronger than the H100’s 989 TFLOP/s), 5.3 TB/s of memory bandwidth, and 192GB of HBM3, 3.35 TB/s of memory bandwidth, and 80GB of HBM3. These specs outstrip those of the H200, which itself is, effectively, a memory-spec bumped version of the H100, delivering 4.8TB/s of memory bandwidth and 141GB of HBM3e. Source: SemiAnalysis, Nvidia, AMD On paper total cost of ownership for an MI300X deployment is extremely compelling, not only due to the lower ASP of the MI300X, but also because it is typically deployed using cheaper Ethernet networking. Comparing a cluster of 16k H200s vs a 16k MI300X ethernet cluster leads to nearly 40% of the cost savings coming from networking alone, with the remainder of the savings from a lower accelerator cost. The use of Whitebox Ethernet switches is a substantial cost savings compared to using Nvidia’s Quantum-2 switches, but the real difference is cheaper transceivers, as Nvidia branded transceivers cost as much as 2-3x over what a typical transceiver OEM charges. At face value, the MI300X seems the best of both worlds: higher performance and lower total cost of ownership. At the time of its launch, it was logical to expect share gains to the underdog AMD from this compelling combination. The table below shows total upfront cluster capex – we present a more detailed breakdown of cluster capex components as well as a detailed networking BoM analysis in the sections at near the bottom of the article. Source: SemiAnalysis AI TCO Model As orders solidified, excitement built up for potential of the MI300X, helped along by bullish commentary and guidance from AMD. With a compelling spec advantage, it was easy to argue for further upside to AMD’s guidance, which most investors assumed management was sandbagging. AMD had a strong hand, in theory. After all they have mid-single digit market share in datacenter GPUs for 2024 and, logically, a glide path towards even 10-12% market share by 2027 could be conservative while offering considerable earnings upside for AMD. However, over from late 2023 and through most of 2024, guidance for full year 2024 datacenter GPU sales repeatedly underperformed those lofty expectations. From its 1Q24 earnings through its 3Q24 earnings, AMD only raised guidance from $4B to $5B, well under the $6-8B investor bogey based on CoWoS and HBM supply agreements. Our demand view in the Accelerator Model tracked Microsoft’s disappointment early in the year and lack of follow on orders. The earlier bullish line of reasoning was like purchasing a certain car model from a magazine without a test drive or soliciting feedback from owners of that model or reading any reviews. But fear not – SemiAnalysis has put the MI300X, H100, and H200 through their paces at scale and can show why AMD’s current software stack issues decisively disprove this line of reasoning. General Matrix Multiply (GEMM) Performance Most FLOPS in a transformer-based architecture (i.e. ChatGPT, Llama, etc.) go towards matrix multiplication, also known as GEMMs. For this reason, GEMM performance is a good proxy for how well frontier transformers, such as ChatGPT, Llama, Claude, Grok, etc. will train on the hardware. GEMMs take two input matrices, Matrix A and Matrix B, with Matrix A having the shape of (M, K), M rows and K columns, and Matrix B having the shape of (K,N) to produce an output matrix of shape (M,N). Source: Nvidia Conceptually, each element of the resulting matrix is a sum of element-wise multiplications along the “K” dimension of the inputs. For this matter, the K dimension is also known as the reduction dimension. Source: SemiAnalysis Below, we have tested the following real-world shapes, given in the form (M,N,K)—which is short for multiplying a matrix of dimensions (M,K) and (K,N) together. These following matrix shapes were actually used in Meta’s Llama 70B production training: (16384, 8192, 1280) – Fused QKV Projection GEMM shape (16384, 1024, 8192) – Attention Output Projection shape (16384, 8192, 7168) – FFN GEMM shape (16384, 3584, 8192) – FFN GEMM shape (8192, 8192, 8192) – Standard GEMM shape for benchmarking We used OpenAI’s do_bench function for the benchmark setup, an industry standard method of benchmarking PyTorch. The do_bench function provides cache clearing between runs as a default and provides ways to warmup and execute the benchmark multiple times, taking the median result as the given accuracy. We used warmup=30 and rep=200 for these tests. Both input tensor A and B were randomly initialized with a normal distribution with mean 0 and variance 1. This is because a normal distribution comes the closest to matching the actual distribution of weights and activations in modern neural networks. The distribution of the input tensors will affect the results of the TFLOP/s performance benchmark. We will discuss the reasons why the input distribution effects TFLOP/s performance later in the article. For BF16, we can see that the H100 and H200 achieves roughly 720 TFLOP/s against their marketed 989.5 TFLOP/s, while the MI300X reaches a mere ~620 TFLOP/s compared with their marketed 1,307 TFLOP/s. This means that, despite a much higher marketed BF16 TFLOP/s, the MI300X is 14% slower than the H100 and H200. This AMD result used a custom docker image that was hand crafted by an AMD principal engineer yet still achieved slower performance than Nvidia’s GPUs. For our out of the box testing of the MI300X, the TFLOP/s throughput even slower than this! In addition to a custom image, AMD also requires the user to set numerous environment flags that aren’t set by default to reach these performance results. Source: SemiAnalysis Unfortunately, the story is worse for FP8. The H100/H200 achieves ~1,280 TFLOP/s out of the marketed 1979 TFLOP/s. The MI300X, in comparison, only reaches ~990 TFLOP/s. Thus, for FP8, the MI300X is 22% slower than H100. This is for both inputs being of the e4m3 FP8 (i.e. 4 exponent bits and 3 mantissa bits) datatype. Source: SemiAnalysis It is important to note that calling GEMM is a simple task, and we shouldn’t expect to run into AMD software bugs. Unfortunately, a major bug that we encountered is that the torch.matmul and F.Linear APIs have been delivering different performances on AMD for a couple of months during the summer. One would expect the torch.matmul and F.Linear APIs to have the same performance, but, surprisingly, F.Linear is much slower! This is a strange bug as torch.matmul and F.Linear are both wrappers around the hardware vendor GEMM libraries, so they should achieve the same level of performance. F.Linear, in particular, is important, as this is the way most end users in PyTorch launch the GEMM kernels. When we started testing AMD five months ago, the public AMD PyTorch still had this bug. The root cause was that AMD in fact has two different underlying GEMM libraries, rocBLAS and hipBLASLt, with HipBLASLt being more optimized for the MI300X. The bug was that torch.matmul uses the optimized hipBLASLt, but AMD had not changed F.Linear by default, leaving it to use the unoptimized rocBLAS library. This major bug was ultimately fixed by AMD a few months ago after our bug reports, and we hope it doesn’t reappear due to a lack of proper regression testing. AMD’s usability could improve considerably if it boosted its testing efforts instead of waiting for users to discover these critical issues. We have open sourced the GEMM benchmark used in our tests into a simple three liner that anyone can easily run: Source: SemiAnalysis Popular GEMM Benchmark Isn’t Accurate Recently, a benchmark has been floating around the internet that claims that, on GEMMs, AMD MI300X’s performance is close to that of the H100. Source: Github There are two main issues with the benchmark: it isn’t properly carrying out L2 Cache clearing and also is simply taking the max performance, instead of the median/mean TFLOP/s over the course of the iterations for a specific shape. Without L2 Cache clearing between iterations, the benchmark does not accurately reflect real-world GEMM performance. Furthermore, since the TFLOP/s change based on which iteration it is on, you need to use a mean/median over at least 100 iterations as the basis for an accurate GEMM benchmark. OpenAI’s do_bench provides L2 cache and mean/median out of the box by default, so we recommend that engineers use it for micro-benchmarking. Below, we have simplified the benchmark into pseudocode and have commented on the issues mentioned above. Source: SemiAnalysis HBM Memory Bandwidth Performance It is widely known that AMD MI300X has better memory bandwidth than the Nvidia H100 and H200, offering 5.3 TB/s of bandwidth vs 4.8 TB/s for the H200 and 3.35 TB/s for the H100. Improved HBM memory bandwidth is very useful in inferencing and is sometimes useful in training. In training, users can set a larger batch size if they have more HBM memory capacity and memory bandwidth. Although if a larger global batch size is used, after a certain size, the model will take longer to convergence. It is easy to run fast with big global batch size but at a high level, it will hurt time to convergence. From our HBM memory bandwidth benchmarking, we see that that MI300X indeed has way better memory bandwidth than both the H200 and the H100. We tested memory bandwidth in Pytorch with Tensor.copy_ & used the industry standard OpenAI do_bench to ensure accuracy. As you will see in our upcoming H100 vs H200 vs MI300X inference article, memory bandwidth is very important for inferencing. Source: SemiAnalysis Source: SemiAnalysis AMD Hand-Crafted VIP Custom Builds and WIP Development Builds The only reason we have been able to get AMD performance within 75% of H100/H200 performance is because we have been supported by multiple teams at AMD in fixing numerous AMD software bugs. To get AMD to a usable state with somewhat reasonable performance, a giant ~60 command Dockerfile that builds dependencies from source, hand crafted by an AMD principal engineer, was specifically provided for us, since the Pytorch Nightly and public PyTorch AMD images functioned poorly and had version differences. This docker image requires ~5 hours to build from source and installs dependencies and sub-dependencies (hipBLASLt, Triton, PyTorch, TransformerEngine), a huge difference compared to Nvidia, which offers a pre-built, out of the box experience and takes but a single line of code. Most users do not build Pytorch, hipBLASLt from source code but instead use the stable release. When using public PyTorch, users have the choice of working with the latest stable images or a nightly PyTorch upload. So, although a nightly PyTorch upload may have the latest commits that could potentially lead to better performance or could fix some bugs, but users must accept that the upload may not be fully tested and could contain new bugs from Meta/AMD/Nvidia or other PyTorch contributors that have not been discovered yet. Note that most end users are using the stable release of PyTorch. Source: SemiAnalysis, AMD Source: Nvidia Delightfully, Nvidia’s Docker images contain the complete set of developer tools needed for profiling and debugging, like Nsight Compute and Nsight Systems. AMD, in contrast, does not include their OmniTrace developer tool out of the box. Until a couple weeks ago, the AMD docker images only supported PyTorch 2.3, which released 8 months ago. Mainline PyTorch 2.4 and PyTorch 2.5 have also since released and PyTorch 2.6 is about to come out in Q1 2025. We recommended to an AMD Principal Engineer and to AMD’s VP of AI that AMD should have the latest AMD PyTorch version – AMD has since started publishing containers for some of these AMD PyTorch versions. Docker image for AMD PyTorch 2.5 is still missing. Source: Nvidia Dec 21st AMD Development Builds Below is AMD’s December 21st development build docker image. As you can see, it uses a number of non stable devlopment branches for dependencies such as hipBLASLt, AOTriton, ROCm Attention and installs everything including PyTorch from source code, taking upwards of 5 hours to build. These versions of the dependencies haven’t even been merged into AMD’s own main branch yet. 99.9% of users will not be installing PyTorch from source code and all of its dependencies from source code on development branches but will instead use the public stable PyPi PyTorch. Furthermore, instead of using Flash Attention through the PyTorch native user friendly torch. scaled_dot_product_attention API, this AMD Development build imports another library (development branch as well) attention implementation. We have seen more users use Flash Attention through PyTorch native torch. scaled_dot_product_attention API since it is more user friendly and bundled into out of box PyTorch. Even AMD’s own public documentation recommends using Flash Attention through torch.scaled_dot_product_attention API. We hope that these kernels get merged into PyTorch flash attention instead of making the end user install a separate library taking hours of their time to build. This is not a user-friendly experience. Furthermore, AMD must support FlexAttention as it has quickly become the go to in the industry. AMD’s December 21st Dev build is on a hanging development branch. That means it is a branch that has not been fully QA’ed and is at use only at a risk branch. There are many concerns about the validity of the results from using a development build and branches and building from source code, as most users are not doing this in real life. Most users will be installing AMD/Nvidia PyTorch from PyPI stable release mostly so we recommend readers keep this in mend when analyzing these results. That being said, we are including these development build results as it is an indication of where AMD public stable release software will be 1-2 quarters from now. However, at the same time, when it comes to compete, 1-2 quarters from now, Nvidia Blackwell will already be widely deployed, while AMD MI355X will not commence shipments until H2 2025. Source: SemiAnalysis, AMD Training Testing Methodology (GPT1.5B, Llama 8B, Llama 70B, Mistral) There are many ways to test training performance. The most accurate way is to take a medium-sized AI startup model’s internal codebases and run them on a 512-1024 GPU cluster. This way, the test run has all the optimizations that a typical user would have. Everything else is just a proxy for the performance of these training runs. Training performance takes into account HBM bandwidth, HBM capacity, TFLOP/s, networking, and system architecture. Comparing on paper HBM bandwidth/capacity is just like comparing on paper camera megapixels. MLPerf GPT3 175B Training is also a good proxy to measure the time it takes to train to a specific convergence. MLPerf benchmark considers global batch sizes and whether a mixed precision implementation incurs a convergence penalty. Unfortunately, MLPerf is quite difficult to run due to a lack of user-friendly documentation and instructions, and the performance is often min-maxed via a custom tuned configuration specifically concocted for MLPerf that an average user would not adopt. Note that Nvidia has submitted MLPerf Training results with over 11k H100s, while AMD runs MLPerf Training internally. AMD’s results are likely weak, so they have never submitted any MLPerf Training, let alone the MLPerf GPT3 175B benchmark. When designing our SemiAnalysis benchmark, we wanted to reflect the average user’s model implementation, and so opted for torch. scaled_dot_product_attention API (which uses flash attention backend), PyTorch Distributed Data Parallel (DDP) and/or Fully Sharded Data Parallel (FSDP) with torch.compile. Also note that AMD recommends users use torch.scaled_dot_product_attention in their own documentation. We believe this is the most representative of a typical user workload. Further, we used a generic PyTorch native implementation of these models to keep it close to a typical ML Scientist user and make it easy to run with a single line of code. In contrast to MLPerf, the goal of our benchmark is to be as simple to run as possible, while still being a good proxy for performance. Note, since we don’t take into account time to convergence, this benchmark has a slight bias towards AMD as we set the micro batch size higher on AMD vs on Nvidia. When taking time to convergence into account, AMD results will be worse than what is stated. As an aside, many AI practitioners have said they are not using Megatron or NeMo or 3D Parallelism due to the high level of complexity and lack of flexibility associated with those libraries, whose rigidity and complexity make their usage for ML Research effectively impossible. Note that in terms of 3D Parallelism, both Nvidia and AMD will get higher performance, assuming their software stack works, which is a big assumption for AMD. AMD Megatron is a fork of Nvidia Megatron and has less than 10 stars which means that it is probably not dogfooded well. Submitting bug reports would take extra months to get AMD Megatron working for simple models. For our SemiAnalysis model training benchmark, we will test four models, with the first being a simple GPT 1.5B DDP, as we believe this is representative of what small-scale experiments/ablations would look like before scale-out to bigger model sizes. DDP is a much simpler and less network-intensive form of parallelism. Next, we tested the standard Llama3 8B and Llama3 70B 4 Layer Proxy as a baseline for a popular model’s performance. Third, we tested Mistral 7B v0.1, which evaluates if hardware will perform well when adding a bit of complexity, as Mistral uses sliding window attention instead of the standard causal attention. Modern models such as ChatGPT, Claude, Genimi, o1, o3 do not use standard causal attention & use a complex attention mechanism. A Modern GPT/Llama/Transformer model is built by stacking the same transformer layer over & over again. As such, measuring the performance of just 4 layers is a great proxy for the overall performance of the model. Source: Imgur Furthermore, in modern LLM training for all frontier LLM models, pipeline parallelism is used which means that a couple of transformer layers are placed in each GPU server. Never in modern pretraining is a whole model placed on a single node. Source: SemiAnalysis The model FLOP for each token trained is defined by the following formula: 6 * non_input_embedding_params + 12 * num_layers * num_heads * head_dim * max_seq_len * density With density being how sparse the attention is relative to a full mask. Causal attention has, for example, a 50% sparsity, while sliding window attention has even lower sparsity. Note that originally our testing harness used 6 * params instead of 6 * non_input_embedding_params which is the wrong way of calculating model FLOP per token. Furthermore, there was another bug in regard to the way we used FSDP. We have since updated our testing harness and retroactively retested as well as updated all of benchmark results across all versions of software for both H100, H200, MI300X, public stable, public nightly, VIP images and AMD development builds. All results listed below are with the updated testing harness. Single Node Training Performance Note that the H100/H200 performance we present in this report reflects an out of the box performance without any hand-crafted tuning from Nvidia engineers, while the results for the MI300X comes after many months of tuning and bug fixes from AMD’s engineers. We did not run into any Nvidia-specific bugs compared to AMD training, which was comparatively bug-filled. Five months ago, many models couldn’t run at more than 150 TFLOP/s on the AMD MI300X due to an AMD software bug in attention backwards and torch compile, which forced the user to manually mark a region of the model as non-compliable instead of having a full graph compile. We see that, for all models, the H100/H200 wins relative to MI300X public releases/public nightly releases/Nov 25thbuild from source VIP image. It is interesting that the MI300X does not perform well on smaller models such as GPT 1.5B or on any model that uses a non-causal attention layer, like Mistral 7B v0.1. This is due to FlexAttention not being fully operational at the time of the deadline, while, on Nvidia GPUs, it has been working since August 2024. As such, the H100/H200 beats MI300X by more than 2.5x in terms of TFLOP/s for MI300X public release/public nightly release/Nov25th VIP build. For the Dec 21st MI300X internal WIP development branches build, we still see it perform worse than H100/H200 on GPT 1.5B. Furthermore, it performs slightly worse than H100 on Mistral 7B. For Llama3 8B and Llama3 70B Proxy, the Dec 21st MI300X WIP development build performs better than H100/H200, but note that this is due to MI300X WIP development using an AMD engineer’s development branch that has not even been merged to the AMD main branch. Source: SemiAnalysis Three months ago, attempting to do FP8 Training on AMD led to segfaults and hard errors. On the off chance it did work, it was, in fact, slower than the same run using BF16. We worked with AMD’s FP8 team to fix this issue, as well as the AMD hipBLASLt team, which created tuning for fixing MI300X FP8 performance. FP8 Training is important as it speeds up training compared to BF16 & most frontier labs use FP8 Training. After many fixes, we can see that the MI300X’s Nov 25th throughput for Llama3 8B and GPT 1.5B is somewhat competitive with H100’s. As usual, H200 wins in this category. However, for Llama3 70B 4 Layer Proxy, AMD Nov 25th’s results are sorely beaten. For Mistral 7B which has a non-causal attention layer, AMD Nov 25th performance is close to half that of an H100. This shows that, for anything that isn’t a simple model, even after months of tuning, AMD is still not competitive due to a slight tweak in the model structure. Many frontier models and AI training startups are using complex attention layers for long context spans and efficient attention, but, AMD is still far behind on those. Unfortunately, FP8 training on AMD only works on custom images such as our November 25th VIP image and December 21st WIP development branch image. When we first started trying AMD FP8 Training, it was slower than AMD BF16 Training on public releases. Source: SemiAnalysis For AMD’s WIP development builds, we see that on Llama3 8B, it wins against H100 but is still slower than H200’s public stable software release. H200 performance completely beats MI300X even on their Dec 21st WIP development branches. It is interesting that the MI300X does not perform well on non-causal attention layer, like Mistral 7B v0.1 even for their internal builds. Mistral using sliding window attention which some of the frontier models uses. It seems that if you want to train a model that doesn’t use causal attention, AMD MI300X will automatically lose. While a lot of people putting out performance comparisons between hardware, most do not open source their testing code and they do not make easily reproducible. We took an open source approach, and we have open-sourced our single node training benchmark and made it easy to run with only a couple of lines: Source: SemiAnalysis Multi-Node Training Performance For multi-node, we benchmarked two nodes of H100 and two nodes of MI300X. Unfortunately, we didn’t get access to a multi-node H200 deployment in time for the article. H100 wins again by a big margin in this benchmark compared to MI300X, with the H100 ranging from 10-25% faster. This gap widens as you add more nodes working together into a single training workload. This is a known problem, which AMD is attempting to fix next year by deploying their new in house 400G AI focused NIC. AMD PYTORCH_TUNABLE_OPS FLAG is a Bad User Experience In order to get AMD training working decently, users need to use PYTORCH_TUNABLE_OPS which is an AMD specific prototype flag for the end user to tune GEMMs. Since this is a prototype feature (i.e. not stable), in the past a lot of bugs with this feature cropped up including but not limited to seg faults, HBM memory leaks, and a whole host of otherissues such as many unit tests being disabled. These known tunable ops bugs have been fixed now but there are likely a many more unknown AMD software bugs. Furthermore, even if users do not encounter any bugs and thus the runway is clear for this prototype AMD flag to work, it still takes users anywhere from 1-2 hours to tune any modern LLM model. Although these GEMMs can be cached by the end user, any minor changes to the end user’s code results in the need for the user to spend another 1-2 hours tuning. As you can imagine, this will slow down an ML Scientist’s iteration cycle speed when trying to conduct model R&D and ablations experiments. On Nvidia, this flag isn’t needed as their GEMM library (cuBLASLt) comes tuned out of the box and cuBLASLt’s heuristic model out of the box picks the correct algorithm for most shapes on H100/H200. In contrast, AMD hipBLASLt/rocBLAS’s heuristic model picks the wrong algorithm for most shapes out of the box, which is why so much time-consuming tuning is required by the end user. We recommend that AMD to fix their GEMM libraries’ heuristic model such that it picks the correct algorithm out of the box instead of wasting the end user’s time doing tuning on their end. Users often iterate quickly when doing research and therefore rerunning tunable ops will slow down research velocity significantly. Scale Up NVLink/xGMI Topology Scale up fabric is extremely important for GPU Clusters, as it provides an extremely fast path for tensor and expert parallelism used in frontier model training. For this reason, we have conducted benchmarks to measure scale up fabric performance. The scale up fabric on H100 and H200 is called NVLink and provides 450GByte/s of bandwidth per GPU and connects 8 GPUs together. On the MI300X, the scale up fabric is called xGMI and, on paper, it connects 8 GPUs, providing 448GByte/s of bandwidth per GPU. On the surface, MI300X’s scale up network is extremely similar and close in performance to that of the H100/H200, providing just 0.5% less on paper bandwidth. Unfortunately, the reality of the situation differs sharply. First, MI300X’s xGMI is a point-to-point fabric, which means that it isn’t actually providing 448GByte/s of bandwidth between GPUs pairs. Instead, each GPU can only talk to one another at 64GByte/s. A GPU can only reach the stated 448GByte/s if one GPU addresses all 7 other GPUs simultaneously. That means that, for Tensor Parallelism TP=2, the maximum bandwidth is 64GByte/s and 189GByte/s for TP=4. Source: SemiAnalysis In contrast, since Nvidia’s NVLink uses a switched topography, one GPU can talk to another GPU at the full 450GByte/s. Furthermore, the four NVSwitches in H100/H200 support in-network reduction (referred to as NVLink SHARP (NVLS), enabled by default), a technique to reduce data movements by carrying out collectives/reductions inside the switch itself. Source: SemiAnalysis All Reduce/All to All/Reduce Scatter/All Gather Collectives Overview We will showcase benchmarks across scale-up and scale-out networks for both the Nvidia H100/H200 and AMD’s MI300. The collectives that we will be testing are the main set of collectives used in frontier LLM training: all_reduce, all_gather, reduce_scatter, and all to all. All reduce is for data parallelism and tensor parallelism, all gather is used for ZeRO/FSDP parallelism (as well as for tensor parallelism), and Reduce Scatter is used for ZeRO/FSDP parallelism. Due to the way that compute-communication overlapping works, real-world message sizes range from 16MiB to 256MiB, with the default PyTorch DDP size being 25MiB (NVIDIA’s MLPerf 11,000 H100 GPT-3 175B run used a message size of max 200MiB). We also test 8GiB and 16GiB just to see what the peak bus bandwidth is, though these message sizes are not used in the real world. All these collectives discussed above are used during 3D Parallelism and FSDP/ZeRO Parallelism, which are common techniques for training frontier models. Source: DeepSpeed Source: Meta Single Node NCCL Collective We see that Nvidia does much better than AMD across all the real-world messages for every single collective. This is not surprising due to the H100/H200’s superior 450GByte/s NVLink switched topology with in-network reduction (NVLS), compared to MI300X’s 7x64GByte/s xGMI point-to-point topology. Source: SemiAnalysis Source: SemiAnalysis Source: SemiAnalysis Source: SemiAnalysis To reproduce this test, you can use our open source ClusterMax-NCCL/RCCL benchmark, which we developed to be easily run with one line of Bash. ClusterMax is our upcoming evaluation quantitative performance and qualitative user experience for ranking H100/B200/GB200/MI300X Neocloud clusters. Look forward to our upcoming “ClusterMax Neocloud EvaluationHow to Rent GPUs” article. Source: SemiAnalysis Multi Node RCCL/NCCL Collectives and Scale Out Network Benchmarks On both Nvidia’s H100/H200 and the MI300X, each GPU is connected to other nodes over the scale out network using a 400G Network Interface Card (NIC), connected directly every GPU. The H100/H200 reference design typically uses ConnectX-7 NICs for InfiniBand NDR or BlueField-3 for Spectrum-X Ethernet. Spectrum-X is NVIDIA’s custom Ethernet solution purpose-built for AI workloads. On the MI300X, the reference design recommends using RoCEv2 Ethernet with Broadcom Thor-2 NIC. Source: Nvidia A typical GPU cluster almost always requires more layers than a single tier network, as a single-tier network can only support 128 GPUs (in the case of Broadcom Ethernet or Nvidia Spectrum X Ethernet) and 64 GPUs (for H100/H200 InfiniBand). In such a multi-tier network, deployments typically use an 8-rail optimized fat tree, where each one of the 8 GPU is connected to a separate switch (such a connection is called a “rail”). In our AI Neocloud Playbook and Anatomy article, we explained in detail how a rail optimized network works. Source: SemiAnalysis Just as Nvidia’s NVLink offers NVLS for its scale-up network, Nvidia’s H100/H200 InfiniBand scale out network also offers InfiniBand SHARP In-network Reduction which is, again, exclusive to Nvidia. AMD does not have an analogous product for the MI300X. InfiniBand SHARP works similarly to NVLink SHARP In-network Reduction as they both provide a way to reduce the amount of traffic going through the network, with the reductions carried out inside of Quantum-2 InfiniBand switches in the case of InfiniBand SHARP. Unfortunately, unlike NVLink SHARP, which is enabled by default, InfiniBand SHARP is not enabled by default in the UFM/IB subnet manager. We have spoken to many Neoclouds, H100 cluster operators, and AI frontier labs, and most have said that they have not enabled SHARP due to increased NCCL_TIMEOUT rates and difficulties installing and configuring the network. We asked NVIDIA which AI customers use InfiniBand SHARP, but they declined to answer in specifics. One could speculate that if InfiniBand SHARP was useful in AI production workloads, NVIDIA marketing would shout at the top of their lungs to promote its successful deployment. Given the apparently limited adoption of InfiniBand SHARP for now, we show here collective performance for Nvidia both when SHARP is and is not enabled. For some of the benchmarks, we have also collected Nvidia Spectrum-X Ethernet data on an Nvidia internal cluster called Israel-1. Nvidia Spectrum-X is used in xAI’s 200k H100/H200 cluster and can support clusters up to 100k GPUs in the Spectrum-X reference architecture version 1.2, but could potentially support up to 512k GPUs with a non-reference custom design. We are also in the process of testing Google Cloud (GCP) H100’s in-house ethernet, as well as AWS’ H100 and H200s that are deployed on AWS’s in-house Ethernet (called EFAv2/EFAv3). We will be sharing the results in our upcoming “Collective Deep Dive” article, which will provide visualizations of the different types of collectives, explain the different NCCL protocols (SIMPLE, LL, LL128), different NCCL algorithms (NVLS, NVLSTREE, RING, TREE, COLNETDIRECT, COLNETCHAIN, PAT), and how collectives run on GCP H100 Ethernet, AWS H100/H200 EFA, InfiniBand H100, Spectrum-X, etc. Below we show a 32 GPU all reduce collective test. You can see that MI300X RoCEv2 is in last place compared to normal InfiniBand H100 and InfiniBand H100 with SHARP enabled. Simply put, poor all reduce performance leads to poor scale-out training. Source: SemiAnalysis The MI300X’s performance decreases if you scale out (i.e. increase) the number of GPUs participating in a collective. As you can imagine, modern frontier training is carried out on clusters of at least 100,000 GPUs. MI300X RoCEv2 runs at half the speed for all the real-world message sizes of 16MiB to 256MiB when compared to the baseline of InfiniBand Non-SHARP. As per the chart below, Nvidia Spectrum-X Ethernet performance is quite close to InfiniBand Non-SHARP’s performance, due to Spectrum-X’s vertical integration with the NCCL collective library as well as its use of good congestion control and adaptive routing. AMD is attempting to vertically integrate next year with their upcoming Pollara 400G NIC, which supports Ultra Ethernet, hopefully making AMD competitive with Nvidia. As always, Nvidia is not standing still and by late next year, it will be ready to go into production with its 800G ConnectX-8 NICs, which provide a line rate twice as fast as AMD’s Pollara NIC. AMD RCCL is a fork of Nvidia NCCL. AMD’s RCCL Team and many other teams at AMD are resource limited and don’t have enough of either compute or headcount to improve the AMD ecosystem. AMD’s RCCL Team currently has stable access to less than 32 MI300Xs for R&D, which is ironic, as improving collective operations is all about having access to many GPUs. This is frankly silly, AMD should spend more on their software teams having access to more GPUs. This contrasts with Nvidia’s NCCL team, which has access to R&D resources on Nvidia’s 11,000 H100 internal EOS cluster. Furthermore, Nvidia has Sylvain Jeaugey, who is the subject matter expert on collective communication. There are a lot of other world class collective experts working at Nvidia as well, and, unfortunately, AMD has largely failed to attract collective library talent due to less attractive compensation and resources – as opposed to engineers at Nvidia, where it is not uncommon to see engineers make greater than a million dollars per year thanks to appreciation in the value of RSUs. To help alleviate these issues, TensorWave and SemiAnalysis are currently working with the AMD RCCL Team to improve collective performance. TensorWave has generously sponsored AMD a medium-sized cluster in order help the RCCL Team have greater resources to do their jobs. The fact that Tensorwave after buying many GPUs has to give AMD GPUs for them to fix their software is insane. Another trend to notice is that for non-SHARP networks, all reduce collective’s speed will reduce logarithmically as you double the number of GPUs. In contrast, with SHARP, the speed/completion time stays the same. We have results for up to 1,024 H100s showing that IB SHARP all reduce is constant time across any number of GPUs in a collective. We will publish this in our upcoming “Collective Deep Dive” article. Source: SemiAnalysis For all gather, all to all, and reduce scatter collectives, MI300X is anywhere from 2-4 times slower than InfiniBand. Unfortunately, we did not have access to Spectrum-X or InfiniBand SHARP benchmark data for all gather or reduce scatter. Source: SemiAnalysis Source: SemiAnalysis Source: SemiAnalysis Below, we provide our nccl/rccl benchmarking script. Unfortunately, due to the nature of cluster-specific setups, it is not as simple as a one-liner. It does require you to follow the README.md of nccl/rccl and nccl-tests/rccl-tests to run properly. On AWS and Google Cloud, there may also be custom nccl adapters that you will need to install. Source: SemiAnalysis AMD’s User Experience is Suboptimal and the MI300X is Not Usable Out of the Box Due to poor internal testing (i.e. “dogfooding”) and a lack of automated testing on AMD’s part, the MI300 is not usable out of the box and requires considerable amounts of work and tuning. In November 2024 at AMD’s “Advancing AI”, AMD’s SVP of AI stated that are over 200k tests running every evening internally at AMD. However, this seems to have done little to ameliorate the many AMD software bugs we ran into, and we doubt AMD is doing proper CI/CD tests include proper performance regression, or functional and convergence/numerics testing. We will outline a few examples here for readers to understand the nature of the AMD software bugs we have encountered and why we feel they have been very obstructive to a good user experience on AMD. Although AMD’s own documentation recommends using PyTorch native Flash Attention, for a couple months this summer, AMD’s PyTorch native Flash Attention kernel ran at less than 20 TFLOP/s, meaning that a modern CPU would have calculated the attention backwards layer faster than an MI300X GPU. For a time, basically all Transformer/GPT model training using PyTorch on the MI300X ran at a turtle’s pace. Nobody at AMD noticed this until a bug report was filed following deep PyTorch/Perfetto profiling showing the backwards pass (purple/brown kernels) took up far more time than the forward pass (dark green section). Normally, the backwards section should take up just ~2x as much time as the forward pass (slightly more if using activation checkpointing). Source: SemiAnalysis Another issue we encountered was that the AMD PyTorch attention layer led to a hard error when used with torch.compile due to the rank of the longsumexp Tensor being incorrect. What was frustrating is that this had already been fixed in internal builds of AMD PyTorch on May 30th, but did not reach any AMD PyTorch distributions or even any PyTorch nightly builds until October when it was pointed out to them that there was a bug. This demonstrates a lack of testing and dogfooding on the packages AMD puts out to the public. Another core reason for this problem is that the lead maintainer of PyTorch (Meta) does not currently use MI300X internally for production LLM training, leading to code paths not used internally at Meta being buggy and not dogfooded properly. We believe AMD should partner with Meta to get their internal LLM training working on MI300X. Source: SemiAnalysis On August 8th, Horace He and the Meta PyTorch Team released FlexAttention, a critical API for creating non-causal attention layers without losing speed. To previously use attention variants like document masking, sliding window attention, softcap, and Alibi, a user would need to spend weeks handcrafting their own kernel in CUDA/HIP language, and subsequently pybinding it to PyTorch. However, with FlexAttention, a user can quickly generate all the attention variants using the API. FlexAttention achieves great performance by using block sparsity by only calculating the blocks of the mask where needed, ignoring the rest. Source: SemiAnalysis Source: Meta With sliding window attention, FlexAttention can improve performance by 10-20x! This is amazing for the end user, but unfortunately, MI300X FlexAttention was in a poor state and suffers from numerous AMD software bugs (including convergence issues) until but a couple days ago. While the latest PyTorch nightly now fixes for convergence issues, this contrasts starkly with FlexAttention on Nvidia, which has been available since August. That means a ~6 month gap exists between the availability of these fantastic Pytorch features on Nvidia and AMD’s platforms. For frontier AI labs, six months is a lifetime, with OpenAI, Anthropic, and Google having released numerous models in such a span. Source: SemiAnalysis Exploring Ideas for Better Performance on AMD AMD recommended we try PYTORCH_ TUNABLE_OPS to improve GEMM performance by sweeping through GEMM algorithms at runtime. However, as we mentioned earlier, this API works poorly because GEMMs should be tuned when compiling the hipBLASLt/RoCBLAS/cuBLASLt and not during the users’ runtime. Users of Nvidia H100s do not need to use PYTORCH_ TUNABLE_OPS for most shapes because cuBLAS heuristic model will pick the correct algorithmn. This contrasts with AMD’s heuristic model, which never seems to pick the correct algorithm for most shapes. We recommend that AMD stop suggesting that users try tunable ops and instead focus on properly tuning their GEMM libraries internally. When we tried PYTORCH_ TUNABLE_OPS on AMD, it led to an HBM memory leak of over 25 GByte out of the total MI300X capacity of 192GBytes, essentially wiping out the MI300’s HBM capacity advantage over the H100. The fix for this is to set a default hipBLASLt and rocBLAS workspace to prevent memory leaks. Source: PyTorch/AMD As we mentioned earlier in this article, another issue we ran into was that there was a plethora of environment flags needed on MI300X to make it actually usable. We recommend to AMD that they stop putting users in the position of having to set these environment flags themselves and, instead, set default flags that lead to a usable environment. It is not simply their number, but also the complex interactions between the flags, making troubleshooting difficult. Getting reasonable training performance out of AMD MI300X is an NP-Hard problem. Another issue is that certain AMD ROCm libraries could not be installed inside Docker due to AMD software CMake bugs leading to hard errors. This has since been fixed. On AMD GPUs, you need to pass in a convoluted set of flags to get the GPUs to be able to work inside a container, whereas with docker, getting GPUs to work is as simple as passing in “—gpus=all”. We recommend to AMD that they partner with Docker and ensure that Docker can autodetect GPUs for AMD as well, making the workflow as streamlined as when working with Nvidia GPUs. Source: SemiAnalysis AMD’s Forked Libraries Many of AMD’s libraries are forked off Nvidia’s open-source or ecosystem libraries. AMD uses a tool called Hipify to carry out source-to-source translation of Nvidia CUDA to AMD HIP. While the motivation is understandable, they arenevertheless building on top of their competitor’s platform and cannot expect to match or surpass Nvidia’s user experience with this software development strategy. They need to contribute their software to the AMD ecosystem. For example, instead of supporting FP8 training by forking Nvidia/TransformerEngine and source-to-source translation, they should attempt PyTorch native FP8 training to work well on their own hardware. Currently, AMD PyTorch native FP8 training recipes don’t work on AMD and the unit tests don’t even pass yet, there is no CI/CD for AMD PyTorch native FP8 training. Source: SemiAnalysis Detailed Recommendations to AMD on How to Fix Their Software First, AMD needs to focus on attracting more software engineering resources and improving compensation for current engineers. The current compensation gap between AMD and Nvidia means that top talent is lured to Nvidia over AMD. This top talent is also attracted to Nvidia as it has far more compute/resources for engineers. AMD should procure more GPUs for their in-house development work and submit an MLPerf GPT3 175B result as soon as possible. Even if the result is not competitive with Nvidia right now, submitting such a benchmark will kick off the process for iterative improvement. We also notice that AMD frequently gives their customers custom images, and, in fact, AMD developers themselves often work on top of such bespoke images. This is not best practice, as this means that AMD engineers have a different experience vs. images available to the public. AMD should instead lift the standard of public images by using these images internally and with its customers, and the AMD executive team should personally internally test (i.e. “dogfood”) what is getting shipped publicly. We recommend that AMD create a public dashboard that runs every night, showing the performance of their hardware on benchmarks such as MLPerf or TorchBench. This dashboard should also include H100/H200 performance as a baseline. Finally, AMD needs to completely transform its approach to environmental flags. Instead of setting a myriad of flags to get running out of the box, it should set them to recommended defaults so users can get started quickly. AMD should collaborate with Meta to get production training workloads working on ROCm, as it is well-known amongst PyTorch users that PyTorch code paths tend to have tons of bugs unless Meta uses it internally. Meta currently hand writes HIP Kernels for their production MI300X inferencing but does not use MI300X for real training. It would be a fantastic improvement for the AMD ecosystem, and a marketing victory, if a smaller version of the next Llama is trained on AMD. Not to mention that this would open the door to AMD progressively moving towards larger models/clusters with Meta. Meta using AMD GPUs for actual model training would be a win-win for both companies as Meta is also looking for alternative training chips to Nvidia. Currently Nvidia offers well over 1,000 GPUs for Continuous improvement and development of Pytorch externally and many more internally. AMD doesn’t. AMD needs to work with an AMD focused GPU Neocloud to have ~10,000 GPUs of each generation for internal development purposes and Pytorch. This will still be 1/8th that of Nvidia with their coming huge Blackwell clusters, but it’s a start. These can be dedicated to internal development and CICD for Pytorch. Lisa, we are open to a meeting on how to fix AMD’s Datacenter GPU User Experience for the better! H100/H200/MI300X Networking BoM Analysis and Performance per TCO In addition to our benchmarking of collectives and GEMM throughput, we have conducted several experiments exploring insightful topics for conducting further benchmarks and running real-world workloads on clusters. These experiments cover benchmarking warmup and repeat effects, VBoost Power Shifting, MLPerf Training GPT-3, BF16 vs FP16 throughput, throughput by GEMM input distribution, power per FLOP, and throughput for the PyTorch PyPi distribution vs Nvidia NGC Stable PyTorch images. We also present a detailed networking bill of materials (BoM) analysis for the 1k GPU Ethernet, 1k GPU InfiniBand, 16k GPU Ethernet, and 16k GPU InfiniBand clusters. We also discuss the impact of using 51.2T Radix vs. 25.6T Radix switches for back-end networking. Lastly – we present a performance per TCO analysis that shows how the H100/H200/MI300X stacks up in terms of $/hr per effective training petaflop. These items are available below to all SemiAnalysis subscribers and will be of great interest to datacenter operators, ML scientists, and investors. Subscribe for full access to this article With a SemiAnalysis subscription you get full access to all articles, Data Explorer graphs, article discussions, and further insight into deep dives. Log in or Subscribe By subscribing, you agree to the Privacy Policy and Terms and Conditions. Previous Article Scaling Laws – O1 Pro Architecture, Reasoning Training Infrastructure, Orion and Claude 3.5 Opus “Failures” AI Lab Synthetic Data Infrastructure, Inference Tokenomics of Test Time Compute, The Data Wall, Evaluations are Broken, RLAIF, Inference Time Search, Scale Needed More Than Ever Keep Reading AMD MI300 Performance – Faster Than H100, But How Much?MI400 Broadcom + AMD Anti-Nvidia Alliance Coming With UEC and Open XGMI Microsoft Infrastructure – AI & CPU Custom Silicon Maia 100, Athena, Cobalt 100Specifications, Volumes, GPT-4 performance, Next Generation Timing / Name, Backend Design Partner Inference Race To The Bottom – Make It Up On Volume?Mixtral Inference Costs on H100, MI300X, H200, A100, Speculative Decoding Comments dantrump@gmail.com December 22, 2024 Sign up for a paid plan and join the discussion With a SemiAnalysis subscription you get full access to all articles, Data Explorer graphs, article discussions, and further insight into deep dives. Log in or Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=42489844",
    "commentBody": "CUDA Moat Still Alive (semianalysis.com)189 points by pella 20 hours agohidepastfavorite145 comments nrp 19 hours ago> Give AMD Engineers more compute and engineering resources to fix and improve the AMD ecosystem, they have very few internal gpu boxes relative to what Nvidia provides to their engineers. This is real. We’ve found ourselves having to give hardware to engineers at AMD because they’re unable to get allocation of it internally. reply theptip 1 hour agoparentThis is baffling. I’m sure there are many technical reasons I don’t grok that AMD’s job is challenging, but it’s wild that they are dropping the ball on such obvious stuff as this. The prize is trillions of dollars, and they can print hundreds of millions if they can convince the market that they are closing the gap. It’s embarrassing that whoever actually tries to use their product hits these crass bugs (same with geohot who was really invested in making AMD’s cards work; I think he just ran their demo script in a loop and produced crashes). It seems they really don’t understand/value the developer flywheel. reply mring33621 1 hour agorootparentIn the late 90's, US manufacturers, including high-tech electronics, had 2 mantras: 1) Cash is king 2) Inventory is evil I think this mindset may still be here, in 2024 reply AlotOfReading 17 hours agoparentprevSadly common at hardware companies. The most extreme case I've heard of is ASML, who supposedly doesn't keep any machines of their own. They test against \"almost-ready\" machines right before they go out the door to customers. reply Flenkno 5 hours agorootparentYou are comparing a machine the size of a container with GPUs? its nice to be aware of this but this is so fastly different from a critisism point of view that i don't think that matters. reply AlotOfReading 4 hours agorootparentIt's not a criticism, it's an extreme example from a company people know that I have no particular NDA restrictions with. reply llamaimperative 1 hour agorootparentprevI found it interesting ¯\\_(ツ)_/¯ reply whazor 10 hours agorootparentprevActually, they shipped not-ready machines to customers. In hope they could find solutions that can fix the machine later. reply dmoy 17 hours agorootparentprevASML might be an extreme outlier though, don't those things cost like $50 million+ each? reply tonetegeatinst 16 hours agorootparentMany for last gen process nodes, and from a second or third hand supplier if you could even find one. ASML makes very few fully working machines each year, and the cost and throughput those machines have is astronomical. They have spare parts you'd bet, and I'd bet they have some SLA agreement with each customer where an engineer is basically on call nearby in case a single thing dosnt work or a random part breaks or needs servicing. Asianometry did a great video on the cost of downtime when it comes to ASML device in any fab. While I am not directly in this field and can't speak to the accuracy of the numbers john gives, he does not seem one to just make stuff up as his quality of video production for niche topics is quite good. reply bobmcnamara 14 hours agorootparentAlmost a decade ago KFAB had a fire, power was cut, everything in process was dumped, they planned to restart but ended up being was cheaper to close the whole facility . Probably for the best though, KFAB had been discharging several tons of solvents, cleaning agents, and reagents per year into the surrounding area [for as long as it ran](https://enviro.epa.gov/facts/tri/ef-facilities/#/Release/640...) https://enviro.epa.gov/facts/tri/ef-facilities/#/Release/640... reply adgjlsfhk1 15 hours agorootparentprevtry 400 mil reply amirhirsch 16 hours agorootparentprevPut a 1 or 2 in front of that reply rahkiin 10 hours agorootparentprevThat’s why they cooperate closely with imec and their FAB in Leuven reply Archit3ch 18 hours agoparentprevComing up next: \"We bought AMD stock on the open market and used it to compensate AMD engineers\". reply Lerc 17 hours agorootparentYou joke, but it is almost a genuine investment opportunity here for a large player. Spend a billion on AMD shares, Spend another Billion on a out-of-house software team to solve the software solution to more than double the share price. Taking into account that there are players that already own billions in AMD shares, they could probably do that as well. On the other hand perhaps it would be better for them, as major shareholders, to have a word with AMD management. reply schmidtleonard 15 hours agorootparentI don't have the inside baseball but I have seen those weird as hell interviews with Lisa Su where she gets asked point blank about the software problems and instead of \"working on it, stay tuned\" -- an answer that costs nothing to give -- she deflects into \"performance is what matters,\" which is the kind of denial that rhymes exactly with the problems they are having. No, the horsepower of your F1 racecar doesn't matter if the engine doesn't start and there's a wheel missing! You need to fix those problems before the horsepower can matter! Please tell me you are fixing the starter and the wheel! Hopefully I am reading too much into this. Hopefully she doesn't have any weird hangups over investing in software and it all just takes time to Do It Right after GPGPU got starved in the AMD winter. But if it is a weird hangup then yeah, 100%, ownership needs to get management in line because whiffing a matmul benchmark years into a world where matmul is worth trillions just ain't it. reply sangnoir 15 hours agorootparent> she deflects into \"performance is what matters,\" which is the kind of denial that rhymes exactly with the problems they are having. It's not a deflection, but a straightforward description of AMDs current top-down market strategy of partnering with big players instead of doubling down to have a great OOBE for consumers & others who don't order GPUs by the pallet. It's an honest reflection if their current core competencies, and the opportunity presented by Nvidia's margins. They are going for a bang-for-buck right now aiming at data center workloads, and the hyperscalers care a lot about perf/$ than raw performance at. Hyperscalers are also more self-sufficient at software: they have entire teams working on PyTorch, Jax, and writing kernels. reply Twirrim 13 hours agorootparentEngineers at hyperscalers are struggling through all the bugs too. It's coming at notable opportunity cost for them, at a time when they also want an end to the monopoly. Do they buy AMD and wade through bug after bug, regression after regression, or do they shell out slightly more money for Nvidia GPUs and have it \"just work\". AMD has to get on top of their software quality issues if they're ever going to succeed in this segment, or they need to be producing chips so much faster than Nvidia that it's worth the extra time investment and pain. reply sangnoir 8 hours agorootparent> Engineers at hyperscalers are struggling through all the bugs too [citation needed] reply almostgotcaught 5 hours agorootparentLolol 100% accurate. Go trawl through PRs to Triton by FB people to the AMD portion of the codebase. reply Twirrim 5 hours agorootparentprevSorry, but NDAs mean I can't say any more. reply schmidtleonard 14 hours agorootparentprevThat's the excuse used by every big company shitting out software so broken that it needs intensive professional babysitting. I've been on both sides of this shitshow, I've even said those lines before! But I've also been in the trenches making the broken shit work and I know that it's fundamentally an excuse. There's a reason why people pay 80% margin to Nvidia and there's a reason why AMD is worth less than the rounding error when people call NVDA a 3 trillion dollar company. It's not because people can't read a spec sheet, it's because people want their expensive engineers training models not changing diapers on incontinent equipment. I hope AMD pulls through but denial is _not_ the move. reply sangnoir 14 hours agorootparentWhat exactly are they in denial about? They are aware that software is not a strength of theirs, so they partner with those who are great at it. Would you say AMD is \"shitting the bed\" by not building it's own consoles too? You know AMD could build a kick-ass console since they are doing the heavy-lifting for the Playstation, and the XBox[1] , but AMD knows as much as anybody that they don't have the skills to wrangle studio relationships or figure out which games to finance. Instead, they lean hard in their HW skills and get Sony Entertainment/the Xbox division do what they do best. 1.and the Steam Deck, plus half a dozen Deck clones. reply roenxi 12 hours agorootparentThere is probably one employee - either a direct report of Su's or maybe one of her grandchildren in the org chart - who needs to \"get it\". If they replaced that one manager with someone who sees graphics cards as a tool to accelerate linear algebra then AMD would be participating more effectively in a multi-trillion dollar market. They are so breathtakingly close to the minimum standards of competence on this one. We know from the specs that the cards they produce should be able to perform. This is a case-specific example of failure, it doesn't generalise very well to other markets. AMD is really well positioned for this very specific opportunity of historic proportions and the only thing holding them back is a somewhat continuous stream of unforced failures when writing a high quality compute driver. It seems to be pretty close to one single team of people holding the company back although organisational issues tend to stem from a level or two higher than the team. This could be the most visible case of value destruction by a public company we'll see in our lifetimes. Optimistically speaking maybe they've already found and sacked the individual responsible and we're just waiting for improvement. I'm buying Nvidia until that proves to be so. reply pjmlp 8 hours agorootparentprevIt would go nowhere, games history is full of great hardware that died because they failed to have a profitable ecosystem. Even Steam Deck is only a success, because it depends on Windows ecosystem, and the moment Microsoft decides it is enough, lets see how long it holds. reply Comma2976 7 hours agorootparentSteam Decks run on Arch Linux reply pjmlp 6 hours agorootparentAs means to avoid paying for Windows licenses. All the games that matter are Windows games running via Proton, as Valve has failed to actually build a GNU/Linux native games ecosystem, in spite of UNIX/POSIX underpinnings of Android NDK, PlayStation, the studios hardly bother. The day Microsoft actually decides to challenge Proton, or do a netbooks move on handhelds with XBox OS/Windows, the SteamDeck will lose, just like the netboooks did. Additionally, it is anyone's guess what will happen to Valve when Gabe steps down. reply Comma2976 34 minutes agorootparentAs a means of control reply nemothekid 13 hours agorootparentprev>Hyperscalers are also more self-sufficient at software: they have entire teams working on PyTorch, Jax, and writing kernels. None of this matters because AMD drivers are broken. No one is asking AMD to write a PyTorch backend. The idea that AMD will have twice the silicon performance than nvidia to make up the performance loss for bad software is a pipedream. reply sangnoir 12 hours agorootparent> None of this matters because AMD drivers are broken Do you honestly think the MI300 has show-stopper driver bugs, or that Meta/Amazon doesn't have a direct line to AMD engineers? reply david-gpu 7 hours agorootparentprev> None of this matters because AMD drivers are broken How do you know that the problems arise from broken drivers rather than broken hardware? Real world GPU drivers are full of workarounds for hardware bugs. reply wjnc 3 hours agorootparentprevI enjoy this train of thought a lot. Capturing shareholder value by just creating it yourself. The destruction of a moat I fear is worth less than the existence of a moat a competitor has successfully built. So don’t forget to buying puts Nvidia. reply claytonjy 17 hours agoparentprevI was surprised to hear recently that the same happens at NVIDIA! Hopefully less frequently, but I can understand why it's hard to keep many units on hand given the level of external demand. reply mountainriver 18 hours agoparentprevThis seems so insane, is anyone actually doing the work to provide an alternative to CUDA? Maybe Google? reply ryao 16 hours agorootparenthttps://www.intel.com/content/www/us/en/developer/articles/t... reply pjmlp 8 hours agorootparentprevThe problem is that everyone that tries, keeps missing the CUDA forest and focus only in a specific kind of tree. reply anon291 14 hours agorootparentprevHonestly, probably NVIDIA itself, since they contribute significantly to many open-source projects (MLIR), and also make their SoTA GEMM/Conv implementations open-source and available for study (Cutlass). reply david-gpu 7 hours agorootparent*> also make their SoTA GEMM/Conv implementations open-source and available for study (Cutlass)\" Cutlass is a fine piece of engineering, but it is not quite as good as their closed source libraries in real world workloads. There is secret sauce that is not open sourced. reply ryao 15 hours agoprev> AMD is attempting to vertically integrate next year with their upcoming Pollara 400G NIC, which supports Ultra Ethernet, hopefully making AMD competitive with Nvidia. Infiniband is an industry standard. It is weird to see the industry invent yet another standard to do effectively the same thing just because Nvidia is using it. This “Nvidia does things this way so let’s do it differently” mentality is hurting AMD: * Nvidia has a unified architecture so let’s split ours into RDNA and CDNA. * Nvidia has a unified driver, so let’s make a different driver for every platform. * Nvidia made a virtual ISA (PTX) for backward compatibility. Let’s avoid that. * Nvidia is implementing tensor cores. Let’s avoid those on RDNA. Then implement them on CDNA and call them matrix cores. * Nvidia is using Infiniband like the rest of the HPC community. Let’s use Ethernet. I am sure people can find more examples. Also, they seem to have realized their mistake in splitting their architecture into RDNA and CDNA, since they are introducing UDNA in the future to unify them like Nvidia does. reply dralley 15 hours agoparentYou're painting this like AMD is off to play in their own sandbox when it's more like the entire industry is trying to develop an alternative to Infiniband. Ultra Ethernet is a joint project between dozens of companies organized under the Linux Foundation. https://www.phoronix.com/news/Ultra-Ethernet-Consortium >> The Linux Foundation has established the Ultra Ethernet Consortium \"UED\" as an industry-wide effort founded by AMD, Arista, Broadcom, Cisco, Eviden, HPE, Intel, Meta, and Microsoft for designing a new Ethernet-based communication stack architecture for high performance networking. You probably can't call it \"industry standard\" yet but the goal is obviously for it to become one. reply ryao 12 hours agorootparentI wrote: > It is weird to see the industry invent yet another standard to do effectively the same thing just because Nvidia is using it. This is a misstep for all involved, AMD included. Even if AMD is following everyone else by jumping off a bridge, AMD is still jumping too. reply mrlongroots 1 hour agoparentprev> Infiniband is an industry standard Infiniband is not an industry standard lol. Maybe it used to be, but it definitely is not anymore. Most Infiniband vendors are dead. The only product from those days that endures is Cornelis' Omnipath, and even that only emulated the Infiniband API back with its first gen, and then evolved to be its own thing. At this point, Infiniband is as good as a proprietary interconnect only sold by Nvidia/Mellanox. reply ryao 1 hour agorootparentInfiniband is very much an industry standard: https://www.infinibandta.org/member-listing/ As far as I know, everyone is free to sign up with the infiniband trade association and implement the specification. Furthermore, if you use RDMA over Ethernet, you are using infiniband at a low level. RoCE which enables it was originally called Ethernet over infiniband. It is maintained by the infiniband trade association. Omnipath was Intel’s failed effort to try to kill an open standard. It purchased QLogic’s infiniband business, killed it in favor of omnipath and sold it when it failed. reply mrlongroots 1 hour agorootparent> Furthermore, if you use RDMA over Ethernet, you are using infiniband at a low level. RoCE which enables it was originally called Ethernet over infiniband. This is wrong. RDMA over Ethernet is... RDMA over Ethernet. There is no Infiniband involved. RoCE was motivated by supporting RDMA, which was then an IB-only feature, over regular Ethernet. The user-level APIs are the same (verbs), but the underlying architecture is all different --- it is traditional Ethernet with link-level flow control to make it lossless (pause frames). reply ryao 41 minutes agorootparentThis is not what I have been told by others. reply mixblast 6 hours agoparentprevInfiniband is a monopoly from NVidia (Mellanox). Everyone else would much rather use Ethernet which is the actual industry standard. reply ryao 5 hours agorootparentOthers can build infiniband hardware according to the standard. There used to be at least two companies building infiniband hardware until Intel killed QLogic’s infiniband division in a misguided attempt to make its own monopoly. :/ reply mrlongroots 1 hour agorootparentOr they can develop a RoCEv2 that works (which is basically what UltraEthernet is). Infiniband is not fun - it's a special snowflake of an interconnect that sits parallel to the rest of your datacenter network, and can not really run a standard TCP/IP codebase (yeah IPoIB is a thing but still). Do the Nvidia boxes really need a scale-out IP network as well as an Infiniband network? Plus the spec is old. Packet spraying and trimming, better ordering guarantees, queue pair scalability... a whole bunch of enhancements have been incorporated into UE all the while being compatible with regular Ethernet. Qlogic was never really an Infiniband vendor --- their qib driver is still in the Linux codebase and essentially emulates verbs on top of a messaging-based design. reply iszomer 5 hours agorootparentprevDepends on deployment: Ethernet has more drawbacks encapsulating PCIe packet traffic than Infiniband does, or lack thereof with RDMA. reply Flenkno 5 hours agoparentprevNvidia has a unique problem, wants to move fast and has a shit load of money. No need for Nvidia to go first to an industry standard and neither for AMD. Personally would be great its getting backported but its so far away from an normal use case. reply ryao 2 hours agorootparentNvidia is the one who went to an industry standard before their competitors in this space. It was created in 1999 and is called infiniband: https://en.wikipedia.org/wiki/InfiniBand Infiniband is extremely popular in the HPC space, which is why Nvidia adopted it. Everyone else saw Nvidia adopt it and said \"Let us make a new network standard to be incompatible\". This is mind boggling. Even more mind boggling is that many of the companies in the Ultra Ethernet Consortium are members of the Infiniband Trade Association, AMD included: https://www.infinibandta.org/member-listing/ This would be like the automotive industry forming a consortium to invent new incompatible wheels to exclude a successful upstart that adopted their existing standard wheel designs. With trillions of dollars in revenue on the line, you would think that companies would use existing networking standards to focus on building competitive hardware with reduced time to market, yet they are instead reinventing networking standards just because they can. This is a huge gift to Nvidia, since it means that everyone else is wasting time and money instead of being competitive. reply mrlongroots 1 hour agorootparentSorry to hound you for the third time, but this is wrong: > Infiniband is extremely popular in the HPC space Not anymore. There used to be Cray Aries/GNI, psm/psm2, and now there's Slingshot, the new Cornelis stuff etc. There's almost no Infiniband now. reply croes 11 hours agoparentprevYou hardly beat someone by copying him. They have way more experience in the field you try to catch up. reply ryao 5 hours agorootparentYou don’t beat someone by doing everything worse either. reply imtringued 6 hours agorootparentprevAMD doesn't need to beat Nvidia, they just need to match them at a lower price point. reply croes 7 minutes agorootparentThat‘s beating them at the price point. reply sokoloff 2 hours agorootparentprevIn business, that combination is nearly impossible to distinguish from beating them. reply Lerc 17 hours agoprevThat MatMul performance is fairly shocking. To be that much below theoretical maximum on what should be a fairly low overhead operation. I would at least hope that they know where the speed is going, but the issue of torch.matmul and F.Linear using different libraries with different performance suggests that they don't even know which code they are running, let alone where the slow bits in that code are. reply kevingadd 17 hours agoparentLow overhead in what sense? matmul is kinda complicated and there are varying, complex state-of-the-art algorithms for it, no? And then if you know things about the matrices in advance you can start optimizing for that, which adds another layer of complexity. reply ryao 12 hours agorootparentThere are, but everyone uses variations of the same O(n^3) algorithm taught in college introduction to linear algebra classes because it is numerically stable and can be made extremely fast through tweaks that give spatial locality and good cache characteristics. Meanwhile the asymptomatically faster algorithms have such large constants in their big O notation that they are not worth using. FFT based matrix multiplication, which is O((n^2)log(n)), also has numerical instability on top of running slower. reply petters 11 hours agorootparent> matrix multiplication, which is O((n^2)log(n)) Isn't the fastest theoretical algorithm something like O(n^2.37) ? reply jey 11 hours agorootparentYes, but it's impractical unless you have galactic-scale matrices to multiply (at least). reply eapriv 10 hours agorootparentprev> FFT based matrix multiplication, which is O((n^2)log(n)) What? reply ryao 4 hours agorootparenthttps://en.wikipedia.org/wiki/Schönhage–Strassen_algorithm I forgot the log(log(n)) factor. In any case, for matrix multiplications that people actually do, this algorithm runs slower than a well optimized O(n^3) matrix multiplication implementation because the constant factor in the Big O notation is orders of magnitude larger. reply jhanschoo 9 hours agorootparentprevFFT is fast Fourier transform, and our best theoretical bounds on multiplication come from methods involving FFT. reply adrian_b 9 hours agorootparentprevLow overhead in the sense that matrix multiplication is almost the only algorithm that is able to reach computational throughput values very close to the theoretical maximum for a given hardware. Good CPUs and GPUs have a throughput in Flop/s for matrix multiplication that is between 60% and 90% of the maximum possible throughput, with many (especially the CPUs) reaching values towards the high end of that range. As shown in the article, the AMD GPUs attain only slightly less than 50% (for BF16; for FP8 the AMD efficiency is even less than 40%). Such a low efficiency for the most important operation is not acceptable. reply Lerc 15 hours agorootparentprevBy overhead I'm talking about the things that have to be done supplementary to the algorithm. While there are complex state-of-the-art algorithms, those algorithms exist for everyone. The overhead is the bit that had to be done to make the algorithm work. For instance for sorting a list of strings the algorithm might be quick sort. The overhead would be in the efficiency of your string compare. For matmul I'm not sure what your overhead is beyond moving memory, multiplying, and adding. A platform touting a memory bandwidth and raw compute advantage should have that covered. Where is the performance being lost? I guess the only real options are stalls, unnecessary copies, or unnecessary computations. reply dhruvdh 14 hours agorootparentWhich algorithm you pick for what shape of matrices is different and not straightforward to figure out. AMD currently wants you to “tune” ops and likely search for the right algorithm for your shapes while Nvidia has accurate heuristics for picking the right algorithm. reply 0-_-0 6 hours agorootparentNvidia's heuristics are not accurate, and it's not possible to achieve peak performance without search. reply anon291 14 hours agorootparentprev> For matmul I'm not sure what your overhead is beyond moving memory, multiplying, and adding. A platform touting a memory bandwidth and raw compute advantage should have that covered. Where is the performance being lost? The use of the word 'algorithm' is incorrect. Look... I do this sort of work for a living. There has been no useful significant change to matmul algorithms. What has changed is the matmul process. Modern perf optimization on GPUs has little to do with algorithms and everything to do with process optimization. This is akin to factory floor planning and such. You have to make sure the data is there when the processing units need it, and the data is coming in at the fastest rate possible, while keeping everything synchronized to avoid wrong results or deadlocks. Really compute power has nothing to do with it. It's a waste of time to even consider it. We can compute matmuls much faster than you can naively bring memory to the processing units. Whoever solves that problem will become very rich. To that end, NVIDIA ships libraries that will choose from a wide variety of implementations the appropriate trade-offs necessary for SoTA perf on matmuls of all shapes and data types. reply ryao 12 hours agorootparentTo be fair, GEMV is memory bandwidth bound and that is what token generation in transformers uses. GEMM is the compute bound one, provided you do not shoehorn GEMV into it. That special case is memory bandwidth bound. reply jiggawatts 15 hours agorootparentprevYes and no. Conceptually it's just three nested loops. The fiddly part is unrolling the inner loop and swizzling the data layouts in such a way that the cores can be kept \"fed\" efficiently. This usually means breaking things up into cache-sized chunks along some axis. It's easy enough that there's blog articles showing single developers getting within spitting distance of NVIDIA's highly optimised code. As in, 80-something-percent of the best available algorithms! All NVIDIA did was \"put the effort in\", where the effort isn't some super clever algorithm implemented by a unique genius, but they simply made hundreds of variants of the matmul algorithm optimised for various scenarios. It's a kind of algorithmic brute force for eking out every last percentage point for every shape and size of input matrices on every GPU model and even for various SLI configurations. From what I've seen, AMD has done... none of this. reply Fronzie 10 hours agorootparent> From what I've seen, AMD has done... none of this. There are a number of pull-requests to ROCMblas for tuning various sizes of GEMV and GEMM operations. For example: https://github.com/ROCm/rocBLAS/pull/1532 reply jiggawatts 10 hours agorootparentMerged two days ago!? That’s about half a decade after they should have done this foundational work! I guess it’s better late than never, but in this case a timely implementation was worth about a trillion dollars… maybe two. reply ryao 2 hours agorootparentThere are likely plenty of unrealized opportunities to improve mature BLAS libraries. For example, this guy who was able to outperform OpenBLAS' GEMM on Zen 4: https://salykova.github.io/matmul-cpu Concidentally, the Intel MKL also outperforms OpenBLAS, so there being room for improvement is well known. That said, I have a GEMV implementation that outperforms both the Intel MKL and OpenBLAS in my tests on Zen 3: https://github.com/ryao/llama3.c/blob/master/run.c#L429 That is unless you shoehorn GEMV into the Intel MKL's batched GEMM function, which then outperforms it when there is locality. Of course, when there is no locality, my code runs faster. I suspect if/when this reaches the established amd64 BLAS implementations' authors, they will adopt my trick to get their non-batched GEMV implementations to run fast too. In particular, I am calculating the dot products for 8 rows in parallel followed by 8 parallel horizontal additions. I have not seen the 8 parallel horizontal addition technique mentioned anywhere, so I might be the first to have done it. reply cavisne 11 hours agorootparentprevHow do the cache sizes compare between AMD GPU’s and nvidia? I remember reading a while ago they were quite different (enough to make flash attention painful to implement) reply imtringued 6 hours agorootparentprevMatmul is trivial to get right, especially since you won't be calculating dot products manually to begin with. You're going to use the tensor cores or equivalent, which already perform almost the entire matrix multiplication for you. Your primary goal in developing a custom matmul kernel is in adjusting the algorithm to the specific hardware by knowing how many tiles you can store in your local registers and SRAM and how to simultaneously intertwine loading new data from HBM and performing the calculations. reply the_king 19 hours agoprev> It’s not just that it’s immature software, they need to change how they do development. I remember geohot saying something similar about a year ago reply roenxi 18 hours agoparentI expect everyone has been saying it for a while, the calls are just getting more strident and public as it becomes clear that AMD's failures are strategic rather than tactical. And as people try to build business on their half-hearted attempts. I still think it is a mistake to say that CUDA is a moat. IMO the problem here is that AMD still doesn't seem to think that GPGPU compute is a thing. They don't seem to understand the idea that someone might want to use their graphics cards to multiply matricies independently of a graphics pipeline. All the features CUDA supports are irrelevant compared to the fact that AMD can't handle GEMM performantly out of the box. In my experience it just can't do it, back in the day my attempts to multiply matrices would crash drivers. That isn't a moat, but it certainly is something spectacular. If they could manage an engineering process that delivered good GEMM performance then the other stuff can probably get handled. But without it there really is a question of what these cards are for. reply ryao 16 hours agorootparentI wonder to what extent vulkan compute could be used for this. Of course, it is only an option on their RDNA GPUs since CDNA is not for graphics, even though that is the G in GPU. reply pjmlp 8 hours agorootparentUnless it provides the polyglot capabilities of CUDA, and related IDE and graphical debugging capabilities, not really. reply Fronzie 10 hours agorootparentprevThere has been some testing within llama.cpp, which supports both Vulkan and ROCM-Blas. When it works, the latter is about 2x faster than the Vulkan version. reply schmidtleonard 16 hours agorootparentprevYeah, 80% margins on matrix multiplication should be a puddle not a moat but AMD is more scared of water than the witch that melts in Wizard of Oz so I guess the puddle is a moat after all. reply dogma1138 18 hours agoparentprevAnyone who looks at the mess that is ROCm and the design choices they made could easily see that. GPU support lagged behind for years, no support for APUs and no guaranteed forward compatibility were clear signs that as a whole they have no idea what they are doing when it comes to building and shipping a software ecosystem. To that you can add the long history of both AMD and ATI before they merged releasing dog shit software and then dropping support for it. On the other hand you can take any CUDA binary even one that dates back to the original Tesla and run it on any modern NVIDIA GPU. reply ryao 16 hours agorootparent> GPU support lagged behind for years, no support for APUs and no guaranteed forward compatibility were clear signs that as a whole they have no idea what they are doing when it comes to building and shipping a software ecosystem. This is likely self inflicted. They decided to make two different architectures. One is CDNA for HPC and the other is RDNA for graphics. They are reportedly going to rectify this with UDNA in the future. However, that is what they really should have done from the start. Nvidia builds 1 architecture with different chips based on it to accommodate everything and code written for one easily works on another as it is the same architecture. This is before even considering that they have PTX to be an intermediate language that serves a similar purpose to Java byte code in allowing write once, run anywhere. reply dogma1138 15 hours agorootparentThis was happening before CDNA was even a thing. They didn’t release support even for all GPUs from the same generation and dropped support for GPUs sometime within 6 months of releasing a version that actually “worked”. The entire core architecture behind ROCM is rotten. P.S. NVIDIA usually has multiple CUDA feature levels even within a generation. The difference is that a) they always provide a fallback option, and usually this doesn’t require any manual intervention and b) is that as long as you define the minimum target framework when you build the binary you are guaranteed to run on all past hardware that is supported by the feature level you targeted and on all future hardware. reply ryao 4 hours agorootparentThe differences between CUDA feature levels appear minor according to the PTX documentation: https://docs.nvidia.com/cuda/parallel-thread-execution/index... They also appear to be cululative. reply almostgotcaught 16 hours agorootparentprev> On the other hand you can take any CUDA binary even one that dates back to the original Tesla and run it on any modern NVIDIA GPU This particular difference stems the fact that NVIDIA has PTX and AMD does not have any such thing. Ie this kind of backwards compatibility will never be possible on AMD. reply dogma1138 15 hours agorootparentBackward compatibility is one thing but not having a forward compatibility is a killer. Having to create a binary that targets a very specific set of hardware and having no guarantees and in fact having a guarantee that it won’t on future hardware is what make ROCM unusable for anything you intend to ship. What’s worse is that they also drop support for their GPUs faster than Leo drops support for his girlfriends once they reach 25… So not only that you have to recompile there is no guarantee that your code would work with future versions of ROCM or that future versions of ROCM could still produce binaries which are compatible with your older hardware. Like how is this not the first design goal to address when you are building a CUDA competitor I don’t fucking know. reply almostgotcaught 15 hours agorootparent> Like how is this not the first design goal to address when you are building a CUDA competitor I don’t fucking know. The words \"tech debt\" do not have any meaning at AMD. No one understands why this is a problem. reply pjmlp 8 hours agorootparentprevBackwards compatibility, and polyglot ecosystem, thanks to the amount of compiler toolchains that support PTX. reply nabla9 18 hours agoparentprevIn buggy numerical code many bugs go trough the software stack without any problems. No crash, no errors. For example,you might switch two double parameters to a function and if their value range is similar, everything works fine except it's all bullshit. If there are bugs in AMD code that prevent running tests, I bet there are even more bugs that don't manifest until you look at results. reply delusional 19 hours agoparentprev\"The software needs to be better\" is (and was) an easy call to make for anyone paying attention. The problem is that \"AMD just needs to do better\" is not and will never be an implementable strategy. Engineering isn't just about money. It's also about the process of exploring all the edge cases. \"We recommend that AMD to fix their GEMM libraries’ heuristic model such that it picks the correct algorithm out of the box instead of wasting the end user’s time doing tuning on their end.\" Is such a profoundly unhelpful thing to say unless you imagine AMDs engineers just sitting around wondering what to do all day. AMD needs to make their drivers better, and they have. Shit just takes time. reply the_king 18 hours agorootparentSounds more like they were (and still are) being sloppy. “be better” is one thing. “runs without fatal crash” is what semi is talking about. reply 9cb14c1ec0 17 hours agoprevI once tried installing AMD ROCM to run a small llm on a consumer-grade AMD GPU. It was the most horrible software install experience I ever had. Never did manage to get it working. reply cavisne 11 hours agoprevI think it’s hardware not software. “Cuda moat” is a misnomer. The PTX spec is relatively short (600 page pdf). Triton directly writes PTX, skipping cuda. Flash attention was created by a non nvidia employee without access to any of the secret sauce within Cuda or its libraries. The hardware is just not as good, and no software can paper over its flaws. reply pjmlp 8 hours agoparentNVidia also refactored their hardware design to follow C++ memory model, I think this still isn't the case of others. reply benreesman 14 hours agoprevAMD could spend their market cap in one year to get this done in three and it would be a coup for the shareholders. They could hire all of the best NVIDIA engineers at double their current comp, crush the next TSMC node on Apple levels, and just do it and if it got them a quarter of NVDA’s cap it would be a bargain. They don’t fucking want to! Believing this is anything like a market is fucking religion. reply atq2119 11 hours agoparentYou make it sound like that's a sure thing, but I doubt it. A lot of this is about processes, team structures and incentives, all those fuzzy things between the people. Remember, most acquisitions fail. For the same reason, the likelihood of failure with your scenario seems high. Do you really think nobody at AMD is aware of all the points made in this thread? That seems too bizarre to be true. There are probably some issues in upper management which could perhaps be fixed with some targeted hiring decisions, but do you really believe some random person on here would have a chance making that call? reply benreesman 10 hours agorootparentWe were arguing about this two years ago, maybe five. I was sharing NVIDIA dev boxes with other hackers doing CUDA in 2016. There’s this meme that it can’t change on a dime and I believe that. You could build this from scratch in a decade. JFK sent NASA to the moon in less time for comparable money. If NVIDIA shareholders can’t come close? What fucking good are they? Why do our carrier battle groups guard their supply chain? reply benreesman 10 hours agorootparentEveryone is so concerned about “losing” the “AI” “race” to the PRC. I say ship them Altman and an exaflop and watch their society corrupt itself at a fractal nature at machine speed. Good fucking riddance. I see your fentanyl crisis: raise you Sam and a failure to ship GPT-5. Have fun with that. reply robocat 13 hours agoparentprevTry to make sense... They can spend their market cap by either: 1: issuing new shares worth their market cap, diluting existing shareholders to 50%. 2: Or borrow their market cap and pay interest by decreasing profits. \"AMD operating margin for the quarter ending September 30, 2024 was 5.64%\" so profits would be extremely impacted by interest repayments. Either way your suggestion would be unlikely to be supported by shareholders. > crush the next TSMC node on Apple levels I would guess Apple is indirectly paying for the hardware (to avoid repatriating profits) or guaranteeing usage to get to the front of the line at TSMC. Good luck AMD competing with Apple: there's a reason AMD sold GlobalFoundries and there's a reason Intel is now struggling with their foundry costs. And it comes across as condescending to assume you know better than a successful company. reply benreesman 13 hours agorootparentWhen 4 trillion dollars are at stake, the financing is available or it fucking better be. What in God’s name do we pay these structured finance, bond-issue assholes 15% of GDP for if not to finance a sure thing like that? It sure as hell ain’t for their taste in Charvet and Hermes ties, because the ones they pick look like shit. reply klelatti 10 hours agoprevAMD doesn’t just have to fix these issues it has to build up a record of fixing issues like those discussed here. Otherwise who will bet their firm / cash / career on new hardware without a successful track record. reply nromiun 14 hours agoprevSo basically Nvidia is like Windows and AMD is like Wine. I think trying to emulate CUDA and using forked Nvidia libraries is not the best strategy for AMD. They should have made a clean break and come out with a fresh API, like Apple's Metal. reply stefan_ 19 hours agoprevI made the mistake of clicking on one of the links to commits they mentioned only to end up at a MR changing multiple autogenerated yaml files with 10k line diffs and incomprehensible names. I guess this is where the whole \"bad talent\" thing comes in - a year later and you are thousands of YAML files deep but still no one can run a simple PyTorch compile ops and get the performance you sold, absolutely unhinged. reply nabla9 19 hours agoprevB100, B200 ramp-up is only 4-6 months away. reply shrubble 16 hours agoprevBack in the late 1990s I met the ATI guys, and they were slipshod then as well. That the ATI legacy of special-casing things lives on is sadly, not too surprising for me. reply MortyWaves 10 hours agoprevWow they really botched the title. Why wasn’t it “Still Filled With Water”? reply gigel82 19 hours agoprevWhat I couldn't find is inference benchmarks for consumer hardware. Just pick a reasonable workload with llama.cpp or ollama and show us some numbers. I'm particularly interested in building a Home Assistant machine that can run the voice assistant locally (STT/TTS/LLM) while using the least amount of power / generating the least amount of heat and noise. reply curt15 19 hours agoparentAMD's software for consumer GPUs demonstrates a lack of seriousness. ROCm only officially supports RDNA2 and RDNA3 GPUs (their last two generations of hardware), and for some reason most of them are supported on only Windows (https://rocm.docs.amd.com/projects/install-on-windows/en/lat...) and not Linux (https://rocm.docs.amd.com/projects/install-on-linux/en/lates...), where most AI training and inference occurs. In particular, Linux users can only start playing with ROCm with a top-of-the-line, power-guzzling unit whereas they can get started with CUDA using basically any Nvidia GPU on desktops or laptops. reply cherryteastain 19 hours agorootparentIn practice, consumer Navi 21 based cards (RX 6900XT etc) and Navi 31 cards (RX 7900 XTX etc) are compatible with Pytorch on Linux. What they write about ROCm and Windows is equivocation. They target only one app: Blender. Pytorch+ROCm+Windows does not work. I had bought a 6900XT myself around launch time (the RTX3080 I ordered was not coming, it was the chip shortage times...) and it took around 2 years for Pytorch to become actually usable on it. reply thousand_nights 16 hours agorootparentin practice everyone who wants to do ML at home buys nvidia and pays the premium reply tonetegeatinst 16 hours agorootparentSad but true. Years ago, pre2018 nvidia was the goto hardware supplier if you were doing anything with neural networks. I remember CUDA being much more buggy back then but it still worked pretty good. Back then AMD wasn't considered a real competition for ML/AI hardware. Glad as always to see more competition in the market to drive innovations. AMD seems to be letting larger VRAM onto consumer cards, which is nice to see, just hope the AI/ML experience can get better for their software ecosystem. reply ryao 16 hours agorootparentprevThe obligatory link: https://xkcd.com/644/ That said, I would not expect it to stay working for long as long as ROCm is a dependency since AMD drops support for its older GPUs quickly while Nvidia continues to support older GPUs with less frequent legacy driver updates. reply cherryteastain 19 hours agoparentprevBased on a machine we had bought at my university with 4 AMD W6800s (which are just RX 6800s with double the VRAM), it's bad _even if it works at all_. reply sroussey 19 hours agoparentprevYou might just check out the Home Assistant Voice: https://ameridroid.com/products/home-assistant-voice-preview... reply gigel82 18 hours agorootparentYes, that's exactly what I was checking out. You need fast enough hardware to run the speech to text, text to speech and (most importantly) LLM locally: https://www.youtube.com/watch?v=XvbVePuP7NY (he has dual 3090 GPUs but that's not a practical setup for most people - budget / power / noise). reply GaggiX 19 hours agoparentprevIt would be cool to see these benchmarks on the newly released Jetson Orin Nano Super, like faster-whisper. reply anon291 14 hours agoprevMy anecdata on AMD hiring: they just aren't moving fast enough. They still wanted to fly people out scheduling 3 weeks in advance for AI compiler work. That's just not going to work. Startups and companies like NVIDIA, OpenAI are hiring much faster with much less onerous interview processes, with higher compensation. This is not a mystery. People work for money and aren't going to hop through more hoops to be paid less. reply dhruvdh 14 hours agoprevDisappointed that there wasn’t anything on inference performance in the article at all. That’s what the major customers have announced they use it for. reply cyberax 18 hours agoprev> CUDA Moat Still Alive Wrong conclusion. AMD is slower than NVidia, but not _that_ much slower. They are actually pretty cost-competitive. The just need to do some improvements, and they'll be a very viable competitor. reply SideQuark 18 hours agoparentThe amount of effort this team took, literally co-opting AMD engineers, and working for 5 months, to get closer but not yet usable, means they are not even close to usable. What team wanting to do ML training/inference can afford so much down time for zero benefit? How many except a few big ones can get AMD to devote so many resources simply for that team? And, if you’re training a model costing you millions, the last thing you need is a buggy, untested stack, breaking training or perhaps worse giving you noise that makes your models perform worse or increases training time. By the time AMD gets usable out of the box at this point, NVidia will have moved further ahead. reply cyberax 17 hours agorootparentSure. But this work is done, and can be reused by others. Meanwhile, Nvidia hardware is expensive and still is in short supply. AMD might look quite tempting. reply SideQuark 17 hours agorootparentIt sure doesn’t sound done. It’s a one off hacked set of scripts tied to incompatible chunks of a ton of libraries. What happens when you want or need other parts of the PyTorch/billion libs ecosystem? You’re gonna get more AMD engineers and waste 5 months getting those to work? Meanwhile those libs release running CUDA on NVidia’s old and newest releases out of the box. So no, it cannot be reused by others in production any more than my custom hacked car engine mod can be added by Ford to every car in existence. Have you done any deep professional production work on any of these stacks? I have, and would never, ever put stuff like the stuff in the article in production. It’s no where near ready for production use. reply ryao 15 hours agorootparentprevThere is a difference between doing something just for yourself and making it usable by others. reply m3kw9 14 hours agorootparentprevLike the article say, if the model change a little this work need to be almost thrown out reply YetAnotherNick 12 hours agoparentprevAll the cloud providers list MI300x as more expensive than H100. So if you compare performance/cost it is even worse. reply m3kw9 14 hours agoparentprevJust like that? So a little work and now they are competitive. You know how much work “just a little bit of work” is doing? They us a cultural issue and it will take months to fix if they are lucky and then you start tackling the tech debt they’ve built up. By that time it will be another generation reply dboreham 19 hours agoprevSounds like a buy signal for AMD. If you run the right branch and set the right env cars, the thing flies. reply rikafurude21 18 hours agoparentWould be a a buy signal if their actions (better drivers) show that they are seriously working on improving software. \"This could be great _if_ you go through the trouble of doing it right!\" is not persuasive, and any sane person would go with green if they know they have the choice between troubleshooting shitty software and things just working. Look at the george hotz archive youtube channel and watch the videos where he's debugging the amd drivers, it damn near ruins the man. And george is not the type to give up at the first roadblock, there are multiple 5-8 hour videos where he just tries to get the thing to work. The mad man ended up just writing his own driver lol. reply ip26 16 hours agorootparentIt does seem like an improvement. Six or twelve months ago, I recall a lot of crashes and even more basic problems. “If you tune it right, it’s awesome” is a big step forward compared to that. reply latchkey 17 hours agoparentprevAnthony has been doing training on 4 of our MI300x systems and has been getting great results... but of course, he is a genius, and writing his own code... https://x.com/HotAisle/status/1870984996171006035 https://x.com/zealandic1/status/1869857713280430349 https://x.com/zealandic1/status/1868810042168033623 reply cherryteastain 19 hours agoparentprevUnfortunately, > Getting reasonable training performance out of AMD MI300X is an NP-Hard problem. reply ryao 15 hours agoparentprevI expect Nvidia shares to increase tomorrow because of the article while AMD shares are not likely to do well. It is odd how we read the same thing and came to opposite conclusions. reply incrudible 17 hours agoparentprev1. AMD always had a lot of hype already priced in, it is no different with AI. 2. AMD has always shipped a bad software stack, it is no different with AI. reply yaro330 10 hours agoprevTL;DR: AMD still doesn't take software seriously? reply dzdt 18 hours agoprev [8 more] [flagged] redox99 16 hours agoparentPeople call out Gelsinger all the time. And yes, she is definitely responsible for this. Probably more than Gelsinger. At Intel it is not so obvious what they should have done to improve their fabs to better compete with TSMC, which is groundbreaking tech where you often have to make risky bets. At AMD it was pretty obvious what had to be done to better compete in AI, and it was basic software engineering, not lithography wizardry. Totally achievable by just spending money, increasing head count, hiring top talent, and firing underperformers. They have so much low hanging fruit that could have been solved just by hiring 5 or 10 software engineers. reply nrp 18 hours agoparentprevIn this case it’s because Dylan Patel of Semianalysis interviews Lisa Su regularly and presumably has a direct line to her, and because Lisa and the rest of AMD leadership are absolutely reading the article. It’s unclear if Pat would have (e.g. I don’t think Pat ever sat down for a chat with Semianalysis like Lisa has). reply Implicated 14 hours agoparentprev> Is it a kind of misogynism? -.- At what point did any of the criticism have anything to do with her gender? Honest question, I'm scratching my head trying to see where misogyny comes into play. Surely it's not that _because_ she's a woman any criticism from men must be misogynistic? Would it be different if Intel's CEO was female? Or do the people criticising need to be of the same gender as those they're criticising in order for there to be no misogyny? Truly just trying to get an idea of what sort of perspective it takes to get to > Is it a kind of misogynism? reply hedgehog 17 hours agoparentprevIt might be because Lisa has been so outstandingly effective at making AMD competitive across multiple product lines against bigger competitors and this feels to some people like an oversight that AMD could easily solve. I suspect the current situation with ML software at AMD is a consequence of a very focused company and not an easy fix without sacrificing something else. I don't think many people can keep track of who's running Intel let alone have hope that with a little work they can deliver reasonable substitutes for NVIDIA's products. reply kaliqt 15 hours agoparentprevThis is Lisa Su's fault. reply bloodyplonker22 17 hours agoparentprevThis is the exact type of victim mentality that we don't need. There are absolute insane amounts of people calling out Gelsinger by name and blaming solely him for failures at Intel. reply anon291 14 hours agoparentprev [–] Lisa Su is an exemplary CEO, and widely recognized as such. She is exemplary for doing what she did with AMD, and did it without appealing at all to her sex... just on sheer competence. I think it's a bit presumptuous to suddenly call out her sex as if it matters. In reality, she's being talked about exactly like any male CEO. I have great faith in her though. She is clearly extraordinarily capable, and honestly a real inspiration to women in tech reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Benchmarking of the MI300X, H100, and H200 GPUs shows that AMD's MI300X underperforms due to software issues, despite its promising specifications.",
      "Nvidia's GPUs demonstrate superior performance out-of-the-box, whereas AMD's MI300X requires significant tuning and bug fixes to reach its potential.",
      "Recommendations for AMD include enhancing software quality, increasing internal testing, and collaborating with partners to improve its competitive position against Nvidia."
    ],
    "commentSummary": [
      "AMD faces challenges in software development, affecting its competitiveness against Nvidia, particularly in AI and machine learning sectors.- Despite having strong hardware, AMD's issues with driver support and backward compatibility limit its performance, contrasting with Nvidia's successful CUDA ecosystem.- Critics suggest that AMD should enhance its software engineering efforts to improve its position in the AI market and compete more effectively with Nvidia."
    ],
    "points": 190,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1734907629
  },
  {
    "id": 42490191,
    "title": "Classified fighter jet specs leaked on War Thunder forums",
    "originLink": "https://ukdefencejournal.org.uk/classified-fighter-jet-specs-leaked-on-war-thunder-again/",
    "originBody": "Home Air Classified fighter jet specs leaked on War Thunder – again Air Classified fighter jet specs leaked on War Thunder – again By George Allison - December 22, 2024 15 Share Facebook Twitter Pinterest WhatsApp Email EXCLUSIVE – The War Thunder forums are at the centre of yet another controversy following the leak of classified documents related to the Eurofighter Typhoon’s radar systems. The discussion, which revolved around the scanning capabilities of the CAPTOR radar, led to a user posting restricted material in an attempt to prove their argument. War Thunder, a free-to-play online combat game developed by Gaijin Entertainment, was launched in 2012, with development and operations now spread across Europe, including offices in Germany, Hungary, and Cyprus, and a team of 200 employees. While the material was swiftly removed and the user suspended, this latest incident has reignited concerns about the repeated sharing of sensitive information on the platform. A forum community manager addressed the situation, reminding users of the risks and responsibilities involved: “I will take this opportunity to again remind everyone here, please do not, under any circumstances, try to post, share any sources unless you are 100% certain they are legally declassified and publicly safe for use. We will never handle or use them, and all it does is actively harm any possible future changes being possible by trying to use them. Do not do it. No good will ever come from it for you or the vehicle you are trying to post for.” The user was warned before sharing the documents, but it appears they proceeded regardless, resulting in the immediate removal of the material. The Italian Ministry of Defence, whose documents may have been cited, has previously stated that manuals like these are excluded from public access for both security and commercial reasons. A Worrying Pattern This is not the first time War Thunder forums have faced issues with classified leaks. Previous incidents have involved technical details of the Challenger 2 tank, Leclerc main battle tank, and Chinese ammunition systems. Each case has underscored the platform’s struggle to balance enthusiasm for military accuracy with the protection of sensitive information. Classified specs leaked on War Thunder forum for third time Radar Controversy The CAPTOR radar was at the heart of the debate. Discussions centred on comparisons between its mechanically scanning (CAPTOR-M) and electronically scanning (CAPTOR-E) variants, particularly the latter’s ability to reduce scanning times significantly. Players disagreed over the exact technical capabilities, prompting the ill-advised sharing of restricted data. Moderators have clarified that any claims based on classified information will not be entertained and have reminded users that no in-game content will be adjusted based on unauthorised sources. Larger Implications The repeated leaks from these forums pose broader concerns, particularly regarding the unauthorised dissemination of restricted information in public spaces. Defence analysts warn that such actions can carry serious legal consequences and even impact the operational security of military platforms. A contact in the defence industry, obviously wishing to remain anonymous, told me last night, “These leaks might seem harmless to some, but they can have real-world consequences. Sharing restricted information, even in a gaming context, risks undermining the security of platforms and could lead to serious legal repercussions for those involved and potential harm to operational effectiveness and safety”. At the UK Defence Journal, we aim to deliver accurate and timely news on defence matters. We rely on the support of readers like you to maintain our independence and high-quality journalism. Please consider making a one-off donation to help us continue our work. Click here to donate. Thank you for your support! 💷 Buy us a coffee? Share Facebook Twitter Pinterest WhatsApp Email George Allison George has a degree in Cyber Security from Glasgow Caledonian University and has a keen interest in naval and cyber security matters and has appeared on national radio and television to discuss current events. George is on Twitter at @geoallison RELATED ARTICLESMORE FROM AUTHOR Air Italy places order for up to 24 more Typhoon jets Air RAF reflects on key achievements of 2024 Air American bombers ‘demonstrate power’ of NATO alliance Air Over 10,000 British personnel deployed overseas on Christmas Air Glasgow’s QEUH hospital helipad upgraded with new name Air American warship shoots down own fighter jet over Red Sea Air NATO and Russian forces investigate North Pole build-up Air Spain orders 25 additional Eurofighter jets Air RAF Lossiemouth hosts Canadian aircrew for training Air UK announces £225m military aid package for Ukraine 15 COMMENTS Supportive Bloke December 22, 2024 At 08:46 Unbelievable that someone security cleared was so detached from reality that they posted something this sensitive. Reply Sailorboy December 22, 2024 At 12:57 Apparently this time it was a document that was actually available on Google if you looked hard enough. Some of the previous ones, however… I imagine some very clever people get so frustrated at claims they know are incorrect being made online that they make stupid decisions. Reply Lonpfrb December 22, 2024 At 14:12 So the document security breach was not directly part of this inappropriate upload. Gaijin Entertainment doesn’t sound like a European enterprise rather CCP though I don’t have that detail. Lets hope the Italian Ministry of Defence can get control of document security as classification and document protection are different things. Reply Crabfat December 22, 2024 At 21:39 Gaijin Entertainment is, apparently, a Hungarian company. Gaijin is a Japanese word. Reply OrcaLoddy December 23, 2024 At 17:49 THis company is russian Joe16 December 23, 2024 At 09:03 Indeed, the need to be proven correct is powerful… Reply Shithead December 23, 2024 At 05:30 War is fucking stupid and video games hurt nobody. The leaker has the correct mindset. Reply Lord Baddlesmere December 22, 2024 At 09:45 The obvious question is why has the user not been identified, tried and jailed?? Unless there is a consequence to illegal actions, they will proliferate and increase Reply Spock December 22, 2024 At 11:03 Forums like that should be required to block the uploading of documents (eg pdfs), the posting of images, or links to file-sharing sites. Reply Exclusive December 22, 2024 At 15:20 “Exclusive”??? You do realize your not the first to report this and by no means is this exclusive to ukdj. Literally an article from a week ago here https://www.sportskeeda.com/mmo/classified-military-documents-leaked-war-thunder-forums-yet-again Reply mosh December 23, 2024 At 05:46 Not the same Eurofighter leak Reply Watcherzero December 22, 2024 At 15:38 It wasn’t a classified document, was a NATO eyes only non-classified document from 2001 about the Eurofighter prototype testing thats apparently been floating around the internet for some time. Reply Viperzero December 23, 2024 At 13:41 What was the documents title? Reply Henry Joseph the third December 23, 2024 At 03:39 Never ask someone in WarThunder forum to proof of his words Attack the D point! Reply OrcalOddy December 23, 2024 At 17:50 FINALLY A WT PLAYER; FOR GODS SAKE ATTACK THE D POINT!!!! Reply LEAVE A REPLY Please enter your comment! Please enter your name here You have entered an incorrect email address! Please enter your email address here Save my name, email, and website in this browser for the next time I comment.",
    "commentLink": "https://news.ycombinator.com/item?id=42490191",
    "commentBody": "Classified fighter jet specs leaked on War Thunder forums (ukdefencejournal.org.uk)185 points by Trasmatta 19 hours agohidepastfavorite75 comments scrlk 16 hours agoList of Classified Document Leaks for War Thunder: https://steamcommunity.com/sharedfiles/filedetails/?id=29240... Apparently the latest incident was a repost of a document that was first leaked on the forum back in 2023 (see Eurofighter Typhoon). reply dralley 15 hours agoparentMost of the \"leaks\" are just documents that had been leaked elsewhere and circulated around the internet for years previously, that got reposted and reported without context. reply coldblues 3 hours agorootparentPretty much. The reason why these \"leaks\" get mentioned so much is because the idea of leaking classified documents on a video game forum is mildly amusing. The game is popular among people enrolled in the military, so it's not implausible that someone would leak documents for laughs and clout. reply DuckConference 12 hours agoprevIs this an actual leak or another case of an export-controlled document that's already circulating around the internet getting posted on their forums? Most of the war thunder \"classified leaks\" have just been that. reply Animats 12 hours agoparentThere's a huge amount of info available about the CAPTOR radar, its E-CAPTOR successor, and the common European radar successor. There are Wikipedia articles, promotional videos, marketing materials, and so forth. This video[1] gives enough info that a game dev could make up a basic simulator for a game. But that's just the basic mode. The thing has an large number of modes. Apparently it mostly manages them by itself, which is the clever part. It can act as a search radar, a targeting radar, a jammer, an RF weapon, a ground target mode, and even a bistatic mode, where one plane sends and another receives, so the attacker can get in close while not emitting. Now that's a really hard user interface problem. [1] https://www.youtube.com/watch?v=FpUhIwGjI7U reply quietbritishjim 11 hours agorootparent> Now that's a really hard user interface problem I'm not sure whether or not you meant this, but it would be a hard (/interesting) user interface problem in the planes themselves, never mind the game. Especially the bistatic mode. reply sandworm101 7 hours agorootparentprev>> and even a bistatic mode, where one plane sends and another receives, so the attacker can get in close while not emitting. That is one concept. The tactics of bistatic radar are far more complex, with many possible modes for many different situations and advantages. reply AdrianB1 6 hours agorootparentAnd in order to prove a point, one of you will post the link to the full schematics and manuals. This is exactly what the article describes. reply Manuel_D 17 hours agoprevAt this point, they ought to ask if you play War Thunder on the SF-86. reply sandworm101 7 hours agoparentNever ask a question of a friend when you know the answer will be a lie. reply bmcahren 16 hours agoparentprevIf I'm not mistaken it was a factor in recruiting in general as evidenced by another leak. reply paul-tharun 13 hours agorootparentThis originated from this reddit thread and has been proved as fake. https://www.reddit.com/r/Warthunder/comments/10j1hqr/congrat... source: the pinned comment on that thread reply ethbr1 17 hours agoprevThese seem like first person source, early-Wikipedia arguments. (Before citation guidelines came into full effect) I.e. How do you correct something that you know to be wrong, when you can't point to something public? Except, in the case of classified military technical specs -- just let it be wrong. reply pyrale 9 hours agoparent> I.e. How do you correct something that you know to be wrong, when you can't point to something public? Shake my head, and accept that it's enough hn/reddit for today. reply AdrianB1 6 hours agoparentprevYou don't have to correct everything all the time, especially if it is a sensitive matter. Correcting Covid information 2-3 years ago was a one way ticket to being cancelled everywhere, for example. One needs to accept that being right is not something to prove every single time. Remember Galileo. reply bagels 16 hours agoprevAgain? Intelligence people are sleeping on the job if they're not scraping those forums all day every day. reply whamlastxmas 16 hours agoparentI really doubt there’s anything meaningful coming out of these documents. It’s not going to change any literally a single country spends their military budget reply wutwutwat 16 hours agorootparentI don’t think anyone is concerned about budgets. Having intimate details into the systems of these craft and understanding how they operate in great detail makes it much easier to 1. Copy it, 2. Defend against it, 3. Find critical vulnerabilities in its design, 4. Build offensive systems that take advantage of any shortcomings, 5. Fast tracks their own jet fighter programs (which to your point does affect budgets because someone else had paid for the R&D) The same is true for any IP/competitive advantage. It’s funny how in our industry security through obsecurity is a thing we avoid, but other industries are literally built on the foundation of hiding information in order to stay ahead reply stavros 16 hours agorootparentWe avoid it because there are better ways, not because security through obscurity isn't security. Sometimes, obscurity is all you have. reply gpm 15 hours agorootparentAlso because so much of software is uniquely poorly suited to obscurity. Attackers will often be able to probe the \"obscure\" system at their leisure trying to reverse it, or in non-SAAS cases get their hands on the actual (compiled) algorithm itself. And once you figure it out every single copy works exactly the same. It's not like trying to measure the penetrating capabilities of a tank round, where you need physical access to a batch of large, expensive, explosive, tightly controlled objects to figure it out. reply brutal_chaos_ 15 hours agorootparentEverything is open source* if you know assembly, so to speak. Edit: *source available reply KennyBlanken 14 hours agorootparentprevAt least some classified stuff is well known to other country's intelligence services, friend, neutral, and foe. It can be so far out of the bag, it's had kittens. Lot of stuff is classified not because it would actually prevent other nations from finding out stuff, but to hide from the American people and press embarrassing things like how much money is being spent on that particular project or weapons system, how much of a failure it has been, how much toxic waste is being created in its manufacture, and so on. There's also all the stuff Internet Armchair Intelligence Officers think is \"sensitive\" or \"classified.\" reply giancarlostoro 14 hours agorootparentprevChina has access to all of our tech leaks, and yet, they cannot replicate it, neither can Russia, heck Russia has these really powerful jet planes... but they got less than a handful of them, and their GDP is comparable to the state of Florida's GDP. reply JumpCrisscross 14 hours agorootparent> yet, they cannot replicate it, neither can Russia, heck Russia has these really powerful jet planes Russia has powerful Soviet-era planes. Their new kit is demonstrably crap, being unable to establish even air supremacy against a foe wielding handfuls of decades-old air defence equipment. There is also a P != NP difference between replicating a war machine and reverse engineering it sufficiently to defeat it, e.g. designing a radar that mitigates its stealth. reply Roark66 7 hours agorootparentI can confirm, as someone who collects Soviet era electronics and buys replacement parts. Russian manufacturing is really bad now and very far from what it was during the Soviet era. Perhaps in part because of past cooperation between various countries of the communist block that is no longer there. For example Belarus and Poland made a lot of the integrated circuits(chips) back then. There are certain items, for example military radios like the R-140M almost every country had a local version of. For example the Russian version still used a vacuum tube as a delay timer well into the 80s while the Polish version had a box of digital logic. reply shadowerm 7 hours agorootparentprevI just really doubt this when it comes to China. Betting against China being able to build anything at this point seems pretty dumb. Almost delusional I would say. reply throwaway48476 10 hours agorootparentprevTo a large extent China has been able to replicate. reply kbolino 3 hours agorootparentChina has faced less strict sanctions and has been able to engage in much more aggressive academic and industrial espionage than post-Soviet Russia. It remains to be seen whether, now that sanctions have ramped up, and access to Western/Japanese research and expertise is being curtailed, they can maintain what they've already replicated, never mind build on it to make newer and better things. reply altairprime 11 hours agorootparentprevThere is one meaningful thing coming out of these documents; WarThunder has become a reliable form of education in “how to evaluate information distribution restrictions regarding militarily-applicable topics” for the generations that grew up without the events of PGP / ITAR fresh in mind. reply righthand 15 hours agoparentprevIt’s probably some other intelligence agency leaking it. reply GiorgioG 18 hours agoprevIf a forum has it, foreign agents already had it. reply LeifCarrotson 18 hours agoparentThe forum has it not because it's common knowledge but because people with top secret clearance happen to frequent the forum and like to talk about/boast about the tech they work with. An enemy state's ability to get an agent deep inside the military, or to entice someone already inside to turn traitor, or to hack through the layers of security to extract data over the network is largely disconnected from an insider's desire to chat about what they did at work. reply roenxi 17 hours agorootparentI'd expect a enemy state's ability to get information out of a system would in fact be correlated with insider's desires to chat about what they did at work. Find some bloke who is a bit lonely, a bit talkative and has good security access, send out a hot woman with an unexpected interest in military specifications. I'm no spymaster but that seems like something a spy agency could manage. Might be able to grab a stickynote with a few key passwords on it to sweeten the operation. reply LeifCarrotson 4 hours agorootparentA hot woman with an unexpected interest in military specifications is unexpected and therefore suspicious. Their interest in the protagonist will be equally unexpected. Some dudes on a forum about military games having an interest in the same? Totally normal. reply d883kd8 16 hours agorootparentprevEven better a woman who kinda looks like the target’s ex - a duper hot woman stands out and might activate the bloke’s memory of training. Spies whole thing is to blend in, appear unremarkable. reply bsder 16 hours agorootparentprevMaria Butina demonstrates that the woman doesn't need to even be particularly hot ... reply lmm 14 hours agorootparentprev> The forum has it not because it's common knowledge but because people with top secret clearance happen to frequent the forum and like to talk about/boast about the tech they work with. Not really. The forum has it because the low-level maintenance grunts are on the forums and like talking about the thing they work on, and don't think of the manual as some super-secret state secret because honestly it mostly isn't, and is classified out of habit rather than out of deep thought. > An enemy state's ability to get an agent deep inside the military, or to entice someone already inside to turn traitor, or to hack through the layers of security to extract data over the network is largely disconnected from an insider's desire to chat about what they did at work. It's hardly \"deep inside\", and an enemy state is surely capable of befriending some low-level military personnel, at which point they'll say much the same technically-classified-but-not-super-important things to their in-person friends as they do on internet fora. reply JumpCrisscross 14 hours agorootparent> don't think of the manual as some super-secret state secret The bright-red “classified” markings they include in their dumps suggest otherwise. It’s edgy because it’s demonstrably subversive. reply ExoticPearTree 5 hours agorootparentThe commenter said \"... they are classified out of habit\" and you omitted that in your reply. I think it is a fair point to consider. reply petesergeant 16 hours agorootparentprev> but because people with top secret clearance The kind of stuff being leaked on War Thunder forums is generally Restricted or Confidential at best reply labster 18 hours agoprevThe siren call of proving someone wrong on the internet strikes again. reply uncomplexity_ 11 hours agoparentwhat law is it again? where if you want the right answers to come out, spam people with wrong answers? lol reply touisteur 10 hours agorootparentCunningham's Law states \"the best way to get the right answer on the internet is not to ask a question; it's to post the wrong answer.\" From https://meta.m.wikimedia.org/wiki/Cunningham%27s_Law reply kitd 6 hours agorootparentI was really hoping you'd quote the wrong law. That would have been a sigma move. reply brookst 6 hours agorootparentprevThing is, this law way predates the internet. It’s a human nature thing. As a long time product manager, I’ve always found that the best way to get engineering engagement on a P1-to-me, P2-to-engineering issue is to propose my own solution. Someone is bound to solve the issue just to prove my solution wasn’t the right one. reply ls612 17 hours agoprevIt has been _0_ days since classified information was leaked on the War Thunder forums. reply codedokode 7 hours agoprevGlad to read that a russian game is well known around the world and regularly gets in the news. reply swozey 16 hours agoprevMy big takeaway from this is that War Thunder is only 200 employees. I know this is a massive income generating game so I'm googling around.. I found this post from Dec 2023 that has a mention of 121 million euros net revenue. > https://steamcommunity.com/app/236390/discussions/0/40347264... it's specifically referencing ONLY Hungarian income data for the company directly- https://www.ceginformacio.hu/cr9311454780_EN If you look at that today in 2024- https://www.ceginformacio.hu/cr9311454780_EN It's 137,097,280 euros a year today. $131 million revenue with 200 employees.. I've worked at 1200-2500 employee companies that were 10ths of that, if that. Gajin has companies all over the world, I'm assuming this is only their hungarian revenue. I don't know if its only their hungarian employees, I assume so.. edit: I just noticed the ceginfomacio.hu link shows 56 people, 1 owner reported 1/1/2023. So, 56 hungarian employees? reply mrguyorama 3 hours agoparentThe reason every game studio is trying to force their way into a \"Live service\" game that hits big is because they are basically unmatched when it comes to profit per unit effort. Whales will pay ANYTHING to buy EVERYTHING you offer, and everything you offer took literally an afternoon for your cheapest artist to throw together. Honestly Gaijin isn't even the worst offender in this regard. Sure the game is so goddamned grindy that you basically HAVE to spend money if you ever want to play with the fun toys at the top of the tech trees, but a new plane model based on a real machine definitely takes more implementation effort than a hat. reply ycombinatrix 17 hours agoprevThis is going to keep happening, isn't it? reply demarq 7 hours agoparentyes, yes it is. reply fnord77 17 hours agoprevI know this is serious, but this trope is kinda hilarious to me. Someone needs to prove themselves right so badly that they risk prison time by posting classified docs. reply nradov 16 hours agoparentDepending on where they're located and how they came into position of the classified file it isn't necessarily a criminal offense. reply zdragnar 11 hours agorootparentDepends entirely on what kind of connections you have. The government will happily waive rules to let you off the hook, or raid your dwelling early in the morning with guns drawn, then stage evidence if need be. reply chgs 9 hours agorootparentUS government only raids peoples homes (other than those in the US) for really serious crimes like copyright infringement reply sandworm101 6 hours agoprev20s on the difference between the captor-m and captor-e, with lasers. Remember to unmute. https://youtu.be/5l8LdM9_3PU reply rinzero 17 hours agoprevMy pet conspiracy theory about these leaks is that the military uses WarThunder as a training sim, and every now and then someone notices the physics/render is off so they \"leak\" the specs knowing some gaming nerd is going to fix it for them. reply Trasmatta 17 hours agoparentThe developers never make changes based on these leaks. reply mrguyorama 3 hours agorootparentThe devs cannot make the game match the reality, because that would be very very unfair for the people who play non-NATO countries. The game needs a semblance of balance. At least at one point, Russian tanks in the game had just free magic armor to protect the ammo carousel from cooking off from literally any hit. \"Spall\" and damage fragments would just magically not trigger the ammo cookoff, specifically on some Russian tanks. reply hypeatei 16 hours agoparentprevWhy would they use an arcade like War Thunder when DCS exists? reply Diti 16 hours agorootparentYeah, both Arma 3 and DCS World have commercial versions of their simulators for the armies to train on. reply stackghost 14 hours agorootparentI can't speak for Arma but DCS isn't even close to accurate enough to count as a qualified simulator. It's a nice toy, but still a toy. reply titaniumtown 14 hours agorootparentprevSource? I've never heard of either having such a version. reply nic547 9 hours agorootparentFor Arma it's VBS, but the games company (Bohemia Interactive) and the simulator company (Bohemia Interactive Simulations) have been seperate for quite some time, so VBS is based on older Arma titles. IIRC the PLA actually used modded Arma 3 for training. reply refulgentis 14 hours agoprevAI written article that's a poorly reused template of earlier story, applied to not-actually-classified information. And we're 44 comments in and most someone has noticed is observing it sounds like \"early Wikipedia\" We are so fucked reply brcmthrowaway 17 hours agoprevIsnt this tech 20 years old at this point? reply pixelesque 17 hours agoparentThe Captor-E AESA variant is not yet in service with the main countries developing it, despite the project starting in 1993. In fact, only export customers (Kuwait and Qatar) have the Captor-E ECRS Mk0 on their Typhoons so far, Germany and the UK (in particular) are holding out for future improvements, and are scheduled to get later versions, so they still have the mech-scanned PESA version as far as I've heard. reply killingtime74 17 hours agoparentprevI think according to Wikipedia only 10 years old. That's for the latest variant, not sure what was posted reply dieortin 17 hours agoparentprevThat is not old by military standards reply loxias 16 hours agoprevHonestly, this makes me curious to try playing the game. Good advertisement. :) reply siltcakes 18 hours agoprev [–] Military propaganda media (like Top Gun and Call of Duty) seems to be a double edged sword for the state, since the people it works the best on are also most susceptible to compromising secrets due to their deep seated insecurities. reply tejohnso 18 hours agoparent [–] > the people it works the best on are also most susceptible to compromising secrets due to their deep seated insecurities Can you elaborate on that? Who are the people it works best on? How are deep seated insecurities related to propensity to revealing secrets? Is this just an online thing or is this type of person more likely to reveal secrets in an intelligence operation scenario? How do you know all of this? reply siltcakes 17 hours agorootparent [–] It works best on young men who are socially, romantically and sometimes financially struggling. It gives them a sense of power and belonging to the state, something greater than themselves. The same mechanisms that make this propaganda successful can be used (willingly or organically) to cause the effected individual to take other actions. Here someone wanted to brag so they leaked secrets online. One would argue people like Timothy McVeigh were similar, with much different results. Further reading: - https://en.wikipedia.org/wiki/Edward_Bernays - https://archive.org/details/aberrationinhear0000pain/page/70... reply XorNot 17 hours agorootparent [–] Financial struggles are a contra-indication for receiving a security clearance however. reply siltcakes 17 hours agorootparent [–] That's why I said sometimes financial, because there are people who are well off or at least have it together financially that can still be exploited with propaganda. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Classified specifications of the Eurofighter Typhoon radar were leaked on the War Thunder forums, causing controversy and raising security concerns.",
      "This incident is part of a pattern, as previous leaks on the platform have involved other military systems like the Challenger 2 tank.",
      "The user responsible for the leak was suspended, and the material was promptly removed, underscoring the challenges War Thunder faces in balancing military accuracy with information security."
    ],
    "commentSummary": [
      "Classified fighter jet specifications were leaked on the War Thunder forums, highlighting the issue of information security in online communities.",
      "Discussions suggest these leaks are often reposts of already circulating documents, with the novelty being the platform used for the leak—a gaming forum.",
      "Despite the leaks, game developers do not alter the game based on them, indicating the leaks' limited impact."
    ],
    "points": 185,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1734911165
  },
  {
    "id": 42490948,
    "title": "New Google Sheet on half of 13.6\" MacBook Air screen is fully covered by popups",
    "originLink": "https://imgur.com/a/NQskWzI",
    "originBody": "sponsored sponsored sponsored",
    "commentLink": "https://news.ycombinator.com/item?id=42490948",
    "commentBody": "New Google Sheet on half of 13.6\" MacBook Air screen is fully covered by popups (imgur.com)171 points by kaonwarb 17 hours agohidepastfavorite80 comments wackget 13 hours agoYes, this is horrible UX design and there's absolutely no need for such inflated amounts of padding around everything. Huge companies are the absolute worst offenders for crap like this. Seriously, have you ever tried using Facebook to manage business pages? It is unbelievably poor. Google is by far the most hypocritical, however, because their search engine ranks pages lower for UX sins like \"cumulative layout shift\" while at the same time committing those sins themselves. (Having said all that, I would not personally expect an online spreadsheet app to look great on \"half a 13\" screen\" on first load.) reply tgma 13 hours agoparent> Google is by far the most hypocritical Blame it on Kennedy Gmail rollout which started the waste-the-space movement. Android is also quite a space waster compared to iOS. It's much less comfortable to use a small Android than a small iOS device. reply maeil 12 hours agorootparent> Android is also quite a space waster compared to iOS. It's much less comfortable to use a small Android than a small iOS device. Now in terms of reading I'm sure you're right, but on small iOS devices the bounding boxes around certain buttons are so unbelievably tight that I even see people who've been using an iPhone mini for years still have to press 4 times to get them to register. Never had that issue with small Androids. reply sumuyuda 10 hours agorootparentThe minimum tap bounds for a button in the Apple Human Interface guidelines is 44x44 points. So the developer must of not followed those guidelines. The size of the screen shouldn’t have an impact on this. reply tcptomato 6 hours agorootparentmust of? reply croemer 6 hours agorootparentCommon spelling error of \"must have\" because in some accents \"must've\" and \"must of\" sound identical reply tgma 12 hours agorootparentprevWhich \"small Android\" is the size of iPhone mini? The ones I have encountered were basically feature phones running Android that I can't remember a single model name. Android went big a lot earlier than iPhone did (iPhone 6/Plus). [Remember \"phablet\" was a word?] Just the goddamn top bar in Android consumes (used to?) 30% of screen in landscape. reply maeil 10 hours agorootparent> Which \"small Android\" is the size of iPhone mini? Unfortunately, only old ones. For Samsung, S8 is when the phones got really big. S7 (5.1 inch) and earlier were smaller than the iPhone 13 mini (5.4) My first Android was the original Galaxy S - comparitively tiny, but I definitely didn't struggle as much to tap buttons as on the 13 mini. reply anal_reactor 11 hours agoparentprevWhy the fuck I cannot turn off in Google Maps the huge green box \"in 300 meters turn left\" reply bolognafairy 12 hours agoparentprevHa. Facebook Page management. I used to work for a 5-person company, and setting up the Facebook page was one of the things that ended up on my plate. We were acquired over two years ago, by a Real Company, with Actual Marketing People. It’s been my mission to get this damn Facebook page out of my life. I have no business having anything to do with it, especially now. Every few months I manage to wrangle someone from Marketing to take it off my hands, but most of the time it takes me at least half an hour for me to work out how to even grant someone else permission to manage the page. This is AFTER I find the official Facebook help article for this process. Through a combination of the Marketing person losing interest, and my ineptitude, I think I only managed to actually close this one off a few weeks ago. There’s every chance that in a few months I’ll find out that I didn’t do it properly. I’m in my late 20s. I understand this generation of social networks. I was a very heavy Facebook user up until a couple of years ago. I’m not some clueless old idiot. This isn’t an operation that Facebook has any incentive to ‘hide away’. I can’t possibly see how this is anything other than egregious incompetence on Meta’s part. They’re simply too big to make…anything, anymore. reply postepowanieadm 9 hours agorootparentI used to work in a company that hired dozens of programmers do do things like that. \"Move fast and break things\". Out QA team were saints. reply MrBananaSnacks 5 hours agorootparentprevLate 20s you'd be just a regular idiot. Wait until you get to your 40s then you'll have enough experience to start thinking that.;) reply SoftTalker 13 hours agoprevPopups announcing new features should be banished to hell. 99.8% of the time I do not care, I just want to open my fucking document and not have to click \"got it\" on a bunch of feature announcements. If I want to learn about new features I'll go to the Help menu. reply thiht 10 hours agoparent> If I want to learn about new features I'll go to the Help menu. No you won’t. There’s nothing wrong with announcing new features, as long as it’s not disruptive and obnoxious, like a pop up. Linear[1] for example adds a small insert at the bottom of the sidebar. It doesn’t take too much space, it’s easy to close, and you can ignore it for as long as you want, it doesn’t prevent you from working. IMO this is the right way to show new features. [1]: https://linear.app reply doctorpangloss 13 hours agoparentprev> Popups announcing new features should be banished to hell. But then how will the product manager make the chart go up and to the right? reply r4indeer 13 hours agorootparentAt least for the \"to the right\" part, I think the monotonicity of time has that one covered unless someone invents a time machine to improve metrics in the past. reply maeil 12 hours agorootparentDon't need a time machine for that, just need some good accountants like PWC ;) reply bolognafairy 12 hours agorootparentprevThat’s only 50% of the KPI! reply aembleton 4 hours agoparentprevAt least give an option to never show them again. reply aitchnyu 13 hours agoprevAround 2004 there was an invite only email that... took you straight to inbox once you logged in, not to a landing page with news and weather. Years later, I'm dumbfounded by other Google products like Firebase and GCP which send massive js blobs and marketing pages in the product. reply TiredOfLife 10 hours agoparent> Around 2004 there was an invite only email that... took you straight to inbox once you logged in, not to a landing page with news and weather. In 2024 it's still like that. Only inbox no news or weather. reply gardnr 13 hours agoprevThey could enable dismissing the popups with the escape key or by clicking outside the popup. I have to admit, I've just closed the tab a few times instead of acquiescing to click \"got it\". reply c54 14 hours agoprevIronically even clicking this imgur link on mobile is half covered by popups (on iOS) reply bravo33 13 hours agoparentha pop ups pop ups every where except the terminal long live the terminal; reply tomrod 13 hours agorootparentNpm packages solicit for donations in the terminal. reply rudedogg 13 hours agoprevStackOverflow is almost like that if you don't have an adblocker. reply metadat 13 hours agoparentAll StackOverflow sites have is a GDPR cookie banner. It's annoying, but not the same. reply wtallis 12 hours agorootparentI just went to StackOverflow, where I'm logged in and don't get the GDPR box. With my browser at 100% zoom, it's over 400 pixels from the top of the page down to the top of the first question's box. Over 100 of those pixels are a banner saying \"Welcome back, (username)\". Their landing page is absolutely polluted with UI bloat that is far beyond just the GDPR box. The first piece of real content is literally on the bottom half of my laptop screen. Even the ~56px sticky floating header is clearly superfluous; getting back to the top of the page to access those UI elements is far too easy to justify keeping those controls on screen at all times. reply netsharc 12 hours agorootparentprevI've managed to create an uBlock Origin element hider to hide the DIV with that particular CSS class (or ID?) across all of their sites - hiding it on just 1 site would disappoint because they use so many goddamn domains. reply Squeeeez 8 hours agorootparentEver heard of violentmonkey? Sounds like a good opportunity for userscripts reply fanf2 7 hours agorootparentprevThey also have a Google popup. reply e-clinton 3 hours agoprevIf only Google’s Product teams were a tenth as good as their engineering teams. reply ashleyn 14 hours agoprevi set my browser zoom to 80% or lower. on a high-DPI screen everything is still perfectly legible and readable. i don't understand the fascination with extra-big-ass fisher-price sized UI elements. reply Borealid 14 hours agoparentThe huge UI elements are a consequence of optimizing for touchscreens. Mice (and even touchpads) are very precise pointing devices compared to a toddler's chunky fist. Modern UIs are created assuming you'll be using fingers to interact with them. Mice are an afterthought. reply Gigachad 13 hours agorootparentI’m pretty sure what happened was all the UI started out big, and then high dpi displays made everything tiny, and eventually UI design caught up and made things legible again. There’s no reason why I need 20,000 items on the screen at the same time. Fewer but more legible items are better. I can scroll for more. reply wtallis 13 hours agorootparentNo, high dpi displays did not make everything tiny and then huge. They just forced Microsoft to finally put some effort into making their UI scalability work properly. Displays with less than 100 dpi are still widespread, and today's software and web pages have far more padding and wasted space than they did 20 years ago. A 21\" 4:3 CRT back then was at least as usable as a 27\" 16:9 LCD today despite the newer displays having twice as many pixels. 17\" displays in any aspect ratio are frustrating to the point of uselessness on today's desktops, but were fine in the '90s. The UI bloat is real whether you measure in pixels or inches, and very strongly correlated with the rise of touchscreens. reply peeters 13 hours agorootparentprev> There’s no reason why I need 20,000 items on the screen at the same time. Fewer but more legible items are better. I can scroll for more. I'm reasonably sure most spreadsheet power users would have the opposite preferences to you. I do, for one. Data density is everything in non-trivial spreadsheets. reply toasterlovin 12 hours agorootparentExactly. Every time I see a new spreadsheet SaaS with the data density of a non-spreadsheet SaaS I immediately dismiss it as a toy. reply moribvndvs 14 hours agoparentprevI used to hate them, too, then my mid-40s happened. I do still think UX designers go a little too ham on padding, though. reply ikidd 13 hours agoparentprevPhones. People will seriously edit spreadsheets on a phone, I've seen it with my own eyes. reply allset_ 25 minutes agorootparentPhone apps can have their own UI layout, font size, etc. though. No reason to force that on desktop users. reply mklepaczewski 10 hours agorootparentprevWell, yeah. Companies store a lot of information in spreadsheets. CRM, financial info or really anything. People need to access it from their phones when the data is urgently needed. reply Sharlin 13 hours agoparentprevNobody bothers to think about large screen and mouse UX anymore. Everything's optimized for phones and tablets, and I guess that's where 95% of users are anyway. reply invalidname 13 hours agoparentprevYou're probably young. I can barely see in 100% zoom and going back and forth with reading glasses is very uncomfortable. I spend 90% of my time looking for them. I have an air and a 16 inch Dell. I don't want to lug the Dell around because also, back pain. reply bolognafairy 12 hours agorootparentYep. If there’s one thing that Big Tech is doing even remotely better at in 2024, it’s letting the self-interest of the increasingly aging top brass guide the ship into making products that don’t assume that everyone is 20 years old with perfect vision. It’s one of the few things that Apple is doing well at, but they’re doing really well at it. reply cma 13 hours agoparentprevExtreme levels of padding just means zoom makes thing tinier than they should have to be, its still a problem even if continuing to zoom out and get more on the screen. reply dawnerd 12 hours agoparentprevA little on accessibility too but there’s a line all these companies cross. Designers are not always UX experts. Google hasn’t had any good UX, period. reply anal_reactor 11 hours agoparentprevPeople have a limit of information they can process at once, which goes higher the more familiar you are with given domain. Previously, having all the buttons visible on main screen was a good thing, because average tech-savvy user had no problem navigating that. But then we made technology accessible to general public, who cannot handle such complexity, therefore we needed to make UIs way simpler. reply spaceman_2020 13 hours agoprevSomeone is getting 400k/year to make that padding reply metadat 13 hours agoparent400k base salary. 800k - 2m USD with stock. reply joebob42 13 hours agorootparentThat's director or higher (and the upper end is at least a vp). I mean, there probably is a director involved, but they definitely aren't the one making the padding. reply bolognafairy 12 hours agorootparentThey’re the one that demanded it. Don’t try to pretend that any more than 1% of the people at Google are actually doing anything that moves even the slightest needle, regardless of ‘rank’. reply new_user_final 11 hours agoparentprevAnd didn't even bother to check the responsiveness of the Google AI popup. How is it even possible with so many peer review! reply mysterydip 5 hours agoprevIronically, imgur took up half my screen on mobile with popups while I was trying to look at this image. reply bmcahren 13 hours agoprevGoogle is becoming less useful in many ways. Their \"push for AI adoption\" in sheets and docs proves they no longer know how to build products people want to use and must use tactics like this to farm engagement. They are playing the same playbook as Google+. Soon we'll be forced to use Gemini to open Sheets in a tone-deaf IVR voice \"If you could tell me a few words about what you're opening Sheets about, I can better direct your experience.\" I almost think they want people to hate AI so they think it's garbage and don't switch to using chat.com vs google.com for searching which has been scary how much better it is without all the ads and the relevance to what you want to find. reply nguyentranvu 13 hours agoprevFor many years, I cannot get the UX of Google. It's just me or anyone? reply Gigachad 13 hours agoparentI’ve always found Google sheets to be the easiest one of the lot to use. The OP article is funny but I’ve never seen anything like that any of the times I’ve used it. reply koinedad 13 hours agoparentprevI’ve never been a fan of google UI/UX myself reply y42 10 hours agoprevIt's not only Google Sheets, you can find this on every (major) website. Linking to a thread on x shows you 4 pop overs. One with a notice, the second showing a cookie advice and two of them asking you to register, while one of the two is overlapping another register element on the side. I wonder if the ones in charge are even trying to offer a reasonable ux? reply userbinator 12 hours agoprevI have a userscript that changes \"Got it\" to \"Fuck off\", because that's what I mutter to myself every time I get bombarded with those distractions. reply likeclockwork 9 hours agoparentGot it. reply ziofill 12 hours agoparentprevhow did you do it? care to share the code? reply skhr0680 12 hours agorootparentdocument.getElementById(\"divA\").textContent = \"This text is different!\"; from: https://developer.mozilla.org/en-US/docs/Web/API/Node/textCo... reply baal80spam 10 hours agorootparentWell this would work, but I think the parent asked how to change all \"Got it\"s to \"Fuck off\"s :-) reply superultra 13 hours agoprevAs a marketer I recognize this and it’s the result of territorial marketing agendas at war over IAM, In-App Messaging, the holy grail of conversion funnels. It’s a shame that Google is being run by marketers rather than engineers. And I say that as a marketer. When engineers were running the company this shit didn’t happen. Now we’re watching the inch by inch trench warfare of various teams at Google vying for quarterly KPIs and it sucks bad. I anxiously await the next Google to usurp what Google has become, which is a kinda of proto IBM/GE. reply seb1204 13 hours agoprevUI should be considered of my input device, mouse, touchpad or finger all have different need with regards to aiming. reply oofabz 13 hours agoprevNever had this problem with Gnumeric reply LeFantome 13 hours agoparentYa, when Gnumeric fails to load in my browser, there are never any annoying “Got it” buttons that make it work. reply qq99 12 hours agoprevRough lol. I can always recommend people who are looking for a web spreadsheet to try https://www.thebricks.com/ instead reply revskill 13 hours agoprevThe imgur website is the worst. reply asciimov 13 hours agoprevThat’s just The Google Rot™ leaking out. It’s the one product I wish they would kill off already. reply tlhunter 12 hours agoprevEvery sheet I open lately wants to turn my data into a \"table\". What the fuck? It's already a table. reply noio 3 hours agoparentI’m thinking “tables” are part of some kind of “extend, extinguish” strategy? I.e. reducing compatibility with Excel to make it harder to just leave with your data. I could be wrong. reply TiredOfLife 10 hours agoprev1) 13\" Air has a resolution of 2560x1664 this screenshot is 1446x1534. So the window was intentionally resized to almost half width to mislead. 2) one of the popups is an ad for their ai thing, but it has a huge close button and supposedly shows once. 3) the other popup is showing you that google has added easy table templates (and from what i have seen those types of tables are like 90% of google sheets uses) reply foogazi 5 hours agoparentThe title says it’s half the screen 1446x1534 should be enough to see actual content reply alecthomas 13 hours agoprevPeak enshittification. reply LeFantome 11 hours agoparentYou will come to regret this comment when you realize later how much you wish the way things are now ( as we near the true peak ). reply rochak 14 hours agoprevI mean, that is just a one time thing right (the first time you land on it)? reply moogleii 14 hours agoparentThat's still awful. We've all become complicit of sites spamming popups after first load. It's so annoying. reply geor9e 13 hours agoprev [–] Oh no, 3 clicks to dismiss them forever, how horrible reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Users criticize Google's UX design for Google Sheets on a 13.6\" MacBook Air, citing excessive popups and large UI elements that hinder usability.",
      "The discussion compares Google's design unfavorably to other platforms, pointing out a trend of prioritizing mobile and touch interfaces over desktop usability.",
      "Frustration is expressed over Google's focus on AI features, which some feel detracts from improving basic user experience and interface design."
    ],
    "points": 171,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1734918183
  },
  {
    "id": 42494067,
    "title": "Xerox to acquire Lexmark",
    "originLink": "https://newsroom.lexmark.com/2024-12-23-Xerox-to-Acquire-Lexmark",
    "originBody": "This site uses cookies for various purposes including enhancing your experience, analytics, and ads. By continuing to browse this site or by clicking \"Accept and close\", you agree to our use of cookies. For more information, read our Cookies page. Accept and close Search Printers × Small & Medium Business Enterprise & Large Business Supplies & Accessories × Find Supplies Why Choose Genuine Lexmark Supplies? Check to Protect Recycling Programs Supplies Warranty Extended Warranty Genuine Parts Industries & Solutions × Industries Banking Government Healthcare Insurance Education Manufacturing Retail Solutions IoT Cloud Security Print Capture Software Partners Services × OnePrint Subscription Managed Print Services Cloud Services Hardware Warranty and Repair Service Professional Services Customer Support × Product Support Support Hub Product Registration Warranty & Service Downloads Drivers & Downloads Universal Print Driver Markvision Enterprise Search Lexmark Newsroom Lexmark creates cloud-enabled imaging and IoT technologies that help customers worldwide quickly realize business outcomes. Through a powerful combination of proven technologies and deep industry expertise, Lexmark accelerates business transformation, turning information into insights, data into decisions, and analytics into action. Lexmark Newsroom News Releases Lexmark in the News News Alerts Get News Alerts by Email All RSS Feeds Analyst Insights Media Resources Overview Lexmark Newsroom News Releases Lexmark in the News News Alerts Get News Alerts by Email All RSS Feeds Analyst Insights Media Resources Xerox to Acquire Lexmark Transaction between printing industry icons is expected to close in second half of 2025 NORWALK, Conn., and LEXINGTON, Ky., Dec. 23, 2024 – Xerox Holdings Corporation (NASDAQ: XRX) today announced it has agreed to acquire Lexmark International, Inc., from Ninestar Corporation, PAG Asia Capital, and Shanghai Shouda Investment Centre in a deal valued at $1.5 billion, inclusive of assumed liabilities. This acquisition will strengthen the Xerox core print portfolio and build a broader global print and managed print services business better suited to meet the evolving needs of clients in the hybrid workplace. “Our acquisition of Lexmark will bring together two industry-leading companies with shared values, complementary strengths, and a deep commitment to advancing the print industry to create one stronger organization,” said Steve Bandrowczak, chief executive officer at Xerox. “By combining our capabilities, we will be better positioned to drive long-term profitable growth and serve our clients, furthering our Reinvention.” Lexington, Ky.-based Lexmark, a valuable partner and supplier to Xerox, is a leading provider of innovative imaging solutions and technologies including a best-in-class line of printers and multifunction printers. By combining Lexmark’s solutions with Xerox® ConnectKey® technology and advanced Print and Digital Services, the acquisition will create a superior offering portfolio and underscores Xerox commitment to increasing value for clients and partners. The transaction will also strengthen the ability of Xerox to serve clients in the large, growing A4 color market and diversify its distribution and geographic presence, including the APAC region. The new organization will serve more than 200,000 clients in 170 countries with 125 manufacturing and distribution facilities in 16 countries. Combined, Lexmark and Xerox have a top five global share in each of the entry, mid and production print markets and are key players in the large, stable managed print services market. “Lexmark has a proud history of serving our customers with world-class technology, solutions and services, and we are excited to join Xerox and expand our reach with shared talent and a stronger portfolio of offerings,” said Allen Waugerman, Lexmark president and chief executive officer. “Lexmark and Xerox are two great companies that together will be even greater.” “Our shared values and vision are expected to streamline operations and drive efficiencies, taking the best of both companies to make it easier to do business with Xerox,” added Bandrowczak. Transaction Rationale Strategic fit: Xerox and Lexmark have complementary sets of operations, offering strengths and end-market exposures. Combined, the companies form a vertically integrated manufacturer, distributor and provider of print equipment and MPS, covering all geographies and client types with a well-rounded portfolio of print and print services offerings. Growth opportunities: Lexmark is a leader in the large, growing A4 color print and supplies market and has an opportunity to expand its OEM platform within the A3 equipment category. Once combined, Xerox expects to have a more comprehensive portfolio of products to enhance its offerings and reinforce its value proposition to clients, enabling growth across the portfolio of equipment and MPS, as well as incremental opportunities to increase penetration of its advanced Digital Services and IT Solutions. Financial benefits: The transaction is expected to be immediately accretive to earnings per share and free cash flow. Xerox expects this transaction to accelerate the realization of its Reinvention financial targets of revenue stabilization and double-digit adjusted operating income through an improved competitive position and exposure to faster-growing segments within print, as well as more than $200 million of identified cost synergies to be realized within two years of transaction close. Improved balance sheet: The transaction will immediately reduce Xerox pro forma gross debt leverage ratio, from 6.0x as of Sept. 30, 2024, to approximately 5.4x before synergies. Pro forma gross debt leverage will be reduced to approximately 4.4x with the benefit of $200 million of cost synergies. With improved free cash flow and a priority of repaying debt, Xerox expects to reduce its gross debt leverage ratio to below 3.0x over the medium term. Transaction Detail Under the terms of the agreement, Xerox will acquire Lexmark for total consideration of $1.5 billion, inclusive of net debt and other assumed liabilities. Xerox expects to finance the acquisition with a combination of cash on hand and committed debt financing. In conjunction with this financing, the Xerox Board of Directors approved a change in the dividend policy to reduce the Xerox annual dividend from $1 per share to 50 cents per share starting with the dividend expected to be declared in the first quarter of 2025. This lowered dividend payment provides incremental capacity to reduce debt while continuing to reward shareholders with an above-market yield. The Xerox Board of Directors has unanimously approved the transaction. The transaction is subject to regulatory approvals, approval of Ninestar’s shareholders, and other customary closing conditions. It is expected to close in the second half of 2025. Until then, both Xerox and Lexmark will maintain their current operations and operate independently. Further Transaction Detail Advisors Jefferies LLC is serving as the financial advisor to Xerox and Citi is also providing financial advice. Ropes & Gray LLP and Willkie Farr & Gallagher LLP are serving as legal advisors to Xerox. Morgan Stanley & Co. LLC is serving as financial advisor to Lexmark and Strait Capital Management is serving as financial advisor to Ninestar Corporation. Dechert LLP is serving as legal advisor to Lexmark, as well as Ninestar Corporation, PAG Asia Capital and Shanghai Shouda Investment Centre and King & Wood Mallesons is serving as PRC counsel to Ninestar Corporation. Xerox Conference Call Details Xerox will host an investor conference call today, Dec. 23, 2024, at 8:00 am ET to discuss this transaction. The webcast and presentation materials are available at https://investors.xerox.com. An archived edition will be available after the call. Forward-Looking Statements Certain statements contained in this communication may be characterized as forward-looking under the Private Securities Litigation Reform Act of 1995. These statements involve a number of risks, uncertainties and other factors that could cause actual results to differ materially. Statements in this communication regarding Xerox and Lexmark that are forward-looking may include statements regarding: (i) the transaction; (ii) the expected timing of the closing of the transaction; (iii) considerations taken into account in approving and entering into the transaction; (iv) the anticipated benefits to, or impact of, the transaction on Xerox's and Lexmark's businesses; and (v) expectations for Xerox and Lexmark following the closing of the transaction. There can be no assurance that the transaction will be consummated. Risks and uncertainties that could cause actual results to differ materially from those indicated in the forward-looking statements, in addition to those identified above, include: (i) the possibility that the conditions to the closing of the transaction are not satisfied, including the risk that required shareholder and regulatory approvals are not obtained, on a timely basis or at all; (ii) the occurrence of any event, change or other circumstance that could give rise to a right to terminate the transaction, including in circumstances requiring Xerox or Lexmark to reimburse the other’s expenses or pay a termination fee; (iii) possible disruption related to the transaction to Xerox's and Lexmark's current plans, operations and business relationships, including through the loss of customers and employees; (iv) the amount of the costs, fees, expenses and other charges incurred by Xerox and Lexmark related to the transaction; (v) the risk that Xerox's stock price may fluctuate during the pendency of the transaction and may decline if the transaction is not completed; (vi) the diversion of Xerox and Lexmark management's time and attention from ongoing business operations and opportunities; (vii) the response of competitors and other market participants to the transaction; (viii) potential litigation relating to the transaction; (ix) uncertainty as to timing of completion of the transaction and the ability of each party to consummate the transaction; (x) Xerox’s ability to finance the transaction; (xi) the ability of the combined company to achieve potential market share expansion; (xii) the ability of the combined company to achieve the identified synergies; (xiii) Xerox’s indebtedness, including the indebtedness Xerox expects to incur and/or assume in connection with the transaction and the need to generate sufficient cash flows to service and repay such debt;(xiv) the ability to integrate the Lexmark business into Xerox and realize the anticipated strategic benefits of the transaction within the expected time-frames or at all; (xv) that such integration may be more difficult, time-consuming or costly than expected; (xvi) that operating costs, customer loss and business disruption (including, without limitation, difficulties in maintaining relationships with employees, customers or suppliers) may be greater than expected following the transaction; (xvii) rating agency actions and Xerox’s ability to access short- and long-term debt markets on a timely and affordable basis; (xviii) general economic conditions that are less favorable than expected; and (xix) other risks and uncertainties detailed in the periodic reports that Xerox filed with the Securities and Exchange Commission, including Xerox's Annual Report on Form 10-K. All forward-looking statements in this communication are based on information available to Xerox as of the date of this communication, and Xerox intends these forward-looking statements to speak only as of the date of this release and does not undertake to update or revise them as more information becomes available, except as required by law. About Xerox Holdings Corporation (NASDAQ: XRX) For more than 100 years, Xerox has continually redefined the workplace experience. Harnessing our leadership position in office and production print technology, we’ve expanded into software and services to sustainably power the hybrid workplace of today and tomorrow. Today, Xerox is continuing its legacy of innovation to deliver client-centric and digitally driven technology solutions and meet the needs of today’s global, distributed workforce. From the office to industrial environments, our differentiated business and technology offerings and financial services are essential workplace technology solutions that drive success for our clients. Learn more at www.xerox.com and explore our commitment to diversity and inclusion. About Lexmark Lexmark creates cloud-enabled imaging and IoT technologies that help customers worldwide quickly realize business outcomes. Through a powerful combination of proven technologies and deep industry expertise, Lexmark accelerates business transformation, turning information into insights, data into decisions, and analytics into action. Media Contacts: Xerox: Callie Ferrari, APR, Callie.Ferrari@xerox.com, +1-203-615-3363 Lexmark: Scott Shive, Scott.Shive@lexmark.com, +1-859-232-2131 Investor Contact: David Beckel, David.Beckel@xerox.com, +1-203-849-2318 Note: To receive RSS news feeds, visit https://www.news.xerox.com. For open commentary, industry perspectives and views, visit https://www.linkedin.com/company/xerox, https://twitter.com/xerox, https://www.facebook.com/XeroxCorp, https://www.instagram.com/xerox/, https://www.youtube.com/XeroxCorp. Xerox® and ConnectKey® are trademarks of Xerox in the United States and/or other countries Lexmark® and the Lexmark logo are trademarks of Lexmark International, Inc., registered in the United States and/or other countries. All other trademarks are property of their respective owners. Home Printers Supplies & Accessories Industry & Solutions Services Customer Support About Careers Contact Us Find a Dealer Sustainability Analyst Insights Lexmark Blog Small Business Resources Newsroom Partners Product Registration Communication Preferences Success Stories Lexmark International, Inc. ©2021 All rights reserved. Privacy Terms and Conditions Terms of Use Return to Top of Page Search Printers × Small & Medium Business Enterprise & Large Business Supplies & Accessories × Find Supplies Why Choose Genuine Lexmark Supplies? Check to Protect Recycling Programs Supplies Warranty Extended Warranty Genuine Parts Industries & Solutions × Industries Banking Government Healthcare Insurance Education Manufacturing Retail Solutions IoT Cloud Security Print Capture Software Partners Services × Managed Print Services Cloud Services Hardware Warranty and Repair Service Professional Services Customer Support × Product Support & Service Interactive User Guide How-to Videos Device Support Virtual Technical Support Drivers & Utilities Drivers & Downloads Universal Print Driver Markvision Enterprise OS Compatibility List Device Registration & Information Register a Device Warranty Information Tech Library Change Region Shop Sign in/Register My account Profile Orders Sign out",
    "commentLink": "https://news.ycombinator.com/item?id=42494067",
    "commentBody": "Xerox to acquire Lexmark (lexmark.com)164 points by taubek 6 hours agohidepastfavorite134 comments jasoneckert 4 hours agoWhile many here will note the potential downsides for Lexmark here, the strategic fit statement of \"Xerox and Lexmark have complementary sets of operations\" likely means that Xerox will keep Lexmark operating as usual in the short term. And in the long term, there is a greater possibility of them growing the Lexmark side with their resources because Lexmark is an established brand, was already an existing partner/supplier for Xerox, as well as focused on certain growth areas (e.g., IoT, WFA) that Xerox did not. Now, if Broadcom were to acquire Lexmark, they'd likely get rid of 70% of the people and focus on extracting more money from the top 10% of Lexmark users via a subscription model that would make HP look tame by comparison. reply ryao 4 hours agoparentIf I recall correctly, Xerox printers are rebadged Lexmark printers, with the exception of the highest end models. reply mastax 3 hours agorootparentDell printers are also rebadged Lexmark’s. Or at least they were 20 years ago. reply lukevp 54 minutes agoparentprevI haven’t heard much about Broadcom before. Is this why Rally is so awful? They just rent seek on old software and don’t improve it at all? Like the people who bought Heroku? reply bluGill 35 minutes agorootparentRally didn't really change much before the purchase either. But the price going up by a lot is why we dropped it. don't count on whatever work tracker you have is my advice. I've never seen a company stick with one for more, than 10 years. And now I have a lot of code comments about something weird that should not be simplifed because of some bug in the old system. And since closed bugs don't move - and even if they did they get new numbering - I have no ability to look upthat bug and ensure it doesn't break if I need to change the code. reply acedTrex 52 minutes agorootparentprevYou havent at least heard about them gutting and draining VMWare? reply delfinom 39 minutes agorootparentThey also gutted and drained Symantec a few years earlier too. Strange how much they hate money because they made it difficult to even renew the Symantec endpoint. reply Clubber 2 hours agoparentprev>\"Xerox and Lexmark have complementary sets of operations\" To me that means, \"we can save money because we now don't need 2 marketing departments and 2 accounting departments and 2 support departments, etc. for the same amount of combined market share.\" reply hk1337 1 hour agorootparentPartly true but I don't think they would (or should) just ax those departments. By merging those departments will be taking on more work too, so they will not need everyone but they will likely let some people go. reply pessimizer 2 hours agorootparentprevPretty sure that's what it means to everyone and that's what they meant to say. reply Y_Y 3 hours agoparentprevApparently WFA stands for Wi Fi Alliance. Or maybe Wilderness First Aid, which makes more sense in the context of a \"growth area\". reply layer8 1 hour agorootparentWFA stands for workflow automation. reply jasoneckert 3 hours agorootparentprevIt stands for Work from Anywhere reply TheRealPomax 1 hour agoparentprevEven if they're purely complementary, which they're not, when one of them can only \"survive\"by being bought by the other, that's not survival. Let it fail, and then sell off all the parts to not yet established companies who are trying to make it in, or move into, that space. Mergers and acquisitions of established businesses puts a stranglehold on the market and should be illegal. This is the kind of bullshit that's resulted in four media conglomerates and three ISPs for a population of 300+ million. reply sunnytimes 3 hours agoprevI used to fix Lexmark printers , they are \"PC load letter\" printers. not as bad as HP but lexmark printers stop working for various no reasons. Most of the time the trays would mess up due to a little dust or you would have to get the person to smack the drum over the phone and that would usually help. they never stop working but they will crumble 10 pages into a ball inside the machine ha.. side note . HP printers are the worst for PC load letter. I've fixed HP printers my whole life. I love reading the manuals and they use \"might\" or \"maybe\" to describe fixes or errors. reply rrr_oh_man 2 hours agoparentPC Load Letter: https://en.m.wikipedia.org/wiki/PC_LOAD_LETTER reply Shakahs 2 hours agoparentprevCounterpoint: I also used to fix Lexmark printers as a field tech servicing pharmacies. It was routine to see Lexmark MS711dn printers with page counts in the millions. They did not need more than basic maintenance. reply Suppafly 1 hour agorootparent>They did not need more than basic maintenance. The other guy was probably working on ones that didn't get the basic maintenance. People always skimp on preventative care and then are surprised when things break. When I worked in a computer lab 20ish years ago, being consistent about which side of the paper we loaded when we loaded paper made a huge difference. They are always stacked on the pallet the same way and had a little indicator on the flap of the wrapper around the ream that pointed down. I'm always surprised when schools want people to donate reams of paper, instead of just ordering a pallet of paper, swapping between a bunch of different weights and qualities is going to cause more way more costs in repairs than just ordering paper by the pallet. reply sunnytimes 1 hour agorootparentone of the main ones with the tray printers is dusty rollers . the rubber rollers that roll the paper out of the tray will get dirty and the paper will slip and cause jams or miss printing. of course theirs no error code for dirty rollers so the printer would say all kinds of shit because it would end up being a paper jam. we also used labels in the pharmacy and those would peel off and end up stuck all over the inside of the printer, good times haha. reply tssva 1 hour agorootparentprevSchools still want people to donate paper? My local school system got rid of most printers/copiers years ago and it takes administration approval to print something on the few that remain. My daughter is a senior and I don’t think she has brought home a piece of paper from school since 5th grade. reply Suppafly 1 hour agorootparentThe grade schools do, high schools barely seem to use paper. reply sunnytimes 2 hours agorootparentprevno fuckin way .. i used to work for Kroll Pharmacy in Toronto!! where were you!? reply chiph 3 hours agoprevLexmark was the supplier of Model M keyboards for a while, after IBM spun their printer & keyboard business off. Which they later spun off into Unicomp (pckeyboard.com) who still manufacture them in the town of Lexington. If you've never used a Model M, they're beasts. Great mechanical feel (they have buckling spring technology). And they're heavy enough to not slide around on your desk. reply kstrauser 3 hours agoparentI bought a Unicomp Type M once to replace my IBM because it had USB, more keys, etc. It was Model M “Lite”: same key feel, same delightful clicky, but much lighter and more flexible. You could march into battle with an IBM Model M. I don’t think you could take on more than one local thug with the Unicomp version. reply saulpw 2 hours agorootparentI was all excited for the TKL Unicomp keyboard they introduced a couple years ago. I bought one the second it went up for sale, and plugged it in right away when I got it. I had to ditch it after a few hours. It turns out that certain keypresses won't register if another key is pressed at the same time, and I type fast enough that it was losing keypresses regularly. I haven't had a keyboard with this problem since about 1985. An unconscionable design mistake which tarnishes the entire Unicomp brand for me. reply jhickok 3 hours agorootparentprevNew keyboard testing metric just dropped over Christmas break. reply kstrauser 2 hours agorootparentI mean, this is the keyboard people use to run through the dishwasher to clean it. You have to come up with more advanced criteria to really tease them apart. (I just realized that \"flexible\" above might be interpreted positively, like \"applicable in more situations\". No, I mean \"flexible\" as in \"may not be satisfactorily used as a bridge over a pothole in case of emergency\", which the original would be able to pull off and still be used to write a blog post about the experience afterward.) reply wholinator2 1 hour agorootparentIs that, is that true? A dishwasher? Was this a regular occurrence? Did they have to turn down the water heater before hand? Did they take the keys off or leave them on and how did they dry if so? reply bluGill 25 minutes agorootparentMost electronics are run through a 'dishwasher' as part of manufacturing. Hot water removes a lot of gunk that if left causes earay failure. chips are plastic, the board is fiberglass, the resisters are ceramic or plastic most capacters are ceramic, the conductors are metal - none of that cares about water once it dries. The only thing to worry about is impurities in the water since they can leave something conductable behind. Some capacters however cannon take water. Likewise I'm note sure if LCDs are sealed enough. be careful of what soap you use though, dishwasher soap is too harsh. Manufactures are using deionized water and if any soap it is specific to electronics. Your house water isn't pure enough to do this often but once ever few years and you will normally get away with it. reply michaelsshaw 1 hour agorootparentprevBasically: no. The construction of the Model M disallows removing the backplate after the assembly is removed from the case. On the Model F, however, the backplate is not attached with plastic rivets, and had no rubber membrane that could trap liquids, so after the electronics were removed, it is feasible to wash it in a dishwasher. Inadvisable to say the least, if you want your PCB to remain non-warped. reply chiph 1 hour agorootparentprevI've done that a few times but not \"regularly\". Use the top rack, no soap, no heated dry, zip-tie a plastic bag over the connector. Afterwards let it sit on the counter face-down for a couple of days to air-dry. Comes out sparkly clean. reply kstrauser 1 hour agorootparentprevThat is true. I don't think anyone routinely did that instead of using canned air or something like that, but it was absolutely on the table for more substantial messes. Spill a can of soda into your keyboard? Run that sucker through the washer. I have not personally done this, but google \"ibm model m dishwasher\" and you'll see lots of anecdotes. reply a2tech 1 hour agorootparentprevYou can do it today. Don’t run the heated dry and put it in with the keys facing down. Run it and take it out and leave it upside down over night on a towel. Let the connector hand down as well. Should work perfectly in the morning. reply SoftTalker 43 minutes agoparentprevThey're not so great if you have an open office. You can hear the person with the Model M clattering away from across the room. reply ryao 3 hours agoparentprevMy unicomp model m died after a few years. I never found time to return it to unicomp for repair and ended up discarding it to clear my long to do list. Reportedly, there were cost saving changes over the years that reduced weight and reliability. The weight reductions are definitely real. I might have just been unlucky with the keyboard failure, although I assume it involved the circuitry for making it work with USB, which the original model m keyboards did not have. reply bluGill 23 minutes agorootparenta couple years ago they redid their tooling. They were getting bad because the factory was worn out. Reportably things are much better now. reply philistine 2 hours agoparentprevThey're wonderful things, unfortunately Unicomp has not introduced the latest keyboard layouts that have been introduced since 1988. So I can't get my beloved CAN/CSA Z243.200 for a battle-capable keyboard. reply onre 3 hours agoparentprevI have the 122-key version and a ton of Emacs bindings to make the most out of the 24 function keys and the 12-key panel on the left. It is simply the best. reply acjohnson55 3 hours agoparentprevI used to work at Lexmark in the same lab as the son of Neil Muyskens, who founded Unicomp. reply MisterTea 3 hours agoparentprevMy 1986 M is still going strong. > If you've never used a Model M, they're beasts. I jokingly call it the preppers keyboard as it can double as a clubbing weapon when SHTF. reply ryao 3 hours agorootparentThat is only the older ones. The newer ones are mostly made out of plastic to save money. reply MisterTea 2 hours agorootparentIt's amazing how they had a whole stamped metal frame in there to stiffen it up. Like what was the design criteria that made them think they needed all that steel? Violent cavemen users? reply Wohlf 58 minutes agorootparentThey may have expected typists coming over from typewriters to hammer on it for 8 hours a day, or to give them a similar feel to ease the transition and limit complaints. reply dangoodmanUT 3 hours agoprev> Lexmark creates cloud-enabled imaging and IoT technologies that help customers worldwide quickly realize business outcomes. Through a powerful combination of proven technologies and deep industry expertise, Lexmark accelerates business transformation, turning information into insights, data into decisions, and analytics into action Great, so I have no idea what they do. reply ryandrake 3 hours agoparentI'm convinced only CEOs know how to translate that into English. Company descriptions like this ought to be written as if you're explaining what your company does to a 6 year old. Imagine explaining your job to a class of first graders and telling them \"I accelerate business transformation!\" Yea, they'll be as lost as the rest of us. reply rrr_oh_man 2 hours agorootparentEveryone at the top is clueless, faking it, and anxious they’ll be found out. reply deskr 1 hour agorootparentAnd based on social media, taken out. reply layer8 1 hour agoparentprevIt seems that they can only turn stuff into other stuff if it starts with the same letter. That space is ripe for disruption! reply gausswho 2 hours agoparentprevSomeone please turn that into a the backing vocals of a techno song. reply echelon 2 hours agoparentprev> Lexmark creates cloud-enabled imaging and IoT technologies We make printers > that help customers worldwide quickly realize business outcomes. To help you do business > Through a powerful combination of proven technologies and deep industry expertise, We've been doing this for a while > Lexmark accelerates business transformation, turning information into insights, data into decisions, and analytics into action We help your company get important work done On this last point, a company is more than just its products, technology, and IP. It's people. People that are hopefully empowered and educated to make nimble decisions and rapidly respond to changing conditions. But yeah, they sell printers. reply i80and 5 hours agoprevI got a Lexmark for their driverless IPP Everywhere support and it's the best zero-fuss printer I've ever owned. Xerox doesn't have any IPP Everywhere devices, so I hope this isn't Lexmark's death knell. reply nottorp 5 hours agoparentI don't know what an IPP is but I bought a networked Xerox all in one, plugged it into my network and it just showed up on all my devices (mind, they're mostly Apple). So whatever they're doing, it ain't so bad. I did have to cover the power led with black nail polish though. It was lighting up the whole room. reply MrMcCall 4 hours agorootparentI use black electrical tape for those ever-more-common nuisances. reply vidarh 4 hours agorootparentYou can also get sheets of hundreds of small little black dot stickers that are perfect to use on LEDs. Some of them are thin enough to let some light through an individual one, so you can choose to dim or entirely block by adding one or 2-3... reply layer8 1 hour agorootparenthttps://www.lightdims.com/ reply nottorp 45 minutes agorootparentYou can get a lot of toys but if your or your significant others' cosmetic habits include black nail polish why not use it? reply layer8 14 minutes agorootparentYou can’t remove the nail polish (without likely damaging the base surface) when you want to sell or gift the device to someone else, and it doesn’t look good on non-black devices. (Even on black devices you might not like its glossiness.) Black nail polish also completely blacks out the light, whereas one might only want to reduce the brightness (which the product I linked to supports). reply xattt 4 hours agorootparentprevPainting the printer in Vantablack is also an acceptable answer. reply rrr_oh_man 2 hours agorootparentThat might be the one thing more toxic than toner powder particles. reply imp0cat 2 hours agorootparentprevCurrent Xerox printers definitely do support IP, but you need Windows to configure them (the proprietary app that will connect the printer to the network only runs in Widows). reply nottorp 47 minutes agorootparentI configured the internal IP via the printer's control panel actually. But of course, you need a printer with a control panel on it. I got the cheapest b&w laser (i print so little that inkjet is out of the question) all in one that had ethernet (B225). It has a tiny display and some buttons. You can set it up and use it like a copy machine at the least from it. reply i80and 1 hour agorootparentprevSee, that's what I'm avoiding with IPP Everywhere[1]: no configuration required. It just magically shows up instantly as a printer on Linux. It's the best printing experience I've ever had. [1]: https://www.pwg.org/ipp/everywhere.html reply imp0cat 13 minutes agorootparentI may be misunderstanding, but it seems to me that you're talking about a printer that is already connected to the local network? But in my case, the software was needed to connect the printer to the wifi. reply tecleandor 5 hours agoprevSpun off from IBM to end up at Xerox 30 years later. I haven't followed Xerox in the last - 20? years, so I don't know how terrible could this be. reply ryao 3 hours agoparentXerox is essentially a Lexmark reseller at this point. You can look at what Xerox technicians are posting on reddit if you need evidence: https://www.reddit.com/r/printers/comments/15sa0q4/why_do_th... As per a comment there, even the toner cartridges are the same, with the only difference being the chip used. This acquisition should make Xerox into a company that builds its own printers again. reply tecleandor 3 hours agorootparentAh, interesting. I guess they were manufacturing the bigger corporate machines, and rebranding this medium/small sized printers. They also had solid ink printers (we had one in the office a long time ago) but I think they aren't doing them anymore. I bought past year a bunch of Xerox branded toners for an HP printer, and I didn't know they had diversified the business that much. I wonder if they manufacture them. reply drewda 3 hours agoparentprevIntersting, I didn't realize there's a relationship. But Wikipedia says: > Lexmark was formed on March 27, 1991, when investment firm Clayton & Dubilier completed a leveraged buyout of IBM Information Products Corporation, the printer, typewriter, and keyboard operations of IBM https://en.wikipedia.org/wiki/Lexmark reply epc 1 hour agorootparentThis was the pre-Gerstner era of the “Baby Blues”: Lexmark, AdStar, Pennant, Eduquest, Advantis/ISSC, and some others I’ve forgotten. In the end IBM spun off Lexmark, Federal Systems (to Loral), AdStar never spun out but was the division sold to Hitachi. Lexmark was “small” printers and keyboards, Pennant was the room sized beasts. Advantis became IBM Global Network, sold to AT&T in 2000. reply ryao 3 hours agorootparentprevLexmark being IBM’s former printer division is well known. Former employees purchased the keyboard business from Lexmark and made Unicomp. Meanwhile, Hitachi Global Storage Technologies was IBM’s hard drive division and Lenovo was their PC division. IBM has sold off so many parts of itself over the years it is surprising there is much left. reply tecleandor 2 hours agorootparentYep, I still have a couple or three Model M keyboards, and although all are IBM branded, if you disassemble them you'll find that, depending on the year, they were IBM or Lexmark manufactured. Also, IBM laser printers from the 4019, 4029, 4039... series started to appear branded as Lexmark. At least if I remember correctly from when my father worked at a bank. Our equipment at home was a less fancy IBM Proprinter XL24. Noisy! reply echelon 59 minutes agorootparentprevThe IBM / Lexmark relationship persisted for years after this. IBM Thinkpads would be cross-sold with promos for Lexmark printers. https://web.archive.org/web/19990423063310/http://www.direct... reply ks2048 56 minutes agoprevIs there any printer company that is not 30+ years old? What is stopping a start-up from making a printer people actually like? reply SoftTalker 35 minutes agoparentPrint is dying. It took a while to realize the \"paperless office\" and we aren't quite there yet, but in my office, the amount of stuff people print has really dropped in the past decade. Stores offer email or text message receipts, doctor's offices have you fill out forms online. Printers are not a growth market, so not very attractive to a start-up. reply starik36 13 minutes agorootparentEven back in the last decade, the only thing I ever printed in the office was return labels to slap on a package. reply thinkingtoilet 35 minutes agoparentprevPeople speak highly of Brother printers and I've been happy with mine. On top of it, lots of people don't really print anything day to day. reply hollerith 34 minutes agoparentprevPrinters are a shrinking market. reply sedatk 34 minutes agoprevI didn’t know these brands still existed. The title felt like “East India Company to acquire Silk Road”. reply TomMasz 5 hours agoprevThis is unfortunate for Lexmark employees. reply hipadev23 58 minutes agoprevIs this comparable to when two extremely old widowers decide be roommates and live out their remaining days? reply bentt 3 hours agoprevGrowing up in Rochester, NY, where Xerox was founded and has/had the most employees... I'm just glad to hear they have enough resources to acquire something. Been a rough couple decades. reply volkk 3 hours agoprevthis headline feels like i'm in 1996 again reply agmater 4 hours agoprevWhy would Ninestar sell off Lexmark, is it just that they got a good price? I thought the pantum and printer business was an interesting move, but maybe they just couldn't make it work. reply tecleandor 4 hours agoparentNinestar was already having problems in the US. In 2023 they got an import ban by the DHS [0] and Lexmark had to find a new supplier for whatever Ninestar was sending them. Lexmark had to sell some assets this year to add a bit of liquidity [1]. I guess this is Ninestar \"just\" getting rid of Lexmark because it was getting a bit messy for them. 0: https://www.wsj.com/articles/u-s-puts-chinese-company-with-kentucky-ties-on-forced-labor-ban-list-ce2e8d00 1: https://www.opi.net/news/region/001-north-america/ninestar-offloads-lexmark-assets/ reply onemoresoop 5 hours agoprevI wasn’t even aware Xerox was still around.. reply jolt42 4 hours agoparentSame. I had to look them up, apparently PARC is still a thing. I had no idea. reply cowsandmilk 3 hours agorootparentPARC isn’t part of Xerox any more. https://www.news.xerox.com/news/xerox-announces-donation-of-... reply astura 4 hours agoparentprevNot only are they still around, they are a Fortune 500 company with over 7 billion dollars in revenue. They offer not only printers and copiers, but they are also a a business services company. reply ghaff 4 hours agorootparentBasically there are a ton of very large companies that even people in at least adjacent spaces just aren't aware of. Way back in my product manager days, we'd have companies into our executive briefing center who made 80% of the country's . reply addicted 3 hours agorootparentSo many people have and are making so much money just doing mundane business things. It’s quite a change from the fast moving world of tech startups. reply tonyedgecombe 2 hours agorootparentMeanwhile Xerox is making little money or even a loss doing mundane things. Look at their share price over the last 25 years if you want to see what a dismal company Xerox is. reply Y_Y 3 hours agorootparentprevIt's probably a market failure if you can make loads of money (profit rather than revenue) doing mundane things. If your moat isn't something like novelty or patents or concentrated unique expertise then you should be just scraping by in an ideal scenario. You might say they have trust or brand recognition or whatever, but that shouldn't prevent new entrants in a market where the products aren't developing quickly and you can undercut them by taking a smaller margin. reply linksnapzz 3 hours agorootparentDoing the mundane exceptionally well is...exceptional. No reason why that shouldn't also be profitable. reply Y_Y 3 hours agorootparentNobody has ever made a printer exceptionally well. But seriously, you'd have to say how it is that your maker of mundane widgets can do a much better job than any competitor. Maybe the company is run by a printer savant, ok. But if it's just because you have good practices they should be copyable, if it's the best employees it should be possible and worthwhile to coax them away, etc. A reasonably defined \"efficient\" market is one that will chip these differences until you have only normal profit being made while making an acceptable product. A long term super-normal profit making a commodity is the opposite of efficiency. reply freedomben 2 hours agorootparentBrother has made exceptional printers. I have one of their small office lazer printers/scanners and it is the first time I've ever enjoyed a printer. Works great in my Linux-only house. I have it hooked up as a network printer. reply ghaff 2 hours agorootparentprevI'm not sure anyone is saying there's an extraordinary profit margin being made. But if you're the dominant supplier in some niche and your customers don't have any real complaints, you can still make a lot of money and, as a potential new entrant, your niche probably doesn't have a lot of appeal to me unless I have a genuinely new idea that would have broad customer appeal and I can execute on it. reply tonyedgecombe 2 hours agorootparentprev>Nobody has ever made a printer exceptionally well. My first job (in 1982) was writing barcode software for Printronix printers. They still make them now, largely unchanged [1]. They were built like a tank. [1] https://printronix.com/line-matrix-printers/ reply philistine 2 hours agorootparentprev> Nobody has ever made a printer exceptionally well. Brother. reply ghaff 2 hours agorootparentAt least as a consumer printer, they do seem to have emerged as a can't really go wrong option. I finally junked my inkjets because I didn't use them enough to keep the ink from drying out. I don't print a lot but I find it useful to have a printer in the house to casually print out recipes, travel info, and the like. reply elorant 3 hours agoprevBought a Lexmark E232 laser printer back in 2006. 18 years and 110k pages later the damn thing still works flawlessly. I have nothing but admiration for their printers. reply rrr_oh_man 2 hours agoparentHow do you know it’s been 110k? reply layer8 1 hour agorootparentPrinters maintain a page counter. reply acjohnson55 2 hours agoprevI spent some of the formative time in my career at Lexmark. They sponsored my GEM Fellowship for grad school, and I worked there for 4 internships in 4 years in the mid 2000s. It was an interesting window into the business world. - Lexmark came into existence when IBM wanted to spin off their declining printer, keyboard, and typewriter businesses, which were headquartered in Lexington, KY, hence the name. - According to some of my coworkers, IBM brought all the most dynamic leaders back to the mothership, so Lexmark was left with whoever stayed behind or was left behind. These folks weren't highly respected by the engineers I knew, but I can't really judge, personally. - As many of you all know, some IBM/Lexmark manufacturing folks arranged a deal to take the keyboard business independent, as Unicomp. - In a major settlement with HP over patents, the two companies had a full exchange of printing technology, resulting in Lexmark gaining cutting edge laser printing tech. According to people I know, this turned a moribund company into a player. - Lexmark became most well known for bringing the \"razor blade\" business model to consumer inkjet printing. They would literally give printers away with a manufacturer's rebate, hoping to make the money back on supplies (e.g. ink cartridges). Unfortunately, there were so many printers floating around that many people would just throw out the old one when it was out of ink. It was a catastrophe. - When I was working there, one of the major initiatives was to create the cheapest possible inkjet printer. On the other hand, there was still a lot of pretty cool R&D going on. Just nowhere near the level of investment HP was making. - Lexmark became infamous for attempting to enforce DRM on its supplies to prevent people from refilling ink cartridges, forcing them to buy high margin supplies. While I was there, we were shipping cartridges with write-once memory for tracking usage. - In parallel to consumer inkjet, Lexmark had an almost completely separate business unit doing business printers, based on laser printing technology. In this market, you sell full on documents capabilities and services, with the printer merely being the central piece of hardware. - A few years after my last stint there, Lexmark exited the consumer inkjet business and became solely B2B. I didn't follow the company closely after this point. Working at Lexmark was one of the things that convinced me to leave tech for education. I enjoyed my short stints there, but just found the environment completely uninspiring as a place to really establish my career. Being my main exposure to the tech career (along with previous internships at manufacturing companies), I assumed that this was what the whole industry was like. (I returned to tech a few years later, but that's a whole other story.) reply linotype 1 hour agoparentNow I want to know why you left education for tech! :) reply casenmgreen 5 hours agoprevWonder why it happened? anyone know the back story? reply mritchie712 4 hours agoparentMaybe the both had a similar AI strategy reply rbanffy 4 hours agorootparentAnd the AIs decided they should merge. reply Narishma 1 hour agoprevTIL both Xerox and Lexmark still exist. I haven't heard those names for a couple of decades. reply donohoe 5 hours agoprevnext [5 more] [flagged] dudeinjapan 5 hours agoparentI had to Google what \"send me a Xerox\" means. reply kstrauser 3 hours agorootparentYour comment made me reach for a kleenex to wipe my eye. reply astura 4 hours agorootparentprevXerox PARC invented the desktop metaphor, the GUI, and the computer mouse. reply hobs 4 hours agorootparentprevThen you're just dating yourself at likely less thanI would think that the market for printing is shrinking with more documents being sent via the Internet https://m.youtube.com/watch?v=Ol-wwJBVncQ reply WesolyKubeczek 4 hours agoprev...and they both will sell rebadged Samsung printers happily ever after. reply ryao 3 hours agoparentHP brought Samsung’s printer division, while Lexmark makes most Xerox branded printers, so that seems unlikely. reply lexicality 5 hours agoprevoh no reply geraldwhen 5 hours agoprevThis may be devastating for the small city of Lexington, Kentucky. Seems like this was one of the only major businesses in the area. reply PAPPPmAc 4 hours agoparentI'm local, I know a ton of former Lexmark people, because they've already been all-but dead in Lexington for some time. They mostly only did R&D here for decades, and that group has been dwindling. Large groups of Ex-Lexmark folk have ended up in other local tech companies, many ended up at OpenText (via HP via Exstream, the eventual successful startup from a local serial entrepreneur that basically makes the tools to do semi-individualized bulk mailing like bills), Badger (robots for doing retail work) was founded by folks leaving Lexmark, etc. Amazon has been buying up their old buildings (long, long ago it used to be a sprawling IBM campus that did typewriters, printers, keyboards, compilers, EMI testing...) as they contract. Like much of the US, Lexington has lost a bunch of manufacturing, but IBM/Lexmark as a major entity is already long gone. It is funny that they've been bought by a cartridge cloner, and foreign private equity, and are now being bought by a competitor, they keep dying in new ignominious ways. reply mrweasel 3 hours agorootparent> many ended up at OpenText I really want to know what the deal is with OpenText (formerly MicroFocus). If you're not careful they will eventually buy your business and you will disappear. reply rrr_oh_man 2 hours agorootparent> OpenText offers cloud-native solutions in an integrated and flexible Information Management platform to enable intelligent, connected and secure organizations. That… wow… reply _xerces_ 4 hours agoparentprevI remember Lexmark just before things started to go bad for them, back when they still had a huge campus with multiple buildings, developers had their own office or shared with one other person and they owned their own huge park with a disk golf course. They even built a new building on campus for an employee daycare and acquired multiple software companies to add services on top of print. We had huge teams of software engineers, embedded software developers, mechanical engineers, electrical engineers chemists and specialists in microfluidics. We designed our own image processing ASICs, had specialists in color and perception, had a whole team dedicated to just Linux and dedicated software librarians. There was a team working just on Android. We contributed back to Linux as well as Yocto/Bitbake. The first sign of decline was when suddenly (for me anyway) they announced the closure and sale of their entire consumer inkjet division followed not long after by commercial inkjet. They sold all the inkjet assets off to a partner manufacturer company a bit like Foxconn. It was wonderful for a while and I am sad to see things get potentially worse for them. When they hired me out of college, they paid me a $5000 relocation bonus, paid a specialized company to organize the move (even offered to find me a realtor and help sell my house if I had one), paid the moving company in full, paid to have my car relocated there, paid for hotels during the move and then paid me another $5000 to cover any miscellaneous costs from moving that I might incur. They also paid the taxes for me somehow, I guess by adding extra to my paycheck. Never seen the like before or since. reply acjohnson55 2 hours agorootparentThose were the days! I have great memories playing in the basketball league and playing pickup soccer on the giant fields. I worked one internship in the color / image science lab, which was super interesting. I learned a lot about the human visual system and theories of image reproduction technology. One of the guys in the lab reached some legendary engineering status (\"laureate\", I think) for inventing a form of dithering that improved perceptual image quality. reply malfist 5 hours agoparentprevLexington has a surprising amount of employers, we should be fine. I doubt they're going to close the HQ anyway. Did you know Tempur-Pedic is Lexington grown? Fast food chain fazolis is based here, long John silver's was. Valvoline moved here a long long time ago. Hall Rogers is trying to make \"silicon hallow\" a thing so there's a lot of funding for tech companies to setup shop in Kentucky reply parpfish 5 hours agorootparentFazoli’s is still around? I don’t think I’ve seen one in 20 years reply DHPersonal 4 hours agorootparentI have a Fazoli's and a Xerox facility within 20 minutes of where I sit. I'm unsure I should see that as sign of a healthy economic situation for my neighborhood. reply joshstrange 4 hours agorootparentprevYep, land-locked Lexington, Kentucky, home of Long John Silvers though we don’t even have one in town anymore. The last one closed a few years ago. But yes, a Lexmark sale won’t make a large impact even if they shut down the HQ. There are local and remote (obviously) opportunities and the CoL is low here. reply rgreasons 5 hours agoparentprevLexington is the home of the University of Kentucky. Lexmark shuttering their plant wouldn’t be _good_ for the economy, but Lexington is first and foremost a “college town.” reply joshstrange 4 hours agoparentprevI live there and while I know people who used to work there and have friends of friends who do work there it’s not considered a major player in my mind. Toyota leaving would be a much bigger deal and we have a decent number of local tech jobs not to mention remote work from elsewhere. That’s not to say I don’t care or am happy they got bought but Lexmark has been circling the drain for a solid decade. Lexington, Kentucky will not be “devastated” by this at all. I doubt Lexmark is even in the top 10 of businesses people would name for being big players in Lexington. reply primeradical 59 minutes agorootparentToyota just announced a massive paint facility expansion so they're not going anywhere. reply tecleandor 4 hours agoparentprevLexmark has been progressively closing their facilities there for years. They even sold some buildings this year [0]. Thay have only 14 positions open for their Lexington location, so I don't think they're a huge employer there anymore. 0: https://www.opi.net/news/region/001-north-america/ninestar-offloads-lexmark-assets/ 1: https://www.lexmark.com/en_us/careers/job-locations/lexington.html reply quink 4 hours agoparentprev> https://en.wikipedia.org/wiki/List_of_employers_in_Lexington... Apparently Xerox, out of all companies in the world, is in the small list of even bigger employers than Lexmark there. reply stickfigure 4 hours agoparentprevLexington is 320k people, the second biggest city in the state. It's the only major shopping destination for eastern KY. It'll be fine. (I lived for a year in Morehead, drove to Lexington regularly) reply dgatwood 1 hour agorootparentshrugs Most of the time, all you need is Walmart and Meijer, and you can also find those in Richmond, along with a decent number of other big-box stores and clothing stores and stuff. But yeah, Lexington is definitely a popular shopping destination. Back when my grandparents lived in Richmond, we would go to Lexington for the mall (not the green roof one, the real one), because Richmond's mall was a decaying husk even in the 1990s, before Walmart moved out next to it, Sears went under, etc. I was surprised to see that it is still open, but I digress. For context, Walmart employs about as many people in Lexington's three superstores and one neighborhood market as Lexmark does. Losing 2k jobs in a city of 320k people would not be catastrophic. And most of those jobs probably don't overlap with Xerox's business anyway, so I wouldn't expect that to happen. reply numbsafari 4 hours agoprev [–] T-Rex feasts on Triceratops carcass, Asteroid Nears Earth, … all that and more Dino News just ahead… but first, why are some brontosaurus investing in these new small fuzzy creatures? reply rrr_oh_man 2 hours agoparent [–] Brontosaurus and T-Rex are separated by 80 million (!) years. To put it another way: We are closer to the the asteroid than Brontosaurus is to T Rex. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Xerox plans to acquire Lexmark for $1.5 billion, aiming to enhance its print services and expand its global reach.",
      "The acquisition, expected to close in the second half of 2025, will be financed through cash and debt, with a reduced dividend to manage debt.",
      "Regulatory approvals are pending, and the deal seeks to combine the strengths of both companies to offer a comprehensive portfolio and improve financial performance."
    ],
    "commentSummary": [
      "Xerox is acquiring Lexmark, intending to sustain and potentially expand Lexmark's operations using its resources.",
      "Lexmark, recognized for its brand and focus on growth areas such as the Internet of Things (IoT), was already a partner of Xerox.",
      "The acquisition highlights shifts in the printer industry, with Xerox seeking to enhance its market position by leveraging Lexmark's capabilities."
    ],
    "points": 164,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1734958665
  },
  {
    "id": 42491196,
    "title": "Litestack: All your data infrastructure, in one Ruby gem",
    "originLink": "https://github.com/oldmoe/litestack",
    "originBody": "All your data infrastructure, in a gem! Litestack is a Ruby gem that provides both Ruby and Ruby on Rails applications an all-in-one solution for web application data infrastructure. It exploits the power and embeddedness of SQLite to deliver a full-fledged SQL database, a fast cache , a robust job queue, a reliable message broker, a full text search engine and a metrics platform all in a single package. Compared to conventional approaches that require separate servers and databases, Litestack offers superior performance, efficiency, ease of use, and cost savings. Its embedded database and cache reduce memory and CPU usage, while its simple interface streamlines the development process. Overall, Litestack sets a new standard for web application development and is an excellent choice for those who demand speed, efficiency, and simplicity. You can read more about why litestack can be a good choice for your next web application here, you might also be interested in litestack benchmarks. With litestack you only need to add a single gem to your app which would replace a host of other gems and services, for example, a typical Rails app using litestack will no longer need the following services: Database Server (e.g. PostgreSQL, MySQL) Cache Server (e.g. Redis, Memcached) Job Processor (e.g. Sidekiq, Goodjob) Pubsub Server (e.g. Redis, PostgreSQL) Fulltext Search Server (e.g. Elasticsearch, Meilisearch) To make it even more efficient, litestack will detect the presence of Fiber based IO frameworks like Async (e.g. when you use the Falcon web server) or Polyphony. It will then switch its background workers for caches and queues to fibers (using the semantics of the existing framework). This is done transparently and will generally lead to lower CPU and memory utilization. Installation Add the litestack gem line to your application's Gemfile: $ bundle add litestack To configure a Rails application to run the full litestack, run: $ rails generate litestack:install Usage litestack currently offers six main components litedb litecache litejob litecable litesearch litemetric litedb is a wrapper around SQLite3, offering a better default configuration that is tuned for concurrency and performance. Out of the box, litedb works seamlessly between multiple processes without database locking errors. litedb can be used in multiple ways, including: Direct litedb usage litedb can be used exactly as the SQLite3 gem, since litedb inherits from SQLite3 require 'litestack' db = Litedb.new(path_to_db) db.execute(\"create table users(id integer primary key, name text)\") db.execute(\"insert into users(name) values (?)\", \"Hamada\") db.query(\"select count(*) from users\") # => [[1]] ActiveRecord litedb provides tight Rails/ActiveRecord integration and can be configured as follows In database.yml adapter: litedb # normal sqlite3 configuration follows Sequel litedb offers integration with the Sequel database toolkit and can be configured as follows DB = Sequel.connect(\"litedb://path_to_db_file\") litecache is a high speed, low overhead caching library that uses SQLite as its backend. litecache can be accessed from multiple processes on the same machine seamlessly. It also has features like key expiry, LRU based eviction and increment/decrement of integer values. Direct litecache usage require 'litestack' cache = Litecache.new(path: \"path_to_file\") cache.set(\"key\", \"value\") cache.get(\"key\") #=> \"value\" ActiveResource::Cache In your desired environment file (e.g. production.rb) config.cache_store = :litecache, {path: './path/to/your/cache/file'} This provides a transparent integration that uses the Rails caching interface litecache spawns a background thread for cleanup purposes. In case it detects that the current environment has Fiber::Scheduler or Polyphony loaded it will spawn a fiber instead, saving on both memory and CPU cycles. More info about Litejob can be found in the litejob guide litejob is a fast and very efficient job queue processor for Ruby applications. It builds on top of SQLite as well, which provides transactional guarantees, persistence and exceptional performance. Direct litejob usage require 'litestack' # define your job class class MyJob include ::Litejob queue = :default # must implement perform, with any number of params def perform(params) # do stuff end end #schedule a job asynchronusly MyJob.perform_async(params) #schedule a job at a certain time MyJob.perform_at(time, params) #schedule a job after a certain delay MyJob.perform_after(delay, params) ActiveJob In your desired environment file (e.g. production.rb) config.active_job.queue_adapter = :litejob Configuration file You can add more configuration in litejob.yml (or config/litejob.yml if you are integrating with Rails) queues: - [default, 1] - [urgent, 5] - [critical, 10, \"spawn\"] The queues need to include a name and a priority (a number between 1 and 10) and can also optionally add the token \"spawn\", which means every job will run it its own concurrency context (thread or fiber) ActionCable This is a drop in replacement adapter for actioncable that replaces async and other production adapters (e.g. PostgreSQL, Redis). This adapter is currently only tested in local (inline) mode. Getting up and running with litecable requires configuring your cable.yaml file under the config/ directory cable.yaml development: adapter: litecable test: adapter: test staging: adapter: litecable production: adapter: litecable Litesearch Litesearch adds full text search capabilities to Litedb, you can use it in standalone mode as follows: require 'litestack/litedb' db = Litedb.new(\":memory:\") # create the index idx = db.search_index('index_name') do |schema| schema.fields [:sender, :receiver, :body] schema.field :subject, weight: 10 schema.tokenizer :trigram end # add documents idx.add({sender: 'Kamal', receiver: 'Laila', subject: 'Are the girls awake?', body: 'I got them the new phones they asked for, are they awake?'}) # search the index, all fields idx.search('kamal') # search the index, specific field, partial workd (trigram) idx.search('subject: awa') Litesearch integrates tightly with ActiveRecord and Sequel, here are integration examples ActiveRecord class Author < ActiveRecord::Base has_many :books end class Book < ActiveRecord::Base belongs_to :author include Litesearch::Model litesearch do |schema| schema.fields [:title, :description] schema.field :author, target: 'authors.name' schema.tokenizer :porter end end # insert records Author.create(name: 'Adam A. Writer') Book.create(title: 'The biggest stunt', author_id: 1, description: 'a description') # search the index, the search method integrates with AR's query interface Book.search('author: writer').limit(1).all Sequel class Author < Sequel::Model one_to_many :books end class Book < Sequel::Model many_to_one :author include Litesearch::Model litesearch do |schema| schema.fields [:title, :description] schema.field :author, target: 'authors.name' schema.tokenizer :porter end end # insert records Author.create(name: 'Adam A. Writer') Book.create(title: 'The biggest stunt', author_id: 1, description: 'a description') # search the index, the search method integrates with Sequel's query interface Book.search('author: writer').limit(1).all Litemetric Litestack comes with a module that can collect useful metrics for its different components, in each component, you need to add the following to the respective .yml file (database.yml in case of Litedb) metrics: true # default is false If you have the metrics enabled, it will start collecting data from the various modules and will store them in a database file called metric.db located in the Litesupport.root folder Litemetric has an API that would enable collecting arbitrary metrics for non-litestack classes. The metrics will be in the database but currently the Liteboard is only able to show correct data for Litestack modules, displaying arbitrary metrics for other components will be included later. Liteboard Liteboard is a simple web server that provides a web interface for the collected metrics, it should be available globally, for usage instructions type liteboard -h It allows you to point to a specific metrics database file or a config file and then it will display the data in that metrics database Example metrics views: Litedb Database size, number of tables & indexes Number of read/write queries Read/Write query ratio over time Read/Write query time over time Slowest queries Most expensive queries (total run time = frequency * cost) Litecache Cache size, % of size limit Number of entries Reads/Writes over time Read hits/misses over time Most written entries Most read entries Contributing Bug reports and pull requests are welcome on GitHub at https://github.com/oldmoe/litestack. License The gem is available as open source under the terms of the MIT License.",
    "commentLink": "https://news.ycombinator.com/item?id=42491196",
    "commentBody": "Litestack: All your data infrastructure, in one Ruby gem (github.com/oldmoe)140 points by thunderbong 16 hours agohidepastfavorite20 comments otikik 7 hours agoRelevant: Just Use Postgres for Everything [1] [1]: https://www.amazingcto.com/postgres-for-everything/ reply husam212 6 hours agoparentUsing Postgres is becoming easier with the new Rails Solid [Queue, Cable, Cache] stuff. reply datadrivenangel 3 hours agorootparentAnd DuckDB's Postgres Extension / Postgres's DuckDB extension! https://duckdb.org/docs/extensions/postgres.html / https://github.com/duckdb/pg_duckdb reply Lio 11 hours agoprevWow! The performance benchmarks against redis are very interesting. Considering that Rails 8 gives you Progressive Web Apps out of the box, this is a very competitive setup for a small team. https://github.com/oldmoe/litestack/blob/master/BENCHMARKS.m... reply hipadev23 10 hours agoparentI think this speaks more to a fundamental issue in ruby’s default redis. How is it only able to do 4k/s sets? reply hiharryhere 6 hours agoprevI’ve used this for a side project. Coupled with Litestream for backups it’s awesome. I would 100% recommend. reply barefootford 11 hours agoprevThis gem is a really nice all-in-one setup! (Also worth noting that Rails 8 now supports sqlite for your application database (no more warnings about using it in prod), job queue (active job/solid queue) and cache (solid cache) out of the box.) reply axelthegerman 6 hours agoparentTHIS! Litestack seems great and probably partially paved the way for similar functionality directly in Rails 8 with the new solid* gems. Those plus litestream are a treat reply tillcarlos 5 hours agoprevYeah I used the full litestack (and litestream for backups) for my SaaS. Oldmoe is also auper helpful on twitter. Once Rails 8 came out I switched it all over to the solid* gems which pretty much did the same thing. Sqlite is awesome in the beginning, especially as there are no extra services to start (I only have a web process and a jobs process), all is sqlite. Sometimes there are db locking issues (using blazer gem, so I guess that uses a different adapter), and I had to optimize some queries. But that’s about it. The SaaS processes thousands of job daily (mission control gem is awesome) and stable so far. At one point our DB grew to 16GB because I stored all Api requests. Even then it was still fast (except queries on that api requests table). Running four sqlites definitely saved me some time which I could use for customer focused things. Highly recommend for MVPs, probably more. reply nehalem 12 hours agoprevThis looks great! Is anyone familiar with something similar for Python in general and Django in particular? reply drchaim 10 hours agoparentalso interested! reply freen 3 hours agoprevMain-Main Replication: LiteFS Streaming Backups: LiteStream Just missing logs and, well, for me, graph database. reply noelwelsh 6 hours agoprevSQLite is great, but the README is a bit disingenuous in not mentioning any of the downsides of using it (e.g. scalability concerns if you go beyond what a single box can handle.) reply eterps 34 minutes agoparentAt what scale would this become a significant concern? While some tech startups aspire to achieve Google-level success, such massive scale is extremely rare in reality. I'm curious about the threshold where issues begin to materialize. reply thunky 2 hours agoparentprev> README is a bit disingenuous My guess is that they assume the reader is already familiar with sqlite. And \"lite\" is in the name Litestack. And it says in the second sentence of the description: \"It exploits the power and embeddedness of SQLite\". Personally I think that's good enough. reply elif 5 hours agoprevLol sorry but if you are going to claim to be a faster cache than redis you need to bring receipts reply bdcravens 2 hours agoparentTrue for any performance claim, in any library, in any language. reply Alifatisk 8 hours agoprevGorgeous, bookmarked it! reply nc 13 hours agoprevLooks fantastic! reply sroerick 12 hours agoprev [–] Very interesting reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Litestack is a Ruby gem that consolidates multiple data infrastructure components into a single package for Ruby and Ruby on Rails applications, using SQLite.- It offers functionalities like a SQL database, cache, job queue, message broker, full-text search engine, and metrics platform, enhancing performance and reducing costs by eliminating the need for separate servers.- The gem integrates with Fiber-based IO frameworks for optimized CPU and memory usage and is open source under the MIT License, with contributions welcomed on GitHub."
    ],
    "commentSummary": [
      "Litestack is a Ruby gem that provides a comprehensive data infrastructure and integrates effectively with Rails 8, which now supports SQLite for databases, job queues, and caching.",
      "Users appreciate Litestack's performance, particularly when used with Litestream for backups, though scalability concerns with SQLite remain for larger projects.",
      "There is interest in similar solutions for Python and Django, as users compare Litestack's performance claims, especially against Redis, which require further validation."
    ],
    "points": 140,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1734921141
  },
  {
    "id": 42495077,
    "title": "Fogus: Things and Stuff of 2024",
    "originLink": "https://blog.fogus.me/2024/12/23/the-best-things-and-stuff-of-2024/",
    "originBody": "read read or learn more Send More Paramedics λ λ λ Fogus' Thoughts on life, programming, and thinking ❤ c clj erl pl frink fth cl org pure icl qi ❤ Follow me on Twitter... or RSS... Run this blog in mobile 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 The best things and stuff of 2024 Dec 23, 2024 Great things and people that I discovered, learned, read, met, etc. in 2024. No particular ordering is implied. Not everything is new. also: see the lists from 2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011 and 2010 Great postsarticlestalks read/watched ELITE: The game that couldn’t be written from Alexander the ok – Elite was one of my favorite games on my Commodore 64 1,000,000 years ago and so I’m a sucker for articles on this gem. If you’re interested, also check out the annotated C64 source code. 1 The Rich History of Ham Radio Culture by Kristen Haring – I missed out on the Ham radio craze and only recently learned about its rich history. This article is a good overview and starting point if you’re interested in learning too. Get to Know Your Japanese Bathroom Ghosts by Eric Grundhauser – Describes the interesting Japanese cultural folklore around bathroom ghosts. The History of WordStar by Abort Retry Fail LLC – A great historical article about one of the most influential software suites ever created. Additionally, the comments are a goldmine of additional information and corrections and should not be skipped. Combinatory Programming by zdsmith – Describes combinatorial programming using motivated examples — a technique that’s surprisingly scarce in articles about the topic. Philip K. Dick’s Favorite Classical Music by Open Culture – Discusses PKD’s love for classical music and the references to composers and their works in his fiction. The post also, includes an 11-hour classical music playlist for your listening pleasure. Goodbye, Kory by Andy Looney – The world lost Kory Heath, a game designer whom I admire immensely. I’ve talked about his magnum opus Zendo on this blog before and have run numerous play sessions over the years. He was single-handedly responsible for hundreds of hours of enjoyment around my home and within my group of friends. The world is much the poorer without him in it. RIP. 2 Most viewed blog posts by me On method values, part 1 – We released Clojure 1.12.0 this year and so I wanted to write about one of the features that I worked on. Method values are symbolic references to Java methods that can be used in value contexts and the design and implementation of this feature was interesting enough to talk about. The feature has been generally well received by the Clojure community. Favorite technical (and technical-adjacent) books discovered (and read) And so FORTH by Timothy Huang – I found this long out of print Forth tome via inter library loan and enjoyed it immensely. It’s a nice blend of the ideas in Brodie’s Thinking Forth and something like Geere’s Forth: The Next Step. It was a sad day when I had to return this beauty back to the library because I could have used another read or two at least. BASIC and FORTH in Parallel by S.J. Wainwright – This style of book is exactly the kind of book that I would one day like to write. While the specifics of any such book would be different, the central conceit is perfect. That is, this book uses BASIC to create a simple stack machine and Forth interpreter and then presents simple Forth programs exercising them. Favorite non-technical books read Butcher’s Crossing by John Williams – Follows Harvard drop-out Will Andrews as he escapes to the American frontier with a wad of cash to find adventure and “an original relation to nature”. Andrews eventually finds Miller who is more than happy to help the young man part with his money in an attempt to find a hidden Colorado valley filled with buffalo that may or may not still (if it ever did) exist. The book follows Miller and Andrews’ (plus a skinner Schneider and driver Hoge) trek throw the frontier and describes in harrowing detail their tribulations. I could not stop reading and finished the book in a weekend. This one demands multiple reads to really absorb the nuance. The Spectral Link by Thomas Ligotti – Contains two stories by Ligotti: “Metaphysica Morum” and “The Small People”. The first is quite different than most of Ligotti’s work that I’ve read so far. It follows a self-described “metaphysical mutant” and blends overtly dark humor with an underlying pessimistic philosophy centered on a theme of euthanasia. “The Small People” is a dream-like exploration of paranoia and isolation. Both stories are a good introduction to the range in Ligotti’s work if you’re interested in checking him out. The Corvo Cult by Robert Scoble – Frederick Rolfe (aka Baron Corvo) was an little-known Edwardian author who is often remembered more for his bombastic personality than his fictional works. This book talks about the rise and growth of the still active “Corvo Cult” — an obscure literary fandom. In many cases, Rolfe’s fervid devotees matched the controversial author in eccentricity, but the true fascination lies in the broad range of people drawn to his eclectic works. Number of books written or published 0 Number of programming languages designed 0.5 Favorite music discovered The Paragons – At some point I became interested in the roots of ska and The Paragons were the best group that I discovered during my explorations. That’s All! by Sammy Davis Jr. – *A fantastic performance from a master of the vocal form. The songs are brilliant but the banter between songs will keep me listening into the distant future. Favorite films discovered Withnail & I – Sam Aaron recommended this film to me years ago but I only managed to watch it in 2024. It’s a great example of a dry comedy following a couple of screw-ups and their misadventures. Jodorowsky’s Dune – A documentary about the most influential film that never was. Requiem for a Dream – I’m probably the last person in the world to watch this relentless survey of despair. Not for the faint of heart. Favorite podcasts Will Radio – Will Byrd started the year promising a KiloTube of videos (i.e. 1024 videos) in 2024 and it’s been a blast following along! There’s no one quite like Will and so any chance that I can get to experience more of him I will jump on. Eros + Massacre – Another podcast triumph by Samm Deighan surveying the weird world of psychotronic cinema. Favorite programming languages (or related) I hacked on/with on my own time Joy – Joy is a mindfrak of a programming language in the concatenative functional language family. The core of Joy is beautiful and among the foundational programming languages in my opinion. Forth – Sticking with the concatenative family in 2024, I continued to explore Forth. Interestingly the language is incredibly rich in history and conducive to a wide range of techniques and paradigms. I’m unsure if I’ll ever find the opportunity to use Forth in anger, but I will say that I should come out of my explorations a stronger programmer and program designer. Programming languages used for work-related projects Java – Working deep in the Clojure compiler means that much of my work in 2024 was in Java. Clojure – 2024 marks the 15th year3 as a full-time Clojure programmer and the 1st year as a full-time Clojure core developer. ClojureScript – Less-so now than when I was consulting full-time but I occasionally dig into explore the implications of changes to Clojure on CLJS. Datalog – The Datomic flavor of Datalog is the flavor of choice for database access, be it in-process or in the cloud. Again, my day-to-day usage is limited, but I have my share of personal databases hosted on Datomic. Programming languages (and related) that I hope to explore more deeply Joy – There’s a mountain of deep information on Joy that I would like to devour in 2025.4 Mouse – Yet another concatenative language to explore that’s long-dead but still has some lessons to teach one such as myself. POP-11 – Another dead language that was designed to support AI applications in the 70s and 80s. I love the idea of exploring the language and the suite of applications that built up around it. Favorite papers discovered (and read) Recursion Theory and Joy by Manfred von Thun – Joy’s underlying reliance on combanatory programming manifests deep in the language even to the degree that recursion in the language is implemented in userspace via recursive combinators. This paper describes the “Joy Way” and its relationship to recursion. A Simple Applicative Language: Mini-ML (PDF) by D. Clement and J. Despeyroux and T. Despeyroux and G. Kahn – Presents a beautiful definition of ML language and its compilation to an abstract machine. Still haven’t read… I Ching, A Fire upon the Deep, Don Quixote, and a boat-load of sci-fi Favorite technical conference attended Clojure/conj 2024 – This was the first Clojure conference that I played a somewhat active part in organizing. Let me be clear, my part in the matter was minimal at best, but it did provide me a window into the complexities of organizing a conference. The conference itself was a blast and it was great to meet old and new Clojure friends as well as colleagues! Favorite code read Restrained Datalog in 39loc by Christophe Grande – I’ve learned over the years that if Christophe writes a technical article then it behooves me to study it deeply. The highlight of the year from Christophe was his simple, yet rich, Datalog implementation in 39 lines of Clojure code. It’s clear that 39 lines of Clojure goes a long way and especially so when a master of the language plays in it. Post-Apocalyptic Programming by Serge Zaitsev – I love the central conceit of the post, summarized as “what technology could/should we create in the absence of modern computing niceties?” The post starts with a CPU emulator, builds a language for it, and motives its decisions along the way. There’s a brilliant hard science fiction story in here somewhere, I can feel it. MINT – MINT is highly inspirational to me as a lesson in minimal programming language design. Based on Forth, MINT makes various design decisions and trade-offs to remain small and fast. Life-changing technology “discovered” Nothing this year. State of plans from 2023 Clojure 1.12 – Released in early September and one of the biggest releases in years as far as feature additions go. Go much deeper down the concatenative rabbit-hole – An unmitigated success! Publish even more non-technical writing – My research into the Corvo-related archives stored at Georgetown University was a success. However, my efforts in writing up my findings has stalled. Plans for 2025 Clojure 1.13 – Thinking around the 1.13 release is ongoing and we’d like to get it out sooner rather than later. Stay tuned. clojure.core.async next – We’ve laid the groundwork for a new version of core.async and released it as version 1.7.701. We’d love to leverage JDK 21+ virtual threads to vastly simplify core.async’s implementation and have started along this path in earnest. Simplify my blog – I’d love to move away from WordPress in 2025. Juxt – Juxt is my exploration in functional concatenative language design built on the JVM. It’s not yet clear to me if or when I would ever release this into the wild, but the explorations have been great fun and I’ve used Juxt as a vehicle for finding relevant books and papers.5 That said, most of my programming time is spent maintaining and evolving Clojure, but there are rare moments of time that I can spend on Juxt, and I plan to continue to do so in 2025. 2024 Tech Radar try: Boox Go 10.3 tablet – recommended by many colleagues adopt: Blank Spaces app – helps to avoid phone brain-drain assess: TypeScript – What does it buy me over JS? hold: Zig – This looks like a dead-end for me stop: Joy of Clojure 3rd edition – Another edition is unlikely but hopefully something else may come of this work… this is an evolving situation. People who inspired me in 2024 (in no particular order) Yuki, Keita, Shota, Craig Andera, Carin Meier, Justin Gehtland, Rich Hickey, Nick Bentley, Paula Gearon, Zeeshan Lakhani, Brian Goetz, David Nolen, Jeb Beich, Paul Greenhill, Kristin Looney, Andy Looney, Kurt Christensen, Samm Deighan, David Chelimsky, Chas Emerick, Stacey Abrams, Paul deGrandis, Nada Amin, Michiel Borkent, Alvaro Videla, Slava Pestov, Yoko Harada, Mike Fikes, Dan De Aguiar, Christian Romney, Russ Olsen, Alex Miller, Adam Friedman, Tracie Harris, Alan Kay, Janet A. Carr, Wayne Applewhite, Naoko Higashide, Zach Tellman, Nate Prawdzik, Bobbi Towers, JF Martel, Phil Ford, Nate Hayden, Sean Ross, Tim Good, Chris Redinger, Steve Jensen, Jordan Miller, Tim Ewald, Stu Halloway, Jack Rusher, Michael Berstein, Benoît Fleury, Rafael Ferreira, Robert Randolph, Joe Lane, Renee Lee, Pedro Matiello, Jarrod Taylor, Jaret Binford, Ailan Batista, Matheus Machado, Quentin S. Crisp, John Cooper, Conrad Barski, Amabel Holland, Ben Kamphaus, Barry Malzberg (RIP), Kory Heath (RIP). Onward to 2025! :F I also recommend and excellent YT video “The Making of ELITE”. ↩ Dave Chalker also wrote about Kory on his blog at “Remembering the Master: An Inelegant Eulogy for Kory Heath“. ↩ This is strictly my work-life time. My total use of Clojure has been longer. ↩ Sadly the death of Manfred von Thun brought the death of Joy with it. The literature the language is indeed deep but it’s finite and has stopped growing entirely. I would like to help fix this stagnation if I can in 2025. ↩ You can see the current Juxt bibtex on Github. ↩ Related posts: The best things and stuff of 2023 Goodbye Sir Arthur Clarke No Comments, Comment or Ping Comments are closed. Copyright © 2002 - 2011 by Fogus (license information) read about my policy on affiliate links Theme heavily influenced by Ryan Tomayko [Log intop]",
    "commentLink": "https://news.ycombinator.com/item?id=42495077",
    "commentBody": "Fogus: Things and Stuff of 2024 (fogus.me)124 points by janvdberg 3 hours agohidepastfavorite12 comments leetrout 48 minutes agoSome commentary about the radar... try: Boox Go 10.3 tablet Agree. Avoid reMarkable™. Hostile to the community and better options are out there or on the way including Boox, Onyx and the new Daylight Computer. https://daylightcomputer.com/product https://www.zdnet.com/article/best-note-taking-tablet/ --- adopt: Blank Spaces app No need to pay money on ios. Clear off all the icons and when you need something swipe down in the middle of the screen and open search or swipe over to the alphabetical listing. Windows Phone was way ahead of its time on this one. --- assess: TypeScript – What does it buy me over JS? This one is a little bit flame bait... at the cost of a build step you get a much more reasonable development experience for JS targets with reliable types. The problem is smart people want to flex their brain a lot more than their restraint (where are my grug brains at?) and type astronauting makes the experience much worse. As with all things there is a balance however TS should be \"Adopt\". --- hold: Zig – This looks like a dead-end for me I keep looking at Zig and playing with it but I am so productive with Go and Python for things that need to be fast enough Zig doesn't have much that I need. However Mitchell Hashimoto is using it to great success for his new MacOS terminal emulator which makes me think I just haven't tried using it in the appropriate domain... maybe a raytracer is in my future. https://mitchellh.com/ghostty https://github.com/ghostty-org reply wonger_ 4 minutes agoparentGhostty is cross-platform, but it feels extra nice on macOS. https://gpanders.com/blog/ghostty-is-native-so-what/ reply MisterKent 19 minutes agoparentprevRemarkable seems to be a pretty hackable device? They give SSH ability and have stated they're not removing it. I think it's a very good balance of letting the community do weird things while they focus on their core product. I say this as someone who has not purchased one, but is considering it. reply 8n4vidtmkvmk 24 minutes agoparentprevI was trying zig yesterday. There seems to be a bunch of churn in build.zig; changed APIs. ChatGPT wasn't able to help. Rocky start but I did eventually get GLFW running, and the c-interop seems good. Also got some freezing in my IDE maybe from ZLS. So it still seems a bit rough at the moment but I'm still optimistic about it for things like game dev and compiling to wasm. reply jwhitlark 11 minutes agoprevNerd sniped on the first entry. The first sentence of the first entry. The first minute of the video in the first sentence. The first comment in the video, etc. If anyone was going to manage recursive, fractal, nerd sniping, it would be fogus, of course. reply voidUpdate 3 hours agoprevThis is the second time I've seen the Alexander the OK video about Elite referenced here in two workdays. I love that channel so much, he does excellent dives into the history of more obscure vintage computers, such as the D-17B Minuteman guidance computer and the CK37 used in the Viggen jet reply gandalfgeek 25 minutes agoprevLove Fogus, but zero mention of how LLMs impacted programming in 2024? reply bachmeier 7 minutes agoparentIt may not have been important or interesting to him, or maybe he just figured the 10-digit number of articles written on the topic in 2024 (most by LLMs) was enough. reply zem 8 minutes agoprevthe linked article on combinatory programming is lovely reply hubraumhugo 43 minutes agoprevAny good alternatives for \"adopt: Blank Spaces app\" on Android? reply mamoul 26 minutes agoparentCheck Niagara Launcher: https://play.google.com/store/apps/details?id=bitpit.launche... reply datpiff 2 hours agoprev [–] The History of Wordstar is there for the second year in a row :) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fogus reflects on his experiences and discoveries from 2024, covering a wide range of topics including programming, media, and personal interests.",
      "He discusses his work with the programming language Clojure and his future plans, such as exploring concatenative languages and simplifying his blog.",
      "Notable mentions include the game Elite, the history of Ham radio, and Japanese bathroom ghosts, along with acknowledgments of people who inspired him."
    ],
    "commentSummary": [
      "Fogus highlights tech trends for 2024, including devices like the Boox Go tablet and critiques reMarkable™ for its community approach.- TypeScript is recommended over JavaScript for an improved development experience, while Zig is considered less productive than Go and Python, despite its potential in niche areas.- The article notes the under-discussed impact of Large Language Models (LLMs) on programming in 2024 and mentions combinatory programming and historical tech like Wordstar."
    ],
    "points": 124,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1734967835
  },
  {
    "id": 42489072,
    "title": "German watchdog orders Sam Altman's biometric ID project World to delete data",
    "originLink": "https://www.euronews.com/next/2024/12/19/german-watchdog-orders-sam-altmans-biometric-id-project-world-to-delete-data",
    "originBody": "Now playing Next No Comment Osaka shrine hosts annual laughter ritual",
    "commentLink": "https://news.ycombinator.com/item?id=42489072",
    "commentBody": "German watchdog orders Sam Altman's biometric ID project World to delete data (euronews.com)123 points by belter 21 hours agohidepastfavorite56 comments emporas 20 hours agoDoesn't Worldcoin produce Zero Knowledge Proofs of biometric data? If yes i do not see what kind of personal data the ledger may hold. It holds the proof of their data, not their data. If some people are not aware of ZKP here is a short really like [1]. [1] https://www.youtube.com/shorts/c6gpq9nKogo reply tim333 2 hours agoparentIt's all a bit confusing, but I think there are two separate parts to the process of joining and using Worldcoin. To join up they scan your iris and keep a record of the iris scans to stop you applying for multiple accounts. It's anonymous apart from the iris scan - they don't need your name or dob or anything like that. The second part is once you have an account you can use it to prove you are the person with that account, and this is probably where the zero knowledge proofs come in. In this second stage your irises are irrelevant, they only come up when first applying. The accounts are very much like crypto wallet public/private key accounts I think although they try to hide the details from you a bit in the app. reply emporas 58 minutes agorootparentThanks for the info. That's what i thought the process would be. Although i do consider worldcoin to be irrelevant, and eventually to fail at some point in the future. reply coppsilgold 12 hours agoparentprevIf they use ZKP's correctly then what you are proving is membership in a set. However, they are collecting iris scans which means they are the ones building the set and therefore own all the data about all the members of that set. To do it properly the government should be the ones who build the set. There are also problems with respect to stolen identities. If addressed, it's not fully private - you are given a token which you must reuse, making you pseudonymous. Or else a single stolen identity may be used infinitely without anyone realizing it. reply emporas 1 hour agorootparent>If they use ZKP's correctly then what you are proving is membership in a set. >However, they are collecting iris scans which means they are the ones building the set and therefore own all the data about all the members of that set. Totally agree, but any other official of any kind can do the same and create their own membership set. A mayor of a town can create a similar set of citizens. A football's team coach can create their own membership set, of players and fans. I don't see why that particular membership set is more special than any other. > To do it properly the government should be the ones who build the set. To do it properly governments around the world should agree on a standard or two, and use that standard worldwide for the next 50 to 100 years. See for example screws and screwdrivers [1] which are effectively identical for so many years. When a standard is established for identity, for ownership, for ownership transactions, for signatures, for contracts, for passports and many other things then it is government's job to keep the data safe and private, we agree on that. As soon as identities for example are a technological screw, then we don't want incompatibilities between governments. > There are also problems with respect to stolen identities. The ZKP does not need to be the identity by itself. Another identity can be tied to the ZKP proof, and use it's children identities for everyday use. They implemented something like that for Python to avoid supply chain attacks [2]. [1] https://www.youtube.com/watch?v=MXWSn8rMeEo [2] https://news.ycombinator.com/item?id=42136375 reply shafyy 20 hours agoprev> Those three codes, which are extremely difficult to break are then stored in databases that are owned by third parties, which include the University of Berkeley, Zurich, Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) university and NeverMind What do they mean by \"Zurich\" here? reply javaunsafe2019 20 hours agoparentI guess university reply Luc 20 hours agorootparentETH Zurich I assume, their Chief Economist is professor there. reply echelon 21 hours agoprevI was raised in the evangelical south to conservative parents. World(coin) sounds like it's right out of the plot of some Sunday morning preacher's sermons [1] about Revelations and the \"mark of the Beast\". Central organization scanning people and controlling how they transact? Literally the antichrist's M.O. [1] https://youtu.be/zjHrExOM-ww reply AnarchismIsCool 20 hours agoparentI'm a flaming atheist but holy fuck do I get uncomfortable with universal IDs and the growing drumbeat of identity verification. Borders, passports, IDs, personal documentation, it's all just a modern caste system. Yes, it's uncomfortable to think of a world without them but to me, after global warming, digital class slavery is probably the second biggest issue of our time. So much of the world works because bureaucracy is inefficient and non-omniciant, just like humans, yet so many people want the world to be one big TSA checkpoint where everyone must be unnaturally perfect at all times. It's utopian thinking that is leading us towards a type of hell I don't think any of us can even begin to imagine. reply tim333 2 hours agorootparentBorders and passports have been with us for a while now and are not really a tech thing. I'll give you modern KYC stuff is annoying - it seems you have to provide proof of identity, address where you live, and often now where your money came from which can be tricky if you haven't documented it. Worldcoin is actually a step away from the KYC situation where you can prove you are a unique person but not have to give them your name address and finances etc. But so far that's a bit theoretical - all the financial institutions I deal with want KYC stuff and don't accept Worldcoin. reply oytis 20 hours agoparentprevTogether with the vision of the future where all labour is automated and controlled by a few megacorps it paints a truly apocalyptic picture reply btown 20 hours agoprevPress release primary sources from the German watchdog, BayLDA: https://www.lda.bayern.de/media/pm/pm2024_08_en.pdf (EN) https://www.lda.bayern.de/media/pm/pm2024_08.pdf (DE) Quote from the officlal English version: > As a result, despite the improvements already introduced, adjustments are still required to bring the company's data processing in line with the applicable provisions Among other things, the company will be obliged to provide a deletion procedure that complies with the provisions of the GDPR within one month of the decision taking effect. In addition, “Worldcoin” will be obliged to provide explicit consent for certain processing steps in the future. Moreover, the deletion of certain data records previously collected without a sufficient legal basis was ordered ex officio. The company has already received the decision and has informed us that it is going to appeal it. The allusion to \"improvements already introduced\" would seem to refer (though I'm uncertain of this) to https://world.org/blog/announcements/worldcoin-foundation-un... - which was described there as \"reinforced after conversations with data protection authorities focused around further biometric template protection, particularly the Bavarian Data Protection Authority (“BayLDA”), the Worldcoin Foundation’s Lead Supervisory Authority in the EU.\" Cryptographic systems that ensure no single party can access data at rest, even if that party were to be compromised, corrupted, or forced to reveal secrets by law enforcement, are absolutely incredible technical achievements - but it seems that, at least in this case, they are insufficient solutions in the eyes of EU regulators. (Not a lawyer, this is not legal advice.) I hope the stance towards cryptographic erasure evolves thoughtfully over time in general, but World's approach here, beginning to collect data for seemingly unlimited purposes before having a completed system for SMPC, was never going to be one that would lend itself towards establishing positive regulatory precedent. reply TiredOfLife 19 hours agoprevIsn't Germany a country where you have to publish your name, home address and phone if you have a blog or twitter/bluesky. reply oytis 1 hour agoparentNormally not for Twitter, but for a standalone blog, yes, you do. reply 29athrowaway 20 hours agoprevThe history of modern technology is the history of running away from government regulation. reply ptek 19 hours agoparentWhich is why when people make it to mars it will thrive. No taxes, no debt and no government regulation. Just build, build, build baby. Hopefully 3D printing and materials science will have some cool tech in the future otherwise it could be a slow start. reply tim333 2 hours agorootparentYou'd be entirely dependent on Musk City with its regulations for having resources to stay alive. At least on Earth you can go live in the woods or become a tax exile or some such. reply NeutralCrane 18 hours agorootparentprevAssuming humans make it to Mars is already a leap, but assuming a government won’t form almost immediately, if somehow one doesn’t exist from the beginning of colonization, seems even less likely reply brnt 14 hours agorootparentprevIf you understood TESCREAL, you wouldnt think this a good thing. reply blackeyeblitzar 21 hours agoprevWill they delete it for real? I feel like many companies either just hide the data or have it sitting in older backups, leaving everyone’s privacy vulnerable. reply tim333 2 hours agoparentI'd quite like if they deleted my iris data as they've given me about $300 so far and if so I could open another account or two and do it over. reply Cyclone_ 21 hours agoparentprevI usually wonder if they do that as well. In some cases it may be hard to depending on how data is stored. In vertica, a database I worked with would never truly delete data on disk. reply Y-bar 21 hours agorootparentOnly marking as “deleted” while indefinitely keeping it is illegal in the EU/EEA. The GDPR _requires_ a hard deletion in cases like this, but allows a grace period of a few weeks for the deletion to propagate throughout systems. reply adastra22 21 hours agorootparentThere are backup systems that are write-only. What’s to be done then? reply mtmail 20 hours agorootparentFacebook used an encryption key per user for their backups. For deletion they just delete the encryption key which makes the data unreadable. There was an article years ago about their cold storage infrastructure, Blueray discs if I recall. https://www.datacenterfrontier.com/cloud/article/11431537/in... reply polskibus 21 hours agorootparentprevYou could replay this backup, and skip problematic record when writing new copy of the backup. Delete old backup. What’s important is to keep such log of „records to be deleted from backup”. reply cyberpunk 20 hours agorootparentHow does one do this with a 20TB SQL database? Our approach would be to add some filters into our 'restore' pipeline which drops the problematic data should we ever attempt a restore, but I don't think it's good enough, and we have to maintain a list of user id hashes or such to power the filters. Edit: I mean, in a way that won't eat a lot of costs. I can imagine a malicious group opening and demanding deletions for 1000s of users which would mean a deletion job running on a large number of these 20TB backups, say 100 daily backups and for multiple users? reply martijnarts 20 hours agorootparentYou don't need to delete data instantly, you just need to do it within a reasonable timeframe. So batching data deletion requests and running a clear out once a week should be fine. You may even be okay to just reply to the user that you've deleted all active copies of the data and it'll be fully gone when your backups expire in 30 days. IANAL tho. reply Y-bar 2 hours agorootparentprev> a malicious group opening and demanding deletions for 1000s of users I am not aware of any provision within GDPR that allows anyone else but the individual person (and courts) to request deletion of their personal data. So I think your example is highly unlikely to ever happen. reply adastra22 18 hours agorootparentprevThat cost real money and requires literally throwing out the old backup (which may or may not be destroyable). Think optical media and stuff like that. reply post-it 20 hours agorootparentprevIt's imprudent to use technology that makes it impossible to comply with the law. reply williamdclt 20 hours agorootparentprevI’ve had a cursory look into that recently (just a simple googling) and it seems that it’s considered OK to keep the data in backups. Which does seem weird… but to be fair, it would be near impossible to delete from backups as they exist today, it would be a law that can’t be practically applied. reply tzs 15 hours agorootparentprevDepends on which country's GDPR authorities you ask. At one point the French authorities said you don't have to delete data from backups, the Danish authorities said you have to delete when technically feasible, and the UK authorities said you had to put the data \"beyond use\" which has been interpreted to mean that if you ever restore from the backup you have omit the \"deleted\" data. My guess is that most places go with not taking any active steps to delete things from the backups themselves, counting on media rotation to eventually overwrite the data. When restoring they omit anything that is on the \"should be deleted\" list. reply unit149 20 hours agorootparentprevStore everything on a decentralized P2P server for privacy enhancing technologists (PETs) to deconstruct. reply fh973 20 hours agorootparentprevEncrypt it and delete keys. reply loriverkutya 20 hours agorootparentprevSimple. Destroy the backup physically. reply im3w1l 20 hours agorootparentprevEncrypt write-once backups and store the keys on rewritable backups. reply dboreham 20 hours agorootparentprevThe acid bath. reply noprocrasted 20 hours agorootparentprevIllegality matters only if you get caught - and when it comes to the GDPR it turns out even \"getting caught\" isn't actually a problem, as the continued existence of Facebook, Google, the data broker industry, etc demonstrates. reply delusional 21 hours agorootparentprevThat seems like nonsense. Software cannot constrain the physical world. I could touch the bits on the drive itself, or I could physically destroy the hard-drive. Both would \"truly delete\" the data. reply dietr1ch 21 hours agorootparentGood luck deleting data from my 5th backup drives that I didn't tell you about. It's not hard because destroying a hard drive is hard, it's hard because you need to find not one, but all of the drives that are likely replicated and distributed around the globe already if you ever intended to do business with that data. reply post-it 21 hours agorootparentIt's not a technical problem to solve, it's a legal one. If there is a crushing penalty if data that was supposed to be deleted shows up one day, companies will find a way to delete it. reply im3w1l 20 hours agorootparentOne issue I foresee is that you can't legislate bugs away. reply post-it 20 hours agorootparentA bug is just a mistake, and the legal system already deals with mistakes in a variety of ways. reply okanat 20 hours agorootparentprevUmm you can. You can force companies to pass their code through an examination (even by a third party) and define a procedure of ensuring strict data hygiene. If they cannot pass each year, they will be subject to fines. reply pfoof 21 hours agoparentprevNow imagine backups stored on tapes. How many companies would resort to rewinding all of them in search of this single record. reply rollcat 20 hours agorootparentEasy: - Rotate old tapes to store the freshest backup (according to retention policy) - Store row ID for each deletion request - Replay deletions during restore Either way you want (or already have) a scrubbing procedure to import production data into a staging environment, so this is not a technical issue. reply onetokeoverthe 20 hours agoparentprevAgree. The file locker site i use said my account was deactivated due to inactivity. But after a simple email pw reset all my uploads are back online. Makes me aware any deletion i do is probably NOT done server side. reply oytis 20 hours agorootparentAt least before GDPR it was a common wisdom among backend people that deleting things is just not worth it. I remember when I joined an otherwise cloud-focused team as an embedded engineer and suggested that we add a way to delete an account it was made clear to me that I am asking for an impossible thing. I hope GDPR has managed to change something reply onetokeoverthe 20 hours agorootparentRight. Most all users want the restore option much more than a clean delete. reply 23B1 21 hours agoprev [6 more] [flagged] adastra22 21 hours agoparentSurely we are not talking about Worldcoin? reply duskwuff 21 hours agorootparentFun little thought experiment: what differentiates World(coin) from any other \"airdrop, pump, and dump\" crypto scheme, beyond the eye scanning gimmick? reply adastra22 18 hours agorootparentNothing? The whole thing is a scam. reply delusional 21 hours agoparentprevThat's not coincidence. He's the head of one of the most \"transformative\" companies because he's a brazen liar. reply mplewis 20 hours agoparentprev [–] Yeah, what a shame. Which company did you mean here? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The German watchdog has mandated the Worldcoin project, led by Sam Altman, to delete biometric data, citing privacy and data handling concerns.",
      "Worldcoin uses iris scans to prevent duplicate accounts, but critics worry about the extensive personal data collected by the company.",
      "The situation underscores broader issues regarding digital identity systems and privacy, with the German authority emphasizing compliance with GDPR (General Data Protection Regulation) for data deletion."
    ],
    "points": 123,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1734901354
  },
  {
    "id": 42492753,
    "title": "I built a platform for discovering and sharing Chrome extension collections",
    "originLink": "https://webextension.net/collections",
    "originBody": "WebExtension.net WebExtension.net Extensions Themes Collections Analytics Feedback Login WebExtension.net WebExtension.net Extensions Extensions ChromeOS Apps ChromeOS Games Themes Explore Collections Analytics Top Publishers Popular Categories Most Updated Trending Extensions Trending Themes Stats Feedback Bookmarks Collections Chrome Extension Collections & Curated Lists Discover hand-picked collections of the best Chrome extensions and themes, curated by our community Create Your First Collection Productivity Shared by X/Tools 1 extensions 0 themes Public 4 hours ago Social / Tools (X/Twitter, BluSky, LinkdIn, Reddit) Shared by X/Tools 2 extensions 0 themes Public 4 hours ago 4 hours ago AI / LLM Productivity Extensions that improve productivity using AI (LLMs) Shared by Ryan Atkins 2 extensions 0 themes Public 4 hours ago 4 hours ago Free Resume Creation Using AI to create a resume from your LinkedIn profile. Use my LinkedIn to create a resume. Shared by Rady 1 extensions 0 themes Public 6 hours ago 6 hours ago Japanese Learning Learn Japanese as you browse the web Shared by Nuenki 2 extensions 0 themes Public 7 hours ago Essential Extensions for Screen Capture Useful extensions for screen capture, featuring essential tools for screen recordings, screenshots and color picking. Perfect for design and marketing teams. Shared by CyberHusky 3 extensions 0 themes Public 7 hours ago 7 hours ago My Automation Tools Local (In-Browser) AI Translation for WhatsApp Web - X Bot Remover Shared by vanrohan 2 extensions 0 themes Public 8 hours ago 8 hours ago AI Extensions AI extensions I use daily Shared by wong2 3 extensions 0 themes Public 9 hours ago YouTube Essentials Extensions every YouTube user should have. Shared by Rudolph 3 extensions 0 themes Public 9 hours ago 9 hours ago work Shared by SeedAI Chris 0 extensions 0 themes Public 15 hours ago Showing 1 to 10 of 14 results 1 2 WebExtension.net WebExtension.net Track and analyze Chrome Web Store extensions with comprehensive analytics and insights. Contact hey@webextension.net © 2024 WebExtension.net. All rights reserved. Disclaimer: WebExtension.net is not affiliated with Google or the Chrome Web Store. All product names, logos, and brands are property of their respective owners. All extension data is collected from publicly available sources.",
    "commentLink": "https://news.ycombinator.com/item?id=42492753",
    "commentBody": "I built a platform for discovering and sharing Chrome extension collections (webextension.net)112 points by trungpv1601 10 hours agohidepastfavorite54 comments seanwilson 5 hours agoFor what it's worth, if you create a new profile for Chrome extensions, they won't have access to your regular browser profile. I do this for web developer extensions that typically need a lot of powerful permissions. So I have a Chrome profile that's full of web developer extensions, so they're isolated from private and security sensitive stuff like email and banking. Similarly, you can do this by installing Chrome Beta or Chrome Canary for use with different extensions you don't want to take a risk with. reply franze 10 hours agoprevI found it more trustworthy to code my own extensions via ChatGPT. These are the last 2 ones. I pretty much dont care if people use them, as the mostly fulfill my own usecases. https://chromewebstore.google.com/detail/comparative-chatgpt... https://thisismy.franzai.com/ The cost of (small) software is fast approaching 0, and it can be faster now to code your own solution, instead of looking for one that nearly mostly fulfills your usecases. reply richrichardsson 9 hours agoparent> I found it more trustworthy to code my own extensions I used to have a ton of little single use extensions that I barely ever used but thought at the time of installation, ”that could be useful one day\". Then I started noticing I was liking really random shit on Facebook. That immediately ceased when I uninstalled all but Bitwarden, Leechblock and uBlock Origin. I will never install another random 3rd party extension again reply collinvandyck76 9 hours agorootparentWhich of your single use extensions was causing you to like things on Facebook? reply aragonite 8 hours agorootparentNot the GP, but just last week Google automatically removed a single use extension (https://readermode.io) from my browser after flagging it as malware (as I recall the extension updated itself a day before the removal). The extension has also been taken down from the Chrome web store (https://chromewebstore.google.com/detail/reader-mode/llimhhc...) though Google hasn't provided any details about what it was doing that led to the removal. reply nomilk 8 hours agorootparentI think the asymmetry in payoffs explains this, since a bad actor who baits and switches their extension could do massive damage to users. So google try to catch this behaviour and inevitably have some false positives (extensions labelled malware that actually aren't). The cost of a false positive is annoyance. The cost of real malware getting through could be your bank balance. reply hackinthebochs 3 hours agorootparentAutomatic extension updates is a stupid practice. The attack surface for a legit extension is minimal, while being huge for a malware update. I'm against almost all automatic software updates in general, but browser extensions take the cake for having an obscene cost/benefit ratio. Chrome won't even let you turn it off. Personally I extract and load all my extensions in developer mode. reply freehorse 5 hours agorootparentprevhttps://web.archive.org/web/20240927002632/https://chromeweb... There are several complains in the reviews, though it all seems a bit bizarre in that the issue was with an opt-in so-called \"eco-mode\" that basically was throwing pop-ups with affiliate links. reply amelius 8 hours agorootparentprevI heard (on HN) that often an extension changes owners just before turning bad. Curious if that was the case here. reply cynicalsecurity 8 hours agorootparentprevTons of them. Custom download page, to avoid seeing the old ugly Chrome'a download shelf at the bottom of the screen. reply JadeNB 3 hours agorootparent> Tons of them. Custom download page, to avoid seeing the old ugly Chrome'a download shelf at the bottom of the screen. Just to make sure—you seem to be replying to https://news.ycombinator.com/item?id=42493152, which currently says: > Which of your single use extensions was causing you to like things on Facebook? Are you saying tons of your single-use extensions caused this Facebook liking, and a custom download page was one of them? Or was this meant to be a response to https://news.ycombinator.com/item?id=42492881? (Or maybe something's wrong with parent links today. For me, on the main page, they are now turning into anchored links that don't seem to go where intended, which wasn't happening yesterday.) reply polotics 8 hours agoparentprevHaha exactly! Except I didnt bother to publish mine on chromewebstore, it's just on gitlab: https://gitlab.com/natural_aliens/geminiwrap_plugin ...it makes Gemini-Chat voice dictation a bit more useful. My other one, the remove-youtube-shorts, is almost an one-liner, so I didnt even publish it it's too trivial I think. Everyone just make your own! reply dsauerbrun 4 hours agorootparentI'm gonna need the remove youtube shorts one reply vunderba 1 hour agorootparentAn extension doesn't make sense for a simple DOM manipulation - I'd recommend installing Tampermonkey then finding a script like the following: https://github.com/hallzy/remove-youtube-shorts/blob/master/... reply DaSHacka 3 hours agorootparentprevYou can do it natively inside ublock origin if you don't want to install an extra extension (often the case for a surprising number of simple extensions, actually). I used to block it myself with my own filter, but after YouTube changed things up and broke it I've just been using someone else's filterlist and it works the same. https://github.com/gijsdev/ublock-hide-yt-shorts reply 77pt77 3 hours agorootparentprevBut did you \"code\" them with chatgpt also? reply franze 8 hours agorootparentprevYeah, creating the marketing screenshots and filling out the publishing form takes longer nowadays then coding the actual extension. reply polotics 8 hours agorootparentAlso if anyone wants to uses an extension I would much rather they make the minuscule effort to create a local folder, put the files in there, and load the extension's folder with the chrome extension mgr. Maybe even they can peek at the source code... I really don't see why I'd have to push my name, address, email etc on some google storefront and submit myself to spam reviews at the big \"google-internet\" party in the cloud. reply oefrha 7 hours agorootparentprevI was updating my Chrome Web Store extensions to MV3 the other day. Had to fill a fair bit of new stuff. Then one dead simple extension I haven’t touched in a decade got its update rejected due to “description provided is insufficient to understand the functionality of the item”, even though anyone who bothered to seek it out absolutely would have no trouble understanding what it does (according to analytics on the dashboard, there are a grand total of ~20 active users and a couple hundred throughout its lifetime), never mind what those lay reviewers think. The review process is really dumb. reply amelius 8 hours agoparentprevI'm too scared to download extensions, so I use bookmarklets (on Firefox). But I like your suggestion of using ChatGPT to write extensions. reply RustySpottedCat 7 hours agorootparentTampermonkey scripts with chatgpt is even faster. Adding a functionality to a website just by pasting the site's html in chatgpt and in 2min I get what I need. reply DaSHacka 3 hours agorootparentMaking a simple tool for a site or two is the perfect use case for a userscript manager like TamperMonkey/ViolentMonkey (FOSS alternative), I think making your own extension is somewhat overkill Easier to share with others, too reply croes 7 hours agoparentprevYour cost but not the cost. reply vermayash8 6 hours agoprevInteresting, I find it useful. A couple of features you could think about: 1. Is there a feature to create a \"pull request\" to the collection maintainer to propose adding some extensions to their collection? Otherwise, there would be several public shared collections for the same use-case and it may become scattered. 2. I'd like to be able to favorite / like / star a collection, and that to be used as a signal to search results ranking. Another adjacent domain to expand could be Tampermonkey scripts. reply trungpv1601 4 hours agoparentThank you for the feedback. I will add it to the roadmap. reply dizhn 9 hours agoprevNot a good day to have Honey in the list :) reply nomilk 9 hours agoparentWas curious so just searched. Apparently Honey would try to get the best coupon codes on the web, but they started partnering with businesses to give (say) 10% off via a Honey-specific discount code (e.g. HONEY10), but Honey would ignore other (possibly greater) discounts, thus lulling users into a false sense of security that they were getting the best deal when they often weren't. reply shreddit 8 hours agorootparentIt’s even worse. They steal from other promoters. Say you watch a LTT video and use one of their affiliate links. If you have honey installed they will replace the link with their own affiliate link and cash the promotion bonus without any promotion by themselves. reply CodesInChaos 7 hours agorootparentOf the three bad things they've been accused of, I'd consider that by far the least. Selling tracking data is an invasion of privacy. Deliberately not showing better discounts violates their core value proposition. Replacing deferral links doesn't hurt the user, and isn't much different from blocking ads. reply 369548684892826 7 hours agorootparentAs a user that might use referral links to support the youtube channel, I do feel in an indirect way this does hurt the user reply microbass 8 hours agorootparentprevAnd, they highjacked referral links, ensuring they got referral commission, not the original referrer. reply firtoz 7 hours agorootparentprevSigh, and I was just thinking about installing it. Time to find another one, or perhaps it will also fall to Goodhart’s Law. reply shreddit 5 hours agorootparentI’d really like to know what exactly you are looking for? There is no such thing as “free” and no browser extension will give you something for free. You are paying, one way or the other… reply andelink 3 hours agorootparentMy thoughts as well. Given their business model, any Honey replacement will be engaging in the same sort of behavior. Never seemed worth it to me. reply firtoz 3 hours agorootparentprevI want to see and/or collect discount codes for things reply JadeNB 3 hours agorootparentprev> I’d really like to know what exactly you are looking for? There is no such thing as “free” and no browser extension will give you something for free. You are paying, one way or the other… Sometimes there are really-free things. Old-style open-source software is a collection of such things. Extensions, at their beginning, were too, and some of them still are. As far as I know, for example, there's no 'gotcha' in uBlock Origin (although there is the 'gotcha' of knowing to look for them instead of the myriad other solutions that are non-free). reply handsclean 8 hours agorootparentprevBefore the rest of these abuses, Honey was blatantly tracking users and selling that data, which I think is a good example of how privacy abuse is often a canary of generally immoral behavior. reply shreddit 5 hours agoparentprevI’m actually impressed by honey. They could have either just sold the user data, or only switched the referral links, or just showed their users the “best” coupons. But they went for all of it. I’d have wanted to be in the room when the higher ups chose this path. reply d3vr 6 hours agoparentprevFor anyone else outside the loop, MegaLag released a video [1] yesterday exposing the shady practices by Honey 1: https://youtube.com/watch?v=vc4yL3YTwWk reply RadiozRadioz 9 hours agoprevAny plans to support FireFox extensions? reply trungpv1601 9 hours agoparentYes. I'm working on it reply demaga 6 hours agoprevNeat site. Nice to see my extension that I just published a couple of days ago here! It doesn't even have any users yet. How did you obtain this info? Is there an API for that? https://webextension.net/chrome/extension/epjjmfojjmbgignfnk... reply anonymous344 8 hours agoprevI used to use extensions in the chrome, but then I noticed that my dev server private urls were being botted. Only way for them to get leaked was virus, google or some extension. For me it seemed like the Window Resizer -addon was leaking every url I visited. And it's not the only addon that seem to use google analytics. wtf? reply deanc 6 hours agoprevWhat UX library did you use here? It looks like bootstrap but isn't (as far as I can tell). reply vallode 5 hours agoparentLooks like the website is built with Laravel[1] using Livewire[2] (Alpine JS on the front-end) and the UI library used is Flux[3]. [1]: https://laravel.com/ [2]: https://livewire.laravel.com/ [3]: https://fluxui.dev/ reply trungpv1601 5 hours agoparentprevShout-out to FluxUI.dev reply cynicalsecurity 8 hours agoprevThe less of Chrome extensions, the better. reply hieunc229 7 hours agoprevInteresting extension reply trungpv1601 4 hours agoparentThanks Hieu reply ned99 8 hours agoprevI like the UI. Neat. reply trungpv1601 5 hours agoparentShout-out to FluxUI.dev reply 2Gkashmiri 5 hours agoprev [–] Please promote Firefox extensions. Chrome and Google are on a dangerous trajectoryin ruining peoples lives. Please dont be part of the problem in promoting chrome / chromium reply JadeNB 3 hours agoparent> Please promote Firefox extensions. Chrome and Google are on a dangerous trajectoryin ruining peoples lives. Unfortunately, Firefox is on a trajectory of \"imitate what Chrome's doing.\" I certainly no longer trust Mozilla as I once did. They're probably better than Google, but they've made it clear they're going to make decisions for market share over respecting their historical ideals. reply trungpv1601 5 hours agoparentprev [–] Yeah. I'm working on it reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WebExtension.net is a platform that curates collections of Chrome extensions and themes, emphasizing productivity, AI tools, and social media.- Users can explore various categories, including popular and trending extensions, with analytics provided for Chrome Web Store extensions.- The platform is independent and not affiliated with Google or the Chrome Web Store, sourcing all data from public sources."
    ],
    "commentSummary": [
      "A new platform has been developed for sharing collections of Chrome extensions, sparking discussions on privacy and custom extension development.- Users express interest in supporting Firefox extensions and provide feedback for platform improvements, while also raising concerns about data misuse by certain extensions like Honey.- The platform's user interface receives positive feedback, and future updates are anticipated."
    ],
    "points": 112,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1734941748
  }
]
