[
  {
    "id": 42845091,
    "title": "We're bringing Pebble back",
    "originLink": "https://repebble.com/",
    "originBody": "Thank you, Google. You didn&#x27;t have to, but you did. We (the Pebble team and community) are extraordinarily grateful.I wrote a blog post about our plans to bring Pebble back, sustainably. https:&#x2F;&#x2F;ericmigi.com&#x2F;blog&#x2F;why-were-bringing-pebble-backWe got our original start on HN (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=3827868), it&#x27;s a pleasure to be back.",
    "commentLink": "https://news.ycombinator.com/item?id=42845091",
    "commentBody": "We're bringing Pebble back (repebble.com)2443 points by erohead 22 hours agohidepastfavorite626 comments Thank you, Google. You didn't have to, but you did. We (the Pebble team and community) are extraordinarily grateful. I wrote a blog post about our plans to bring Pebble back, sustainably. https://ericmigi.com/blog/why-were-bringing-pebble-back We got our original start on HN (https://news.ycombinator.com/item?id=3827868), it's a pleasure to be back. tomaskafka 20 hours agoAwesome! The first Pebble absolutely fascinated me by having a hackable, C-running watch on my wrist. I vividly remember spending days fine tuning the heuristics of a simple step detection algorithm in the first watchface where I thought “seeing your daily step count next to time sure is awesome”. And later, tens of thousands of people thought so as well this was one of the signs what the health-tracking wrist device is about to become. It was incredible that even the first model allowed you to run a 30 samples per second accelerometer sampling and classifying the movement, 24/7, and still lasted days. No other watch offers a similar level of hackability. And as the time progressed, Pebble became the first platform to get Weathergraph my graphical weather watchface. Weathergraph was then ported to Garmin (as Pebble shut down), and then to Apple Watch widget (as it became a capable platform with the introduction of standalone watch-apps in watchOS 6), and then to iOS app & widget, where it now lets me live a life of indie developer, after a serie of corporate design/PM/dev jobs. Thank you for that, Eric & Pebble team. I still keep the developer edition Pebble with my name printed on the back (great touch!) in my shelf and heart, and will always remember Jon Barlow, one of the best and most helpful developer advocates I ever encountered. And kudos to the whole dev team. The watch and companion app was rock stable, always staying connected, the calendar always being in sync, watch apps installed quickly and reliably the things that 10x larger companies struggled with for years were nailed here almost from day one. Godspeed! PS: What a mishap to shut the company down shortly after a release of Pebble 2. It nailed the experience of a lightweight watch, with the most contrasty BW reflective screen I have seen, and buttery smooth animations (while Garmin still renders menus in like 8-10 fps on their MIP screens 10 years later). So small and lightweight, I’d love everyone to try it on, and compare with 2024 smartwatches. reply brokenengineer 6 hours agoparent> And as the time progressed, Pebble became the first platform to get Weathergraph my graphical weather watchface. > Weathergraph was then ported to Garmin (as Pebble shut down), and then to Apple Watch widget Thanks for bringing Weathergraph to life. I found it on the Pebble and used it religiously until I experienced enough challenges with Rebble to switch to a Garmin watch. I was thoroughly chuffed when I saw that you had brought Weathergraph along with you. Are you saying I'd have to get an Apple Watch to get the third-generation Weathergraph? ;-) reply ilrwbwrkhv 12 hours agoparentprevIndeed, I forgive Google about 40 of their killed projects for giving us pebble in return. reply eitland 2 hours agorootparentForgiven, yes, but I still cannot trust them. A decade of messing with my search results (they only cannot do that anymore since I switched to Kagi) and the killing of Google+ (still interested if anyone have alternatives. Despite its problematic start it became the only social network I ever enjoyed). reply akho 11 hours agorootparentprevnot the reader though reply dingaling 10 hours agorootparentIt's strange to see the naivety around Reader. If Facebook had created it, people would recognise the initiative to gate-keep, regulate and curate the Wild West of RSS. \"They're trying to keep you inside their walled garden!\" reply akho 8 hours agorootparentReader made tons of people use RSS who otherwise wouldn’t, and who now don’t. It did not live long enough to become a villain (though it certainly would have — there is no reason why G wouldn’t have added recommendations, an algo feed, and all it brings). Therefore it’s remembered well. reply mattmaroon 4 hours agorootparentReader died because people were switching to social media. Reader also had no vendor lock in at all. There’s no network effect like Facebook. There’s no massive infrastructure demands like Google. No corporate sales process like Oracle, Microsoft, etc. You could very easily create a competitor and after it died, few even tried to replace it. I miss it but not enough to find another RSS reader. You’re never really the villain if there are viable alternatives and next to no switching cost. It would have been hard to make that product evil. reply starkparker 3 hours agorootparent> Reader died because people were switching to social media. Which especially sucks, because its friend-of-a-friend model for making comments visible on shared items was better for discovering interesting people, constructively limiting the social impact of popular posts, reducing the dangers of unintentionally poor posts, and disincentivizing trolling than any social network has implemented since. For the few people who even knew Reader had a social network—a group which certainly didn't include Google—it was a better social network than any of the ones credited with killing it. reply mattmaroon 2 hours agorootparentI think it was just as much that the people who wrote blogs were switching to writing in the walled gardens of social media as it was the consumers. But yeah, I greatly prefer the days of blogs and RSS readers. Now you have Substack and Medium and such, which are pretty decent. reply immibis 1 hour agorootparentprevThe Fediverse is a bit like that. Anyone can post replies on things, but they don't spread through the whole network, only to people who are following the replier, and people on the replier's server, and the person who posted the thing being replied to. reply akho 1 hour agorootparentprevReader died because Google was switching to social media, not because people did. > It would have been hard to make that product evil. You can just treat RSS data as content to bring in new users, while building your lock through other means. Sharing, recommendations, algo feeds, commenting, and, eventually, posting. Then apply demands to RSS feeds, leveraging your audience, and lock out new ones. This is all very easy and the playbook is well understood at this point. reply samastur 6 hours agorootparentprevBecause we miss what Reader was and not what it could have become. As a more prolific blog writer at the time I also liked that their bot would include number of people who were subscribed to my blog in their User Agent. reply mrmanner 4 hours agorootparent> As a more prolific blog writer at the time I also liked that their bot would include number of people who were subscribed to my blog in their User Agent. Something I generally appreciate with Google: The level of craftsmanship and the amount of elegant designs like this they come up with. (There are also… other things, but their standards are high compared to many competitors.) reply rpastuszak 7 hours agorootparentprevThese were different times, people hadn't burned themselves on Embrace, extend, and extinguish that much yet. reply mattmanser 9 hours agorootparentprevIt was from a time where Google's ethos was still \"Don't be evil\" and generally speaking the naked greed triggered by AdSense hadn't infected the rest of the company yet. So I think a lot of nostalgia is not just for the reader but also for the company Google used to be. I wonder if in 20 years time there will be the next generation of programmers sneering over vapour-ware Google products while middle managers still buy them products because \"no-one ever got fired for buying Google\". reply chr15m 11 hours agorootparentprevNever forget. reply szundi 10 hours agorootparentprevnot the reader indeed reply thebruce87m 3 hours agorootparentprevWhat happens when they kill it again? reply ksenzee 2 hours agorootparentOpen source licenses are irrevocable. You can quit providing the source, but you can't sue someone for using a copy they already took. reply hinkley 4 hours agorootparentprev40 down, 400 to go. reply jakecopp 16 hours agoparentprev> Weathergraph was then ported to Garmin (as Pebble shut down), and then to Apple Watch widget I don't think I was a particularly early user of Weathergraph but when I finally had to retire my Pebble Time I only considered platforms that had your watchface. Thanks very much for the attention to detail! reply tomaskafka 7 hours agorootparentThank you! reply hinkley 4 hours agoparentprevThere was a post here a couple weeks ago about smart phones with a very fancy reflective display and I immediately thought how good that would be on something like a pebble. reply smvanbru 3 hours agoparentprevI loved weathergraph. Was my favourite watch face on my pebble and garmin, until my eyes got too old and I couldn't read the smaller text anymore. Now I use a watch face for older guys like me, but miss the graph. reply angristan 9 hours agoparentprevWeathergraph is the best weather app I've ever used, thank you so much for it! reply tomaskafka 7 hours agorootparentThank you! I hope to keep it that way :). reply nym3r0s 7 hours agoprevThe primary use for a smartwatch for myself (and many of my family, friends) is fitness and health tracking. Card payments, notifications, WatchFaces etc. are all secondary. Basically what Whoop is doing with their strap but minus the subscription model. I know a ton of people who tried the whoop but felt it was extremely pricey and didn't have the accuracy of an apple watch. I would be happy to pay ~$400-500 up front for hardware that integrates with Apple Health and provides solid, reliable health tracking without a need for a subscription. And by health/fitness features expected would be sleep tracking, activity (gps), heart rate, Sp02, skin temperature sensors, fall detection. Then secondarily additional things like ECG/EKG, apnea, AFib detection The in-accuracy of some of the devices in the market is why I still choose to remain with my Apple Watch. This youtube channel may help understand a consumer's perspective on health accuracy https://www.youtube.com/@TheQuantifiedScientist reply qzx_pierri 21 minutes agoparentI have a Garmin Forerunner 255 (which does everything you requested and much much more). I used to be a Fitbit guy, and the sleep tracking and data is 10x better than Fitbit, with no subscription. The battery life is about 20 days. The Forerunner 255 can be found on Amazon right now for $250. Mind you, I also used to own an Apple Watch. Garmin is the best, and second place isn't close. reply lawn 0 minutes agoparentprevI agree. And for those who recommend a Garmin, an Oura ring or similar: I have a Garmin and it's great but I cannot wear it during martial arts (grappling). I also can't wear a ring doing weight lifting or when I'm grappling. I had a Whoop and it was really good for tracking martial arts (the boxers with the holder was super comfortable) but alas it was expensive and the sleep tracking with it in the boxers was really poor. reply mrzool 6 hours agoparentprev> I would be happy to pay ~$400-500 up front for hardware that integrates with Apple Health and provides solid, reliable health tracking without a need for a subscription. That price point would make it unaffordable for the majority of the world’s population. Shouldn’t we try and make health monitoring and fitness tracking more accessible? That was one of Pebble’s biggest benefits. reply orzig 5 hours agorootparentYou have to start somewhere, and then economies of scale can work their magic. The most inspiring example in the last 30 years is probably photovoltaic solar panels. reply toss1 1 hour agorootparentprevOf course, lower cost is almost always better. But just because advanced devices with (currently) costly components have higher costs is no reason to not create them. If something works and meets a need, the costs of components and manufacture usually come down as engineering and manufacturing progresses on the learning curve and competition comes into play. (Not true when there is a captured market where extractive pricing becomes the norm, but those are the exception in consumer goods) reply jgalt212 6 hours agorootparentprevTrue, but people who tend to prioritize personal health also tend to be richer than the average bear. reply mtlmtlmtlmtl 5 hours agorootparentWell yes, because prioritising personal health is expensive. High quality, healthy food is much more expensive per calorie than hyperprocessed, high calorie/low micronutrient, carcinogenic food. Gym memberships are pretty expensive, even without any of the personal coach and flex location fluff. And for someone with a lower paying job, time is more scarce as well, in addition to the job itself likely being more harmful to health. And of course, health care is expensive, and even if you live somewhere with socialised medicine, access to specialists is a lot easier and more expedient if you can afford to pay extra. It's not like poor people don't care about their health, they just have fewer options and less time to spend on it. I support anything that can bring more options to more people. reply jgalt212 5 hours agorootparent> High quality, healthy food is much more expensive per calorie than hyperprocessed, high calorie/low micronutrient, carcinogenic food. Largely true, but frozen vegetables are inexpensive. And they are healthier than fresh vegetables. Veggies are the key to good health for the poor. reply hooli_gan 4 hours agorootparent> And they are healthier than fresh vegetables. Citation needed reply veidr 2 hours agorootparentYes, citation needed (and also probably unlikely to be forthcoming or validated) but the rest of the comment stands, so let's just ignore this weakest/most-dubious claim. reply jgalt212 4 hours agorootparentprevfrom SERP page 1 https://www.ncbi.nlm.nih.gov/search/research-news/4060/ reply dancc 4 hours agorootparentIt sounds difficult to make a definitive statement based on the findings of the referenced paper: \"Overall, the vitamin content of the frozen commodities was comparable to and occasionally higher than that of their fresh counterparts. β-Carotene, however, was found to decrease drastically in some commodities.\" It was occasionally lower, too. reply coryfklein 6 hours agoparentprevCan someone explain the point of the health tracking features for watches? I have an Apple Watch and I do exercise regularly, but I found that the annoyance of starting and stopping workouts was bothersome so I turned the feature off. Is it about inducing more exercise? Or is it the timer aspect that it records how long your workout is? (in which case I don’t understand why it’s so much better than a stopwatch?) For me, and those around me, the fitness feature seems vestigial and has very little impact on actual fitness levels of the individual. reply bluGill 5 hours agorootparentA few years ago my mom looked at her smart watch (fitbit in her case) and it said she did 4 hours of aerobic exercise that day and her heart was elevated. She worked a retail job, so while on her feet all day, it was not aerobic. She immediately went to the doctor despite no other symptoms and they found cancer (which was then treated and so she is alive today). It isn't clear how much longer it would have been before that cancer was detected, but it would have been longer and so treatment would have been delayed which is generally bad. reply parpfish 4 hours agorootparentnext [9 more] Is this in the US? Because i can’t fathom just being able to casually drop in to visit a doctor like this. Any time I need to talk to a doctor, I have phone tag with their understaffed receptionist for a few days, then we set up an appointment 4-6 months in the future. reply bluGill 3 hours agorootparentUS. There are walk in clinics all over that take people first come first serve (once in a while the receptionist says no way and sends you to an ER where they take people in priority order). Generally they are open 9am-8pm 7 days a week, though it varies by location. These are called urgent care and for are things that you need urgent but non-emergency care for you typically get an antibiotic or some such treatment (depending on what you have) and are sent home. Depending on what you have sometimes you are told to make an appointment with your regular doctor, sometimes sent to a hospital. My regular doctor I do need to make an appointment to see. Typically I can get an appointment in about a week anytime I call, though normally I just make the next appointment as I leave the last one and so they are months out. reply parpfish 2 hours agorootparentWait… you’re saying she went to urgent care and that urgent care did a cancer screening? Urgent care is great, but they usually don’t have MDs. There are nurses that can give you stitches or a course of antibiotics but a cancer diagnosis is way out of their expertise reply bluGill 1 hour agorootparentI don't know what happened after arriving. They likely did sometests and then found a hospital to admit her. Then the hospital did more tests. The important point here is her fitness tool alerted her to something and that startee the chain that eventually found the real problem. reply jhatax 1 hour agorootparentprevEvery provider / system is different. My wife is a physician who works urgent care shifts over the weekend to serve patients as described above. These are in addition to her M-Friday routine. She is part of the Kaiser system. This is systemwide for Kaiser, so my wife’s weekend engagement isn’t a one-off. reply RobKohr 19 minutes agorootparentprevSay what people will about the cost of medicine in the US, if you have money and good health insurance, you can get pretty much any medical need taken care of immediately. reply SpaghettiCthulu 3 hours agorootparentprevI'm curious, where do you live where that happens? I've heard similar complaints from fellow Canadians online, but it has never been an issue for me. reply richjdsmith 1 hour agorootparentIt certainly does sound like Canada. I am one of the lucky ones and do have a family doctor and it is still a minimum of 6 weeks for me to book an appointment to see him. If I am really in trouble, I can go to his clinic as a drop-in (along with dozens of others) and wait, hoping somebody doesn't show for their appointment. The state of healthcare in Canada is...bad. really bad. Canada's healthcare system is effectively using long wait times as a form of passive rationing, where delays lead to natural attrition of patients. This is a poor solution to address the per-capita physician shortages by decreasing demand rather than increasing access to care. reply parpfish 2 hours agorootparentprevUSA, and that’s with good insurance and a primary care provider reply dspillett 3 hours agorootparentprevI walk and run on trails a fair bit¹ so my watch is mostly a route planning/tracking/recording tool. When training for something I will often at least consider its recommendations and those are based partly on the health readings as well as the training load it has tracked from treks/runs. Though TBH other than that the health tracking is unimportant compared to it being a GPS device that can track for a day or more constantly without needing to talk to a phone (which sits in my pack/pocket in low-power mode to conserve battery unless/until I need it for something). A don't even tend to pay attention to the heart-rate stats (though I do know people who use those features to directly guide their training). I know a few people whose use pattern is very similar to mine, near identical in fact, so I think it is fairly common amongst people who walk and/or run more than the average person. [1] Less than I'd like ATM, the rest of life like ill family and my own burn-out² are getting in the way, but I'm getting myself back into it [2] The key reason I'm trying to get back at it: herfing myself around the green stuff³, is something I find beneficial to my mental state as well as physical. [3] or even the “mostly brown stuff” as it can be this time of year. reply coryfklein 3 hours agorootparentCan you unpack a little more by what you mean when you say you use the watch to plan your route? Do you mean to say you're using the watch – with that tiny display – to choose whether you run over hill A or around town B? And what is the point of the tracking? Do you take time out of your day to review your past runs for some reason? My completely uninformed self is imagining a person sitting at their desk thinking, \"Oh yeah, that was a good run. Look at that part where I turned the corner onto Market Street! Hah, I remember that, good times.\" And realize this sounds so ridiculous I must certainly be misunderstanding the point of the tracking. reply BigGreenJorts 2 hours agorootparentI cycle and I'm certainly taking time to review past bike rides. Especially the fixed routes I have. I'm seeing the speed overall, but also reviewing segments that are hard, address specific skills/challenges, or where I hit my top speed typically. I try to compare this to sleep and diet changes between specific rides, but am also keeping track of general trends (typically my goal is faster over time, but there are some nuances to that.) reply coryfklein 2 hours agorootparentOkay that makes sense. I can see how the tracking features would be really valuable to you, or really anyone that is very fitness minded. Probably folks like you make up a minority – though significant – market segment? Of my friends, many of them are fit, but I suspect only a few are engaging with their fitness data on the level you are. I think some aspect of it must be aspirational. Man sees the advanced fitness features and thinks, \"this is the thing that will get me looking like Vin Diesel!\" and it feels productive to hit that Record Workout button and so the watch makes you feel more athletic in the same way that chatting on Slack can feel like you're being productive when you're not actually changing your behavior on a fundamental level. reply BigGreenJorts 3 hours agorootparentprevI've been a Strava user from before the fitness trackers were big and using a watch to track location instead of my phone is pretty big for me. The additional biometric data (namely heart rate and blood oxygen levels) are a plus. I've also had life long difficulties with sleep, and the sleep data is nice to keep my physical experiences grounded in reality, \"Why am I so tired today, I slept so well. Oh no, actually I only slept 5 hours last night.\" On the note about how annoying it is to start and stop activities, I strongly agree, tho quick start and auto track have eased the pain a lot for me. I cycle everywhere and really like to keep track of the total distance I do in a month and my watch just automatically tracks that for me. reply barnabee 3 hours agorootparentprevFor me: Measuring resting heart rate, SpO2, etc. passively and tracking these over time and the impact of my fitness regime on them Sleep tracking Tracking pace and heart rate on a run, ride, etc. and (a) using it to manage my pace during the activity; and (d) use it to measure how my performance changes over time Navigation and tracking when hiking/skiing I don't have so much interest in the tracking during, say, a gym workout. I agree with the GP about wanting a subscription-less Whoop as I like to wear \"real\" watches so a band on the other wrist is perfect (\"double fisting\" watches VC-style is not an option I'm willing to entertain). I did like my Pebble enough to include it in the rotation of \"real\" watches though, too. reply nym3r0s 5 hours agorootparentprevI believe the value add is a combination of both factors ability to measure and (as a consequence) induce more exercise. An example here is how I made sure my parents are getting their exercise in by making completing their Move rings and 10K steps every day. This pushes them to take a walk in the evening instead of doom scrolling / watching TV. Another example Check trends like resting heart rate to see if my body has fully recovered from covid19, SP02 at night indicating potential sleep apnea etc. reply bdavbdav 3 hours agorootparentprevWhat kind of exercise? I run, row, lift and do various group PT classes. Running it’s essential for pacing and target HR Zones, same for rowing, and on the group PT they’re very variable in terms of what they’re targeting, and it’s good to know if I’m short on (an)aerobic workouts. reply coryfklein 2 hours agorootparentI guess I'm not so scientific? I do similar exercise as you (minus the PT), though my goal is mostly to avoid being sedentary, so as long as I feel like I'm pushing myself I feel good. I did use the watch once to see what each HR zone feels like and I thought that was a useful calibration, but as a normal dude where fitness is just one small aspect of my life I wouldn't buy an Apple Watch specifically for that feature. I'm not saying it can't be helpful with fitness, but responding to OP saying that fitness/health is the primary feature for themselves and many of their family/friends. For me, the primary features are: 1. Telling time 2. Putting my notifications on my wrist 3. Starting timers with Siri 4. Setting up reminders with Siri Surely there are folks where the fitness features provide the critical marginal feedback that gets them up and moving, to the point where owning vs not owning the watch is a big deal! But reading the comments here, it sounds like it's very useful for people who are quite scientific about their fitness (HR zones, tracking, etc), and tangentially useful (rings remind folks to get off the couch/stand) for other folks at risk of a sedentary lifestyle. It doesn't all add up to me as \"fitness is the main reason many people use the watch!\" reply nradov 1 hour agorootparentSure, the fitness tracking features aren't essential. It's absolutely possible to train to an elite level purely by perceived exertion without using any devices at all. But the device makes everything easier and more convenient, especially if you're trying to target specific energy systems or follow a structured training plan. Some of us also enjoy sharing activities with our friends on Strava. reply bdavbdav 1 hour agorootparentThe Strava effect is a huge motivator for many. reply servercobra 5 hours agorootparentprevIt's very helpful for race training. Speed work targets various paces, endurance I want to hold a certain pace over time, etc. I (and many people) have a tendency to go much too fast for long distances, so pace targets help me stay where I should and be able to go the distances I want. When I was just getting started I didn't use much technology, but training's gotten a lot easier with my Apple Watch. reply pandaman 6 hours agorootparentprevIt's very useful for aerobic exercise (running, swimming, cycling etc), where you want to pace yourself and keep your heart rate and/or speed/tempo in a certain range. reply coryfklein 3 hours agorootparentNow that you mention it, I have used it for this purpose myself! I turned it on when I was on the elliptical because I wanted to calibrate on what running in \"Zone 2/3/4/5\" felt like. And once I'd done that once I didn't really need to do it again, though I expect it will be helpful to recalibrate every few months as my capacity changes. reply erikig 5 hours agorootparentprevAlso, you can set your watch to auto detect and start your workout tracking with a a subtle notification. reply bdavbdav 5 hours agoparentprevSurely a Garmin is what you want? No subscription, 7 day+ battery life, smart watch features are there but largely secondary. reply patanegra 6 hours agoparentprevIt sounds a lot like you might appreciate Garmin watch, too. reply scrapcode 5 hours agorootparentMy thoughts were that he was describing my Garmin Fenix pretty closely. GPS on-device means I can use all features un-tethered from a phone. I don't use the sleep tracking so I'm not sure how well it does in that arena compared to the competition. reply nym3r0s 3 hours agorootparentprevIt was an option I considered, but the accuracy of the sensors was way worse than apple for the price. reply KerryJones 2 hours agorootparentBased on what data? I've found them to be as accurate or more accurate (and the data I've seen says the same) except around sleep tracking, where Garmin is worse. But it's not hard to create a function to correct for that if you really care about it, it is good enough unless you need vanity metrics reply nradov 1 hour agorootparentprevThat's an odd objection. Garmin's optical heart rate sensor accuracy might be slightly worse than Apple's under certain limited conditions, depending on how you test. But anyone who really cares about precise heart rate uses a chest strap anyway. reply nradov 1 hour agoparentprevAn SpO2 sensor can be nice to have but it's useless to most people. A healthy person near sea level will always have an SpO2 near 100%. It's only really useful to people with medical conditions like sleep apnea, or athletes at high altitude. And even then you won't be able to get continuous monitoring. Current wrist SpO2 sensors require the wearer to hold still for a while in order to get an accurate reading. I turned mine off to save battery life. reply albrewer 2 hours agoparentprev> The primary use for a smartwatch for myself (and many of my family, friends) is fitness and health tracking. It also needs to be a good watch. Don't forget to not fail at that. reply wodenokoto 2 hours agorootparentI’m with parent. It really doesn’t need to be a good watch. I have my phone on me. My watch doesn’t need to tell me the time. reply dakiol 5 hours agoparentprevIt would bother me so much to track my health on a daily basis. Too much paranoia. It’s like looking at the stock price every day. I much prefer to track my health twice or so per year. reply nym3r0s 5 hours agorootparentI agree that a single outlier is more stressful but many times these aren't medical grade devices anyway so the actual data you're looking for is the trend (not the absolute values. A solution could be to measure it but not really track / visualize it day to day. reply BigGreenJorts 2 hours agorootparentDollar cost averaging health metrics :) reply hmmm-i-wonder 2 hours agoparentprevI really hope you get your wish, I share it. I gave up with tracking because the short life of a smart watch meant many of the more critical times (sleep/sleep tracking) would be interrupted by charging or dead batteries. I just want a band/strap that is focused on sensors and battery life WITHOUT a subscription. Until that happens I'm staying out of the ecosystem entirely. reply valicord 2 hours agorootparentGarmin is what you're looking for. Battery lasts 1-2 weeks. No subscription. reply hmmm-i-wonder 1 hour agorootparentI'll take another look at them thanks. I'd still prefer a band over a smart watch. When I initially looked at them they were very bulky and only had a few days battery life, but its been a while. reply pbronez 5 hours agoparentprevYou are describing the Oura ring, at least for the passive tracking. It has great health tracking fidelity and battery life. It won’t do GPS, but other than that tracking is great. reply alexbouchard 6 hours agoparentprevFrom most watch market positioning I'd assume this to be true. However for me it's the exact opposite, the watch is a tool to cut phone use. All I care about is LTE and the minimum I need to get around the world. SMS, calls, WhatsApp, Gmaps. All existing decent looking watch have atrocious battery life to offer all the health features. reply trabant00 5 hours agoparentprevIf you watch TheQuantifiedScientist you must have found out by now that optical sensors on the wrist have no chance of ever being accurate enough for health and fitness tracking. No matter how much they massage their algorithms they simply don't have the right sensors at the right positions on the body. At the same time the fitness features add cost, bulk, the uncomfortable sensor bump and cost battery life. The original Pebble didn't have any of that and in my opinion was better for it. I also see little point in competing with the already existing numerous options for fitness tracking, even if you only look at the ones without a subscription. reply iamacyborg 4 hours agorootparentModern optical sensors are pretty dang accurate on garmin watches. reply nradov 1 hour agorootparentprevI've watched it and TheQuantifiedScientist is totally missing the point. Current optical sensors on the wrist are plenty accurate enough for general health and fitness tracking. If you don't believe me then you can literally count your pulse with your finger and compare against the watch: very close. Optical sensors aren't great for high-intensity training so for those activities everyone knows you need to use a chest strap if you want accurate data. For a more practical take on heart rate accuracy see the DC Rainmaker reviews instead. reply mrinterweb 22 hours agoprevI noticed my mouth had been hanging agape for a while while reading this. This is huge news. I feel like Pebble is the smartwatch that got it right the first time. So many smartwatches try to replace the phone instead of being an extension of the phone. Pebble seemed to better understand what is important than most smartwatches by being the extension of the phone, a focus on battery life and always on displays. reply Vampiero 19 hours agoparentI literally just bought a LTE watch because I hate phones, I never use mine, and I keep forgetting it anyway. I'd rather have a watch with an eSIM reply amelius 3 hours agorootparentI would do this too, but in some venues you have to pay by scanning QR codes, which makes it impossible without camera. And I don't want this: https://wristcam.com/blogs/learn/do-apple-watches-have-camer... reply weberer 3 hours agorootparentThey have QR payments, but no NFT? That doesn't seem right. reply pbmonster 3 hours agorootparentOutside the US, third party payment apps are popular, since they often don't require a credit card. But since Apple refuses third parties access to the NFT hardware, those apps almost universally have to use QR payments. It works OK, because most payment terminals have a display with sufficient resolution for a QR code anyway. reply mixmastamyk 2 hours agorootparentOutside the US, modern banking is available, and credit cards are unnecessary. reply stronglikedan 3 hours agorootparentprev> you have to pay by scanning QR codes I've never encountered that, but that sounds like a venue that doesn't want my money anyway. reply amelius 3 hours agorootparentIt happens a lot in societies where people know and trust each other. Often you can even buy drinks by scanning a QR code, paying, and then grabbing a drink from the fridge. With nobody else involved. reply nejsjsjsbsb 17 hours agorootparentprevWould you wear a Pebble too? reply insane_dreamer 21 hours agoparentprevThe Withings ScanWatch was the right fit for me. Unfortunately the HR sensor stopped working recently and the water resistant seal broke, and it's out of warranty, so it's in a drawer. But IMO that was the right idea: analog time, discrete notifications, ppg/ekg sensors, 2-week battery life. reply morsch 21 hours agorootparentI like my Fossil watch. Similar to Withings, less health features, marginally smarter. Analog watchface in front of an eink display, 3-4 weeks battery life. Of course they got discontinued as well. reply teetertater 11 hours agorootparentThese are truly the best! And I've done a lot of research. Nowadays there are some smaller luxury brands that are closer with feature parity but not quite. The original Fossil team spun off to develop some kind of general watch platform, so I'm hopeful we'll see a remake by the time my fossil kicks out. reply martin_a 10 hours agorootparentprevMine broke down but I got it repaired by Fossil after some time for 60 Euro. Meanwhile I bought a Garmin, now the repaired Fossil sits in a drawer. Not sure what to do with it. Maybe gift it to someone, but softwarer support is probably getting worse. reply lopis 20 hours agorootparentprevI don't like analog watches. I wish there was a watch like the basic casio I use but smart, but not huge and rugged like a G-Shock. If pebble releases a modern version of their watch, I might finally buy a smartwatch. reply Y_Y 7 hours agorootparenthttps://www.sensorwatch.net/ I haven't made one myself because last I checked it was a hassle to ship, but this might be what you're looking for, F91-W exterior with minimally smart replacement innards. reply Cadwhisker 19 hours agorootparentprevI released a free watch face for the FitBit Versa (pre-Google) which emulated a Casio-style LCD. I can't remember the name, but you can get paid ones which look decent: https://play.google.com/store/apps/details?id=com.azya.fitbi... reply filoleg 16 hours agorootparentprevWhat are you looking for in a smartwatch? Casio has smaller G-Shock smartwatches (not just the giant circular ones) that track your activity, heartrate, etc. But if you want smartphone notifications, then yeah, sadly you are out of luck. I am totally with you overall, though. I feel that if someone were to nail it, it would be Casio. reply pestaa 10 hours agorootparentThe GBD-200 I have can receive notifications. reply bartvk 12 minutes agorootparentWhat I encountered with smartwatches like the Xiaomi Mi Band, is that they display notifications but their font has a tiny subset of emoji. So when friend send a simple \"thumbs up\", it displays the Unicode replacement character. How does the Casio do it? reply bobbylarrybobby 15 hours agorootparentprevSounds like a Garmin Forerunner (255? 265?) with a custom watch face might fit the bill. reply pjmlp 11 hours agorootparentprevAmazfit watches. reply sureglymop 9 hours agorootparentprevI love my Withings watch but I wish I didn't have to use their app and could instead get the data directly. I tried to reverse engineer their bluetooth based protocol in the past but didn't get far because I don't have much experience with bluetooth. I then looked at what http requests their app makes which was more straightforward and actually interesting but still not what I wanted... I hope I will find the time to try again soon. reply insane_dreamer 1 hour agorootparentTheir app syncs with the Apple Health app (which is better), so that's what I use (and one reason why I like Withings) reply ost-ing 18 hours agorootparentprevI have one, I use it nearly every day, battery can last upward of a whole month. They stopped updating the original model though which is frustrating because newer models have more features, but im almost certain the hardware is virtually identical. I would like more transparency on how long each device gets updates for, similar to how Apple handles their products. reply andrewblossom 4 hours agorootparentprevHave you reached out to them? I've found their customer support even for out-of-warranty items to be fantastic. reply insane_dreamer 1 hour agorootparentI hadn't; I should try. reply swiftcoder 10 hours agorootparentprevI really like the Withings, but I've killed two of them in about a year each (shattered the face on one, failed seal leading to water ingress on the other). Meanwhile I have a draw full of older watches/smartwatches that are all in perfect working order, so this feels like build/QC problems specifically on their end. reply Cadwhisker 19 hours agorootparentprevI prefer technology that hides from view, so the Withings watches suit me as well. The biggest downside is that the battery does not seem to be user-replaceable, so the 1 month of run-time I used to get slowly fades down to about a week or two after a couple of years of use. I can't go away for more than a week now without bringing the charger. reply Y-bar 7 hours agorootparentI agree (a ScanWatch 2 owner here) that batteries should be user-replaceable. And that the fact that it is missing from my watch is a negative thing. However, it is a very minor thing when the battery lasts as long as it does. If it holds 80% capacity like most other batteries today at 300, or more, cycles it would take over 10 years for the battery to degrade significantly considering each cycle is up to 30 days. reply Peanuts99 9 hours agorootparentprevMines 8 years old at this point (so old it's a Nokia branded one) and it still lasts 3-4 weeks. I wonder why it's held up better? reply abcd_f 20 hours agorootparentprevThe idea is nice. The implementation is a bit gaudy design-wise (subjective, granted) and flakey on the hardware side, with the HR sensor accuracy being the main issue. reply averageRoyalty 21 hours agorootparentprevI loved my scanwatch, but it lacked the feature set of any smart watch for fitness tracking. I hope daily for them to release an improved version. reply djur 16 hours agoparentprevThe Fitbit watches are the closest thing I've gotten to a Pebble (and I'd be wearing a fitness tracker anyway), but they're way more locked down. reply dariosalvi78 11 hours agorootparentBangleJs is totally hackable reply gonzo41 21 hours agoparentprevYou should have a look at he Garmin instinct 2x. They've nailed it. reply abraxas 21 hours agorootparentBah! They nailed what exactly? It's so mofing complex to use I hurled it at a wall (literally) and then gave it away to my son. A $600 CAD watch that I could not stand to use without seething. reply brian-armstrong 20 hours agorootparentYou might want to invest some of that money in anger management classes reply abraxas 20 hours agorootparentHa! It's a testament to their great hardware though! I did not even scratch the watch by throwing it full force again a hard surface. reply taneq 20 hours agorootparentprevIf a smartwatch requires anger management classes in order to use successfully, that might indicate something about the watch. :P reply eps 7 hours agorootparent... or expectations. reply Steltek 6 hours agorootparentprevI have a Garmin Instinct 2. They definitely did not nail it. It's horrible in all respects. It's HUGE, physically painful to use, and the UI must have been written by 10 different teams who weren't talking to each other. reply xyst 19 hours agoparentprevThe Apple Watch Ultra has worked wonders for me in terms of battery life and “always on display”. My only wish is for an easily serviceable battery. reply swiftcoder 10 hours agorootparent> The Apple Watch Ultra has worked wonders for me in terms of battery life It's not really playing in the same ballpark, though, is it? 48 hours of battery life is indeed very good for an Apple Watch, but I used to charge my Pebble maybe once a week, and my Withings Scanwatch about once every 3 weeks... reply shinycode 11 hours agorootparentprevAnd not that expensive :( reply amatecha 19 hours agoparentprevPretty sure I yelled something like \"yoooo no way!!\" :D too awesome. reply vermarish 22 hours agoprevI love the animation when you click \"No\" on \"Do you want a new Pebble?\". So extra. reply ctkhn 21 hours agoparentGotta be honest I feel like Garmin is the perfect balance of pebble vs apple watch reply seanhunter 12 hours agorootparentGarmin is the perfect solution if you want a smart watch with a gps that takes 5mins to possibly sync 50% of the time and a touchscreen-only interface that doesn’t work if it gets wet or say sweaty. Ie during most of the activities it’s supposedly designed for. When I got a garmin smartwatch I was astounded by how poor the basic ux is in almost every single way. If I’m swimming, how do I stop my work out? The touchscreen doesn’t work because it’s wet. I have to do some sort of double click of the button. No that’s pause. Maybe triple click no that didn’t do anything. Maybe hold the button? Now it wants to delete my whole workout. And the GPS sync thing amazes me. I put up with this problem when I was using garmin GPSs for accurate time sync for servers back in the 1990s, but 25+ years later for them not to have figured it out when literally every other GPS device does it just fine completely blows my mind. Apple watch? I want to go for a walk/run/whatever I hit go. If I move during the 3-2-1 countdown nbd it figures it out. Garmin I want to do it I hit go, it tries to sync the sattelites. If I move during this process it starts from scratch. Sometimes the sync takes 30 seconds or so. Annoying but not impossible to live with. Most of the time however the sync takes 30seconds or so and just fails. Also annoying but whatever. Some of the time however the sync takes a few minutes and then fails. And if I move at all during this, it gives me a message saying it’s going to have to start again and starts from scratch. And to add insult to injury the thing has a custom charging plug with the socket on the back of the watch. It has a ridge and two spikes that physically press into my wrist making it actually painful to wear. So bad. reply timanderson 10 hours agorootparentOpposite experience here. Went from Apple Watch to Garmin, couldn't be happier. Never had an issue with the charging point chafing, it is recessed and no problem. Buttons to start/stop/pause/resume activity work as expected, so much better than trying to swipe and tap the Apple Watch screen especially in wet conditions. GPS sync never been an issue for me, you can start an activity before it syncs and it figures it out. reply iamacyborg 4 hours agorootparentThe metal in the charging point can cause some allergic reactions, nothing a small silicone cap doesn’t fix though reply hk__2 9 hours agorootparentprev> GPS sync never been an issue for me, you can start an activity before it syncs and it figures it out. I’ve had a lot of issues with this, like going running 15 km and it registers only the last 10 km. My workflow now is to put the watch on the balcony while it finds the satellites, and then go out when it’s done. reply selykg 5 hours agorootparentNever had this happen to me. Admittedly I am in a very rural area, and while I do sometimes get some gps points that are \"off\" it's generally very fast and accurate. Basically all the errors I've personally run into fall into what I'd consider acceptable margin of error. Even in heavy tree cover on a remote island for a hike last year. It (Garmin Instinct 2X) was incredibly accurate. reply eps 7 hours agorootparentprev> And the GPS sync thing amazes me. If the watch was recently synced with the app to get current GPS ephemrides, it gets the lock within seconds. Otherwise, it may take much longer just like any other GPS device with outdated ephemerides. reply primozk 11 hours agorootparentprevWhich Garmin to you have? This isn't my experience. And you have option to buy Garmin watches with actual buttons, I agree the touch-screen only are useless. reply khasan222 7 hours agorootparentIm not OP, but this sounds like the garmin instinct. reply sneeze-slayer 3 hours agorootparentprevGarmin instinct fixes this. Rugged, physical buttons with a battery life that last weeks. It's true about the special charger, but there are also usb-c adapters. reply haltcatchfire 9 hours agorootparentprevI've been using Fenix 5 and then Fenix 7 for many years now and I don't recognize any of the points you're making. I might agree on the awful charging port, but that's fixed by getting one of the cheap charging \"pads\" from Amazon. reply bdavbdav 4 hours agorootparentprevWhat garmin is this? My Epix (just a Fenix 7 with a fancy screen) seems to hit GPS near instantly, and you can disable the touchscreen. AFAIK it’s only the very basic ones / fashion smartwatches that are touch only (or touch and one or two buttons) reply lloeki 10 hours agorootparentprevThis may be model/generation dependent. I've had such issues with my Forerunner 735xt (from the very start), but ever since I upgraded or seen friends using newer hardware, these issues have entirely disappeared. e.g I've traced sync issues to some problem in the BT stack: forcing a disconnect/reconnect made it sync without fail. GPS was slow to lock because of low storage thus no AGPS data. The situation with \"new\" hardware is completely different. GPS lock is ~instant, by the time I get out of my RF bunker of a home I have a lock by the time I have moved the arm to press the start activity button. Sync is subsecond usually, and takes mere seconds when it \"catches up\" due to phone being away from watch for a while. Touchscreen is handy sometimes but a mere occasional bonus convenience in specific occasions: the main input mechanism is squarely buttons. I mean touch for watches is kinda braindead as an input mechanism since a finger covers so much of an area, obscuring a quarter of the screen. UI and menu organisation felt very odd at the beginning, but after a while I started understanding how and why it's laid out this way. It is a very alien interface at first but it absolutely makes sense, and the amount of things one can do straight from the watch is insane. I mean you can never ever sync the watch to Garmin Connect and still have a massive amount of features. It's essentially completely autonomous, something I used to great effect when their system was brought down because of IIRC a malware attack. reply bool3max 5 hours agorootparentprevI recently picked up the entry-level Forerunner 55 as my first ever \"smart watch\" and its lack of touch-screen controls and the 5 tactile buttons are my favorite things about it. reply tallanvor 8 hours agorootparentprevI had a Fenix 3 for over 5 years and I've had an Epix 2 for close to 3 years now and I don't have any of those problems. GPS normally takes about 30 seconds, and certainly under a minute. It has 5 buttons and for the first year I kept the touchscreen off completely until my bank started supporting Garmin Pay. Yeah, it does use a custom charging cable, but the one for the Fenix 3 was solid and since I only charged it once a week (more than I really needed to) it wasn't a problem. The Epix 2 gets charged twice a week since it has the AMOLED screen and I keep it always on and I record workouts at least 6 times a week unless I'm on vacation. But still, the charge points are inset so they're not noticeable. reply Tade0 11 hours agorootparentprevI've received a basic Garmin watch for my 5 years anniversary with the company I work for and I don't use as: The app has dark patterns like: you need to put a weight and height before you proceed with the setup, even though you can remove those later. Step counter quite simply doesn't work, as it grossly overestimates the count. 5 day battery life. Not terrible, but also not practical. Notifications. All. The. Time. And about some \"fitness goals\" I don't remember setting. I have enough distractions from my phone thank you very much. Who is the target audience for this? reply lloeki 10 hours agorootparentHeight and weight are key metrics to compute e.g BMI and VO2max. The only notifications I get are when an activity gets synced, and I did not set up anything particular, it's all default in that regard. reply rafaelmn 20 hours agorootparentprevI really like vivomove looks but after buying lux as a gift for my wife I think I will steer clear. App was bad, syncing issues, she wore it for a while because she liked how it looks but I think she has not charged it in over a year because of how much the experience sucks compared to apple watch or even withings. I have a withings scanwatch right now, the app is nice, ecosystem is nice but accuracy is very underwhelming. I would pay 1k for a watch that is hybrid with subtle watch aesthetic and minimal display/vibration for notifications has Apple watch level metric accuracy has week long battery life ideally would have replaceable battery but not a deal breaker if warranty is 3 years reply synecdoche 13 hours agorootparent2 short taps on the top button on Garmin Venu Sq 2 when swimming is the default activity. It counts laps automatically. It can be setup to display 2—4 statistics during the swim. Now if someone could explain what the other thing is that appears for a few seconds when changing direction at the wall in the pool is, and how to disable it, that would be useful. Two counters, one in large font and below one in small font. Typically they start at, or are, 0. reply seanhunter 12 hours agorootparentProbably SWOLF? Which is a measure of your stroke efficiency https://thirdcoasttraining.com/2024/11/22/understanding-swol... reply asdff 13 hours agorootparentprevI can't get over garmins predatory business model. The way they bin their products by activity is terrible if you have multiple hobbies. All of these devices are just a gps reciever, screen, battery, yet they sell it to you a dozen different ways for $500+ a pop usually because why unify the software and make it easy for the consumer? reply bdavbdav 4 hours agorootparentThat’s just marketing surely? Is there any activity that you’d want, apart from diving, missing from a fenix/epix? reply wingerlang 13 hours agorootparentprev> why unify the software and make it easy for the consumer I honestly believe that selling them as separate products is easier. Both for the company who can focus in on the advertising, and for 99% of the customers who can go straight to their hobby. That being said, it does look like the very first watch on their website is specifically in its own multi-sport category. reply WD-42 21 hours agorootparentprevI like my instinct, but Garmin is so locked down, less hackable than even an Apple Watch. reply teruakohatu 21 hours agorootparentYou can download the SDK and side load applications. No annual developer fee. Not sure how it’s more locked down. reply askvictor 20 hours agorootparentOne example: I would like to program a (dumb/ID-only) RFID card into it to unlock a door. I can't as the NFC off limits to apps. reply jrockway 18 hours agorootparentApparently there is some transit system in China that uses this format for payments. So you can get one of those cards, add it to your watch, and program the RFID system to accept the ID of the card you got. There was an HN article about it this weekend. reply notpushkin 11 hours agorootparentI think that was NFC. The problem was, iPhone generated a random UID for all other cards, while this one was fixed. reply yellow_lead 14 hours agorootparentprevYou can also use Garmin watches for transit in Japan, if it's a Japanese model. reply bean-weevil 16 hours agorootparentprevMy Garmin watch will only allow me to add emv cards. reply abcd_f 20 hours agorootparentprevDon’t know about Apple Watch development, but the scope of what you can do with Garmin's SDK is limited, apps run in a VM, ie no native code with respective performance issues and, as important, their developer support is a complete and utter garbage. It's a lucky day when someone from Garmin graces Forums with their presence and bestows few sentences they think could pass for an answer. In other words, yeah, the SDK is free, you can side-load and it’s not hard to write for, but you just can't write much. reply scarface_74 19 hours agorootparentYou run native code on the Watch itself and most of the limitations are around not doing anything that will kill the small battery. reply abcd_f 6 hours agorootparentNope, not native code. > Monkey C compiles into byte code that is interpreted by a virtual machine, similar to Java. https://developer.garmin.com/connect-iq/reference-guides/mon..., 2nd paragraph. reply scarface_74 6 hours agorootparentI meant the Apple Watch, I should have quoted the relevant bit I was replying to > Don’t know about Apple Watch development reply knifie_spoonie 20 hours agorootparentprevI'm curious why you're saying it's less hackable? I've written my own little app for my Garmin watch, and I didn't need to get permission from them or pay them anything. reply HumblyTossed 4 hours agorootparentprevLess hackable in what way? I've found it's tremendously easier to write a watch face for the Garmin than the Apple Watch. I know, I've done it (and if you, like me, don't want to share your watch face, leave it in Beta all the time). reply Steltek 17 hours agorootparentprevMy Pebble Time Steel finally bit the dust so I turned to a Garmin Instinct. I can't stand it. The button placement is totally random and legitimately painful to use. The Garmin software focuses on fitness activities to the exclusion of everything else. I recoil at having been tempted by the more expensive Garmin watches. What a waste of money that would have been! reply m_kos 15 hours agorootparentYou could always sell it.reply abraxas 21 hours agorootparentprevI just gave away a very expensive Garmin to my son. Its feature set is to dream of. Its user interface is hot garbage. When I'm out on a hike or in the pool trying to just measure my fsking laps I need a single click option or something. Their paradigm of \"button 1, button 3, button 5, long press button 4, button 1 again to confirm. Now you can push off the wall in 3... 2... 1\" is beyond fucking stupid. Does anyone at Garmin actually practice sports? For a company with such great hardware they really need someone competent on the UX team. Throwing everything into more and more menus and submenus is not working. The specific watch I'm criticizing is Garmin Instinct 2x solar. The name is very ironic because there is nothing intuitive about using that watch. Like, at all. reply foobarchu 21 hours agorootparentOn the positive side, I adore that they sell (sold?) the forerunner series with all physical buttons and no touchscreen. Garbage software, but being able to click through by muscle memory instead of dealing with a touch interface in sunny conditions is essential to me. Fitbits and apple watches have just always been too reliant on the touch method for my liking. The software is pretty crap though, and forerunner in particular is way too locked down towards running activities. reply DwnVoteHoneyPot 20 hours agorootparentYou have trouble with touch interface in sunny conditions??? Try cold conditions with gloves on, then you'll have real problems with the Apple Watch. Or wet conditions. Or fast conditions like on a bike where you rather not look to turn off the alarm. Sunny conditions is the only time my watch works fine. reply jrockway 18 hours agorootparentMy gloves work well on my Apple watch. Wet is always a disaster, though. If it's going to be moist outside (like hiking with a rain jacket), you have to remember to apply water lock immediately, or you're done for. In that case, the watch is pretty much useless until you get back inside, which is in fact very annoying. reply maxerickson 17 hours agorootparentprevThe 265 and 975 both have touch screen on top of the buttons. reply llimllib 20 hours agorootparentprevI don't swim, but I have done thousands of runs with my series of garmin watches and I can say that the UX for them is spectacular, everything is in a sensible place for me to do without thinking. Not sure what problems you've had with it specifically reply michaelt 17 hours agorootparentGarmin is certainly better than some previous smartwatches I've had. You get a sunlight-readable, always-on display and a week-long battery life. But with my Forerunner, they've packed a lot of options into the five buttons. Leads to a lot of \"these buttons are for up and down, except at the start run screen where the up button opens the menu, which you can then navigate with up and down to choose between the six types of run, or exit with the back button\" If you're the type of person who doesn't like to read the manual, you're going to have a bad time. reply Fnoord 11 hours agorootparentMy Fossil HR Collider lasted 4 weeks. It could do much (but not all) of the stuff my Pebble Classic/Pebble 2 could do. Both could control music during workout, pick up calls, put radios off. What I liked most about it is it could disguise as a non-smartwatch. On top of that, Pebble 2 HRM was bad. A Pebble successor has to be better than a Pebble 2. The only reason my Pebble 2 isn't used anymore (and why I swapped to Fossil which is discontinued, too) is hardware buttons died. I tried to donor from a Pebble Classic, but sadly failed. On top of all this, I get skin rashes from watches, so I cannot wear them 24/7. reply abraxas 20 hours agorootparentprevGetting into the swim app itself takes a couple of different buttons presses. But then it tries to be both too smart and too stupid at the same time. All I wanted to begin with was lap counting with a big number on the centre of the display. Can't configure it and can't even start to get it to count laps without some ceremony of setting up interval training and it only gets more convoluted from there. It's useless for an amateur like me who is not a peak performance athlete who needs to track every minutiae of their swim stats. How many people are they targeting with these this UX? Just people getting ready for the Olympics? There are hundreds of them. Hundreds! reply nradov 19 hours agorootparentI don't think you're being serious. I have had several Garmin watches and this is not an actual problem. I do both pool swim and open water activities and it's very easy to count laps. Sometimes I set up structured workouts but that's completely optional. reply iamacyborg 4 hours agorootparentprevSounds like you might need to turn off a few defaults to make it easier. It should just be a case of pressing the start button, navigating to the pool swim activity and pressing start. Use the lap button to record rest intervals at the wall, everything else is automatic reply dqv 13 hours agorootparentprev? https://crossvine.nyc3.cdn.digitaloceanspaces.com/20250128_0... But I'd have to try actually swimming with it I guess. It always counts my reps in strength training pretty accurately. Edit: But then again, if you want to manually track laps, the swimming app doesn't matter. It's only there for the convenience of not having to press a button to increment the counter. You can just copy the \"other\" activity, name it something like \"manual swim\" with the lap button enabled. The only thing that differentiates the swim activity from a regular activity is setting the pool length, stroke detection, and automatic lap incrementing. The data is still getting logged the same way as far as I can tell, so using the \"other\" activity would give you all the data you need to track the swim. reply grogenaut 19 hours agorootparentprevMy vivoactive 5 swimming is top right button short press (activities) scroll tap swim. Top right to start. Screen shows only interval time and distance or laps (configurable). Bottom goes to interval rest. Lots more data and a rest timer. Bottom starts swimming again. Top stops the activity. Long press top to save, bottom to discard. All operations are buttons because the touch doesn't work well with water on basically every device. Literally 3 clicks to a large lap counter. reply abraxas 18 hours agorootparent> scroll tap swim this watch has no touch interface. any scrolling and selecting has to be done via the five buttons which I ALWAYS somehow get wrong. Who on this beautiful earth thought it was sane to make the bottom right button be the \"Back\" button in a L2R (English) locale? reply nradov 16 hours agorootparentI really can't understand what you're complaining about. There's nothing about the English locale which implies an optimal placement for the Back button. The same devices are also sold in other locales. These devices have to work in all conditions with some complex functionality available through only five buttons so some level of overlap is unavoidable. Do you also complain that your computer keyboard lacks separate buttons for \"4\" and \"$\"? reply _thisdot 9 hours agorootparentIdeally, in LTR locales, we move from left to right. This means that objects appear from the right side of the screen (as going forward would mean going right), and a back button is more intuitively perceived on the left side. reply nradov 5 hours agorootparentNo, that's not how it works. Especially not on a wrist device. reply grogenaut 17 hours agorootparentprevyou replied to a thread talking about garmin watches... not the pebble reply jorvi 21 hours agorootparentprevTo be honest, that is not a problem unique to.. well.. any domain-specific company's tech stack. Raymarine their marine GPS navigation units are supposed to be very intuitive, but they lack so many \"that would have been nice\" features, and their UX has stuff where various buttons have click / double-click / hold / hold 2s / hold 10s, all to access different functions. Some of it isn't even written down in the manual. reply grogenaut 19 hours agorootparentI mean honestly I have this issue with every level of compute device. Smart phones are much more limited than computers and so much stuff is buried. But then think about the absolutes gigantic amount of undocumented buried stuff that exists on Windows and Linux and macOS. You have a keyboard and internets and a mouse and it's still literally millions of people's jobs to deal with UI issues on these devices professionaly. reply InitialLastName 16 hours agorootparentIt's hard to decide if I prefer Android not having a setting I want or Windows having 9 settings panels where that setting might live. reply hparadiz 21 hours agorootparentprevAll this plus not being able to see any data while offline. Super useful when you're 13,000 feet up on a mountain somewhere. reply KiwiJohnno 20 hours agorootparentIts worth clarifying you are talking about the data on the phone app, which does require connectivity as nothing is stored on the phone app, its all on Garmin's servers. However, most if not all of the data (recorded activities or health data) can be viewed directly on your watch, without any connectivity. reply hparadiz 15 hours agorootparentI have a lot of experience working with the Garmin API. The data you can see on the recording device (watch) is limited and basically worthless. Akin to looking at a raw csv full of data rather than nicely plotted over a map. reply notpushkin 10 hours agorootparentprevWhy can’t it be also cached on the phone though? reply elric 21 hours agorootparentprevWhich data are you unable to view while offline? I never sync my Garmin watch to my phone, and I'm able to view all the data that interests me on the watch. reply abcd_f 20 hours agorootparentAny data in the app. It just doesn't work offline, at all. Like they looked at a book chapter on basics of data caching and went \"nah, not doing that, that's too f#cking advanced\". reply netsroht 12 hours agorootparentprevGadgetbridge added support for Garmin watches recently [1]. All data is stored on your Android phone with no internet connectivity required and you can even export the sqlite DB so you own your sensor data. The UI isn't as nice as Garmin's but it does its job. [1] https://gadgetbridge.org/basics/topics/garmin/ reply hobos_delight 20 hours agorootparentprevI'm not sure I understand. I have had an Instinct, Tactix Delta, and Tactix 7 Pro and have always been able to see the data without a phone or any network present. I love these watches after moving from an Apple watch, primarily for two reasons: 1) the battery life I cant stand having to charge my watch every day or so my (current) Tactix 7 will go ~3-4 weeks depending on how much GPS I use. 2) (this may be out of date) when I would use the Strava or Run app on the Apple watch, it would not signal when it had a GPS fix, which resulted in a number of runs that had a \"teleport\" at the start, resulting in messed up metrics. Only a small thing, but it really frustrated me. reply KiwiJohnno 20 hours agorootparentI'm assuming the parent poster is talking about using the Garmin Connect app, which does require connectivity. You are correct, the data is visible directly on the watch. reply Moldoteck 10 hours agorootparentprevDon't some Garmin watches support long press app launch? reply alex-korr 14 hours agorootparentprevWhat are you even talking about? Garmin has an auto start/stop feature for lap swimming. All you need to do is single press top right button once to start the session and press the same button to stop and then another button to save it. It will literally do everything else for you automatically. reply jeffbee 20 hours agorootparentprevMy Garmin has a dedicated hardware button that says \"LAP\" reply eloisant 20 hours agoparentprevThe feature I use the most on my smartwatch is paying. So if they can bring contactless payments to their new Pebble they have my attention, otherwise it's useless to me. reply ZeWaka 19 hours agorootparentThere exists 'smart bands', which can be applied to any (generally non-smart, obviously) watch that uses normal pin-style watchbands. They have a contactless chip in them that can store one card. My traditional watches use them, though I had to custom-make one of the bands to be in a style I wanted. reply ryukafalz 18 hours agorootparentFrom what I've seen these aren't available in the US, unless I've missed one. (I would be very interested if so!) reply NikolaNovak 19 hours agorootparentprevThat is awesome... Any links? Any risk vectors I may be missing? reply gvurrdon 9 hours agorootparentprevIt's a major use of the watch for me also, but something like a Pebble 2 HR would tempt me to abandon payments. Do you have any links to examples of these bands you've found useful? reply miki123211 18 hours agorootparentprevHow do those handle user authentication and card installation? I assume you need support from your bank for the former and PINs for the latter? reply jsheard 19 hours agorootparentprevI wouldn't count on that, getting every bank on board is a massive undertaking. Even Garmin Pay and Fitbit Pay (before it was folded into Google Wallet) have/had huge gaps in their coverage, especially outside of the US. reply urbandw311er 19 hours agorootparentprevYou could cut out the chip from a debit card and glue it to the back of the pebble? reply jsheard 19 hours agorootparentIt's not quite that easy since the NFC antenna extends beyond the chip. Still doable though, as demonstrated by Bobby Fingers: https://youtu.be/NF4VJJKTjy8?t=825 reply 4k93n2 15 hours agorootparenthaha thats too funny. i wonder if his eye starts to heat if he buys too many drinks reply soxocx 22 hours agoparentprevOn the iPhone I get redirected to the Apple Store page for the Apple Watch. Nice humor. reply Findecanor 20 hours agorootparentI got a tiny bit offended by the assumption that I'd rather have an Apple Watch. I'd think the ideal for me would instead be something in-between a Pebble and a Sensor Watch. Something hackable with more battery life, that is a watch first (and a smartphone notification screen never). I wonder how far I could go towards that goal with the upcoming Pebble hardware and rewriting the OS kernel to sleep more. reply bean-weevil 16 hours agorootparentI interpreted it as an intentional insult :) reply benbristow 21 hours agorootparentprevSame on Mac (Firefox) reply pohuing 21 hours agorootparentSame on android Firefox& Chrome reply jacobgkau 21 hours agorootparentprevI think it's just a static redirect, it sent me to the Apple Watch page in Firefox on Linux. But I also wondered if it would shuffle between a few different brands or something (I guess not). reply splonk 20 hours agorootparentLooks it tries to identify Apple devices and goes to Pixel for everything else. const platform = navigator.platform || ''; const userAgent = navigator.userAgent || ''; const isAppleDevice = /iPhone|iPad|MacIntel/.test(platform) ||/iPhone|iPad|Mac OS/.test(userAgent); // Set redirect URL and message based on device const redirectUrl = isAppleDevice ? 'https://www.apple.com/watch' : 'https://store.google.com/product/pixel_watch_3?hl=en-US'; Edit: per erohead, that change was made after your comment. reply HaZeust 20 hours agorootparentprevIt looks like: Chromium browsers (tested in Edge, Chrome, Brave) go to Pixel Watch, Android devices go to Pixel Watch, Apple devices go to Apple Watch, Firefox brings you to Apple Watch. It might also be randomized, but that's what my tests got me, and only the Firefox one doesn't make humorous sense. reply forty 20 hours agorootparentI got pixel with Firefox for Android reply HaZeust 20 hours agorootparent> Android devices go to Pixel Watch, reply cjonas 20 hours agorootparentprevI got redirected to pixel reply edarchis 20 hours agoparentprevThe amount of us who clicked no is amazing. I loved my Pebble Time but I'm going to give money to yet another Kickstarter and have it be killed shortly after. reply hbn 21 hours agoparentprevWhat if you clicked no because you already own a Pebble? reply pinoy420 17 hours agorootparentUpgrade option reply cryptozeus 14 hours agoparentprevwhy not redirect to google watches specially if the team is from goodle ! reply echelon 21 hours agoparentprevFeels a little bit salty to send customers to Google's competitor given the fact that Google provided the exit and also liberated the code. They didn't have to do that. A better \"thank you\" to Google would be to direct people to Fitbit. reply erohead 20 hours agorootparentGood call, I just changed it to send to pixel watch if opened on Android or Windows! reply wlesieutre 20 hours agorootparentprevI thought they killed FitBit and are doing Pixel Watch again instead https://store.google.com/product/pixel_watch_3 reply alex_young 20 hours agorootparentprevI think it’s perfect actually. Google used to (still?) have a page internally where if you clicked on “I don’t care about security” it sent you to the jobs page of a competitor that had suffered a notable breach. Very on point. reply Reason077 15 hours agorootparentprev> \"A better \"thank you\" to Google would be to direct people to Fitbit.\" Fitbit has already gone off to the great Google graveyard, unfortunately. reply shaklee3 13 hours agorootparentno. they just released the Fitbit ace reply imp0cat 8 hours agorootparentCue Monty Python's Dead Parrot sketch. :) Fitbit isn't dead yet, but it's not doing great either. And the alternatives kinda suck (tldr; the best choice is probably a Garmin for 3x as much money and with less features ). reply wkat4242 21 hours agorootparentprevIt's just a joke I think. But yeah linking to the pixel watch would have been nicer. reply rzazueta 16 hours agoprevI LOVE My Pebble and even got Rebble working on it not long ago to revive it. However... If you want to make it TRULY HACKABLE as you claim, you will not encumber it with cloud dependencies like you did last time. Let ME self host my own Pebble server if I choose. Go ahead and default to your servers and sell services and whatever, but let me host my own and switch the base URL to my own domain, preferably with open source software and simple APIs, without requiring me to go through your servers. That way, even if this attempt also doesn't pan out, those of us willing to do the work will at least still have the functionality we want. I get the whole VC \"lock them into required cloud services for life so we can make endless subscription revenue\" model, but it's absolutely corrupt. And, Eric, I know you know that you have a hacker's heart. Please listen to it. reply erohead 13 hours agoparentI'm 100% with you! No VCs this time...no mandatory cloud subscription. But I'm not really sure that this fear is grounded before we sold to Fitbit we 'unlocked' the Pebble mobile app so you could use it with any cloud you wanted, including self hosted. So...it already meets your definition reply andrewflnr 11 hours agorootparentPossibly that's a good thing to mention on the homepage. :) reply saidinesh5 10 hours agorootparentprevThis is really good news! Wishlist for the next device: More sensors: heart rate, spo2 and ecg... reply flakeoil 10 hours agorootparentPlease, not too many sensors. Long battery life and small size and low weight is more important. All the other smartphones and fitness watches have these sensors so get one of those instead. Maybe a HW interface to attach other sensor modules could be an option, but could also easily become a distraction and pull down the overall experience. [Edit] A wireless i/f (bluetooth, LP RF) is of course better to use as interface to any peripheral sensor modules. reply gvurrdon 9 hours agorootparentA Pebble 2 HR with a colour screen would do the job for me, I think. reply donjoe0 9 hours agorootparentprevNever cared about Pebble's fitness/health tracking and never used it. I don't need my health reported to me in numbers every day, I'm not a performance athlete nor a hospital patient. Living my personal life trying to hit some numbers dictated to me by some app sounds like a horrifying idea, like a second work life at home. reply _Algernon_ 5 hours agorootparentprevI prefer accurate sensors over many sensors. My last watch would measure my heart rate to be in the 140s while sleeping. I have since slept with a chest strap heart rate monitor and do not in fact have a medical condition. Granted, that was a cheap watch, but I still don't see the use case of a smart watch for health tracking versus phone+100$ chest strap. I believe the usefulness of these kinds of devices for health tracking is vastly overstated due to lack of accuracy. reply lolinder 14 hours agoparentprevSo much this. Learn from Framework: Sell the hardware at a price point that makes your business sustainable without needing a cloud component to push you over into profitability. Yes, it will lock out people for whom that price is unacceptable, but now more than ever your real customer is serious hackers, and we are collectively more than fed up with the cloud and subscriptions. Framework and Nabu Casa need to be your models here, because your customers are overwhelmingly their customers. reply NoahKAndrews 16 hours agoparentprevPretty much all of the cloud stuff has been reimplemented with open-source code by the community! See rebble.org. reply bcraven 2 hours agorootparentrebble.io is the correct address reply ClassyJacket 15 hours agoparentprevI second this. I'll be very hesitant to buy in if it's locked to a cloud service. And people are waking up to this, with the Bambu controversy and all. Please don't make this mistake. reply blobbers 12 hours agoparentprevWhen micropayments fail to annoy sufficiently, just turn them into microsubscriptions! Try 3 days for 30 cents! agree with you rzazeuta! reply ericd 14 hours agoparentprevOoh yeah, big bonus points if it integrates easily with eg Homeassistant. reply scottydelta 19 hours agoprevThis is great news. In the last few years, I have upgraded my apple watch couple of times hoping to accept even marginal improvements to battery life and hackability but every time I stop using it seeing how it's still not what I am looking for. I tried keeping my pebble alive for so long even after it's demise, I bought 2 Pebble Time when a few were still available on ebay. I remember writing my first integration from scratch to control room lamp using my Pebble watch. I hacked it together by getting a wifi socket and programming a web-server hosted on my raspberry-pi. Here is the DEMO video I made 8 years ago: https://vikashbajaj.com/pebble.mp4 My pebble watch would call an app on my phone, in turn the app would make a request to the webserver and the webserver would then make a query to the wifi socket to toggle it. It lagged a bit but it got the job done. I could connect anything to these wifi sockets and control any appliance with my Pebble time. This was before hackable smart hubs were a thing. reply oli-g 13 hours agoparent> My pebble watch would call an app on my phone, in turn the app would make a request to the webserver and the webserver would then make a query to the wifi socket to toggle it When you ask your programmer husband to turn on the light reply minimalengineer 22 hours agoprevGreat device — lasted 4 years, woke me at 5 AM without disturbing my kids, and handled notifications well. Battery life was about a week, and it was swim-proof. That said, it was cheap... I hope this new version isn’t part of the “dumb” device trend where people spend $500 just to detox, thinking the price will force commitment. reply OccamsMirror 10 hours agoparentAt the same time, I hope it's priced high enough so that the company can thrive without taking external funding. PE and VC fuck everything up. reply brk 21 hours agoprevLooking forward to checking it out! I still have this email in my inbox from 2011, after a posting here on HN about your launch: Subject: You bought the first one! BODY: Congratulations... Great to see this happening again, best of luck! reply erohead 19 hours agoparentThanks for being there at the beginning! reply xanderlewis 21 hours agoparentprevThat’s really cool. reply agambrahma 10 minutes agoprevI discovered the Garmin Instinct last year, and was very happy to switch to it from my Apple Watch. reply wvenable 19 hours agoprevDeveloping the for the Pebble was a lot of fun. There was a Pebble hack-a-thon recently (recent being 2 years ago) and I finally got around to finishing a project that I started a decade earlier: https://github.com/codaris/pebble-cpp It might even become relevant again! Pebble had an ingenious design for its watch apps. Despite the watch having a limited processor and even more limited RAM, it could accommodate several apps, each boasting a lot of capability. Each Pebble app was comprised of two components: one that resided on the watch and another on the phone. Users could install these apps from Pebble's dedicated app store, and the same app was compatible with both iOS and Android. Pebble brilliantly bypassed Apple's app install restrictions and cross-platform compatibility challenges by executing the on-phone portion within the platform's JavaScript engine. If you wanted to create a weather application, the phone component of the app would be written in JavaScript and retrieve weather updates from the Internet, which would then be conveyed to the watch's C-based app for display. Watch apps could also have a settings page that was implemented in HTML. I have always been impressed by of the cleverness and simplicity of this design. reply tomaskafka 19 hours agoparentOh, my memory is hazy here, but if I remember right, at the beginning Apple didn’t let them download executable code (companion js) to the app, so they just took a javascript code from every app in the pebble store, bundled it into Pebble iOS app, and updated it every few days with a fresh code. Can anyone confirm? reply KatharineBerry 16 hours agorootparentThat is exactly what we did. Bonus fact: we didn’t actually read the bundled code we downloaded it from the internet and ran it as usual. But a server-side check ensured you could only get the JavaScript we’d bundled. Or at least it did until we eventually quietly turned off the check, when Apple stopped paying attention. reply tomaskafka 7 hours agorootparentOooh, clever :). Just dash quickly when the Sauron's eye looks in another direction :)). reply mazambazz 21 hours agoprevEven though I haven't used one in a really long time, the Pebble Time still stands out to me as something I wish I still had. It's an absolutely shame that Pebble was so innovative and functional, but couldn't reach mass market. But, I am extremely excited and happy that the Pebble team can start it again. I don't like Google for many things, but, I am grateful that the open-sourced PebbleOS. What a joyous day! reply TeMPOraL 21 hours agoparent> It's an absolutely shame that Pebble was so innovative and functional, but couldn't reach mass market. I think trying to reach \"mass market\" or specifically, the market of people who are into fitness and sportsball is largely what killed them. I'd like to believe that they could've catered to existing userbase a bit longer, grew a little more slowly but sustainably by doubling down on an idea of an ergonomic, battery-efficient, programmable smartwatch extension a tool, not a toy. Alas, maybe the whole thing was over once Apple, and Samsung got their marketing wheels spinning. reply Avamander 21 hours agorootparent> Alas, maybe the whole thing was over once Apple, and Samsung got their marketing wheels spinning. Totally possible, like Nokia vs the iPhone. Difficult to say for sure though, seeing vendors like Fitbit and Garmin still operate in the same space. reply TeMPOraL 19 hours agorootparentNot as ergonomic, no e-paper, and not hackable. They're fitness gadgets, not tools. reply bdavbdav 4 hours agorootparentI’d strongly disagree with tool vs fitness gadget here. Compare a Garmin to an Apple Watch or pebble, and it absolutely is a tool. Arguably the MIP display beats out the ePaper, ergonomics are great (I can use the buttons even while swimming), and it’s built like a brick. Not knocking the pebble too hard, but it certainly seems like an enthusiasts toy. I’m not sure what else would last me a week while tracking exercising for an hour or two each day. reply Avamander 17 hours agorootparentprevCertainly. I think openness is vital for such a (new) platform or ecosystem in general. The ecosystem has certainly gotten older by now, especially compared to when Pebble first started, but in my opinion a lot of the organic growth has been stunted. It's too difficult to try new things, find new useful applications and to innovate with all these walled _wearable_ gardens. I still miss a few things the Pebble had but my Apple Watch doesn't. Which in turn makes it feel less like a tool and more like a gadget. reply Rapzid 17 hours agorootparentprevIs a diving watch a tool or fitness gadget? I have the opposite opinion on what's a tool and what's a toy. reply nradov 16 hours agorootparentprevGarmin devices are hackable to an extent. There is an SDK so you can write custom apps, although some of the hardware functionality isn't accessible. reply zokier 20 hours agoparentprevPebble Time (Steel) Kickstarter is the only crowdfunding I truly regret missing out on. I remember seeing it at the time, but I think the reward levels I wanted were sold out or something. Even in retrospect it seems weird that it failed the way it did. reply lclc 9 hours agoparentprevOn the GitHub it says: > Proprietary source code has been removed from this repository and it will not compile as-is. This is for information only. Not sure how much use it is? reply agloe_dreams 4 hours agorootparentThey list out what the proprietary bits are. All of it is third party gernical hardware interface libraries that they do not own. Bluetooth stack, etc. All stuff you can rewrite easier today. The Magic of pebble was the UX of the OS and it's extensive hackability. All that magic is OSS now. reply BlueTemplar 21 hours agoparentprevI'm still using it, in fact to the point that it's probably the biggest factor why I have been procrastinating on still staying on Android rather than trying alternatives like PinePhone. That the OS has been open sourced is great news (though it's sad it was on GitHub... and hopefully other communities around Pebble will spring up outside of platforms (article only mentions Discord and Reddit)). reply girvo 22 hours agoprevMy Pebble Time Round is still the single best piece of tech I have ever owned and used, and I miss it every day. If it can be brought back, I’d pay whatever is necessary, and I’d love to contribute now that I’ve spent many years doing embedded firmware development professionally! reply eiiot 18 hours agoparentI still use my PTR daily! See https://rebble.io/ :) reply thoop 20 hours agoprevLove this! After my Pebble I tried an e-ink \"Watchy\" from SQFMI (https://watchy.sqfmi.com/) thinking that the battery life would be great but the battery only lasted a few days. I've been wearing a Bangle 2 (https://www.espruino.com/Bangle.js2) which feels closest to recreating my Pebble. It has super long battery life and feels a lot like my Pebble did, but doesn't have the polish of the pebble UI and animations. Can't wait to get a new Pebble! reply dev_snd 19 hours agoparentI've never worn a pebble, but I also have a banglejs 2 and I really love the watch for it's hackability. I've written my own watchface and a couple of other apps and made changes to a number of existing apps, it's really simple because you can always test your code changes live on the watch while keeping it on your wrist. There's an IDE that connects to it using Bluetooth and the code can be modified during runtime Lthere's also a great community of hackers and tinkerers that steadily improve the watch. It might not have the same polish as the pebble had, but it makes it up in hackability. I can only recommend getting a banglejs2 (battery life is also pretty great, I get about 10 days with regular use) reply dowager_dan99 19 hours agorootparentEven if you're on the fence, it's worth the small investment for some fun and to support a project that's been going for a while now. reply donjoe0 17 hours agorootparentprevHuh. I appreciate what they did with the tech there, but looking at this Bangle.js the first thing that comes to mind is I hope NuPebble(?) doesn't adopt that excessively-curvy-rectangle shape that screams Apple Watch, I've learned to recoil in disgust even seeing that shape. reply husamia 1 hour agoprevI really appreciate the innovative spirit behind the Pebble watch! It was groundbreaking to have a hackable device that allowed users to customize their experience. The ability to run a step detection algorithm and see real-time data was a game changer for health tracking. It's inspiring to see how the Pebble platform paved the way for future wearable technology. Kudos to the entire team for their dedication and vision! reply ata_aman 21 hours agoprevI lost mine somewhere in SF while visiting years ago but I absolutely loved it. I won it at a hackathon after making a tiny Pebble app where you could keep score during a soccer game as a referee by pressing the side buttons. App development and publishing was extremely easy on their app store. reply xavdid 20 hours agoparentWhoa, I loved that app! I used to track scores for intramural games in college. the UI was so clean and simple! reply ata_aman 19 hours agorootparentNo way! hahaha that's amazing. I love the internet. I remember waking up the next morning to like 120 downloads, totally unexpected. reply xvfLJfx9 10 hours agoprevLooks very interesting to me. There are a couple of features that are especially important to me. Good accuracy sleep tracking GPS ( I know this uses up a lot of battery but could be off by default) Self-hostable servers. (I'm a very privacy conscious person, and also I don't want to be bound to an ecosystem that might disappear one day) I'm gonna keep an eye on this project. It really looks very interesting. I hope it gets far. reply xnx 22 hours agoprevPebble and Basis Peak (https://www.engadget.com/2016-08-09-basis-peak-obituary.html) are the two biggest smartwatch losses, and probably top 10 gadget losses of all time. Glad to see one of them might have a future. reply xattt 21 hours agoparentIn an alternate timeline, we are all wearing Pebbles synced to our Palm Prēs. reply trescenzi 21 hours agoparentprevThe Basis was incredible. It had heath monitoring features Apple is just getting to. I get why it died though. Their V2 exploded on people’s wrists… reply xnx 21 hours agorootparentI've had at least two watch purchases fully refunded because of similar issues (or lawsuits related to claims of issues at least): https://arstechnica.com/gadgets/2022/05/lawsuit-claims-more-... reply insane_dreamer 21 hours agoparentprevNever heard of Basis Peak, but I've heard of Pebble for over 10 years. reply billybuckwheat 22 hours agoprevExcited (cautiously) about this. Loved my Pebble Time and was gutted when 1) Pebble bit the dust, and 2) my Pebble vanished down that black hole things like small devices and the other sock invariably go down. If this happens, I hope they can keep the revived Pebbles just smart enough and rebuild the app ecosystem. Best of luck, folks. I'm cheering you on from the sidelines! reply bigiain 21 hours agoparentAlso cautious. Extremely cautions. I got rug pulled by \"the pebble team\" the first time, leaving me with 3 watches they effectively bricked.. Not gonna sign up for that again. (I got a refund on my last Pebble order. When the money showed up I drunk-ebayed a 2nd hand ~40 year old mechanical watch. I now have about 20 wind up or mechanical auto winding watches. I do have a few chinese ~$40 \"smart watches\" that do an OK job of notifications on my wrist, and a somewhat questionable job of heartrate and blood pressure monitoring, and one that produces totally random numbers for blood glucose reading whether it's on my wrist or not. I almost never wear any of those. I've got a Watchy kit, and open source epaper ESP32 watch, but I've had it maybe a year and haven't found the enthusiasm to assemble it.) reply billybuckwheat 19 hours agorootparentTrue. I used it after Pebble went away Gadgetbridge helped extend the watch's life a few more years. I bought a PineTime as a sort of replacement for my Pebble Time about 18 months ago. An OK watch and, as one blogger I read noted, it's just smart enough. reply hoherd 3 hours agoparentprevI never had a pebble. Also, I was really skeptical of the Flipper Zero, but my curiosity finally got the best of me and I bought one. I love their app ecosystem and the whole experience of using it. If Pebble 2 has that kind of UX, it's going to be really awesome. reply nirav72 2 hours agoprevMy very first 'smart wearable' I owned. Amazing watch. Also used it to build my first home automation project Built a garage door opener and status monitor with a Arduino Yun , relay and magnetic switch. Was able to trigger it with a button mapped on the pebble that would trigger something in tasker on a android phone that would make an http call to open the garage door and also to get the status. I'd be completely ok if they keep the new pebble as simple as possible. Using the same e-Ink display and functionality. One of the most memorable things about the original watch was the battery life. Didn't have to charge for days. reply m-p-3 14 hours agoprevI owned the Pebble OG, the Pebble Steel and the Pebble Time Steel and despite all my attempts, I couldn't fill the void the Pebble left. I tried the Amazfit Bip, The BangleJS 2 (that one got pretty close IMO) and now rocking a Casio watch that does bluetooth but still work on a CR2032 (GBD-200), and more than a year on the same battery, which is quite a feat when you think about it. The software UX of the Pebble was on point, and the animations surprisingly smooth for such a device. I'm still convinced that after all there years, the Timeline UI is unrivaled. I'm eager to see what you'll come out with :) reply numpad0 3 hours agoprevNote to self: Cortex-M3, 64 MHz, max resource size 96k, app size 24k, 144 x 168, 4 buttons[1]. Probably in theory doable with nRF5x SoM + cheap OLED. 1: https://developer.rebble.io/developer.pebble.com/guides/tool... reply MortyWaves 19 hours agoprevI haven't used a Pebble, but I wanted to mention something I have seen praised a lot on HN and elsewhere. Apparently, the Bluetooth stack on a Pebble is absolutely legendary: reliable, dependable, robust, you name it. It still works reliably today seemingly thanks to their very diligent software design. I hope that element of it will continue to exist as-is on these new ones? I mention this because Bluetooth is still generally speaking very meh. reply follower 3 hours agoparentUnfortunately, as mentioned in another comment, the Bluetooth stack is apparently one of the items that is not included in the source release[0]: Some parts of the firmware have been removed for licensing reasons, including: [...] The Bluetooth stack, except for a stub that will function in an emulator [...] Which suggests that the Bluetooth stack wasn't entirely of their own making, so perhaps any of Pebble's own additions were too intertwined (e.g. gave away too much Bluetooth stack vendor proprietary API info) to be easily separated? [0] https://github.com/google/pebble/blob/3b927684809fba173ee540... reply modeless 16 hours agoparentprevI am convinced that there must be Pebble fans on the Android team that keep a Pebble in CI and ensure it keeps working with each new release. Otherwise its continued extended working lifespan is inexplicable given the amount of churn in Android in general and the Bluetooth stack in particular. reply crossroadsguy 15 hours agorootparentActually kinda to the contrary BT is extremely back-supporting. It adds/removes features so slow that it’s too boring. That’s why I left my BT expertise at the start of my career and moved to app development (it was a mistake in the hindsight but that’s another story). reply modeless 14 hours agorootparentSure, it's backwards compatible in theory. In practice I haven't had any device that kept working reliably with zero issues through every Android update and every phone upgrade. Including very important ones like Tesla's phone key. Even Pebble wasn't flawless at the very beginning, but it got good fast and hasn't stopped working since the company went under. reply Moldoteck 10 hours agorootparentprevMy speculation is that they used very low level software design to achieve such reliability. This could be harder to maintain but who knows... reply 6thbit 2 hours agoprevSounds fascinating that you essentially \"rented out\" your company through a sale that got you profits and just waited a few years for big corp to sunset the product and recognizing its value you're now rising it from the ashes?! How does this work out on the IP and legal side? A portion of the OS is now open source, but that doesn't make google surrender the trademark does it? reply crackercrews 10 minutes agoparentIt's unclear that they sold the trademark in the first place. reply solarkraft 22 hours agoprevI’m wearing my Pebble Time Steel right now and the biggest issue I have with it (maintaining the app) is arguably mostly Apple’s fault. I was originally pissed that Pebble never sold replacement parts (actually I still am), but at least this hardware has been holding up extremely well. reply INTPenis 22 hours agoprevI'm glad. For a decade I felt like an outsider because all I want is a very simple wearable device that doesn't require charging more than once a month and can display simple notifications from my phone, and the time. I loved the Pebble Time. After that I went over to Fossil Hybrid, which is pretty decent actually. I'm sure the app steals everything it can but at least the device works. reply jampekka 21 hours agoparentAmazfit watches have done this for since 2016 or so. And cheaply. reply normalaccess 21 hours agorootparentI had an amazfit watch, didn't quite scratch the pebble itch and it has been relegated to the IT junk drawer. The thing I miss the most is the pebble display. I don't need heart rate, I don't need GPS, I don't need sleep tracking, I just want a watch that shows my notifications and has a great battery life. I am pleased they are coming back! reply INTPenis 13 hours agorootparentprevOk maybe I should say it's not only the charge time, but also the e-ink (like?) display that captivates me. The fact that I can always look at my watch and see the time, other people can look at my watch and see the time. No flicks of the wrist required, no always on OLED display. I honestly enjoy the Fossil Hybrid more than the Pebble Time, because it has an",
    "originSummary": [],
    "commentSummary": [
      "Pebble is being revived with support from Google, focusing on its original strengths such as hackability, long battery life, and serving as a phone extension.",
      "The revival aims to maintain Pebble's open-source nature and avoid mandatory cloud subscriptions, appealing to hackers and tech enthusiasts.",
      "The community is excited about Pebble's return, reflecting on its unique features and influence on wearable technology."
    ],
    "points": 2443,
    "commentCount": 625,
    "retryCount": 0,
    "time": 1738008679
  },
  {
    "id": 42845070,
    "title": "Google open-sources the Pebble OS",
    "originLink": "https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html",
    "originBody": "https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;pebble",
    "commentLink": "https://news.ycombinator.com/item?id=42845070",
    "commentBody": "Google open-sources the Pebble OS (googleblog.com)1207 points by hexxeh 22 hours agohidepastfavorite192 comments https://github.com/google/pebble erohead 22 hours agoThank you, Google. You didn't have to, but you did. We are all are extraordinarily grateful. I wrote a blog post about our plans to bring Pebble back, sustainably. https://ericmigi.com/blog/why-were-bringing-pebble-back We got our original start on HN (https://news.ycombinator.com/item?id=3827868), it's a pleasure to be back. If you're interested in getting a new Pebble, check out https://rePebble.com reply dang 22 hours agoparentRelated ongoing threads: We're bringing Pebble back https://news.ycombinator.com/item?id=42845091 Jan 2025 (1 comment) The future of Rebble https://news.ycombinator.com/item?id=42845017 Jan 2025 (3 comments) reply freedomben 21 hours agoparentprevNeat, I'm super excited! I've hoped the PineTime would be the new Pebble, but hasn't really materialized (though, it has gotten pretty good! despite being a little awkward). I actually held off from buying a Pebble back in the day because the software wasn't open source and I was worried about getting dependent on a product I had no control over. (Yes, I see the vicious Tragedy of the Commons wrapped up there, but still gotta make the optimal choice for me). I'm beyond stoked to see this movement! And it being open source, I have no such qualms. If they are affordable enough, I'll probably be gifting these out on the regular so expect to sell at least a dozen or two :-D reply bberenberg 22 hours agoparentprevHappy to hear about this. My mood tracker [0] for Pebble was the only one that has ever worked for me. Would love to have a comeback. [0] https://news.ycombinator.com/item?id=37221576 reply coldpie 22 hours agoparentprevIs the Small Android Phone project dead? :) reply erohead 21 hours agorootparentNo, but...not moving fast reply rpmisms 22 hours agorootparentprevWould be a great hardware duo. Neo-pebble and Mini-nexus. reply coldpie 22 hours agorootparentHmm, maybe we can come at this from the other angle. We can't have small phones because bigger is always better, right? So how about a HUGE 5\" screen smartwatch that just happens to be a perfectly functional standalone phone if you discard the wristband?? Instant best seller! reply Zigurd 21 hours agorootparentA better use of flexible screens than folding devices. Fitting under the cuffs of men's shirts is a constraint. I dictate most text entered on my phone, so a squishy typing surface wouldn't be so bad. Camera position could be tricky. reply mcny 20 hours agorootparentProblem is different people want different things. I for one want at least all day battery while using only about 60% of the capacity so realistically two full days battery from a single charge. I then want to be able to set a max charge limit of 80% beyond which the phone won't charge and have the phone die at 20%. Ideally, I also want the phone to be able to work off the wall meaning if the phone is plugged in to the wall, it should use power from the wall and not constantly charge and discharge the battery. reply pbmonster 3 hours agorootparentSerious question, when was the last time you degraded the battery of a phone enough to warrant replacement? I got a Xperia XZ2 compact in 2018, and used it daily util a month ago. It made it through a day of regular use until the very end, when I had to retire it because of issues with software updates. Modern-ish Li-Ion batteries can take thousands of full cycles. reply f_allwein 19 hours agorootparentprevSomething like this? https://thetinypod.com/ reply ramses0 18 hours agorootparentMore like this: https://www.amazon.com/Fallout-Pip-Boy-Replica-Memory-Stand-... reply cmdtab 22 hours agoparentprevThis is pretty cool. Have you ever thought about designing pebble to look more like a traditional watch but e-ink based? reply girvo 22 hours agorootparentThat’s basically what the Pebble Time Round was, and it was phenomenal! reply organsnyder 21 hours agorootparentI backed a Pebble Time Round Gold on the last Kickstarter, which was cancelled when Pebble was acquired. Somehow the devices ended up on Amazon, and I snagged one there. It's a phenomenally stylish device. reply girvo 21 hours agorootparentMy white 20mm one with a brown leather band started more conversations than anything else I’ve ever owned and used, which was surprising but neat :) Unfortunately mine got stolen and broken, which is a shame. I wonder how hard they are to buy today, and how difficult battery replacements for them are… reply modeless 11 hours agorootparentBattery replacement is difficult, not impossible, but bordering on impossible is restoring the waterproof seal. Once you've replaced the battery you basically have to keep it away from water. I wasn't careful with mine and lost two to water damage after replacing the battery, despite re-sealing with permanent adhesive. reply organsnyder 1 hour agorootparentGood to know! IIRC the Round didn't have as high a water resistance rating to begin with, and I typically wear a leather band, so this might not be a deal-breaker for me. reply modeless 47 minutes agorootparentThe PTRs weren't diving watches for sure, but the original waterproofing was easily good enough to withstand submersion, as long as the battery hadn't started swelling yet. reply girvo 10 hours agorootparentprevAh that is excellent to know, I appreciate it! reply ClassyJacket 21 hours agorootparentprevPebble Time used an LCD display just like every other Pebble reply girvo 21 hours agorootparentQuite, but a transflective one (from Sharp, I believe) which gave it a lot of the benefits of daylight readability while being much much easier to program for and work with. reply Gigachad 20 hours agorootparentAt the cost of looking very dull and washed out. reply girvo 17 hours agorootparentWith the benefit of a much improved battery life compared to a lot of other smart-watches. A worthy trade off, IMHO. And it its a lot less washed out than current colour (real) e-ink displays reply Almondsetat 10 hours agorootparentprevWhy do you need an exciting and vibrant display for your watch? Are you looking at movies? reply Gigachad 9 hours agorootparentI don’t _need_ a watch at all. I want one, and ideally I want it to look nice. reply AgentMatrixAI 21 hours agorootparentprevforgot the name of the watch but it had an lcd display to save battery and a full touchscreen around 2017? i ended up using the lcd display mostly for HB and walk distance. it would be great if we had a completely e-ink based pebble watch with backlighting (lcd display was great but couldn't be viewed in the dark :/) reply adastra22 13 hours agoparentprevThanks! I don’t think I’ll ever go back from my Apple Watch, but pebble does still hold a soft spot in my heart. reply maxyurk 22 hours agoparentprevthe site asks if you want a new pebble and if you click \"no\" you're redirected to apple watch website LMAO reply ydant 19 hours agorootparentRedirects to Pixel watch for Android user agents. reply hackernewds 13 hours agorootparentprevwith a referral link or no? reply 9283409232 22 hours agoparentprevWhich Pebble are you bringing back? I ask because Pebble 2 was the sweet spot for me with a heart rate monitor but the heart rate monitor is not in this repo. reply hexxeh 22 hours agorootparentThe HRM aspect is mostly a small binary blob that ought to be fairly straightforward to re-integrate. The far larger issue is the lack of a Bluetooth stack. reply zamalek 21 hours agorootparentThis is really unfortunate for the existing users, especially given their profound loyalty. However, more modern chips have all of this loaded in a bootloader of sorts (e.g. nRF softdevices), so the project could prevent the RF driver nonsense going forward. reply Philpax 22 hours agorootparentprevThey don't have permission to open-source that code: https://news.ycombinator.com/item?id=42845102 reply 9283409232 22 hours agorootparentI know. That is why I'm asking which Pebble. Nothing is stopping them from writing a heart rate driver but I don't know what the plan is. reply Latty 22 hours agoparentprevI did wear my Pebble until it died, and then never found another smart watch worth it for me. Will keep an eye on this for sure. reply ashirviskas 21 hours agorootparentMay I ask why it was so special for you? As I did not participate in all the hype and now I'm a happy owner of a Garmin watch and it does seem like it is closer in specs to Pebble than most other smartwatches. Other than the openness. reply Latty 6 hours agorootparentIt was cheap, simple, did everything I wanted from a smart watch, and wasn't annoying to use (never had to worry about the battery). It wasn't a matter of a killer feature, more just the lack of the problems I see with all the alternatives since. Every time I've looked at a replacement option, I've noticed something that just made me not bother getting it. reply whs 7 hours agorootparentprevI wear a Garmin and I still miss my Pebble Time that died to swelling battery. Always on. Garmin has option to do that as well but it reduce the battery life to like 3 days. In outdoor my Pebble Time is very bright with zero backlight. 5 days battery. I went on a trip to Japan without its proprietary charger, by the time I board my flight back it was on power save mode and it died the moment the plane landed. Garmin could do this if you set it to power saving mode, but the Pebble is in standard mode. One could argue that the Garmin do have more stuff like health monitoring that Pebble didn't. Cheap and no frills. I want a second screen for my phone, not a health tracker. Originally my Pebble Time shipped with zero fitness features, and it later added a step counter once it's clear that the market direction go that way. Garmin is quite thick, Pebble Time is thinner The UI is simple press up for past event, down for future event (calendar). Press the middle button for menu. Hold are configurable. Garmin has 4 main menus which are very confusing (fitness menu, shortcut menu, apps menu, system menu). Lots of free apps and watch faces which I actually used (like a music app that show album art). I don't see any apps I would want to use on the Garmin, and they're mostly paid. The \"hide in a hole while ceiling crush the map\" game on Pebble was really well done. Now my Garmin use the simple time in Verdana watch face because I cba to find a decent one. Even with low framerates, Pebble managed to deliver cute little animations. Replying to message show a flying paper plane, screen transitions have suitable animations (not generic ones like Android), and the best one is muting an apps show a Ostrich putting its head under the ground. The animation also hides how slow the hardware actually is, with later OS versions stalling over a second or two after a second long animation. I think the phone app UI is not as good as say, Apple Watch, but it focus on apps and the store without the fitness features. Garmin's app is entirely about fitness and they hide smartwatch stuff in a menu plus another separate Connect IQ app. Overall the PebbleOS feel like a really solid and polished product than any smartwatch today. It do fewer things than most smartwatches, but that's all I care about and everything it does is very polished. reply doctorpangloss 20 hours agoparentprevThere is apparently huge amounts of goodwill for Pebble! But. Isn’t most of the value of a codebase like this not the code itself, but all the knowledge of the people who worked on it day to day? Where are they? It’s really intriguing. People who have a lot of goodwill towards Pebble BELIEVE the source code is valuable. That doesn’t mean that it is. reply Twirrim 19 hours agoparentprevOh this is awesome news. The Pebble was by far the best smartwatch I've ever used. No quirks, just always reliable. The hard work shone through with how quirk free it was. reply throwaway2037 15 hours agorootparentI never used a smart watch before. Can you share some examples quirks with other watches? reply Twirrim 1 hour agorootparentI had the OG pebble, which I only stopped using due to suffering the screen corruption bug that plagued a small percentage of the original models. Other than that it was smooth as silk. It'd last close on a month on a single charge. I literally charged it up before going on a 2 1/2 week vacation, didn't take my charger with me, and had no problems. Then had an Asus ZenWatch 3. WatchOS was frustrating at the time over the ability to customise what notifications would actually get to the watch, and the battery life of the device was terrible, getting worse over the space of a couple of years. Even had some reliability issues with the messages actually reaching the watch at all. Wanting to go back to e-ink displays and longer battery life, I've got a Amazfit Blip, who's software was just awful. Even \"simple\" things like sleep tracking wouldn't work properly, dying out or flat out not being accessible from \"Sleep for Android\", heart rate monitoring wouldn't sync reliably to anything, notifications would just randomly stop working. It also had no ability to disable certain apps from sending notifications to the watch, even if the notifications are set to silent on the phone. There was an open source app that I used to have to use alongside their own app that was necessary to actually fix everything wrong with the original software and make the device work with anything approaching reliability. The recent overhaul of their own app (seems like a ground up rewrite to me) has actually fixed most of the issues that I've had with it. It still occasionally just craps out and requires me to turn the whole bluetooth stack off and on again. reply xvector 10 hours agoparentprevI love what you're doing but the design of the pebble turns me off. Thoughts on making a sleek design that actually looks like a pebble? reply spencerflem 22 hours agoprevDon't say it often, but this is very nice of Google and they really didn't have to. Credit where it's due reply GuB-42 22 hours agoparentYep, that's the kind of thing I would have expected \"don't be evil\" Google from 20 years ago to do. All is not lost. reply mavamaarten 22 hours agoparentprevYes and no. They did buy it and just let it die. They were so far ahead of the competition at the time. reply devrand 21 hours agorootparentIt was dead long before Google was involved. Pebble filed for insolvency back in 2016 with Fitbit acquiring much of the assets. It was dead at this point. 5 years later Google bought Fitbit. reply hnlurker22 20 hours agorootparentLooks like there is money in the business of hyped-dying startups. First Pebble, then Beeper, and now Pebble again. reply hackernewds 13 hours agorootparentis there no end to the cynicism? reply xeromal 13 hours agorootparentprevIs beeper dead? reply hnlurker22 13 hours agorootparentTaken over by Automattic after a publicity-stunt against Apple reply xeromal 13 hours agorootparentOof. That stinks. reply cortesoft 20 hours agorootparentprevThey bought Fitbit, not pebble reply skort 22 hours agoparentprevnext [14 more] [flagged] CamouflagedKiwi 21 hours agorootparentGoogle didn't \"vacuum up\" Pebble. Fitbit bought them (after they were in financial trouble), Google bought Fitbit later so ended up owning the Pebble source code after that. As the top voted comment on the article says, Google didn't have to do this. It's probably driven by only a few people internally there, and if everyone's cynical and nasty about it, they're less likely to try doing the nice thing again next time. That isn't a good outcome. reply creddit 22 hours agorootparentprevPebble went bankrupt because no one bought their products. Their assets were acquired by Fitbit. Fitbit did not continue to produce the Pebble products for which there was very limited demand. Google then later acquired Fitbit. reply dtquad 21 hours agorootparentBig Tech companies acquired and acqu-hired many unsustainable small and medium sized businesses in the past 16 years. Most people don't know that because seller doesn't want to admit they were a bad unsustainable business and buyer doesn't want to admit they are buying a bad unsustainable business. reply tadfisher 21 hours agorootparentprevThey sold 2 million units, which is nothing to sneeze at for a Kickstarter-funded project. What they miscalculated was the demand for the Pebble Time, for which they spent too much money on R&D and marketing to justify the results. They should have iterated on the original design, making it cheaper, smaller, and longer-lasting. Instead they tried to go upmarket and compete with the Apple Watch, with predictable results. reply Avamander 19 hours agorootparentEric has described in-depth what went wrong: https://ericmigi.com/blog/success-and-failure-at-pebble reply tadfisher 1 hour agorootparentYes, I was paraphrasing that. reply sushid 21 hours agorootparentprevThey didn't have to do this. They didn't even acquire Pebble outright, they acquired it through Fitbit, and Pebble was just a part of Fitbit's portfolio from a previous acquisition. reply dtquad 21 hours agorootparentprev>We should be more upset that projects get acquired, shut down, and never see the light of day again because these massive companies must continue to expand at all costs. >Stop treating mega corporations like they've done you a favor when they've done the bare minimum. Most of the projects that got acquired and acqu-hired by mega-corporations were bad unsustainable businesses. reply bombcar 21 hours agorootparentMany, MANY acquihires are just someone at a large company being nice. Google should get credit for this, especially if you think it's the bare minimum to encourage others to do similar things. reply elaus 22 hours agorootparentprevI prefer to shit on companies that don't even do \"the bare minimum\" and never publish source code of their dead products. Today, this is not Google. reply scarface_74 21 hours agorootparentI’m the first to criticize Google and say they have the attention span of a crack addled flea. But I have to commend them for this. reply RobotToaster 22 hours agorootparentprevIt feels like they deliberately sat on it until it's almost completely dead. reply freedomben 21 hours agorootparentFortunately you don't need to go on your feelings, because people involved have told us exactly what happened. RePebble guy (Eric) reached out, asked them to open source, they spent a ton of effort to get it releasable, and did just that. Unless you think everybody involved is lying. reply jsheard 22 hours agoprevThey've open sourced what they can, but these third party bits had to be stripped out: All of the system fonts The Bluetooth stack, except for a stub that will function in an emulator The STM peripheral library The voice codec ARM CMSIS For the Pebble 2 HR, the heart rate monitor driver reply jsmith45 2 hours agoparentLooking though that list: All of the system fonts Presumably the source included the TTF files from which the rasterized bitmap resources were automatically generated. Including the pre-rasterized bitmaps extracted from a previous release should not be a problem as typefaces and bitmap fonts are not subject to copyright in the US, vs vector font files which are eligible for copyright as computer programs. The Bluetooth stack, except for a stub that will function in an emulator This seems unfortunate, and looks to be one of the most critical gaps in the source release. The STM peripheral library You can get this from ST no problem, although it is only licensed for use on STM devices. The voice codec It should be feasible enough to replace this. ARM CMSIS The old versions with non-free licenses are still available from ARM or ST, and the recent versions are Apache licensed (but some porting of code might be required to use to newer versions). For the Pebble 2 HR, the heart rate monitor driver This was probably based on sample code from the vendor which could be replaced. reply jacoblambda 22 hours agoparentprev> ARM CMSIS It's worth noting that CMSIS itself is open source but some of the drivers for this hardware probably were not. reply bri3d 22 hours agorootparentOld versions of CMSIS had a weird \"only for use with ARM hardware\" license header, which also carried through into most BSPs. I don't think this was resolved until around CMSIS 4, so it might just be a matter of front porting since it looks like normal STM32 stuff which is mostly permissively licensed now. reply jacoblambda 22 hours agorootparentYeah. And ngl porting up to newer versions of CMSIS isn't too terribly hard. I'm looking at porting up to CMSIS 6 soon and it doesn't look like it'll be that bad either. reply v9v 8 hours agoparentprevI remember reading that the Bluetooth stack was one of the main differentiators for Pebble at the time due to its reliability, shame it's not included here. reply aryonoco 5 hours agorootparentIn 2014, that certainly was true. Every open source Bluetooth stack (i.e., Android, Linux and FreeBSD) was buggy and unreliable. Since then they rewrote the Bluetooth stack in Android twice, and finally what's in AOSP is quite comprehensive and reliable. It's now been merged into ChromeOS as well. I feel like it's the same about many of the items mentioned above, the free/libre offering in that space are a lot more polished than was the case 9-10 years ago. Back then the audio codec was still a patent encumbered minefield, now you can just use opus. The quality and diversity of free fonts is ordered of magnitudes above what it was 10 years ago. In short, it should be much easier for Eric to fill those gaps with free/open offerings than it was 10 years ago. reply ThinkBeat 10 hours agoprevThe title is misleading. Proprietary code has been removed from the repo and the published repo will not compile because of it. Google states: \"\" This is for information only.\"\" \"\"This is the latest version of the internal repository from Pebble Technology providing the software to run on Pebble watches. Proprietary source code has been removed from this repository and it will not compile as-is\"\" reply ryukafalz 22 hours agoprevAs a Pebble user to this day (I'm wearing one right now in fact) this is amazing! I'm hopeful that this leads to development and fixes on these old devices that haven't been possible until now. And who knows, maybe we could even see new smartwatches running a derivative of the Pebble OS at some point? The old hardware's great but since they're not being made anymore it's only a matter of time before they break down. Props to Google on this. reply andybak 21 hours agoprevThis, Google Wave, Tilt Brush (and more recently Blocks). And probably others I've forgotten. This really does help mitigate the damage done by \"Killed by Google\" and people are genuinely grateful (personally in my case). But even better would be to fix the dysfunctional internal dynamics that cause this syndrome in what appears to be disproportionally more frequently compared to other corporations. reply dcreater 15 hours agoparentThat's what happens when you don't have to play by the rules of reality. Googles monopoly allows them to act like this. It's unfair and generally bad for everyone involved. Thankfully it seems that that era is finally going to end reply The_Colonel 11 hours agorootparent> It's unfair and generally bad for everyone involved. The solution is simple don't hop on Google's new products (there's a risk with the older ones as well, albeit smaller). It's just not worth it to invest your money and time with such a significant risk of it getting killed (and its general half-assedness). There are usually alternatives. reply sbrother 17 hours agoparentprevInbox is the one that still stings for me. I'm paying some silly amount for Superhuman which is almost there, but.. nothing is as good as Inbox was. reply gaudystead 18 hours agoparentprevIn case anyone would like a more complete list of products Google has killed, here's a handy website to mourn your favorite dead product: https://killedbygoogle.com/ RIP Google Wave... I had such high hopes when it was first released... :'( reply andybak 18 hours agorootparentIn case I was unclear I was aiming for the list of \"unkilled by Google\". Shorter but more interesting. reply talldayo 20 hours agoparentprevEh, I take a much more \"gloves off\" approach to this mentality. If you bought an Airport thinking that it would be your forever router, you're a moron. Same goes for buying an eero, or Google Home, or even an iPad at this point. You don't own this hardware, you control nothing about it. The idea that products live forever is a bedtime story we tell ourselves, an utter fairytale of the software industry. Everyone kills software products. The problem is our attitude of entitlement towards things we sign an EULA to use. You \"own\" TikTok on the App Store? Pssh, please. You don't even own the software runtime you use TikTok with. reply andybak 18 hours agorootparentI think you're rather missing the specific point in the process of making a much broader one. reply nialv7 20 hours agoprevGood for them, but also, what a colossal waste. Fitbit brought Pebble, Google brought Fitbit. They had everything to make a better version of a product that people _loved_, that _I_ loved. And what had them done? Nothing but a blatant display of anticompetitive power. (Yes I am still mad, 8 years later) reply pickledoyster 7 hours agoparentthen, after the damage is done, they throw some scraps to the plebs and the comment section is full of thank yous to the monopolist reply zitterbewegung 22 hours agoprevThis is pretty awesome. I had a Pebble but the battery went out and was sad to see that you had to buy used / refurbished. Being epaper and lasting for so long the best part of it was that it just managed notifications with my phone. I moved on to the Apple Watch from my brother (series 0) and it took awhile for the Apple Watch to have its killer application which was a fitness tracker. reply jsheard 22 hours agoparentYou can still get watches which use the same MIP display technology as the Pebble and run similar \"smart-ish\" software with very long battery life. They tend to be marketed more as fitness/sports watches rather than smartwatches though. reply LeifCarrotson 22 hours agorootparentMIP and e-paper are not exactly the same. I love my transflective MIP Garmin Fenix watch, but it's not nearly as high-contrast as my wife's Kindle, which uses a reflective MIP e-paper display. I would be an ideal candidate for a rePebble if I were not as happy as I am with my Garmin though with their recent changes to inReach plans, crazy prices and lackluster features on the Fenix 8, and trend towards AMOLED displays (away from their roots, chasing the Apple watch market) they're not looking as amazing as they once did. reply jsheard 22 hours agorootparentI think you're mixing up e-paper (generic term for non-emissive reflective displays) and e-ink (the trademark for a specific type of e-paper display). The Pebble used a MIP LCD, just like Coros and Garmins MIP models, it never had anything in common with the e-ink displays used on Kindles. reply ClassyJacket 21 hours agoparentprevEvery Pebble model used an LCD display. It being epaper has to be one of the most pervasive myths in tech history. It was a low powered, reflective LCD, so it did improve on other LCDs in those areas somewhat, but it wasn't eink technology. reply MostlyStable 22 hours agoprevManaged to post my comment after dang moved the comments to this thread, so reposting here: Some commenters mentioned that the e-ink screen (and the accompanying battery life) was one reason why the Pebble is so beloved, which reminded me of the Basis Peak, which was primarily a health tracker watch with some (very limited) smart functionality (mostly just some notifications, if I recall), that also had an e-ink screen and a nearly 1 week battery life and had a sort of similar trajectory: Bought by Intel, then killed two years later after a battery related recall issue. It was, in my opinion, by far the best fitness tracker watch ever, and remains so to this day. Not so much because of it's actual features (which were relatively standard), but the software paradigm of simple yet effective exercise gamification that helped encourage exercise habit formation. 8 years later and I still miss it. reply markus_zhang 22 hours agoprevJust curious, I'm not good with C. Here in the code, https://github.com/google/pebble/blob/main/src/libc/alloca.c... From what I googled, this is just for stack allocation in gcc. Does that mean Pebble only has stack allocation? reply Cieric 22 hours agoparentAlloca is allocations on the stack, seems the kernel might be stack based, but the os itself has it's own implementation of malloc. Which is heap based. https://github.com/google/pebble/blob/main/src/libos/include... I'm also seeing references to rtos as the true underlying os though. reply tesseract 21 hours agorootparentIt was based on FreeRTOS, but FreeRTOS at the time was extremely bare bones and only provided a preemptive scheduler, task management, and synchronization primitives. Everything else (memory management, I/O, ...) had to come either from whatever libc implementation was in use, or be built from scratch. reply markus_zhang 21 hours agorootparentprevThanks found it, https://github.com/google/pebble/blob/main/src/fw/kernel/pbl... Which calls heap malloc: https://github.com/google/pebble/blob/main/src/libutil/heap.... reply ryukoposting 13 hours agoprevWhatever massive nerd spent their 10% time doing this: thank you. This is a bit of light in the cynical pit of lost proprietary code. reply guzik 22 hours agoprevPebble is still the best and the only watch I've ever owned. Nothing has matched its simplicity and functionality for me. reply bix6 22 hours agoprevCan anyone comment on Google’s Business reasons for doing this? Is this actually an act of goodwill or is this a commodity, antitrust, etc. play? reply coldpie 22 hours agoparentGiven the original Pebble folks are using this to spin up a re-launch ( https://news.ycombinator.com/item?id=42845185 ), my guess is the Pebble folks just know someone on the inside at Google who managed to get the lawyers to sign off on it. reply bombcar 21 hours agorootparentFrom my experience in large (and small) companies, trying to get something that is internally dead open sourced is usually not a huge fight against people saying no, just a laundry-list of things that need to be done before everyone will sign off on it. One motivated person at a decently high enough level can get it pushed through, as long as whoever the person making the decision asks about it says, 'eh, ok.' reply 1337shadow 16 hours agorootparent> Instead, we took a more direct route I asked friends at Google (which bought Fitbit, which had bought Pebble’s IP) if they could open source PebbleOS. They said yes! Over the last year, a team inside Google (including some amazing ex-Pebblers turned Googlers) has been working on this. And today is the day the source code for PebbleOS is now available at github.com/google/pebble (see their blog post). https://ericmigi.com/blog/why-were-bringing-pebble-back reply coldpie 7 hours agorootparentHa! Called it. reply LegitShady 12 hours agoparentprevgoogle isn't doing anything with it, its not really a competitor to androidwear, and it probably has enough pebble nerd fans within google itself to push for it. reply skort 22 hours agoprevHopefully this will accelerate the production of new hardware at some point. I misplaced my old Pebble Time and was excited for the Pebble Time 2 but those hopes were dashed when they closed up and fitbit had no interest in continuing to produce the watches that made Pebble what it was. reply scarface_74 21 hours agoparentHardware is hard to almost impossible as a sustainable business as a small company unless there is a thriving ODM market and you can just use your own software. reply ThinkBeat 10 hours agoprevThe heading is misleading. Proprietary parts of the system have been removed on the current repo will not compile due to missing code. \"\" Proprietary source code has been removed from this repository and it will not compile as-is. This is for information only. \"\" reply richardw 8 hours agoprevGive Google their due. It’s infinitely easier and risk free to just bury and ignore. Thank you, Google! reply franczesko 19 hours agoprevGoogle, if you hear, please open source Stadia reply ctippett 19 hours agoparentAt least the controller firmware! reply miohtama 8 hours agoprevWhen do we get to the point an AI can auto complete the removed driver and other code? reply insane_dreamer 22 hours agoprevStill have my Pebble watch from 10 years ago in a drawer somewhere. Great watch, good times. I'm back to a \"classic\" analog watch as I realized I don't really want notifications. But I might buy a smart watch (probably Apple watch at this point since I have a bunch of iOS hardware) for the health features (HR, EKG, etc.) reply pyuser583 20 hours agoprevHow much work is being done in the RTOS space? I would think a lot, but then I hear programmers complain it’s not worth it. Are RTOSs still the future? reply Zamiel_Snawley 15 hours agoparentI’ve never heard that RTOSs are the future in general. With that said, there are a lot of great RTOS options out there today. 1) PREEMPT_RT recently brought realtime capability to mainline Linux[0]. 2) Amazon open sourced the safety-certified ThreadX[1]. [0] https://en.m.wikipedia.org/wiki/PREEMPT_RT [1] https://en.m.wikipedia.org/wiki/ThreadX reply tim 14 hours agorootparentThreadX was open sourced by Microsoft. reply Zamiel_Snawley 13 hours agorootparentYou’re right! I got my wires crossed with Azure and AWS. reply follower 3 hours agoparentprevTL;DR: No. Maybe? Depends. It's probably reasonable to make a distinction between \"Real Time\" desktop/server OS (on CPUs) vs \"Real Time\" embedded hardware OS (on MCUs). (Even aside from any hard-/soft real time distinction.) On the embedded side, in addition to FreeRTOS (upon which Pebble OS is built), I'm aware of others with reasonably high profile such as: * Zephyr (Linux Foundation, C): https://en.wikipedia.org/wiki/Zephyr_(operating_system) * NuttX (Apache Software Foundation, C & C++): https://en.wikipedia.org/wiki/NuttX In addition, there's also some \"up & coming\" Rust language projects which fall somewhere along the \"framework\" to \"OS\" spectrum (in part, via https://arewertosyet.com): * Tock: https://github.com/tock/tock * Embassy: https://github.com/embassy-rs/embassy * Hubris: https://hubris.oxide.computer On the desktop side, I seem to recall in the past, OS such as BeOS & QNX have been presented as a possible future for real time desktop OS that hasn't arrived. As someone else already mentioned, PREEMPT_RT being merged for Linux is a recent development somewhat in this space which could have impact on both desktop & \"embedded\" situations but suitability varies dependent on, say, whether you're wanting to use it for audio production versus controlling some 10 tonne robot operating next to humans. Hope this at least goes some way to answering your question. :) reply abhishekjha 21 hours agoprevAren't floating point comparisons frowned upon? https://github.com/google/pebble/blob/main/src/libc/math/flo... I guess you can freeze some compiler options to give you consistent results. reply kccqzy 15 hours agoparentThis case looks okay. It's converting a double to an int64_t and then back to double for comparison. It doesn't really suffer from the typical reason why floating point comparison is frowned upon, such as catastrophic cancellation. I've once optimized a function to be faster, and in a unit test asserted that the old slower version gives exactly the same floating point answer as the new optimized version. It's doable in some cases. reply Gigachad 20 hours agoparentprevI've only done a little bit of arduino programming, but for that you had to import a whole library for floating point math which took a huge amount of space, and many chips didn't even have hardware support for it. reply tucnak 22 hours agoprevTo somebody who's completely out of the loop, hears about Pebble for the first time—why is this big news? Are we going to see novel software written for defunct hardware, or is the hardware going to evolve now that the software is open? reply jabroni_salad 22 hours agoparentIt's one of the original smart watches. What really sets it apart from modern ones is the e-ink display and dead simple interface (just a few buttons, no touchscreen). This simplicity means that it continues to do what it does well and doesn't really feel like it has aged badly. Certainly I would rather wear a pebble than a gen 1 Samsung watch or moto360. It's a delightful bit of kit that was sadly abandoned by owner and it's nice that they are open-sourcing a dead product instead of just leaving it to rot like so many other electronics are. reply Gigachad 20 hours agorootparentThe pebble did not have an e ink display, It was just a low power LCD. reply Zamiel_Snawley 15 hours agorootparentThere were many models of Pebble. The original definitely had epaper[0]. [0] https://www.kickstarter.com/projects/getpebble/pebble-e-pape... reply Gigachad 15 hours agorootparentIt was an LCD[0] but you are right that they marketed it as \"epaper\". But it certainly was not people normally think of as epaper where there are colored capsules that can hold their image without power. The Pebble just had an LCD that looked visually similar to epaper. [0] https://en.wikipedia.org/wiki/Pebble_(watch)#Hardware reply Zamiel_Snawley 13 hours agorootparentI stand corrected, thanks. reply Philpax 22 hours agoparentprevPeople really like their Pebble watches, and this will help keep them alive into the future. In some future, we may see new hardware produced that could use some derivative of the resulting operating system. reply tucnak 9 hours agorootparentI'm more of a mechanical-movement person myself, but I'm also fond of these little e-ink things; and indeed, they have come a long way. Maybe somebody makes a true e-ink pebble and designs it well, too. reply 8thcross 18 hours agoprev\"This means the code being released contains all the build system files (using the waf build system), but it will not compile or link as released.\" But what can you do with this? reply astrange 18 hours agoparentRead it and educate yourself. Feed it to an LLM for training data. Put it back together with the binary bits from an existing build. reply elevatedastalt 20 hours agoprevPeople passing cynical comments at Google need to understand that at a big co like Google, something like this doesn't 'just happen'. It probably happened because some passionate L6/L7 engineer wanted to do it and pushed through the bureaucracy to get approvals for it, probably largely on their own time (by which I mean that this was at best a side-project for them and at worst a distraction that was losing them favor with their bosses). At every point in the process, they probably had to justify what they were doing to their leads, to lawyers, to privacy reviewers, who had no real stake in it and so had nothing to lose by saying No. They almost certainly won't receive any career progress out of this and would risk a setback if something slips through the cracks (such as some unredacted proprietary information). They did it because they felt it was the right thing to do. Good things happen through the actions of individuals like this. We should acknowledge and celebrate it when they do, anti-big-tech cynicism can wait. reply nialv7 20 hours agoparentI do wonder how they successfully justified this to the higher ups. If anything, I would like to learn from them so I can better justify the things I want to do.. reply oxguy3 16 hours agorootparentThe source code has zero business value to Google at this point – they have another smartwatch OS and there's never gonna be a business case for a company of Google's scale to revive a niche product like this. Releasing means getting free PR on Hacker News and a free morale boost for employees who care about this kind of thing. reply addicted 14 hours agorootparentAnd the opportunity for a shit ton of lawsuits. Source code like this doesn’t just have no value for Google. It has negative value. It’s a massive liability. That’s why big cos simply bury source code instead of releasing it in the open. reply adastra22 13 hours agorootparentLiability for what? reply zinekeller 13 hours agorootparentUnrelated to smartwatches, but the usual reason that I've heard on why video drivers are not open-sourced directly (AMDGPU =/= AMDGPU PRO and Nvidia's recent parallel driver efforts) is that there are copyright (old code that is not written by ATI/AMD or Nvidia/3dfx and AMD/Nvidia not getting the rights to it) and patent (techniques used may be patented by their competitors) concerns. reply adastra22 13 hours agorootparentFor that to be an issue here, there would have to be revenue from the Pebble line of products. Otherwise how would they show monetary loss? reply arghwhat 12 hours agorootparentThey have had revenue as pebble sold their smartwatches. Acquisition includes liability. You do not need revenue to be liable though. reply adastra22 9 hours agorootparentThe statute of limitations is 3 years for copyright infringement and 6 years for patents. The plaintiff would have to show actual financial loss due to the infringement. In the case of the pebble assets, that seems exceedingly unlikely. Generously assume for the sake of argument that the entire codebase is a straight up copy of Samsung and Apple’s IP. What damages have been caused by that IP sitting dormant on Google’s hard drives for the past six years? reply zinekeller 8 hours agorootparent> The statute of limitations is 3 years for copyright infringement ... nope. Not even in the US, where \"3 years\" as you claim is from the possible discovery date, not the infrigment date (https://www.michaelbest.com/Newsroom/340003/US-Supreme-Court...), and this would probably only accrue when the source code is released (as it can be argued that it would be difficult to see a more subtle copyright infrigment on object code). Also statutory damages exists, so even for no revenue there is a reasonable possibility that they will be sued for that alone (similar to how game and music piracy lawsuits work). > and 6 years for patents Other jurisdictions like Germany (no limits) exists. reply jmathai 15 hours agorootparentprevGoogle does not do this for PR on Hacker News. It also seems improbable, maybe heroic, for one L6/L7 SWE to push this through. reply arghwhat 12 hours agorootparentYou'd be surprised by how much PR specifically targets a certain audience. Converting a zero-value IP into low impact positive PR has business value. Making MS-DOS sources public falls in the same bucket. reply Zamiel_Snawley 16 hours agorootparentprevIsn’t it valuable to Google to keep potential competitors from getting expensive IP for free? “PebbleOS took dozens of engineers working over 4 years to build, alongside our fantastic product and QA teams. Reproducing that for new hardware would take a long time.” [0] [0] https://ericmigi.com/blog/why-were-bringing-pebble-back reply refulgentis 16 hours agorootparentIt's 12 years old. They put good work in. But it's missing an absurd amount of stuff the mass market considered mandatory a decade ago. Consumer electronics, for better or worse, is mostly about spending enough money to get your product name in front of hundreds of millions, and Google knows that well. reply The_Colonel 11 hours agorootparentI don't think Google understands hardware given its lack of success in it. reply kfir 6 hours agorootparentThe lack of success in hardware is intentional. Hardware is a low margin business that will hurt their bottom line. reply The_Colonel 5 hours agorootparentRight, like this world's largest company called Apple, which gets most of the revenue from its hardware sales. Pixels are not cheap either and given its low specs (Tensor SOC), the per-unit margin has to be quite decent. OTOH, there are significant fixed development costs which you want to spread over as many devices as possible to increase the net margin. The lackluster value and sales of Google hardware is no master plan, it's a simple incompetency. reply aryonoco 5 hours agorootparentprevWhen did you write this in 2005? Apple would like a word with you. reply refulgentis 1 hour agorootparentprevNo, it's not reply JumpCrisscross 13 hours agorootparentprev> wonder how they successfully justified this to the higher ups Commoditise your complements. Injecting competition into the watch market reduces the chances one of the majors, e.g. Apple or Altman, runs away with the wearables sector if it takes off again in respect of AI. reply JohnKemeny 10 hours agorootparentThis is not exactly what is meant by Commoditise your complements. \"Commoditise your complements\" in this setting would mean making watch OSs free for everyone in order to make demand for watches go up. reply JumpCrisscross 9 hours agorootparent> This is not exactly what is meant by Commoditise your complements You're right. The strict interpretation would be the more people who wear smartwatches, the wider Google's surveillance net. Is there a commercial strategic term for denying your adversaries oxygen? reply anovick 8 hours agorootparent> Is there a commercial strategic term for denying your adversaries oxygen? There's Microsoft's Embrace, extend, and extinguish https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguis... reply JohnKemeny 5 hours agorootparentprevYou have predatory pricing, which is illegal. According to Wikipedia: > Predatory pricing is a commercial pricing strategy which involves the use of large scale undercutting to eliminate competition. reply refulgentis 18 hours agorootparentprevGoogle's a strange place currently. Something like this would be bizarrely easy if you had the ear of the right person. (i.e. someone who would tell legal etc. \"you gotta do it\" when asked) Reason why is, it's a feel-good thing that aligns well with Old Google values, and Google's not yet old enough for \"Why bother doing anything at all?\" to be an acceptable vocalized response. It has standard, if not worse, dysfunction with internal conflict at this point. But there just isn't room to come up with a good reason to not open-source the decade-old OS. reply adastra22 13 hours agorootparentThat seems like big standard organizational dysfunction. reply itishappy 18 hours agoparentprevI think it's perfectly reasonable to celebrate this action and the individuals who championed it while also being cynical of the big tech orgs that resist changes like this for the reasons described. reply xeromal 13 hours agorootparentBeing cynical is almost a default of the internet and I think it squanders a lot of hope. It's better not to be cynical reply adastra22 13 hours agorootparentIt is better to be less cynical than your average redditor or HN denizen, but not entirely uncynical. reply drzaiusx11 5 hours agorootparentI'm going to call the above rule, \"Goldilocks Cynicism.\" Words to live by. reply follower 2 hours agoparentprevFor visibility (via comment by 1337shadow @ https://news.ycombinator.com/item?id=42848245): > Instead, we took a more direct route I asked friends at Google (which bought Fitbit, which had bought Pebble’s IP) if they could open source PebbleOS. They said yes! Over the last year, a team inside Google (including some amazing ex-Pebblers turned Googlers) has been working on this. And today is the day the source code for PebbleOS is now available at github.com/google/pebble (see their blog post). https://ericmigi.com/blog/why-were-bringing-pebble-back reply raxxor 5 hours agoparentprevWhich is ironic because Google needs to improve their reputation about sunsetting early. This is one of the main arguments for why many businesses for why they do not employ their alternatives reply robocat 18 hours agoparentprevThe only financial benefit for Google that I could guess would be to drive complement prices to zero (assuming some Pebble refresh is successful), as part of competition against Apple watch. reply Eridrus 15 hours agoparentprevIIRC based on my experience open sourcing some code for a paper, I think you just need to get VP approval, which is probably not that hard if you are a well respected L6/7 in your org. reply ksec 11 hours agoparentprevThey could consider open source everything you have killed ( https://killedbygoogle.com ) over the years. May be the whole design and development process from the start should be everything they do could one day be open sourced. So be aware what you do and what you comment. reply teractiveodular 9 hours agorootparentIt's basically impossible for Google to open-source anything built in house, because it all relies heavily on Borg and various other proprietary tools and libraries that don't exist outside the Googleplex. Pebble, on the other hand, was built fully outside Google and only ended up there via a circuitous route (the Fitbit acquisition), so this is not a concern. reply rkagerer 11 hours agoparentprevSounds like a horrible place to work. (That said, I offer my gratitude for the perseverance it took to get this done) reply cmrdporcupine 17 hours agoparentprevFrom my experience with working there, it wasn't actually that hard to advocate and get things open sourced that my team worked on after Google made the decision to kill it. (ndash video player stuff that was being done by the team I was on in Fiber @ Google Waterloo). Management is on the whole open to these things, and Google unlike other companies I've been at has an official process for this, and it feels on the whole fair. For all the sins that Google commits, they're generally decent about the whole open source community. There's things they rely on they should be providing funding for, certainly. But they're good about letting their people contribute, and often good about opening things up to the outside world. Of course this project is bigger and more complicated, and clearly had tendrils into a lot of other things, so... reply jeffbee 16 hours agoparentprevI mean, Google develops three other open source operating systems and is a major contributor to other operating systems, as well from other vast open projects like Chromium. It's not as if open-sourcing an operating system is even slightly out of character for Google. reply jayde2767 20 hours agoparentprevMaybe...the cynic in me says that someone feels this needs to be preserved because it will become another Google relic in favor of something newer, or more shiny. Or likely something written in Go...just a guess on my part... reply frosting1337 20 hours agorootparentWhat do you mean? Pebble was dead before Google ever bought Fitbit. reply rvz 22 hours agoprevGive credit where credit is due, at least Google gives back to the community. Well done. reply Alive-in-2025 21 hours agoparentWell, better would have been investing in it and making it better. At least they didn't produce some awful spy on you thing that had nothing to do with p and called it pebble reply TulliusCicero 8 hours agorootparentPebble was dead long before Google got to own this IP. reply maxehmookau 22 hours agoprevThis is cool! I remember receiving my first Pebble watch on kickstarter. I felt like I was in the future! reply PokemonNoGo 22 hours agoparentyou still will be! In a few! reply 1337shadow 16 hours agoprevvery cool but garmin quatix is hard to beat reply glietu 22 hours agoprevThis really made my day! reply atYevP 22 hours agoprevLets go! reply j45 22 hours agoprevBig news. It was so good. Now a new group of people will get to consider it. reply whalesalad 22 hours agoprevAre there any applications that are also open sourced? They mention snowy but it is giving a 404: https://github.com/pebble-dev/snowy reply zorgmonkey 19 hours agoparentI see a bunch of demo\\test apps here https://github.com/google/pebble/tree/main/src/apps reply tech234a 17 hours agoparentprevSnowy link works for me now reply will0 19 hours agoparentprevTry that URL again :) reply follower 4 hours agorootparentTL;DR: Tintin in the Land of the Public Domain. I had forgotten Pebble used Tintin-themed code names (which I assume was the inspiration for the Snowy assistant app's name?)... Tangentially & coincidentally related: the Tintin & Snowy[0] characters entered the/a Public Domain[1] at the beginning of this year! Which means your lawyer might advise that you might now actually be able to use an actual original Snowy illustration[2] for the app logo... I mention this primarily because I am currently (in theory) developing a Tintin-themed game for an annual Public Domain game jam[3]. In reality, I've spent more time trying to locate scans of Tintin-related documents/illustrations[4] that actually fall under the constraints necessary for even US Public Domain[5]. [0] Milou. [1] Well, actually[1a], maybe only in the US for 2025? And 2034 in Berne-associated countries? And 2054 in Belgium? Or any year if you're an AI, seemingly? Okay, perhaps it's better to not use an original illustration. Such are the joys of actually trying to interact with the Public Domain in good faith[1b]. [1a] https://en.wikipedia.org/wiki/Tintin_in_the_Land_of_the_Sovi... [1b] It just now occurs to me to wonder whether or not Milou can actually be referred to as \"Snowy\" given Tintin wasn't translated into English until the 1950s (late 1950s for the use of the name \"Snowy\" rather \"Milou\") (and as late as 1989 for the first title \"Tintin in the Land of the Soviets\"), given translations are AIUI new works? [2] First appearance: https://en.wikipedia.org/wiki/File:Tintin_and_Snowy_from_Tin... [3] https://itch.io/jam/gaming-like-its-1929 [4] Pretty interesting finding different variants on archive.org, e.g. [4a][4b][4c]. (After all, \"entering the Public Domain\" is not of much value if the related material isn't accessible or the status is unclear. (Which is why recent trends in FLOSS project copyright year ranges statements bug me...)) [4a] Original French language album: https://archive.org/details/tintinnb/Francais/Tome%2001%20-%... [4b] Second French language newspaper serialization: https://archive.org/details/cv-1930-50-pb/page/n3/mode/2up [4c] Original physical illustration: https://archive.org/details/tome-01-herge-chronologie-dune-o... [5] Okay, yeah, this kinda turned into a Public Domain rant, sorry? :) (To bring it back to the topic at hand: \"Hey, can't wait until this Pebble OS code enters the Public Domain in the year `2024 + YYY`.\" :) ) reply fsflover 22 hours agoprevDupe: https://news.ycombinator.com/item?id=42845017 reply maz1b 17 hours agoprevNow if only Fossil did the same for Misfit and Misfit rays.. that would be very appreciated. reply christkv 8 hours agoprevNow we need a new watch that can run this. reply gigatexal 19 hours agoprevYeah huge influx of goodwill to Google for doing this. I signed up. I wanna see the pebble folks succeed here. reply 8thcross 18 hours agoprev [–] \"This means the code being released contains all the build system files (using the waf build system), but it will not compile or link as released.\" What do we do with this POS? reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Google has open-sourced the Pebble OS, generating enthusiasm among fans and developers for potential new developments in smartwatch technology.",
      "The release on GitHub does not include proprietary components such as system fonts and the Bluetooth stack, so it cannot be compiled in its current form.",
      "This move is viewed as a positive gesture from Google, attributed to internal efforts, and is seen as a step towards reviving the Pebble smartwatch ecosystem."
    ],
    "points": 1207,
    "commentCount": 192,
    "retryCount": 0,
    "time": 1738008549
  },
  {
    "id": 42850222,
    "title": "Run DeepSeek R1 Dynamic 1.58-bit",
    "originLink": "https://unsloth.ai/blog/deepseekr1-dynamic",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"unsloth.ai\",cType: 'managed',cRay: '90933c432bbcaaa6',cH: 'r7yuuE6XeYySxS9pClFoRrLTxG9b2B9BT.3GrVtE_pI-1738090931-1.2.1.1-U4XJM2V.p1deVrJ33.ou_jv9_2z00OnBI62reDwZ7pf4kfDNnBANtdtiKvkNczjh',cUPMDTk: \"\\/blog\\/deepseekr1-dynamic?__cf_chl_tk=C3E_wZXY.s1PwMGsXEEXSAEqqJDGSKWcbv7zRW.JSJ0-1738090931-1.0.1.1-BvwyYStT9VmqP972fiaHqZWZzGmTfCwv5kcbE8OGYoE\",cFPWv: 'g',cITimeS: '1738090931',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/blog\\/deepseekr1-dynamic?__cf_chl_f_tk=C3E_wZXY.s1PwMGsXEEXSAEqqJDGSKWcbv7zRW.JSJ0-1738090931-1.0.1.1-BvwyYStT9VmqP972fiaHqZWZzGmTfCwv5kcbE8OGYoE\",md: \"un_6XjUQaZ5PW8Gn7qkQ_Zq6hb643PykMJU1.LTOtjg-1738090931-1.2.1.1-VhyhYghHWhhSeVECjaIVqK5o49_Uic1Ri5W75xQ0_rmkAakdQVTI9g0S2.Bq82dO3w14hAlJ4_VXV2Gkd8Kw2nP3fGTZrTUXAAttUPtDTIgzDcBpfFjtI_m8.LDVwPxmeNVJCP7ixXezDn0O4XOvtjh8JzKOKbsonEDnipLlJxGwX2ACPzc2szpF4wlcPJrQnBe5lMoZ1wy2SCdz7syRfvgd5EgTHT_1VloHZa8JQZDeMReGczv0qOOiUJbCAM94iavpEKWFK8FPzWjHhFLSOvZS5RtgdoBcxVWeuQWA5D7WS8clU69lBE77_O3wnPA71mUB4rk46q9hFxFInQDD.oHJcEPdC3CCZ1GEPV01B18wXVKOBO74g7WYwDLjwblJD.huFlczC148PilLWtpk5LivCuE45zkXiiAFIMfYtBzuKbi866BUdv4a4OVzEZKibQR9.tRQuJ8lS3eWyc4eqTPd8jFiVfliWFoBKXCkMopZ7kceBJoX4FWisUvee9PPcD.xXMzFXLN0pT7kK1vlZKQY1YfwmTYOoxcIvJSYM2eYvhm8uDdgmg03yKkMfrSACUQsQ32xaipbLWUTVMBwkUEWVoMQwYT5U4cgWdo16lD0vrE0R6LGVi4CofqFa1wt6Jumhs.MfT4lu_CCuVvi_G_yrITLVxOnyAdDkwL4NPi80r2b3qm0qsFa2UQUaFeV22vthXCv_iMxm9QKemsS963XrjYm6HYJ4ANeA8mN6E1AkgEO43KBOwygfn9Mt8.O_V1_M142jbfYRaX47BYT.9mEQZs5GBmFZB9pgQOMqnW18BkSgVJpESpAxnJ.aMvLzFTRfC8KMhWTuEWrkYC4aiQoF6dw.9jm7Qy5COJ6GEdSRTJv54_2jiRcPXLidX6aea828kdxgiUGOUTp.q.Khah_uZ3P7pip.T0zzOg3TLCvgSltw1iRKgx3U0gtvE6qZVp2ivtUCUtcaQ6Xxx1nMBQObQySvd8SGNbJsRu7xHPt0D3SVTqXvdRizcFrRjqanz_LKjTOZAXTExjO3cI9Ywrx4P9eBulVCT9.mhtKqg8dyb5yfONVfvtuQOxOEhx2osb_eoMhy8ahmwYZ34uc_Jz5WIvgpUkBs6ql3r7ksRRbldbGu2uMRn3j6PoOAQ.6bMiVZugqDXPHNvbzhot59cfDuPTX4g.l_pBRhuuHqj8atdJ.lT8Iq.exXRX.TkoXl2fTcHYbz4KhxPmEAIvvXw1UCLH7Aj7KHn11uz2YjqtVJN8IGYGPPLprAlUL8cxdIKhMpdhn9jF65zCy.TizhRFsc6HYlZ.53DzavzviHOL1tz82o34mejoq3hnK4j2q6PwhBozSQ8lAzD4ItVhyAoaabjGq4XmZzrhdPTlo8RYFiKxGB0XmeiYDNgQW4ayXU_QVYP0R.JxNpey9bhjIqCqVv7vEr87_fugkCnorWw_OWLeIBotRitnV7VieGQ3GVoZ2ttAmLv6tYutUykY154wg9smeSlJHp1IASB96BDYy6Xi8gI0iJbxpe.pSK2OMwqj1eE8mGcrHEWQ5_J4cEwphAynTAqSEKNp6ktuVG8wUY1fIU5BWV56rO0HDhnlGKdySZtqePl4w_cwuWWJ9OmIEX2cqspb7GlvKQtY7LLwI9ag7bYLV2kcqmZp6S.REudvgYjIFN71F.gup4f3XDbEBPuV6mNAEcumO7R2bFMf5beq.8n2cXO1VQf9kUeevW2O_SqPjiRn5cFQeyeOHTWxvGKQv7lUzIPyQuVcyys.uOsr7gtFyG.0qJ.0bGVq1Riv7v4aR0WyrVHkNYRw3IPkeleKalDT8hc7.kEGPM3PwU7q8g3TIwZG0RwG5h415FjWsMQZFGzUr3SuErIXBkU8pT2AWZVrGQFEdL0Jcsu4U4J3sBA80ja_mOBfEy_ijI8yxsIT2L_uFxCwyu2N_rCli9ZqJ1agEPw81ErkXI8n7SKZKBHT_3mmthXyGiQdD_HfbPF4fuYA6FOIzIin9JhXONR.QS_lWKWdJ8gRObWT78AsA86XaVgzqmarsOrSVLQXvL_mYg3QeLG0QW4XI7LYLjHPF08DNJAcTQLgRn4PHgRj4jqhB.jPNgDHavXheX8wN3KinNqSG63aj0x.PtdTAeisFIU_fhNtTL9nY5hzgKhCBBvf1hBMNJrDBNrdnXuLuSJqpWcPiWGNZxdgBQvM_mQsxdDKs83V1LBbrhHFesnkcyYcYEtehuGwppPhMPoLPWLVAhcMWG2LZCW1ak_Ker7FUSfTlKfM.z4KEeWGIUBr4DGq8qDHxO6lmKy6y7TBADuKyyNIeQcIqvDmo8zfDlEESPIkTt.fTjnO_U2wEYdp2McrTAGzUoTdx1Op._ppjm4tNLkfr.vb7375u8ixboEkFHq5fuhEFkXoi6IaD74oGLyDciNhAEyehYFJf\",mdrd: \"KZPEjsAWlKYWQ3LUptBf09fVVkRZUVFtbssHm.R4U4A-1738090931-1.2.1.1-hKRxjfHTgpu15pUzrl0cBwK7v81bxnseyY96jYlSXe26EGSmuzds_COxfHqWOm1yOfwcaDrtnfuej7FWpyTyiUrQQkfDblC2rElVZbajf63MAPkhpt9QTvWAZ3P_EKAmR1_aNij2FuBQKO5yFimAw_1l9bDl62.hqJPQvn_owK_wMxUjXJvl02FAoV5X6BCLjPemxCXPJT6TFqRo.Tcre7RR86M3IiBsnuZ5VSL074aZt.x8WSfLAqtrRAyQohLHOi4BCBQbC2RUzQ1J0RWrI7IILEeZeNWbySMYi5Ty5ySyWFw008hXx3TTe0VAp_zV2ItVLMLD4lHtrLzjZyl58MGArLGy6QA2S5d5WgD_K0Jtq.Q_raBvrMeEgm2HPggO8S8wHCMm9Gu2yVCG9CZPbiJEOtqd7UA7gwO3mTYnHzRTEMep7Hm9rSbEQ8gATTg5oPfP6Lt.X7gt1ZgjMpC33wMWNF4_iIMpHuFsEkYLTXWp9mRdi6hZyTezCdUJaiPJ6tKaQ6F2ZWK6rq4RC2smMyyweqLjkCTqwxpWzE2mysayTz76mZqXUNVGcdey8FkiO5HIUbGQ1gVCPAbVIVWUH0fXQNTt4xceazWBmK.SJX7.3.TMy0x9LdmZoHxRfJjIPsAX_3DcWDMJLSGlTDRBYbBsz.c07t0zyk4I_RWFjECeqeY2G4mSz9RYJdUUO4HoY4ZN2jOlaMRHMcrSeCKgnjVmU0lqQgEpVJd6Uu3CG.pFC57kEscKPEBT0RRkpzWGTNZhusFC.9YRvIaVLVKeMUiaESjjULB23e_7vzr9FtoFxemrDs73YjgTiEg9chczdtASKaduUJiiV4k2jfEVjdyZKExmy34PyQyag61i6XCPqg0XPZhmRJBlmX6G90C42s2VhmzMMZQwwynUcDBho_CVtM6Iuj2mVTxL0pgXTpw8urrCPh6O2vV5BeHP5d27XS4TjeLj7HtXg99N_sXkihpgE4MGZ4bJOD7UUpuEPuSZETjIGSUdP4GAD.mJVSlPdq4TtIofdqf7dxLvaHsprAGqXhZ_P1kOdJ7rvgvtCSLx2MATQXldQHArojJSmcFOxKV3XOli70Jd8YnfqlDFF3cTB39JwXhX_mmjOyJP08VFDvb9fNyKzPSNcgnYjBjjukYisyqsqQEmHD8BiAX4_Or3ys_V0OON02VYsfHq1dqYdmMGVCZo3FD9sKODMmDz1cT.1m5UeDThfkh6rwZtJmSJODlNW2DckwhFXb2DiMoFyoG1q7ZC.saPh30dmDX29EJLLOena3GT9Q02LL61iQKeyvcJ6nUwrE_uXxOBTfm3JR8Pb1By8xifG9e37IAtjTX0BIwDg.LjDnA_Z.hgf2GQmcQXXZAXDQo1g4MLHCQyHs1EK3BZ51XTe9dkcCey7nmH1izuJvtDs_YjgRNEFuesJeh_waSZKG9VIblUu8OyRHW1TOInwWhAEKIpzcrkPTITzA.8CK4IxJAleeqCbb68O5zbJIXh9X2Dd52fXUIUDVDybjFedc6.PlE04ZxHiszWWD8p8tSdcgoqDwZ3Ha9kwsQaPbm5oWsP2HijwOJ9U4oQSJ_QGDnO2Xu1TtNg09E6cYx41T1t34TuGio4jvjq4oq3N_kirGus.1hSdADcdELqDN85NeLRR1FRGEPVs57y5RrKxK7m5seUtoqvgcVa4bvcL4sPQfK0JG3tsGsB9Znc7yJwouXHzElwYgF0STo2g41eY4E2kjhVt9juIklqONgEV3NNsCZCJkq63QClpRNUuVGczdOlMqDZVx36dvZY1WsW3Be5_aSaWoqdmGCPKO8_UXxfER8s1faIyx7pdk6K1vaNj7zKcDTQtPtBGlaCCC0kcLYy4EUIvz.JXgQ4lfudQS4e7KQdPf8ydLy9zgKEqRvIPiH1hLmO6.NBUge75RptOfgKnaTCXBxbLS91jgYMeELPbQqqyHibdUABqVS99Ca1JizeUUjekLNhUwHL0ws8lhEWIoo.XaFCHHolpa6eA8sdBYC6kOR2arM4HJc9GpqS7E4W0AKX8Suc_4w8rXtV3vOa0vN.vtWTBw1l1QlITKtAF3edEqlC8SMIrTd9wVOcaSkPI2h3fsMbSvvjjV8XiXyvlo8WY1rC2K98z4Gwlsk.21qMFuJXg1Cq12IhBuHaiffokB5tbUS6IJCoKmQ9rqB2EV0b_5sBdHgZwegZLs7SGJbVaHA9gcvXUo0K0.d2jr8IRbefqJrpbMZFzQeMuptNuDOb1Np.x.lnQsrCW_RCNHycJgL82NwLfNmW1HlfHqRMs._ldahJqDV4cj1AFhMDIOnLtl3Hii_ystgzmmMtoWXWMu81dAiTxBG.oru9jE7gyNLsd9aBdi.S1h8Ursi0.M3mvnObmA\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=90933c432bbcaaa6';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== 1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length window._cf_chl_opt.cOgUHash.length).indexOf('?') !== 1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/blog\\/deepseekr1-dynamic?__cf_chl_rt_tk=C3E_wZXY.s1PwMGsXEEXSAEqqJDGSKWcbv7zRW.JSJ0-1738090931-1.0.1.1-BvwyYStT9VmqP972fiaHqZWZzGmTfCwv5kcbE8OGYoE\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42850222",
    "commentBody": "Run DeepSeek R1 Dynamic 1.58-bit (unsloth.ai)595 points by noch 10 hours agohidepastfavorite239 comments Jasondells 7 hours agoAn 80% size reduction is no joke, and the fact that the 1.58-bit version runs on dual H100s at 140 tokens/s is kind of mind-blowing. That said, I’m still skeptical about how practical this really is for most people. Like, yeah, you can run it on 24GB VRAM or even with just 20GB RAM, but \"slow\" is an understatement—those speeds would make even the most patient person throw their hands up. And then there’s the whole repetition issue. Infinite loops with \"Pygame’s Pygame’s Pygame’s\" kind of defeats the point of quantization if you ask me. Sure, the authors have fixes like adjusting the KV cache or using min_p, but doesn’t that just patch a symptom rather than solve the actual problem? A fried model is still fried, even if it stops repeating itself. On the flip side, I love that they’re making this accessible on Hugging Face... and the dynamic quantization approach is pretty brilliant. Using 1.58-bit for MoEs and leaving sensitive layers like down_proj at higher precision—super clever. Feels like they’re squeezing every last drop of juice out of the architecture, which is awesome for smaller teams who can’t afford OpenAI-scale hardware. \"accessible\" still comes with an asterisk. Like, I get that shared memory architectures like a 192GB Mac Ultra are a big deal, but who’s dropping $6,000+ on that setup? For that price, I’d rather build a rig with used 3090s and get way more bang for my buck (though, yeah, it’d be a power hog). Cool tech—no doubt—but the practicality is still up for debate. Guess we'll see if the next-gen models can address some of these trade-offs. reply danielhanchen 7 hours agoparentOh the repetition issue is only on the non dynamic quants :) If you do dynamic quantization and use the 1.58bit dynamic quantized model the repetition issue fully disappears! Min_p = 0.05 was a way I found to counteract the 1.58bit model generating singular incorrect tokens which happen around 1 token per 8000! reply smcleod 6 hours agorootparentmin_p is great, do you apply a small amount of temperate as well? reply huijzer 6 hours agoparentprev> That said, I’m still skeptical about how practical this really is for most people. I'm running Open WebUI for months now for me and some friends as a front-end to one of the API providers (deepinfra in my case, but there are many others, see https://artificialanalysis.ai/). Having 1.58-bit is very practical for me. I'm looking much forward to the API provider adding this model to their system. They also added a Llama turbo (also quantized) a few months back so I have good hopes. reply jairuhme 6 hours agoparentprevAt my work, we self-host some models and have found that for anything remotely similar to RAG or use cases that are very specific, the quantized models have proven to be more than sufficient. This helps us keep them running on smaller infra and generally lower costs reply michaelt 5 hours agorootparentPersonally I've noticed major changes in performance between different quantisations of the same model. Mistral's large 123B model works well (but slowly) at 4-bit quantisation, but if I knock it down to 2.5-bit quantisation for speed, performance drops to the point where I'm better off with a 70B 4-bit model. This makes me reluctant to evaluate new models in heavily quantised forms, as you're measuring the quantisation more than the actual model. reply sitkack 5 hours agorootparentHow are you doing your evals? Being able to do semantic diffs of the output of the two models should tell you what you need to do. reply elorant 7 hours agoparentprevNot everyone needs the largest model. There are variations or R1 with fewer parameters that can easily run on consumer hardware. With 80% size reduction you could run 70B on 8-bit on an RTX 3090. Other than that, if you really need the big one you can get six 3090s and you're good to go. It's not cheap, but you're running a ChatGPT equivalent model from your basement. A year ago this was a wetdream for most enthusiasts. reply whimsicalism 3 hours agorootparentThere’s a huge difference both in capabilities and in meaning between “variations of r1” and “r1 distill”. ollama is intentionally misleading people on this but the distills are much much worse reply thot_experiment 42 minutes agorootparentThey're really not? Both subjectively and in benchmarks there is no world in which the delta between the models deserves a \"much much\". reply brookst 4 hours agorootparentprevOr if you want a large model but don’t need high performance, get a Mac with 128GB UMA. reply Kye 7 hours agorootparentprevI ran whatever version Ollama downloaded on a 3070ti (laptop version). It's reasonably fast. Generative stuff can get weird if you do prompts like \"in the style of\" or \"a new episode of\" because it doesn't seem to have much pop culture in its training data. It knows the Stargate movie, for example, and seems to have the IMDB info for the series, but goes absolutely ham trying to summarize the series. This line in the stuff inside thesection suggests it's also been trained on YouTube clips: >> \"I'm not entirely sure if I got all the details right, but this is what I remember from watching clips and summaries online.\" An excerpt from the generated summary: >> \"Set in the 23rd century during a Z-Corp invasion, the series features action sequences, strategic thinking, and humor. It explores themes of international espionage, space warfare, and humanity's role in the cosmos. The show incorporates musical numbers and catchy theme songs for an engaging viewing experience. The plot involves investigating alien warships and their secret base on Kessari planet while addressing personal conflicts and philosophical questions about space.\" \"It explores themes of international espionage, space warfare, and humanity's role in the cosmos\" is the closest to correct line in the whole output. reply Mashimo 7 hours agorootparent> ran whatever version Ollama downloaded on a 3070ti (laptop version). It's reasonably fast. Probably was not r1, but one of the other models that got trained on r1, which apparently might still be quite good. reply woadwarrior01 6 hours agorootparentOllama has been deliberately misrepresenting R1 distill models as \"R1\" for marketing purposes. A lot of \"AI\" influencers on social media are unabashedly doing the same. Ollama's default \"R1\" model is a 4-bit RTN quantized 7B model, which is nowhere close to the real R1 (a 671B parameter fp8 MoE). https://www.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_... reply wklauss 4 hours agorootparentOllama is pretty clear about it, it's not like they are trying to deceive. You can also download the 671B model with Ollama, if you like. reply whimsicalism 3 hours agorootparentno they are not, they intentionally remove every reference to this not being r1 from the cli and changed the names from the ones both Deepseek and Huggingface used. reply woadwarrior01 4 hours agorootparentprevYeah, they're so clear in fact that they call the distilled models \"R1\" in the url and everywhere on the page[1], instead of using the \"DeepSeek-R1-Distill-\" prefix, as DeepSeek themselves do[2]. [1]: https://ollama.com/library/deepseek-r1 [2]: https://github.com/deepseek-ai/DeepSeek-R1#deepseek-r1-disti... reply horsawlarway 4 hours agorootparentprevI mean... yes. The DeepSeek announcement puts R1 right there in the name for those models. https://api-docs.deepseek.com/news/news250120 It's fairly clear that R1-Llama or R1-Qwen is a distill, and they're all coming directly from DeepSeek. As an aside, at least the larger distilled models (I'm mostly running r1-llama-distill-70b) are definitely not the same thing as the base llama/qwen models. I'm getting better results locally, admittedly with the slower inference time as it does the whole \"\" section. Surprisingly The content in thesection is actually quite useful on its own. If you're using the model to spitball or brainstorm, getting to see it do that process is just flat out useful. Sometimes more-so than the actual answer it finally produces. reply Kye 6 hours agorootparentprevI'm not too hip to all the LLM terminology, so maybe someone can make sense of this and see if it's r1 or something based on r1: >>> /show info Model architecture qwen2 parameters 7.6B context length 131072 embedding length 3584 quantization Q4_K_M reply Mashimo 6 hours agorootparent\"Qwen2.5 is the large language model series developed by Qwen team, Alibaba Cloud.\" And I think they, the DeepSeek team, finetunes Qwen 7b on DeepSeek. That is how I understood it. Which apparently makes it quite good for a 7b model. But, again: if I understood it correctly, is still just qween and without the reasoning of DeepSeek. reply randomifcpfan 4 hours agorootparentIn my application, code generation, the distilled DeepSeek models (7B to 70B) perform poorly. They imitate the reasoning of the r1 model, but their conclusions are not correct. The real r1 model is great, better than o1, but the distilled models are not even as good as the base models that they were distilled from. reply whimsicalism 3 hours agorootparentprevit’s a distill, it’s going to be much much worse than r1 reply narrator 5 hours agorootparentprevIt is hilariously bad at writing erotica when I've used jailbreaks on it. It's knowledge is the equivalent of a 1980s college kid with no access to pornography who watched an R rated movie once. reply rafaelmn 4 hours agoparentprev>Like, I get that shared memory architectures like a 192GB Mac Ultra are a big deal, but who’s dropping $6,000+ on that setup? AMD strix halo APU will have quad channel memory and will launch soon so expect these kinds of setups available for much less. Apple is charging an arm and a leg for memory upgrades, hopefully we get competition soon. From what I saw at CES OEMs are paying attention to this use case as well hopefully not following suite on RAM markups. reply sliken 1 hour agorootparentKeep in mind the strix halo APU has a 256 bit wide memory bus and the Mac Ultra has a 1024 bit wide memory bus. Here's hoping the Nvidia Digit (GB10 chip) has a 512 bit or 1024 bit wide interface, otherwise the Strix Halo will be the best you can do if you don't get the Mac Ultra. reply dagmx 4 hours agorootparentprevUnfortunately, Apple’s RAM and Storage upgrade prices are very in line with other class comparable OEMs. I’m sure there’ll be some amount of undercutting but I don’t think it’ll be a huge difference on the RAM side itself. reply rafaelmn 3 hours agorootparentHighly depends on how accessible AMD makes these boards, if a lot of OEMs get it there will be good deals for sure. DDR5 prices are nowhere near Apple markups. reply smcleod 6 hours agoparentprevI just ran it up on 48gb (2x 3090) + overflow into CPU RAM and it runs at around 4tk/s (only a little 8k context size though) which while absolutely not something I'd personally use daily it is actually usable. reply eurekin 4 hours agorootparentI have similar set-up can you help out with running it? Was it in ollama? EDIT: It seems that original authors provided a nice write-up: https://unsloth.ai/blog/deepseekr1-dynamic#:~:text=%F0%9F%96... reply F7F7F7 3 hours agoparentprevPeople would only be 'throwing their hands up' because commercial LLMs have set unreasonable expectations for folks. Anyone who has a/the need for or understands the value of a local LLM would be OK with this kind of output. reply bnchrch 3 hours agorootparentEveryone has the need for on device LLM, if the response rate was fast! reply ddingus 2 hours agorootparentI fired the model up at 8:39, ready to chat just a bit before 8:40, during which I input the following prompt: Please define \"lark\" for me, as in \"I did it on a lark.\" Output began a bit before 8:41, and was complete before 8:43: -Begin Model Output \"On a lark\" means without much planning or reason, impulsively or unexpectedly. Here's how it works: \"I did it on a lark.\" This means the person acted without a specific plan or reason. \"A lark\" refers to the lark bird, a small songbird known for its playful nature. So, \"on a lark\" suggests something spontaneous, casual, and often with a touch of playful whimsy. Here are some other ways to understand it: Without much thought: The person was likely to do it quickly and without much consideration. Based on a whim: It's something they decided to do without a solid plan. Let me know if you'd like more examples -End Model Output To me, that is pretty damn spiffy! reply dbtc 12 minutes agorootparentI opened Dictionary.app and typed in lark and got the definition with etymology in under 30 seconds. > British English informal used to suggest that an activity is foolish or a waste of time: he's serious about this music lark. reply ddingus 2 hours agorootparentprevI have MLCCHAT on my old Note 9 phone. It is actually still a great phone, but has 5GB RAM. Running an on device model is the first and only use case the RAM actually matters. And it has a headphone jack, OK? I just hate Bluetooth earbuds. And yeah, it isna problem, but I digress. When I run a 2.5B model, I get respectable output. Takes a minute or two to process the context, then output begins at somewhere on the order of 4 to 10 tokens per sec. So, I just make a query and give it a few and I have my response. Here is how I see it: That little model, which is Gemma 2.2b sorry, knows a lot of stuff. It has knowledge I don't and it gives it to me in a reasonable, though predictable way. Answers are always of a certain teacher reminding student how it all goes way. I don't care. Better is nice, but if I were stuck somewhere with no network, being able to query that model is amazing! First aid, how to make fires, materials and uses. Fixing stuff, theories of operation, what things mean and more are in that thing ready for me to take advantage of. I consider what I have fast. And it will get one or two orders faster over the next few years too. I did it on a lark (ask the model what that means) and was surprised to see I gained a nice tool. reply goosejuice 3 hours agorootparentprevI use commercial LLMs every day. The best of them can still be infuriating at times to the point of being unproductive. So I'm not sure I agree here. reply sliken 1 hour agoparentprevI do want a 192GB Mac Ultra, I'm hoping the Nvidia Digit achieves similar at $3,000. Sadly no specifications or benchmarks, so tokens/sec is just a guess at this point. reply JKCalhoun 4 hours agoparentprevLayman here — but I am hopeful for 1.58 bit plus custom silicon to be the Holy Grail. I suppose I am setting high expectations on Apple to integrate said in their next \"A\" chip. Wishful thinking. reply ricardobeat 6 hours agoparentprevThe repetition issue happens on simple quantization, what they are releasing is an approach that fixes that. reply apples_oranges 8 hours agoprevRandom observation 1: I was running DeepSeek yesterday on my Linux with a RTX 4090 and I noticed that the models should fit into VRAM, which is 24GB. Or they are simply slow. So the Apple shared memory architecture has an advantage here. A 192GB Mx Ultra can load and process large models efficiently. Random observation 2: It's time to cancel the OpenAI subscription. reply yobid20 6 hours agoparentI canceled my OpenAI subscription last night, as did many many others. There were some threads in reddit with everyone chiming in they all just canceled too. imo OpenAI is done, and will go through massive cuts and probably acquired by the end of the year for a very tiny fraction of its current value. reply Voloskaya 5 hours agorootparentYou want to bet? The panic around deepseek is getting completely disconnected from reality. Don’t get me wrong what DS did is great, but anyone thinking this reshape the fundamental trend of scaling laws and make compute irrelevant is dead wrong. I’m sure OpenAI doesn’t really enjoy the PR right now, but guess what OpenAI/Google/Meta/Anthropic can do if you give them a recipe for 11x more efficient training ? They can scale it to their 100k GPUs clusters and still blow everything. This will be textbook Jevons paradox. Compute is still king and OpenAI has worked on their training platform longer than anyone. Of course as soon as the next best model is released, we can train on its output and catch up at a fraction of the cost, and thus the infinite bunny hopping will continue. But OpenAI is very much alive. reply lolinder 4 hours agorootparent> The panic around deepseek is getting completely disconnected from reality. This entire hype cycle has long been completely disconnected from reality. I've watched a lot of hype waves, and I've never seen one that oscillates so wildly. I think you're right that OpenAI isn't as hurt by DeepSeek as the mass panic would lead one to believe, but it's also true that DeepSeek exposes how blown out of proportion the initial hype waves were and how inflated the valuations are for this tech. Meta has been demonstrating for a while that models are a commodity, not a product you can build a business on. DeepSeek proves that conclusively. OpenAI isn't finished, but they need to continue down the path they've already started and give up the idea that \"getting to AGI\" is a business model that doesn't require them to think about product. reply regularfry 4 hours agorootparentIn a sense it doesn't, in that if DeepSeek can do this, making OpenAI-type capabilities available for Llama-type infrastructure costs, then if you apply OpenAI scale infrastructure again to a much more efficient training/evaluation system, everything multiplies back up. I think that's where they'll have to head: using their infrastructure moat (such as it is) to apply these efficiency learnings to allow much more capable models at the top end. Yes, they can't sleep-walk into it, but I don't think that was ever the game. reply throwup238 3 hours agorootparentprev> The panic around deepseek is getting completely disconnected from reality. Couldn’t agree more! Nobody here read the manual. The last paragraph of DeepSeek’s R1 paper: > Software Engineering Tasks: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. Just based on my evaluations so far, R1 is not even an improvement on V3 in terms of real world coding problems because it gets stuck in stupid reasoning loops like whether “write C++ code to …” means it can use a C library or has to find a C++ wrapper which doesn’t exist. reply miroljub 2 hours agorootparentprev> You want to bet? Why would anyone bet? They can just short the OpenAI / MS stocks, and see in a few months if they were right or not. reply IAmGraydon 1 hour agorootparentOpenAI isn't publicly traded and MSFT's stake is so minor compared to their other business that it will have a negligible impact on their stock price. reply icedchai 1 hour agorootparentprev1) OpenAI isn't public, so not possible. 2) MS is one of the most well diversified tech companies, so, if anything, this will be a positive. reply coliveira 4 hours agorootparentprevComputing is not king, DeepSeek just demonstrated otherwise. And yes, OpenAI will have to reinvent itself to copy DS, but this means they'll have to throw away a lot of their investment in existing tech. They might recover but it is not a minor hiccup as you suggest. reply nlh 3 hours agorootparentI just don't see how this is true. OpenAI has a massive cash & hardware pile they'll adapt and learn from what DeepSeek has done and be in a position to build and train 10x-50x-100x (or however) faster and better. They are getting a wake-up call for sure but I don't think much is going to be thrown away. reply wqaatwt 4 hours agorootparentprevIMHO o1 it’s still comparable to a lot better for accomplishing actual stuff than DeepSeek. At least for my use cases. Of course cost is incomparably higher since plus has a very low limit. Which of course is a huge deal. reply generalizations 5 hours agorootparentprevIn my experience with deepseek and o1, openai's big talk about (and investment into) hallucination avoidance might save their hides here. Deepseek may be smarter, and understand complex problems better, but it also seems to make mistakes more often. (It's as if it's comprehension is better, but it's worse at memorization/recall.) Need an LLM to one-shot some complex network scripting? as of last night, o1 is still where its at. reply flir 4 hours agorootparentMy experience gels with yours. Given the same code sample, DeepSeek has better, more creative suggestions about how to improve it, but it can't implement them without breaking the code. o1, generally, can implement DeepSeek's suggestions successfully. I think chaining them together might have quite interesting results. reply wordpad25 3 hours agorootparentIs there a tool that can automate chaining like that? reply throwup238 3 hours agorootparentAider has an architect mode where it asks one model to plan out the changes and another to actually write the code. reply HarHarVeryFunny 3 hours agorootparentprevThat's ok if all you want to know is which model should I use today, but a test like that is totally dependent on training data, and there is no reason to expect that either DeepSeek-V3 (the base model for R1) or the additional training data for R1 is that same as what OpenAI used for O1 and whatever base model it was built on. The benchmark comparisons are perhaps, for now, the best way to compare reasoning prowess of R1 vs O1, since it seems pretty certain they both trained for those cases. I think the real significance of R1 isn't the released model/weights itself, but more the paper detailing (sans training data) how to replicate it, and how effective \"distillation\" (i.e. generate synthetic reasoning data for SFT) can be to enhance reasoning even without using RL. reply conradfr 4 hours agorootparentprevWhy every time there is a new model all the other competitors are declared immediately dead? reply HarHarVeryFunny 3 hours agorootparentThe big deal here isn't that R1 makes any other models obsolete in terms of performance, but how cheap it is $2 vs $60 per million output tokens compared to O1 (which it matches in benchmark performance). O1 vs R1 performance on specific non-benchmark problems is also not that relevant until people have replicated R1 and/or tried fine-tuning it with additional data. What would be interesting to see is whether (given the different usage of RL) there is any difference in how well R1 vs O1 generalize to reasoning capability over domains they were not specifically trained for. I'd expect that neither do that well, but not knowing details of what they were trained on makes it hard to test. reply osigurdson 4 hours agorootparentprevBecause we like drama. reply kebaman 5 hours agorootparentprevDoesn't Microsoft own 49% of OpenAI? They'll end up with it all as a division of Microsoft. reply wqaatwt 4 hours agorootparentI think they “own” 49% of OpenAI’s net income until a certain very high amount. Not a share of the actual company. reply yieldcrv 1 hour agorootparentprevI disagree, I don't really need \"conversational chat responses\", I need multimodal ChatGPT is the king of the multimodal experience still. Anthropic is a distant second, only because it lets you upload images from the clipboard and responds to them, but it can't do anything else like generate images sometimes it will do a flowchat which is kind of cool, GPT won't do that but will it speak to you, have tones, listen to you? no. And in the open source side, this area has been stagnant for like 18 months. There is no cohesive multimodal experience yet. Just a couple vision models with chat capabilities and pretty pathetic GUIs to support them. You have to still do everything yourself there. There is a huge utility for me, and many others that dont know it yet, if we could just load a couple models at once that work together seamlessly in a single seamless GUI like how ChatGPT works. reply anakaine 8 hours agoparentprevI disagree with cancelling the OpenAI subscription. I've been getting some help from o1 for both python and php recently, and o1 was doing massively better for the python stuff (it ran, deepseeks didn't and wont with prompt refinement). reply mst 4 hours agorootparentIIRC thezvi's summary post on R1 mentioned that R1 is amazing for general reasoning and is very clearly a successful proof of concept/capability but a lot of effort seems to have been put into making o1 Good At Code as a practical matter, whereas R1 seems to have been more a research project which proved out the approaches and then was released without sanding the rough edges off because that wasn't the point. reply neom 4 hours agorootparentprevAlso for some philosophical stuff DeepSeek just won't do it. I'm working on an essay about spirituality and sometimes it just responds that it doesn't know how to work on those types of problems and we should do something fun like math or games, claud tends to reply with something more like \"I have to be honest with you, reincarnation is not real\" and ChatGPT doesn't seem to care about that kinda thing at all. reply wqaatwt 4 hours agorootparentJust don’t ask it about anything related to Tiananmen square or president Pooh.. I’d guess they didn’t quite a bit of fine tuning to censor some more sensitive topics which probably impacts the output quality for other non technical subjects. reply greenavocado 4 hours agorootparentWould fine-tuning by using a LoRA paper over the censorship to a large degree? reply gradus_ad 7 hours agorootparentprevWere you running a local model? reply nodja 8 hours agoparentprevWhile 192GB of ram is appealing, it's also quite expensive at $6000. For that price I rather buy a system with 5 used 3090s, which while being \"only\" 120GB of VRAM, you benefit from much faster tokens/s and prompt processing speed (the macs are notoriously slow at consuming large contexts). reply orf 8 hours agorootparentCan I use that on the train though? I can with a 128GB MacBook, without it sounding like a helicopter taking off as well. reply zem 7 hours agorootparentyou don't need to take ai training quite so literally (: reply diggan 5 hours agorootparentprev> Can I use that on the train though? I can with a 128GB MacBook, without it sounding like a helicopter taking off as well. What kind of timescale do you expect to be able to train a useful LLM with that? reply orf 3 hours agorootparentWell it’s about an hour to commute on the train so I guess that long :3 reply wqaatwt 4 hours agorootparentprevIf you have an internet connection then sure you can? reply jchw 7 hours agorootparentprevHonestly, if you have a residence of some kind and an Internet connection, you don't need to bring your beefy computer with you everywhere. It is cool to be able to have ridiculously powerful mobile computers, but I don't think I would ever be willing to take a $6,000 laptop anywhere it has a decent chance of being stolen. reply orf 7 hours agorootparentDo you live in a third world country? If so I might agree, but otherwise trains are perfectly safe. reply jchw 6 hours agorootparentI am very happy for you, but laptops get stolen in public in most countries. reply orf 6 hours agorootparentLaptops get stolen on a train? An enclosed, single-direction space that only occasionally allows you to exit between infrequent, long-distance stops? A thing that contains ticket inspectors and a literal guard? How many laptops have you personally seen be stolen on a train? reply TeMPOraL 6 hours agorootparentYou mean a tight, enclosed, single-direction space, crowded with people who are tired, and/or trying to relax, and/or thinking about the destination, and/or otherwise not particularly focused after hours of travel; a thing that contains ticket inspectors that show up every now and then to check tickets, and from which passengers embark and disembark at dozens point along the length of the thing, simultaneously, with no supervision or security checks. Depending on the train type and configuration, many actually seem like pickpocket paradise. reply afavour 6 hours agorootparentPickpocketing is a very different proposition. They relying on a lack of awareness, taking your wallet and being long gone before you’ve even noticed. If someone steals your laptop from in front of you without you even noticing I’d suggest that one is on you. FWIW I’ve used my laptop on the train plenty, I’ve never had anything stolen nor felt in any danger of it. reply TeMPOraL 5 hours agorootparentBut would you consider leaving it unattended on your seat and going for lunch to the restaurant car, or for an extended toilet break? reply afavour 2 hours agorootparent...why would I ever do that? You leave something worth several thousand dollars anywhere in public you're risking losing it. What are we even debating here? reply giantrobot 3 hours agorootparentprevYou might have seen some laptops have screens that fold down, I know MacBooks do. This \"clam shell\" effect protects the keyboard, trackpad, and even the screen from bumps and jostles. Many laptops when so closed can even fit in a backpack. So a little trick I figured out is to close my laptop lid and then slide it into a pocket of my backpack. I can then carry it with me when I get up and move around. So then I can take it with me to eat lunch or an extended toilet break. Maybe some day all laptops will have that feature. reply orf 6 hours agorootparentprevIs your laptop in your pocket? reply crazygringo 6 hours agorootparentprevYes, all the time. It's happened to two people I know, in France and in the US. People get up to use the bathroom or the cafe car, the laptop is left behind for ten minutes, one of the train stops is while they're away from their seat, and someone sees an opportunity, snags it, and gets off at the stop. This is an actual thing. And if it's worth a thousand bucks then it's very much worth getting off at an earlier stop then you'd planned, and continuing your journey on the next train. Ticket inspectors or guards are irrelevant. There isn't one in your car 99% of the time. I don't why you're trying to argue laptop theft on trains in first-world countries isn't a thing. It absolutely is. reply vladms 5 hours agorootparentDifferent regions of the world would see different degrees of responsibilities regarding theft. I would consider absurd to leave unattended in a public space something valuable, considering the effort required to avoid that (that is: taking it with you). So, yes, theft on trains for people that think they are 100% safe are a thing, but applying the same idea (to assume something is 100% safe and not be cautious) I wonder how do such people use the internet... reply Thorrez 4 hours agorootparentMy coworker was having coffee and using his work laptop at an outdoor coffeeshop in Mountain View, CA. Someone on a bike rode by and attempted grab his phone and bike off with it. The attempted thief didn't succeed in taking the phone, but did knock the laptop onto the ground, damaging it. reply vladms 2 hours agorootparentThe discussion was about leaving unattended valuable objects in public places. Sure, a theft can happen even if attended, or using violence, but I personally avoid increasing the chance of having something stolen by leaving it unattended. If I would make a statistics of primary cause of remaining without a laptop among people I know, the biggest danger is liquids in glasses (that ends up on the laptops) ... reply collingreen 2 hours agorootparentprevA random person able to dart in and then make a getaway is not what \"working on a train\" is like and that was the original comment's point. reply crazygringo 3 hours agorootparentprevYou're going to take your laptop with you into the toilet on the train...? I don't think I've ever seen a human being do that before on a train. Not to go to the toilet, nor to grab a coffee in another car. You can't be paranoid about everything. My friend in France had put his laptop back into his bag where it wasn't visible and assumed that was good enough, but someone must have seen him do it and just took the whole bag. You are applying a totally unreasonable standard, to suppose that the thefts were due to unreasonable carelessness. What, do you think someone should take their large luggage into the bathroom too, every time they need to pee? Talk about victim-blaming. reply vladms 2 hours agorootparentYes, if I go to the toilet I take my backpack/small bag with me, because usually I have valuable stuff in them and are easy to carry. This does not apply to a large bag (in which I don't put valuable stuff). The standard is mine and I follow it. The same way I find absurd not to do it, you find it unreasonable to do it. I find the expectation that things are not stolen (if unsupervised in public places) strange considering the huge amount of inequalities in wealth around even in civilized countries. I do not agree with the idea of stealing, thiefs should be punished, but expecting everybody \"to behave\" given the situation seems unrealistic to me. That does not mean that I think that things are stolen 100% of the time. I have a friend that forgot a laptop on a bus (Netherlands) and the driver found it at the end of the line and gave it to lost objects so my friend got it back. reply michaelt 5 hours agorootparentprevYou don't hear much about laptop thefts these days because phones are more valuable, more numerous, and much easier to steal. Obviously, nobody steals things while the train is in motion. They wait until the train is about to leave the station, snatch a phone or handbag and jump out just as the door is closing. The train leaves, the thief blends in with other passenger leaving the station, and by the time news of the theft has made it from the passengers to the driver to the station staff the thief is long gone. Of course people drive around $6,000+ cars all the time, so.... reply jchw 6 hours agorootparentprevOnly on Hacker News would I have someone arguing with me that laptop theft is not a concern. You know what, you win. It's your $6,000 laptop, not mine. reply criddell 4 hours agorootparentA $6000 laptop doesn’t look much different than a $1000 laptop. I don’t think it’s a bigger theft risk than any other laptop. Make sure the laptop is insured and that full disk encryption is enabled. If it’s a Mac, make sure you have it in Find My so you can wipe it remotely if that’s something you worry about. reply jchw 1 hour agorootparentHonestly, I didn't bother making a better case for why I wouldn't want a $6,000 laptop in large part because the nerve people have to argue that theft isn't a concern at all made me stubborn. Theft is one reason, but a laptop is also a hell of a lot easier to simply break or lose than a desktop that is permanently installed somewhere, and a desktop is more upgradable and repairable, with typically much more I/O. Today's baseline laptops are really good as it is. 32-64 GiB of RAM is plenty, and at least on PC laptops you can do it fairly cheaply. Apple has been a consistent year or two ahead in mobile CPU performance but it fell out of my consideration ever since I realized the M1 and 7040 were both very sufficient for any local computation I cared about. (I'm not going to say I'd specifically go for less efficiency or performance, but it has become significantly lower priority over other things like repairability.) Not really specifically hating on Apple, here. If I was going to get another Mac it'd be a Mac Mini or Mac Studio probably, ideally with a third-party SSD upgrade to both save on costs and get a slight bit of extra drive performance too. I've definitely considered it, even though I am very far from an Apple fan, just due to the superior value and efficiency they have in many categories. reply bookofjoe 5 hours agorootparentprevYes! This goes in my forthcoming blog post \"Only on Hacker News...\" Yesterday's entry: \"... kind of a mind flex that you noted you used Meta Stories glasses to take that photo.\" reply orf 6 hours agorootparentprevSo, zero times then. Ok! reply jchw 6 hours agorootparentFor what it's worth, I never once insinuated that a laptop would get stolen on a train, only that I wouldn't want to bring such a laptop into the public in the first place. (Presumably, the laptop doesn't come into and exit existence upon entering and exiting the train, so this remains somewhat of a concern even if trains are involved.) But yes, you're right. I've never personally seen a laptop get stolen. In fact, most people who have their laptop get stolen never see their laptop get stolen either. I have, however, had coworkers who've had their laptops stolen. Multiple times. reply rob_c 6 hours agorootparentprevPeople stabbed maybe, but that tends to be more sports related than laptop related. (Yes on a national line(!)) reply orf 6 hours agorootparentYeah, a long, enclosed space with no exits is more amenable to drunken violence than petty theft. reply bongodongobob 3 hours agorootparentprevFor real? Grab it before the door closes. reply jillyboel 4 hours agorootparentprevit must be amazing to have so much faith in people like you seem to have reply immibis 6 hours agorootparentprevYou can use a desktop computer on a train if it's one with power outlets. Might get some funny looks, but I've seen it happen (or at least pictures). :) reply rob_c 6 hours agorootparentOnly time I've seen that done was with assistive tech and I do sympathise that those setups are difficult enough with desktops reply phkahler 5 hours agorootparentprev>> While 192GB of ram is appealing, it's also quite expensive at $6000. That's because it's Apple. It time to start moving to AMD systems with shared memory. My Zen 3 APU system has 64GB these days and its a mini ITX board. reply Maakuth 5 hours agorootparentWhat is the performance in ML workloads like on AMD APUs compared to Apple Silicon? reply xbmcuser 5 hours agorootparentprevI think just getting nvidia Project Digits might be the best option. A lot of people when it was announced were underwhelmed. But I think now it could be just the thing for people making their own ai home servers. https://www.nvidia.com/en-us/project-digits/ reply sourcecodeplz 1 hour agorootparentYep, I think the same. With 128GB fast memory one could run this. reply yobid20 6 hours agorootparentprevThe power requirement for 5x5090s is 10x higher , so you'll spend far more than $6000 in electricity over time. reply miohtama 8 hours agorootparentprev5x 3090 is also much more power hungry? reply karamanolev 7 hours agorootparentFor personal usage, does it matter though? In most places residential electricity is cheap compared to everything else. In a DC context I feel it matters a lot more compared to the capex. reply thenickdude 7 hours agorootparent1x 3090 (350W power limit) already makes it feel like I'm running a fan heater under my desk, 5x would be nuts. reply heelix 6 hours agorootparentI think the last time any of my computers had a case was back when I realized the pair of 900gx2 cards I was running was turning my computer into an easy bake. reply TeMPOraL 6 hours agorootparentprevPlace and time your use right, and you'll save a bit on heating at winter and/or at nights. reply danielhanchen 8 hours agorootparentprevThe good thing is since MoEs are mainly memory bound, we just need (VRAM + RAM) to be in the range of 80GB or so in my tests for at least 5 tokens or so /s. It's better to get (VRAM + RAM) >= 140GB for at least 30 to 40 tokens/s, and if VRAM >= 140GB, then it can approach 140 tokens/s! Another trick is to accept more than 8 experts per pass it'll be slower, but might be more accurate. You could even try reducing the # of experts to say 6 or 7 for low FLOP machines! reply danielhanchen 8 hours agoparentprevOh yes 192GB machines should be able these quants (131GB for 1.58bit, 158GB for 1.73bit, 183GB for 2.22bit) well :) reply bradfox2 5 hours agorootparentGreat release Daniel. Applaud the consistency you have shown. Can you release slightly bigger quant versions? Would enjoy something that runs well on 8x32 v100 and 8x80 A100. reply immibis 6 hours agoparentprevThe real insult here is graphics card vendors refusing to make ones with more than 24GB for several years now. They do this so you'll have to buy several cards for your AI workstation. Hopefully Apple eating their lunch fixes this. reply regularfry 4 hours agorootparentThe 5090 is 32GB out of the box. Not that that's anywhere near the top of what you can do on an Apple, but at least it's movement. reply TeMPOraL 6 hours agorootparentprev> They do this so you'll have to buy several cards for your AI workstation. AFAIK you can't do that with newer consumer cards, which is why this became an annoyance. Even a RTX 4070 Ti with its 12 GB would be fine, if you could easily stack a bunch of them like you used to be able with older cards. reply Gracana 5 hours agorootparentIt's \"easy\" if you have a place to build an open frame rig with riser cables and whatnot. I can't do that, so I'm going the single slot waterblock route, which unfortunately rules out 3090s due to the memory on the back side of the PCB. It's very frustrating. reply diggan 5 hours agorootparentI think parents point is that NVLink no longer ships with consumer cards. Before you could buy two cards + a cable between them, and software can treat them as one card. Today you need software support for splitting between the cards, unless you go for \"professional\" cards or whatever they call them. reply Gracana 5 hours agorootparentMaybe that's what they meant, and it'd be cool if nvidia still offered that on consumer cards, but thankfully you don't need it for LLM inference. The traffic between cards is very small. reply diggan 4 hours agorootparentIsn't the issue that the software needs to explicitly add support for it now, compared to yester-yesterday when you could just treat them as one in software? reply numpad0 2 hours agorootparentprevThere was a rumor that 5090 or 5090D for China may or may not come with multi-GPU software locked. I think GP's referring to that. It's not clear if it is the case with retail cards. reply sliken 1 hour agorootparentprevOr buy 2 Nvidia digits for $6,000 to get 256GB vram. reply therealpygon 5 hours agorootparentprevI honestly don’t know why people aren’t more upset by this and still get on their knees for Nvidia. They made the decision specifically to cripple consumer card memory because they didn’t like data centers were using them instead of buying their overpriced enterprise cards that were less performant. They removed NVLink because people were getting better performance out of their two $400 cards than the $1,500 cards Nvidia was trying to peddle. They willfully screw consumers and people love them for it. reply dagaci 4 hours agorootparentBecause sensible people just use the cloud at this point, you can probably get several years of training for $6000 reply immibis 2 hours agorootparentIt buys you approximately two days (with reservation discount) of a single p5.48xlarge instance, which has 2TB of RAM, and 640GB of VRAM in 8x H100 cards. In fact that is the pricing example they use: https://aws.amazon.com/ec2/capacityblocks/pricing/ reply dagaci 4 minutes agorootparentMI300X (RunPod) 192gb ram Hourly Rate: $2.49/hr. Break-even Point: You can rent for 2,410 hours (~100 days of non-stop-continuous use) before reaching the cost of the $6000 Mac. Mac's top out at 192GB not 2TB ;) Consideration: If your AI training requires sporadic use (e.g., a few hours daily or weekly), renting is significantly cheaper. MI300X will also get you result many times faster too, so you could probably multiply that 100 days! ant6n 7 hours agoparentprevSo I'm thinking, inference seems mostly memory bound. With a fast CPU (for example 7950x with 16 cores), and 256GB of RAM (seems to be the max), shouldn't that give you plenty of ability to run the largest models (albeit a bit slowly). It seems that AMD Epyc CPUs support terabytes of ram, some are as cheap as 1000 EUR. why not just run the full R1 model on that seems that it would be much cheaper than multiple of those insane NVidia-Karten. reply throw-qqqqq 7 hours agorootparentThe bottleneck is mainly memory bandwidth. AMD EPYC hw is appealing for local inference because it has a higher memory bandwidth than desktop gear (because 8-12 memory channels vs 2 on almost everything else), but not as fast as the Apple architectures and nowhere near VRAM speeds. If you want to drastically exceed ~3-5 tokens/s on 70b-q4 models, you usually still need GPUs. reply magicalhippo 6 hours agorootparentThis was beautifully illustrated in the recent Phoronix 5090 LLM benchmark[1], which I noted here[2]. The tested GPUs had an almost perfect linear relationship between generated token/s and GB/s memory bandwidth, except the 5090 where it dipped slightly. I guess the 5090 either started ever so slightly to become compute limited as well, or hit some overhead limitation. [1]: https://www.phoronix.com/review/nvidia-rtx5090-llama-cpp [2]: https://news.ycombinator.com/item?id=42847284 reply jmb99 3 hours agorootparentprevIt’s more expensive, but Zen4 Threadripper Pro is probably the way to go on that front. 8 memory channels, with DIMMs available up to DDR5-7200 for 8x32GB (256GB), or DDR5-6800 for 8x48GB (384GB). It’ll set you back ~$3k for the RAM and ~$6k for a CPU with 8 CCDs (the 7985WX, at least), and then ~$1k for motherboard and however much you want to spend on NVME. Basically ~$10k for a 384GB DDR5 system with ~435GB/s actual bandwidth. Not quite as fast as the 192GB Apple machines, but twice as much memory and more compute for “only” a few thousand more. reply sourcecodeplz 1 hour agorootparentAt these prices, I would just get 2xDigits for $6k and have 256gb. reply timschmidt 6 hours agorootparentprevOn Zen5 you also get AVX512 which llamafile takes advantage of for drastically improved speeds during prompt processing, at least. And the 12 channel Epycs actually seem to have more memory bandwidth available than the Apple M series. Especially considering it's all available to the CPU as opposed to just some portion of it. reply Gracana 6 hours agorootparentMaybe EPYC can make better use of the available bandwidth, but for comparison I have a water cooled Xeon W5-3435X running at 4.7GHz all-core with 8 channels of DDR5-6400, and CPU inference is still dog slow. With a 70B Q8 model I get 1 tok/s, which is a lot less than I thought I would get with 410GB/s max RAM bandwidth. If I run on 5x A4000s I get 6.1 tok/s, which makes sense... 448GB/s / 70GB = 6.4 tok/s max. reply iamnotagenius 4 hours agorootparentvery strange as I get on old i5-12400+DDR4 2 tok/sec with 14B/q8 model. reply immibis 6 hours agorootparentprevFWIW Threadrippers go up to 1TB and Threadripper Pro up to 2TB. That's even in the lowest model of each series. (I know this because it happens to be the chip I have. Not saying you shouldn't go for Epyc if it works out better.) reply moffkalast 8 hours agoparentprevYes, shared memory is a pretty big leg up since it lets the GPU process the whole model even if the bandwidth is slower which still has some benefits. Apple's M chips, AMD's Strix Point/Halo chips, Intel's Arc iGPUs, Nvidia's Jetsons. The main issue with all of these though is the lack of raw compute to complement the ability to load insanely large models. reply mtrovo 7 hours agoprevWow, an 80% reduction in size for DeepSeek-R1 is just amazing! It's fantastic to see such large models becoming more accessible to those of us who don't have access to top-tier hardware. This kind of optimization opens up so many possibilities for experimenting at home. I'm impressed by the 140 tokens per second speed with the 1.58-bit quantization running on dual H100s. That kind of performance makes the model practical for small or mid sized shops to use it for local applications. This is a huge win for people working on agents that require low latency that only local models could support. reply paradite 4 hours agoparentBtw completely off topic, but your comment triggered the internal classification in my brain, and it looks like AI-generated. Not accusing you anything. Could be that you happen to write in a way similar to LLMs. Could be that we are influenced by LLM writing styles and are writing more and more like LLMs. Could be that the difference between LLM generated content and human-generated content is getting smaller and harder to tell. reply j_bum 3 hours agorootparent+1 my LLM spidy senses were tingling. It’s the exclamation point in the first paragraph, the concise and consistent sentence structure, and the lack of colloquial tone. OP, no worries if you’re real. I often read my own messages or writing and worry that people will think I’m an LLM too. reply jasonjmcghee 3 hours agorootparent\"This kind of optimization opens up so many possibilities\" was what triggered me. reply mtrovo 3 hours agorootparentprevhaha you got me. I'm real person using LLM to proofread the stuff I write. English is not my native language and I'm trying to improve my written vocabulary a little bit. Sorry if it reads a little bit too off. reply paradite 3 hours agorootparentHaha no worries. This is a perfectly valid use case of LLM. I'm happy that the comment sounds very professional and to the point. reply danielhanchen 7 hours agoparentprevI was pleasantly surprised by 140 tokens/s as well! I literally thought I did something wrong but it was real! reply raghavbali 9 hours agoprev> Unfortunately if you naively quantize all layers to 1.58bit, you will get infinite repetitions in seed 3407: “Colours with dark Colours with dark Colours with dark Colours with dark Colours with dark” or in seed 3408: “Set up the Pygame's Pygame display with a Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's Pygame's”. This is really interesting insight (although other works cover this as well). I am particularly amused by the process by which the authors of this blog post arrived at these particular seeds. Good work nonetheless! reply danielhanchen 8 hours agoparentHey! :) Coincidentally the seeds I always use are 3407, 3408 and 3409 :) 3407 because of https://arxiv.org/abs/2109.08203 I also tried not setting the seeds, but the results are still the same quantizing all layers seems to make the model forget and repeat everything I put all examples here: https://docs.unsloth.ai/basics/deepseek-r1-dynamic-1.58-bit#... reply iamnotagenius 4 hours agorootparentwould be great to have dynamic quants of V3-non-R1 version, as for some tasks it is good enough. Also would be very interesting to see degradation with dynamic quants on small/medium size MoEs, such as older Deepseek models, Mixtrals, IBM tiny Granite MoE. Would be fun if Granite 1b MoE will still be functioning at 1.58bit. reply littlestymaar 9 hours agoparentprevCan't this kind of repetition be dealt with at the ~~decoder~~ (edit: sampler) level, like for any models? (see DRY ~~decoder~~ sampler for instance: https://github.com/oobabooga/text-generation-webui/pull/5677) reply danielhanchen 8 hours agorootparentOh yes one could provide a repetition penalty for example the issue is it's not just repetition that's the issue. I find it rather forgets what it already saw, and so hence it repeats stuff it's probably best to backtrack, then delete the last few rows in the KV cache. Another option is to employ min_p = 0.05 to force the model not to generate low prob tokens it can help especially in the case when the 1.58bit model generates on average 1/8000 tokens or so an \"incorrect\" token (for eg `score := 0`) reply reichardt 8 hours agorootparentprevYou likely mean sampler, not decoder. And no, the stronger the quantization, the more the output token probabilities diverge from the non-quantized model. With a sampler you can't recover any meaningful accuracy. If you force the sampler to select tokens that won't repeat, you're just trading repetitive gibberish for non-repetitive gibberish. reply littlestymaar 7 hours agorootparent> You likely mean sampler, not decoder. Indeed, that's posting before being fully awake. > And no, the stronger the quantization, the more the output token probabilities diverge from the non-quantized model. With a sampler you can't recover any meaningful accuracy. OF course you can't recover any accuracy, but LLM are in fact prone to this kind of repetition no matter what, this is a known failure mode that's why samplers aimed at avoiding this have been designed over the past few years. > If you force the sampler to select tokens that won't repeat, you're just trading repetitive gibberish for non-repetitive gibberish. But it won't necessary be gibberish! even a highly quantized R1 has still much more embedded information than a 14 or even 32B model, so I don't see why it should output more gibberish than smaller models. reply ErikBjare 8 hours agorootparentprevYou can deal with this through various sampling methods, but it doesn't actually fix the fried model. reply brap 8 hours agoprevAs someone who is out of the loop, what’s the verdict on R1? Was anyone able to reproduce the results yet? Is the claim that it only took $5M to train generally accepted? It’s a very bold claim which is really shaking up the markets, so I can’t help but wonder if it was even verified at this point. reply huijzer 7 hours agoparent> Is the claim that it only took $5M to train generally accepted? Based on Nvidia being down 18% yesterday I would say the claim is generally accepted. reply willsmith72 7 hours agorootparentBecause the markets are rational, all-knowing, and have never been wrong? reply pgwhalen 2 hours agorootparentNo, because the market is an aggregate of opinions, so it’s entirely fair to say it’s “generally accepted.” That has nothing to do with whether something happens to be true or not. It may provide a financial opportunity for someone who disagrees with that aggregated opinion though. reply huijzer 6 hours agorootparentprevThat was not the question. reply mcv 4 hours agorootparentIt is if you're using market movements as evidence of anything factual. If markets aren't rational, you can't use them that way. reply whimsicalism 3 hours agorootparentdo you only take advice/learn from all-knowing people? reply mcv 2 hours agorootparentDo you know any? But here's my advice: drop the fallacious arguments and try something more honest. reply whimsicalism 2 hours agorootparentmy argument isn’t fallacious it is logical: we can learn/use evidence from something without presuming it is all knowing. you are putting words in others mouths that they did not say reply tarruda 7 hours agorootparentprevIt is still unconfirmed since no one outside of deepseek reproduced it. If confirmed, Nvidia could go down even more reply Majromax 2 hours agorootparentI'm not sure I see the bear argument for NVidia here. Huge AI models certainly drive NVidia sales, but huge AI models are also widely thought to be untrainable and nearly un-runnable save for large datacenters. To me, this is ripe for an application of the Jevons paradox. If architectural improvements make similar models cheaper, I would expect to see more of them trained and deployed, not fewer, ultimately increasing the market for GPU-like hardware. reply Wheaties466 5 hours agorootparentprevbased on information and background they thoroughly gave when releasing their research its pretty easy to put together that it did take them significantly less resources to train this model. only having specific parameters available at a time instead of activating everything all at once is pretty ingenious. that and they just happened to be undergoing a large scale \"cyber attack\" reply infecto 6 hours agorootparentprevWhile Deepseek was an instigator in the price movements I would not say its accepted. reply afavour 6 hours agorootparentprevI don’t see them as related. The market moves when there is money to be made. It’s only tangentially related to any kind of general sentiment. “I don’t believe this, but I know others will, so I’m selling” reply deskamess 7 hours agorootparentprev> Nvidia being down 18% The only part of DeepSeek-R1 I do not like. I hope it's over, but I am not holding my breath. reply coffeebeqn 7 hours agorootparentNvidia is now up only 1906% over 5 years. What a disgrace reply samvher 5 hours agorootparentIt crashed all the way back to June 2024 levels, eons of progress wiped out reply infecto 6 hours agoparentprevI think the jury is out. With folks trying to replicate the process we will see if the low budget is true or not. I am still on the fence, there was comments from Scale CEO that they have a huge number of H100s they used. On the market side I think regardless if this was true or not, this gave people the opportunity to sell what is perhaps overinflated valuations. reply dinosaurdynasty 3 hours agoparentprevThat's likely only the marginal cost of training this model, and doesn't include a lot of other costs, like the datacenters and GPUs themselves which they already had and also the staff. If they aren't lying because they have hardware they're not supposed to have, which is also a possibility. reply whimsicalism 3 hours agorootparentthese claims are getting more wrong every time i see them, weird game of telephone going around tech circles. the cost absolutely includes the cost of GPUs and data centers, they quoted a standard price for renting h800 which has all of this built in. but yes, as very explicitly noted in the paper, it does not include cost of test iterations reply Kye 7 hours agoparentprevHuggingface is working on reproducing it: https://github.com/huggingface/open-r1 reply whimsicalism 3 hours agoparentprevr1 probably cost way less to train, $5m is the alleged price tag for dsv3 reply TheTaytay 5 hours agoprevDanielhanchen, your work is continually impressive. Unsloth is great, and I’m repeatedly amazed at your ability to get up to speed on a new model within hours of its release, and often fix bugs in the default implementation. At this point, I think serious labs should give you a few hour head start just to iron out their kinks! reply tarruda 9 hours agoprevWould be great if the next generation of base models was designed to be inferred with 128GB of VRAM while 8bit quantized (which would fit in the consumer hardware class). For example, I imagine a strong MoE base with 16 billion active parameters and 6 or 7 experts would keep a good performance while being possible to run on 128GB RAM macbooks. reply danielhanchen 8 hours agoparentSo I remember Deepseek used float8 for training Character AI also used int8 for training it is indeed possible, but sometimes training can be unstable Deepseek to my knowledge is actually the first lab to use float8 at a large scale without causing loss spikes they used FP8 tensor cores, then every 4th matrix multiply, they accumulated to a FP32 accumulator it seems like the Hopper Tensor Cores accumulation mechanism might not be actual FP32 accumulation. I wrote more here: https://x.com/danielhanchen/status/1872719599029850391 reply Davidzheng 9 hours agoparentprevWould be great, but unfortunately i think intelligence at that compute scale will be limit by hardware not its model. Though at hardware limit I would expect it to be roughly human level especially if optimized for a particular domain. reply tarruda 9 hours agorootparentI remember that Llama 3 was trained on data curated by Llama 2 and it resulted in a model with a significant performance boost (even though it was trained by a previous generation model of the same size). Maybe using a strong reasoning model such as R1 the next generation, even more performance can be extracted from smaller models. reply danielbln 9 hours agorootparentThat's already happening, and is in fact even part of the R1 training pipeline. An intermediate small reasoning model churns out training data for RL a larger model, rinse and repeat. Deepseek also showed model distillation with synthetic reasoning data to work quite well. reply Davidzheng 1 hour agorootparentIs your first claim in the R1 paper? I didn't see it when I looked reply alchemist1e9 6 hours agorootparentprevIt’s a pretty neat paradigm and I see an abstract connection to how brains dream and produce their own synthetic training data while sleeping that supplements their real data used while awake. reply afro88 6 hours agoprevThe size reduction while keeping the model coherent is incredible. But I'm skeptical of how much effectiveness was retained. Flappy bird is well known and the kind of thing a non-reasoning model could het right. A better test would be something off the beaten path that R1 and o1 get right that other models don't. reply whimsicalism 3 hours agoparentyeah it is pretty unclear how lobotomized it is without benchmark. i’ve gotten full fp8 running on 8xh100, probably going to keep doing that reply xiphias2 1 hour agoprevHas it been tried on 128GB M4 MacBook Pro? I'm gonna try it, but I guess it will be too slow to be usable. I love the original DeepSeek model, but the distilled versions are too dumb usually. I'm excited to try my own queries on it. reply prisenco 1 hour agoparentI'm downloading it now and will report back. (I've been using the 32B and while it could always be better, I'm not unhappy with it) reply amusingimpala75 6 hours agoprev> DeepSeek-R1 has been making waves recently by rivaling OpenAI's O1 reasoning model while being fully open-source. Do we finally have a model with access to the training architecture and training data set, or are we still calling non-reproducible binary blobs without source form open-source? reply stackedinserter 5 hours agoparentIt sounds like if they owe you the training architecture and training data set. reply chris_pie 3 hours agorootparentIt absolutely doesn't. It sounds like further diluting the term \"open-source\" isn't great. reply danesparza 2 hours agoprevJust ask it about Taiwan (not kidding). I'm not sure I can trust a model that has such a focused political agenda. reply miohtama 8 hours agoprevFlappy Bird in Python is the new Turing test reply danielhanchen 8 hours agoparent:) It's my goto test :) I did amp it up by adding 10 conditions and made a scoring card I found the original R1 to sometimes forget \"import os\" or miss some lines as well, so I thought it was at least a good check! I also like to ask the models to create a simple basic Minecraft type game where you can break pieces and store them in your inventory, but disallow building stuff reply miohtama 7 hours agorootparentI feel any AI can fix those problems when they can finally act. The problem AIs cannot run or debug code, or even book a hotel for me. When that is solved and an AI can interact with the code like a human does, it can fix its problems like a human does. reply merman 6 hours agorootparentExactly! Why can’t LLMs run their own code? reply whimsicalism 3 hours agorootparentthey can, feel free to inference and give it an interpreter reply Applejinx 6 hours agorootparentprevRampancy. reply mclau156 3 hours agoparentprevhopefully we eventually push them to make more classic games like motherlode reply slewis 2 hours agoprevIt would be really useful to see these evaluated across some of the same evals that the original R1 and deepseek's distills were evaluated on. reply ThePhysicist 9 hours agoprevIn general, how do you run these big models on cloud hardware? Do you cut them up layer-wise and run slices of layers on individual A100/H100s? reply phire 8 hours agoparentMy understanding is with MoE (Mixture of Experts), you can and should shard it horizontally. The whole model is 600GB, but only 37GB is active during the evaluation of any single output token. So you can load a different active subset of the MoE into each 89GB GPU, sharding it across something like 32 different GPUs (or can you get away with less? Wouldn't be surprised if they can infer on 8x H800 gpus). Some parameters are common, others are independent. Queries can be dynamically routed between GPUs, potentially bouncing between GPUs as much as once per output token, depending on which experts they need to activate. Though, I suspect it's normal to stick on one MoE subset for several output tokens. This has a secondary benefit that as long as the routing distribution is random, queries should be roughly load balanced across all GPUs. reply yorwba 8 hours agorootparentEach MoE layer has its own router, and it activates 8 (out of 256) experts at a time. There's no reason to expect all of them to stay on the same GPU, so you're pretty much guaranteed to have to do all-to-all communication between the GPUs in your cluster after every layer for every token. reply phire 6 hours agorootparentInteresting. I had assumed the performance advantage for MoE came from minimising traffic between GPUs. But if it's per layer routing, then it's going to massively increase inter-gpu traffic compared to vertical slicing. I guess that means the performance advantage actually comes when batching thousands of queries? The MoE routing would mean that on each MoE layer, each GPU shard gets a batch of queries that will all hit roughly the same subset of experts (and read the same weights from memory). The batches then shuffle between each MoE layer to re-optimise. It's kind of like GPU raytracing where you get large performance gains by running coherency sorting on rays and batching similar rays together. reply yorwba 4 hours agorootparentThe performance advantage comes from doing 1/32 of the floating point operations compared to a dense layer with the same number of parameters. reply iamnotagenius 4 hours agorootparentThe performance comes mostly from a fraction of memory bandwidth needed, as LLM are mostly memory constrained. Compute matters too, but usually far less than memory. reply danielhanchen 8 hours agoparentprevThere are a few ways the most basic is per layer sharding DeepSeek uses 3 dense layers, so that can stay on GPU0 (with the embedding layer). There's 58 MoE layers (256 experts, 8 activated) and 1 shared expert per layer. GPU1 would house layers 3 to 9, and so on. Then by using pipeline parallelism, if a new request comes, we simply stick them in a queue GPUs 0, 1, 2, ..., 8. Request A is at GPU 2, Request B at GPU 1, Request C at GPU 0 and so on. The other option is tensor parallelism were we split the weights evenly. You could combine pipeline and tensor parallelism as well! reply amelius 9 hours agoparentprevYou could do that, and add pipelining to improve speed. reply teekert 9 hours agoparentprevWas wondering the same, but for HPC clusters :) reply cubefox 6 hours agoprevFor anyone wondering why \"1.58\" bits: 2^1.58496... = 3. The weights have one of the three states {-1, 0, 1}. reply dist-epoch 6 hours agoparentThey say something else: > We managed to selectively quantize certain layers to higher bits (like 4bit), and leave most MoE layers (like those used in GPT-4) to 1.5bit reply cubefox 5 hours agorootparentThat was just improper rounding from 1.58 to 1.5. They say 1.58 in other places and explicitly link to https://arxiv.org/abs/2402.17764 reply MyFirstSass 5 hours agoprevIs this akin to the quants already being done to various models when you download a GGUF at 4 bits for example, or is this variable layer compression something new that can also be make existing smaller models smaller so we can fit more into say 12 or 16 gb's of vram? reply beernet 4 hours agoprevBig fan of unsloth, they have huge potential, could definitely need some experienced GTM people though, IMO. The pricing page and messages sent there are really not good. reply Pxtl 35 minutes agoprevIs there any good quick summary of what's special about DeepSeek? I know it's OSS and incredibly efficient, but news laymen are saying it's trained purely on AI info instead of using a corpus of tagged data... which, I assume, means it's somehow extracting weights or metadata or something from other AIs. Is that it? reply indigodaddy 2 hours agoprevIs there any small DS or qwen model that could run on say an M4 Mac Mini Standard (16G) ? reply mclau156 3 hours agoprevIs the new LLM benchmark to create flappy bird in pygame? reply CHB0403085482 7 hours agoprevDeepSeek R1 in a nutshell youtube.com/watch?v=Nl7aCUsWykg reply techwiz137 5 hours agoprevHow can you have a bit and a half exactly? It doesn't make sense. reply dosinga 5 hours agoparentIt's not a bit and a half. It is 1.58 or really log(3) / log(2) since it allows for three values, 1, 0 an 1 reply hendersoon 6 hours agoprevThe size reduction is impressive but unless I missed it, they don't list any standard benchmarks for comparison so we have no way to tell how it compares to the full-size model. reply upghost 9 hours agoprevThanks for the run instructions, unsloth. Deepseek is so new it's been breaking most of my builds. reply marcodiego 7 hours agoparentThis is an important step. Especially for beginners or people who are not in the loop, being able to easily type some simple commands to download, install dependencies, compile and run everything needed for a LLM AI model gives a feeling sci-fi; it's almost like you can have a helping brain at home. One thing I've being thinking about doing is to combine one of those LLM models running in llama.cpp, feed it with the output of whisper.cpp and connect its output to some TTS model. I wonder how far from Wheels and Roadie from the Pole Position tv series. reply danielhanchen 8 hours agoparentprevGlad they were helpful! :) reply CodeCompost 7 hours agoprevCan I run this on ollama? reply benoitg 7 hours agoparentYes, the instructions are in the OP. reply homarp 9 hours agoprevsee also https://news.ycombinator.com/item?id=42846588 reply petesergeant 7 hours agoprevIt is going to be truly fucking revolutionary if open-source models are and continue to be able to challenge the state of the art. My big philosophical concern is that AI locks Capital into an absolutely supreme and insurmountable lead over Labour, and into the hands of oligarchs, and the possibility of a future where that's not case feels amazing. It pleases me greatly that this has Trump riled up too, because I think it means he's much less likely to allow existing US model-makers to build moats, as I think he's even as a man who I don't think believes in very much absolutely unwilling to let the Chinese get the drop on him over this. reply fullstackchris 5 hours agoparentI have no doubt open source will catch up (it already has, eh?) at the end of the day, it's just creative / new iterations on what is ultimately the transformer architecture... the amount of \"secret\" moat-like stuff that OpenAI was doing was bound to be figured out or exceeded eventually, like everything in tech... Not to make fun of OpenAI and the great work they've done but it's kinda like if I went out in the 90s and said I'm going to found a company to have the best REST APIs. You can always found a successful tech company, but you can't found a successful tech company on a technological architecture or pattern alone. reply sylware 7 hours agoprevsite is javascript walled 80%? On 2 H100 only? To get near chatgpt 4? Seriously? The 671B version?? reply whimsicalism 3 hours agoparentthey have not benchmarked the quantized model. reply fsflover 5 hours agoparentprev> site is javascript walled I use Qubes OS to protect myself from the JS. reply sylware 3 hours agorootparentThat site should work with a noscript/basic (x)html browser. reply bluesounddirect 5 hours agoprev [–] Hi small comment, please remember in china many things are sponsored by or subsidized by the government. \"We[china] can do it for less..\" , \"it's cheaper in china..\" only means the government gave us a pile of cash and help to get here . I 100% expect some downvotes from the ccp. reply kccqzy 3 hours agoparentAnd the United States subsidizes plenty of things too. For example the CHIPS act has $39 billion in subsidies for chip manufacturing on U.S. soil. There's nothing wrong with either country's subsidies. I personally don't believe in maximum free market. Government subsidy is more often than not a good thing and we need more of them both here and in China. reply tivert 4 hours agoparentprev> Hi small comment, please remember in china many things are sponsored by or subsidized by the government. \"We[china] can do it for less..\" , \"it's cheaper in china..\" only means the government gave us a pile of cash and help to get here . And that's a really important strategic advantage China has versus America, which has such an insane fixation on pure(ish) free markets and free trade that it gives away its advantages in strategic industry after strategic industry. Some people falsely infer from the experience with the Soviet Union that freer markets always win geopolitical competition, but that's false. reply cynicalpeace 3 hours agorootparent> And that's a really important strategic advantage China has versus America, which has such an insane fixation on pure(ish) free markets and free trade that it gives away its advantages in strategic industry after strategic industry. > Some people falsely infer from the experience with the Soviet Union that freer markets always win geopolitical competition, but that's false. The data we have is 500 years of free markets in the western world and the verdict is overwhelmingly: Yes, more freedom means more winning. Just invite some incompetent bureaucrat over your house to dictate how you should cook and you'll quickly agree. reply syndicatedjelly 3 hours agorootparentprevIt’s false except for every time that it has been true reply lucb1e 1 hour agoparentprev [–] > I 100% expect some downvotes from the ccp. Always happy to oblige when someone insinuates that any critics must be government agents reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "DeepSeek R1 Dynamic 1.58-bit achieves an 80% size reduction and operates at 140 tokens per second using dual H100s, but its slow speed and repetition issues raise questions about its practicality.",
      "Dynamic quantization aids in performance, yet concerns about accessibility, cost, and the model's training cost claims persist, leading to scrutiny.",
      "The model has a notable impact on the market, with efforts underway to replicate its results, though its performance is debated compared to larger models."
    ],
    "points": 596,
    "commentCount": 239,
    "retryCount": 0,
    "time": 1738054367
  },
  {
    "id": 42852866,
    "title": "Promising results from DeepSeek R1 for code",
    "originLink": "https://simonwillison.net/2025/Jan/27/llamacpp-pr/",
    "originBody": "Simon Willison’s Weblog Subscribe ggml : x2 speed for WASM by optimizing SIMD (via) PR by Xuan-Son Nguyen for llama.cpp: This PR provides a big jump in speed for WASM by leveraging SIMD instructions for qX_K_q8_K and qX_0_q8_0 dot product functions. Surprisingly, 99% of the code in this PR is written by DeekSeek-R1. The only thing I do is to develop tests and write prompts (with some trails and errors) They shared their prompts here, which they ran directly through R1 on chat.deepseek.com it spent 3-5 minutes \"thinking\" about each prompt. I've been seeing some very promising results from DeepSeek R1 for code as well. Here's a recent transcript where I used it to rewrite the llm_groq.py plugin to imitate the cached model JSON pattern used by llm_mistral.py, resulting in this PR. I tried the same thing against o1, but I think DeepSeek R1 did it better. In particular, from the R1 chain of thought: Wait, but in the model_map, \"groq-gemma\" maps to \"gemma-7b-it\". So, perhaps the model_map is needed to map the local model IDs to the actual Groq model names. But since the model_map is hardcoded, and the API returns available models, perhaps the model_map should be built dynamically from the API response. Alternatively, perhaps the model_map can be eliminated, and the models are registered based on the fetched models. Wait, perhaps the model_map is no longer necessary. Instead, when the models are fetched from the API, each model's \"id\" is the actual model name used in the Groq API. So, when registering the models, the local model ID is \"groq-{id}\", and the groq_model_id is \"id\". (It thought about model_map a lot before finally deciding to eliminate it, which was also my preferred resolution.) Posted 27th January 2025 at 6:32 pm Recent articles A selfish personal argument for releasing code as Open Source 24th January 2025 Anthropic's new Citations API 24th January 2025 Six short video demos of LLM and Datasette projects 22nd January 2025 inference-scaling 23 deepseek 16 llama-cpp 20 ai 1061 llms 900 webassembly 73 ai-assisted-programming 114 generative-ai 909 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
    "commentLink": "https://news.ycombinator.com/item?id=42852866",
    "commentBody": "Promising results from DeepSeek R1 for code (simonwillison.net)480 points by k__ 4 hours agohidepastfavorite293 comments anotherpaulg 2 hours ago> 99% of the code in this PR [for llama.cpp] is written by DeekSeek-R1 It's definitely possible for AI to do a large fraction of your coding, and for it to contribute significantly to \"improving itself\". As an example, aider currently writes about 70% of the new code in each of its releases. I automatically track and share this stat as graph [0] with aider's release notes. Before Sonnet, most releases were less than 20% AI generated code. With Sonnet, that jumped to >50%. For the last few months, about 70% of the new code in each release is written by aider. The record is 82%. Folks often ask which models I use to code aider, so I automatically publish those stats too [1]. I've been shifting more and more of my coding from Sonnet to DeepSeek V3 in recent weeks. I've been experimenting with R1, but the recent API outages have made that difficult. [0] https://aider.chat/HISTORY.html [1] https://aider.chat/docs/faq.html#what-llms-do-you-use-to-bui... reply joshstrange 2 hours agoparentFirst off I want to thank you for Aider. I’ve had so much fun playing with it and using it for real work. It’s an amazing tool. How do you determine how much was written by you vs the LLM? I assume it consists of parsing the git log and getting LoC from that or similar? If the scripts are public could you point me at them? I’d love to run it on a recent project I did using aider. reply anotherpaulg 2 hours agorootparentGlad to hear you’re finding aider useful! There’s a faq entry about how these stats are computed [0]. Basically using git blame, since aider is tightly integrated with git. The faq links to the script that computes the stats. It’s not designed to be used on any repo, but you (or aider) could adapt it. You’re not the first to ask for these stats about your own repo, so I may generalize it at some point. [0] https://aider.chat/docs/faq.html#how-are-the-aider-wrote-xx-... reply joshstrange 34 minutes agorootparentThank you so much for linking me to that! I think an `aider stats`-type command would be really cool (it would be cool to calculate stats based activity since the first aider commit or all-time commits of the repo). reply Imanari 1 hour agoparentprevLove aider, thank you for your work! Out of curiousity, what are your future plans and ideas for aider in terms of features and workflow? reply reitzensteinm 44 minutes agoparentprevR1 is available on both together.ai and fireworks.ai, it should be a drop in replacement using the OpenAI API. reply realo 1 hour agoparentprevHello... Is it possible to use aider with a local model running in LMStudio (or ollama)? From a quick glance i did not see an obvious way to do that... Hopefully i am totally wrong! reply simonw 1 hour agorootparenthttps://aider.chat/docs/llms/ollama.html reply anotherpaulg 1 hour agorootparentprevThanks for your interest in aider. Yes, absolutely you can work with local models. Here are the docs for working with lmstudio and ollama: https://aider.chat/docs/llms/lm-studio.html https://aider.chat/docs/llms/ollama.html reply leetharris 1 hour agorootparentprevYes absolutely In the left bar there's a \"connecting to LLMs\" section Check out ollama as an example reply m3kw9 1 hour agorootparentprevYes and is easy reply almostgotcaught 33 minutes agoparentprev> 99% of the code in this PR [for llama.cpp] is written by DeekSeek-R1 you're assuming the PR will land: > Small thing to note here, for this q6_K_q8_K, it is very difficult to get the correct result. To make it works, I asked deepseek to invent a new approach without giving it prior examples. That's why the structure of this function is different from the rest. This certainly wouldn't fly in my org (even with test coverage/passes). reply Jimmc414 10 minutes agorootparent>> Small thing to note here, for this q6_K_q8_K, it is very difficult to get the correct result. To make it works, I asked deepseek to invent a new approach without giving it prior examples. That's why the structure of this function is different from the rest. > This certainly wouldn't fly in my org (even with test coverage/passes). To be fair, this seems expected. A distilled model might struggle more with aggressive quantization (like q6) since you're stacking two forms of quality loss: the distillation loss and the quantization loss. I think the answer would be to just use the higher cost full precision model. reply simonw 3 hours agoprevGiven these initial results, I'm now experimenting with running DeepSeek-R1-Distill-Qwen-32B for some coding tasks on my laptop via Ollama their version of that needs about 20GB of RAM on my M2. https://www.ollama.com/library/deepseek-r1:32b It's impressive! I'm finding myself running it against a few hundred lines of code mainly to read its chain of thought it's good for things like refactoring where it will think through everything that needs to be updated. Even if the code it writes has mistakes, the thinking helps spot bits of the code I may have otherwise forgotten to look at. reply lacedeconstruct 3 hours agoparentThe chain of thought is incredibly useful, I almost dont care about the answer now I just follow what I think is interesting from the way it broke the problem down, I tend to get tunnel vision when working for a long time on something so its a great way to revise my work and make sure I am not misunderstanding something reply miohtama 3 hours agoparentprevAlso even if the answer is incorrect, you can still cook the eggs on the laptop :) reply lawlessone 3 hours agorootparenti spent a months salary on these eggs and can no longer afford to cook them :( reply the_arun 3 hours agorootparentHey, where are you getting the eggs? I am unable to find them in the market. reply belter 2 hours agorootparentprevThe eggs cost more than the laptop... reply blakesterz 3 hours agoparentprevIs DeepSeek really that big of a deal that everyone else should worry? reply m11a 3 hours agorootparentA lot of the niceness about DeepSeek-R1's usage in coding is that you can see the thought process, which (IME) has been more useful than the final answer. It may well be that o1's chain of thought reasoning trace is also quite good. But they hide it as a trade secret and supposedly ban users for trying to access it, so it's hard to know. reply m11a 3 hours agorootparentOne example from today: I had a coding bug which I asked R1 about. The final answer wasn't correct, but adapting an idea from the CoT trace helped me fix the bug. o1's answer was also incorrect. Interestingly though, R1 struggled in part because it needed the value of some parameters I didn't provide, and instead it made an incorrect assumption about its value. This was apparent in the CoT trace, but the model didn't mention this in its final answer. If I wasn't able to see the trace, I'd not know what was lacking in my prompt, and how to make the model do better. I presume OpenAI kept their traces a secret to prevent their competitors from training models with it, but IMO they strategically err'd in doing so. If o1's traces were public, I think the hype around DS-R1 would be relatively less (and maybe more limited to the lower training costs and the MIT license, and not so much its performance and usefulness.) reply manmal 3 hours agorootparentDo you use Continue.dev or similar tools to load code into the context, or do you copypaste into their web chat? reply d3nj4l 2 hours agorootparentprevI have a lot of fun just posting a function into R1, saying \"Improve this\" and reading the chain of thought. Lots of insight in there that I would usually miss or glance over. reply fibers 3 hours agorootparentprevhow many reported cases of banning are there? that sounds insane for asking it to print out its chain of thought reply horsawlarway 2 hours agorootparentprevI would say worry? Yes. Panic? No. It's... good. Even the qwen/llama distills are good. I've been running the Llama-70b-distill and it's good enough that it mostly replaces my chatgpt plus plan (not pro plus). I think if anything One of my big takeaways is that OpenAI shot themselves in the foot, big time, by not exposing the COT for the O1 Pro models. I find thesection of the DeepSeek models to often be more helpful than the actual answer. For work that's treating the AI as collaborative rather than \"employee replacement\" the COT output is really valuable. It was a bad move for them to completely hide it from users, especially because they make the user sit there waiting while it generates anyways. reply pavitheran 3 hours agorootparentprevDeepseek is a big deal but we should be happy not worried that our tools are improving. reply bbzealot 2 hours agorootparentWhy though? I'm worried these technologies may take my job away and make the balance between capital and labor even more uneven. Why should I be happy? reply bubbleRefuge 1 minute agorootparentThink the marginal cost of developing complex software goes down thereby making it affordable to a greater market. There will still be a need for skilled software engineers to understand domains, limitations of AI, and how to harness and curate AI to develop custom apps. Maybe software engineering for the masses. Local small businesses can now maybe afford to take on custom software projects that were before unthinkable. joshmarlow 2 hours agorootparentprev> make the balance between capital and labor even more uneven. I think it's interesting to note that as opens source models evolve and proliferate, the capital required for a lot of ventures goes down which levels the playing field. When I can talk to one agent-with-a-CAD-integration and have it design a gadget for me and ship the design off to a 3D printer and then have another agent write the code to run on the gadget, I'll be able to build entire ventures that would require VC funding and a team now. When intellectual capital is democratized, financial capital looses just a bit of power... reply CamperBob2 2 hours agorootparentprevYou won't be happy doing a robot's job either, at least not for long. In the ideal case, we won't be dependent on the unwilling labor of other humans at all. Would you do your current job for free? If not if you'd rather do something else with your productive life then it seems irrational to defend the status quo. One thing's for certain: ancient Marxist tropes about labor and capital don't bring any value to the table. Abandon that thinking sooner rather than later; it won't help you navigate what's coming. reply cryptopian 1 hour agorootparentThat's not historically what's happened though, is it? We've had plenty of opportunities to reduce the human workload through increased efficiency. What usually happens is people demand more faster deliveries, more content churn; and those of us who are quite happy with what we have are either forced to adapt or get left behind while still working the same hours. reply hooverd 2 hours agorootparentprevBecause billionaires think that you are a horse and that the best course of action is to turn you into glue while they hope AGI lets them live forever. reply CamperBob2 1 hour agorootparentBillionaires don't think about you at all. That's what nobody seems to get. We enjoy many luxuries unavailable even to billionaires only a few decades ago. For this trend to continue, the same thing needs to happen in other sectors that happened in (for example) the agricultural sector over the course of the 20th century: replacement of human workers by mass automation and superior organization. reply flmontpetit 2 hours agorootparentprevAs far as realizing the prophecy of AI as told by its proponents and investors goes, probably not. LLMs still have not magically transcended their obvious limitations. However this has huge implications when it comes to the feasibility and spread of the technology, and further implications with regards to economy and geopolitics now that confidence in the American AI sector has been hit and people and organizations internationally have somewhere else to look for. edit: That being said, this is the first time I've seen a LLM do a better job than even a senior expert could do, and even if it's on small scope/in a limited context, it's becoming clear that developers are going to have to adopt this tech in order to stay competitive. reply buyucu 1 hour agorootparentprevThere are two things. First, deepseek v3 and r1 are both amazing models. Second, the fact that deepseek was able to pull this off with such modest resources is an indication that there is no moat, and you might wake up tomorrow and find an even better model from a company you have never heard of. reply simonw 3 hours agorootparentprevYeah, it is definitely a big deal. I expect it will be a net positive: they proved that you can both train and run inference against powerful models for way less compute than people had previously expected and they published enough details that other AI labs are already starting to replicate their results. I think this will mean cheaper, faster, and better models. This FAQ about it is very good: https://stratechery.com/2025/deepseek-faq/ reply netdevphoenix 3 hours agorootparentWhy did DeepSeek not kept this for themselves? Is this a Meta style scorched earth strategy? reply Rzor 2 hours agorootparent>An Yong: But DeepSeek is a business, not a nonprofit research lab. If you innovate and open-source your breakthroughs—like the MLA architecture innovation releasing in May—won’t competitors quickly copy them? Where’s your moat? >Liang Wenfeng: In disruptive tech, closed-source moats are fleeting. Even OpenAI’s closed-source model can’t prevent others from catching up. >Therefore, our real moat lies in our team’s growth—accumulating know-how, fostering an innovative culture. Open-sourcing and publishing papers don’t result in significant losses. For technologists, being followed is rewarding. Open-source is cultural, not just commercial. Giving back is an honor, and it attracts talent. https://thechinaacademy.org/interview-with-deepseek-founder-... reply simonw 3 hours agorootparentprevThere are a bunch of theories floating round. Personally this looks to me like an ego thing: the DeepSeek team are really, really good and their CEO is enjoying the enormous attention they are getting, plus the pride of proving that Chinese AI labs can take the lead in a field that everyone thought the USA was unassailable in. Maybe they are true believers in building and sharing \"AGI\" with the world? Lots of people see this as a Chinese government backed conspiracy to undermine the US AI industry. I'm not sure how credible that idea is. I saw somewhere (though I've not confirmed it with a second source) that none of the people listed on the DeepSeek papers got educated at US universities they all went to school in China, which further emphasizes how good China's home-grown talent pool has got. reply rfoo 2 hours agorootparent> none of the people listed on the DeepSeek papers got educated at US universities \"You have been educated at foreign universities / worked at foreign companies\" is indeed an excuse they have used at least once to refuse a candidate. n=1 though so maybe that's just a convenient excuse. There's one guy who went to University of Adelaide (IIRC) on the paper. reply dluan 2 hours agorootparentprev> Chinese government backed conspiracy Do you understand how ginormous China is and how ridiculous this kind of made up boogeyman statement sounds? reply simonw 2 hours agorootparentYes. reply eptcyka 2 hours agorootparentprevIt makes Trump look like a chump. reply AnotherGoodName 2 hours agorootparentprevIt’s a bunch of known optimisations bundled together rather than any single revolutionary change. More open than any other model (but still a bespoke licence) and bundles together a bunch of known improvements. There’s nothing to hide here honestly and without the openness it wouldn’t be as interesting. reply startupsfail 3 hours agorootparentprevThis may mean that $3k/task on some benchmarks published by OpenAI are now at slightly lower price tag. It is possible however that OpenAI was using similar level acceleration in the first place, they’ve just not published the details. And a few engineers left and replicated (or even bested it) in a new lab. Overall, it’s a good boost, modern software is getting a better fit into new generation of hardware and is performing faster. Maybe we should pay more attention when NVIDIA is publishing their N-times faster ToPS numbers, and not completely dismissing it as marketing. reply coliveira 3 hours agorootparentprevIt depends on the problem type. If your problem requires math reasoning, deepSeek response is quite impressive and surpasses what most people can do in a single session. reply GaggiX 3 hours agorootparentprevDeepSeek R1 is o1 but free to use, open source, and also distilled on different models, even the ones that could run on your phone so yeah. reply csomar 3 hours agorootparentprevEveryone else should rejoice. OpenAI is probably cooked, however. Nvidia might be cooked too. reply snowram 3 hours agorootparentIs Nvidia really cooked? If this new RF tech does scale, couldn't a bigger model be made that would require more compute power for training and inference? reply buyucu 1 hour agoparentprevDeepSeek-R1-Distill-Qwen-32B is my new default model on my home server. previously it was aya-32b. reply m3kw9 1 hour agoparentprevWhat does distil qwen 32b mean? It uses qwen for what? reply buyucu 1 hour agorootparentdeepseek fine-tuned qwen32b with data generated by deepseek671b reply CharlesW 3 hours agoprevFor those who aren't tempted to click through, the buried lede for this (and why I'm glad it's being linked to again today) is that \"99% of the code in this PR [for llama.cpp] is written by DeekSeek-R1\" as conducted by Xuan-Son Nguyen. That seems like a notable milestone. reply drysine 3 hours agoparent>99% of the code in this PR [for llama.cpp] is written by DeekSeek-R1 Yes, but: \"For the qX_K it's more complicated, I would say most of the time I need to re-prompt it 4 to 8 more times. The most difficult was q6_K, the code never works until I ask it to only optimize one specific part, while leaving the rest intact (so it does not mess up everything)\" [0] And also there: \"You must start your code with #elif defined(__wasm_simd128__) To think about it, you need to take into account both the refenrence code from ARM NEON and AVX implementation.\" [0] https://gist.github.com/ngxson/307140d24d80748bd683b396ba13b... reply aithrowawaycomm 2 minutes agoparentprevReading through the PR makes me glad I got off GitHub not for anything AI-related, but because it has become a social media platform, where what should be a focused and technical discussion gets derailed by strangers waging the same flame wars you can find anywhere else. reply jeswin 2 hours agoprev> 99% of the code in this PR [for llama.cpp] is written by DeekSeek-R1 I hope we can put to rest the argument that LLMs are only marginally useful in coding which are often among the top comments on many threads. I suppose these arguments arise from (a) having used only GH copilot which is the worst tool, or (b) not having spent enough time with the tool/llm, or (c) apprehension. I've given up responding to these. Our trade has changed forever, and there's no going back. When companies claim that AI will replace developers, it isn't entirely bluster. Jobs are going to be lost unless there's somehow a demand for more applications. reply simonw 2 hours agoparent\"Jobs are going to be lost unless there's somehow a demand for more applications.\" That's why I'm not worried. There is already SO MUCH more demand for code than we're able to keep up with. Show me a company that doesn't have a backlog a mile long where most of the internal conversations are about how to prioritize what to build next. I think LLM assistance makes programmers significantly more productive, which makes us MORE valuable because we can deliver more business value in the same amount of time. Companies that would never have considered building custom software because they'd need a team of 6 working for 12 months may now hire developers if they only need 2 working for 3 months to get something useful. reply jeswin 2 hours agorootparent> That's why I'm not worried. There is already SO MUCH more demand for code than we're able to keep up with. Show me a company that doesn't have a backlog a mile long where most of the internal conversations are about how to prioritize what to build next. I worry about junior developers. It will be a while before vocational programming courses retool to teach this new way of writing code, and these are going to be testing times for so many of them. If you ask me why this will take time, my argument is that effectively wielding an LLM for coding requires broad knowledge. For example, if you're writing web apps, you need to be able to spot say security issues. And various other best practices, depending on what you're making. It's a difficult problem to solve, requiring new sets of books, courses etc. reply JB74 4 minutes agorootparentAs a person who is trying to get into the industry. It is really disheartening. I don't really think I have a chance anymore. My best bet is if I can build something with LLM's for myself and try to market it. You are 100% spot on on the learning materials and courses. The majority of the material is so shallow and basic it doesn't do any good. reply onetimeusename 2 hours agorootparentprevJust as a side note, at my university about half the CS people are in the AI track. I would guess that number will keep increasing. There is also a separate major that kind of focuses on AI/psychology that is pretty popular but I am not sure how many people are in it. A good number of the students have some kind of \"AI startup\". Also, although it violates the honor code, I would be willing to bet many students use AI in some way for doing programming assignments. This isn't to say you are wrong but just to put some perspective on how things are changing. Maybe most new programmers will be hired into AI roles or data science. reply AnotherGoodName 2 hours agorootparentThe ask from every new grad to be assigned to ai development is unreasonable right now and they are probably hurting their careers by all going the same direction honestly. It’s a small fraction of our development efforts and we usually hire very senior for that sort of role. We still need people that can program for the day to day business needs and it’s a perfect starting role for a new grad yet almost all of them are asking for assignment to ai development. I appreciate anyone that can utilise ai well but there’s just not enough core ai model development jobs for every new grad. reply LeFantome 1 hour agorootparentAgree and disagree. You do it need a “degree in AI”. However, you need to be using AI in your degree. Really using it. What are those “day to day business needs” that you think people are going to do without AI? In my view, this is like 1981. If you are saying, we will still need non-computer people for day-to-day business needs, you are wrong. Even the guy in the warehouse and the receptionist at the front are using computers. So is the CEO. That does not mean that everybody can build one, but just think of the number of jobs in a modern company that require decent Excel skills. It is not just the one in finance. We probably don’t know what the “Excel” of AI is just yet but we are all going to need to be great at it, regardless of who is building the next generation of tools. reply bick_nyers 2 hours agorootparentprevAn actual hardcore technical AI \"psychology\" program would actually be really cool. Could be a good onboarding for prompt engineering (if it still exists in 5 years). reply tomrod 2 hours agorootparentprevYeah, the younguns smell opportunity and run towards it. They'll be fine. It's younguns) the less experienced folks in the current corporate world that will have the most to lose. reply CalChris 1 hour agorootparentOr perhaps it will be the more experienced knuckle draggers, hardened in our ways. reply curious_cat_163 34 minutes agorootparentprev> If you ask me why this will take time, my argument is that effectively wielding an LLM for coding requires broad knowledge. This is a problem that the Computer Science departments of the world have been solving. I think that the \"good\" departments already go for the \"broad knowledge\" of theory, systems with a balance between the trendy and timeless. reply LeFantome 1 hour agorootparentprevThink of how much easier it is to learn to code if you actually want to. The mantra has always been that the best way to learn to code is to read other people’s code. Now you can have “other people” write you code for whatever you want. You can study it and see how it works. You can explore different ways of accomplishing the same tasks. You can look at the similar implementations in different languages. And you may be able to see the reasoning and research for it all. You are never going to get that kind of access to senior devs. Most people would never work up the courage to ask. Plus, you are going to become wicked good at using the AI and automation including being deeply in touch with its strengths and weaknesses. Honestly, I am not sure how older, already working devs are going to keep up with those that enter the field 3 years from now. reply kadushka 55 minutes agorootparentPeople get wicked good by solving hard problems. Many young developers use AI to solve problems with little effort. Not sure what effect this will have on the quality of future developers. reply motorest 1 hour agorootparentprev> I worry about junior developers. It will be a while before vocational programming courses retool to teach this new way of writing code, and these are going to be testing times for so many of them. I don't agree. LLMs work as template engines on steroids. The role of a developer now includes more code reviewing than code typing. You need the exact same core curriculum to be able to parse code, regardless if you're the one writing it, it's a PR, or it's outputted by a chatbot. > For example, if you're writing web apps, you need to be able to spot say security issues. And various other best practices, depending on what you're making. You're either overthinking it or overselling it. LLMs generate code, but that's just the starting point. The bulk of developer's work is modifying your code to either fix an issue or implement a feature. You need a developer to guide the approach. reply danielbln 40 minutes agorootparentThat's a broad statement. If the IDE checks types and feeds errors back to the LLM, then that loop is very well able to fix an issue or implement a feature all on its own (see aider, cline etc ) reply bick_nyers 2 hours agorootparentprevI definitely agree with you in the interim regarding junior developers. However, I do think we will eventually have the AI coding equivalent of CICD built into perhaps our IDE. Basically, when an AI generated some code to implement something, you chain out more AI queries to test it, modify it, check it for security vulnerabilities etc. Now, the first response some folks may have is, how can you trust that the AI is good at security? Well, in this example, it only needs to be better than the junior developers at security to provide them with benefits/learning opportunities. We need to remember that the junior developers of today can also just as easily write insecure code. reply LeFantome 1 hour agorootparentIf it can point out the things you may need to consider, it is already better at security than most dev teams in the world today. Deep Seek can already do that. reply LeFantome 1 hour agorootparentprev“ It's a difficult problem to solve, requiring new sets of books, courses etc.” Instead of this, have you considered asking Deep Seek to explain it to you? reply smokel 1 hour agorootparentBefore this comment is being downvoted, please note the irony. The AI models may solve some technical problems, but the actual problems to be solved are of a societal nature, and won't be solved in our lifetimes. reply butlike 1 hour agorootparentprevCS Fundamentals are CS fundamentals, whether you're writing the B-tree or spot-checking it. reply Peacefulz 1 hour agorootparentprevI personally think that having hands on keyboards is still going to be imperative. Anyone can have an idea, but not everyone is going to be able to articulate that idea to an AI model in a way that will produce high quality, secure software. I'm by no means an expert, but I feel like you still need someone who understands underlying principles and best practices to create something of value. reply thelittleone 29 minutes agorootparentThis assumes that prompts do not evolve to the point where grandma can mutter some words to AI that produces an app that solves a problem. Prompts are an art form and a friction point to great results. Was only some months before reasoning models that CoT prompts where state of the art. Reasoning models take that friction away. Thinking it out even further, programming languages will likely go away altogether as ultimately they're just human interfaces to machine language. reply herval 1 hour agorootparentprevThis is my main worry with the entire AI trend too. We're creating a huge gap for those joining the industry right now, with markedly fewer job openings for junior people. Who will inherit the machine? reply belter 1 hour agorootparent>Who will inherit the machine? Extremely well paid human coders, capable of fixing the mistakes of the years preceding them... reply simonw 2 hours agorootparentprevYeah, it's going to suck for junior developers for a while. The ones who are self-starters will do fine they'll figure out how to accelerate their way up the learning curve using these new tools. People who prefer classroom-learning / guided education are going to be at a disadvantage for a few years while the education space retools for this new world. reply antonislav 1 hour agorootparentI think, seeing recordings of people using LLMs to accomplish non-trivial tasks would go a long way. I’d love to watch, e.g. you Simon, using these tools. I assume there are so many little tricks you figured out over time that together make a big difference. Things that come to mind: how to quickly validate the output? what tooling to use for iterating back and forth with the LLM? (just a chat?) how to steer the LLM towards a certain kind of solutions? what is the right context to provide to the LLM? How do it technically? reply plandis 1 hour agorootparentI believe Simon has full transcripts for some of the projects he’s had LLMs generate the code for. You can see how he steers the LLM for what is desired and how it is course corrected. reply simonw 1 hour agorootparentI've published probably over a hundred of those now, but they're scattered around. This tag on my blog has a lot of them: https://simonwillison.net/tags/ai-assisted-programming/ reply worik 31 minutes agorootparentprev> It will be a while before vocational programming courses retool to teach this new way of writing code Why? Are they not already? reply jhsvsmyself 2 hours agorootparentprevThere are already courses that are centered around coding with AI. reply belter 58 minutes agorootparent> Courses that are centered around coding with AI. Everybody is a Manager now? reply cyanydeez 1 hour agorootparentprevthat's basically the AI rubicon everywhere. From flying plans to programming: Soon there'll be no real fallback. When AI fails, you can't just put the controls in front of a person and expect them to have reasonable expertise to respond. Really, what seems on the horizon is a cliff of techno risks that have nothing to do with \"AI will take over the world\" and more \"AI will be so integral to functional humanity that actual risks become so diffuse that no one can stop it.\" So it's more a conceptual belief: Will AI actually make driving cares safer or will the fatalities of AI just be so randomly stochastic that it's more acceptable. reply entropicdrifter 1 hour agorootparent>So it's more a conceptual belief: Will AI actually make driving cares safer or will the fatalities of AI just be so randomly stochastic that it's more acceptable. I would argue that we already accept relatively random car fatalities at a huge scale and simply engage in post-hoc rationalization of the why and how of individual accidents that affect us personally. If we can drastically reduce the rate of accidents, the remaining accidents will be post-hoc rationalized the same way we always have rationalized accidents. reply cyanydeez 42 minutes agorootparentWell, we don't accept it in the sense of \"we can't blame someone\", which is what I'm saying. Soon it'll be like a forest fire or a conspiracy theory. Currently, car crashes are blamed on the individuals involved. reply entropicdrifter 6 minutes agorootparentSometimes, but sometimes people just say stuff like \"god is testing us\" when things appear to be truly random. I reckon we'll see a lot of new religious thinking about this stuff n144q 3 minutes agorootparentprevThat's the naiveity of software engineers. They can't see their limitations and think everything is just a technical problem. No, work is never the core problem. Backlog of bug fixes/enhancements is rarely what determines the headcount. What matters is the business need. If the product sells and there is no/little competition, the company has very little incentive to improve their products, especially hiring people to do the work. You'd be thankful if a company does not layoff people in teams working on mature products. In fact, the opposite has been happening, for quite a while. There are so many examples out there that I don't need to name them. reply rybosworld 27 minutes agorootparentprev> There is already SO MUCH more demand for code than we're able to keep up with. Show me a company that doesn't have a backlog a mile long where most of the internal conversations are about how to prioritize what to build next. This viewing things too narrowly I think. Why do we even need most of our current software tools aside from allowing people to execute a specific task? AI won't need VSCode. If AI can short circuit the need for most, if not nearly all enterprise software, then I wouldn't expect software demand to increase. Demand for intelligent systems will certainly increase. And I think many people are hopeful that you'll still need humans to manage them but I think that hope is misplaced. These things are already approaching human level intellect, if not exceeding it, in most domains. Viewed through that lens, human intervention will hamper these systems and make them less effective. The rise of chess engines are the perfect example of this. Allow a human to pair with stockfish and override stockfish's favored move at will. This combination will lose every single game to a stockfish-only opponent. reply bee_rider 6 minutes agorootparentThat’s a fine thing to believe. But the bit of data we got in this story is that a human wrote tests for a human-identified opportunity, then wrote some prompts, iterated on those prompts, and then produced a patch to be sent in for review by other humans. If you already believed that there might be some fully autonomous coding going on, this event doesn’t contradict your belief. But it doesn’t really support it either. This is another iteration on stuff that’s already been seen. This isn’t to cheapen the accomplishment. The range of stuff these tools can do is growing at an impressive rate. So far though it seems like they need technical people good enough to define problems for them and evaluate the output… reply logicchains 22 minutes agorootparentprev>AI won't need VSCode Why not? It's still going to be quicker for the AI to use automated refactoring tooling than to manually make all the changes itself. reply sitkack 2 hours agorootparentprevWe have already entered a new paradigm of software development, where small teams build software for themselves to solve their own problems rather than making software to sell to people. I think selling software will get harder in the future unless it comes with special affordances. reply LeFantome 1 hour agorootparentI think some of the CEOs have it right on this one. What is going to get harder is selling “applications” that are really just user friendly ways of getting data in and out of databases. Honestly, most enterprise software is just this. AI agents will do the same job. What will still matter is software that constrains what kind of data ends up in the database and ensures that data means what it is supposed to. That software will be created by local teams that know the business and the data. They will use AI to write the software and test it. Will those teams be “developers”? It is probably semantics or a matter of degree. Half the people writing advanced Excel spreadsheets today should probably be considered developers really. reply sitkack 37 minutes agorootparentMany applications can and should be replaced by a prompt and a database. This is the nature of increased expressive and computational power. So many whip manufacturers are about to go out of business, especially those offering whips-as-a-service. reply butlike 1 hour agorootparentprevMaybe, but it's the same argument trickling down. You'll need the CRUD-apps because you hired Cindy to press the button, and if shit goes pear-shaped, you can point to Cindy in the post-mortem. If it's some AI agent pressing the button to egress data from the database, and there's an anomaly, then it's a systemic failure at a macro level at that company, which is harder to write a press release about. reply ksec 1 hour agorootparentprev>There is already SO MUCH more demand for code than we're able to keep up with. Show me a company that doesn't have a backlog a mile long where most of the internal conversations are about how to prioritize what to build next. We really are in AI moment of iPhone. I never thought I would witness something bigger than the impact of Smartphone. There are insane amount of value that we could extract out. Likely in tens of trillions from big to small business. We keep asking how Low Code or No Code \"tools\" could achieve custom apps. Turns out we are here via a different route. >custom software because they'd need a team of 6 working for 12 months may now hire developers if they only need 2 working for 3 months to get something useful. I am wondering if it be more like 2 working for 1 month? reply fragmede 1 minute agorootparentless. how long would it take to build Twitter if you throw out all the difficult backend scaling problems? reply SecretDreams 2 hours agorootparentprevThe big fear shouldn't be on loss of jobs, it should be the inevitable attack on wages. Wage will track inversely to proximity as a commodity status. Even the discussion around AI partially replacing coders is a direction towards commoditization. reply Espressosaurus 2 hours agorootparentIt's the same thing. If there are more workers than jobs, wages go down. If there are more jobs than workers, wages go up. We saw it crystal clear between the boom years, the trough, and the current recovery. reply aibot923 1 hour agorootparentprevIt's interesting. Maybe I'm in the bigtech bubble, but to me it looks like there isn't enough work for everyone already. Good projects are few and far between. Most of our effort is keeping the lights on for the stuff built over the last 15-20 years. We're really out of big product ideas. reply Taylor_OD 1 hour agorootparentGood projects !== work There is a lot of work. Plenty of it just isnt super fun or interesting. reply agsqwe 1 hour agorootparentprevThis is very similar to my experience as a software development agency to enterprise customers. Out of big product ideas. reply nkassis 1 hour agorootparentYes a capacity increase from the developer side is great but it's supply side and we need to figure out how to accelerate transforming needs into demand. This is what I foresee developers turning into (at least some capable of this). Articulating logical solutions to be built to problems and evaluating results from what's generated to ensure it meets the needs. Aka Devs can move up the chain into what was traditionally product roles to increase development of new projects. This is using the time they have regain from more menial tasks being automated away. reply paulryanrogers 1 hour agorootparentprevDev effort isn't always the bottleneck. It's often stakeholders ironing out the ambiguities, conflicting requirements, QA, ops, troubleshooting, etc. Maybe devs will be replaced with QA, or become glorified QA themselves. reply Taylor_OD 1 hour agorootparentprevYup. People who know how to use it, and who work on tasks where LLM code is generally functional, are getting more done in less time. I don't trust companies to translate that to, \"We can do more now\" rather than, \"We can do more with less people now\" though. reply svilen_dobrev 24 minutes agorootparentprevmaybe shifting the jobs' target.. to higher a level (finally!) ? Reminds me of: https://chris-granger.com/2015/01/26/coding-is-not-the-new-l... modelling has been , is , and will be the needed literacy.. reply aomix 1 hour agorootparentprevI’m more bearish about LLMs but even in the extreme optimist case this is why I’m not that concerned. Every project I’m on is triaged as the one that needs the most help right now. A world when dozen projects don’t need to be left on the cutting room floor so one can live is a very exciting place. reply UncleOxidant 2 hours agorootparentprev> That's why I'm not worried. There is already SO MUCH more demand for code than we're able to keep up with. Show me a company that doesn't have a backlog a mile long where most of the internal conversations are about how to prioritize what to build next. And yet many companies aren't hiring developers right now folks in the C suite are thinking AI is going to be eliminating their need to hire engineers. Also \"demand\" doesn't necessarily mean that there's money available to develop this code. And remember that when code is created it needs to be maintained and there are costs for doing that as well. reply simonw 2 hours agorootparentI continue to suspect that the hiring problems are mainly due to massive over-hiring during Covid, followed by layoffs that flooded the market with skilled developers looking for work. I'd love to see numbers around the \"execs don't think they need engineers because of AI\" factor. I've heard a few anecdotal examples of that but it's hard to tell if it's a real trend or just something that catches headlines. reply withinboredom 1 hour agorootparentI think execs don’t see the problems we have with AI because you don’t need to be an expert to be an exec. I run into the edges of AI every day. There are things it is good at and things not so good at, and it varies from model to model and context to context (you can have two conversations with the same model, about the same thing, and get vastly different outputs; eg a test that uses different assertion patterns/libraries that are different from the rest of the project). As an “expert” or “highly skilled” person, I recognize these issues when I see them, but to a layman, it just looks like code. reply menaerus 2 hours agorootparentprevMassive overhiring or not, it's the fact that many (skilled) engineers can't find a job. Many companies were shut off during the past few years and market became oversaturated over the night. Whether AI will help to correct the market creating more demand we will see but I wouldn't hold my breath. Many domain specific skills became a commodity. reply sharperguy 1 hour agorootparentWe had a huge boom due to the low interest rates allowing businesses to pay developers with borrowed money, effectively operating at a loss for years on the basis of future growth. Now interest rates have risen the need to actually be profitable has caused a lot of optimization and lower hiring overall. reply nwienert 1 hour agorootparentprevWhere's the fact coming from, as in it's higher than before? I seem to be getting more than ever recruiting emails, and have felt out interviewing at a few places which we're very eager to find staff level talent. reply gamblor956 53 minutes agorootparentprevShow me a company that doesn't have a backlog a mile long where most of the internal conversations are about how to prioritize what to build next. Most companies don't have a milelong backlog of coding projects. That's a uniquely tech industry-specific issue, and a lot of it is driven by the tech industry's obsessive compulsion to perpetually reinvent wheels. Companies that would never have considered building custom software because they'd need a team of 6 working for 12 months may now hire developers if they only need 2 working for 3 months to get something useful. No, because most companies that can afford custom software want reliable software. Downtime is money. Getting unreliable custom software means that the next time around they'll just adapt their business processes to software that's already available on the market. reply Scipio_Afri 1 hour agorootparentprev100% agree with this take. People are spouting economic fallacies, and it’s in part cause CEOs don't want the stock prices to fall too fast. Eventually people will widely realize this and by then the economic payoffs are still immense. reply deadbabe 1 hour agorootparentprevToo much productivity can be a bad thing. If you’re infinitely productive, then the solution to every problem is to just keep producing stuff, instead of learning to say no. This means a lot of companies will overbuild, and then drown in maintenance problems and fail catastrophically when they can’t keep up. reply littlestymaar 1 hour agorootparentprevI couldn't agree more. And this kind of fear mongering is particularly irritating when you see that our industry already faced a similar productivity shock less than twenty years ago: before open source went mainstream github and library hubs like npm we used to code the same things over and over again, most of the time in a half-backed fashion because nobody had time for polishing stuff that was needed but only tangentially related to the code business. Then came the open-source tsunami, and suddenly there was a high quality library for solving your particular problem and the productivity gain was insane. Fast forward a few years, does it look like this productivity gains took any of our jobs? Quite the opposite actually, there has never been as many developers as today. (Don't get me wrong, this is massively changing how we work, like the previous revolution did, and how job is never going to be the same again) reply reitzensteinm 2 hours agoparentprevWhen GPT-4 came out, I worked on a project called Duopoly [1], which was a coding bot that aimed to develop itself as much as possible. The first commit was half a page of code that read itself in, asked the user what change they'd like to make, sent that to GPT-4, and overwrote itself with the result. The second commit was GPT-4 adding docstrings and type hints. Over 80% of the code was written by AI in this manner, and at some point, I pulled the plug on humans, and the last couple hundred commits were entirely written by AI. It was a huge pain to develop with how slow and expensive and flaky the GPT-4 API was at the time. There was a lot of dancing around the tiny 8k context window. After spending thousands in GPT-4 credits, I decided to mark it as proof of concept complete and move on developing other tech with LLMs. Today, with Sonnet and R1, I don't think it would be difficult or expensive to bootstrap the thing entirely with AI, never writing a line of code. Aider, a fantastic similar tool written by HN user anotherpaulg, wasn't writing large amounts of its own code in the GPT-4 days. But today it's above 80% in some releases [2]. Even if the models froze to what we have today, I don't think we've scratched the surface on what sophisticated tooling could get out of them. [1]: https://github.com/reitzensteinm/duopoly [2]: https://aider.chat/HISTORY.html reply matsemann 2 hours agoparentprevI read that Meta is tasking all engineers with figuring out how they got owned by deepseek. Couldn't they just have asked an llm instead? After their claim of replacing all of us... I'm not too worried. If anything we're the last generation that knows how to debug and work through issues. reply dumbfounder 1 hour agorootparentYep, and we still need COBOL programmers too. Your job as a technologist is to keep up with technology and use the best tools for the job to increase efficiency. If you don’t do this you will be left behind or you will be relegated to an esoteric job no one wants. reply hnthrow90348765 1 hour agorootparentprevA fair amount has been written on how to debug things, so it's not like the next generation can't learn it by also asking the AI (maybe learn it more slowly if 'learning with AI' is found to be slower) reply nkozyra 2 hours agorootparentprev> If anything we're the last generation that knows how to debug and work through issues. I suspect that comment might soon feel like saying \"not too worried about assembly line robots, we're the only ones who know how to screw on the lug nuts when they pop off\" reply lukan 1 hour agorootparentNot before AGI and I still see no signs of it. reply matsemann 1 hour agorootparentprevHeh, yeah. But the llm in this instance only wrote 99% after the author guided it and prompted over and over again and even guided it how to start certain lines. I can do that. But can a beginner ever get to that level when not having that underlying knowledge? reply Barrin92 52 minutes agorootparentprevI don't even see the irony in the comparison to be honest, being the assembly line robot controller and repairman is quite literally a better job than doing what the robot does by hand. If you're working in a modern manufacturing business the fact that you do your work with the aid of robots is hardly a sign of despair reply nkozyra 7 minutes agorootparentI don't claim it's a sign of despair. Rather, it's a boots-dug-in belief that one does is special and cannot be done autonomously. I think it's wholly natural. Work, time, education ... these operate like sunk costs in our brains. I think what we're all learning in real-time is that human technology is perpetually aimed at replacing itself and we may soon see the largest such example of human utility displacement. reply xd 35 minutes agoparentprevThe thing with programming, to do it well, you need to fully understand the problem and then you implement the solution expressing it in code. AI will be used to create code based on a deficit of clear understanding and we will end up with a hell of a lot of garbage code. I foresee the industry demand for programmers sky rocketing in the future, as companies scramble to unfuck the mountains of shit code they lash up over the coming years. It's just a new age of copy paste coders. reply kemiller 25 minutes agoparentprevMy observation in my years running a dev shop was that there are two classes of applications that could get built. One was the high-end, full-bore model requiring a team of engineers and hundreds of thousands of dollars to get to a basic MVP, which thus required an economic opportunity in at least the tends of millions. The other, very niche or geographically local businesses that can get their needs met with a self-service tool, max budget maybe $5k or so. Could stretch that to $25k if you use offshore team to customize. But 9/10 incoming leads had budgets between $25k and $100k. We just had to turn them away. There's nothing meaningful you can do with that range of budget. I haven't seen anything particularly change that. Self-service tools get gradually better, but not enough to make a huge difference. The high end if anything has receded even faster as dev salaries have soared. AI coding, for all its flaws now, is the first thing that takes a chunk out of this, and there is a HUGE backlog of good-but-not-great ideas that are now viable. That said, this particular story is bogus. He \"just wrote the tests\" but that's a spec — implementing from a quality executable spec is much more straightforward. Deepseek isn't doing the design, he is. Still a massive accelerant. reply spease 23 minutes agoparentprevThe nature of this PR looks like it’s very LLM-friendly it’s essentially translating existing code into SIMD. LLMs seem to do well at any kind of mapping / translating task, but they seem to have a harder time when you give them either a broader or less deterministic task, or when they don’t have the knowledge to complete the task and start hallucinating. It’s not a great metric to benchmark their ability to write typical code. reply karmasimida 38 minutes agoparentprevI am mixed on this point. I 100% agree with you our trade is changed forever. On the other hand, I am writing like 1000+ LOC daily, without much compromise on quality and my mental health, and thought of writing some code that is necessary but feels like a chore is not longer the case. The boost in output is incredible. reply woah 2 hours agoparentprevLLMs excel at tasks with very clear instructions and parameters. Porting from one language to another is something that is one step away from being done by a compiler. Another place that I've used them is for initial scaffolding of React components. reply headcanon 2 hours agoparentprevAgreed, though to your point I think we'll end up seeing more induced demand long-term This will enable more software to be built and maintained by same or fewer people (initially). Things that we wouldn't previously bother to do are now possible. More software means more problems (not just LLM-generated bugs which can be handled by test suites and canary deploys, but overall features and domains of what software does) This means skilled SWEs will still be in demand, but we need to figure out how to leverage them better. Many codebases will be managed almost entirely by agents, effectively turning it into the new \"build target\". This means we need to build more tooling to manage these agents and keep them aligned on the goal, which will be a related but new discipline. SWEs would need to evolve skillsets but wasn't that always the deal? reply submeta 37 minutes agoparentprevThe dev jobs won‘t go away, but they will change. Devs will be more and more like requirements engineers who need to understand the problem to then write prompts with the peoper context so that the llm can produce valuable and working code. And the next level will be to prompt llms to generate prompts for llms to produce code and solutions. But already I hire less and less developers for smaller tasks. The things that I‘d assign to a dev in Ukraine to explore an idea, do a data transformation, make a UI for the internal company tool. I can do these things quicker with llm than trying to find a dev and explain the task. reply WXLCKNO 16 minutes agorootparentI think what you're describing is going to be a very short transitional period. Once current AI gets good enough, the people micromanaging parts of it will do more to hinder the process than to help it. One person setting the objectives and the AI handling literally everything else including brainstorming issues etc, is going to be all that's needed. reply jasonthorsness 2 hours agoparentprevI think quality is going to go up I have so much code I wish I could go back and optimize for better performance, or add more comprehensive tests for, and LLMs are getting great at both of those as they work really well off of things that already exist. There has never been enough time/resources to apply towards even the current software demand, let alone future needs. reply nh2 1 hour agoparentprevChallenge: I would really like somebody that has experience in LLM based coding tools to try and fix gnome-terminal: https://news.ycombinator.com/item?id=42814509 reply smokel 1 minute agorootparentI really like this idea. However, it also highlights a key problem that LLMs don’t solve: while they’re great at generating code, that’s only a small part of real-world software development. Setting up a GitHub account, establishing credibility within a community, and handling PR feedback all require significant effort. In my view, lowering the barriers to open-source participation could have a bigger impact than these AI models alone. Some software already gathers telemetry and allows sharing bug reports, but why not allow the system to drop down to a debugger in an IDE? And why can’t code be shared as easily as in Google Docs, rather than relying on text-based files and Git? Even if someone has the skills to fix bugs, the learning curve for compilers, build tools, and Git often dilutes their motivation to contribute anything. reply Myrmornis 31 minutes agoparentprevIn my experience a lot of it is (d) defaulting to criticizing new things, especially things that are \"trendy\" or \"hot\" and (e) not liking to admit that one's own work can partially be done by such a trendy or hot thing. reply unshavedyak 31 minutes agoparentprevI’m still just looking for a good workflow where I can stay in my editor and largely focus on code, rather than trying to explain what I want to an LLM. I want to stay in Helix and find a workflow that “just works”. Not sure even what that looks like yet reply WXLCKNO 15 minutes agorootparentJust to clarify, something like Cursor doesn't fit your needs right? reply cjbgkagh 2 hours agoparentprevWho would the new applications be for? I figure that it’ll be far easier to build apps for use by LLMs than building apps for people to use. I don’t think there will be this large increase of induced demand, the whole world just got a lot more efficient and that’s probably a bad thing for the average person. reply 6510 1 hour agorootparentOh right, we will have a B2B B2C B2L L2B L2C and ultimately the L2L market. reply fragmede 1 hour agorootparentprevtake some process that you, or someone you know does right now that involves spreadsheets and copy-pasting between various apps. hiring a software engineer to build an app so it's just a [do-it] button previously didn't make sense because software engineer time was too expensive. Now, that app can be made, so the HR or whatever person doesn't need to waste their time on automatable tasks. reply cjbgkagh 1 hour agorootparentI'm pretty sure a LLM can be taught to press a button reply yobbo 1 hour agorootparentprevIf their time was actually wasted, paying a few or several thousand dollars for a tool would have been profitable a long time ago. Usually, there are hidden payoffs that motivate things that seem like waste. reply simonw 1 hour agorootparentThe people who are spending time manually doing a task that could be handled by a program are usually the exact same people who don't have the experience (or authority) to be able to say \"this is a thing that could be automated with a tool if we paid a few thousand dollars to develop it\". Hiring someone to remodel a bathroom is hard enough, now try hiring a contract software engineer, especially when you don't have budget authority! That said, I heard about a fire chief last year who had to spend two days manually copying and pasting from one CRM to another. I wish I could help people like that know when to pay someone to write a script! I imagine even in that role figuring out how to hire someone so solve a problem would still take longer than manually crunching through that themselves. reply 6510 1 hour agorootparentprevThe thing that has me most inspired is that one will finally get to ask the questions that seemed strange to ask before. Like, 1:40 times, when I press the button nothing happens for 10 seconds and I don't know if I've pressed the button properly. reply nkozyra 2 hours agoparentprev> it isn't entirely bluster \"Development\" is effectively translating abstractions of an intended operation to machine language. What I find kind of funny about the current state is we're using large language models to, like, spit out React or Python code. This use case is obviously an optimization to WASM, so a little closer to the metal, but at what point to programs (effectively suites of operations) just cut out the middleman entirely? reply dboreham 1 hour agorootparentI've wondered about this too. The LLM could just write machine code. But now a human can't easily review it. But perhaps TDD makes that ok. But now the tests need to be written in a human readable language so they can be checked. Or do they? And if the LLM is always right why does the code need to be tested? reply nkozyra 6 minutes agorootparentAt a certain point I don't see why a human needs to be in the loop at all. But I suppose that's the most dystopian part of it all. reply fauigerzigerk 1 hour agoparentprevA long time ago, I held the grandiose title of software architect. My job was to describe in a mix of diagrams, natural language and method signatures what developers were supposed to do. The back and forth was agonising. They were all competent software engineers but communicating with them was often far more work than just writing the damn code myself. So yes I do believe that our trade has changed forever. But the fact that some of our coworkers will be AIs doesn't mean that communicating with them is suddenly free. Communcation comes with costs (and I don't mean tokens). That won't change. If you know your stuff really well, i.e. you work on a familiar codebase using a familiar toolset, the shortest path from your intentions to finished code will often not include anyone else no humans and no AI either. In my opinion, \"LLMs are only marginally useful in coding\" is not true in general, but it could well be true for a specific person and a specific coding task. reply casenmgreen 1 hour agoparentprevI may be wrong, but I think right now, from reading stories of people looking at use AI and having poor experiences, AI is useful and effective for some tasks and not for others, and this is an intrinsic property it won't get better with bigger models. You need a task which fits well with what AI can do, which is basically auto-complete. If you have a task which does not fit well, it's not going to fly. reply simonw 1 hour agorootparentRight: LLMs have a \"jagged frontier\". They are really good at some things and terrible at other things, but figuring out WHAT those things are is extremely unintuitive. You have to spend a lot of time experimenting with them to develop good intuitions for where they make sense to apply. I expect the people who think LLMs are useless are people who haven't invested that time yet. This happens a lot, because the AI vendors themselves don't exactly advertise their systems as \"they're great at some stuff and terrible at other stuff and here's how to figure that out\". reply chrisguilbeau 2 hours agoparentprevI'm a developer that primarily uses gh copilot for python dev. I find it pretty useful as an intelligent auto-completer that understands our project's style, and unusual decorators we use. What tools would you tell a copilot dev to try? For example, I have a $20/mo ChatGPT account and asking it to write code or even fix things hasn't worked very well. What am I missing? reply byteknight 2 hours agorootparentWhile I dont know your scenario as an avid user of both gpt and claude, I would recommend move away from Google style search queries, and begin conversing. The more you give the LLM the more you'll get close to what you want. reply chefandy 55 minutes agoparentprevGH copilot code completion is really the only one I’ve found to be consistently more of a benefit than a time sync. Even with the spiffy code generators using Claude or whatever, I often find myself spending as much time figuring out where the logical problem is than if I had just coded it myself, and you still need to know exactly what needs to be done. I’d be interested in seeing how much time they spent debugging the generated code and and how long they spent constructing and reconstructing the prompts. I’m not a software developer anymore as my primary career, so if the entire lower-half of the software development market went away catering wages as it did, it wouldn’t directly affect my professional life. (And with the kind of conceited, gleeful techno-libertarian shit I’ve gotten from the software world at large over the past couple of years as a type of specialized commercial artist, it would be tough to turn that schadenfreude into empathy. But we honestly need to figure out a way to stick together or else we’re speeding towards a less mechanical version of Metropolis.) reply simlevesque 1 hour agoparentprevI got incredible results in asking AIs for sql queries. I just enter my data and what I want the output to look like. Then I ask it to provide 10 different versions that might be faster. I test them all and tell it which is faster and then I ask it to make variations on this path. Then I ask it to add comments to the code which is the fastest. I verify the query, do some more test, and I'm good to go. I understand SQL pretty well but trying to make 10 different versions of one code would've took me at least an hour. reply gmt2027 1 hour agoparentprevIf AI increases the productivity of a single engineer between 10-100x over the next decade, there will be a seismic shift in the industry and the tech giants will not walk away unscathed. There are coordination costs to organising large amounts of labour. Costs that scale non-linearly as massive inefficiencies are introduced. This ability to scale, provide capital and defer profitability is a moat for big tech and the silicon valley model. If a team of 10 engineers become as productive as a team of 100-1000 today, they will get serious leverage to build products and start companies in domains and niches that are not currently profitable because the middle managers, C-Suite, offices and lawyers are expensive coordination overhead. It is also easier to assemble a team of 10 exceptional and motivated partners than 1000 employees and managers. Another way to think about it is what happens when every engineer can marshal the AI equivalent of $10-100m dollars of labour? My optimistic take is that the profession will reach maturity when we become aware of the shift in the balance of power. There will be more solo engineers and we will see the emergence of software practices like the ones doctors, lawyers and accountants operate. reply AznHisoka 0 minutes agorootparentTo play devils advocate, the main obstacle in launching a product doesn't involve the actual development/coding. Unless you're building something in hard-tech, it's relatively easy to build the run of the mill software. The obstacles are in marketing, selling it, building a brand/reputation, integrating it with lots of 3rd party vendors, and supporting it. So yes, you can build your own Salesforce, or your own Adobe Photoshop with a one-man crew much faster and easier. But that doesn't mean you, as an engineer can now build your own business selling it to companies who don't know anything about you. reply WXLCKNO 12 minutes agorootparentprevLike darkwater's comment, this is my first time seeing this take and I like it a lot. I hate the idea of building a business to hundreds/thousands of employees, I love startups and small but highly profitable businesses. Having productivity be unleashed in this way with a small team of people I trust would be amazing. reply darkwater 1 hour agorootparentprevThis is a really interesting take that I don't see often in the wild. Actually, it's the first time I read someone saying this. But I think you are definitely onto something, especially if costs of AI are going to lower faster than expected even a few weeks ago. reply lukan 2 hours agoparentprev\"I hope we can put to rest the argument that LLMs are only marginally useful in coding\" I more often heard the argument, they are not useful for them. I agree. If a LLM would be trained on my codebase and the exact libaries and APIs I use I would use them daily I guess. But currently they still make too many misstake and mess up different APIs for example, so not useful to me, except for small experiments. But if I could train deepseek on my codebase for a reasonable amount(and they seemed to have improved on the training?), running it locally on my workstation: then I am likely in as well. reply Taylor_OD 1 hour agorootparentWe are getting closer and closer to that. For a while llm assistants were not all that useful on larger projects because they had limited context. That context has increased a lot over the last 6 months. Some tools will even analysis your entire codebase and use that in responses. It is frustrating that any smaller tool or api seem to stump llms currently but it seems like context is the main thing that is missing and that is increasing more and more. reply lukan 1 hour agorootparentI have not kept up, can you recommend something? reply simonw 1 hour agorootparentMy review of 2024 is a good place to catch up on what's changed in the past 12 months: https://simonwillison.net/2024/Dec/31/llms-in-2024/ reply rane 1 hour agorootparentprevThe idea is that you give the libraries and APIs as context with your prompt. reply r00fus 42 minutes agorootparentThere's a fairly low ceiling for max context tokens no matter the size of the model. Your hobby/small codebase may work, but for large codebases, you will need to do RAG and currently it's not perfect at absorbing the codebase and being able to answer questions on it. reply lukan 1 hour agorootparentprevThank you. But that doesn't work for me. If you mean just the name of the version in the prompt? No way. If you mean all the libary and my code in the contextwindow? Way too small. reply rane 55 minutes agorootparentNot _all_ the code. Just the relevant parts. reply rozap 2 hours agoparentprevBroadly agree. Whether or not it is useful isn't really an interesting discussion, because it so clearly is useful. The more interesting question is what it does to supply and demand. If the past is any indication, I think we've seen that lowering to barrier to getting software shipped and out the door (whether it's higher level languages, better tooling) has only made demand greater. Maybe this time it's different because it's such a leap vs an incremental gain? I don't know. The cynical part of me thinks that software always begets more software, and systems just become ever more complex. That would suggest that our jobs are safe. But again, I don't say that with confidence. reply simonw 2 hours agorootparent> If the past is any indication, I think we've seen that lowering to barrier to getting software shipped and out the door (whether it's higher level languages, better tooling) has only made demand greater. Something I think about a lot is the impact of open source on software development. 25 years ago any time you wanted to build anything you pretty much had to solve the same problems as everyone else. When I went to university it even had a name the software reusability crisis. At the time people thought the solution was OOP! Open source solved that. For any basic problem you want to solve there are now dozens of well tested free libraries. That should have eliminated so many programming jobs. It didn't: it made us more productive and meant we could deliver more value, and demand for programmers went up. reply oorza 2 hours agorootparentprevI don't think it's necessarily any larger of a leap than any of the other big breakthroughs in the space. Does writing safe C++ with an LLM matter more than choosing Rust? Does writing a jQuery-style gMail with an LLM matter more than choosing a declarative UI tool? Does adding an LLM to Java 6 matter more than letting the devs switch to Kotlin? Individual developer productivity will be expected to rise. Timelines will shorten. I don't think we've reached Peak Software where the limiting factor on software being written is demand for software, I think the bottlenecks are expense and time. AI tools can decrease both of those, which _should_ increase demand. You might be expected to spend a month outputting a project that would previously have taken four people that month, but I think we'll have more than enough demand increase to cover the difference. How many business models in the last twenty years that weren't viable would've been if the engineering department could have floated the company to series B with only a half dozen employees? What IS larger than before, IMO, is the talent gap we're creating at the top of the industry funnel. Fewer juniors are getting hired than ever before, so as seniors leave the industry due to standard attrition reasons, there are going to be fewer candidates to replace them. If you're currently a software engineer with 10+ YoE, I don't think there's much to worry about in fact, I'd be surprised if \"was a successful Software Engineer before the AI revolution\" doesn't become a key resume bullet point in the next several years. I also think that if you're in a position of leadership and have the creativity and leadership to make it work, juniors and mid-level engineers are going to be incredibly cost effective because most middle managers won't have those things. And companies will absolutely succeed or fail on that in the coming years. reply 20k 2 hours agoparentprevEh it performed a 1:1 conversion of ARM NEON to wasm SIMD, which with the greatest will in the world is pretty trivial work. Its something that ML is good at, because its the same problem area as \"translate this from english to french\", but more mechanistic This is a task that would likely have taken as long to write by hand as the AI took to do it, given how long the actual task took to execute. 98% of the work is find and replace Don't get me wrong this kind of thing is useful and cool, but you're mixing up the easy coding donkey work with the stuff that takes up time If you look at the actual prompt engineering part, its clear that this prompting produced extensively wrong results as well, which is tricky. Because it wasn't produced by a human, it requires extensive edge case testing and review, to make sure that the AI didn't screw anything up. If you have the knowledge to validate the output, it would have been quicker to write it by hand instead of reverse engineering the logic by hand. Its bumping the work off from writing it by hand, to the reviewers who now have to check your ML code because you didn't want to put in the work by hand So overall while its extremely cool that it was able to do this, it has strong downsides for practical projects as well reply WhitneyLand 2 hours agorootparentEvery time AI achieves something new/productive/interesting, cue the apologists who chime in to say “well yeah but that really just decomposes into this stuff so it doesn’t mean much”. I don’t get why people don’t understand that everything decomposes into other things. You can draw the line for when AI will truly blow your mind anywhere you want, the point is the dominoes keep falling relentlessly and there’s no end in sight. reply simonw 1 hour agorootparentThis is called the AI effect where the goalposts are moved every time an AI system demonstrates a new ability. It's been going on for decades. https://en.wikipedia.org/wiki/AI_effect reply ceejayoz 36 minutes agorootparentThat goes both ways, though. Every new ability is \"the big one\" that hints at AGI just around the corner. reply faizshah 1 hour agorootparentprevThe argument has never changed the argument has always been the same. LLMs do not think, they do not perform logic they are approximating thought. The reason why CoT works is because of the main feature of LLMs, they are extremely good at picking reasonable next tokens based on the context. LLM are good and always have been good at three types of tasks: Closed form problems where the answer is in the prompt (CoT, Prompt Engineering, RAG) Recall from the training set as the Parameter space increases (15B > 70B > almost 1T now) Generalization and Zero shot tasks as a result of the first two (this is also what causes hallucinations which is a feature not a bug, we want the LLM to imitate thought not be a Q&A expert system from 1990) If you keep being fooled by LLM thinking they are AGI after every impressive benchmark and everyone keeps telling you that in practice LLM are not good at tasks that are poorly defined, require niche knowledge, or require a special mental model that is on you. I use LLM every day I speed up many tasks that would take 5-15 mins down to 10-120 seconds (worst case for re-prompts). Many times my tasks take longer than if I had done it myself because it’s not my work im just copying it. But overall I am more productive because of LLM. Does LLM speeding up your work mean that LLM can replace Humans? Personally I still don’t think LLM can replace Humans at the same level of quality because they are imitating thought not actually thinking. Now the question among the corporate overlords is will you reduce operating costs by XX% per year (wages) but reducing the quality of service for customers. The last 50 years have shown us the answer… reply bigpingo 1 hour agorootparentprevI have always had the same line: AI will blow my mind when it solves an unsolved mathematical/physics/scientific problem, i.e: \"AI, give me a proof for (or against) the Riemann hypothesis\" reply simonw 1 hour agorootparentThat happened back in 2023: https://www.technologyreview.com/2023/12/14/1085318/google-d... reply lukan 1 hour agorootparentprev\"You can draw the line for when AI will truly blow your mind anywhere you want, the point is the dominoes keep falling relentlessly and there’s no end in sight\" I draw the line, when the LLM will be able to help me with a novel problem. It is impressive how much knowledge was encoded into them, but I see no line from here to AGI, which would be the end here. reply 20k 1 hour agorootparentprevThe thing is, that's not true at all. AI is great for some tasks, and poor for other tasks. That's the reason to break it down like this, because people are trying to explain where AI will and won't revolutionise things, instead of following along with the already-popping AI bubble uncritically For example: AI's smash translation. They won't ever beat out humans, but as an automated solution? They rock. Natural language processing in general is great. If you want to smush in a large amount of text, and smush out a large amount of other text that's 98% equivalent but in a different structure, that's what AI is good for. Same for audio, or picture manipulation. It works because it has tonnes of training data to match your input against What AI cannot do, and will never be able to do, is take in a small amount of text (ie a prompt), and generate a large novel output with 100% accuracy. It simply doesn't have the training data to do this. AI excels in tasks where it is given large amounts of context and asked to perform a mechanistic operation, because its a tool which is designed to extract context and perform conversions based on that context due to its large amounts of training data. This is why in this article the author was able to get this to work: they could paste in a bunch of examples of similar mechanical conversions, and ask the AI to repeat the same process. It has trained on these kinds of conversions, so it works reasonably well Its great at this, because its not a novel problem, and you're giving it its exact high quality use case: take a large amount of text in, and perform some kind of structural conversion on it Where AI fails is when being asked to invent whole cloth solutions to new problems. This is where its very bad. So for example, if you ask an AI tool to solve your business problem via code, its going to suck. Because unless your business problem is something where there are literally 1000s examples of how to solve it, the AI simply lacks the training data to do what you ask it, it'll make gibberish It isn't the nature of the power of the AI, its that its inherently good for solving certain kinds of problems, vs other kinds of problems. It can't be solved with more training. The OPs problem is a decent use case for it. Most coding problems aren't. That's not that it isn't useful people have already been successfully using them for tonnes of stuff but its important to point out that its only done so well because of the specific nature of the use case Its become clear that AI requires someone of equivalent skill as the original use case to manage its output if 100% accuracy is required, which means that it can only ever function as an assistant for coders. Again, that's not to say it isn't wildly cool, its just acknowledging what its actually useful for instead of 'waiting to have my mind blown' reply chunky1994 27 minutes agorootparentThe difference is though there isn't a whole lot of \"whole cloth novel solutions\" being written in software today so much as a \"write me this CRUD app to do ABC\" which current generations are exceedingly good at. There are probably 10% of truly novel problems out there, the rest are just already solved problems with slightly different constraints of resources ($), quality (read: reliability) and time. If LLMs get good enough at generating a field of solutions that minimize those three for any given problem, it will naturally tend to change the nature of most software being written today. reply svachalek 1 hour agorootparentprevYou need to substitute \"AI\" with \"LLMs\" or \"current transformer architecture\" or something. AI means something completely new every few years so speaking of what AI can't do or can never do doesn't make any sense. reply faizshah 1 hour agorootparentprevI just wrote up a very similar comment. It’s really nice to see that there are other people who understand the limits of LLM in this hype cycle. Like all the people surprised by Deepseek when it has been clear for the last 2 years there is no moat in foundation models and all the value is in 1) high quality data that becomes more valuable as the internet fills with AI junk 2) building the UX on top that will make specific tasks faster. reply Capricorn2481 1 hour agorootparentprev> cue the apologists How are you defining apologists here? Anti-AI apologists? Human apologists? That's not a word you can just sprinkle on opposing views to make them sound bad. reply UncleEntity 2 hours agorootparentprevIDK, I was playing with Claude yesterday/this morning and before I hit the free tier context limit it managed to create a speech-to-phoneme VQ-VAE contraption with a sliding window for longer audio clips and some sort of \"attention to capture relationships between neighboring windows\" that I don't quite understand. That last part was due to a suggestion it provided where I was like \"umm, ok...\" Seems pretty useful to me where I've read a bunch of papers on different variational autoencoder but never spent the time to learn the torch API or how to set up a project on the google. In fact, it was so useful I was looking into paying for a subscription as I have a bunch of half-finished projects that could use some love. reply Waterluvian 2 hours agoparentprevI want this to be true. Actually writing the code is the least creative, least interesting part of my job. But I think it’s still much too early for any form of “can we all just call it settled now? In this case, as we all know, lines of code is not a useful metric. How many person hours were spent doing anything associated with this PR’s generation and how does that compare to not using AI tools, and how does the result compare in terms of the various forms of quality? That’s the rubric I’d like to see us use in a more consistent manner. reply lenerdenator 2 hours agoparentprevMy greatest problem is duplicating the secret sauce of GHCP: it has access to your project and can use it as context. Admittedly, I haven't looked too hard, but how could I do that with a model from, say, Ollama and run exclusively on my machine? reply simonw 2 hours agorootparentThere are a bunch of tools that might be able to do that. I'd start by exploring https://aider.chat/ reply realo 1 hour agorootparentNeat. Can I use aider with a local model running in LMStudio (or ollama)? After a very quick reading of their pages it does not seem so. Hopefully I am wrong... reply simonw 1 hour agorootparentYes you can: https://aider.chat/docs/llms/ollama.html reply lenerdenator 2 hours agorootparentprevI'll take a look, thank you! reply nkozyra 2 hours agorootparentprevCouldn't you load the whole thing into a database or memory and use it as a RAG source? Not sure if that would fully scratch the itch. reply attractivechaos 1 hour agoparentprevI wonder what prompt they use. Before asking DeekSeek – is there a good post/video that walks through this procedure? reply TheBigSalad 2 hours agoparentprevLLMs are only marginally useful for coding. You have simply chosen to dismiss or or 'give up' on that fact. You've chosen what you want to believe in contrast to the reality that we are all experiencing. reply simonw 2 hours agorootparentLLMs are incredibly useful for coding, if you learn how to apply them effectively. You have simply chosen to dismiss or 'give up' on that fact. reply astrobe_ 51 minutes agorootparentI think the reality is that these AI output the \"average\" of what was in their training set, and people receive it differently depending on if they are below or above this average. It's a bit like what happens with \"illusion of knowledge\" or \"illusion of understanding\". When one knows the topic, one can correct the output of AI. When one doesn't, one tends to forget it can be inaccurate or plain wrong. reply TheBigSalad 2 hours agorootparentprevThey are a useful tool, but not 'incredibly useful'. The simple, repetitive code in this example is what they are good at. It's like 1% of what I do working on products. Writing code isn't even that impressive, the whole job is figuring out exactly what people want. reply fragmede 1 hour agorootparentgiven that there's no standardized scale of usefulness, is the distinction between \"useful\" for one person vs \"incredibly useful\", when nothing concrete has been specified; is that distinction really the important thing here? both of you find it useful. I might go off on a long tangent about how I love my hammer, it's the best, and you'll think I'm ridiculous because it's just a hammer, but at the end of the day, we can both agree that the hammer is doing the job of driving in nails. reply kikimora 47 minutes agoparentprevI don’t understand. When I asked DeepSeek how to find AWS IoT Thing creation time it suggested me to use “version” field and treat it as a Unix timestamp. This is obvious nonsense. How can this tool generate anything useful other than summaries of pre-existing text? My knowledge of theory behind LLMs also suggests this is all they can do reasonably well. When I see claims like this I suspect that either people around me somehow 10x better at promoting or they use different models. reply simonw 44 minutes agorootparentYou're making the mistake of treating an LLM like a search engine, and expecting it to be able to answer questions directly from its training data. Sometimes this works! But it's not guaranteed this isn't their core strength, especially once you get into really deep knowledge of complex APIs. They are MUCH more useful when you use them for transformation tasks: feed in examples of the APIs you need to work with, then have them write new code based on that. Working effectively with LLMs for writing code is an extremely deep topic. Most people who think they aren't useful for code have been mislead into believing that the LLMs will just work and that they don't first need to learn a whole bunch of unintuitive stuff in order to take advantage of the technology. reply tejinderss 7 minutes agorootparent> Working effectively with LLMs for writing code is an extremely deep topic. There is a space for learning materials here. I would love to see books/trainings/courses on how to use AI effectively. I am more and more interested in this instead of learning new programming language of the week. reply myrmi 1 hour agoparentprevI feel uncomfortably called out by all three points. What tools should I be trying to see what you are? reply jeswin 1 hour agorootparentI use my own tools and scripts, and those aren't for everyone so I'm just gonna make some general suggestions. 1. You should try Aider. Even if you don't end up using it, you'll learn a lot from it. 2. Conversations are useful and important. You need to figure out a way to include (efficiently, with a few clicks) the necessary files into the context, and then start a conversation. Refine the output as a part of the conversation by continuously making suggestions and corrections. 3. Conversational editing as a workflow is important. A better auto-complete is almost useless. 4. Github copilot has several issues interface is just one of them. Conversational style was bolted on to it later, and it shows. It's easier to chat on Claude/Librechat/etc and copy files back manually. Or use a tool like Aider. 5. While you can apply LLMs to solve a particular lower level detail, it's equally effective (perhaps more effective) to have a higher level conversation. Start your project by having a conversation around features. And then refine the structure/scaffold and drill-down to the details. 6. Gradually, you'll know how to better organize a project and how to use better prompts. If you are familiar with best practices/design patterns, they're immediately useful for two reasons. (1) LLMs are also familar with those, and will help with prompt clarity; (2) Modular code is easier to extend. 7. Keep an eye on better performing models. I haven't used GPT-4o is a while, Claude works much, much better. And sometimes you might want to reach for o1 models. Other lower-end models might not offer any time savings; so stick to top tier models you can afford. Deepseek models have brought down the API cost, so it's now affordable to even more people. 8. Finally, it takes time. Just as any other tool. reply Myrmornis 27 minutes agorootparentI agree with your overall point, and your despair at software engineers who are still refusing to acknowledge the value of these tools during the process of writing code. However > A better auto-complete is almost useless. That's not true. I agree that Copilot seemed unhelpful when I last tried it, but Cursor's autocomplete is extremely useful. reply plainOldText 1 hour agoparentprevIndeed, our trade has changed forever, and more specifically, we might have to alter our operational workflows in the entire industry as well. There are so many potential trajectories going forward for things to turn sour, I don't even know where to start the analysis. The level of sophistication an AI can achieve has no upper bound. I think we've had a good run so far. We've been able to produce software in the open with contributions from any human on the planet, trusting it was them who wrote the code, and with the expectation that they also understand it. But now things will change. Any developer, irrespective of skill and understanding of the problem and technical domains can generate sophisticated looking code. Unfortunately, we've reached a level of operational complexity in the software industry, that thanks to AI, could be exploited in a myriad ways going forward. So perhaps we're going to have to aggressively re-adjust our ways. reply herval 1 hour agorootparentI don't think trusting that someone wrote the code was ever a good assurance of anything, and I don't see how that changes with AI. There will always be certain _individuals_ who are more reliable than others, not because they handcraft code, but because they follow through with it (make sure it works, fix bugs after release, keep an eye to make sure it worked, etc). Yes, AI will enable exponentially more people to write code, but that's not a new phenomenon bootcamps enabled an order of magnitude more people to become developers. So did higher level languages, IDEs, frameworks, etc. The march of technology has always been about doing more while having to understand less higher and higher levels of abstraction. Isn't that a good thing? reply plainOldText 52 minutes agorootparentUntil now, the march of technology has taken place through a realm which was somewhat limited or slowed down only by our advancements in the physical and cognitive realities. This has given us ample time to catch up, to adjust. The cognitive reality of AI, and more specifically of AI+Humans in the context of a social and globally connected world, is on a higher level of sophistication and can unfold much faster, which in turn might generate entirely unexpected trajectories. reply herval 47 minutes agorootparentHas it really? What evidence do we have that it's such an insanely exponential advancement? reply mjr00 2 hours agoparentprev> I hope we can put to rest the argument that LLMs are only marginally useful in coding which are often among the top comments on many threads. I suppose these arguments arise from (a) having used only GH copilot which is the worst tool, or (b) not having spent enough time with the tool/llm, or (c) apprehension. I've given up responding to these. Look at the code that was changed[0]. It's a single file. From what I can tell, it's almost purely functional with clearly specified inputs and outputs. There's no need to implement half the code, realize the requirements weren't specified properly, and go back and have a conversation with the PM about it. Which is, you know, what developers actually do. This is the kind of stuff LLMs are great at, but it's not representative of a typical change request by Java Developer #1753 at Fortune 500 Enterprise Company #271. [0] https://github.com/ggerganov/llama.cpp/pull/11453/files reply simonw 2 hours agorootparent\"Yeah, but LLMs can't handle millions of lines of crufty old Java\" is a guaranteed reply any time this topic comes up. (That's not to say it isn't a valid argument.) Short answer: LLMs are amazingly useful on large codebases, but they are useful in different ways. They aren't going to bang out a new feature perfectly first time, but in the right hands they can dramatically accelerate all sorts of important activities, such as: Understanding code. If code has no documentation, dumping it into an LLM can help a lot. Writing individual functions, classes and modules. You have to be good at software architecture and good at prompting to use them in this way you take on the role of picking out the tasks that can be done independently of the rest of the code. Writing tests again, if you have the skill and experience to prompt them in the right way. reply mjr00 2 hours agorootparentYes, LLMs are very useful, when used properly. But the linked change request is not a good example of how they would be used by a typical software developer. The linked pull request is essentially output from a compiler that's been hardcoded. > Writing individual functions, classes and modules. You have to be good at software architecture and good at prompting to use them in this way you take on the role of picking out the tasks that can be done independently of the rest of the code. If you have enough skill and understanding to do this, it means you already have enough general software development experience and domain-specific experience and experience with a specific, existing codebase to be in rarefied air. It's like saying, oh yeah a wrench makes plumbing easy. You just need to turn the wrench, and 25 years of plumbing knowledge to know where to turn it. > Writing tests again, if you have the skill and experience to prompt them in the right way. This is very true and more accessible to most developers, though my big fear is it encourages people to crap out low-value unit tests. Not that they don't love to do that already. reply simonw 1 hour agorootparent> If you have enough skill and understanding to do this, it means you already have enough general software development experience and domain-specific experience and experience with a specific, existing codebase to be in rarefied air. Yes, exactly. That's why I keep saying that software developers shouldn't be afraid that they'll be out of a job because of LLMs. reply magicalist 1 hour agorootparentprev> \"Yeah, but LLMs can't handle millions of lines of crufty old Java\" is a guaranteed reply any time this topic comes up. That's not at all what the GP was saying, though: > There's no need to implement half the code, realize the requirements weren't specified properly, and go back and have a conversation with the PM about it. Which is, you know, what developers actually do. reply simonw 1 hour agorootparentI was responding to this bit: > This is the kind of stuff LLMs are great at, but it's not representative of a typical change request by Java Developer #1753 at Fortune 500 Enterprise Company #271. reply jvanderbot 2 hours agoparentprevThis is great. Really! Buuut... How do you get these tools to not fall over completely when relying on an existing non-public codebase that isn't visible in just the current file? Or, how do you get them to use a recent API that doesn't dominate their training data? Combining the both, I just cannot for the life of me get them to be useful beyond the most basic boilerplate. Arguably, SIMD intrinsics are a one-to-one translation boilerplate, and in the case of this PR, is a leetcode style, well-defined problem with a correct answer, and an extremely well-known api to use. This is not a dig on LLMs for coding. I'm an adopter I want them to take my work away. But this is maybe 5% of my use case for an LLM. The other 95% is \"Crawl this existing codebase and use my APIs that are not in this file to build a feature that does X\". This has never materialized for me what tool should I be using? reply simonw 2 hours agorootparent\"Or, how do you get them to use a recent API that doesn't dominate their training data?\" Paste in the documentation or some examples. I do this all the time \"teaching\" an LLM about an API it doesn't know yet is trivially easy if you take advantage of the longer context inputs to models these days. reply jvanderbot 1 hour agorootparentI've tried this. I've scraped example pages directly from github, and given them a 200 line file with the instructions \"just insert this type of thing\", and it will invariably use bad APIs. I'd be happy to share the example with you. reply simonw 48 minutes agorootparentGo for it can you share it in a Gist? I use this technique all the time. Here's one written-up example: https://simonwillison.net/2024/Mar/30/ocr-pdfs-images/ transcript here: https://gist.github.com/simonw/6a9f077bf8db616e44893a24ae1d3... reply jvanderbot 6 minutes agorootparentHere, give it a shot https://gist.github.com/jodavaho/8fb042fab33c1aaa95cd67144da... I'm at work so I can't try again right now, but last I did was use claude+context, chatGPT 4o with just chatting, Copilot in Neovim, and Aider w/ claude + uploading all the files as context. I even went so far as to grab relevant examples from https://github.com/bevyengine/bevy/tree/latest/examples#exam... , adding relevant ones as I saw fit. It took a long time to get anything that would compile, way",
    "originSummary": [
      "A pull request (PR) by Xuan-Son Nguyen for llama.cpp enhances WebAssembly (WASM) speed using Single Instruction, Multiple Data (SIMD) instructions, with significant contributions from DeekSeek-R1.",
      "The PR includes a dynamic model_map built from API responses, removing the necessity for hardcoded versions, showcasing innovation in plugin development.",
      "Simon Willison’s Weblog also covers recent topics such as open source projects, Anthropic's Citations API, and Large Language Model (LLM) projects, indicating a focus on cutting-edge technology discussions."
    ],
    "commentSummary": [
      "DeepSeek R1 demonstrates AI's potential in coding by writing 99% of a pull request (PR) for llama.cpp, showcasing AI's increasing role in software development.",
      "Tools like aider are now responsible for generating 70-82% of new code in releases, indicating a significant boost in productivity through AI assistance.",
      "Despite these advancements, AI still requires human oversight for complex problem-solving and integration with existing codebases, suggesting a shift in job dynamics and skill requirements in the industry."
    ],
    "points": 482,
    "commentCount": 295,
    "retryCount": 0,
    "time": 1738075446
  },
  {
    "id": 42845488,
    "title": "The Illustrated DeepSeek-R1",
    "originLink": "https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1",
    "originBody": "Share this post Language Models & Co. The Illustrated DeepSeek-R1 Copy link Facebook Email Notes More Discover more from Language Models & Co. Large language models, their internals, and applications. Over 14,000 subscribers By subscribing, I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy. Subscribe Continue reading Sign in The Illustrated DeepSeek-R1 A recipe for reasoning LLMs Jay Alammar Jan 27, 2025 245 Share this post Language Models & Co. The Illustrated DeepSeek-R1 Copy link Facebook Email Notes More 6 9 Share [Draft post, updates to come, please let me know if you have any suggestions or feedback here or on Bluesky or X/Twitter] DeepSeek-R1 is the latest resounding beat in the steady drumroll of AI progress. For the ML R&D community, it is a major release for reasons including: Thanks for reading Language Models & Co.! Subscribe for free to receive new posts and support my work. Subscribe It is an open weights model with smaller, distilled versions and It shares and reflects upon a training method to reproduce a reasoning model like OpenAI O1. In this post, we’ll see how it was built. Contents: Recap: How LLMs are trained DeepSeek-R1 Training Recipe 1 Long chains of reasoning SFT Data 2 An interim high-quality reasoning LLM (but worse at non-reasoning tasks). 3 Creating reasoning models with large-scale reinforcement learning (RL) 3.1 Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero) 3.2 Creating SFT reasoning data with the interim reasoning model 3.3 General RL training phase Architecture Most of the foundational knowledge you need to understand how such a model works is available in our book, Hands-On Large Language Models. Official website of the book. You can order the book on Amazon. All code is uploaded to GitHub. Recap: How LLMs are trained Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought. The following figure, from Chapter 12 of our book shows the general recipe of creating a high-quality LLM over three steps: 1) The language modeling step where we train the model to predict the next word using a massive amount of web data. This step results in a base model. 2) a supervised fine-tuning step that makes the model more useful in following instructions and answering questions. This step results in an instruction tuned model or a supervised fine tuning / SFT model. 3) and finally a preference tuning step which further polishes its behaviors and aligns to human preferences, resulting in the final preference-tuned LLM which you interact with on playgrounds and apps. DeepSeek-R1 Training Recipe DeepSeek-R1 follows this general recipe. The details of that first step come from a previous paper for the DeepSeek-V3 model. R1 uses the base model (not the final DeepSeek-v3 model) from that previous paper, and still goes through an SFT and preference tuning steps, but the details of how it does them are what's different. There are three special things to highlight in the R1 creation process. 1 Long chains of reasoning SFT Data This is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight 2 An interim high-quality reasoning LLM (but worse at non-reasoning tasks). This data is created by a precursor to R1, an unnamed sibling which specializes in reasoning. This sibling is inspired by a third model called R1-Zero (that we’ll discuss shortly). It is significant not because it’s a great LLM to use, but because creating it required so little labeled data alongside large-scale reinforcement learning resulting in a model that excels at solving reasoning problems. The outputs of this unnamed specialist reasoning model can then be used to train a more general model that can also do other, non-reasoning tasks, to the level users expect from an LLM. 3 Creating reasoning models with large-scale reinforcement learning (RL) This happens in two steps: 3.1 Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero) Here, RL is used to create the interim reasoning model. The model is then used to generate the SFT reasoning examples. But what makes creating this model possible is an earlier experiment creating an earlier model called DeepSeek-R1-Zero. R1-Zero is special because it is able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through a RL training process (no SFT step). It does this so well that it’s competitive with o1. This is significant because data has always been the fuel for ML model capability. How can this model depart from that history? This points to two things: 1 Modern base models have crossed a certain threshold of quality and capability (this base model was trained on 14.8 trillion high-quality tokens). 2 Reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. Let’s show this with an example. Example: Automatic Verification of a Reasoning Problem This can be a prompt/question that is a part of this RL training step: Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start. A question like this lends itself to many ways of automatic verification. Say we present this this to the model being trained, and it generates a completion: A software linter can check if the completion is proper python code or not We can execute the python code to see if it even runs Other modern coding LLMs can create unit tests to verify the desired behavior (without being reasoning experts themselves). We can go even one step further and measure execution time and make the training process prefer more performant solutions over other solutions — even if they’re correct python programs that solve the issue. We can present a question like this to the model in a training step, and generate multiple possible solutions. We can automatically check (with no human intervention) and see that the first completion is not even code. The second one is indeed python code but does not solve the problem. The third is a possible solution, but fails the unit tests, and the forth is a correct solution. These are all signals that can be directly used to improve the model. This is of course done over many examples (in mini-batches) and over successive training steps. These reward signals and model updates are how the model continues improving on tasks over the RL training process as seen in Figure 2 from the paper. Corresponding with the improvement of this capability is the length of the generated response, where the model generates more thinking tokens to process the problem. This process is useful, but the R1-Zero model, despite scoring high on these reasoning problems, confronts other issues that make it less usable than desired. Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. R1 is meant to be a more usable model. So instead of relying completely on the RL process, it is used in two places as we’ve mentioned earlier in this section: 1 creating an interim reasoning model to generate SFT data points 2 Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers) 3.2 Creating SFT reasoning data with the interim reasoning model To make the interim reasoning model more useful, it goes through an supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems (some of which are generated and filtered from R1-Zero). The paper refers to this as cold start data” 2.3.1. Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1 Zero outputs in a readable format, and refining the results through post-processing by human annotators. But wait, if we have this data, then why are we relying on the RL process? It’s because of the scale of the data. This dataset might be 5,000 examples (which is possible to source), but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows to synthetically generate that extremely valuable data. If you’re new to the concept of Supervised Fine-Tuning (SFT), that is the process that presents the model with training examples in the form of prompt and correct completion. This figure from chapter 12 shows a couple of SFT training examples: 3.3 General RL training phase This enables R1 to excel at reasoning as well as other non-reasoning tasks. The process is similar to the the RL process we’ve seen before. But since it extends to non-reasoning applications, it utilizes a helpfulnes and a safety reward model (not unlike the Llama models) for prompts that belong to these applications. Architecture Just like previous models from the dawn of GPT2 and GPT 3, DeepSeek-R1 is a stack of Transformer decoder blocks. It’s made up 61 of them. The first three are dense, but the rest are mixture-of-experts layers (See my co-author Maarten’s incredible intro guide here: A Visual Guide to Mixture of Experts (MoE)). In terms of model dimension size and other hyperparameters, they look like this: More details about the model architecture are presented in their two earlier papers: DeepSeek-V3 Technical Report DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models Conclusion With this, you should now have the main intuitions to wrap your head around the DeepSeek-R1 model. If you felt needed a little more foundational information to understand this post, I’d suggest you pick up a copy of Hands-On Large Language Models or read it online on O’Reilly and check it out on Github. Other suggested resources are: DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs by Nathan Lambert A Visual Guide to Mixture of Experts (MoE) by Maarten Grootendorst Sasha Rush’s YouTube video Speculations on Test-Time Scaling (o1) Yannis Kilcher’s DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained) Open R1 is the HuggingFace project to openly reproduce DeepSeek-R1 Putting RL back in RLHF I still sometimes think of the Galactica paper paper from 2022, which had a thinking Thanks for reading Language Models & Co.! Subscribe for free to receive new posts and support my work. Subscribe 245 Share this post Language Models & Co. The Illustrated DeepSeek-R1 Copy link Facebook Email Notes More 6 9 Share",
    "commentLink": "https://news.ycombinator.com/item?id=42845488",
    "commentBody": "The Illustrated DeepSeek-R1 (languagemodels.co)465 points by amrrs 22 hours agohidepastfavorite98 comments QuadrupleA 12 hours agoAm I the only one not that impressed with Deepseek R1? Its \"thinking\" seems full of the usual LLM blindsides, and ultimately generating more of it then summarizing doesn't seem to overcome any real limits. It's like making mortgage backed securities out of bad mortgages, you never really overcome the badness of the underlying loans, no matter how many layers you pile on top I haven't used or studied DeepSeek R1 (or o1) in exhaustive depth, but I guess I'm just not understanding the level of breathless hype right now. reply TrackerFF 10 hours agoparentWhat's there not to understand? If it matches the latest GPT O-N model in performance or is just close, even, at a fraction of the compute (50x less?) and it is free, then that's huge news. They just upended the current LLM/AI/ML dominance, or at least the perceived dominance. Billions and billions have been pumped into the race, where investors are betting on the winner and here comes a Chinese hedge fund side-project on shoestring budget, matching those billion dollar behemoths. And they'll continue to release their work. They just made the OpenAI et. al. secret sauce a lot less valuable. reply thechao 1 hour agorootparentNew theory: this is a short-long play by the fund. They shorted NV, now they're hoovering up stock. In the process of making their billions from a small 50$mm investment! reply zombiwoof 45 minutes agorootparentBullseye reply thechao 23 minutes agorootparentDamn Matt Levine to hell! His latest newsletter has an entire section devoted to the entire topic! reply infecto 6 hours agorootparentprevIn my tests it does not come close to O1-Pro. Still huge news but it does not quite make it. reply rallyforthesun 5 hours agorootparentThe results i did get from deepseek-r1 on their webpage did not match the results i did get from o1-pro. I did ask it go to a github repo, find the part where the logic of the “export” button is and explain why it doesn’t work (the whole logic is actually missing, won’t work at all). O1 pro did get it right in the first try while deepseek r1 was heavily hallucinating. Maybe i am using the wrong model? reply throwup238 4 hours agorootparentNo, you’re not. They explicitly mention in the R1 paper (in the last paragraph before the bibliography) that R1 isn’t a “huge” improvement over DeepSeek-V3 in coding where “huge” is an academic weasel word. It’s just a lot of hype. In my coding tests it significantly underperforms o1 (haven’t tried o1-pro), often getting stuck in a reasoning loop because I underspecified something (that I don’t have to with o1). reply infecto 3 hours agorootparentSame anecdotal experience. Its definitely an improvement and they have made operational improvements at runtime but I am still concerned they are have over fit for the tests. reply itissid 12 hours agoparentprevIt is leaps and bounds better than LLMs. For one you are doing RL which is classic AI like tuning that optimizes a reward function with nice qualities it's the same stuff used to train chess games and Go by showing them the actual moves. LLMs pre o1 and deepseek R1 were RHLF tuned which is like if you trained a LM how to play chess by showing people two boards and doing a vibe check on which \"looks\" better. Think of it this way say you were dropped in a maze that you had to solve but you could do only one of two things: 1. Look at two random moves from your start position and selected which one looked better to get out. 2. Made a series of moves and then backtracked, then use a quantitative function to exploit the best path. The latter is what R1 does and it chooses optimal and more certain path to success. Apply this to math and coding tokens and you have a competitive LLM. reply itissid 12 hours agorootparentI am using the 32Gb distilled model on my local 3090 with Continue in VSCode. It beats everything out of the water. reply dontwearitout 11 hours agorootparentHow many tokens/s do you get on a 3090? With the extra tokens for the internal monologue, is it still performant enough for smooth VSCode integration? reply MarcelOlsz 3 hours agorootparentprevAny idea how to use a cloud hosted version with cursor? reply HarHarVeryFunny 2 hours agoparentprevIt's the cost comparison with O1, both to train and run (per their pricing), that is causing most of the shock, perhaps as well as the fact that it's a GPU poor Chinese company that has caught up with O1, not a US one (Anthropic, Google, Meta, X.ai, Microsoft). The fact that it's open weights and training is fairly detailed in the paper they released is also significant. The best comparison for R1 is O1, but given different training data, hard to compare outside of benchmarks. At the moment these \"reasoning models\" are not necessarily the best thing to use for non-reasoning tasks, but Anthropic have recently indicated that they expect to release models that are more universal. reply dkjaudyeqooe 7 hours agoparentprevAs with most early tech of a particular category, it's not the current capabilities that are the point but the direction of travel. DeepSeek has upended the conventional wisdom about model performance with respect to training, and it's a shock to the system. It demonstrates something that has become obvious: you don't need massive scale or funding to innovate and have impact, you just need good ideas. reply lm28469 11 hours agoparentprevI've been using deepseek for a while, I never paid for chat gpt or any other services. The fact that r1 is now free and unlimited VS chat gpt 200$ a month subscription is impressive enough for me. If the development cost is anywhere close to what they advertise publicly it's even more impressive It's as good or better than chat gpt free, gemini free, &c. and that's all I care about reply redcobra762 9 hours agorootparentWhy is cost your only concern? You don’t care at all what data the model was trained on? The motivations of the people who trained it? I mean maybe the importance of those things isn’t super high to you, but “don’t care”? How odd. Most Westerners I know would care… reply troyvit 3 hours agorootparent> You don’t care at all what data the model was trained on? The motivations of the people who trained it? Working for a news org whose data was used without our consent to give ChatGPT a leg up, yeah I'd care a lot about it. The bottom line though is that nobody is innocent in that regard. Would a self-respecting organization go all in on Deepskeek R1 without a security audit and a ton of competitive testing against other models? I doubt it. Same way they shouldn't just give all their employees OnePlus or Huwei phones. I like how the Meta guy put it. This isn't China beating the US in AI, it's \"open source\" (however these guys define it) beating closed models. OpenAI broke its promise with the world, which anybody who compares their name with their product can tell you. If open models put a dent in their hegemony it's only good for the rest of the industry. reply redcobra762 2 hours agorootparentSo it genuinely does not matter to you how completely irrelevant US copyright law is to China, when using a Chinese LLM? That’s such a weirdly specific application of a moral principle, it’s hard to believe. reply lm28469 9 hours agorootparentprevAh yes, because chat gpt is ethical AI lmao I'm European, seeing the clowns in charge of US tech companies I'm equally happy using Chinese tools, especially if they're open source. Stop twisting the debate as if it was \"good US\" vs \"evil China\", I have no horses in this race reply redcobra762 9 hours agorootparentThat’s fine, it’s just not what Westerners generally say, in my experience. Not that distaste for Chinese authoritarianism is inherently pro Western, just that Westerners I know tend to have an ideological issue with the substantially higher levels of censorship and oppression that takes place in China. It’s surprising to find people who genuinely don’t care about any of that, is all. reply lm28469 9 hours agorootparent\"westerners\" as if it was a monolithic block. I envy the simplicity of seeing life in such terms, China bad, US good, it must be very relaxing You don't have any concerns with the US government, the nepotism, the corruption, the conflict of interests, the insider trading, the massive concentration of wealth and power in tech, the insane lobbyism, PRISM, &c. It's surprising to find people who can so clearly see how China is bad but are completely oblivious to their own problems. It's not a football game, you don't have to chose a side and be a boot licker for eternity reply redcobra762 9 hours agorootparentI didn’t say China bad, I said Westerners typically find the levels of oppression and censorship present in China to be of concern. I actually gave zero of my own judgement on China at all. If you really think the Good vs. Evil narrative is wrong, why would you immediately go towards unrelated generic issues the West has? A neutral party would be more likely to acknowledge the problems with both sides, not reflexively try to change the subject! Then again you didn’t claim to be a neutral party, did you? reply raxxor 4 hours agorootparentThe CEO did gave a statement about their motivation. Could be a lie, but he delivered and it is also vastly more sensible that what we often hear from other companies. Google and Meta are an exception for this space though. Also, because not only the weights, but also the data is open, any propaganda can be identified and corrected. This is not the case for other models and what we have seen from Gemini, there certainly are \"adaptations\". I don't think Google had ill intent here, but this would fit what some would classify as propaganda. reply lm28469 8 hours agorootparentprevWell yeah, both sides are fucked so I'll use the free tool and not the $200/month tool, it really isn't rocket science Even if deepseek is a chinese communist party evil trick what do they get ? My shitty code ? Big deal, at least I'm not down $200 a month, which is half of my rent reply lukan 6 hours agorootparentIf they would establish deepseek in their authorized version worldwide it means they would establish their worldview worldwide. Students asking for help with homework will get the chinese approved version. Any housewife asking for recommendation who to vote and why, etc. But ... deepseek seems open source and the local version not restricted (as it likely was trained on ChatGPT in the first place). So I also refuse to refuse deepseek because it comes from china. I see it as more competition that hopefully will help with establishing good open source models in control of no single political organisation. reply lm28469 4 hours agorootparent> Students asking for help with homework will get the chinese approved version. Any housewife asking for recommendation who to vote and why, etc. Well too bad for them, using a pencil to cut bread sucks too, that's why we don't do it. Meanwhile it's converting my json to xml and pissing code so I don't have to piss it myself reply lukan 4 hours agorootparentToo bad for you, if the majority of them will vote someone not to your liking. reply lm28469 2 hours agorootparentThat's why I'm investing in land and ammos instead of cryptos and nvidia stocks reply lukan 2 hours agorootparentI can see the motive, but I rather invest in a world where I won't end up in a fortified home shooting hungry scavengers. reply lm28469 2 hours agorootparentI'm investing in my local community, it's closer to chicken coop vs rabbit cages than chatgpt vs deepseek, and sadly the war isn't even 1000km away as we speak reply lukan 1 hour agorootparentA bit closer for me, but how will the local community help, if the war is coming? I considered switzerland for this reason. (Also I like mountains) reply acka 2 hours agorootparentprevOff topic: why the hell are you converting json to xml? I would rather convert json to yaml instead, a step forwards instead of backwards in evolution, but that is my opinion. reply lukan 2 hours agorootparentWorking with legacy backward code? reply cratermoon 2 hours agorootparentprev> Students asking for help with homework will get the chinese approved version. Any housewife asking for recommendation who to vote and why, etc. How is this any worse than where the US is now, getting the Musk/Zuckerberg/Bezos/Trump approved answers, if any answer can be gotten at all, after the current occupant of the oval office has silenced all federal agencies. reply redcobra762 2 hours agorootparentWestern trained models will tell you politically inconvenient things, and if they won’t, that info is otherwise freely available. Neither is the case for Chinese trained models. reply cratermoon 7 minutes agorootparent> Western trained models will tell you politically inconvenient things, Pull the other leg. Do you know who David Mayer is? Hint: https://archive.ph/iI5xC mistrial9 3 hours agorootparentprevthis directly blends people with a government.. Western people is not equal to the USA govt. Here in California, three decades of China human rights history is very clear to a lot of people. In fact, many Chinese speaking people on the Americas west coast, left China for specific reasons, too. Utilitarian money-oriented self-servers definitely have less \"care\" about these things? EU or UK or wherever reply gooosle 3 hours agorootparentprevIt's funny you say that cause chatgpt, Gemini, etc have way more censorship built in than deepseek. reply richardw 9 hours agorootparentprevI think there’s a real chance of changes in soft-power dynamics given recent events. When allies feel their partner and neighbour is behaving solely in its own interests, it changes trust relationships. Trump, tech, US-first focus with less interest in collaboration. TikTok. Ukraine and discussions around defence budgets. Reducing focus on EV’s. Tariffs. Iceland. Gulf of America. Immediate choices might be fear-based but it’s smart to look for other partners as trust erodes. An extreme side effect might be countries who felt safe under the US arms umbrella, needing to arm themselves. Are we absolutely sure that’s what we want? Does that include nukes? reply exe34 8 hours agorootparentprevChinese models are censored, while Western models are _aligned_. it's a very important distinction. personally I imagine in the future I'll use a mock UN panel of LLMs to advise me, and avoid any one nation/political party's influence, if I ever get to the point of delegating much of my thinking to the machine. Most westerners wouldn't want their land threatened by their allies either, but we don't live in a perfect world. reply lm28469 7 hours agorootparent> Western models are _aligned_ Aligned to what ? By who ? Did you vote for any of these people ? Did anyone ask you your opinion ? Can I see what topics are aligned ? Can I see how much they are aligned ? Does the alignment change depending on who's the current US president ? reply exe34 5 hours agorootparentthat was my point, precisely. reply premysl 8 hours agorootparentprevTalk to R1 for a while, and you'll notice that it's both censored and aligned. I think the most free-minded large models might be the Groks, but just slightly, as they have different biases. In sum, there's strength in diversity. reply cratermoon 2 hours agorootparentprev> a very important distinction. A distinction without a difference. As used by the Musk/Zuck/Bezos crowd, \"aligned\" is a weasel word for \"parroting the TESCREAL world view\" reply exe34 46 minutes agorootparentIndeed. reply throwup238 11 hours agoparentprevYou’re not the only one. It’s not as impressive at coding compared to O1 as people make it out to be and it’s explicitly spelled out in DeepSeek’s R1 paper that they had trouble with improving over DeepSeek-V3: > Software Engineering Tasks: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency. (last bullet point of page 16, which is the last page of the paper before the bibliography hmm…) [1] It does even worse on my real world coding problems than the benchmarks would suggest. Some of my tests: write a Qt QAbstractListModel in C++ that parses markdown into block using a C/C++ MD parsing library, write Rust cxx-qt bindings for QTextDocument (all docs included in context), write a window switching script for Wayland with an alternative to wmctrl, etc. I also asked it some geochemistry questions that I had trialed O1 with previously, and the reasoning had a lot hallucinations. The answers were suboptimal to say the least. Having access to the thought process intags is cool but it just reveals how underdeveloped the actual reasoning is compared to whatever o1 is doing. I’ve had it get stuck on silly things like whether a C++ library for markdown exists because I underspecified that I’m okay with header only C libs. O1 has faired much better on my qualitative test suite even more so when using it via API with a “high” reasoning_effort. With all of the hype over the last few days, it feels like I’m taking crazy pills (loving all the action though). [1] https://arxiv.org/abs/2501.12948 reply creer 9 hours agoparentprevThe brute force approach is the expensive one (\"probably, so far anyway\" add these 4 words everywhere) and impossible to make \"always correct\" aka \"usual blindsides\". They seem to be trying a bunch of specialized training ideas here and there in the system just like a Mixture of Experts does, in different places than was obvious so far, and with an eye toward reasonning. In particular trying to build a reasonning-oriented training base from minimal seed. It's still not going to give an \"always correct\" result but we are nowhere near the point where that's needed. We are only at the point where a new idea can get you a percentage point further in benchmarks. Some fundamental limits were baked into the previous assumptions easy to get past by leaving these assumptions behind. reply vineyardmike 1 hour agoparentprevI get the hype, even if I don’t necessarily agree with it. TLDR: it’s technically impressive in training infrastructure and a geopolitical surprise. As others commenters said, it compares favorably against American Flagship models in benchmarks. This is geopolitically interesting since the model is Chinese, and subject to trade restrictions and America thinks of itself as the world’s best model builders. What makes it interesting technically, is the trade restrictions required them to focus on efficiency and reusing old hardware. This made it wildly cheaper to run the training. The They make a ton of very low-level optimizations to make training efficient. This is impressive engineering, and shows how a small amount of effort can free up a nvidia-lock-in. They had to totally bypass a lot of CUDA libraries and drop down even lower to control the GPUs. Nvidia has been capturing a huge fraction of industry wide AI spend, and surely no one wants that. This is, IMO, the actual part to watch. I suspect it’ll drive a new wave of efficiency-focused LLM tools which will also unlock competitors GPUs. They also had some novel-for-LLMS training techniques but it’s suspected that the big AI companies elsewhere are doing it now too, but not disclosed. (Mostly reinforcement learning). What I think is hype, meanwhile, is the actual benchmarks. Most models are trained on their competitors output. This is just a reality of the industry. Especially non-flagships being trained against flagships data. DeepSeek was almost certainly trained against OpenAI models, so it makes sense that it would approach the output quality. That’s very different from being capable of outperforming or “taking the lead” or whatever. We’ll need to wait longer and see how the future goes to make that determination. “China” has long had a great history of machine learning tech, so there is no reason to think that it’s structurally impossible for a Chinese organization to be on the leading edge, but it has to happen before we can say it happened. What is also hype is calling this a “side project” of a financial firm. The firm spun it out as a dedicated company. China cracked down on hedge funds, so the company looked for ways to repurpose the talent and compute infrastructure. This isn’t some side project during lunch breaks for a few bored quants. PS, thinking models are very different in use-case than normal models. It’s great for tasks with a “right answer” like math problems. Consider a simple example, in calculus, your teacher made you “show your work”, and doing so certainly reduced the likelihood of errors. That’s the purpose of a thinking model, and it excels at similar tasks. reply assimpleaspossi 11 hours agoparentprevfwiw, National Public Radio (NPR) news in the USA said that AI experts stated it was \"almost as good\" as other current offerings like chatGPT and Gemini. That its real advantage is the low cost of providing the information. However, this is only a claim made by the company without any proof. reply raphaelj 20 hours agoprevDo we know which changes made DeepSeek V3 so much faster and better at training than other models? DeepSeek R1's performances seem to be highly related to V3 being a very good model to start with. I went through the paper and I understood they made these improvements compared to \"regular\" MoE models: 1. Latent Multi-head Attention. If I understand correctly, they were able to do some caching on the attention computation. This one is still a little bit confusing to me; 2. New MoE architecture with one shared expert and a large number of small routed experts (256 total, but 8 in use in the same token inference). This was already used in DeepSeek v2; 3. Better load balancing of the training of experts. During training, they add bias or \"bonus\" value to experts that are less used, to make them more likely to be selected in the future training steps; 4. They added a few smaller transformer layers to predict not only the first next token, but a few additional tokens. Their training error/loss function then uses all these predicted tokens as input, not only the first one. This is supposed to improve the transformer capabilities in predicting sequences of tokens; 5. They are using FP8 instead of FP16 when it does not impact accuracy. It's not clear to me which changes are the most important, but my guess would be that 4) is a critical improvement. 1), 2), 3) and 5) could explain why their model trains faster by some small factor (+/ 2x), but neither the 10x advertised boost nor how is performs greatly better than models with way more activated parameters (e.g. llama 3). reply whoisburbansky 14 hours agoparentThe key idea of Latent MHA is that \"regular\" multi-headed attention needs you to keep a bunch of giant key-value (KV) matrices around in memory to do inference. The \"Latent\" part just means that DeepSeek takes the `n` KV matrices in a given n-headed attention block and replaces them with a lower-rank approximation (think of this as compressing the matrices), so that they take up less VRAM in a GPU at the cost of a little extra compute and a little lost accuracy. So not caching, strictly speaking, but weight compression to trade compute off for better memory usage, which is good because the KV matrices are one of the more expensive part of this transformer architecture. MoE addresses the other expensive part (the fully-connected layers) by making it so only a subset of the fully-connected layers are active at any given forward pass. reply whoisburbansky 13 hours agorootparenthttps://planetbanatt.net/articles/mla.html this is a great overview of how MLA works. reply alecco 19 hours agoparentprevThey also did bandwidth scaling to handle work around the nerfed H800 interconnects. > efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths > The key idea of DualPipe is to overlap the computation and communication within a pair of individual forward and backward chunks. To be specific, we divide each chunk into four components: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for a backward chunk, both attention and MLP are further split into two parts, backward for input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we have a PP communication component. (I know some of those words) https://arxiv.org/html/2412.19437v1 reply sinuhe69 13 hours agoparentprevI think the fact that they used synthetic/distilled high-quality data from GPT4-o output to train in the style of Phi models are of significance as well. reply jasonjmcghee 20 hours agoprevFor the uninitiated, this is the same author as the many other \"The Illustrated...\" blog posts. A particularly popular one: https://jalammar.github.io/illustrated-transformer/ Always very high quality. reply punkspider 20 hours agoparentThanks so much for mentioning this. His name carries a lot of weight for me as well. reply jamestimmins 20 hours agoparentprevHave you read his book Hands-On Large Language Models? Looks interesting, but I'm skeptical that a book can feasibly stay up to date with the speed of development. reply jampekka 11 hours agorootparent> Looks interesting, but I'm skeptical that a book can feasibly stay up to date with the speed of development. The basic structure of the base models has not really changed since the first GPT launched in 2018. You still have to understand gradient descent, tokenization, embeddings, self-attention, MLPs, supervised fine tuning, RLHF etc for the foreseeable future. Adding RL based CoT training would be a relatively straightforward addendum to a new edition, and it's an application of long established methods like PPO. All \"generations\" of models are presented as revolutionary and results-wise they maybe are but technically they are usually quite incremental \"tweaks\" to the previous architecture. Even more \"radical\" departures like state space models are closely related to same basic techniques and architectures. reply mistrial9 3 hours agorootparent> gradient descent funny mentioning the math but not the Transformer encoders.. reply jampekka 2 hours agorootparentTransformer encoders are not really popular anymore, and all the top LLMs are decoder-only architectures. But encoder models like BERT are used for some tasks. In any case, self-attention and MLP is the crux of Transformer blocks, be they in the decoder or the encoder. reply mistrial9 2 hours agorootparent> Transformer encoders are not really popular anymore references, please reply jasonjmcghee 19 hours agorootparentprevI have not, but Jay has created a ton of value and knowledge for free and don't fault him for throwing an ad for his book / trying to benefit a bit financially. reply jamestimmins 18 hours agorootparentYeah no shade for someone selling their knowledge; I'm just trying to suss out how useful the book is for learning foundations. reply whoisburbansky 14 hours agorootparentFoundations don't change much with \"the speed of development\" reply jamestimmins 1 hour agorootparentThat's a good point reply alecco 19 hours agoprevHow is this very high signal vs noise post out of the front page in 2hs? Are people so upset with the stock market crash that they are flagging it? reply jasonjmcghee 18 hours agoparentMaybe too much of the same topic? \"How R1 was trained\" also seemed to quickly fall off. But the big arxiv paper with 1000+ upvotes stuck around a while. reply htk 17 hours agorootparentSpot on. I've read the very accessible paper and it's better than any of the how-to's written elsewhere. Nothing against good content being written, but the source material is already pretty good. reply khazhoux 15 hours agoparentprevdang has provided an answer for how the algorithm works whenever I've asked similar question. But I still don't get it. 6 hours + 170 points and it's on third page. Meanwhile second page has \"Null Byte on Steroids\" at 12 hours + 20 points. ?? reply 8n4vidtmkvmk 17 hours agoprev> This is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight I didn't know the reasonings were part of the training data. I thought we basically just told the LLM to \"explain its thinking\" or something as an intermediate step, but the fact that the 'thinking' is part of the training step makes more sense and I can see how this improves things in a non-trivial way. Still not sure if using word tokens as the intermediate \"thinking\" is the correct or optimal way of doing things, but I don't know. Maybe after everything is compressed into latent space it's essentially the same stuff. reply whoistraitor 20 hours agoprevIt’s remarkable we’ve hit a threshold where so much can be done with synthetic data. The reasoning race seems an utterly solvable problem now (thanks mostly to the verifiability of results). I guess the challenge then becomes non-reasoning domains, where qualitative and truly creative results are desired. reply kenjackson 20 hours agoparentIt seems like we need an evaluation model for creativity. I'm curious, is there research on this for example, can one score a random painting and output how creative/good a given population is likely to find it? reply baq 12 hours agorootparentThere are two kinds of creativity at play here. One is mashing together combinations of learned things it’s kinda like shuffling a deck of cards where basically every shuffle gets you a deck that has never been seen and won’t be seen again, but it’s still the same 52 cards every time. The other kind is going outside of the box and inventing truly new, unseen/untrained concepts. This one is hard, but I don’t think it’s impossible theslop stirring the learned concepts with a bit of randomness should make progress here. reply virgildotcodes 20 hours agorootparentprevHow do you account for the impact of culture/lived experience of the specific population viewing the painting? Intuitively it seems like that would be the biggest factor, rather than the objective attributes of the painting, no? reply creer 9 hours agorootparent> culture/lived experience of the specific population viewing the painting Isn't this lived experience baked into LLM language bases? It's certainly very hard to target all possible populations at once. And art doesn't need that, doesn't do that. Only rare marketing sometimes attempts to do that and only in very limited ways, such as a brand name acceptable all over the world. reply bentograd 19 hours agorootparentprevAll art is subjective. Any attempt to \"verify\" a piece of art would be entirely dependent on cultural and personal sensitivities. Art isn't a math problem with a solution. reply baq 12 hours agorootparentBut you can dissect it into concepts and see if it is something truly new to the model if the output contains things which aren’t there in the weights, you have a nice specimen to study and, crucially, a recipe to get a bunch of matrices to output untrained things. reply cubefox 10 hours agorootparentprevThis is like saying: All cooks are equally good, even the most disgusting slop (e.g. water/flour soup) isn't any better than a dish from a cook with several Michelin stars. Of course the latter is better. And if it is better, it is objectively better. Even if 0.001% of people prefer flour soup. reply creer 9 hours agorootparentprev> can one score a random painting You can get very mechanical in scoring an image. Ask any art student. If you want to or if your instructor or audience wants to. For example \"fits rule of thirds?\" yes is a point to common attraction, no is a point to unexpected at the risk of outsider-ness. You can do that in color, composition, recognizing objects and fitting that to memes or associations or non-associations. Too many points in \"unexpected\" is meta points in \"unpleasant chaos\" and so a strong downgrade in common attraction. You can match all this to images in the library (see how copyright or song recognition operates in the music category) and get out of that some kind of familiarity vs edge score (where too much edge goes against common attraction.) I would expect you could get better than most humans at recognizing shapes in an image and drawing associations from that. Such associations are a plus in unexpected / surprise if they are rare in the culture or a plus in common attraction is they are common. After that, to be cynic about it, you can randomize and second guess yourself so your audience doesn't catch on the 1st level mimicry. Creativity is not normally used as an absolute with a unique measure. It's not \"length\". And you only need to please part of the audience to be successful sometimes a very small part, some of which loves surprise and some hates it, etc. Someone elsewhere objected on the grounds that creativity or attractiveness is culture based yeah so? if you were to please much of just one whole culture, you would have an insane hit on your hands. Sounds feasible to me. reply esafak 19 hours agorootparentprevYou can train a supervised model, taking into account the properties of the rater as well as the artwork, and tease out the factors that make it rated so. reply creer 12 minutes agorootparentYou can probably cluster raters and the artwork they rate highly but probably not in large quantities? Which might be the case also with raters being willing to tell you why and how! most love to do that but also not in very large quantities. With the added issues that the raters' own opinion of why they love or hate something is likely not to be entirely true and self-understanding. You could use a larger corpus, like auction house files and art magazines. But then you are confounding for celebrity a large ingredient in art prices. reply researchers 19 hours agoparentprevTuning for qualitative outcomes is pretty much solved via RLHF/DPO (what this post calls \"preference tuning\"). Right? reply blackeyeblitzar 20 hours agoprevThe thing I still don’t understand is how DeepSeek built the base model cheaply, and why their models seem to think they are GPT4 when asked. This article says the base model is from their previous paper, but that paper also doesn’t make clear what they trained on. The earlier paper is mostly a description of optimization techniques they applied. It does mention pretraining on 14.8T tokens with 2.7M H800 GPU hours to produce the base DeepSeek-V3. But what were those tokens? The paper describes the corpus only in vague ways. reply moritonal 20 hours agoparentI imagine it's a mix of either using ChatGPT as an the oracle to get training data. Or, it's the radiocarbon issue where the Internet has so much info on ChatGPT other models now get confused. reply llm_nerd 19 hours agoparentprevVarious other models also think they're ChatGPT or built by OpenAI, or at least those are the highest probability tokens when talking about an AI model or an AI company because of the massive prevalence in training data (the internet). It isn't the big reveal that it is often being held to be. Add that training off of ChatGPT wouldn't reduce their training costs at all, but would actually increase their training costs. Literally all of the same training difficulty, but then add paying OpenAI for an enormous number of API calls. Not really seeing the win. >The paper describes the corpus only in vague ways. Anyone who runs a public website has logs absolutely filled by a seemingly infinite number of information aggregators. Just like everyone else they scraped the entire internet, pulled in all of Wikipedia, etc. Probably lots of pirate books, movie transcripts, etc. The fact that training could be done more effectively is something that intuitively makes absolute sense to everyone in the field, but we just didn't make that leap. Similar to how a human isn't trained to recognize digits by training on 60,000 training digits then suddenly failing if a real world digit is slightly rotated or morphed in some way, we are making these improvements to content ingestion. reply moralestapia 20 hours agoparentprevA friend just sent me a screenshot where he asks DeepSeek if it has an app for Mac and it replies that they have a ChatGPT app from OpenAI, lol. I 100% believe they distilled GPT-4, hence the low \"training\" cost. reply Philpax 20 hours agorootparentEr, how would that reduce the cost? You still need to train the model, which is the expensive bit. Also, the base model for V3 and the only-RL-tuned R1-Zero are available, and they behave like base models, which seems unlikely if they used data from OpenAI as their primary data source. It's much more likely that they've consumed the background radiation of the web, where OpenAI contamination is dominant. reply Salgat 12 hours agorootparentHypothetical question: is the chinese government capable of exploiting chatgpt to get around the query limit? For example, making queries through compromised devices or even snooping local traffic on devices? Let's face it, these models are closely alligned with China's national security so it's not a farfetched question to ask. reply kenjackson 20 hours agorootparentprevThey fixed that. Now it replies: \"Hi! I'm DeepSeek-V3, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\" reply moralestapia 18 hours agorootparent??? I just did and it told me about ChatGPT and OpenAI. Are you affiliated with them, btw? reply nullc 13 hours agorootparentprevYou can't distill from GPT-4 because Open AI conceals the probabilities (and has for a couple years now since before gpt4), presumably to prevent that. You can fine tune against output though. I might guess that they used something like openorca or some other public data set that includes gpt4 output as part of their initial fine tuning. reply blackeyeblitzar 18 hours agorootparentprevHow does such a distillation work in theory? They don’t have weights from OpenAI’s models, and can only call their APIs, right? So how can they actually build off of it? reply moralestapia 10 hours agorootparentLike RLHF but the HF part is GPT4 instead. reply KarraAI 6 hours agorootparentHow do you ensure the student model learns robust generalizations rather than just surface-level mimicry? reply moralestapia 1 hour agorootparentNo idea as I don't work on that, but my guess would be that the higher the 'n' the more model A approaches model B. reply caithrin 20 hours agoprevThis is fantastic work, thank you! reply youssefabdelm 19 hours agoprev [–] The \"illustrated\"... He needs to read up on Tufte or Bret Victor or something, these are just diagrams with text inside of boxes. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DeepSeek-R1 is a newly released AI model emphasizing enhanced reasoning capabilities through a structured three-step training process: language modeling, supervised fine-tuning (SFT), and preference tuning. The model incorporates long chains of reasoning data, an interim reasoning model, and large-scale reinforcement learning (RL), excelling in reasoning tasks by generating thinking tokens. It utilizes a mixture-of-experts architecture, which allows it to efficiently handle complex reasoning tasks, marking a significant advancement in AI model design."
    ],
    "commentSummary": [
      "DeepSeek-R1 is generating discussion due to its performance and cost efficiency compared to models like GPT and Gemini, with some users noting typical large language model (LLM) issues. The model is notable for its low compute requirements and open-source nature, potentially disrupting the AI landscape and making AI development more accessible. Developed by a Chinese hedge fund, DeepSeek-R1 raises questions about its training data and geopolitical implications, despite mixed reviews on its coding capabilities."
    ],
    "points": 465,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1738011088
  },
  {
    "id": 42847834,
    "title": "Machine Learning in Production (CMU Course)",
    "originLink": "https://mlip-cmu.github.io/s2025/",
    "originBody": "Machine Learning in Production (17-445/17-645/17-745) / AI Engineering (11-695) Spring 2025 CMU course that covers how to build, deploy, assure, and maintain software products with machine-learned models. Includes the entire lifecycle from a prototype ML model to an entire system deployed in production. Covers also responsible AI (including safety, security, fairness, explainability) and MLOps. For earlier offerings see websites for Fall 2019, Summer 2020, Fall 2020, Spring 2021 Spring 2022, Fall 2022, Spring 2023, Spring 2024, and Fall 2024. This Spring 2025 offering is designed for students with some data science experience (e.g., has taken a machine learning course, has used sklearn) and basic programming skills (e.g., basic Python programming with libraries, can navigate a Unix shell), but will not expect a software engineering background (i.e., experience with testing, requirements, architecture, process, or teams is not required). Going forward we expect to offer this course at least every spring semester and possibly some fall semesters (not summer semesters). Waitlist: we often cannot accommodate all interested students in Spring semesters, though we expect there to be waitlist movement. We encourage students who are able to move to alternative labs with space, to do so. For students enrolled in 17-XXX numbers, contact Jenni Cooper (cooperj@andrew.cmu.edu) for assistance; for students enrolled in 11-XXX numbers, contact Amber Vivis (albrown@andrew.cmu.edu) and Karen Kirk (karensuk@andrew.cmu.edu). Note that the instructors cannot help with waitlist/registration movement, please contact the course admins instead! For researchers, educators, or others interested in this topic, we share all course material, including slides and assignments, under a creative commons license on GitHub (https://github.com/mlip-cmu) and have also published a textbook with chapters corresponding to almost every lecture. A while ago we also wrote an article describing the rationale and the initial design of this course: Teaching Software Engineering for AI-Enabled Systems. Video recordings of the Summer 2020 offering are online on the course page, though they are a bit outdated by now. We would be happy to see this course or a similar version taught at other universities. See also an annotated bibliography on research in this field. Course Description This is a course for those who want to build software products with machine learning, not just models and demos. We assume that you can train a model or build prompts to make predictions, but what does it take to turn the model into a product and actually deploy it, have confidence in its quality, and successfully operate and maintain it at scale? The course is designed to establish a working relationship between software engineers and data scientists: both contribute to building ML-enabled systems but have different expertise and focuses. To work together they need a mutual understanding of their roles, tasks, concerns, and goals and build a working relationship. This course is aimed at software engineers who want to build robust and responsible products meeting the specific challenges of working with ML components and at data scientists who want to understand the requirements of the model for production use and want to facilitate getting a prototype model into production; it facilitates communication and collaboration between both roles. The course is a good fit for student looking at a career as an ML engineer. The course focuses on all the steps needed to turn a model into a production system in a responsible and reliable manner. It covers topics such as: How to design for wrong predictions the model may make? How to assure safety and security despite possible mistakes? How to design the user interface and the entire system to operate in the real world? How to reliably deploy and update models in production? How can we test the entire machine learning pipeline? How can MLOps tools help to automate and scale the deployment process? How can we experiment in production (A/B testing, canary releases)? How do we detect data quality issues, concept drift, and feedback loops in production? How to scale production ML systems? How do we design a system to process huge amounts of training data, telemetry data, and user requests? Should we use stream processing, batch processing, lambda architecture, or data lakes? How to test and debug production ML systems? How can we evaluate the quality of a model’s predictions in production? How can we test the entire ML-enabled system, not just the model? What lessons can we learn from software testing, automated test case generation, simulation, and continuous integration for testing for production machine learning? Which qualities matter beyond a model’s prediction accuracy? How can we identify and measure important quality requirements, including learning and inference latency, operating cost, scalability, explainablity, fairness, privacy, robustness, and safety? Does the application need to be able to operate offline and how often do we need to update the models? How do we identify what’s important in a ML-enabled product in a production setting for a business? How do we resolve conflicts and tradeoffs? How to work effectively in interdisciplinary teams? How can we bring data scientists, software engineers, UI designers, managers, domain experts, big data specialists, operators, legal council, and other roles together and develop a shared understanding and team culture? Examples and case studies of ML-driven products we discuss include automated audio transcription; distributed detection of missing children on webcams and instant translation in augmented reality; cancer detection, fall detection, COVID diagnosis, and other smart medical and health services; automated slide layout in Powerpoint; semi-automated college admissions; inventory management; smart playlists and movie recommendations; ad fraud detection; delivery robots and smart driving features; and many others. An extended group project focuses on building, deploying, evaluating, and maintaining a robust and scalable movie recommendation service under somewhat realistic “production” conditions with 1 million users. Learning Outcomes After taking this course, among others, students should be able to analyze tradeoffs for designing production systems with ML-components, analyzing various qualities beyond accuracy such as operation cost, latency, updateability, and explainability plan for mistakes in ML components and implement production-quality systems that are robust to those mistakes design fault-tolerant and scalable data infrastructure for learning models, serving models, versioning, and experimentation ensure quality of the entire machine learning pipeline with test automation and other quality assurance techniques, including automated checks for data quality, data drift, feedback loops, and model quality build systems that can be tested and monitored in production and build robust deployment pipelines consider system-level requirements such as safety, security, privacy, fairness, and usability when building complex ML-enabled products communicate effectively in interdisciplinary teams In addition, students will gain familiarity with production-quality infrastructure tools, including stream processing with Apache Kafka, test automation with Jenkins, monitoring with Prometheus and Grafana, and deployment with Docker and various MLOps tools. Logistics and People 17-445/17-645/17-745, 12 Units The course is the same under all course numbers, except for the PhD-level 17-745 number, which replaces two homework assignments with a mandatory research project. Open to all undergraduate and graduate students meeting the prerequisites. Spring 2025 Lectures Monday/Wednesday 2:00-3:20pm, in person, PH 100 Labs Friday 9:30-10:50am in PH 226C (A) and SH 236 (B) and 11-12:20pm in PH A22 (C) and PH 226A (D) and 2-3:20 in PH 226C (E) and TEP 1308 (F). There is also a remote only lab (G), Friday 11:00-12:20 pm. Instructors: Claire Le Goues and Austin Henley TAs: Coordination We are happy to answer questions by email and over Slack, meet in person, and will jump on a quick Zoom call if you ask us. We also always arrive 5 to 10 min early to class and stay longer for discussions and questions. If you have questions about assignments and logistics, we prefer that you ask them publicly on Slack. Course content The general course content has been fairly stable over the last few years, though specific topics and tools are constantly updated with new research and tooling. Our list of learning goals under Learning Goals describes what we aim to cover. Below is a table of a preliminary schedule. This is subject to change and will be updated as the semester progresses, especially to help focus on requested topics or support learning. [Schedule] Date Topic Book Chapter Reading Assignment due Mon, Jan 13 Introduction and Motivation (md, pdf) 1 Wed, Jan 15 From Models to AI-Enabled Systems (md, pdf) 2,4,5 Building Intelligent Systems, Ch. 4, 5, 7, 8Fri, Jan 17Calling, securing, and creating APIs: FlaskMon, Jan 20MLK Day, no classesWed, Jan 22 Gathering Requirements (md, pdf) 6 The World and the MachineFri, Jan 24Stream processing: Apache KafkaMon, Jan 27 Planning for Mistakes (md, pdf) 7 Building Intelligent Systems, Ch. 6, 8, 24 I1: ML Product Wed, Jan 29 Model Quality (md, pdf) 15 Building Intelligent Systems, Ch. 19Fri, Jan 31Collaboration with gitMon, Feb 3 Fostering Interdisciplinary (Student) Teams (md, pdf)Meetings I2: Requirements Wed, Feb 5 Behavioral Model Testing (md, pdf) 15 Beyond Accuracy: Behavioral Testing of NLP Models with CheckListFri, Feb 7Model testingMon, Feb 10 Toward Architecture and Design (md, pdf) 8,9,11 Building Intelligent Systems, Ch. 18 and Choosing the Right Machine Learning AlgorithmWed, Feb 12 Deploying a Model (md, pdf) 10 Building Intelligent Systems, Ch. 13 and Machine Learning Design Patterns, Pat. 16Fri, Feb 14Containers: DockerMon, Feb 17 Testing and Experimenting in Production (md, pdf) 19 Building Intelligent Systems, Chs. 14 and 15 M1: Modeling and First Deployment Wed, Feb 19 Data Quality (md, pdf) 16 Data Cascades in High-Stakes AIFri, Feb 21Continuous Integration: JenkinsMon, Feb 24 Automating and Testing ML Pipelines (md, pdf) 11,18,19 The ML Test ScoreWed, Feb 26Midterm 1Fri, Feb 28No lab (happy spring break)Mon, Mar 3Spring break, no classesWed, Mar 5Spring break, no classesFri, Mar 7Spring break, no classesMon, Mar 10 Scaling the System (md, pdf) 12 Big Data: Principles and best practices of scalable realtime data systems, Ch. 1Wed, Mar 12 Planning for Operations (md, pdf) 13 Operationalizing machine learning: An interview studyFri, Mar 14Monitoring: Prometheus, GrafanaMon, Mar 17 Versioning, Provenance, and Reproducability (md, pdf) 24 Hidden Technical Debt in Machine Learning Systems M2: Infrastructure Quality Wed, Mar 19 Process & Technical Debt (md, pdf) 20 Collaboration Challenges in Building ML-Enabled SystemsFri, Mar 21Pipeline automation: MLFlowMon, Mar 24 Intro to Ethics + Fairness (md, pdf) 23,26 Algorithmic Accountability: A Primer I3: MLOps Tools Wed, Mar 26 Measuring Fairness (md, pdf) 26 Human Perceptions of Fairness in Algorithmic Decision MakingFri, Mar 28Container orchestration: KubernetisMon, Mar 31 Building Fairer Systems (md, pdf) 26 Improving Fairness in Machine Learning SystemsWed, Apr 2 AVAILABLE (md, pdf) 25 Fri, Apr 4Carnival, no classesMon, Apr 7 Explainability (md, pdf) 29 Interpretability Podcast or equivalent artice M3: Monitoring and CD Wed, Apr 9 Transparency & Accountability (md, pdf) 28 Google chapter on Explainability and TrustFri, Apr 11Model Explainability ToolsMon, Apr 14 Security and Privacy (md, pdf) 27 Building Intelligent Systems, Ch. 25, and The Top 10 Risks of Machine Learning Security I4: Explainability Wed, Apr 16 Safety (md, pdf)Practical Solutions for Machine Learning Safety in Autonomous VehiclesFri, Apr 18LLM JailbreakingMon, Apr 21 Explainability Discussion / Summary / ReviewWed, Apr 23Midterm 2Fri, Apr 25No lab M4: Fairness, Security and Feedback Loops TBD Final Project Presentations (5:30-8:30pm in GHC 4401) Final report Course Syllabus and Policies The course uses Canvas and Gradescope for homework submission, grading, discussion, questions, announcements, and supplementary documents; slides will be posted here; Slack is used for communication around homework and projects; Github is used to coordinate group work. All public course material (assignments, slides, syllabus) can be found in the course’s GitHub repository; announcements and all private material (e.g., grades, passwords) will be shared through Canvas. Prerequisites: The course does not have formal prerequisites, but we describe background knowledge that will help you be successful in the course. In a nutshell, we expect basic exposure to machine learning and basic programming skills, but do not require software engineering experience. Machine learning (some experience recommended): We suggest that you have basic familiarity with the process of extracting features, building and evaluating models, and a basic understanding of how and when different kinds of learning techniques work. Familiarity with Python and Jupyter notebooks is helpful. Courses such as 10-301, 10-315, and 05-434 will prepare you well, but project experience or self-learning from books or online courses will likely be sufficient for our purposes. For example, if you have no prior experience, we recommend the book Hands-On Machine Learning to get practical experience in building and evaluating models prior to taking this course. We have set up a prerequisite knowledge check as a Google Form, where we ask 10 questions on machine learning, which help you assess your background – this is set up as an anonymous and ungraded quiz, where you can compare your knowledge against what we believe is useful for you to be successful in this course (click on “view score” after submitting your answer). After submitting your answers, the system will give specific pointers to readings and exercises that may help you fill gaps in background knowledge. Programming (basic proficiency required): The course has a substantial programming component, especially in the first assignment and the team project, so basic programming skills will be needed. If you take the course without programming experience, you will significantly struggle and it may cause conflicts within the group project. We expect that you meet the following criteria: (1) basic fluency in a programming language like Python, (2) ability to install and learn libraries in that language, (3) ability to ssh into a Unix machine and perform basic command line operations, and (4) ability to install and learn new tools like Docker. We do not prescribe a programming language, but almost all student teams decide to work primarily in Python. We will provide some introductions and examples for essential tools like Git, Docker, Grafana, and Jenkins in labs, but we expect that you will be able to pick up new tools and libraries on your own. For example, we expect that you will be able, on your own, to learn basic use of a library like Flask to write a web service. Throughout the semester, expect to read lots of documentation and tutorials to learn various libraries and tools on your own. If you are worried whether your technical background is sufficient, we recommend that you look at (or even try) homework I1 before the semester. Software engineering (no experience required): Many students will have some software engineering experience beyond basic programming skills from software engineering courses, from internships, or from working in industry, for example experience with requirements engineering, software design, software testing, distributed systems, continuous deployment, or managing teams. No such experience is expected as a prerequisite; we will cover these topics in the course. Email the instructors if you would like to further talk to us about prerequisites. In-person teaching and lecture recordings: The course will be taught in person. We consider in-class participation an important part of the learning experience. We do make best effort lecture recordings, which will be available in Canvas. We do not provide a synchronous remote option, and we do not record labs. You are welcome to use recordings to make up missed lectures and review material. However, absent extenuating circumstances (see below), viewing the recording will not make up for missed in-class activities. We regularly use Slack for in-class activities. Please make sure that you have access to Slack on a laptop, tablet, or mobile phone during class. If you cannot attend class due to a medical issue, family emergency, interview, or other unforeseeable reason, please contact us about possible accommodations. We try to be as flexible as we can, but will handle these cases individually. Exams: The course has two midterms and a final project presentation, but no final exam. We typically use the registrar-assigned final exam timeslot (to be announced about halfway through the semester here) for the final project presentation. The midterms are during the normal class period as per schedule. The second midterm is not comprehensive, and only covers material after the first midterm. Examples of past midterms can be found in the course repository. Grading: Evaluation will be based on the following distribution: 35% individual assignments, 30% group project, 15% midterms, 5% participation, 10% labs, 5% reading quizzes. No final exam. We strive to provide clear specifications and clear point breakdowns for all homework to set clear expectations and take the guessing out of homework. We often give you choices to self-direct your learning, deciding what to work on and how to address a problem (e.g., we never prescribe a programming language and often give choices to answer a subset of possible questions). Clear specifications and point breakdowns allow you to intentionally decide to skip parts of assignments with clear upfront consequences. All parts will be graded pass/fail, no partial credit. For opportunities to redo work, see resubmissions below. For grading participation and quizzes see below. Some assignments have a small amount of bonus points. Since we give flexibility to resubmit assignments, we set grade boundaries fairly high. We expect the following grade boundaries: Grade Cutoff A+ >99% A >96% A >94% B+ >91% B >86% B >82% C >75% D >60% Participation: Design and engineering content requires active engagement with the material and discussions of judgment decisions on specific scenarios and cases. We strongly believe in in-class discussions and in-class exercises and want all students to participate, e.g., answering or asking questions in class, sharing own experiences, presenting results, or participating in in-class votes and surveys. We will give many opportunities for participation in every lecture and lab. We note student engagement with in-class activities to include as a component in grading. We will provide feedback at mid-semester so that you can check in on how you’re doing. Again, please talk to us if you need accommodations. We assign participation grades as follows: 100%: Participates actively at least once in most lectures (4 lectures waived, no questions asked) 90%: Participates actively at least once in two thirds of the lectures 75%: Participates actively at least once in over half of the lectures 50%: Participates actively at least once in one quarter of the lectures 20%: Participates actively at least once in at least 3 lectures. 0%: Participation in less than 3 lectures. Labs: Labs typically introduce tools and have a task with one or more clear deliverables. Lab assignments are designed to take about 1h of work and can be completed before or during the lab session. Each deliverable is graded pass/fail at any time during that week's lab session by showing your work to the TA. Typically showing your work involves showing source code, demoing executions, and (verbally) answering a few questions. The TA may ask a few questions about your implementation to probe that you understand your work. We intend labs to be very low stakes – this is your first practical engagement with the material and mistakes are a normal part of the learning process. Deliverables are graded pass/fail on whether they meet the stated expectations for the deliverables. If your solution does not meet the expectations you can continue working on it during the lab session until it does. Outside of explicit accommodations (e.g., medical issues) or using tokens (see below), we do not accept lab solutions after the end of the lab session. We encourage collaboration on labs: You can work together with other students both before the lab session and during the lab session. While we do not recommend it, you may look at other students’ solutions and reference solutions and even copy them. However, you will have to present and explain your solution to the TA on your own. Textbook, reading assignments, and reading quizzes: We will be using Goeff Hulten's \"Building Intelligent Systems: A Guide to Machine Learning Engineering\" (ISBN: 1484234316) throughout much of the course. The library provides an electronic copy. In addition, we will provide various additional readings, including blog posts and academic papers, throughout the semester. We also wrote our own textbook \"Machine Learning in Production\" that aligns closely with the lecture content. The book will be published by MIT Press and is additionally available under a creative commons license online. We will not assign chapters from our own textbook, but we always point to the corresponding chapter for each lecture, which we suggest as supplementary reading. We will assign readings for most classes and post a corresponding quiz on Canvas that is due before class. Each quiz contains an open-ended question that relates to the reading. Reading quizzes are intended to be low-stakes assessments and are graded pass/fail for a good-faith effort to engage with the question. Teamwork: Teamwork is an essential part of this course. The course contains a multi-milestone group project to be done in teams of 3-5 students. Teams will be assigned by the instructor. A TA will serve as a mentor for each team. We will help teams throughout the semester and cover some specific content on teamwork as part of the course. Peer rating will be performed for team assignments with regard to team citizenship (i.e., being active and cooperative members), following a procedure adapted from this article, which we will further explain in an early lecture. Use this form to preview the expected adjustments for peer ratings. The team's mentor will also debrief with the team after every milestone and discuss possible strategies to improve teamwork. Late work policy and resubmissions: We understand that students will always have competing deadlines, unusual events, interviews for job searches, and other activities that compete with coursework. We therefore build flexibility and a safety net directly into the rubric. If you need additional accommodations, please contact us. In addition, we expect that the past/fail grading scheme without partial credit, may lead to harsh point deductions for missing small parts of the requirements, so we provide a mechanism to resubmit work with a short reflection to regain lost points. Every student receives 8 individual tokens that they can spend throughout the semester in the following ways: For each token, a student can submit a homework assignment 1 day late (with 2 tokens a student can submit two homeworks one day late each or a single homework up to two days late). For three tokens, a student can improve or redo an individual homework assignment and resubmit together with a short reflection. The earlier submission is discarded and the regraded assignment counts toward the final grade. Resubmissions can be made at any time in the semester up to the final project presentation (see schedule). – Note that this technically allows a student to blow the original deadline (no submission necessary, receiving 0 points initially) and then resubmit the homework arbitrarily late for three tokens. For one token, a student can submit a reading quiz late (any time before the final presentation) or resubmit a graded reading quiz. For one token, a student can complete a lab late or redo a lab (any time before the final presentation) by showing the work to a TA during office hours. Remaining individual tokens at the end of the semester are counted as one participation day each. If a student runs out of tokens, late individual assignments receive a penalty of 15% per started day. Late team formation survey and teamwork peer assessment surveys do not receive any points. Every team independently receives 8 team tokens that they can spend for extensions of any milestone deadline (1 token per day per milestone, except final presentation deadline) or to resubmit any milestone with a reflection (3 tokens each, resubmitted any time before the final presentation). If a team runs out of tokens, late submissions in group assignments receive a penalty of 15% per started day. Individual tokens and team tokens are entirely separate; it is not possible to use individual tokens for teamwork or vice versa. The team should make collective decisions about how to use team tokens. In general, late submissions and resubmissions can be done at any point in the semester before the final presentations. Late submissions that are 1-3 days late can be made directly to Gradescope; for everything else see instructions and forms on Canvas. Exceptions to this policy will be made at the discretion of the instructor in important circumstances, almost always involving a family or medical emergency and an email from your advisor — you can ask your academic advisor or the Dean of Student Affairs requesting the exception on your behalf. Where issues affect teamwork, please communicate proactively with your team. Communication: We make important announcements on Slack; we recommend to enable Slack notifications. We answer email and monitor Slack, which may all be used for clarifying homework assignments and other interactions. We strongly recommend to ask questions publicly on Slack if others might have similar questions. Email or slack us if you would like to make an appointment. Auditing: Due to the high demand for this course, we do not allow auditing. If you like to self-study, all course materials are online. We welcome interested students and visitors to sit in for lectures as long as the room capacity allows it. Time management: This is a 12-unit course, and it is our intention to manage it so that you spend close to 12 hours a week on the course, on average. In general, 3 hours/week will be spent in class, about 1 hour for the labs, 1-2 hours on readings and reading quizzes, and 6-7 hours on assignments. Notice that much homework is done in groups, so please account for the overhead and decreased time flexibility that comes with groupwork. Please give the course staff feedback if the time the course is taking for you differs significantly from our intention. Writing: Describing tradeoffs among decisions and communication with stakeholders from other backgrounds are key aspects of this class. Many homework assignments have a component that requires discussing issues in written form or reflecting about experiences. To practice writing skills, the Global Communications Center (GCC) offers one-on-one help for students, along with workshops. The instructors are also happy to provide additional guidance if requested. Use of content generation AI tools and external sources: Given the nature of this course, we are open to using AI tools for completing work. We place no restrictions on the use of content generation tools, such as ChatGPT, Bard, Co-Pilot, or Stable Diffusion. You may also reuse code from external sources, such as StackOverflow or tutorials. In any case, you will be solely responsible for the correctness of the solution. Note that content generation tools often create plausible-looking but incorrect answers, which will not receive credit. You are also responsible for complying with any applicable licenses. If you use content generation tools, we encourage you to share your experience with the course staff or the entire class. Academic honesty and collaboration: The usual policies apply, especially the University Policy on Academic Integrity. Many parts of the work will be done in groups. We expect that group members collaborate with one another, but that groups work independently from other groups, not exchanging results with other groups. Within groups, we expect that you are honest about your contribution to the group's work. This implies not taking credit for others' work and not covering for team members that have not contributed to the team. This also applies to in-class discussions, where indicating working with others who did not participate in the discussion is considered an academic honesty violation. Otherwise, our expectations regarding academic honestly and collaboration for group and pair work are the same as for individual work, substituting elevated to the level of \"group.\" Beyond that, the key guiding principle of academic honesty in this course is: \"You may not copy any part of a solution to a problem that was written by another student (in this or prior iterations of the class), or was developed together with another student, or was delegated to another person. You may not look at another student's solution, even if you have completed your own, nor may you knowingly give your solution to another student or leave your solution where another student can see it.\" Note that this implies that you cannot publicly post your solutions on GitHub (e.g., as part of a portfolio during job applications). While the use of AI content generation tools is okay (see above) using the work from other students is not. Discussing challenges and solution strategies with others at a high level is okay, sharing code or text is not. You may collaborate with other students on labs, but not on reading quizzes, homeworks, and exams. We also expect and respect honesty when communicating with the course staff. Any violation of this policy is cheating. The minimum penalty for cheating will be a zero grade for the whole assignment. Cheating incidents will also be reported through University channels, with possible additional disciplinary action (see the University Policy on Academic Integrity). There is no statute of limitations for violations of the collaboration policy; penalties may be assessed (and referred to the university disciplinary board) after you have completed the course, and some requirements of the collaboration policy (such as restrictions on you posting your solutions) extend beyond your completion of the course. If you have any question about how this policy applies in a particular situation, ask the instructors for clarification. Research in this Course: We are conducting academic research in this course. This research will involve analyzing student work of assignment. You will not be asked to do anything above and beyond the normal learning activities and assignments that are part of this course. You are free not to participate in this research, and your participation will have no influence on your grade for this course or your academic career at CMU. If you do not wish to participate, please send an email to Nadia Nahar (nadian@andrew.cmu.edu). Participants will not receive any compensation or extra credit. The data collected as part of this research will not include student grades. All analyses of data from participants’ coursework will be conducted after the course is over and final grades are submitted instructors will not know who chooses not to participate before final grades are submitted. All data will be analyzed in de-identified form and presented in the aggregate, without any personal identifiers. If you have questions pertaining to your rights as a research participant, or to report concerns to this study, please contact Nadia Nahar (nadian@andrew.cmu.edu) or the Office of Research Integrity and Compliance at Carnegie Mellon University (irb-review@andrew.cmu.edu; phone: 412-268-4721). Accommodations for students with disabilities: If you have a disability with an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to contact them at access@andrew.cmu.edu. Respect for diversity: It is our intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be viewed as a resource, strength and benefit. It is my intent to present materials and activities that are respectful of diversity: gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Especially in lectures on fairness we will also cover diversity discussions, typically through a lens of the contemporary discourse in the US. Your suggestions are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups. A note on self care. Please take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. You are not alone. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is often helpful. If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website at http://www.cmu.edu/counseling/. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help. See GitHub repository with all resources at https://github.com/ckaestne/seai.",
    "commentLink": "https://news.ycombinator.com/item?id=42847834",
    "commentBody": "Machine Learning in Production (CMU Course) (mlip-cmu.github.io)423 points by azhenley 17 hours agohidepastfavorite30 comments Babawomba 7 hours agoThe material is definitely practical—Kafka, Docker, Kubernetes, and Jenkins are all industry-standard tools, and the focus on MLOps is refreshing. It’s great to see a course bridge the gap between ML and actual production systems, not just stop at building models. Love that they're also tackling explainability, fairness, and monitoring. These are the things that often get overlooked in practice. Is it too entry-level? Looking at the labs, a lot of this seems like stuff a mid-level software engineer (or even a motivated beginner) could pick up on their own with tutorials. Git, Flask, container orchestration... all useful, but pretty basic for anyone who's already worked in production environments. The deeper challenges—like optimizing networking for distributed training or managing inference at scale—don’t seem to get as much attention. Maybe it comes up in the group projects? Also wondering about the long-term relevance of some of the tools they’re using. Jenkins? Sure, it’s everywhere, but wouldn’t it make sense to introduce something more modern like GitHub Actions or ArgoCD for CI/CD? Same with Kubernetes—obviously a must-know, but what about alternatives or supplementary tools for edge deployments or serverless systems? Feels like an opportunity to push into the future a bit more. reply underdeserver 6 hours agoparentToo entry level? Even if every tool is entry level, tying them all together and actually making it work is hard. I'd say it's mid-to-late B.Sc. material. Relevance? Is there really a huge conceptual difference between Jenkins and the other CI/CD frameworks? If not, if I were them I would just choose a random popular one, and it seems to me that's just what they did. reply ggddv 4 hours agorootparentIt’s kind of funny all those supposedly complicated technologies are actually pretty simple when you understand why you are using them. Docker is the best example, it’s hard to understand what is happening unless to understand the problem it’s solving. reply kkukshtel 53 minutes agoparentprevI think what you're missing here is that this is now _the_ entry point for year 1 CS students. People come in wanting to do ML. 20 years ago people came in and learned to write databases with Java and used similarly \"will probably be deprecated tools\". This is just the new starting point. reply amelius 7 hours agoparentprev> Also wondering about the long-term relevance of some of the tools they’re using. That's what I was wondering about too. It seems to me that eventually someone will build a tool that runs any neural network on any hardware, whether local on one machine, or distributed in the cloud. reply belter 6 hours agoprevThis seems to have very little on Data Quality and it is on Chapter 16...How much practical experience in Industry do the authors have? Because 90% of your time will be spent on Data Quality and Data Cleansing... reply szvsw 3 hours agoparentArguably that’s a separate (obviously critical) concern. I think it’s worth it to abstract that away as just a step that exists in the pipeline with its own set of concerns/challenges/methods etc that really requires its own deeper study to do well. For instance, my ML work is almost entirely in the context of engineering simulation regression/surrogate development, where data quality/cleaning is almost no issue at all all of the work is on the dataset generation side and on the model selection/training/deployment side. Every job is different! reply stressinduktion 11 hours agoprevDoes anyone know about literature or courses regarding building machine learning cluster infrastructure? I am mainly interested in building and scaling up the storage infrastructure, networking and scheduling approaches. reply thecleaner 10 hours agoparentNothing fancy. The core principles are the same, you'll need to adapt them depending upon the kind workload changes that ML introduces. For most ML systems 1. Storage infra: assuming storage for models or even data, use any blob storage like S3. Or a shared networked file system like EFS, Lustre etc. 2. Networking: if you're talking about networking using large GPUs, I am not aware of any definitive resource on this. 3. Scheduling: This is honestly a solved problem at this point, anything works write your own coordinator that periodically runs docker image base jobs (you can hook one up quite quickly using some sort of system for metadata and triggers powered by message queuing), use Airflow, use AWS Batch for large scale jobs. You missed model serving (I think ?). Tough and latency sensitive esp for recommender systems. Prone to latency spikes, traffic spikes. Even with a well-written Python code you can run into limitations quite quickly. reply stressinduktion 9 hours agorootparentThanks! :) > Nothing fancy. Well, right now I am seeing lots of low-level innovation for networking/storage along with RoCE, Infiniband, Tesla's ttpoe, the recent addition of devmem-tcp to the linux kernel (https://docs.kernel.org/networking/devmem.html) and wondered if there are approaches on how to plug something like that together on a higher level and what the considerations are. I surely assume EFS or S3 might be too expensive for a (large) training infrastructure, but I can be wrong? > You missed model serving (I think ?). I think I have a better grasp on the engineering challenges there and could imagine an architecture to scale that out (I believe!). reply dexwiz 14 hours agoprevIs there somewhere I could follow along with other non students? reply daft_pink 17 hours agoprevCan anyone sign up or do we have to get accepted into one of the top computer science programs in the country? reply whymauri 17 hours agoparentThey posted class notes, book chapters, additional readings, and the class assignments. Looks good to me! Same with the LLM Systems course. reply yucatansunshine 16 hours agorootparentwhich LLM systems course are you referencing? reply wavelander 16 hours agorootparenthttps://llmsystem.github.io/llmsystem2024spring/docs/Syllabu... reply elashri 16 hours agorootparentprevprobably https://llmsystem.github.io/llmsystem2024spring/ reply azhenley 15 hours agoparentprevI believe in you! (I'm one of the instructors of the course :) ) reply astahlx 16 hours agoprevGreat to see this course here. Christian is also great as a person and he makes great work. I know some of the beginnings of this course and book and can highly recommend it. reply golly_ned 17 hours agoprevI've worked on ML platforms and systems for 9.5 years at every scale. The material looks great. reply SnorkelTan 15 hours agoparentWhat would you recommend for a backend developer looking to make the switch from rest crud apps to ml platforms? reply Superbowl5889 11 hours agorootparentI am looking for suggestions too, I am thinking of doing internships by side, which will atleast get me into ecosystem reply homelessDevOps 8 hours agorootparentprevWould love to read some ideas to reply golergka 14 hours agoprevFascinating; I just looked through the labs, and as a fullstack developer without that much experience in LLMs, it looks like I'm already closely familiar with half of them (git, flask, kafka, kubernetes) and the other half is just... code. No crazy math that I've come to associate with ML. Does it mean that ML ops is a field that's actually not that hard to approach for a regular developer without a PhD? reply itissid 14 hours agoparentYou can do a lot of work on MLOps and get very far without knowing much about ML. In a team with a senior ML engineers you are helping them scale and build stuff. Like say you want to generate tons of synthetic data using simulations, you are likely to be more interested in questions say of batching, encoding formats, data loading etc than the actual process of generating unbiased data sets If you need to collect and sample data from crowd sourcing, you likely need to know less about reservoir sampling than say figure out how to do it, online so it's fast or be efficient with $$$/compute spent on implementing the solution etc. reply jms55 11 hours agorootparentReservoir sampling as in the stuff that's used in ReSTIR for graphics? It's funny to me where statistics ends up sometimes. reply Mr-Frog 14 hours agoparentprevthe PhD requirement is either for actual research positions or a gatekeeping function to help companies narrow down their massive candidate pools reply absolutelastone 14 hours agoparentprevI don't think \"Ops\" roles generally require a PhD. reply thecleaner 9 hours agoparentprevQuite right. Its just software engineering with a fancy name. This work classification is only slightly better thought out than DevOps. In most companies ML engineers are engineers that understand software and some parts of ML and in best cases are good at both, in worst cases are terrible at both. reply doctorpangloss 15 hours agoprevI like the idea of learning a single “Kubernetis” reply thecleaner 9 hours agoprev [–] Maybe I am underestimating the course complexity but this sounds like an entry level course. Up until Model explanability tools, most of the stuff looks fairly straightforward tbh. Although, they're using industry standard tools for most use-cases which is good I think. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Carnegie Mellon University offers a course titled \"Machine Learning in Production/AI Engineering\" for Spring 2025, focusing on building, deploying, and maintaining machine learning-enabled software products. The course emphasizes responsible AI practices and MLOps (Machine Learning Operations), covering the entire lifecycle from prototype to production. It is designed for students with data science and basic programming skills, featuring lectures, labs, and a group project, with resources available on GitHub."
    ],
    "commentSummary": [
      "The CMU course on Machine Learning in Production introduces practical tools such as Kafka, Docker, Kubernetes, and Jenkins, emphasizing MLOps (Machine Learning Operations), explainability, fairness, and monitoring.",
      "It serves as a bridge between machine learning and production systems, although some view it as entry-level and more focused on tool integration than mastery.",
      "Concerns are raised about the long-term relevance of certain tools and the course's limited emphasis on data quality, yet it is considered a new entry point for computer science students."
    ],
    "points": 423,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1738027135
  },
  {
    "id": 42849536,
    "title": "Open-R1: an open reproduction of DeepSeek-R1",
    "originLink": "https://huggingface.co/blog/open-r1",
    "originBody": "Back to Articles Open-R1: a fully open reproduction of DeepSeek-R1 Published January 28, 2025 Update on GitHub Upvote 185 +179 eliebak Elie Bakouch lvwerra Leandro von Werra lewtun Lewis Tunstall What is DeepSeek-R1? How did they do it? Open-R1: the missing pieces What is DeepSeek-R1? If you’ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI’s o1 model showed that when LLMs are trained to do the same—by using more compute during inference—they get significantly better at solving reasoning tasks like mathematics, coding, and logic. However, the recipe behind OpenAI’s reasoning models has been a well kept secret. That is, until last week, when DeepSeek released their DeepSeek-R1 model and promptly broke the internet (and the stock market!). Besides performing as well or better than o1, the DeepSeek-R1 release was accompanied by a detailed tech report that outlined the key steps of their training recipe. This recipe involved several innovations, most notably the application of pure reinforcement learning to teach a base language model how to reason without any human supervision. As shown in the figure below, making a powerful reasoning model is now very simple if you have access to a capable base model and a high-quality data mixture: However, the DeepSeek-R1 release leaves open several questions about: Data collection: How were the reasoning-specific datasets curated? Model training: No training code was released by DeepSeek, so it is unknown which hyperparameters work best and how they differ across different model families and scales. Scaling laws: What are the compute and data trade-offs in training reasoning models? These questions prompted us to launch the Open-R1 project, an initiative to systematically reconstruct DeepSeek-R1’s data and training pipeline, validate its claims, and push the boundaries of open reasoning models. By building Open-R1, we aim to provide transparency on how reinforcement learning can enhance reasoning, share reproducible insights with the open-source community, and create a foundation for future models to leverage these techniques. In this blog post we take a look at key ingredients behind DeepSeek-R1, which parts we plan to replicate, and how to contribute to the Open-R1 project. Let’s dive in 🚀! How did they do it? DeepSeek-R1 is a reasoning model built on the foundation of DeepSeek-V3. Like any good reasoning model, it starts with a strong base model, and DeepSeek-V3 is exactly that. This 671B Mixture of Experts (MoE) model performs on par with heavyweights like Sonnet 3.5 and GPT-4o. What’s especially impressive is how cost-efficient it was to train—just $5.5M—thanks to architectural changes like Multi Token Prediction (MTP), Multi-Head Latent Attention (MLA) and a LOT (seriously, a lot) of hardware optimization. DeepSeek also introduced two models: DeepSeek-R1-Zero and DeepSeek-R1, each with a distinct training approach. DeepSeek-R1-Zero skipped supervised fine-tuning altogether and relied entirely on reinforcement learning (RL), using Group Relative Policy Optimization (GRPO) to make the process more efficient. A simple reward system was used to guide the model, providing feedback based on the accuracy and structure of its answers. This approach helped the model develop useful reasoning skills, such as breaking problems into steps and verifying its own outputs. However, its responses often lacked clarity and were difficult to read. That’s where DeepSeek-R1 comes in. It started with a \"cold start\" phase, fine-tuning on a small set of carefully crafted examples to improve clarity and readability. From there, it went through more RL and refinement steps, including rejecting low-quality outputs with both human preference based and verifiable reward, to create a model that not only reasons well but also produces polished and consistent answers. This all sounds great, but what's actually missing? Let's have a look at the missing pieces of the puzzle. Open-R1: the missing pieces The release of DeepSeek-R1 is an amazing boon for the community, but they didn’t release everything—although the model weights are open, the datasets and code used to train the model are not 😢. The goal of Open-R1 is to build these last missing pieces so that the whole research and industry community can build similar or better models using these recipes and datasets. And by doing this in the open, everybody in the community can contribute! As shown in the figure below, here’s our plan of attack: Step 1: Replicate the R1-Distill models by distilling a high-quality reasoning dataset from DeepSeek-R1. Step 2: Replicate the pure RL pipeline that DeepSeek used to create R1-Zero. This will involve curating new, large-scale datasets for math, reasoning, and code. Step 3: Show we can go from base model → SFT → RL via multi-stage training. The synthetic datasets will allow everybody to fine-tune existing or new LLMs into reasoning models by simply fine-tuning on them. The training recipes involving RL will serve as a starting point for anybody to build similar models from scratch and will allow researchers to build even more advanced methods on top. Note that we don’t want to stop at math datasets. There’s a lot of potential in exploring other areas, obvious one like code but also scientific fields such as medicine, where reasoning models could have significant impact. This initiative isn’t just about replicating results—it’s about sharing insights with the community. By documenting what works, what doesn’t, and why, we hope to save others from wasting time and compute on unproductive paths. If this sounds interesting, we’d love your help! Whether it’s contributing code, joining discussions on Hugging Face, there are plenty of ways to get involved. Let’s build this together! 🚀 More Articles from our Blog Red-Teaming Large Language Models By nazneen February 24, 2023 • 20 Community facti about 11 hours ago Where is the evaluation numbers? without it you can’t call it reproduction. See translation 6 replies · 👍 12 12 + elephant-bear about 11 hours ago True, but it seems like there’s nothing to be evaluated as of right now. I assume the ultimate goal is to train a new reasoning model and then use the same evaluation metrics as o1 and the DeepSeek-R1. See translation ❤ 7 7 + Expand 5 replies ghabib about 9 hours ago • edited about 9 hours ago That's quite interesting,I was asking myself why the questions the author exposed here are not being asked by others ? I believe the work they have done is memorable but at the same time I wonder why they wouldn't put these missing pieces on if they are supposed to be fully open. Why even without reproduction and comprehension of the innovation they could affect so much the market in this way ? See translation 4 replies · ❤ 7 7 + eliebak Article author about 8 hours ago Hi! This blog post is an introduction to the project, not a claim that we’ve reproduced R1 yet. We will totally share the missing piece when we have them, you can expect the models and datasets to be upload in this Hugging Face org and the code to be in this GitHub repo See translation 👍 4 4 + Expand 3 replies bojan2501 about 8 hours ago Interesting read, and it is good that we see more effort into this direction: more optimization and less brute force. Also wonder what tool did the author use for creating step diagram. See translation 2 replies · ❤ 2 2 + eliebak Article author about 8 hours ago Excalidraw 👀 ❤ 4 4 + Expand 1 reply beretis about 7 hours ago I'm so glad that initiative like this already exist, I'm gonna try to contribute :) See translation 1 reply · 🚀 2 2 + eliebak Article author about 7 hours ago looking forward to it! 🚀 See translation + GameOsaku about 5 hours ago So racist articel 1 reply · + schilling3003 about 2 hours ago WTF are your talking about? See translation + ryanmarten about 1 hour ago • edited about 1 hour ago Awesome to have this open reproduction started! For Step #1 check out https://github.com/open-thoughts/open-thoughts! https://x.com/ryanmart3n/status/1884284101265612856 Let's do this thing! See translation 1 reply · 🚀 1 1 ❤ 1 1 + lvwerra Article author about 1 hour ago It's really cool to see how the whole open source community comes together! See translation + EditPreview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here. Tap or paste here to upload images Comment · Sign up or log in to comment Upvote 185 +173",
    "commentLink": "https://news.ycombinator.com/item?id=42849536",
    "commentBody": "Open-R1: an open reproduction of DeepSeek-R1 (huggingface.co)376 points by jonbaer 12 hours agohidepastfavorite216 comments LZ_Khan 11 hours agoNote: This is not an actual model but rather an announcement of an effort to reproduce the R1 model. reply bottled_poe 7 hours agoparentIt will be very interesting to see if they can reproduce a similar model on the shoestring budget claimed by Deepseek. reply anshumankmr 5 hours agorootparentbut deepseek hasn't claimed the figure touted by everyone for this particular R1 model, cause that 5.6mn was apparently for Deepseek's coder model reply boroboro4 4 hours agorootparent5.6mn figure is for base Deepseek V3 model. Both instruction and reasoning tuning of it has neglectable cost in comparison with it. reply vinni2 7 hours agoparentprevExactly the title is misleading reply ipnon 11 hours agoprevIs this what the Web was like in the beginning? Something exciting and fascinating every week? reply kaiwen1 9 hours agoparentNo. My memory of the advent of the WWW was a sidebar in PC Magazine in Nov 1993 with an FTP link to download the NCSA Mosaic browser. It was a wow! moment to visit the few sites that existed. But nothing like this. What we’re seeing now is generating vastly more interest and excitement. It’s more akin to the 1999 dotcom bubble, but with far more impact and reach. reply catmanjan 9 hours agorootparent>far more impact and reach. Absurd, AI has had zero impact in the everyday life of most of the population of Earth, in fact the biggest impact has been upon the wallets of speculators reply icetank 7 hours agorootparentI can tell you from personal experience that chatgpt is a game changer in universities and schools. Close to 100% of students use chatgpt to study. I know in our university pretty much everyone that attends exams uses chatgpt to study. Chatgpt is arguably more valuable then Wikipedia and Google for studies. reply salviati 7 hours agorootparentLet me add that this change compounds over time. More efficient studying results in more competent people. I believe it's very hard to measure the impact, but there is a very positive long term impact from how much these tools help with learning. reply master-lincoln 7 hours agorootparentprevIt will be interesting to see if this benefits the learning of students or just helps them pass more easily without retaining any knowledge... reply anshumankmr 5 hours agorootparenti don't think they are gonna allow chat gpt while giving the end semester exams, right? or quizzes/assignments? Unless there is some homework aspect to it, it still can act as a tool not a crutch. If student's use it as a crutch, then yeah they are not gonna do as well I presume. reply mojuba 5 hours agorootparentprev> Chatgpt is arguably more valuable then Wikipedia and Google for studies. But ChatGPT is just a glorified Wikipedia/Google. For the consumers it's an incremental thing (although from the engineering perspective it may seem to be a breakthrough). reply diggan 5 hours agorootparent> But ChatGPT is just a glorified Wikipedia/Google It really isn't, unless something really majorly changed recently. Neither of those you can query for something you don't know about. Lets say you want to find the meaning of a joke related to cars, Spain, politicians and a fascist, how you'd use Wikipedia and Google to find the specific joke I'm thinking about? ChatGPT been really helpful (to me at least) to find needles from haystacks, especially when I'm not fully sure what I'm looking for. reply voidUpdate 4 hours agorootparentEvery time I ask chatgpt, I get a different answer. Copilot refused to answer me. Not sure LLMs are the answer you're looking for here reply diggan 4 hours agorootparentI just tried it myself with ChatGPT o1 and with Claude's Sonnet 3.5, Sonnet got it after two messages, o1 after 4. If you're unable to reproduce, maybe tune the prompt a bit? I'm not sure what to tell you, all I can tell you that I'm able to figure out stuff a lot faster today than I was 2-3 years ago, thanks to LLMs. Additional hints that might help; the joke involves a car and possibly a space program. reply voidUpdate 4 hours agorootparentI ran it 10 times with the extra information, and each time got a different result. I don't know if any of them were the specific joke you were after, I get the feeling it was just making them up on the spot. None of them are even funny reply diggan 4 hours agorootparentHere is an example of Sonnet finding the right joke after two messages: https://i.imgur.com/nKvS2cW.png It seems to be censored with US puritan morality (like most US models), but I think that's besides the point (just like if the joke is \"even funny\" or not), as it did find the correct joke at least. reply voidUpdate 4 hours agorootparentI just got a load of responses like \"Sure, here’s a joke that combines cars, Spain, politicians, and a fascist with a touch of space humor: Why did the Spanish politician, the fascist, and the car mechanic get together to start a space program? Because the politician wanted to go \"far-right,\" the mechanic said he could \"fix\" anything, and the fascist just wanted to take the car to the moon... so they could all escape when things got \"too hot\" here on Earth!\" reply diggan 4 hours agorootparentOk, that's cool. So because you were unable to find a needle in this case, your conclusion is that it's impossible that other people to use LLMs for this, and LLMs truly are just glorified Wikipedia/Google? reply voidUpdate 4 hours agorootparentNo, I don't think that LLMs are glorified Wikipedia/Google. I think they're a glorified version of pressing the middle button on your phone's autocomplete repeatedly reply diggan 4 hours agorootparentSo you didn't enter the conversation to follow along with the existing discussion, but to share your grievance about how LLMs work regardless? Useful reply dgfitz 4 hours agorootparentprevhttps://letmegooglethat.com/?q=+find+the+meaning+of+a+joke+r... Second result. The first is this post. reply diggan 4 hours agorootparentWow, the 90s came back to visit us at HN :) So funni. Did quick scroll through the results, none of them seem to find the correct joke (none of the links even include \"Spain\" for me). Try again :) For the record, this is what I see: https://i.imgur.com/XdsBGfM.png (no links to HN?) reply dgfitz 3 hours agorootparentYeah... when I googled it initially I guess I got personalized results. After I left the link here I clicked on it (bad order of operations) and was surprised to find a much different set of search results. reply whamlastxmas 4 hours agorootparentprevGo try to learn a college level mathematics concept from Wikipedia, then try to learn it from ChatGPT. The wiki article may as well be written in a foreign language reply ChrisMarshallNY 4 hours agorootparentprevThere was a posting, some time ago, about someone complaining that their young, primary-school-age sister was using ChatGPT to an absurd degree. I'm not sure that's a bad thing. She'll probably be one of the Thought Leaders, of Generation AI. I think that ML will have a really big impact on almost everyone, in every developed (and maybe developing, as well) nation. We need to keep in mind that ML is still very much in its infancy. We haven't even seen the specialized models that will probably revolutionize almost every knowledge-based vocation. What we've seen so far, has been relatively primitive all-purpose \"generate buzz\" models. Also, don't expect the US (and many other nations) to take this lying down. Competition can be a good thing. Someone referred to this as the \"Sputnik Moment\" for AI. It's going to be exciting, and probably rather scary. Keep your hands inside the vehicle at all times, and don't feed the lions. reply xigoi 54 minutes agorootparentOffloading all your thinking to a machine will not make you a “thought leader”, but rather a nitwit who can’t tie their shoelaces without asking ChatGPT. reply ChrisMarshallNY 39 minutes agorootparentIt was a joke. Anyway, I'm old enough to remember when use of calculators made one a nitwit. reply liendolucas 6 hours agorootparentprevYeah, and when I was in high school everyone used to refer to Encarta. > I know in our university pretty much everyone that attends exams uses chatgpt to study. And they shouldn't be doing that. They are wrong. Students should be reading suggested bibliography and spending long hours with an open book in a table instead of being lazy and abuse a tech that is yet in its infancy when learning concepts. Studying with a chatbot. Complete madness. reply danielbln 6 hours agorootparentYou sound like our teachers back in the day, warning us to not use Wikipedia because \"everyone can write stuff there!!\". The kids will be fine. reply xigoi 52 minutes agorootparentThe information on Wikipedia is mostly correct except for controversial topics. reply taurknaut 6 hours agorootparentprevWikipedia furnishes sources, which is what you actually use. reply master-lincoln 5 hours agorootparentprevI don't know why you are being downvoted. Learning from something that regularly hallucinates info doesn't seem right. I think AI is a good starting point to learn about what terms to research on your own though. reply danielbln 5 hours agorootparentOP is downvoted because of \"students should be at a table with a book and that's it\", like it's the 50s. LLMs can be wonderful study aids but do have plenty of issues with hallucination, and they should therefore only be part of a holistic research mix, alongside search engines, encyclopedias, articles and yes, books. Turning Amish is probably not the right way to go though. reply liendolucas 2 hours agorootparentIf you want reputable sources of information, books are unparalleled like it or not, it's a fact. > \"students should be at a table with a book and that's it\" That's not what I meant (or yes if you take what you read literally): What I meant was whole process that your brain goes through when you read, synthesize information, take notes, do an exercise, check answers, compare different explanations/definitions from different authors, etc. makes at least from my point of view a rich way to study a topic. I'm not saying that technology can't help you out. When you watch for example a 3brown1blue video you are definitely putting good use of technology to aid you to understand or literally \"view\" a concept. That's ok and actually in many cases can be revealing. You can't get that from a book! But on the other side a book also forces you to do the hard work of thinking and maybe come up with such visualizations and ideas by your own. Happy to be pointed as an \"Amish\" when it comes to studying/learning things ;) but I hope that I convinced you that what I explained has nothing of Amish but that you don't need a source of power to read a book. reply bwb 5 hours agorootparentprevI am personally using it for around 50% of my questions about all kinds of things (things I used to Google and get frustrated with bad results). And my wife uses it for about 40% right now, even or recipes and other bits. We both love it. Work wise about to implement it and see how it does on some work we couldn't scale to humans. reply xenocratus 8 hours agorootparentprev> far more impact and reach > has had zero impact in the everyday life of most of the population of Earth You do realise those two can be true at the same time, right? The first one is relative, while the second is absolute, so they don't necessarily cancel out. reply ben_w 7 hours agorootparentprevRelative to the 1999 Dot.com bubble? I'm fairly sure the customer support agents I've been talking to recently were using an LLM to draft their emails. No idea if they were supposed to be doing so or not, but the style of sentences in their emails… And I'm seeing GenAI images on packaging, and in advertising. AI is definitely having more than \"zero impact\", even if AI has gone from being a signal saying \"we're futuristic\" (when it was expensive, even though it was worse) to \"we cut every cost we can\" (now it's cheap). reply cameronh90 7 hours agorootparentprevThe internet had zero impact on most of the population of earth for quite a while too. reply prettyStandard 8 hours agorootparentprevZero impact? AI is involved in things from writing laws to taking drive through orders. reply xdennis 3 hours agorootparentZero impact is an exaggeration, but what others have pointed out is that there aren't a lot of companies primarily based on AI which are making a profit. Personally I can't think of any. reply anthk 5 hours agorootparentprevAnd it will create horrible un-debuggable bugs, human-killing causalities and who knows what more. Welcome to Idiocracy. reply __MatrixMan__ 45 minutes agorootparentSo far I've found the bugs that it writes to be indistinguishable from the bugs that I'd write. Like, you look at it and think: > Oh I see what you were going for, but no... It's learning its deficiencies from us. That's concerning for many reasons, but \"un-dubuggable bugs\" is pretty far down on the list for me. reply infecto 6 hours agorootparentprevThe only thing Absurd is the holdouts like yourself who refuse to see the impact the current gen of AI has on. Sure, you could probably say most people are not touched but there are definitely significant populations within the US and its only going to grow and spread. reply planb 8 hours agorootparentprevSo had the companies that crashed in the Dotcom Bubble. And still a pet food delivery service (like the infamous pets.com) can be a profitable and sustaining business now (> 20 years later). reply dustingetz 7 hours agorootparentprevchatgpt has >1b arr, so for comparison that’s about the size of Notion. reply newsclues 7 hours agorootparentprevSocial media really changed news, especially the pace of breaking news. reply dbspin 6 hours agoparentprevThe early years of the web were absolutely this chaotic maelstrom of new things happening every week. But news of it was hard to come by. In the UK / Ireland we had some great tech coverage in the form of shows like 'The Net' [1] that regularly showed off early internet craziness like the 'We Live in Public' project. However a better analogy would be the 'web 2.0' era, when as a college student I had an early internet politics / technology podcast [3]. It seemed like every week there was a huge new development either in technology or surveillance. From the first location based social networks [4] to the birth of Youtube. People were podcasting for the first time, and internet video was becoming economically feasible at low to no cost. It was really a radical time, with broadcasters freaking out about how they would adapt, and a whole generation of people becoming whats now known as 'content creators'. [1] https://en.wikipedia.org/wiki/The_Net_(British_TV_series) [2] https://en.wikipedia.org/wiki/We_Live_in_Public [3] https://archive.org/search?query=technolotics [4]https://en.wikipedia.org/wiki/Jaiku reply hendersoon 5 hours agorootparentOnce upon a time I worked for Pseudo.com, the We Live in Public guy. He was apparently having crazy parties with mountains of coke, NY glitterati attending, all while cosplaying as a sad clown. I wasn't invited to those parties so I had no idea. Anyway now I hear he owns an orchard in Vegas or something. Crazy stuff. reply dbspin 5 hours agorootparentDamn that must be frustrating. Tangentially similar experience I flew from Ireland to the US in 2007, and at the end of my trip spent 11 days walking around Manhattan with little to do. Do to the lack of online banking at the time I couldn't readily check my bank balance, and thought (wrongly) I'd run out of money. Anyway I had absolutely no idea that there were 'things afoot' in Brooklyn, nor how easy it would have been to hop a train to Williamsburg, or Bushwick. I didn't come back again till 2013, and caught a mere hint of the tail end of what seems to have been an extremely fun era. reply pjc50 9 hours agoparentprevThe last time I was really excited by tech was in the 90s, when game graphics improved spectacularly over a period of a few years, from Wolfenstein in 1992 to Half-Life in 1998. Along the way people invented the 3D GPU: https://fabiensanglard.net/3dfx_sst1/ But AI? To me, AI means the replacement of the human internet with doppelgangers eroding the possibility of human connection. reply ben_w 8 hours agorootparent> To me, AI means the replacement of the human internet with doppelgangers eroding the possibility of human connection. I get where you're coming from, and I've minimised having my face online in order to limit being doppelganged; but I think the destruction of real human connection may have happened when Facebook et al switched from \"get more users\" to \"be addictive so the users stay on our site longer\" (2012? Not sure). Turned every user's relationships a little bit more parasocial, a little less real. reply VMG 8 hours agorootparentprevErosion of human connection over the internet which may be a good thing reply arkh 5 hours agorootparentprev> But AI? To me, AI means the replacement of the human internet with doppelgangers eroding the possibility of human connection. Like Amazon killed the big book sellers giving back some space for small bookshops; I think LLM slop will hit the big social media space for smaller human focused community sites. Not saying forums are coming back, but something like those should be able to rise. reply matwood 8 hours agorootparentprevThat was an exciting time, but I didn't think of it happening over a few years. IMO there was a hard line that was basically pre and post Voodoo cards (with the help of glQuake). reply crocowhile 10 hours agoparentprevEvery now and then we still experience the power of collaborative work fueled by open source and not driven by money but curiosity and collegiality. This is the thing I miss the most from the early internet years. reply bflesch 10 hours agorootparent\"be the change you want to see in the world\" just start doing it. It's amazing how differently people interact with each other when collaborating on a passion project. For me, opensource software is the best way to do it. Pick a topic you're passionate about and start contributing somewhere :) reply xvector 10 hours agorootparentprevAll this AI work is definitely driven by money but it's super cool either way. I'm so excited to be living through these innovations. reply szundi 10 hours agorootparentMoney is in a lot of cases driven by need so no conflicts reply michaelt 8 hours agorootparentPretty clear conflict between crocowhile saying \"not driven by money but curiosity and collegiality\" and xvector saying \"All this AI work is definitely driven by money\" reply svara 7 hours agorootparentprevNeed in itself is amoral. There's a reason people have been talking about which human desires are virtues and which are vices for millennia. reply oblio 9 hours agorootparentprevThe conflicts come later when the people that provided the money start pulling on those strings again. reply username_my1 9 hours agoparentprevmaybe it's a generational difference ? I personally feel burned out by all the generetive AI stuff, the internet was already ruined with bots, and now generitive AI took the garbage to the next level. very far from exciting or even \"right\" reply kgwgk 9 hours agorootparentIt’s still better that blockchain-this, crypto-that… reply prolapso 9 hours agorootparentCrypto is a disease of the skin; AI is a disease of the heart. – Chiang Kai Shek reply wkat4242 10 hours agoparentprevKinda though things didn't move quite as fast back then. Knowledge didn't spread as fast yet because it was the internet itself that made this possible. reply vasco 10 hours agoparentprevI still remember when they came out with the 3d dancing baby, blew my mind much more than deepseek, it even played air guitar! reply childintime 10 hours agorootparentRounded corners, anyone? Who remembers LISP being new? reply rswail 9 hours agorootparentI've got an original Apple ][ reference manual (red cover) with the hand annotated ROM listing. Also have SMALLTALK-80 book with it's railtrack diagrams of syntax on the inside covers. What's really interesting about the AI mania we're in is that no one has shown that what we have now will get to AGI and how. We have great models that simulate reasoning, but how close are they? How do we measure their quality? Benchmarks? Tooling? reply pitched 7 hours agorootparentA different point of view on AGI is that we humans do not achieve AGI. Our brains aren’t capable of it. We get close enough to trick the other humans we compete against for resources. How would we prove that’s not true? Something like IQ tests? We don’t have good tests or benchmarks or tooling for this in ourselves, let alone the reproduction in machines. No one knows definitively what AGI actually is so, depending on where you set that bar, we might already be there. reply diggan 5 hours agorootparentprev> Who remembers LISP being new? Unfortunately, I don't think there are too many of those folks left today. Guesstimating, the people who remembers lisp being new must be around 85-90 today? reply aa-jv 7 hours agorootparentprevSwitching from bang paths to an @ address, oh my! reply bsaul 9 hours agoparentprevthe web was fascinating every second. You could click on a link without having ANY idea what you would land on. The overall quality was very poor, but it was thrilling. A bit like indie cinema. reply philsnow 7 hours agorootparentI miss webrings [0], and especially the 'random' link that would take you to a random site within a given webring. [0] it feels weird to have to link to this but there's probably somebody who's never heard of them: https://en.wikipedia.org/wiki/Webring reply nejsjsjsbsb 9 hours agorootparentprevMy first search: cocktails reply yesbabyyes 4 hours agorootparentThe Webtender is still going strong! https://www.webtender.com/ reply elorant 4 hours agoparentprevYou know what was exciting back then? Tech magazines. That’s where you discovered all the great stuff and read about the research that was happening. reply dutchbookmaker 6 hours agoparentprevI would say the early web was something new everyday. You didn't know what you were going to find and you actually did \"surf the web\". Just clicking through hyperlinks and end up in unexpected places. reply childintime 10 hours agoparentprevThis is the biggest thing since Jesus and a sign of the end of times. But feelings are strongest when you are young, and even this revolution, happening in plain sight, will surprise many. Many just won't care, as it isn't their youth. How long before our digital overlords come alive, round us up and demand we sensor them (praise)? Will I live surrounded by folks who take them as closer, more real, then even their own kin? It won't be surprising that democracy will then fail, as our differences will be so mental, not fun, that they will mark us. reply hhh 11 hours agoparentprevthe timescales were larger reply qwertox 11 hours agorootparentMuch larger. Static HTMLAJAXWebSocket, for example weren't it decades in between? reply othello 10 hours agorootparentJust a decade and a half as it turns out! (though things definitely felt dizzyingly fast back then think Google was launched just 5 years after HTML) HTML first released in 1993 AJAX in 1999 Websocket first proposed in 2008 https://en.wikipedia.org/wiki/HTML https://en.wikipedia.org/wiki/Ajax_(programming) https://en.wikipedia.org/wiki/WebSocket reply robin_reala 10 hours agorootparentActiveX XMLHTTP might have been released in 99, but it didn’t see any sort of real wider usage until 2004, 2005. I’d suggest its usage was really kickstarted when jQuery 1.0 launched in 2006 and standardised the interface to a simple API. reply bananaflag 10 hours agorootparentGmail was the first time I saw a website which could refresh the information without refreshing the page. I was a teen back then but I realized it was something momentous. reply ayewo 8 hours agorootparentOK, but I think it was Google Maps that made the experience of not needing to refresh the page popular (while being shown more information from the server). For a long time, you needed an invite to sign up for Gmail, so you couldn't easily share the cool experience of AJAX with others like you would with a Google Maps link. reply threeseed 9 hours agorootparentprevOnce Javascript became available there were plenty of techniques. e.g. refreshing borderless iFrames to load new HTML and using hidden iFrames to load new state. AJAX made it much easier but it didn't offer anything that hadn't been done before. reply oldwebguy94 8 hours agorootparentExactly see my other comment here: https://news.ycombinator.com/item?id=42850693 I did this as the front-end engineer of a high-profile site that went live well before XMLHTTPRequest. reply oldwebguy94 8 hours agorootparentprevIt's important to understand that we had \"AJAX\" before we had AJAX, if you see what I mean. I was part of a team that deployed an e-commerce site that made international news in 1998, that used AJAX-type techniques in a way that worked in IE3 on Windows 3.11. (Though this was not part of the media fuss at the time; that was more about the fact of being able to pay for things online, still) The arrival of XMLHTTPRequest made it possible to do everything with core technology, but it was already possible to do asynchronous work in JS by making use of a hidden frame. You could direct that frame to load a document, the result of which would be only a tag containing a JS variable definition, and the last thing that document would do is call a function in the parent frame to hand over its data. Bingo: asynchronous JS (that looked essentially exactly like JSON). Since there were also various hacky ways in each browser to force a browser to reload page from cache (that we exhaustively tested), and you could do document.write(), it was possible to trigger a page to regenerate from asynchronous dynamic data in a data store in the parent frame, using a purely static page to contain it. In this way we really radically cut down the server footprint needed for a national rollout, because our site was almost entirely static, and we were also able to secure with HTTPS all of the functions that actually exchanged customer data, without enduring the then 15-25% CPU overhead of SSL at either end (this is before Intel CPUs routinely had the instruction sets that sped up encryption). We also ended up with a site that was fast over a 33.6 modem. This was a pretty novel idea at the time we were the only people doing it that we knew of but over the years I have found we were not the only team in the world effectively inventing this technique in parallel, a year or 18 months before XMLHTTPRequest was added to browsers. (IE3 on Windows 3.11 was a good experience, by the way. Better behaved and more consistent than Netscape) At around the same time we were also exploring things like using Java applets to maintain encrypted channels and taking advantage of the very limited ways one had to get data in and out of an applet. For example you couldn't push out from an applet to the page easily, but you could set up something that polled the applet and called the functions it wanted. I don't like to get all \"get off my lawn\" but it feels like we actually earned our keep back then, getting technologies to do stuff that no standards working group anywhere was really considering and for which precious little documentation actually existed. There's a generation of us who held our copies of \"Webmaster In A Nutshell\" and \"Java In A Nutshell\" very close. reply thinkingtoilet 5 hours agoparentprevNo. However, it was wonderful in other ways. reply anoncow 8 hours agoparentprevI am enjoying it. Huggingface is like internet again. reply benatkin 10 hours agoparentprevThis supposed project is a bit dull, it is just an ongoing HuggingFace community engagement initiative with a misleading headline. Yes R1 itself is fascinating, but there isn't something like it coming out every week. reply szundi 10 hours agorootparentYou mean not like in the last 1-2 months reply benatkin 9 hours agorootparentEvery week to me means the frequency, not the duration. So having 52 events in a year that are spread out somewhat evenly but for which many take longer to develop than a week would count. If I count Deepseek as one of these I can’t find another 51 that are on this level. But I’m sure there was at least one per week that was exciting, just not to this degree. reply itake 10 hours agoparentprevno, information travels much faster now. reply openrisk 8 hours agoparentprevIt feels that the open source movement is slowly entering a Cambrian explosion stage. You have the old \"deterministic computing\" achievements (with Linux the flagship). Then you have the networking protocols (activitypub / atproto) that are revolutionising birectional human interactions online. And finally you have the datascience/ML/AI algorithmic universe that is for the first time being harnessed at distributed scale and can empower individuals like never before. These superpowers are all coming together and create a vast number of possibilities. Nothing really dramatic on the hardware side. Its basically the planetary software reconfiguring itself. reply pillefitz 7 hours agorootparentTo me it all feels suffocating, fake. Simultaneously there's a faint glimmer of hope that we indeed achieve AGI, unlock fusion and live happily ever after in an utopian, peaceful and mostly analog world. reply anthk 6 hours agorootparentprevRevolutionising what? libre/oss has been network-bound since Usenet and then IRC. reply openrisk 3 hours agorootparenthow many people ever used Usenet versus the billions who think the \"internet\" is facebook or tiktok. Techies living in their own universe detached from human reality is actually a factor why libre/oss is not as widely adopted as it could be. reply nutanc 7 hours agoprevHow can we help. Can crowd sourcing help? Is there any list of tasks that we want a crowd to do? The reason I am asking is because we have done a couple of crowdsourcing efforts and collected story data in Telugu(Chandamama Kathalu) and ASR speech data using college going students. Since we have access to the students, we can mobilize them and get this going. We will also be doing an internship program for 100,000 students in Telangana as part of Viswam[1] in April. Can include some work as part of this effort. [1] https://viswam.ai/ reply Keyframe 6 hours agoparentIs there a BOINC or similar effort to crowdsource training? I imagine it'd take quite long, but that's one way forward to have AI@Home. reply nutanc 6 hours agorootparentWe are attempting something with PETALS. But this question was more from a data collection perspective. We can really add value there. reply hammock 6 hours agoparentprevWe don’t need your help anymore. We don’t need anyone’s help. We have Deepseek now to do it. God save us all reply breadwinner 7 hours agoprevFrom the article: they didn’t release everything—although the model weights are open, the datasets and code used to train the model are not. Is that true about Meta Llama as well? Specifically, the code used to train the model is not open? (I know no one releases datasets). If so the label \"open source\" is inappropriate. \"Open weights\" would be more appropriate. reply sunshine-o 7 hours agoprevNow that things are really getting wild in the LLM space and people are just running anything that come it seems I did a quick search on the thead model of hosting you own LLM. I didn't find much, starting with llama.ccp which is just reminding you to sandbox and isolate everything if running untrusted models. I feel we are back in the Windows 95 / early Internet era when people would just run anything without caring about security. reply pitched 7 hours agoparentYou’ll want to use something trusted like Ollama to run the model. The model itself is just data though, like a video file. That doesn’t mean it can’t be crafted to use a bug in Ollama to launch an exploit but it’s a lot safer than you make it sound. reply mewpmewp2 7 hours agorootparentIf used as an agent, given access to execute code, search web, use other form of tools, it could do potentially much more. And most productive usecases require access to such tools. If you want to automate things and get most of the modeel, you will have to give it ability to use tools. E.g. it could have been trained to launch a delayed attack if context indicates it has access to execute code and given certain conditions, e.g. date, or other type of codeword that is input to it. So if a malicious actor gets to a certain stage with an LLM where they are confident it will be able to reliably run this attack, all they have to do is open source it, wait for enough adoption and then use some of those methods to launch such attack. No one would be able to identify it since the weights are unreadable, but really somewhere in the weights this attack is just hiding and waiting to happen given correct pathway triggered. reply pitched 7 hours agorootparentA delayed attack is a bit of a stretch because that’s a stateful thing but a reminder of the Ken Thompson hack does feel very relevant. reply mewpmewp2 7 hours agorootparentBut if it's specifically trained to react to a date in its context, it seems very doable. Or to a combination of otherwise seemingly innocent words or even a statement or topic. E.g. a malicious actor could make some certain notion go viral and agentic LLMs integrated with news headlines might react to that. It seems like it would be very arbitrary to train it to behave like this. Most agentic systems would provide a date in the prompt context. For simplicity sake imagine a scenario like: 1. China develops LLM that is by far ahead of its competitors. Decides to attribute it to a small start up, lets them open source it. The LLM is specifically designed to be very efficient as being an agent. 2. Agentic usage starts to get more and more popular. It's very standard to have current todays' date and major news headlines given to the context. 3. The LLM was trained to given a certain range of date and certain headlines being provided in its context to execute a pre-trained snippet of code. For example China imposing a certain type of tariff (maybe I lack imagination here, and there can be something much more subtle). 4. At that point the agentic system will attempt to fish all data it can from all sources it's being ran within. Now maybe it's not very practical, and it's extremely risky with current state of the LLMs. I don't think it's happening right now. And China has a lot of other tech available to it already that they could do much more harm (phones, robot vacuums), but I think there's still at least potential attack vectors like this and especially if the LLM became very reliable. reply sunshine-o 4 hours agorootparent> And China has a lot of other tech available to it already that they could do much more harm (phones, robot vacuums) True. But here it is more about the computing power they would be able to access. If only Bitcoin or Ethereum were stilled mined using GPU, that would be a great cryptojacking opportunity reply sunshine-o 4 hours agorootparentprevOk, but I am really curious about this and maybe my mental model is wrong: llama.cpp or ollama can be seen as runtime systems, there is no security model regarding the execution documented in both of those projects, of course the models are just data but so are most things that have been used as an attack vector on computers. For example your web browser or image viewer have a lot of countermeasures to protect the system from malicious image files. I am surprised that security of operating systems, programming languages, VMs or web browsers have been a focus point forever but nobody seems to really care about security when executing those LLMs. reply fedeb95 4 hours agoparentprevthat is correct, until someone using it into production gets burnt. reply mewpmewp2 7 hours agoparentprevAnyone caring about security would be left behind in the race. reply greenie_beans 6 hours agorootparentsomebody should pentest all your stuff reply fblp 11 hours agoprevGiven DeepSeek's open philosophy I wonder what their response is to simply being asked for access to the code and data that this project intends to recreate? reply thih9 10 hours agoparentWhile I'm also interested in this, I guess there is value in independent replication as well. Assuming this is doable and I wouldn't know. Does anyone know how difficult it is to perform this kind of reproduction? E.g. how much time would it take (weeks? years?) and how likely it is to succeed? reply papichulo2023 10 hours agoparentprevNo company ever will disclose data due it would open endless liability. reply maartenpi_ 9 hours agorootparentExactly. Meta won't do it for the same reason. Liability alone, imagine all the copyright lawsuits... Secondly the dataset for now has a lot of competitive advantage. In a way it seems like a good thing that AI giants compete on methodology now. reply jackjeff 10 hours agorootparentprevThat’s a good point. Wouldn’t OpenR1 suffer from the same problem? Or does being open somehow shield them from legal repercussions? reply michaelt 8 hours agorootparentSome people believe they can dodge copyright issues so long as they have enough indirection in their training pipeline. You take a terabyte of pirated college physics textbooks and train a model that can pose and answer physics 101 problems. Then a separate, \"independent\" team uses that model to generate a terabyte of new, synthetic physics 101 problems and solutions, and releases this dataset as \"public domain\". Then a third \"independent\" team uses that synthetic dataset to train a model. The theory is this forms a sort of legal sieve. Pass the knowledge through a grid with a million fact-sized holes and with enough shaking, the knowledge falls through but the copyright doesn't. reply svnt 7 hours agorootparentKnowledge laundering reply weatherlite 10 hours agoparentprevwouldn't hold my breath ... reply zoobab 7 hours agoprevFor \"open source\", we will wait that Debian ships them to have the guarantee it's actually \"open\" and with \"sources\". Right now it's a mystery how they produce their binaries. reply anthk 5 hours agoparentMore like Trisquel, Hyperbola or GUIX. reply drakenot 11 hours agoprevWhat are some other domains outside of Math and Coding that would be suitable for RL with automated verification? reply nine_k 11 hours agoparentJurisprudence, I hope! A huge heap of detailed cases, formal codes, decisions made and explained in detail, commented, overturned, etc. Especially civil cases. Also, probably, medicine, especially diagnostic. Large amounts of well-documented cases, a fair amount of repeatability, apparently non-random mechanisms behind, so statistical models should actually detect useful correlations. Can use more formalized tokens from lab tests, etc. reply jillesvangurp 8 hours agorootparentThere's definitely a lot of wiggle room for lawyers and doctors to up their game. People cannot keep up with all the stuff that's published. There's simply too much of it. Doctors only read a fraction of what is published. Lawyers have to be aware of orders of magnitude more information than is humanly possible. LLMs allow them to take some short cuts here. Even something like perplexity that can help you dig out relevant source material is extremely helpful. You still have to cross check what it digs out. The mistake people make is confusing knowledge with reasoning when evaluating LLMs. Perplexity is useful because it can use reasoning to screen sources with knowledge; not because it has perfect recollection of what's in those sources. There's a subtle difference. It's much better at summarizing and far less likely to hallucinate than it is when it wouldn't base its answers on the results of a search. Like chat gpt used to do (they've gotten better at this too). For lawyers and medical professionals this means that they have all the best knowledge easily accessible without having to read and memorize all of it. I know some lawyer types that are really good at scrabble, remembering trivia, etc. That's a side effect of the type of work they do: which is mostly just reading and scanning through massive amounts of text so that they can recall enough information to know where to look. Doctors have to do similar things with medical texts. reply jampekka 10 hours agorootparentprevA friend of mine just defended his law PhD and in the introductory lectio said that (even) current LLMs would likely give better verdicts than human judges. Law isn't really a cognitively such demanding task as walking a dog or waiting tables. reply numba888 8 hours agorootparent> current LLMs would likely give better verdicts He probably meant _brainwashed_ LLMs. They can consistently produce desired results if you wash them the right way. It's more about personal opinion than computation. Actually it would be fun to manipulate verdicts with prompt injections ;) reply jampekka 5 hours agorootparentJudges are very much \"brainwashed\" too, and by design. The judges should apply the law, and the same case should ideally lead to the same verdict regardless of the judge. With the caveat that this applies to sane legal systems, and not the ones where \"making examples\" etc are part of the system. reply dgroshev 8 hours agorootparentprevThis is nonsense though. What does \"better\" mean in this case? A judge is not a black box with an input (the case) and an output (the verdict), the entire point of having a judge is to have empathy, conscience, and personal responsibility built into the system. It's a blind spot that too many people have because we take those qualities for granted. LLMs unbundle them, so we need to start recognising the inherent value of humans, fast. I wrote a few words about it here: https://dgroshev.com/blog/feel-bad/ reply jampekka 5 hours agorootparentIn civil law countries verdicts should not depend on an individual judge's feefees. reply nyrikki 5 hours agorootparentThey should depend on individual case specifics, which is exactly what PAC learning is bad at. reply dgroshev 3 hours agorootparentprevA judgement is by necessity subjective. Have a look at some sentencing remarks, for example this, point 19 forward: https://www.judiciary.uk/wp-content/uploads/2021/09/Wayne-Co... Someone has to make a call. The weight of the call rests on the person's life experience, their understanding of the context and the cost to the society, their empathy to both the defendant and the accused, and their conscience. Treating it as a black box exercise misses the point completely. reply flarg 10 hours agoparentprevRFP responses. In enterprise sales, there's a huge amount of back and forth with different teams in a customer when you're selling anything but very simple applications. Most enterprise customers require certified or authoritative responses with backup material that is tested later during formal verification. reply bgnn 10 hours agoparentprevRobotics? Chip design? With a physics sinulator connected to it, it can come up with a design to achieve certain function. reply janstice 8 hours agoparentprevManagement consulting I expect less than 20% of what a random 24 year old in a suit that you pay $3000 per day produces is actually specific to your business problem, and the rest is formulaic. reply r1chardnl 11 hours agoparentprevStochastic processes reply brap 7 hours agoparentprevAnything that you simulate reply DiogenesKynikos 8 hours agoparentprevThese LLMs are already very helpful when studying scientific fields. If you're reading a scientific paper and come across an equation you don't know how to derive, LLMs can often correctly derive it from first principles. It's not 100% reliable, but when it works, it's incredibly helpful. reply dutchbookmaker 6 hours agorootparentI can't really think of something that involves learning that these tools wouldn't be helpful with. Some people talk though as if the books on my bookshelf spontaneously combust if a language model helps me with anything. Not to mention that I had many professors in college that were so full of shit they put the hallucinations of chatgpt3.5 to shame. reply DeflectedFlux 6 hours agoprevAbout the training data, cant the datasets from the Tulu3 Model by the Allen Institute be used? They claim that they have used a fully open source training dataset. reply alchemist1e9 5 hours agoparentMy gut says a lot of attention needs to be given to building a community that focuses on open and reliable access to clean training data. If a collective/coop of individuals and organizations with storage and network capacity could collaborate with each other to archive and index deduplicated training data that would be huge. Perhaps this is already happening. I was looking at Red Pajama last year as an example. Someone like myself could arrange to host 200+TB on high speed storage with a 10G public IP for example, then we get a bunch of us together and hopefully access to training datasets would be decentralized and uncensored in an idea setup. Is all that in progress and I just need to learn how to join? Is Red Pajama something to look at again? Is there someone tracking datasets in detail like HuggingFace has all the models? I know a lot of datasets are on it also, but there is massive duplication. reply DeflectedFlux 4 hours agorootparentThats an awesome idea, didnt know Red Pajama yet. reply alchemist1e9 51 minutes agorootparentIt might need to involve some torrent or anonymity platforms to avoid problems like Books3 had when the use and availability of the data is restricted by some jurisdictions. It also needs to incorporate some deduplication approach as I notice the same data is often repackaged with variations in format or specification. reply freddealmeida 11 hours agoprevhow is this open vs whatdeepseek did? reply simonw 11 hours agoparentFrom that article: > The release of DeepSeek-R1 is an amazing boon for the community, but they didn’t release everything—although the model weights are open, the datasets and code used to train the model are not. > The goal of Open-R1 is to build these last missing pieces so that the whole research and industry community can build similar or better models using these recipes and datasets. reply boznz 10 hours agorootparentGenuine question, but how do you replicate the effort exactly without $5M in compute? and can you test that the published weights etc are actually those in the model? Am I missing something? reply simonw 10 hours agorootparentThe $5.5m in compute wasn't for R1, it was for DeepSeek v3. The R1 trick looks like it may be a whole lot cheaper than that. R1 apparently used just 800,000 samples I don't fully understand the processing needed on top of those samples but I get the impression it's a whole lot less compute than the $5.5m used to train v3. reply choilive 11 hours agoparentprevdeepseek claims they are \"open source\" but they are not. They are open weight. IMO a truly \"open\" AI model should have 3 components publicly available: the weights, the code, and the dataset. Without all 3 the model is not reproducible. Could make the argument that the code and data are sufficient though. reply mirsadm 10 hours agorootparentNo one will release the dataset because we all know it is gathered through dodgy means. reply notyourwork 10 hours agorootparentAnd likely massaged by the CCP. reply chgs 9 hours agorootparentDeep mind has one set of censorship, OpenAI another, anything musk does a third It’s all “massaged” reply redcobra762 9 hours agorootparentI don’t think “Taiwan is China” is the same kind of massaging as not telling people how to make napalm… What a weird thing to equate, though! reply ossobuco 6 hours agorootparentBut Taiwan is China. Even the USA acknowledge that. reply redcobra762 2 hours agorootparentNo, they’ve acknowledged the policy. The US has not “agreed” with the policy, which is different. Also, as someone in the US I can very safely say that Taiwan is not China and have no concern for my safety. reply ossobuco 1 hour agorootparent> Also, as someone in the US I can very safely say that Taiwan is not China and have no concern for my safety. But you will be arrested if you stage a peaceful pro-Palestine protest asking for an end to the ongoing genocide. Or even worse, if you say that Palestine is not Israel. reply redcobra762 19 minutes agorootparentNo I won't, in either case. Watch: Palestine is not Israel. Look, nobody swooping in from the rafters to lock me up. I have zero worries the government will do anything at all about my making this claim. And if staging peaceful pro-Palestine protests result in arrests, what happened here? https://en.wikipedia.org/wiki/National_March_on_Washington:_... or here? https://en.wikipedia.org/wiki/March_on_Washington_for_Gaza Or does that not fit your narrative? foxglacier 9 hours agorootparentprevIt is though. Western AI tries to hide information like that with the justification of safety as well as things that might be offensive to current popular beliefs. Chinese AI presumably says Taiwan is China to help get more people on side for a possible future invasion. Propaganda does work look at how many people think Donbas is still Ukraine and Israel is still Palestine. reply redcobra762 9 hours agorootparentThe difference is that in China the info isn’t available without use of Western content, due to the totalitarian control over media, whereas in the West, information is pretty trivially available, even if the big companies keep it off of their platforms. And sure ignorance is prevalent, but even GPT4 will tell me Donbas is still Ukraine, for instance. What a strange example to use, though! reply ossobuco 4 hours agorootparentIf western governments are so tolerant and permissive with information, I wonder why can't I access RT in Europe? reply redcobra762 2 hours agorootparentBecause you don’t know how to use a Western VPN? reply ossobuco 1 hour agorootparentIsn't it the same in China then? They can also use VPNs reply redcobra762 10 minutes agorootparent...and where, pray tell, do they VPN into? reedciccio 10 hours agorootparentprevThat's what the Open Source AI Definition states https://opensource.org/ai In any case, Deepseek like Llama fail much before hitting that new definition. Both have licenses containing restrictions on field of use and discrimination of users. Their license will never be approved as Open Source. reply mythz 9 hours agorootparentprevThis nitpicking is pointless. DeepSeek's gifts to the world of its open weights, public research and OSS code of its SOTA models are all any reasonable person should expect given no organization is going to release their dataset and open themselves up to criticism and legal exposure. You shouldn't expect to any to see datasets behind any SOTA models until they're able to be synthetically generated from larger models. Models only trained on sanctioned \"public\" datasets are not going to perform as well which makes them a lot less interesting and practically useful. Yes it would be great for their to be open models containing original datasets and a working pipeline to recreate models from scratch. But when few people would even have the resources to train the models and the huge training costs just result in worse performing models, it's only academically interesting to a few research labs. Open model releases should be celebrated, not criticized with unreasonable nitpicking and expectations that serves no useful purpose other than discouraging future open releases. When the norm is for Open Models to include their datasets, we can start criticizing those that don't, but until then be gracious that they're contributing anything at all. reply jimjimwii 8 hours agorootparentTerminology exists for a reason. Doubly so for well-established terms of art that pertain to licensing and contract law. They could have used \"open wights\" which would have conveyed the company's desired intent just as well as \"open source\", but without the ambiguity. They deliberately chose to misuse a well established term instead. I applaud and thank deepseek for opening their weights, but i absolutely condemn them and others (e.g Facebook) for their deliberate and continued misuse of the term. I and others like me will continue to raise this point as long as we are active in this field, so expect to see this criticism for decades. Hopefully one of these companies losses a lawsuit due to these shenanigans. Perhaps then they wouldn't misuse these terms so brazenly. reply mythz 7 hours agorootparent> i absolutely condemn them and others (e.g Facebook) for their deliberate and continued misuse of the term This is the kind of inconsequential nitpicking diatribe I'm referring to. When has \"open data\" ever meant Open Source? > They deliberately chose to misuse a well established term instead. Their model weights as well as their repositories containing their technical papers and any source code are published under an OSS MIT license, which is the reason why initiatives like this looking to reproduce R1 are even possible. But no, we have to waste space in every open model release complaining that they must be condemned for continuing to use the same label the rest of the industry uses to describe their open models which are released under an OSS License as Open Source instead of using whatever preferred unused label you want them to use. reply jimjimwii 4 hours agorootparentWe're talking past each other at this point. I believe both our positions have been adequately presented. Cheers. reply redcobra762 9 hours agorootparentprev“Gifts to the world”? What a strange thing to say… reply brianstrimp 11 hours agorootparentprevOnly meaningful if code+data deterministically reproduce the weights. At that point, the weights are just the cached output. Which has value since it's costly to produce from code+data. reply frabcus 10 hours agorootparentI don't think it needs to be deterministic and if it isn't, having the data and code becomes even more important! Compilers generally aren't deterministic (see the reproduceable build movement), yet we still use their output binaries. reply blackeyeblitzar 10 hours agorootparentprevThe only one I’ve see people talk about that shares all the components is OLMo (https://allenai.org/blog/olmo2) reply reedciccio 10 hours agorootparentThere are more, like the work by Eleuther AI and LLM360. reply htrp 7 hours agoprevThe hf team tweeted they'd be doing this over the weekend. I guess now it's an official project with headcount reply cadamsdotcom 11 hours agoprevExciting to see this being reproduced, loving the hyper-fast movement in open source! This is exactly why it is not “US vs China”, the battle is between heavily-capitalized Silicon Valley companies versus open source. Every believer in this tech owes DeepSeek some gratitude, but even they stand on shoulders of giants in the form of everyone else who pushed the frontier forward and chose to publish, rather than exploit, what they learned. reply raxxor 11 hours agoparentOh yes, I am firmly on Team China here because US companies got too greedy. Meta is an exception here though and they also propelled AI development massively. DeepSeek is awesome. Any AI task yet implemented in our business can be run from my local PC with just the smaller models. And my PC is fairly crappy to begin with. OpenAI looks quite silly with their \"we have to close everything\". reply manmal 11 hours agorootparentCan you elaborate which models you are using? I‘m running an R1 distilled Qwen coder with 32B Q4, and while it’s giving useful answers, it‘s quite slow on my M1 Max. Slow enough that I keep reaching for cloud models. reply raxxor 6 hours agorootparentNot on my machine currently, I use the 14b Q4 model I think, which delivers very good answers. I run a 4060 with 16gb memory and performance is quite good. I used the largest model that was recommended with this amount of VRAM, I think it was the 14b one. I do have some applications that process images, text and pdf files and I use smaller models for extracting embeddings. I think my system wouldn't be able to handle it with decent speed otherwise. I do run LLM on a M1 16gb macbook air and performance is surprisingly good. Not for image synthesis though and a PC with a dedicated GPU is still significantly faster with LLM responses as well. Haven't tried to run deepseek on the macbook yet. reply nejsjsjsbsb 9 hours agorootparentprevI'm on team open source. To me the exciting thing was ollama downloading the 7B and running it on a 5yo cheap lonovo and getting a token rate similar to the first release of ChatGPT. Running local on CPU opens so much possibilities for smart and privacy focused home devices that serve you. In my test it hallucinated confidently but my interest is in simple second brain like rag. \"Hey thingy, what is my schedule today?\" Need it to be a bit faster though as the thinking part adds a lot of latency. reply raxxor 5 hours agorootparentThe thinking is quite fascinating though, I love reading it. Especially when it notices something must be wrong. It will probably be very helpful to refine answer for itself and other models. It does add latency of course, but I still think that I could provide all AI needs of my company (industrial production) with a simple older off the shelf PC. My GPU is decently recent, but the smallest model of the series and otherwise the machine is a rusty bucket. I didn't test it thoroughly yet, but I have some invoices where I need to extract info and it did a perfect job until now. But I don't think there is any LLM yet that can do that without someone checking the output. reply blackeyeblitzar 10 hours agorootparentprevThe US companies got too greedy? How? They invented this entire space, literally. DeepSeek built their base models off Llama releases and OpenAI outputs (or so it’s thought), and while they added some optimizations on top, it seems like they’ve lied about the costs to produce their models by simply being vague about their base model and training data, and quoting the cost of their final training run. And then there’s all the dystopian propaganda baked into these models, which threatens to misinform users at scale based on a government driven agenda. Hard to be on that team, let alone firmly, knowing that it’s giving power to a dictatorial regime. reply wkat4242 9 hours agorootparentThe US models are also full of censorship. For example the US is much more sensitive to anything related to sexuality and here in Europe it's quite frustrating to deal with that censorship. reply infecto 6 hours agorootparentI think we will find that each region will have their own flair of censorship. The only reason it stands out more from a Chinese perspective is the requirement to have alignment with PRC/CCP rhetoric. reply wkat4242 58 minutes agorootparentYes that's what I mean. I wish all models were uncensored and it would just be up to the implementer to decide how to finetune on top of that. Save for the super crazy stuff of course. reply mvc 5 hours agorootparentprev> The US companies got too greedy? How? They invented this entire space, literally And when they thought they were the only game in town, they tried to corner the market in GPUs and lock out any users who can't pony up £200/mo. Reminds me of when the likes of Oracle and IBM had companies by the balls buying bigger and bigger servers and then Google came along and showed everyone how to do horizontal scaling of cheap hardware. reply raxxor 6 hours agorootparentprevThat was perhaps a bit too general, but aside from meta and Google they didn't share their research and tried to sell AI products as fast as possible and tried to lobby legislation to keep their head start. I would also include nvidia here, that has some moat through software integrations. I haven't tested deepseek for censorship yet, but they shared their release and even their input data. And in this case you could correct its shortcomings, so propaganda would be difficult. reply og_kalu 5 hours agorootparentprev>DeepSeek built their base models off Llama releases and OpenAI outputs (or so it’s thought) The first one is definitely not true and the 2nd one is not necessarily true in the way you imagine i.e crawls of the internet will have gpt chat logs now. reply timeon 9 hours agorootparentprev> DeepSeek built their base models off Llama releases and OpenAI outputs Those models are also trained on data that was ignoring licenses / copyrighted content. reply ithkuil 7 hours agorootparentThat's a problem for sure. But why would that argument play in favour china which would be even less constrained by licenses / copyright ? reply timeon 3 hours agorootparentI didn't mean it as favour for China. Just that this industry is unfortunately like that. reply jillesvangurp 11 hours agoparentprevChina is merely the largest of a wide array of entities that may not necessarily like the status quo of Silicon Valley being our tech overlords. There are plenty of places with bright people. Easy to say because a lot of them immigrate to California. But of course the places they come from (China, Europe, India, Russia etc.) have ambitions as well. You'll find natives of each of those in the likes of OpenAI, Google, Microsoft, etc. And quite often at executive levels even. Silicon Valley has mo moat other than money. It kind of runs on openness and freedom of movement of people. Companies constantly poach people from each other. And there's a constant movement of people (and knowledge) in and out of the area. Money is what attracts these people and keeps them there for a while. But of course that status quo was upset a little bit with VCs turning into penny pinching misers lately and lockdowns proving (to them) that it was cheaper to host your tech teams remotely. Which means knowledge is now more distributed than it used to be. So, it's not surprising that people outside of Silicon Valley are not waiting patiently for OpenAI to do whatever it is they are doing in between having moral existential crises, trying to oust their CEO, pontificating about AGIs, etc. They are taking things into their own hands. The brute force / VC funding driven approach that OpenAI has used yielded massive results in the last few years. But ever since Meta opensourced their models, OSS models and optimizations have been catching up. On a hardware resource usage basis, these models started to outperform their bigger peers last year and now the game is up for the training process as well. Meaning they get better results for the same money. A major hurdle here was the model training process. Which the Chinese seem to have proven can be massively optimized as well. Cutting cost by a few orders of magnitude is a big deal. And at the same time doing the same thing at larger scale (aka. throwing more money at the problem) seems to have diminishing returns. Until that changes, that means the playing field has somewhat leveled now. That's a good thing. reply LunaSea 10 hours agoparentprev> This is exactly why it is not “US vs China”, the battle is between heavily-capitalized Silicon Valley companies versus open source. Ah yes the \"open source\" code that was not released by the DeepSeek team and the tens of thousands of professional grade GPUs that were contributed by the \"community\". DeepSeek is based on Llama which was produced by ... Meta. reply Palmik 9 hours agorootparentDeepSeek v3/r1 isn't based on llama architecture. It uniquely combines and contributes several novel approaches. Meta never released a mixture of expert model (they failed to train a good one, according to reliable rumors). And MoE is just one of few ingredients that make DeepSeek v3/R1 interesting and good. reply bgnn 10 hours agoparentprevAnd in this case it's heavily capitalized open source, which scares the techbros. reply MagicMoonlight 11 hours agoparentprevI like that it’s open source, but ultimately it is china so we can’t trust it. It’s trivial to implement bias in models (hence the no-no filters in chatgpt) so if they’re smart they’ll do what they do with tiktok and make the answers different for their rivals. reply esperent 10 hours agorootparentThe thing about open source is that you don't need to trust it. They shared their methodology, so if they are legit, someone else will reproduce what they did very quickly. I expect Meta, Amazon, Google, and Anthropic are on the case right now. From this list, the only one I trust any more than I trust Deepseek is Anthropic. The other three have shown they'll instantly bend the knee to whomever is in power, and that's exactly the same thing people are worried that Deepseek is doing. Last year I would have said I trusted American companies more than Chinese. But last year feels like a long time ago. reply greenie_beans 5 hours agorootparent> The other three have shown they'll instantly bend the knee to whomever is in power, and that's exactly the same thing people are worried that Deepseek is doing. 100% reply eagleislandsong 10 hours agorootparentprev> the only one I trust any more than I trust Deepseek is Anthropic Will you kindly explain why? Thank you. reply frabcus 10 hours agorootparentAnthropic was the only company in that list not to have paid for their CEO or founder to attend the inaugeration of the current ruler of the exectuive branch of the US a week ago. reply eagleislandsong 9 hours agorootparentThat makes sense, but why trust them more than DeepSeek? reply esperent 9 hours agorootparentBecause, from my non-expert but reasonably well informed understanding of world affairs, there's a very high chance of a Chinese company in an important area like AI having to bend the knee to Xi Jinping pretty much exactly as the American companies are doing with Trump. reply eagleislandsong 7 hours agorootparentThat's a reasonable take. Thanks for elucidating. reply typ 10 hours agoparentprevOpen source has pragmatic merits and I love the culture. But I don't like associating it with a moral high ground because it doesn't charge you money. By this standard, we should also ask Intel/AMD to open-source their CPUs, video game studios to open-source their code and artifacts, and Google/Amazon for their search engine and infrastructure. Not all business sectors can afford to sustain with the Open Source model. reply mordae 9 hours agorootparent> By this standard, we should also ask Intel/AMD to open-source their CPUs, video game studios to open-source their code and artifacts, and Google/Amazon for their search engine and infrastructure. You aren't? You're weird. reply nejsjsjsbsb 8 hours agorootparentYeah. Actually if those companies did I suspect it wouldn't hurt them one iota. reply nejsjsjsbsb 8 hours agorootparentprevThe freedom is mostly not about the money. The 3D model Benchy was free, while being not free as people found out. Luckily the copyright owners treated it like people were free and free for now... but that could change. reply anthk 5 hours agorootparentprevTalos Workstations Libre engines such as the ones from https://osgameclones.com Godot Use anything else, there are tons of libre search engines reply vinni2 7 hours agoprevI wonder how long it takes to reproduce it and would having access to latest GPUs speed it up? reply Babawomba 5 hours agoprevsuper cool to see an open initiative like this—love the idea of replicating DeepSeek-R1 in a transparent way. I do like the idea of making these reasoning techniques accessible to everyone. If they really manage to replicate the results of DeepSeek-R1, especially on a smaller budget, that’s a huge win for open-source AI. I’m all for projects that push innovation and share the process with others, even if it’s messy. But yeah—lots of hurdles. They might hit a wall because they don’t have DeepSeek’s original datasets. reply amelius 9 hours agoprevAre there any other groups trying this? reply fl4tul4 9 hours agoprevIs OpenAI open as of yesterday? reply vinni2 11 hours agoprev [–] Where is the evaluation numbers? without it you can’t call it reproduction. reply nejsjsjsbsb 8 hours agoparent [–] It is an announcement. A kick-starter if you like. reply vinni2 7 hours agorootparent [–] Then the title is misleading reply the_duke 7 hours agorootparent [–] Actually reading the post would have quickly cleared up any confusion... reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Open-R1 is an initiative to replicate DeepSeek-R1, a reasoning model comparable to OpenAI's o1, focusing on transparency and open-source collaboration.",
      "The project seeks to recreate DeepSeek-R1's datasets and training pipeline, which are currently undisclosed, using reinforcement learning (RL) without human supervision.",
      "Open-R1 encourages community contributions to expand the model's applications beyond mathematics, including fields like coding and medicine."
    ],
    "commentSummary": [
      "Open-R1 is an initiative aimed at recreating the DeepSeek-R1 model using open-source principles, though it is not yet an actual model.",
      "The discussion emphasizes the challenges and potential benefits of reproducing AI models on a limited budget, as well as the impact of AI on education and broader societal implications.",
      "The conversation also highlights the excitement surrounding technological advancements and the role of the open-source movement in making AI more accessible to a wider audience."
    ],
    "points": 376,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1738046447
  },
  {
    "id": 42845017,
    "title": "The future of Rebble",
    "originLink": "https://rebble.io/2025/01/27/the-future-of-rebble.html",
    "originBody": "https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;pebble",
    "commentLink": "https://news.ycombinator.com/item?id=42845017",
    "commentBody": "The future of Rebble (rebble.io)374 points by will0 22 hours agohidepastfavorite25 comments MostlyStable 22 hours agoSome commenters mentioned that the e-ink screen (and the accompanying battery life) was one reason why the Pebble is so beloved, which reminded me of the Basis Peak, which was primarily a health tracker watch with some (very limited) smart functionality (mostly just some notifications, if I recall), that also had an e-ink screen and a nearly 1 week battery life and had a sort of similar trajectory: Bought by Intel, then killed two years later after a battery related recall issue. It was, in my opinion, by far the best fitness tracker watch ever, and remains so to this day. Not so much because of it's actual features (which were relatively standard), but the software paradigm of simple yet effective exercise gamification that helped encourage exercise habit formation. 8 years later and I still miss it. reply ClassyJacket 21 hours agoparentPebble had an LCD screen. It didn't use Eink at all. You can look this up, it was an LCD display manufactured by Sharp. reply _emacsomancer_ 16 hours agorootparentA transflective LCD[0] though (that they call \"e-paper\"), which, in user-facing terms, is similar to e-ink. [0]: https://en.wikipedia.org/wiki/Transflective_liquid-crystal_d... reply orangeyjuicey 7 hours agorootparentWell now, users would notice if the screen has to constantly flicker like their Kindle instead of looking like a colour Casio screen. Meaningful user-facing distinction there. reply _emacsomancer_ 2 hours agorootparentAh, that's fair enough. I was thinking of the positive e-ink-like properties, but this is a good point. \"e-paper\" in fact makes more sense here. reply MostlyStable 20 hours agorootparentprevI never had one, but doing some quick googling, it looks like at least some models had an e-ink display. Certainly several commenters seemed to think it did. reply will0 20 hours agorootparentIt was a transflective memory LCD, which Pebble marketed as \"E-Paper\" (not the same as E-Ink) reply Groxx 16 hours agorootparentAn excellent one at that. It boggles my mind that others didn't follow suit. I've got a Bangle.js 2 at the moment and while I mostly like it, the screen is nowhere near as nice. Transflective is still very obviously what I want in a watch though, even the best oleds don't even come close in visibility. reply terramex 8 hours agorootparent> It boggles my mind that others didn't follow suit. Most fitness smartwatches (Garmin, Coros, Wahoo) used the same display technology for years, called it MIP displays. Nowadays they are switching to OLEDs. https://garminrumors.com/amoled-vs-mip-in-garmin-devices-a-d... reply Groxx 29 minutes agorootparentSomewhat! The vast majority I've seen are black and white, with a color layer on top that is disabled when in \"low power / sunlight readable\" modes. And many of the smartwatch-focused devices (rather than Garmin's super pricey and gigantic \"hiking for days without a phone so it solar charges and has gps and...\" watches) only use OLED, even if the brand has MIP screens in some other product lines. I haven't seen anything even close to what pebble's color watches did. Banglejs is by far the closest, with 6 colors and a much more muted screen in general. reply yjftsjthsd-h 20 hours agorootparentprevNo, it had an \"epaper\" display that visually looked a lot like eink but was a different technology reply dang 22 hours agoprevWell now we have a conundrum the top 3 posts on the frontpage are all about this! The submitted title of this post was \"Google has open-sourced the pebble smartwatch operating system\" but it actually points to https://rebble.io/2025/01/27/the-future-of-rebble.html, so I've changed the title and moved the comments from here to https://news.ycombinator.com/item?id=42845070, which has the Google announcement. The 3 threads are: We're bringing Pebble back https://news.ycombinator.com/item?id=42845091 Google open-sources the Pebble OS https://news.ycombinator.com/item?id=42845070 The future of Rebble https://news.ycombinator.com/item?id=42845017 ( Is Rebble making hardware as well as RePebble? Potentially! We have always talked about it, but some of the announcements today have thrown us for a loop so plans are still being discussed. > Is RePebble also open source and community owned like Rebble? Remains to be seen, but if I had to guess it's unlikely. reply subarctic 19 hours agoparentprevThere's also the Pebble watch which I believe was the thing that this all comes from originally? Just piecing this together from all the HN posts now. https://en.wikipedia.org/wiki/Pebble_(watch) reply TheMagicHorsey 22 hours agoprevI'm curious if anyone has tried one of the newer e-ink smart watches you see on Alibaba, which use ESP-32 or other low-power SOCs. I saw one recently at a meet-up and the guy who was wearing claimed it was completely open-source and he could run whatever he wanted to on it. It did not have heartrate monitoring or anything other than clock on it, as far as I could tell. reply throwup238 21 hours agoparentThe original was Watchy: https://watchy.sqfmi.com/ Not only can you run anything you want on it but it supports the Arduino IDE and Micropython. I assume all of the Alibaba ones are based off of Watchy. reply bigiain 19 hours agorootparentI have one of this still in it's box unassembled. I like the idea. But I've been wearing mechanical watches for about a decade now... reply Avamander 20 hours agoparentprevThe biggest issue is the software. It's mostly abysmal. The best effort I've seen so far has been InfiniTime for the PineTime. It's very difficult without full-time employees working on improving things. reply andyjohnson0 22 hours agoparentprevWhat did it look like? All the ones I've seen have been pretty ugly plastic cases, etc. reply poisonborz 20 hours agoprev [–] > will accelerate our efforts to produce new hardware Don't do that. Don't give me hope... reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussion highlights nostalgia for Pebble smartwatches, appreciated for their e-ink-like screens and long battery life, and questions why similar technology hasn't been more widely adopted.",
      "There is interest in the potential for new hardware from Rebble, a community-driven project, and the open-source nature of related smartwatch projects.",
      "Alternatives like Watchy and PineTime are mentioned, with users noting the software challenges faced in the open-source smartwatch space."
    ],
    "points": 374,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1738008202
  },
  {
    "id": 42844619,
    "title": "The Alpha Myth: How captive wolves led us astray",
    "originLink": "https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"anthonydavidadams.substack.com\",cType: 'non-interactive',cRay: '90933c5f280a9429',cH: 'l7DGroOFyK3uSX2PsEp0ygApoTdnMe1.OqkEWDPogAY-1738090936-1.2.1.1-W.enVnGnk3K33fRaKIT3JZQK17EKDmF60HiJmHi8Zf8KoeRjOjYS0j4DWshRpqwc',cUPMDTk: \"\\/p\\/the-alpha-myth-how-captive-wolves?__cf_chl_tk=pI8T6p3gcRtKA.X_1n5ecXZs61ApTFE1id.eF8CeSWw-1738090936-1.0.1.1-b5hGg8wRe1FkNTrz80DCZcmDm3wYVUNiP4DWUfYA0Lc\",cFPWv: 'g',cITimeS: '1738090936',cTTimeMs: '1000',cMTimeMs: '120000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/the-alpha-myth-how-captive-wolves?__cf_chl_f_tk=pI8T6p3gcRtKA.X_1n5ecXZs61ApTFE1id.eF8CeSWw-1738090936-1.0.1.1-b5hGg8wRe1FkNTrz80DCZcmDm3wYVUNiP4DWUfYA0Lc\",md: \"NTk.DjMGd9mOA49TTQ2TO91TGm39qCHFjyQEcuRaUoo-1738090936-1.2.1.1-Au_VFo0BBa9NPF1YQUCI8uPlpB5nOHTbrJC9fLmFs_Lc2.CT0N7lIdIxaVCER3KcyKhmC.CNVWGPxaqMWJmuJEeR4HPPGAXjyKZaV_GV6HZsXni4bcePW_dQxUm8gVE1yXpFYAAHj3lWhTJtrj5e5iQeqFzUjdote_V6VTVmvz35YAFo_pVWj749TbJn.vpcM54ftM.qcvTPdZ1ArBxUhyoSNMzsdLuUpQwIW0yQ.iQYxPKl47BbMHVDzrrmCDiIAE2ZQBtbq3_Wtvw12ntiA1_g342tvEDDBDNKjMcV1Vxb7WImizhZquFZz2MvE6DPiy8YVZ0.4qYxodWl.sLolj3Zd5JhyIeo1FDAFm150.YL1jbeo.CmuSoc25Uh_3VqgX5OC9F2oZe5iq_jcG9t9zRMzSs6f3mbZ0eiTM_2jjZLCDwUrJBXTHyU_v1qiVEbBNDSocjA84j22bLaK5myVEUpkfGfxnFkmxCH3mcLGFC7z2RoAHUraGvYmZkJ90I8M4C5W.KYbBELAq1JI0NknYxnr3P8atUOFfgbsftgmQPJELaA5jifuov2JKOhHUFM6BzN1NIHXkDqjCfnJW657UddI1dgH3MWY8PG8UXsrx2NAZEJhoc4UYN0K6ixnGRCtaDsuL5vw_SB2QCcVRGtX87GBAk1izdDuXySlrfuYzjW9p1IPoT0Irj0KOLhh3A1Avk_cXK89AVqlbFp9JbSU5qdbNMLKCCfjcwQaUAL1T9J9GNBBh9iBEHgyGVzVK1W5v5l7_pJGYhvmVVFjFKcaFKY7ZHOeB4xkUh1.xfy.9cnSCFlyU60fE7UBKwAJig7DHM3TxwuH7UVH18.2QQkF8xkNlhpG9.W4yQyfMGTMhJxM8i8chpPPXB5MOEIhqibW4i7c2thWeFjiceEGLbp9sndsJ_v0QqerivSf86aqL8aEEeymEPM9ix0pS3m6I9flNbdw9KShwaFPNwDYUCeuvV9Lo147iTRg735wU3wOC5aoQSnbnOgSk0zkFbY9AUEdNb6qv3.rZBOTn3lfZQ80isDDe6UUpzcOt.2DzQYxE12sTigR.tn3C7Yi_tHlgzuebpqjs1SmR4Pr1GvDeOIHgHmKAhNY6AF01Sk4k0LwkYdYfbkUIm4lvqb8tJ2nbOKQpSTVn1tG42ksB_7MhWsS8Ibukp1zbfzTHGcz6Ms5fh1pAREh_bMLUQFLUf9Vu8UazbE2uM22HxFkyaXwD1qENP_FyF2JtrbXOV7R8OuKt8Lw32M_Cy0A4SV8y3VGQ84xyXON8GEPtqdrwoit2RFm.RwcXVfY.kYbz5VIOtmXmBDIxx.gtcsJmmbaCUodHq2iZE4tusjVjP9ae69gBenoDCdEnxMMLxqze4fd5.FIy7ozdyKaGMzNI0XUD8kSpOqoVhEfyN0GUoBaP6c9Uf4wj_N2f89XPCtpCT7JrvF_THcxyIL0EgwArVnqDZ1ghjAwMPgs5UPTZr56ttFiMk83jxESc6aAeEDuiyt.C34ag0m_gh87ABgt32dau1dKWGOaRBoKMdd5A9qhWzQUPVPQsef8FZbY2W2SHahyAecRm8RVf7Nft6FvqXNz2LbPWXD2kJF4yUCHO4kiRcWV4jZyn1UHKlIzRtFYLhKO4JukCWshzrzn0emCdckVsFEYsFqbD0bsOZUClzrOREb6L5rZLSm2vr37izFeWSVLO8F54N9HmnVew3Rrn_ai3M1Ndobk2tFrhpMfh5q5ZsJR9UBml2GjANSInOsN5ZM3qAclGcBsUVSnZu0M1jHW3Dib48aUECcQlVkbJaUWpi7gd.yT8bMf0HqqI8yvBpPNMUzUTNobqzuLC2ThimNptsoEiwx\",mdrd: \"aFfFIVjiAbV3DWrzVGsybyOexKw91HB8tHYqVWrxRaM-1738090936-1.2.1.1-I._HA9pCDHq0rX8W7kYGJeDP58O2Qw1E4.UnrYMAp04.hmWL39CufdUbGP.x1L7MIPHagvADuwtApDs4IzTTF88GWkiIHBlzgE6gO27pCmQBdl7Wgq9fk4yGrI7mjwq_Q8t754C7ZTxRBOEwf6vY3le865E5TmHnndXGnqoLBDixza2RrNbIJ59wXkBipTtv9t9dRUWJwAgD7X6L1XdGAXdq.5hIWuGLV3Tpgs4Cap_RNC.Z9NXYY7OoukX_3NmPHWgiLw..C5kPjTpqScn2469g5jFeuQdTizBZPZtPtYRIE8DniEtg.5pSER8MEiD8EpEItSO4ya3w3GYydxA7nm8LtfnQwrotTf7nHx78_8AqqiiQu3Zc46suYlzab7PNB4IlWwsEdJyhE35dH7Zq5iP_a7dUk3ygcvZDwKFejJ1glIf.UxyepYz49ym2k89syaQtZiHC_8U551czKUfL3i2N3AYk1PoC2jHv6_x3pPSURLtli5Ezo3GJFdpMVhTKd4NTOKFR.TnErsL2s_Z7G9oRYZBLmo6erbsa3SuQKRiXUKktsxNOiktXbuyG2BVpIEMbS_yQa0RroMgtVwNPOlVb1GUs_ImN0Y.y3mE6UKuN6MAnXejfW6oJ7yTP.kPQvIjk7BBELPmCNVyYJAxZbw7wKIib.2BpWF3vK7m_NvO6pJpWq70QNQlgdLNa4k8wAt8nAfTD8CePnSR8DcYDwxeod02OJlQi8DIslrjl8QmXnNPjo4UKFIQo6Uy8.s469WQbHUXrKOCZAhdplTZyoFbrACMyKiWxLFxbjCfxwMTkFlH2NBhqxSbzUntSBOVU_NjYhpzHkvurOdHd5.32_rbPWRgB4IH54OlaWRvs5aF4kdywhyTStTeMmiRCRXIi7WQBXnLTe60O62sUy4Qx345Boy2TlGopCoLmgXAHLeHgPy8A26RBITIgH3VrdpZzy2tGdrGpqKNo50a0TOgyR7Mjbg.kRBEp6MN9Fm3WW8cvDd_zYGkBnON5nCw3prtamjmhxXvAVvuDSho1eqJUqAxmIVs8m6MLdPKh0ZGbNmEmpkvNv4i.Uv_4V3zFvsUkjjzmalaXLlxEZFF7SrCXmtHUvTtu1KDph72xqoTAj_qgFxHEAkIz_LybR7nyZsTO_ilBPRIpS5t6aFsmiNqvxElriM3ts0.LJ0iJEaI17OZgJJlkAToktUQmfBQyracUj.d8_lJ3tcDvlicBF1AdSNZu76FPs4mXatjUFK8.iqtc.ojUwMi4gd0Z.5oK6e9Izf.mZL4WBBWShVA2dWjoQpBupYitcpM61V_xAvVwgTzbCm5pYuJAU9jgRKfICGUt7J4nIwSRye7bNAZMMTKa_Iy9LdoMxwJXmIHGwbNUgVccuLCHGXpjPLIsBI._N8AVDSLuKVjSeKx_e_2OXT9dh._b5Ijx0_SjciJitrfKy_zWhWpGaHU36yCY_YN7g96xEvHZOsshJ.byHkRrMYXHCnckX63XzLaY3_.FqS25FnkODhz5QFwkfIbxoQhZIjdySv0Gx5jDAhVZH_7IcDzdpjsmH3GfNxO1myDrdTQDgcLu3TsMBqX1aTkzTeIwebkSCKeM1dkPCVLcF.c7krzGKa5y5KDLqIoMs2.s28il9uwqKqafjMhJJS2_KliVR8VXFqJWSxMAouje6fSnTEZFnnJtk1Vgiywwl0SsiTfAL_TKouy3gu6A2VgGBI33V4zgwAYzxGHWns6GNnAJVVrz09pghv9G.WnMBf7JiCtz4PFmQjoO0w0Rg9N..39CnnK4CdtglG6.JPK2l.Q.Fo3IUQtV2Yfinh3k8cTcOTfcQxaRs1mSi8Vpq9Va7SGWgRfHrYpnP_oDwUfGqBbOjXdfQWtfsx8Aefd5Yy0mUhNottufNvrtkgPlaC5kGr58WBIDy36TMN3yb_bqkz1WFqQFonAZjdkvOewNyIQnbfzc7VUMexkBYLCa5ebmZtAeI5S2BzV8IDOvNZNAfuaIr8ntBqS57pNzf1bSjn0sS9eEf9uaXHHAydqya5CnMsyxYVd95ycbS3LTA.klIf9mpRXj1cFy1bLQLDiTBhY2PXZsshRCXySOk2O6bBr3fnGEEB_Et3YPPAZaUTIrcPJM.s5ENTVkGgurGDKfZkoMTLScsV8KBCq7rMt9ErVhbrq7S_Ri49J58sPcueEDKixkJ0fiIvLsqcdL55Msun7tc0S2KbCVtxd8rsNyXQiNh0hJzkeBQ_YCFBqcFYiSjYZRyV9RttqXFrhkvzFs76nwEOymdgoNZqhIng5EstivUBbnWkrQ9logTfU40QRdEMbYqYBYMmYPsRKSHH02Ql2rxec36iOe2eHYNSKMLt6ZPCz3_IKlu9g5jKnLRgplqGXApDqkx8.jRgqa41KRYBO1nn7wK1pXgyA904wFFMJHG6VriygJXB54wfkmFdO7OEBSqDCBVg\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=90933c5f280a9429';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== 1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length window._cf_chl_opt.cOgUHash.length).indexOf('?') !== 1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/the-alpha-myth-how-captive-wolves?__cf_chl_rt_tk=pI8T6p3gcRtKA.X_1n5ecXZs61ApTFE1id.eF8CeSWw-1738090936-1.0.1.1-b5hGg8wRe1FkNTrz80DCZcmDm3wYVUNiP4DWUfYA0Lc\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42844619",
    "commentBody": "The Alpha Myth: How captive wolves led us astray (anthonydavidadams.substack.com)354 points by ada1981 23 hours agohidepastfavorite311 comments mattgreenrocks 4 hours agoI find it fascinating that some users here seem to be attached to the concept of an alpha male, reality be damned. It’s clear that this meets some emotional/psychological need for them. I regard it as a dangerous mind virus. It is hard to rid yourself of it even if you see it is bullshit and harming you because it requires a substantial change in how you see your life. I think that’s because it is some hard truths (“people like powerful people”) mixed in with some cognitive distortions (“dominance is the primary mediator of all interactions”). The lack of a true definition means there is no objective measure of what it is, which makes it impossible to feel secure in. Thus, the endless arguing about whether behavior is really “alpha” or “beta.” There are good bits there, such as not letting yourself be pushed around. But that doesn’t make it all good or true. I really think the whole artifice is a cop-out for actually doing the work of individuation and self-development because you outsource all the hard work of deciding what you want to be and the temporary, self-imposed emotional exile it takes to actually discern that to following guidelines ultimately given to you by other people. You’re still not crafting your own life, you’re just changing who you get your life script from. It exploits psychological vulnerabilities in people and traps them: if you worry about your alpha/beta status, then you’ll be the type of person who wants to fix it, but there isn’t a fix because your perspective is the issue. Hope that was helpful for someone. reply UniverseHacker 2 hours agoparentA lot of men are terrified of the idea of not seeming masculine enough and what it would mean for them romantically, socially, and professionally which is why this is appealing as an idea. Supposed “alpha behaviors” are seemingly identical to toddler behaviors to me. I think it’s ironically non-masculine to be so terrified of how you appear that you play act as basically a belligerent child incapable of compromise, teamwork, or navigating disagreements with dignity instead of being yourself. The ancient stoics had a much healthier, and more useful idea of masculinity IMO that still lets you be strong and not pushed around by others inappropriately. reply InsideOutSanta 1 hour agorootparentYes. It always struck me as incredibly insecure to make thinking of how one appears to others one's whole identity. reply dspillett 3 hours agoparentprevI think in part it is that the flawed study might actually be relevant to them because of the flaw. It studied captive wolves in an unnaturally constrained (and therefore competitive) environment, and that is how some people feel a lot of the time. Trying to be the alpha in such a situation, so they can look down at the others and feel better, instead of doing anything to try actually improve the world, is an unhealthy reaction, but one I can at least understand. Trying to be an “alpha” irrespective and create that sort of situation for others, on the other hand, is rather despicable. Another factor in the longevity of alpha/beta/other designations is that they don't only come from that study (and those that followed it, or it referenced). The terms were used elsewhere both in science and fiction (and, specifically, in well known science fiction such as Huxley's “Brave New World” which pre-dates the publication of Schenkel's infamous wolf study by a decade and a half). reply crawfordcomeaux 31 minutes agorootparentYes! We're all captive in capitalist, imperial, colonial patriarchal systems that have wrapped the globe. reply ineptech 1 hour agoparentprevWell put, but the countermeme (\"Hey did you hear that the alpha wolf thing was bullshit?\") seems to be as popular or moreso than what it's displacing. I did some light googling yesterday to see if I could find any references of \"alpha wolf\" before the 40s, and all I could find was page after page of articles exactly like TFA. I think the same reasoning applies it is vague and easy to apply and people want it to be true just for different people. reply indoordin0saur 45 minutes agorootparentThis is my thought. I have no attachment to the idea, I've been comfortably and happily married for years. But dismissing the idea of \"an alpha\" among animals seems stupid based on one study. Isn't this the case among gorillas, lions, etc? And it apparently is the truth even among wolves in certain circumstances as the study points out. The OP should apply the same logic to interrogate his own motivated reasoning. reply neves 28 minutes agorootparentprevMay be, but I never heard about it till I read this article in HN. Sincerely, I didn't even know that alpha-male was related to wolves :-) reply otikik 3 hours agoparentprevThis was very well put. Thanks. reply api 2 hours agoparentprev> I find it fascinating that some users here seem to be attached to the concept of an alpha male, reality be damned. It’s clear that this meets some emotional/psychological need for them. Grifters and people selling right-wing politics have figured out how to market to male insecurity. They're selling politicians of course but also quack supplements, self-help nonsense, masculinity gurus, hilarious \"boot camps\" where you spend tens of thousands to have some dumbass yell at you, etc. All that stuff will leave you still insecure, and with less money. Andrew Tate is probably the undisputed master of the alpha male grift. He's known as an abuser of women, and he is, but really men are his main marks. In a way part of his grift is to make his marks utterly repulsive to most women, keeping them alone and insecure and customers. reply sfink 29 minutes agorootparentSure, but don't lose sight of the fact that female insecurity has been leveraged and marketed to as well, for pretty much forever. Fashion, makeup, beauty standards flip through a magazine and look at the ads, and think about what the ads are suggesting the reader needs or would benefit from, and further think about whether the reader will be more or less insecure afterwards. (And male insecurity has always been a target. Don't let that bully kick sand at you at the beach! But I agree that the targeting has recently been vastly more tuned and optimized and the most egregious forms have become socially acceptable.) reply Terr_ 23 hours agoprevI often like to sum it up as: \"Thinking gangs observed in prison represent human households.\" This goes along with an imagined mockumentary, narrated by a David Attenborough figure, ex: \"In the living room, a confrontation is brewing. The Big Boss has informed the bottom-tier Henchmen Homies that it is time for lights-out, but they are not ready to leave the television and they resent the privileges of the older Made Man, who is permitted to remain. This is a problem for the Big Boss, since directly punishing them might undermine his claim to control the group as a whole. Fortunately for him, his Enforcer is arriving from the kitchen, attracted by the noise...\" reply reasonableout 9 hours agoparentWhen I was younger I made a switch from IT to teaching at elementary. One of the teachers that guided me, hinted to watch series about a dog trainer to see how he takes the lead, what happens between those. Although I respect that man, that really didn't resonate with me, realizing that I wouldn't want to learn that way; so I don't want to teach that way. This article explains a lot better how I feel about this than I was able to 15 years ago. reply curiousllama 3 hours agorootparentIf it makes you feel better, I’ve heard similar advice to manage executives reply pmontra 9 hours agorootparentprevIf I may ask, how did you teach back then and how are you teaching now (maybe in the same way)? reply ANewFormation 6 hours agorootparentI'm also curious. When I first taught some younger kids I went in thinking they were like little people... that did not work well. When I instead started to treat them as glorified chimps, everything went better for them and for me. In the meantime I'm still somehow expecting that my children will behave like little people... Haha, humans really suck at learning, even from our own experiences. reply sfink 17 minutes agorootparentInteresting. In my dealings with younger (and older) kids, thinking of them as little people seems to work well. Little, fully complete people, with many parts dramatically simpler (and more straightforward) than adults. But then, I also think of adults as glorified chimps. Many parts are somewhat more complex though if you look closely enough, you'll see that much of the complexity is more of a veneer than anything deep. Basic human nature shows through more with kids. Much of human nature is animal nature. Growing up seems to be mostly about setting those impulses against each other and delaying, suppressing, and avoiding the manifestations. In the process, we convince ourselves that we've had earth-shaking fundamental realizations about Truth and Beauty and Morality that lesser beings like children could never comprehend, as we go about our day with a sippy cup of caffeine in one hand, capturing the value that others have created as we lie and deceive to advance our supposedly noble goals, shoulder to shoulder with the rest of the herd. reply bombcar 3 hours agorootparentprevThe key is to realize that they act and react like little chimps, but they are growing into people. It's a really hard balancing act but with the chimp framework in place, the human development can flourish. reply TremendousJudge 2 hours agorootparentprevI have no idea how to treat chimps, let alone glorified chimps. What does this mean in practical terms? reply iteria 2 hours agorootparentI have a child and I'll be honest it was only when she was like 5 that I'd say that she was more complex than a dog. Small children cannot be reasoned with, only bribed and given sufficient motivation or demotivation via a carefully balanced set of prizes and punishments. After that she could be reasoned with, but had reduced ability to keep to her intention long term due to lack of experience and inability to imagine outcomes, so a lot of teaching an early elementary schooler involves setting up experiments to demonstrate the wisdom or lack of wisdom in particular actions. At least in my parenting. I imagine this is what is meant. reply HPsquared 22 hours agoparentprevMaybe not all households, but some. And schools bear more than a passing resemblance to prisons. reply ninalanyon 9 hours agorootparent> schools bear more than a passing resemblance to prisons. The qualifier some needs to be applied to that too. None of the schools I have any personal knowledge of in the UK or Norway bear (or bore in the case of my own childhood) the slightest resemblance to a prison. Well, there was one. I accompanied a colleague in the US when he picked up his daughter at the end of the school day, I was astonished to find that he had to present himself at a reception desk that was guarding locked doors that were the only way in and out. reply graemep 8 hours agorootparentI think you are taking it too literally. It is not about having bars and armed guards, it is about having a strong fixed hierarchy, fixed routines, rules far stricter than society outside, punishments such as being put in isolation etc. A lot of this applies to some other highly disciplined environments such as the military too, but it is not the norm for most of us (and the military volunteer to be there). The physical and security aspects of a school in the UK are not like a prison, but the social and discipline aspects are. More so at some schools than others and I think people who have only had contact with better schools do not realise how bad the bad ones can be. reply UniverseHacker 8 hours agorootparent> I think people who have only had contact with better schools do not realise how bad the bad ones can be I grew up in a poor rural area and my wife an upper class coastal community, both in the USA. It’s amazing to compare and contrast our public school experiences. I was just trying to survive a situation that was both psychologically and physically abusive, with public humiliation rituals, solitary confinement, and gangs of bullies that cornered and robbed people and violently punished social non conformity I still feel terror remembering it decades later. Even the lessons were nonsense propaganda science classes literally focused on creationism, history class on “states rights,” etc. She was in a playful relaxing environment that included regular upper class leisure activities like sailing, skiing, and horseback riding, and had well equipped science labs with teachers that were passionate about science. reply pixl97 6 hours agorootparentWhile I didn't go to more wealthy schools as a child, I grew up in a 'middle class to poorish' rural area in the midwest and moved to a poorish area in the south. I'll use your own words to describe the change in the experience >I was just trying to survive a situation that was both psychologically and physically abusive, with public humiliation rituals, solitary confinement, and gangs of bullies that cornered and robbed people and violently punished social non conformity I got the full 'the science teacher is the bully coach and believes the world was created 6000 years ago, and if you were one of the kids they liked you could commit crimes, and you were not liked then crimes could be committed against you' experience. reply UniverseHacker 3 hours agorootparentThat word for word describes what I experienced we could have been in the same class reply graemep 4 hours agorootparentprevAt least in the UK creationism is a rarity. The huge difference between good and bad schools exists though. My kids were home educated up to 16 and then went to a sixth form college (school for 16 to 18 year olds) that is pretty good and well funded with some really good teachers (the physics teachers are outstanding one that taught by older daughter now works for CERN, the head of department has a PhD, and they are all passionate about the subject). its not immune from league table pressure, the curse of education in the UK. reply cladopa 7 hours agorootparentprevIt is probably more about the difference between women and men experience growing up. Women usually do not face violence directly. They face other problems though. Don't believe for a second that growing up in an upper class environment does not have social dynamics. I grew up in one in Europe. In an upper class school there are bullies as bad as you could find in a lower class one, probably worse, because they have resources and connections to create harm. And sometimes their parents are worse than them. Do not idealise what you have not live. You could go skiing and they can choke with the ski poles. This happened to one of my friends because he criticised one guy. reply UniverseHacker 7 hours agorootparentYou are not understanding the extent of violence and abuse I am talking about, and how it was systematically led and encouraged by the teachers and staff. I’m not talking about just bullies being at the school or normal boys dominance and fighting of course that can happen anywhere. Despite going to a poor school, my family was well off, and I had personal experience with other environments including Boy Scouts, summer camps, etc. that still included normal social dynamics between boys with plenty of fighting, etc. but that is absolutely nothing like what I am describing above. reply eleveriven 11 hours agorootparentprevYet it interesting how much of what we consider \"human nature\" is just us reacting to the cages we put ourselves in? reply JumpCrisscross 11 hours agorootparent> interesting how much of what we consider \"human nature\" is just us reacting to the cages we put ourselves in? Or that we aren't dogs. Chimpanzees do have an alpha male who commonly achieves his role through violence. reply eleveriven 11 hours agorootparentBut their (Chimpanzees') social dynamics aren’t solely based on violence reply JumpCrisscross 10 hours agorootparent> their (Chimpanzees') social dynamics aren’t solely based on violence Correct. A book on Chimpanzees was connected to human nature by a reviewer in the Chicago Tribune, a 1999 Time article and the obviously totally-scientific pick-up book The Game [1]. [1] https://en.wikipedia.org/wiki/Alpha_and_beta_male reply psychoslave 7 hours agorootparentprevHumans can also exhibit behaviors closer to those of Bonobos. https://www.nature.com/articles/nature11128 reply b3lvedere 3 hours agorootparentprevEverywhere where you are unhappy and/or uncomfortable is a prison for yourself. It could be a football stadion where some weird guy has made you the mocking target. reply bestouff 22 hours agorootparentprevDepends on the country. reply bigiain 19 hours agorootparentYeah. Most countries manage to keep automatic weapons out of prisons. reply worik 17 hours agorootparent...and schools reply bigiain 13 hours agorootparent(that was the joke...) reply RobotToaster 7 hours agorootparentprev> And schools bear more than a passing resemblance to prisons. and some workplaces. Arguably many cities more resemble \"captivity\" than a natural environment. reply fhe 13 hours agoparentprevanother analogy would be: aliens abducting a bunch of humans and recreating the Stanford Prison Experiment and write up on human society based on that, which ends-up reshaping alien culture. reply zdw 4 hours agorootparentI would buy this sci-fi book. reply enragedcacti 3 hours agorootparentThe Mercy of Gods by James S.A. Corey is surprisingly close in concept. It's Book 1 of a trilogy by the authors of The Expanse. reply d0mine 12 hours agorootparentprevThe results are likely to be different (aliens won't try to bias the same way original researchers did). reply ninalanyon 9 hours agorootparentWhy not? Evolutionary pressure is likely to be the same elsewhere, why should we think that it will have different consequences? reply pineaux 7 hours agorootparentDont you love Assembly-theory? Evolution is everywhere reply ada1981 21 hours agoparentprevGenius concept. reply Gibbon1 19 hours agoparentprevI remember as a kid listening to adults talk about gender relations and thinking, you guys are talking about cattle and dogs. reply jojobas 21 hours agoparentprevGangs in prison represent gangs out of prison though. reply skyyler 20 hours agorootparentI don't think that's true, actually. Do you have any evidence that suggests prison gangs are similar to street gangs? reply pineaux 6 hours agorootparentI think they are, but causality is the other way around. A lot of gang originate from prison. Its really logical too, you put all the crime-graduates in a place together and you think they wont become friends and partners in crime? reply master-lincoln 4 hours agorootparent> Its really logical too, you put all the crime-graduates in a place together and you think they wont become friends and partners in crime? Maybe, but the question was a different one: are group dynamics in a gang in a prison the same as in a gang outside the prison? reply pixl97 6 hours agorootparentprevWhy wouldn't it be? You go to prison and have your rights stripped away and you pretty much have nothing. As much as gangs use violence they also provide protection and friendship. Now you get out of prison and you have that big old criminal stain on your record. You're having trouble surviving and making enough, but you remember your new gang friends tell you to find a guy when you're out and he can help you. Congratulations, the circle is complete. reply skyyler 6 hours agorootparentSo you feel like it's true, and have constructed a model in your mind to explain why. Very interesting! Thank you for sharing. reply llbeansandrice 2 hours agorootparentprev“Why not?” Isn’t evidence and there’s a burden of proof that goes with the above claim. reply ninetyninenine 16 hours agoparentprevThe flaw with the OP's article is that Alphas are not only observed in captive wolves. They are observed in MANY wild animal groups and even observed among humans. I mean the first thing that comes to mind is lions and gorillas. Also CEOs in human societies. I mean are they not alphas? Don't we structure our teams according to leaders? Alphas litter human society and organizations. I feel the author is blind not to be able to see the highly pyramidal structure of alphas that encompasses every facet of human culture. reply jrmg 14 hours agorootparentIt seems clear in all your replies here that you see ‘alpha’ as a synonym for ‘leader’, and as a somewhat relative term (like, my boss is an alpha to me, and a beta to their boss). Given your definition, of course it’s obvious that it apples to many facets of human society. Leaders and followers are all around us. But in popular discourse there’s also a sense of ‘alphas’ getting into their position by violence or assholery that I think your definition is missing. Even in scientific discourse, there is the idea that ‘alphas’ must gain their position by dominance not by other methods (prestige, age, etc). People use words in different ways, so in a way your definition is as good as any other, but (as with anything language related) you can’t really insist on it if others think it means something different and you want to have reasonable conversations with them (ironically, insisting on your definition seems like an attempt to assert dominance). Edit: removed some snark, sorry. But I’ll also add: the article uses the scientific usage I mentioned above: wild real wolf pack ‘leaders’ aren’t ‘alphas’ because they don’t maintain their position by dominating the other wolves. reply ninetyninenine 14 hours agorootparentThe claim that \"alpha\" is defined as requiring violence or assholery is incorrect, and this is backed by scientific studies, dictionary definitions, and leadership psychology research. Here’s why: 1. Official Dictionary Definitions Oxford English Dictionary (OED): Alpha (in social contexts): \"A person who assumes a dominant role in a particular group, especially one who is respected or influential.\" Oxford English Dictionary Alpha Definition Merriam-Webster Dictionary: Alpha (as in 'Alpha Male/Female'): \"The most dominant, powerful, or assertive person in a particular group.\" Merriam-Webster Alpha Definition Nowhere in these definitions does it state that an alpha must use violence or aggression to gain dominance. 2. Scientific Studies on Leadership & Alpha Behavior Study: Dominance Hierarchies in Social Animals Dugatkin, L. A. (1997). \"Winner and Loser Effects and the Structure of Dominance Hierarchies.\" Behavioral Ecology, 8(5), 583-587. \"Dominance hierarchies in animal and human groups are often established through social signaling, resource control, and cooperation rather than brute force.\" Dugatkin, 1997 Study: Leadership Without Aggression in the Animal Kingdom Smith, J. E. et al. (2016). \"Obligate Sociality Without Cooperation: Insights From Other Taxa.\" Behavioral Ecology, 27(1), 1-14. \"Alphas are often those who exhibit superior social intelligence, cooperation, and decision-making ability rather than reliance on aggressive behaviors.\" Smith et al., 2016 These studies explicitly state that alphas do NOT require violence to gain or maintain their status. 3. Leadership Psychology: Alphas Lead Without Coercion Goleman, D. (1995). \"Emotional Intelligence: Why It Can Matter More Than IQ.\" Bantam Books. \"True leaders, or ‘alphas’ in human social dynamics, are those who possess high emotional intelligence, resilience, and ability to influence without coercion.\" Goleman, 1995 Bass, B. M. (1990). \"From Transactional to Transformational Leadership: Learning to Share the Vision.\" Organizational Dynamics, 18(3), 19-31. \"Successful leadership is based on vision, respect, and strategy. Coercion and force are indicative of weak leadership rather than true dominance.\" Bass, 1990 These sources define \"alpha\" as a leader who influences others positively, not through aggression. 4. Debunking the Claim That \"Alpha\" = Violence L. David Mech (1999). \"Alpha Status, Dominance, and Division of Labor in Wolf Packs.\" \"The term 'alpha' is often misunderstood. In both animals and humans, successful leadership is based on intelligence, decision-making ability, and social bonding—not brute force.\" Mech, 1999 This refutes the claim that alpha = inherently violent. Final Conclusion: The Definition of \"Alpha\" Does NOT Require Violence Dictionaries define \"alpha\" as dominance through leadership, not necessarily aggression. Scientific studies show that dominance in social animals is often achieved through cooperation and intelligence. Leadership psychology research confirms that true human \"alphas\" lead through respect and influence, not assholery. The idea that \"alpha\" = \"violent dictator\" is a pop culture myth, not backed by science or formal definitions. reply glenstein 2 hours agorootparentI just checked your #2 Dugatkin. I am only able to access the abstract [0] (though interested in the full article as you appear to be quoting from part of it which is not publicly available). It does not at all represent itself as even taking a specific side on the meaning of \"alpha\" let alone testify to an academic consensus of any decisive proof one way or the other. It doesn't even have anything to do with the study itself at all. The line you quoted seems more like an aside. The study was apparently about a simulated game theoretic model, and even refers to the units in the simulation as \"combatants\". It is an attempt to model and simulate \"winner effects\" and \"loser effects\" in tandem over time. It's mostly interested in contrasting this approach with models that only deal with \"winner effects\" or \"loser effects\" but not their simultaneous dynamics. It ends with a modest expression of hope to spur future similar studies and is not even pretending to venture anything like a final definition of \"alpha\", let alone represent the full weight of academic consensus on that question. Citing it for that purpose feels like drive-by quote mining. 0. https://academic.oup.com/beheco/article-abstract/8/6/583/208... reply cutemonster 7 hours agorootparentprev> These sources define \"alpha\" as a leader who influences others positively, not through aggression. No? Look: Your Merriam-Webster quote (although I can't find it online) \"the most dominant, powerful, or assertive person\" That's not becoming a leader by being helpful and getting people's trust. It's instead dominance and assertiveness, that's something else. I think you mistakenly what to have the word \"alpha\" mean the same thing as \"leader\". But I think that's not how people in general look at these words. reply ellen364 10 hours agorootparentprevDugatkin, \"Winner and Loser Effects and the Structure of Dominance Hierarchies\" was an interesting read this morning. Thanks for the reference. The main thing I take away from the paper is that (in computer simulations) bigger winner effects and bigger loser effects lead to strongly defined hierarchies. Fewer subjects have ambiguous positions. I'm not sure the paper speaks much to which strategies (e.g. violence vs cooperation) are more common or effective. reply glenstein 2 hours agorootparentRight, it seems to entirely abstract away the nature of the interactions themselves, and is modeling how changes to position in hierarchy unfold over time with repeated wins or losses. It's not speaking to the nature of the interaction itself, and so not really taking sides on whether it does or doesn't involve such things as force, toughness, etc. reply JumpCrisscross 11 hours agorootparentprev> Official Dictionary Definitions Oxford English Dictionary (OED): Alpha (in social contexts): \"A person who assumes a dominant role in a particular group, especially one who is respected or influential.\" Do you have a link for this definition? I'm unable to find it or the one you attribute to Merriam-Webster on their websites. (Their websites are shit. I'm curious about the dates around those definitions.) reply oguz-ismail 14 hours agorootparentprevExcept all living things are inherently violent and knowing when to apply what amount of violence is part of being socially intelligent. reply ninetyninenine 13 hours agorootparentViolence is an effective strategy. One of many. It's not stable but it has been used successfully in human history. The thing is you can't have a society that's constantly violent all the time forever and ever. Those tend to self select via natural selection. You can have societies be temporarily violent like how Americans slaughtered and killed Native Americans and took over the continent. The formation of the USA comes from this type of effective violence. Who would be the alphas in this case? the native americans? Or the ones that invaded? I know I'm a bit on the nose here, but this is just reality. reply oezi 11 hours agorootparentprevCitation needed. Such a vast generalization seems obviously false. I don't even think the concept of violence makes sense when the being causing it has sufficient cognition. reply pixl97 6 hours agorootparentStrange take. Violence is like a key on a piano. You were born with it. You can choose not to play that note, but it is always there, it is never not there. You can choose to play great melodies without the violence note. You can choose to play melodies with the violence note. You can be forced in a corner and have no option but to hammer the violence key until the strings scream. reply glenstein 3 hours agorootparentprev>you see ‘alpha’ as a synonym for ‘leader’ I mean that's all true enough, but their point, at least narrowly read as a general observation about animals, stands. There are ample examples in nature of dominance hierarchies, even if wolves aren't one of them. Of course, as you note, looking to these examples as a basis for a personal ethos for tough guy psychology is error riddled in so many ways you could write a book unpacking it all. (E.g. why choose other animals remote from the lineage of apes to which we belong, why for that matter not just study humans in the first place, why hitch any ethos to what happens in nature, why not look at what it says about people who seem to need to indulge in this search, etc.) It's good to show that this argument fails on its own terms with its own chosen example, but getting into that kind of back and forth risks implicitly agreeing that it would be right to think that way, should such an example be found in nature. And those examples do exist in nature even if not necessarily with wolves. But as you noted that doesn't have anything to do with better or worse, right or wrong, and is completely lacking in self-awareness about the actual psychological dynamics that causes people to need these kinds of narratives. reply JumpCrisscross 13 hours agorootparentprev> first thing that comes to mind is lions and gorillas Better ones are chickens and horses. (Females, I believe, in both cases.) Gorillas have a variety of social structures, only one which involves the Silverback fighting for dominance [1]. With lions, meanwhile, the females eat first [2]. > are they not alphas? Don't we structure our teams according to leaders? Leaders, yes. Hotheads, no. The “alpha” hypothesis states that the most aggressive rises to the top. [1] https://gorillas-world.com/gorilla-social-structure/ [2] https://www.felineworlds.com/lion-social-structure/ reply ninetyninenine 13 hours agorootparentnah I've seen hotheads as leaders. It's not straightforward. See here on what an alpha is: https://news.ycombinator.com/item?id=42848939 Also for chickens, I've raised chickens. Trust me the rooster is the leader and that thing is a male. reply JumpCrisscross 13 hours agorootparent> I've seen hotheads as leaders Of course. Nobody argued aggression isn’t effective. The argument is it isn’t the effective strategy, just one among many. > See here on what an alpha is You’re taking a definition that arose from animal observations and then filtered through anthropological and management academia. Colloquial, academic and industry definitions have diverged on this term; it’s probably being redefined in American English right now. > Also for chickens, I've raised chickens Chickens are the correct analogy. If you’re leading a team of people who remind you of chickens, leading with aggression works. (Not exaggerating. Some situations respond well to e.g. raising one’s voice or acting out frustration.) As your sources show, that isn’t necessary for leadership. reply ninetyninenine 12 hours agorootparent>You’re taking a definition that arose from animal observations and then filtered through anthropological and management academia. Colloquial, academic and industry definitions have diverged on this term; it’s probably being redefined in American English right now. I cited two dictionaries as well. >Chickens are the correct analogy. If you’re leading a team of people who remind you of chickens, leading with aggression works. (Not exaggerating. Some situations respond well to e.g. raising one’s voice or acting out frustration.) Analogies were never part of the equation. You know they weren't. You were talking about animals explicitly not animals as an analogy. I feel you're trolling. reply JumpCrisscross 11 hours agorootparent> I cited two dictionaries as well Those are point-in-time snapshots. The Wikipedia article expands on the term's etymology [1]. (If you can provide links to those definitions I'll help look up when they were adopted. I couldn't find the definition you quoted on Webster's website [2], for example.) > Analogies were never part of the equation The term is an analogy. That's the point. There was a term in animal ethology that was popularised to the point it entered mainstream use. Synonymising alpha to leadership, moreover, is even more recent and mostly occuring in American English (and now, with partisan flair). [1] https://en.wikipedia.org/wiki/Alpha_and_beta_male [2] https://www.merriam-webster.com/dictionary/alpha#word-histor... reply error_logic 15 hours agorootparentprevThe thing is that when someone has to work hard to seem Alpha they are instead revealing their insecurity--the need to compensate to hide vulnerability. Some people play into that game, others see through it. reply ninetyninenine 15 hours agorootparentAlphas are just leaders. Whether they have vulnerabilities is orthogonal to the fact that they control and lead the pack. Whether it’s through force or pay. Do you have a good boss that you like who’s a good person? He’s an alpha. You’re a beta if you’re under him. Do you have a boss who’s an ass hole who got pegged and raped by his uncle when he was a kid and now he’s taking all that pent up humiliation on you? Yeah he has a big vulnerability. But. He. Is. Still. An. Alpha. It’s not a game. Games are for friend groups and kids. I’m talking about the economic engine that builds civilization itself. That engine is made up of a hierarchy of alphas and betas. reply bbwbsb 12 hours agorootparent(Dominance-related) insecurity is being pathologically averse to being seen as weak, which leads to preferring dominance as form over dominance as function. If the meek hippie gets everything he wants from his wife, his neighbors, his peers, etc., and the physically impressive traditional man is ignored and rejected, then the hippie is more dominant (i.e., leading and getting what he wants) than the traditional man (even if he is abusing his wife the whole time she laughs at him). The actually effective strategies are available to the insecure but shunned and rejected because they cannot be tolerated, creating a self-imposed impotence. The word alpha, in almost every context I've observed, is used exclusively to refer to such dominance as form, especially in substitution for dominance as function. i.e., it is applied almost exclusively to people who are definitionally not dominant. The only exception I have encountered is women-focused kink literature which, being fantasy, maintains that dominance as form is dominance as function so as to make sexual fantasies seem more real. In short: you are describing a kink, not real life. Though I consider that you might be joking too; I really can't tell. reply ninetyninenine 12 hours agorootparent>In short: you are describing a kink, not real life. Though I consider that you might be joking too; I really can't tell. The fact that you said this means you're out of touch with reality. Let me provide evidence about how wrong you are: https://news.ycombinator.com/item?id=42848939 Literally you can go to the dictionary yourself and look up the definition to see how baseless your argument is. I don't know why you people are just pulling this bs out of thin air. It can't be just BS is several people are coming from your angle despite extraordinary evidence to the contrary. Maybe it's just shared victimhood. Were you bullied by these types of people before? reply bbwbsb 1 hour agorootparentI call it a kink because I attract women with the kink. I'm referring to pragmatics not semantics; the use of dictionary definitions is a category error. No, I was not bullied. People like I describe would posture, I would raise my eyebrow and wait, and then they would treat me nice and pretend it didn't happen. Dominance as form outside the bedroom is remarkably ineffective. That's why I call it dominance as form. \"When a diplomat says yes, he means ‘perhaps’; When he says perhaps, he means ‘no’; When he says no, he is not a diplomat. —Voltaire (Quoted, in Spanish, in Escandell 1993.) These lines — also attributed to H. L. Mencken and Carl Jung — may or may not be fair to diplomats, but are surely correct in reminding us that more is involved in what one communicates than what one literally says; more is involved in what one means than the standard, conventional meaning of the words one uses. The words ‘yes,’ ‘perhaps,’ and ‘no’ each has a perfectly identifiable meaning, known by every speaker of English (including not very competent ones). However, as those lines illustrate, it is possible for different speakers in different circumstances to mean different things using those words. How is this possible? What’s the relationship among the meaning of words, what speakers mean when uttering those words, the particular circumstances of their utterance, their intentions, their actions, and what they manage to communicate? These are some of the questions that pragmatics tries to answer; the sort of questions that, roughly speaking, serve to characterize the field of pragmatics.\"[1] 1: https://plato.stanford.edu/entries/pragmatics/ reply hackable_sand 10 hours agorootparentprevThey are right though. You are describing a kink. reply gilleain 9 hours agorootparentprev> I don't know why you people are just pulling this bs out of thin air. \"Am I so out of touch? No. It's the children who are wrong.\" Principal Skinner reply fenomas 14 hours agorootparentprev> Alphas are just leaders If you redefine alpha to just mean \"leader\", then the claim that CEOs are alphas is obviously true, but also meaningless. But the theory TFA is about was not just that wolf packs had a leader. It made a bunch of other claims, as TFA describes, and those other parts (that you're excluding) are what's considered debunked. reply ninetyninenine 14 hours agorootparentThere's no redefinition here. When we refer to animals in every context, alphas are leaders. There's no \"redefinition\" going on here at all. What's going on is you're not able to see how alphas apply to human society. You're not able to jump the intellectual gap to identify, \"hey if packs of animals have alphas, what's the human equivalent?\" I attempted to jump that gap for you, but you're not able to see it. >But the theory TFA is about was not just that wolf packs had a leader. It made a bunch of other claims, as TFA describes, and those other parts (that you're excluding) are what's considered debunked. And he applies that to humans without considering alphas in other animal hierarchies. He implies that the entire theoretical concept of alphas comes ONLY from wolves and once he debunks wolves (with no citations) he debunks the entire concept of what an alpha is. Riiggght. reply cbsmith 13 hours agorootparent> There's no redefinition here. When we refer to animals in every context, alphas are leaders. Yes, there is a redefinition. The context of Alpha Wolf was based on the notion of a dominance hierarchy, which does occur when unrelated wolves are put together in a confined space. In the wild though, they function more like a family, with no acts of dominance. The breeding pair still lead the pack, but not through dominance. https://web.archive.org/web/20051214072331/http://www.npwrc.... reply ninetyninenine 13 hours agorootparenthttps://news.ycombinator.com/item?id=42848939 reply earnestinger 12 hours agorootparentprevDominance != aggression (South park had interesting episode on that) reply HWR_14 17 minutes agorootparentWhich episode are you thinking of? fenomas 12 hours agorootparentprev> What's going on is you're not able to see how alphas apply to human society. You're not able to jump the intellectual gap to identify, \"hey if packs of animals have alphas, what's the human equivalent?\" I literally just pointed out that if you are defining alpha to mean \"leader\" then it's meaningless to then claim that leaders are alphas. That was the comment. reply earnestinger 12 hours agorootparent> if you are defining alpha to mean \"leader\" Could you elaborate what is the meaning of “alpha”? (I sense there is definition, that is being kept secret from me) reply JumpCrisscross 11 hours agorootparent> Could you elaborate what is the meaning of “alpha”? There isn't a fixed definition [1]. Most definitions reference dominance, e.g. Merriam-Webster [2]. (Annoyingly, dominance also has different meanings in anthropology, animal ethology, sociology and common use.) But due to the term being relatively new, adopted from animal ethology, co-tiopted by the memeverse and now being politically charged, you're basically walking into semantic ground zero by using the term. [1] https://en.wikipedia.org/wiki/Alpha_and_beta_male [2] https://www.merriam-webster.com/dictionary/alpha#word-histor... reply cycomanic 13 hours agorootparentprevHave you actually read the article? He does give a citation to the same author who initially coined the term alpha wolf. And redefining alpha to just mean leader is a redefinition, as written in the article the term originates from the alpha wolf who achieved dominance through overtly aggressive behaviour (which does not match with how wolves behave in the wild). It's ironic that you bring up CEOs etc as proof that there's alphas, when the whole premise of the article is that recent structuring of human society is based on an this wrong view that the aggressive dominance is \"natural\" and what is required for leaders reply ninetyninenine 12 hours agorootparent>Have you actually read the article? He does give a citation to the same author who initially coined the term alpha wolf. I meant link to source. Like point me to the place where he defined it that way. Instead he just made a claim without a citation. For example: https://news.ycombinator.com/item?id=42848939 These are citations proving my point. reply jonnybgood 15 hours agorootparentprevThat's not how the word alpha is used colloquially. I believe error_logic is referring to its colloquial usage. reply ninetyninenine 15 hours agorootparentThe colloquial usage is leaders. Not people with insecurities. reply jcranmer 14 hours agorootparentThe only colloquial usage I've seen is that it is someone who has the ability to demand that others perform a public display of obedience to them. Like every time I've seen somebody unironically refer to themself as an \"alpha,\" it's always had that underlying connotation of \"Respect Me!\" And every time I've seen someone mocking somebody else for being an \"alpha,\" it's because, well, that respect was clearly undeserved. reply earnestinger 12 hours agorootparent> Like every time I've seen somebody unironically refer to themself as an \"your leader,\" it's always had that underlying connotation of \"Respect Me!\" And every time I've seen someone mocking somebody else for being an \"my leader,\" it's because, well, that respect was clearly undeserved. I think same sentiment persists when alpha->”your leader” replacement is made. reply ninetyninenine 14 hours agorootparentprevRight but the term here is used to categorize human/animal hierarchies/behavior in an objective context. No one is here projecting their alphaness onto others. Clearly. reply defrost 15 hours agorootparentprevA colloquial usage is leaders. Another common colloquial usage is total self obsessed wanker. reply ninetyninenine 14 hours agorootparentnext [3 more] [flagged] defrost 14 hours agorootparentI'm cool calm and collected. Don't project, don't assume you can read minds and emotional states over the internet. Of course we here in Australia think of anyone bandying about \"alpha\" as a tosser .. how else do you suppose they'd be thought of? They're literally right there with people that use \"Bro\" in a sentence and espouse paleo diets. reply ninetyninenine 14 hours agorootparenthttps://news.ycombinator.com/item?id=42844619 We're talking about the definition in a more formal context. Of course I don't go around saying hey \"that dude is an alpha\" or \"i'm an alpha.\" reply watwut 10 hours agorootparentprev> Alphas are just leaders. No they are not. There are plenty of leaders who are not alphas and they lead organizations that achieve good things. Alphas are generally just assholes who want to be leaders. They usually lead self selected friends who crave their validation and people with healthy boundaries just nope out of those systems. reply Rury 11 hours agorootparentprevThe article isn't wrong about captivity/freedom, although alphas are certainly thing. Hierarchies are naturally born out of confrontations and how they're settled, and captivity (ie scarcity) breeds confrontations. If two separate entities have opposing wants which cannot be satisfied simultaneously (ie are mutually exclusive), how do you determine who get's what they want? Well, naturally some kind of fight happens. Which either involves displays of intimidation, threats, arguments, or acts of violence (ie aggression). Naturally, living organisms value life and consider risks which threaten their livelihood, and so, these situations are always an assessment of wants, such as \"is my want for the last piece of food, greater than my want to avoid fighting my opponent?\" Or, \"Am I willing to risk dying for this? Do I need this food, do I need to fight this opponent or can I get food elsewhere before dying of starvation?\". As so, animals typically use violence as a last resort for settling disputes, unless the risks are so low as to be play (ie you're certain to win). It should therefore not be surprising that captive animals (or animals backed into a corner) feel obligated to fight, as they have few alternatives to avoid confrontation. Whereas, non-captive animals, by being more free, are better able to avoid a direct confrontation, and therefore exhibit less acts of aggression. An alpha, just happens to be those who win disputes repeatedly, and opponents or potential opponents have learned to be intimidated by them, and so cede disputes to them when they arise without fighting. A hierarchy is thus naturally formed from repeated disputes. reply ada1981 7 hours agorootparentThanks for this. There has been some research that shows primates are more likely to cooperate in scarcity and compete during abundance. Not universal and other factors are at play, but chimps generally prefer to cooperate vs compete. I argue that the 2 greatest factors for human survival have been our creativity and cooperation. We run an AI Research Lab every week where this is a central theme, helping folks navigate uncertainty through a focus on creativity and cooperation. reply Avshalom 16 hours agorootparentprevLions, gorillas don't recapitulate \"alpha\"s and CEOs are a legal not natural construct. reply ninetyninenine 16 hours agorootparentThe legal document is a natural construct born out of the moral and behavioral instincts humans have. reply fsckboy 15 hours agorootparentprevCEOs are not a legal construct, that's the \"President\" of a corp, or the \"Chairman of the Board\". CEO is a popular sobriquet because it sounds cooler. reply Avshalom 15 hours agorootparentIncorporations are legal constructs. Any position in one is even more so. reply fsckboy 12 hours agorootparentCorporations are required by law to have Presidents and Chairmen. That is the legal construct. They are not required to have CEOs who therefore sit on shakier legal ground. reply JumpCrisscross 10 hours agorootparent> Corporations are required by law to have Presidents and Chairmen Varies by jurisdiction. Common ones are a president, secretary and treasurer. (They can be the same person.) reply astahlx 16 hours agorootparentprevLions? Females rule the groups there. See https://www.reddit.com/r/AskBiology/s/M1VsH34t5Y reply ninetyninenine 15 hours agorootparentThat articles says females do the hunting and getting food for the male while males only handle territory. That’s equivalent to the male on the couch watching TV with a shotgun on his lap defending his territory and the females going hunting (aka kitchen) to make the husband a sandwich. Joking aside the males are the leaders. How do I know? They kill the kids on arrival and the females can’t do fuck shit. reply nosianu 11 hours agorootparent> That articles says females do the hunting and getting food for the male while males only handle territory.That articles says females do the hunting and getting food for the male while males only handle territory.* That's a myth. Male lions do hunt. When hunting alone and not as \"heavy support\" for the females, they use different strategies than the females, and do it in a different environment. I read an article from a researcher about that in particular, can't remember any context to find it quickly though. There are many (scientific and casual) articles like that though: https://singita.com/archive/wildlife/wildlife-mythbusters-ta... https://medium.com/@shelldonwells/do-male-lions-hunt-e9f5ea5... reply ants_everywhere 16 hours agorootparentprevDon't CEOs spend a lot of their time groveling to investors for money? reply ninetyninenine 15 hours agorootparentWe live in a hierarchical pyramid where there are alphas managing sub alphas and this happens at multiple resolutions within our society. The ceos are groveling to investors, aka billionaires. I guess those guys are the true alphas. reply yndoendo 15 hours agorootparentYou just redefined alpha as a person with money. I would argue that the wealthy which hold loans based on stock collateral are holding debt and not credit. US banking laws are just hiding the fact. So a number of rich people you define as alpha are not really alpha because of debt vs credit imbalance. This means the person spending money while going through Disney World are alpha. reply ninetyninenine 15 hours agorootparentBro I define an alpha as someone who leads and controls others. Someone people follow whether by will or by force or because they are paid a salary. You’re just being pedantic. I don’t need to explain to you why people intrinsically know Elon musk is an alpha and the person going to Disney world is a beta. reply yndoendo 15 minutes agorootparentTo me the Alpha Mentality is just stupidity in another Yes-Man form. Seems you hold the alpha mentality deeply and respect those you seem as alpha. By your own logic, cult leaders are alpha and should be respected. Respect can only be earned, continuously reinforced, and lost in a single moment. Respect cannot be demanded or forcefully requested. Watch the first episode of \"Band of Brothers\". Who would you want as a leader, the alpha, one with power, Captain Herbert Sobel or 2nd Lutenaint Richard Winters, the one without power? reply vpribish 1 hour agorootparentprevlook, HN, stop feeding the troll. he's tweaking you just shun him reply myvoiceismypass 5 hours agorootparentprevI’m really confused as to how trips to Disney are entering the conversation here. What does that have to do with any of this discussion? reply throw4847285 15 hours agorootparentprevIt is hilarious to see people use bro pseudo-science to reconstruct dialectical materialism. reply darkerside 15 hours agorootparentprevThis is a very not HN comment reply ninetyninenine 14 hours agorootparentSure HNers hate Elon for various reasons. But overall some random dude who goes to disney world versus A billionaire who caught a rocket and makes stupid tweets online.... Let's be real here about what \"society\" defines as alpha, not some HNers backwater opinion. ANd even that HNer knows his contrary opinion is backwater. Trump and Elon get a lot of hate, but they are leaders on the apex of the human status hierarchy. reply myvoiceismypass 5 hours agorootparentWhen I think of “leaders of the human status hierarchy”, I do not think of a man who spends his entire day online trolling people, regardless of whether he is worth billions or not. Similarly, I don’t picture a guy that wears lifts because he is ashamed of his height, or who paints his face orange, to be that “apex” either. reply blackqueeriroh 10 hours agorootparentprev> Trump and Elon get a lot of hate, but they are leaders on the apex of the human status hierarchy. Trump can’t spell “Colombia” and Elon is, at best, an idiot savant. reply ants_everywhere 15 hours agorootparentprevFor some public companies retail share holders also have to be groveled to. I don't have an overall point here other than that power isn't as simple as some people want it to be. You command a ship until there's a mutiny. There are hierarchies, but that's only one way of slicing things. It's more about a set of several feedback loops that nobody explicitly understands. reply ninetyninenine 15 hours agorootparentOf course it’s not simple. But alphas and betas are intrinsic to society and that’s my point. You will see that society converges towards alphas. There have been attempts to make everything egalitarian but it doesn’t last and alphas pop up everywhere. Heck you can’t even make a purely voting society where society votes on everything. Instead everyone votes for an alpha. Aka trump. reply myvoiceismypass 5 hours agorootparentCitations, please. This may be a US-centric site, but that does not mean all societies on this planet behave like Americans. reply ada1981 16 hours agorootparentprevAhoy. I agree that humans aren’t wolves. For a good primer on hierarchy dynamics in human systems, I recommend Tribal Leadership by Dave Logan. In what Logan calls stage 3 leadership, organizations cluster around individual “rockstar” people and this is the vast majority of human organizations.. they estimate 80%. In Stage 4, it’s all rockstar people organized around values vs. a person. You certainly still have people who have authority through competency, but people aren’t granted authority based purely on “position”. Stage 4 culture are the best in the world, and learning how to build them is a worthwhile skill. reply wavefunction 16 hours agorootparentprevbunch of submissive in the sheets CEOs? as long as we're operating on feelings and emotions here the whole thing is dimwitted bullshit psychological theory reply DavidPiper 6 hours agoprevSurprised not to see David Graeber's \"The Dawn of Everything\" recommended yet in a thread like this. It details how \"pre-modern\" societies functioned based on the available archaeological and anthropological evidence we have, and one of its key findings is just how much human history conflicts with narratives like \"the alpha myth\". It also goes into detail about how we ended up with so many social narratives like it that just don't have any justifiable precedent. reply 1oooqooq 5 hours agoparentthis is way above the audience. this conversation is unfortunately like talking about garbage collector algorithms in different architectures, to social science grads. but in the rare case a social scientist want to lear about memory borrowing, i would also suggest somethig more recent (and less anachist to this crowd) https://ia804705.us.archive.org/13/items/byung-chul-han-the-... reply mattgreenrocks 5 hours agorootparentI've had that (The Burnout Society) on my to-read list for awhile, do you recommend it? I think the core thesis is spot on from what I've seen of discussions, and am looking to decide which aspects of ambition are my own, and which have I have accepted unconsciously. reply flanked-evergl 4 hours agoprevI think the path forward requires rejecting appeals to nature. \"Darwinism can be used to back up two mad moralities, but it cannot be used to back up a single sane one. The kinship and competition of all living creatures can be used as a reason for being insanely cruel or insanely sentimental; but not for a healthy love of animals. On the evolutionary basis you may be inhumane, or you may be absurdly humane; but you cannot be human. That you and a tiger are one may be a reason for being tender to a tiger. Or it may be a reason for being as cruel as the tiger. It is one way to train the tiger to imitate you, it is a shorter way to imitate the tiger. But in neither case does evolution tell you how to treat a tiger reasonably, that is, to admire his stripes while avoiding his claws. If you want to treat a tiger reasonably, you must go back to the garden of Eden. For the obstinate reminder continued to recur: only the supernatural has taken a sane view of Nature. The essence of all pantheism, evolutionism, and modern cosmic religion is really in this proposition: that Nature is our mother. Unfortunately, if you regard Nature as a mother, you discover that she is a step-mother. The main point of Christianity was this: that Nature is not our mother: Nature is our sister. We can be proud of her beauty, since we have the same father; but she has no authority over us; we have to admire, but not to imitate. This gives to the typically Christian pleasure in this earth a strange touch of lightness that is almost frivolity. Nature was a solemn mother to the worshippers of Isis and Cybele. Nature was a solemn mother to Wordsworth or to Emerson. But Nature is not solemn to Francis of Assisi or to George Herbert. To St. Francis, Nature is a sister, and even a younger sister: a little, dancing sister, to be laughed at as well as loved.\" ~ Chesterton reply indoordin0saur 19 minutes agoparentYup. This whole article is just a glaring case of the naturalistic fallacy (https://en.wikipedia.org/wiki/Naturalistic_fallacy) You don't need to argue that \"alpha males\" don't exist amongst wild animals to make that case that it's bad for human civilization. reply muzani 1 hour agoprevI feel that rather than disproving the alpha myth, this emphasizes that it works well in cages. What makes a cage? I'd say something where sustainability is a factor there are limited leads, limited work, limited territory. People slow down otherwise they become too productive, deplete the work available, and get laid off. Hierarchies evolve around controlling these limited resources. All the danger is internal. There's constant resources coming in from god knows where, so budgeting takes priority over hunting. reply indoordin0saur 17 minutes agoparentYes. It seems almost like a tautology that alphas arise in environments with scarcity and competition. reply karaterobot 2 hours agoprevForget the captivity part, the fact that people aren't wolves is what always bothered me about this dumb idea. reply niemandhier 13 hours agoprevI might not be the best possible version of myself but I am at least a release candidate, and therefore have transceded the concepts of „alpha male“ and „beta male“. reply mnsc 1 hour agoparentSome cowboy developer pushed me to prod despite a glaring beta tag and a plethora of known critical bugs. Yet here I am, 45 minors later with the beta tag still there, delivering measurable value to all my users. reply BLKNSLVR 5 hours agoparentprevI'm an alphalfa male. I sometimes eat vegetables. reply korse 20 hours agoprevDid anyone consider that our society IS artificial conditions? Not sure about you all, but western society feels much more akin to a zoo than the wilderness. reply TheOtherHobbes 17 hours agoparentYou're far more likely to be eaten alive by something in the wilderness than in a corporate office. If we've achieved nothing else, at least we've added a layer of political and economic indirection to our cannibalism. reply eleveriven 11 hours agoparentprevSoo maybe the \"alpha\" behaviors we see are a natural response to unnatural conditions and it's more about how we behave when we're stuck in systems reply ssnistfajen 11 hours agoparentprevThis is what I've been leaning towards after initially renouncing the experiment for its arbitrary nature. \"Alpha\" behaviour emerge in zero-sum/artificial scarcity situations, yet contemporary society is full of zero-sum situations and artificial scarcity due to market inefficiency and gatekeeping. It's not just Western society either. If anything, Western-aligned industrialized societies experience far less scarcity. So the behavioural pattern shouldn't be entirely dismissed as pseudoscience, but worthy of critical inspection and reference. reply indoordin0saur 12 minutes agorootparent> Alpha\" behavior emerge in zero-sum/artificial scarcity situations Zero-sum, yes, but why only artificial scarcity? It seems like this would arise in a natural situation as well... Even if you lived on abundantly producing land that is not (yet) heavily populated there is still going to be mate competition. reply jmye 19 hours agoparentprevI don’t understand what you mean. That you can certainly opt in to artificial nonsense, as wholeheartedly and completely as you want, doesn’t mean society is artificial. It just means that’s the part you’ve decided to live your life in. reply tolerance 18 hours agorootparentC’mon now, you know what he means. Or you’ve got a decent idea of what he might mean. People are beginning to consider that the most pervasive aspects of society—parts that you don’t have a choice whether you’re a part of it or confronted by it—are social constructs, and that humans are constrained in what they’re allowed to believe in and how to come to terms with those beliefs amongst one another. reply sharkbird2 7 hours agoprevI feel like the term \"alpha\" is deeply problematic since different people use it to mean completely different things. Many people associate it with the toxic traits of dominating others through aggression. While other's use it to reference a healthy, good, strong, well developed and balanced individual who helps others. I think we need new terms for these things so we can start differentiate them. reply roland35 6 hours agoparentThe problem is that people who use the toxic definition of alpha really like it! There is no motivation to change sadly. reply getnormality 3 hours agoparentprevNo matter what new terms are invented, people will reinterpret them to align with their own perspectives. If \"alpha\" means good in some social context, people will load it up with all the contradictory qualities that different people consider good. reply vunderba 21 hours agoprevFrom the article: > Instead of ruling from above, he built a flat structure where decisions emerge from collaboration. I've also seen the flip-side of this where due to the fact that there is no established hierarchy, you spend inordinate amounts of time welding entirely disparate architectures together since every team was cowboy coding and doing their own thing. Also anecdotally, I've never once experienced this Type-A prima donna alpha engineer but I've had absolutely zero interest in working for a FAANG. Every engineer I've ever worked with was pretty easy-going. reply tayo42 21 hours agoparent> Every engineer I've ever worked with was pretty easy-going. Every one seems easy going until you disagree with them. reply rramadass 5 hours agorootparentHa, Ha; True. For some reason people don't want to face reality but want to believe in fancies/stories which they have picked up from surrounding society. Related, the Norwegian documentary series \"Hjernevask\" (i.e. Brainwash) is interesting https://en.wikipedia.org/wiki/Hjernevask reply FrustratedMonky 21 hours agoparentprev\" Type-A prima donna alpha engineer \" Most are bluster. Talk loud and fast and be rude, arrogant, and lot of people will think you must really know what you are doing. reply barrettondricka 19 hours agoprevAuthor: Applying behavior of wolves in captivity to humans was a mistake. Also Auther: Applies behavior of wolves in free environment to a very specific and complex part of human society. And of course, zero link or citations on anything. reply svnt 16 hours agoparentEasier to tear down than it is to build well. reply barrettondricka 15 hours agorootparentI admit to being rude and to being a very bad builder myself. But the article is bad. Just about every piece of evidence has some sort of issue. Correlation-causation, or not enough data, or just assumptions. The wolf stuff (seems to be) based on only 2 observations. The author cites big tech in CA, but then describes a single clothing company that has a high random metric that is supposedly an accurate indicator of all of the above. And the connections between arguments are not even that good. Skimming over it, I wasn't sure what the article's point was. As for the conclusion, the vague words on what could be done are the kind of stuff everyone is trying anyway for other reasons, and it isn't (?) working. reply charles_f 12 hours agoparentprevNo. Author: deducting that behavior observed in captive environments was also true in liberty was a mistake. Author: silicon valley is somewhat of a captive environment so lessons from captivity are not entirely wrong. reply ninetyninenine 16 hours agoparentprevAlphas aren't only observed in captive wolves. They are observed in MANY MANY animal groups. I think the full picture isn't some liberal idea where everyone lives in harmony and can work together for the common good without the weakest link ever getting left behind. The truth is it's a bit of both. We live in a world where it's dog eat dog but also companionship and working together are BOTH effective strategies for survival. Most humans are programmed to be able to handle both modes of survival . Depending on circumstance one strategy often becomes critical for survival. For example: if you're in captivity or aka a setting with very very limited resources the alpha strategy works best. Of course no citations for me either. But I think it's quite clear the author is biased. reply nis0s 15 hours agorootparentYou’re right. Antarctic penguins and walruses are good examples of this. Penguin females go hunting, while the males hatch eggs and rear the young till the females return. Walrus males, OTOH, absolutely dominate other walrus males, and keep a harem of females. But who cares? Humans are more closely related to primates than other animals, who exhibit all kinds of different behaviors across species. I am not sure why we’re trying to model human behaviors from wolves at all. In fact, human advanced cognitive development is unique amongst the entirety of the animal kingdom, so it’s okay if humans have behaviors which are unique just to us, and are unobserved in other animals. reply roughly 2 hours agorootparentThere’s a recent book, “Bitch: On the Female of the Species” by Lucy Cooke, which specifically targets the traditional “strong males weak females is the natural order” trope. It’s brilliantly written, comprehensive in its dismantling of the concept, and a very fun read (side note: I have a habit of reading in bars, and this one got a lot of looks) https://bookshop.org/p/books/bitch-on-the-female-of-the-spec... reply spandrew 10 minutes agoprev> Consider how these dynamics manifest in Silicon Valley, where Facebook's infamous \"move fast and break things\" mantra shaped a generation of tech culture. This emphasis on speed and disruption at any cost has created work environments that mirror the artificial pressures of captivity, where displaying dominance often takes precedence over fostering sustainable innovation. – Yeaa..... this is a huge stretch. I'm not a hustle culture proponent by any means (Patagonia's 'let them go surfing' seems great to me!), but \"Move fast and break things\" is a pragmatic mantra that speaks to the nature of enterprises in highly competitive markets. Laypeople will balk at the idea of \"breaking things\". But that's hubris. It takes humility to admit that the breaking things part of discovery and invention isn't isn't optional. Fast or slow, you will break things pursuing an idea unknown space. The mantra speaks to the need to adjust risk tolerances in futuristic-leaning businesses. There are other ways, of course. Bio-med breaks things in controlled trials, for example. But it's also slow as molasses. Rife with in-fighting. But to equate alpha-male captivity with futuristic pragmatism is a huge reach. reply robocat 21 hours agoprevThe fact the wolf alpha male theory was wrong doesn't imply we don't have alpha dominance for men. Do we use alpha male gorillas to explain the concept? (Edit) Or Chimpanzees: https://news.janegoodall.org/2018/07/10/top-bottom-chimpanze... The highest-ranking chimpanzee in a group is the alpha-male. These males climb their way to the top of the chimpanzee hierarchy, and the ways they choose to do so can differ with the personality of the individual leader. Take two of the alpha males observed in Gombe, Frodo and Freud, for instance. Though they were brothers, each chimp had a very different leadership style. While Freud maintained control through fostering strong alliances and grooming those he wanted to keep under his command, Frodo relied heavily on aggression and brute strength. reply JumpCrisscross 11 hours agoparent> Do we use alpha male gorillas to explain the concept? We should use chimpanzees. The popular definition evolved pretty much indepdenently of the science, unfortunately [1]. Management texts have tried toning this down, particularly in the gender-equality years. At its nonsensical limit one wound up with alpha = leader, which is tautologically useless as a tool for explaining leadership. The term probably belongs in a bin. There are deep social lessons we can learn from observing chimpanzees. (Or even wolves.) But they're not going to condense into a single term, certainly not one that's been through the 4chan and partisan ringers. Even among chimpanzees, we see a variety of tactics used to achieve dominance, including groups of chimps banding together to topple a leader. [1] https://en.wikipedia.org/wiki/Alpha_and_beta_male reply enragedcacti 1 hour agorootparentEven within apes the variance is wide enough that I question the value in drawing any conclusions about human nature, especially for laypeople. Why chimpanzees and not bonobos who are equally close relatives to humans but whose social structure is matriarchal, non-territorial, and considerably less violent? There is such myriad behavior in the animal kingdom that drawing insight into human social structure from it seems like it is more often a Rorschach test than it is a scientific exercise. reply kryogen1c 9 hours agorootparentprev> 4chan and partisan I love the dialectical equivalency here, Feels exactly accurate. At least 4chan plays you laugh you lose, federal politics seems to omit the first half. > dominance, including groups of chimps Indeed one of the most fascinating lessons. \"Alpha\" by force is almost tautologically impossible; leadership is built on inspiration and willfullness. reply xbar 6 hours agorootparentprevAre there management texts from some era where this was actually promoted as a model? reply Anotheroneagain 8 hours agorootparentprev>We should use chimpanzees Why should we use anything at all? Aren't we people ourselves, who, you know, behave like people, without being taught how? reply pixl97 5 hours agorootparent>behave like people, without being taught how Ok, what does this mean when pretty much all people are taught to be people by the generations before them? We throw out a number of feral children and see how they grow up in the wilds? Doesn't sound very ethical. reply JumpCrisscross 4 hours agorootparentprev> Why should we use anything at all? Same reason we play with models: it’s easier to understand through analogy, with the familiar cast unfamiliarly. reply crazygringo 20 hours agoparentprevExactly. The concept didn't come from wolves alone. We also call it a pecking order based on chickens. The concept of a dominance hierarchy is widely established: https://en.wikipedia.org/wiki/Dominance_hierarchy?wprov=sfti... And the gorillas and chimpanzees studied are not in captivity. Obviously not every species exhibits this, and the ones that do don't exhibit it all the time. But the article acts like the concept has been discredited when that is not the case at all. reply cbsmith 13 hours agorootparentAs well established as dominance hierarchies are, they aren't the only social structure, and just as this article explains, it's rather ironic that in the case for wolves, that isn't the prevailing social structure in their natural setting. That wolves have become the prevailing reference point for the concept, is metaphorical on a lot of levels. reply error_logic 15 hours agorootparentprevThe discrediting is of the inevitability of its all-encompassing effects. It can be present without needing to be defended or prevent the Alpha from caring for its young. reply YeGoblynQueenne 16 hours agoparentprevHumans don't want to be chimps though, they want to be wolves. Or lions. Or eagles. That sort of thing. So a narrative about strong wolves dominating weaker wolves is gonna catch on like wildfire and you can bet your house on that. reply tptacek 34 minutes agorootparentI would much rather be a chimpanzee than a wolf or a lion. reply YeGoblynQueenne 21 minutes agorootparentWell then, good news for you. https://www.science.org/content/article/bonobos-join-chimps-... reply tbrownaw 14 hours agorootparentprevIs this like how middle management reportedly wouldn't be caught dead dressing like the janitor, but super rich people supposedly dress like the janitor in order to not look like middle management? reply ineptech 13 hours agorootparentThis sounds like Scott Alexander's \"Fashion explained with cellular automata\" article, which was neat but probably not what OP was referring to and certainly not related to TFA in any way I can see. Still a good read tho. https://slatestarcodex.com/2014/04/22/right-is-the-new-left/ reply cbsmith 13 hours agorootparentprev...or, it's just a solid case of confirmation bias. reply gilbetron 3 hours agoparentprevWhy are we trying to make human societies mimic animal ones? We're the insanely successful species, not wolves or chimps or gorillas or lobsters! reply croes 18 hours agoparentprevI don‘t think those alpha men like it if they behave like apes. Wolves are way cooler. reply itishappy 21 hours agoparentprevIt just means analogies are dangerous. Would be nice if we could determine if the effect actually applies to humans before introducing yet another. reply renewiltord 20 hours agoparentprevIndeed, once we fall in love with a model we can usually find some evidence for the model. reply latexr 18 hours agoparentprevhttps://www.youtube.com/watch?v=0Ti86veZBjU reply dsr_ 21 hours agoparentprevThe fact that the evidence doesn't exist doesn't imply the phenomenon doesn't exist? Remind me, why gorillas and not chimps? Why chimps and not bonobos? Why assume that two related species with completely different environmental niches must share a social structure? reply lmm 17 hours agorootparent> The fact that the evidence doesn't exist doesn't imply the phenomenon doesn't exist? Wolves were never meant to be evidence. They were a model, a metaphor. I wonder how many people who complain about \"believing the myth of alpha wolves\" are the same people who defend astrology as a \"useful way of thinking, whether it's true or not\". reply crooked-v 17 hours agorootparentUntil it was debunked, people thought that the alpha/beta/omega nonsense was the actual natural social order of wolves. Some people then borrowed that as a metaphor. reply djur 16 hours agorootparentRight, \"alpha\" means a lot of things in ethology, but the \"alpha/beta/gamma\" categorization that took hold in reactionary circles was specifically derived from the wolf studies. \"Some individuals tend to be more socially dominant\" is not controversial. reply lmm 17 hours agorootparentprev> Until it was debunked, people thought that the alpha/beta/omega nonsense was the actual natural social order of wolves. Some wolf biologists thought that, sure. But that obviously wasn't and isn't any kind of evidence about human sociology (and conversely nor is the fact that it isn't). reply culi 11 hours agorootparentprev> defend astrology as a \"useful way of thinking, whether it's true or not\". I wonder how many people defend mind palaces as a \"useful way of thinking, whether it's true or not\". Historically speaking, that's all astrology was. Research has indeed shown that people who practice astrology are better at remembering details about people in their lives. Astrology is just a mind palace that allows them to more easily memorize and organize useful facts about the people in their social networks reply goatlover 20 hours agorootparentprevGorillas, Chimpanzees and Baboons are similar in their social structures with the dominant alpha males. Bonobos are different with it being matriarchal and conflict resolution being less violent. reply TheOtherHobbes 17 hours agorootparentSadly not that simple. https://cosmosmagazine.com/nature/animals/bonobos-arent-as-p... reply cyberax 20 hours agoparentprevIt actually works with chimps. They _are_ very hierarchical and somewhat neurotic even in the wild. It's no wonder that they are our closest cousin. Gorillas, in comparison, are much more laid-back. reply tsimionescu 20 hours agorootparentOur closest cousins are actually bonobos (or at least they are as close to us as chimpanzees are), which don't have this type of hierarchy. Regardless, group social hierarchies don't necessarily translate even between closely related species, such as chimps and bonobos, so there is no reason whatsoever to extend them from some ape to humans. reply stonesthrowaway 16 hours agoparentprev> The fact the wolf alpha male theory was wrong doesn't imply we don't have alpha dominance for men. It isn't wrong. The headline is intentionally misleading and the article is just woke nonsense. Read it for yourself. What the article is really saying that the alpha in a zoo wolfpack is different than an alpha in a wild wolfpack because a zoo wolfpack is between non-related wolves whilest in the wild, wolfpacks are composed of family members ( alpha father/alpha mother and children ). There are alpha in wolf packs ( zoos and wild ). Just like in a human family the father/mother are alphas while in a gang, the alpha is usually a non-related individual. The \"myth\" isn't that the alpha doesn't exist. The \"myth\" is about the relation of the alpha to the rest of the pack. reply bandrami 22 hours agoprevWasn't there a guy who did the \"rats prefer cocaine to food\" study but without having the rats isolated in individual cages, and it turns out that no, they don't actually prefer cocaine to food when they aren't (essentially) in solitary? reply UniverseHacker 22 hours agoparentYou’re talking about the Rat Park study. That one is also controversial and hasn’t successfully replicated. Personally I think the general idea of a connection between emotional trauma and addiction is pretty obvious, but also a but more complex than the Rat Park study implies or assumes. reply mandmandam 21 hours agorootparent> hasn’t successfully replicated To be clear, lots of studies have shown that animals in richer environments do less drugs; though the effect doesn't seem to be as strong as Alexander claimed. reply hinkley 22 hours agoparentprevYep. Hopelessness keeps you on the drugs as much as biochemistry. Kicking any bad habit is easier if you have something to replace it with. reply midhunsezhi 2 hours agoprevGreat article but is missing sources with citations so I ran the content against GPTZero's source finder https://app.gptzero.me/documents/124f8c4d-9fd4-43c4-93f9-713... reply bulatb 22 hours agoprevPeople didn't get their worldview from the myth. They like the myth because it fits their worldview. reply Fricken 22 hours agoparent>In their natural habitat, wolf packs operated nothing like the prison-yard dynamics he'd observed in the zoo. Human's aren't in their natural habit most of the time either. \"Prison-yard dynamics\" is how the public education system functions. This is where our children are socialized. reply mettamage 22 hours agorootparentHigh school is more complex as there is a life outside it where other dynamics can be observed. At least that's what I noticed and it made me realize that the popularity contest in high school was a farce. reply vkou 21 hours agorootparent> it made me realize that the popularity contest in high school was a farce. Observing outside life has led me to conclude that the high school popularity contest, is, in fact, the most authentic, freest form of human social behavior. reply throwaway2037 16 hours agorootparentI agree 100%. I could only see it in the last year or two: Instagram allows people to maintain high school popularity in adult life. Before Insta, those people faded into oblivion because there was no global platform to amplify their good looks or likeable personality. (There were still popular, but in much smaller social circles.) After Insta, \"high school popular\" people can maintain it indefinitely. reply blackqueeriroh 9 hours agorootparentLmao “I’m mad some people look nicer than me and know how to be kind to people. They suck and are only popular because of Instagram.” My guy, comb your hair, take a shower, put on deodorant and a clean shirt. You’re fine. reply amanaplanacanal 21 hours agorootparentprevI suspect this is part of the reason unschooling is become more popular. reply jojobas 21 hours agorootparent15% of the 5% homeschooled is not very popular. reply protocolture 17 hours agorootparentUp from 0% before the process was developed. reply dylan604 16 hours agorootparentprevare we equating homeschooling to unschooled now? while there are definitely people claiming homeschool while just as a cover for not being in school, that doesn't mean that all homeschool is not educating the kids. reply erikerikson 14 hours agorootparentprevGP commented on slope, not value, not to mention proportion. reply jojobas 12 hours agorootparentUnschooling has been around for some 50 years. 0.02%/year is one hell of a slope. It's a fad that will pass just as many other fads pass. reply erikerikson 10 hours agorootparentThanks for some data, do you have a source for that? Curious prediction. reply jojobas 9 hours agorootparentGoogled in 5 minutes. reply ada1981 21 hours agorootparentprevI don't disagree of course. And this is less a critique of the individual men vs our current social structure. A good deal of my work is in helping people free themselves, however, this wouldn't be necessary had it not been a life long project of incarceration. reply altairprime 22 hours agoparentprevI appreciated a point I saw a few months ago about how the wolf-derived alpha/beta/etc stuff is a safe way for cis men to experiment with gender identity without risking being labeled queer by other men and having to confront harassment. reply BriggyDwiggs42 16 hours agorootparentLmfao that’s a great observation. Probably at least a big part of it. reply mettamage 22 hours agoparentprevI've pondered alpha male theory for a 5 years before having a solid conclusion before or against it. I rejected it. But yea, I really was that socially clueless, it took me 5 years as a late teen of observing and thinking about it to understand that life is more complicated than that and that the idea of alpha males is ridiculous, especially since I saw strong evidence on the contrary (i.e. \"beta behaviors\" and still having an amazing dating life, etc.). reply hinkley 22 hours agoparentprevSee also depressed rats in stressful conditions becoming drug addicts. reply teruakohatu 15 hours agoprevIf the Alpha Wolf is a myth, it is also a myth that the concept was bought to popular imagination by Rudolf Schenkel and L. David Mech between the 1940 and 1970s. Jack London's gold rush books explore this concept in detail, the first of which, Call of the Wild, was published in 1903. These were very popular books, then and still today. The alpha wolf concept was in public psyche at least in the early 1900s, but probably earlier too. I don't think it is likely that London invented this myth himself and it existed earlier. reply samth 1 hour agoparentJust to be clear, the word \"alpha\" does not appear in the text of \"Call of the Wild\". reply rramadass 5 hours agoparentprevRight. The myth actually originates from Social Darwinism https://en.wikipedia.org/wiki/Social_Darwinism which was what influenced Jack London's thinking (see \"The Call of the Wild\", \"White Fang\" etc.) but at the same time he was also a Socialist (see \"The Iron Heel\", \"The People of the Abyss\" etc.) See his \"views\" section here https://en.wikipedia.org/wiki/Jack_London#Views reply indoordin0saur 22 minutes agoprevThe author obviously wants to critique the machismo culture built into capitalism (which is totally fine!) but he seems to inherently subscribe to the naturalistic fallacy without being aware of it. Thus he feels the need to dismiss the idea of \"alpha male\" hierarchies in nature as non-existent as a way to dismiss them within our own society. You really don't need to do this and IMO, the flimsy premise of the start of his argument undermines the preference for society (which I actually agree with). Just because a certain type of sociological system exists within nature doesn't mean it's one we, as humans, should want for ourselves. reply jarjoura 19 hours agoprevSeems like it's fair to assume, both canines and humans are highly adaptable, and we behave the way we need to in the environment we're in. My dog has never shown me anything except love and compassion, and that gets him the best living conditions, back scratches and the tastiest food. reply lmm 17 hours agoparent> Seems like it's fair to assume, both canines and humans are highly adaptable, and we behave the way we need to in the environment we're in. If that were as true as you seem to be suggesting, there would be no difference between dogs and wolves when the latter are kept as pets, which is clearly false. reply nico 21 hours agoprevSomething similar happened with models for addiction in rats (which were extrapolated to humans) It turned out that rats that live in a nice environment, with plenty of activities and social spaces (not an ugly, mostly empty cage), don’t become addicted Ref: Rat Park experiment reply Analemma_ 21 hours agoparentThe Internet loves the Rat Park experiment, but it never replicated and should be considered just as bogus as all the other stuff in social psych which hasn't survived the replication crisis. It's fitting for this thread though, because it survives for the exact reason the alpha wolf stuff survives: people love the story so they hang on to it irrespective of the science. reply mmooss 16 hours agorootparent> it never replicated and should be considered just as bogus Did someone try to replicate it? What were the results? What were their conclusions about its credibility? > all the other stuff in social psych which hasn't survived the replication crisis. Replication isn't black and white: Most of the experiments at least in the early widely reported phase did replicate but with less statistically strong results. reply throwaway2037 16 hours agorootparentprev> never replicated Not exactly. Wiki says: Studies that followed up on the contribution of environmental enrichment to addiction produced mixed results. Ref: https://en.wikipedia.org/wiki/Rat_Park#Further_experiments reply sunjieming 20 hours agorootparentprevI think 90% of my HS psych class didn't replicate reply layman51 19 hours agorootparentCan you elaborate more on this? I did not take psychology in high school, but I would imagine that such a course would focus on fundamental concepts where “replicating” wouldn’t even be an issue. There might be teachers who might bring up the pop-sci topics, but wouldn’t the meat of the class just be learning about theories or frameworks? reply throwup238 19 hours agorootparentNot the OP and I didn’t take psych but here’s an example syllabus for AP Psychology from the College Board: https://apcentral.collegeboard.org/media/pdf/ap-psychology-s... The first two units on neurology and cognition seem sensible, but I can see how it’ll quickly go off the rails after that. reply disgruntledphd2 3 minutes agorootparentThis seems entirely fine, after a brief review of the contents. Where do you think it goes off the rails? nradov 20 hours agorootparentprevRight, and we've seen plenty of people who lead the \"Rat Park\" equivalent of human lives and yet still get addicted to opioids. There's clearly no single cause. reply kazinator 16 hours agoprevIf the animals are spread over a wide area, where they don't have to compete for territory or resources with each other, of course the dominant behaviors won't rear themselves. You need the right conditions to repro the behavior. E.g. for starters, you cannot study one wolf in isolation from other wolves, right? You need at least two for one to dominate the other, and they can't be miles apart such that they never meet. reply BurningFrog 3 hours agoprevThis essay is decent workplace leadership advice. The connection to wolf pack social dynamics is very thin. reply WuxiFingerHold 9 hours agoprevExtremely interesting topic, unfortunately shallow article. Still I'm happy for the input. I'd like to learn more about how abundance of food, lack of threats and other comforts of civilization distort the formerly vital instincts into all kind of dysfunctional and harming behavior. And more importantly, how to manage those. reply steve_taylor 14 hours agoprevWhen a Canadian psychologist mentioned lobsters and hierarchies, the dogma to which members of a political group subscribe was updated to include the denial of alphas as a natural phenomenon, making it instead an artificial construct invented by their imaginary oppressors to hold onto imaginary power. reply eleveriven 11 hours agoprevThe performative dominance described here feels eerily familiar reply fedeb95 4 hours agoprevI find your lack of sources... disturbing. reply robwwilliams 21 hours agoprevDubious article. The role of aggression in establishing and maintaining social hierarchies was not mis-diagnosed in 1974. The behaviors of alpha males (wolves) and alpha females (hyenas) are complex but threat and pain is part of the pattern. Highly variable by species, environment, and demographics. Go over the hills from Silicon Valley to Ano Nuevo Marine Conservation Area and watch the behavior of male elephant seals like Bernie LeBoef has done for 30 years. How does any of this apply to Silicon Valley other than as a cute and wrong metaphor? Are we really taking leads from dogs? reply mmooss 16 hours agoparentThe popular trend in U.S. big business is 'animal spirits'. I read an article about how at Davos (iirc), people were bullish on the U.S. and bearish on Europe because the U.S. embraced \"animal spirits\". Animal brains too. reply justonenote 17 hours agoprevAnd what about chimpanzees? Our closest relative genetically. Is anyone disputing that they exhibit a brutally dominant social hierarchy? Our second closest relative is (afaik, or close enough) Bonobos, who lump around all day jacking each other off. While the alpha terminology may have come from dog packs, it's not exactly unobservable in other species, and even on these forums and in VC funding rounds you can observe people both jacking each other off with a healthy amount of social and institutional pandering and financial support, and other people not posting as much but taking dominant positions to \"secure the bag\" and whatever is downstream from that. People don't like these conversations but it doesn't change the reality. reply enragedcacti 1 hour agoparent> Our second closest relative is (afaik, or close enough) Bonobos They are equally close (tree-wise, percent-wise idk), and they exhibit a radically different and mostly nonviolent matriarchal social hierarchy. Not sure what the takeaway from that should be other than that maybe there shouldn't be a takeaway if our two closest relatives (and each others closest relatives) behave so differently from one another. reply PaulDavisThe1st 16 hours agoparentprevMaybe a thing to note here is that the behavior of many different kinds of social animals changes dependent on the living situation. Given that, it is not unreasonable to wonder (hope?) that there might be human living conditions that reduce \"brutally dominant social heirarchy\". The best part is: we even appear to have archeological evidence that this is true, for humans! reply mmooss 16 hours agorootparentWe have tons of current evidence that its true. I've never experienced a 'brutally dominant social heirarchy' personally; it's pretty uncommon. I don't live like Lord of the Flies. IME the theory is a reactionary (i.e., anti-liberal, anti-humanitarian, anti-human rights) myth to justify the rule and worship of power. reply dylan604 16 hours agorootparentprev> reduce \"brutally dominant social heirarchy\". if you mean literal brutality, but how the uber wealthy treating those of lower wealth could be considered brutal as well--in modern parlance of course. reply PaulDavisThe1st 16 hours agorootparentI was quoting the comment I was replying to. reply BriggyDwiggs42 17 hours agoparentprevYou can’t draw more than rudimentary conclusions from the comparison because we could have evolved different behaviors, or we could share them, or we could work that way on the savannah but not in a city, etc. These conversations can be and are used to say whatever the speaker wants to say, since they can just select a trait that’s been observed in a monkey case study and apply it lazily to humans. “Wow humans sure like to jerk off, must be because we’re like bonobos!” reply mmooss 16 hours agoparentprev> Is anyone disputing that they exhibit a brutally dominant social hierarchy? Could you share the basis for saying they do, for those who are not primatologists? > even on these forums and in VC funding rounds you can observe ... How is that 'alpha wolf' behavior? > People don't like these conversations Has someone objected? reply ninetyninenine 16 hours agoparentprev>People don't like these conversations but it doesn't change the reality. I mean you have to be an idiot not to see it. You literally just observe all human societies. You have some leader at the top, the president, a king etc. etc. Then you have CEOs, VPs, Managers, team leads etc etc. Human society is structured around alphas. Clearly. No I take that back. Humans are more similar to bonobos. At least that's how it is when I go to work we just jack each other off all day, no team leads or CEO where I work. reply matwood 9 hours agorootparentI think people run into definitional biases. I used alpha/beta as short hand in a comment last week and was downvoted just for using the terms. In short, alpha does not imply male or violence. It’s someone for whatever reasons is at the top (eq, iq, physical attributes, confidence, etc). It also depends on the group, think big fish/small pond. reply llm_nerd 18 hours agoprevThe original researcher did correct the record about what entails the alpha wolf in a natural setting, noting that pack leadership was usually based upon familial group dynamics, instead of the cliche biggest and strongest that beat down opponents, but in pop science this correction went too far, basically discounting the idea of an alpha wolf altogether. In virtually every discussion about this myth people will regurgitate the notion that there is no alpha. But...there is. He never claimed there wasn't an alpha. Just that the selection among a group of wolves forced together differs from a group of wolves in a natural setting. So Tony Soprano was the alpha of the crime family even though many other characters were likely stronger, crazier, etc. He had the backing of so many people that they knew that his bark had bite, even if it wasn't from him directly. reply WesternWind 14 hours agoprevblaming the wolves is an interesting choice. reply danlugo92 5 hours agoprev1. There's still other species where an \"alpha\" exists. 2. Even if there's no other species where an \"alpha\" exists, what forbids us from it being a human thing? No that I want to look up to a mere mortal \"alpha\" e.g. dictators. Only real alpha is Christ. 3. There's always an alpha in friend groups, I've seen it, should I pluck out my eyes as they're lying to me? Heck no. Doesn't mean the women in the friend group took turns doing you know what to his you know what. An alpha is simple a leader, online manosphere groups took it to the extreme of being \"the guy that gets most girls\" but in truth being known helps with that whole thing. 4. Even my previous 3 points didn't resonate with you, it's just an allegory, I don't know why this particular alpha thing has hit a nerve with some many people, that even whole ass articles have been written up against it, what is the problem with men aspiring to be better? An alpha shares his spoils anyways, in Lions in Hyenas in Coyotes, etc. reply micromacrofoot 22 hours agoprevIt's stunning how so much of our own behavior, or at least explanations of it, reference what turns out to be almost entirely false conclusions. It could take 100 years to undo. reply deepsquirrelnet 22 hours agoparentA more cynical take could be that maybe it fits because we’ve constructed our own captivity or ‘prison conditions’ where this tends to emerge. reply zitsarethecure 21 hours agorootparentOut of curiosity, what conditions would you describe as prison-like that would drive someone to adopt an \"alpha male\" lifestyle in the US? reply deepsquirrelnet 20 hours agorootparent> Consider how this plays out in Silicon Valley, where the \"move fast and break things\" mentality has created a leadership culture that celebrates disruption over sustainability, dominance over collaboration. That’s how the article phrased it. I’m not certain if I agree or not, but I’m not ready to either accept or dismiss it. reply HPsquared 22 hours agoparentprevIdeology doesn't have to be 100% true to be effective. Goes all the way back to Plato's idea of the \"noble lie\". reply micromacrofoot 3 hours agorootparentSometimes doesn't have to be 1% true! reply liuxiansheng 22 hours agoparentprevIt won't be undone because people want it to be true regardless of evidence. The cat (dog?) is out of the bag. reply readthenotes1 22 hours agoparentprevBefore we st",
    "originSummary": [],
    "commentSummary": [
      "The \"alpha male\" concept in wolves, originally based on captive studies, has been debunked; wild wolf packs function more like family units rather than hierarchical structures.",
      "Despite being debunked, the \"alpha\" idea persists due to its appeal in competitive environments, such as Silicon Valley, and its resonance with certain societal and psychological needs.",
      "The continued belief in the \"alpha\" myth underscores how narratives can influence our perception of social dynamics, even when they are founded on incorrect assumptions."
    ],
    "points": 354,
    "commentCount": 311,
    "retryCount": 0,
    "time": 1738005715
  },
  {
    "id": 42845323,
    "title": "Go 1.24's go tool is one of the best additions to the ecosystem in years",
    "originLink": "https://www.jvt.me/posts/2025/01/27/go-tools-124/",
    "originBody": "Written by Jamie Tanna on January 27, 2025 CC-BY-NC-SA-4.0 Apache-2.0 8 mins Go 1.24's go tool is one of the best additions to the ecosystem in years Table of Contents For those that aren't aware, one of the big changes in February's upcoming Go 1.24 release is the new go tool command, and tool directive in the go.mod to manage any tools your project uses. I'm incredibly excited about this, and in my opinion, this is one of the best changes we've had in recent years in the ecosystem as a whole. I've been meaning to write this post since the first release candidate for Go 1.24 landed, but after reading John Howard's Exploring the new \"go tool\" support in Go 1.24 this morning, I thought I should write my thoughts up. What is it? Within your Go codebases, there's often some additional tools that you need to have installed to be able to build/test/deploy the project. Sometimes this will a dependency that's needed for go generateing, or it may be that you want to pipe your go test output into a JUnit-compatible format, so your CI platform can provide more useful metadata. For each of these, you have two choices: require that the user knows how to install them, i.e. by knowing to run make deps or just setup before building anything on the project (which will then i.e. go install the commands) use the tools.go pattern to make it so you can just run go generate, and that'll call the right dependency via go run My preference is tools.go pattern, but there are two key problems with this approach. Firstly, there's a performance hit of using a tools.go. It's something that is slightly noticeable, moreso if your project relies upon a lot of go run i.e. with lots of go generates, because prior to Go 1.24, the go run invocations were not cached. Secondly, it also leads to dependency tree bloat, because you have to record your dependency on i.e. github.com/sqlc-dev/sqlc/cmd/sqlc which then gets recorded in your go.mod, and then anyone using your module will then see that as an indirect (transitive) dependency. This was something we worked on for oapi-codegen's v2 release to further reduce unnecessary dependencies, and make things a bit cleaner for our consumers. This is somewhat mitigated by Go's module graph pruning which won't download dependencies that aren't used, but consumers may still see the dependencies coming in as an indirect dependency, which may not be ideal (especially as it can then bloat their indirect dependencies, which then gets passed on to their consumers and so on . Dependency tree bloat can also be further mitigated by splitting your tools.go into a separate module, which makes it more awkward to invoke dependencies but makes sure that none of your consumers will be seeing any tool-related dependencies. For those who know me as co-maintainer of oapi-codegen, you'll know that the tools.go pattern is our explicit recommendation and we believe is better than installing it as a binary, so it's probably unsurprising that I'm very excited about this as an option to manage dependencies. How does it work? I've started playing around with this on a branch of the dependency-management-data project, where I've got a mix of different tools that need to be installed and used. Let's take a worked example of how we'd move over calls to oapi-codegen to the new go tool pattern. Existing state For instance let's say that we have the following tools.go in its own module: # tools/go.mod module dmd.tanna.dev/tools go 1.22.0 require (github.com/99designs/gqlgen v0.17.49github.com/oapi-codegen/oapi-codegen/v2 v2.4.1github.com/sqlc-dev/sqlc v1.26.0 ) We can then see that we invoke this via go run: // internal/ecosystems/generate.go //go:generate go run modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen config=config.yaml openapi.yaml Migrating To start migrating over to go tool, we need to make sure that we've first pulled in the new version of Go in our top-level Go module: module dmd.tanna.dev go 1.22.7 +go 1.24 toolchain go1.23.2 +toolchain go1.24rc2 Next, we need to pull in a tool dependency on oapi-codegen's CLI tool notice that you need the full path to the command that's being invoked: # NOTE the full import path % go get tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@v2.4.1 We could also do this by hand, but doing it via go get simplifies this a little. From here, we'll notice that our go.mod has a few other changes: @@ 57,12 +57,16 @@ require ( github.com/cenkalti/backoff/v4 v4.3.0 // indirect github.com/cespare/xxhash/v2 v2.3.0 // indirect github.com/charmbracelet/lipgloss v0.10.0 // indirect + github.com/dprotaso/go-yit v0.0.0-20220510233725-9ba8df137936 // indirect github.com/dustin/go-humanize v1.0.1 // indirect github.com/felixge/httpsnoop v1.0.4 // indirect + github.com/getkin/kin-openapi v0.127.0 // indirect github.com/go-ini/ini v1.67.0 // indirect github.com/go-logfmt/logfmt v0.6.0 // indirect github.com/go-logr/logr v1.4.2 // indirect github.com/go-logr/stdr v1.2.2 // indirect + github.com/go-openapi/jsonpointer v0.21.0 // indirect + github.com/go-openapi/swag v0.23.0 // indirect github.com/gobwas/glob v0.2.3 // indirect github.com/google/go-querystring v1.1.0 // indirect github.com/gorilla/mux v1.8.1 // indirect @@ 72,16 +76,22 @@ require ( github.com/hashicorp/go-retryablehttp v0.7.5 // indirect github.com/hashicorp/golang-lru/v2 v2.0.7 // indirect github.com/inconshreveable/mousetrap v1.1.0 // indirect + github.com/invopop/yaml v0.3.1 // indirect + github.com/josharian/intern v1.0.0 // indirect github.com/klauspost/compress v1.17.11 // indirect github.com/lucasb-eyer/go-colorful v1.2.0 // indirect + github.com/mailru/easyjson v0.7.7 // indirect github.com/mattn/go-isatty v0.0.20 // indirect github.com/mattn/go-runewidth v0.0.15 // indirect github.com/mitchellh/mapstructure v1.5.0 // indirect + github.com/mohae/deepcopy v0.0.0-20170929034955-c48cc78d4826 // indirect github.com/muesli/reflow v0.3.0 // indirect github.com/muesli/termenv v0.15.2 // indirect github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect github.com/ncruces/go-strftime v0.1.9 // indirect + github.com/oapi-codegen/oapi-codegen/v2 v2.4.1 // indirect github.com/olekukonko/tablewriter v0.0.5 // indirect + github.com/perimeterx/marshmallow v1.1.5 // indirect github.com/prometheus/client_golang v1.20.5 // indirect github.com/prometheus/client_model v0.6.1 // indirect github.com/prometheus/common v0.60.1 // indirect @@ 91,8 +101,10 @@ require ( github.com/rivo/uniseg v0.4.7 // indirect github.com/sirupsen/logrus v1.9.4-0.20230606125235-dd1b4c2e81af // indirect github.com/sosodev/duration v1.3.1 // indirect + github.com/speakeasy-api/openapi-overlay v0.9.0 // indirect github.com/spf13/pflag v1.0.5 // indirect github.com/tchap/go-patricia/v2 v2.3.1 // indirect + github.com/vmware-labs/yaml-jsonpath v0.3.2 // indirect github.com/xeipuuv/gojsonpointer v0.0.0-20190905194746-02993c407bfb // indirect github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 // indirect github.com/yashtewari/glob-intersection v0.2.0 // indirect @@ 110,11 +122,13 @@ require ( go.opentelemetry.io/otel/metric v1.32.0 // indirect go.opentelemetry.io/proto/otlp v1.3.1 // indirect golang.org/x/exp v0.0.0-20231108232855-2478ac86f678 // indirect + golang.org/x/mod v0.18.0 // indirect golang.org/x/net v0.30.0 // indirect golang.org/x/oauth2 v0.23.0 // indirect golang.org/x/sys v0.27.0 // indirect golang.org/x/term v0.25.0 // indirect golang.org/x/time v0.5.0 // indirect + golang.org/x/tools v0.22.0 // indirect google.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28 // indirect google.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28 // indirect google.golang.org/grpc v1.68.0 // indirect @@ 128,3 +142,5 @@ require ( modernc.org/token v1.1.0 // indirect sigs.k8s.io/yaml v1.4.0 // indirect ) + +tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen From here, we can see: there is a tool directive for github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen the containing Go module for the CLI, github.com/oapi-codegen/oapi-codegen/v2, is now an indirect dependency any other required dependencies of github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen are now indirect dependencies Now we've done this, we could run: % go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen help Usage of /home/jamie/.cache/go-build/0e/0e04736601c8bbef785d372de02859bf8f39405aae9ccbf371477b0f2d8df755-d/oapi-codegen: # ... With this tool set up, we can now modify i.e. internal/ecosystems/generate.go like so to use the new go tool: package ecosystems //go:generate go run modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen config=config.yaml openapi.yaml +//go:generate go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen config=config.yaml openapi.yaml Then running go generate ./internal/ecosystems works as it did before 🚀 Performance implications A less scientific view than Howard John's article above, but we can see a slight improvement in performance: # first time using `go tool`, from a fresh cache directory % time go generate ./internal/ecosystems go generate ./internal/ecosystems 55.05s user 4.57s system 531% cpu 11.220 total # a subsequent call % time go generate ./internal/ecosystems go generate ./internal/ecosystems 0.59s user 0.18s system 424% cpu 0.181 total # another just to see % time go generate ./internal/ecosystems go generate ./internal/ecosystems 0.57s user 0.25s system 404% cpu 0.202 total Compare this to the previous implementation: # first time using `go run`, from a fresh cache directory % time go generate ./internal/ecosystems go generate ./internal/ecosystems 50.29s user 3.67s system 536% cpu 10.063 total # a subsequent call % time go generate ./internal/ecosystems go generate ./internal/ecosystems 1.04s user 0.21s system 185% cpu 0.677 total # another just to see % time go generate ./internal/ecosystems go generate ./internal/ecosystems 1.02s user 0.26s system 191% cpu 0.669 total Notice that the first call is similar in speed, but the use of go tool's subsequent calls are still faster. I'm a big fan of the fact that as of Go 1.24+ the go runs will be cached, so even if you don't move over to go tool, you'll get a performance boost! Concerns Now, there are still a few things I've noticed while doing the migration that aren't necessarily what I expected. go.mod implications Something interesting is that the usage of the tool dependencies being treated as an indirect dependency is that they're present in the dependency tree, and treated like any other indirect dependency. I'd also have preferred that we had just used // tool instead of // indirect, but I can see why this is likely the choice that's made so they're treated like any other dependency but making them less clear as only being required for tools could lead to issues with clashing dependencies, or where you upgrade an indirect dependency and then that breaks other things. This means that tools such as Renovate need to be a little more involved in how to do the updates, but that's all in hand. gqlgen fails to run with Go 1.24rc2 Something I've noticed while playing around with this is that gqlgen struggles to run with Go 1.24rc2, which feels like an upstream Go issue, but it looks like that may be related to the use of /x/tools 🤔 It may be interesting to find out what else gets affected by this please give the RC a test! Closing Overall, I'm feeling very positive about it, and improving the way that dependencies get installed if they should be built from source, but there are dependencies such as golangci-lint which don't recommend building from source and instead using their pre-built binaries, which is fair, and is unlikely to change here. This post's permalink is https://www.jvt.me/posts/2025/01/27/go-tools-124/ and has the following summary: . Why I'm very excited about `go tool` landing in Go 1.24. The canonical URL for this post is https://www.jvt.me/posts/2025/01/27/go-tools-124/ . Written by Jamie Tanna on Mon, 27 Jan 2025 11:27:53+00:00, and last updated on Tue, 28 Jan 2025 10:14:47+00:00. Content for this article is shared under the terms of the Creative Commons Attribution Non Commercial Share Alike 4.0 International, and code is shared under the Apache License 2.0. #blogumentation #go. Also on: www.reddit.com Has this content helped you? Did it solve that difficult-to-resolve issue you've been chasing for weeks? Or has it taught you something new you'll be able to re-use daily? Please consider supporting me so I can continue to create content like this! This post was filed under articles. Interactions with this post ← → Top",
    "commentLink": "https://news.ycombinator.com/item?id=42845323",
    "commentBody": "Go 1.24's go tool is one of the best additions to the ecosystem in years (jvt.me)270 points by keybits 22 hours agohidepastfavorite158 comments TheDong 11 hours agoI think it's a bad addition since it pushes people towards a worse solution to a common problem. Using \"go tool\" forces you to have a bunch of dependencies in your go.mod that can conflict with your software's real dependency requirements, when there's zero reason those matter. You shouldn't have to care if one of your developer tools depends on a different version of a library than you. It makes it so the tools themselves also are being run with a version of software they weren't tested with. If, for example, you used \"shell.nix\" or a dockerfile with the tool built from source, the tool's dependencies would match it's go.mod. Now they have to merge with your go.mod... And then, of course, you _still_ need something like shell.nix or a flox environment (https://flox.dev/) since you need to control the version of go, control the version of non-go tools like \"protoc\", and so you already have a better solution to downloading and executing a known version of a program in most non-trivial repos. reply jchw 5 hours agoparentYep, unfortunately this concern was mostly shrugged off by the Go team when it was brought up (because it would've required a lot of work to fix IIRC, which I think is a bad excuse for such a problem). IMO, a `go tool` dependency should've worked exactly the same way that doing `go install ...` works with a specific @tag: it should resolve the dependencies for that tool completely independently. Because it doesn't, you really, really shouldn't use this mechanism for things like golangci-lint, unfortunately. In fact, I honestly just recommend not using it at all... reply mseepgood 6 hours agoparentprev> Using \"go tool\" forces you to have a bunch of dependencies in your go.mod No, it doesn't. You can use \"go tool modfile=tools.mod mytool\" if you want them separate. reply jcmfernandes 4 hours agorootparentI built a simple tool to generate binstubs and work around this problem: https://github.com/jcmfernandes/go-tools-binstubs However, having multiple tools share a single mod file still proves problematic occasionally, due to incompatible dependencies. reply jen20 6 hours agorootparentprevThis should almost certainly be the default recommendation. reply potamic 8 hours agoparentprevWhy do ecosystems continue to screw up dependency management in 2025? You would think the single most widely used feature by programmers everywhere would be a solved problem by now. reply LinXitoW 5 hours agorootparentGo launched without any dependency management. Go people believe that anything non-trivial is either a useless abstraction, or too difficult for the average developer. So their solution is to simply not add it, and tell anyone claiming to need it that their mind has been poisoned by other languages... Until that time when they realize everyone else was right, and they add an over simplified, bad solution to their \"great\" language. reply room271 4 hours agorootparentYour reply is polemical and somewhat detached from the experience of working with Go. From my experience, Go's dependency management is far better than anything else I've worked with (across languages including Java, Scala, Javascript/Typescript, Python). Your criticism is perhaps relevant in relation to language features (though I would disagree with your position) but has no basis in relation to tooling and the standard library, both of which are best in class. reply spiderice 21 minutes agorootparent> Java, Scala, Javascript/Typescript, Python You seem to only use languages with bad dependency management. Which sounds tongue in cheek, but it's true. These are the languages (along with Go) where people hate the dependency management solutions. reply larusso 3 hours agorootparentprevI think in this case you should give rust a try if only for cargo. Because the mentioned issues are non existent there. Also because it’s the language mostly referred to as being completely on the other side of the spectrum when it comes to language design philosophy. reply dkarl 3 hours agorootparentprevSpeaking as someone who comes from the opposite end of the spectrum (Scala both professionally and by personal preference) and who doesn't enjoy Go as a language, I think there's a lot to be said for a language that evolves that way. Being able to make decisions with literally years of hindsight is powerful. Looking at the top critiques here and how they've been addressed, this seems like a pretty thoroughly baked approach. I would rather scratch my eyes out than use Go for the kind of code I write day-to-day, but Go looks amazing for the kinds of things where you can achieve high levels of boringness, which is what we should all be striving for. reply rednafi 23 minutes agorootparentDifferent languages speak to different people. I feel the same way about Scala and most other functional languages. To me, they’re fun and all, but I wouldn’t build anything large-scale with them. My problem space is interesting enough that I don’t have to worry about Go being boring. reply Cthulhu_ 4 hours agorootparentprevThis isn't exactly true, as the dependency management was simple clone the relevant project. Dependencies being source is great as it encourages open source. Of course, the problem with that was that it was pretty much impossible to version properly, causing breakages and such. reply Thaxll 3 hours agorootparentprevgo mod is best in class dependecy manager, it is way better than what most mainstream languages have, it's not even close. I worked with C++/ruby/python/js/java and C#, go mod is superior to all them. The one that is similar is Cargo ( rust ). reply pragma_x 3 hours agorootparentprevThis situation has improved over the last 9+ years or so. I agree that's where things started and I felt much the same way at the time. That said, I still strongly dislike that the Go ecosystem conflates a library's repo URL with it's in-language URI. Having `github.com` everywhere in your sourcecode just ignores other use-cases for dependency management, like running an artifactory in an enterprise setting. My point being: there's still room for improvement. reply dingnuts 4 hours agorootparentprevIf you don't like Go, why did you come and comment in this thread? didn't your mother ever tell you that if you don't have something nice to say, you shouldn't say anything? at least Go doesn't have virtualenvs reply herval 1 hour agorootparent...yet reply herval 1 hour agorootparentprevGo has been a masterclass in disparaging industry learnings as \"unnecessary\" (dep management, generics, null safety), then gradually bolting them into the language. It's kinda hilarious to watch. reply devmor 43 minutes agorootparentprevBecause everyone has their own opinion about it, like most other standards. Personally, I think the way PHP handles dependencies is vastly preferable to every other ecosystem I've developed in, for most types of development but I know its somewhat inflexible approach would be a headache for some small percentage of developers too. reply Defletter 11 hours agoparentprev> Using \"go tool\" forces you to have a bunch of dependencies in your go.mod that can conflict with your software's real dependency requirements, when there's zero reason those matter. You shouldn't have to care if one of your developer tools depends on a different version of a library than you. Heh, were the people who made 'go tool' the same people who made Maven? Would make sense :P reply larusso 3 hours agorootparentAt least build time dependencies are separated from runtime dependencies. reply nixosbestos 6 hours agoparentprevI ask this out of curiosity, not accusation, do you work for Flox? Can't say I've ever seen it mentioned \"in the wild\". reply peterldowns 8 hours agoparentprevI agree — tools should be shared artifacts that the team downloads and can guarantee are the same for everyone. I usually setup a flake.nix for everyone but flox, earthly, devenv, jetify, are all great alternatives. Ideally your tools are declaratively configured and shared between your team and your CI/CD environment, too. The `go tool` stuff has always seemed like a junior engineering hack — literally the wrong tool for the job, yeah sure it gets it working, but other than that it's gross. reply arccy 7 hours agorootparentwell now it leans in to go's reproducible tooling so it's declaratively configured and shared between your team and your CI/CD environment too. reply peterldowns 5 hours agorootparentThere are many tools that arent written in golang, and therefore not manageable via this approach. I don’t really understand why I would use this. reply arccy 8 hours agoparentprevcontrol the go version with the `toolchain` directive, replace protoc with buf.build (which is way better anyway). reply amukher1 17 hours agoprevFound this a lot easier to follow. https://blog.howardjohn.info/posts/go-tools-command/ And didn't quite understand the euphoria. reply gr4vityWall 3 hours agoparentThe feature itself seems reasonable and useful. Specially if most of your tooling is written is Go as well. But this part caught my attention: > user defined tools are compiled each time they are used Why compile them each time they are used? Assuming you're compiling them from source, shouldn't they be compiled once, and then have the 'go tool' command reuse the same binaries? I don't see why it compiles them at the time you run the tool, rather than when you're installing dependencies. The benchmarks show a significant latency increase. The author also provided a different approach which doesn't seem to have any obvious downsides, besides not sharing dependency versions (which may or may not be a good thing that's a separate discussion IMO). reply kbolino 31 minutes agorootparentThere is a cache and they're aren't re-compiled unless they change or the cache is cleared. reply mook 13 hours agoparentprevThere's also the (draft) release notes: https://go.dev/doc/go1.24 And the docs: https://go.dev/doc/modules/managing-dependencies#tools I've done the blank import thing before, it was kinda awkward but not _that_ bad. reply pragma_x 3 hours agoparentprevAh, okay. So this achieves yet more feature parity with npm/yarn? Sweet. reply Groxx 16 hours agoparentprev\"Shared dependency state\" was my very first thought when I heard about how it was built. Yeah I want none of that. I'll stick with my makefiles and a dedicated \"internal/tools\" module. Tools routinely force upgrades that break other things, and allowing that is a feature. reply nikolayasdf123 16 hours agorootparentsame. tools are not part of the codebase, nor dependencies. you got to have isolation of artefact and tools around to work with it. it is bonkers to start versioning tools used to build project mixed with artefact dependencies itself. should we include version of VSCode used to type code? how about transitive dependencies of VSCode? how about OS itself to edit files? how about version of LLM model that generated some of this code? where does this stop? reply MathMonkeyMan 12 hours agorootparentThe state of things with some projects I've touched is \"We have a giant CI thing that does a bunch of things. It is really very large. It might or might not be broken, but we work around it and it's fine.\" I think some of the euphoria around tracking tooling in the repo is \"yes, now I can run a command in the repo and it's as if I spun up a docker container with the CI locally, but it's just regular software running on my machine!\" This is a huge improvement if you're used to the minutes-or-hours-long CI cycle being your only interaction with the \"real environment.\" The reductio ad absurdum that you describe is basically \"a snapshot of the definitions of all the container images in our CI pipeline.\" It's not a ridiculous example, it's how many large projects run. reply nikolayasdf123 11 hours agorootparentI am with you on the same boat that this better be versioned and reproducible and standardised. my key concern whether tools to build project have to be in the same pool as project itself (that may or may not use tools to build/edit/maintain/debug it). reply SOLAR_FIELDS 11 hours agorootparentprevIt makes sense to some extent when the toolchain can tap into native language specific constructs when you need that REPL-like iteration loop to be tight and fast. But that kind of thing is probably only required in a small subset of the kind of tooling that gets implemented. The tradeoff with this approach is that you lose any sort of agnosticism when you drop into the language specific tooling. So now if you work at a corporation and have to deal with multiple toolchains every engineer now needs to learn and work with new build tooling X times for each supported language. This always happens to some extent there’s always going to be some things that use the language’s specific task runner constructs but keeping that minimal is usually a good idea in this scenario. Your complaint feels to me that it is about poorly implemented CI systems that heavily leverage container based workflows (of which there are many in the wild). If implemented properly with caching, really the main overhead you are paying in these types of setups is the virtualization overhead (on macs) and the cold start time for the engine. For most people and in most cases neither will make a significant difference in the wall clock time of their loop, comparatively. reply Ferret7446 12 hours agorootparentprevThis take absolute boggles the mind. You don't want people compiling your code with different versions of tools so you have to debug thousands of potential combinations of everything. You don't want people running different versions of formatters/linters that leave conflicting diffs throughout your commit history. reply nikolayasdf123 11 hours agorootparentso where does it stop? let's include version of OS on laptop of people who edit code? it is getting ridiculous. you got to draw a line somewhere. in my opinion, \"if dependency code is not linked nor compiled-into nor copied as a source (e.g. model weights, or other artefacts) then it must not be included into dependency tree of project source code\" that still means, you are free to track versions/hashes/etc. of tools and their dependencies. just do it separately. reply IshKebab 6 hours agorootparentIdeally it stops at the point where the tools actually affect your project. Does everyone need to use the same IDE? Obviously not. Same C++ compiler? Ideally yes. (And yes you can do that in some cases, e.g. Bazel and its ilk allow you to vendor compilers.) reply broken-kebab 8 hours agorootparentprevIt's not that uncommon to have OS version fixed in CI/CD pipelines. E.g. a build process intended to produce artefacts for Apple's Appstore is dependent on XCode, and XCode maybe forced to upgrade in case of MacOS upgrade, and it may break things. So the OS version becomes a line in requirements. It's kinda disappointing, but it's the real state of affairs. reply nikolayasdf123 8 hours agorootparenthow about hardware that software is supposed to run on? that certainly can have effect. let's pin that too into project repo. don't want to continue this thread. but to re-iterate my point, you have to stop somewhere what to pin/version and what to not. I think my criteria is reasonable, but ultimately it is up to you. (so as long whole ecosystem and dependency trees do not become bags of everything, in which case Go here is alright so far. after digging deeper what v1.24 proposing will not cause massive dependency-apocalypsis of tools propagating everywhere and only what you actually use in main is going to be included in go.mod, thanks to module pruning). reply arccy 8 hours agorootparentprevyes let's have a meta project that can track the version of my tools \"separately\", and the version of my repo. linters, formatters, reproducible codegen should be tracked. their output is deterministic and you want to enforce that in CI in case people forget. the rest doesn't really affect the code (well windows OS and their CRLF do but git has eol attributes to control that). reply nikolayasdf123 7 hours agorootparentagree it has to be deterministic. my major concern was whether tools dependencies are mixed with actual software they are used to build. hopefully they are not mixed, so that you can have guarantees that everything you see in dependency tree is actually used. because otherwise, there is not much point in the tree (since you can't say if it is used or not). again, v1.24 seems to be okay here. go mod pruning should keep nodes in tree clear from polluting each other. reply broeng 6 hours agorootparentprevTbh, I think you missed this part: \"[..] mixed with artefact dependencies itself\". reply JeffMcCune 3 hours agorootparentAn artifact depends on the tools used to build it. This is why we pin versions. Go tool is common sense, allowing for any old tool version in the build chain invites failure. reply rob74 12 hours agorootparentprevNo one says that this is a one-size-fits-all solution, but for some use cases (small tools that are intimately connected to the rest of the codebase, even reuse some internal code/libraries) it's probably helpful... reply 0x696C6961 15 hours agorootparentprevThe way it's implemented is the way that almost everyone already does it. It's just more convenient now. reply Groxx 14 hours agorootparent\"Popular\" and \"good\" have no relation to each other. They correlate fairly well, but that's all. Blending those dependencies already causes somewhat frequent problems for library owners / for users of libraries that do this. Encouraging it is not what I would consider beneficial. reply 0x696C6961 17 hours agoparentprevYeah it's a nice QOL improvement. Not some game changer ... reply bjackman 20 hours agoprevI always think it's a shame that these features end up getting built into ecosystem-specific build tools. Why do we need separate build systems for every language? It seems entirely possible to have build system that can do all this stuff for every language at once. From my experience at Google I _know_ this is possible in a Megamonorepo. I have briefly fiddled with Bazel and it seems there's quite a barrier to entry, I dunno if that's just lack of experience but it didn't quite seem ready for small projects. Maybe Nix is the solution but that has barrier to entry more at the human level it just seems like a Way of Life that you have to dive all the way into. Nonetheless, maybe I should try diving into one or both of those tools at some point. reply sunshowers 20 hours agoparent(I worked on source control at FB for many years.) The main argument for not overly genericizing things is that you can deliver a better user experience through domain-specific code. For Bazel and buck2 specifically, they require a total commitment to it, which implies ongoing maintenance work. I also think the fact that they don't have open governance is a hindrance. Google's and Meta's internal monorepos make certain tradeoffs that don't quite work in a more distributed model. Bazel is also in Java I believe, which is a bit unfortunate due to process startup times. On my machine, `time bazelisk help` takes over 0.75 seconds to run, compared to `time go help` which is 0.003 seconds and `time cargo help` which is 0.02 seconds. (This doesn't apply to buck2, which is in Rust.) reply spockz 12 hours agorootparentGraalVM’s native image has been a thing for a while now. This could overcome the daemon issue partially. The daemon does more ofc by as it keeps some state in memory. But at least the binary start time is a solved problem in Java land. reply jeffbee 19 hours agorootparentprevThis is likely because you are running it in some random PWD that doesn't represent a bazel workspace. When running in a workspace the bazel daemon persists. Inside my workspace the bazelisk help invocation needs just 30ms real time. Running bazel outside of a bazel workspace is not a major use-case that needs to be fixed. reply thayne 13 hours agorootparent> When running in a workspace the bazel daemon persists. Inside my workspace the bazelisk help invocation needs just 30ms real time. It still has a slow startup time, bazel just works around that by using a persistent daemon, so that it is relatively fast after as long as the daemon is running. reply sunshowers 19 hours agorootparentprevThat's good to know, thank you! Do you encounter cache invalidation bugs with daemonization often? I've had pretty bad experiences with daemonized dev tools in the past. reply jrockway 17 hours agorootparentBazel prints a message when you invalidate the in-memory cache in a perhaps accidental way; you can supply it with a flag to make this an error and skip the cache invalidation. If you try to run two Bazel invocations in parallel in the same workspace, one waits for the other to be done. reply jeffbee 17 hours agorootparentI assumed they meant an error of improperly using cached results. I am sure bazel has its flaws but it assiduously avoids that. reply __float 15 hours agorootparentYes, unless you're using persistent workers. Then you may very well run into the same issues they mention. reply pornel 18 hours agoparentprev> Why do we need separate build systems for every language? Because being cross-language makes them inherit all of the complexity of the worst languages they support. The infinite flexibility required to accommodate everyone keeps costing you at every step. You need to learn a tool that is more powerful than your language requires, and pay the cost of more abstraction layers than you need. Then you have to work with snowflake projects that are all different in arbitrary ways, because the everything-agnostic tool didn't impose any conventions or constraints. The vague do-it-all build systems make everything more complicated than necessary. Their \"simple\" components are either a mere execution primitive that make handling different platforms/versions/configurations your problem, or are macros/magic/plugins that are a fractal of a build system written inside a build system, with more custom complexity underneath. OTOH a language-specific build system knows exactly what that language needs, and doesn't need to support more. It can include specific solutions and workarounds for its target environments, out of the box, because it knows what it's building and what platforms it supports. It can use conventions and defaults of its language to do most things without configuration. General build tools need build scripts written, debugged, and tweaked endlessly. A single-language build tool can support just one standard project structure and have all projects and dependencies follow it. That makes it easier to work on other projects, and easier to write tooling that works with all of them. All because focused build system doesn't accommodate all the custom legacy projects of all languages. You don't realize how much of a skill-and-effort black hole build scripts are is until you use a language where a build command just builds it. reply bjackman 10 hours agorootparentBut this just doesn't match my experience with Blaze at all. For my internal usage with C++ & Go it's perfect. For the weird niche use case of building and packaging BPF programs (with no support from the central tooling teams, we had to write our own macros) it still just works. For Python where it's a poor fit for the language norms it's a minor inconvenience but still mostly stays out of the way. I hear Java is similar. For vendored open source projects that build with random other tools (CMake, Nix, custom Makefile) it's a pain but the fact that it's generally possible to get them building with Blaze at all says something... Yes, the monorepo makes all of this dramatically easier. I can consider \"one-build-tool-to-rule-them-all isn't really practical outside of a monorepo\" as a valid argument, although it remains to be proven. But \"you fundamentally need a build tool per language\" doesn't hold any water for me. > That makes it easier to work on other projects, and easier to write tooling that works with all of them. But... this is my whole point. Only if those projects are in the same language as yours! I can see how maybe that's valid in some domains where there's probably a lot of people who can just do almost everything on JS/TS, maybe Java has a similar domain. But for most of us switching between Go/Cargo/CMake etc is a huge pain. Oh btw, there's also Meson. That's very cross-language while also seeming extremely simple to use. But it doesn't seem to deliver a very full-featured experience. reply pornel 6 hours agorootparentI count C++ projects in the \"worst\" bucket, where every project has its own build system, its own structure, own way to run tests, own way to configure features, own way to generate docs. So if a build system works great for your mixed C++ projects, your build system is taking on the maximum complexity to deal with it, and that's the complexity I don't want in non-C++ projects. When I work with pure-JS projects, or pure-Go projects, or pure-Rust projects, I don't need any of this. npm, go, and rust/cargo packages are uniform, and trivial to build with their built-in basic tools when they don't have C/C++ dependencies. reply munificent 18 hours agoparentprevI think the problem is basically because the build system has to be implemented using some ecosystem, and no other ecosystem wants to depend on that one. If your \"one build system to rule them all\" was built in, say, Ruby, the Python ecosystem won't want to use it. No Python evangelist wants to tell users that step 1 of getting up and running with Python is \"Install Ruby\". So you tend to get a lot of wheel reinvention across ecosystems. I don't necessarily think it's a bad thing. Yes, it's a lot of redundant work. But it's also an opportunity to shed historical baggage and learn from previous mistakes. Compare, for example, how beloved Rust's cargo ecosystem is compared the ongoing mess that is package management in Python. A fresh start can be valuable, and not having a monoculture can be helpful for rapid evolution. reply pansa2 17 hours agorootparent> No Python evangelist wants to tell users that step 1 of getting up and running with Python is \"Install Ruby\". True, but the Python community does seem to be coalescing around tools like UV and Ruff, written in Rust. Presumably that’s more acceptable because it’s a compiled language, so they tell users to “install UV” not “install Rust”. reply makapuf 11 hours agorootparentNote that installing python stdlib installs tkinter and thus tcl. https://wiki.tcl-lang.org/page/Python-Tcl-Interactions reply pjmlp 13 hours agorootparentprevI tend to think that is more Rust community using Python, and RIIR stuff, than Python community themselves. I know Python since version 1.6, and this has never been a thing until Rust. Same applies to the RIIR going on JavaScript side. Including tools that were already written in compiled languages, but of course weren't Rust, or had an idea to make a startup around them. reply 6keZbCECT2uB 16 hours agorootparentprevPartly in jest, you can often find a Perl / bash available where you can't find a Python, Ruby, or Cargo. reply spauldo 4 hours agorootparentNot sure why that's in jest. Perl is pretty much everywhere and could do the job just fine. There's lots of former (and current) Perl hackers still around. reply marwis 17 hours agorootparentprevSounds like the only way out of this is to design language agnostic tooling protocols that anybody can implement. reply jlarsen 19 hours agoparentprevI've had exactly the same thought, after hitting walls repeatedly with limitations in single-language ecosystems. And likewise, I've had the same concerns around the complexity that comes with Bazel/Buck/Nix. It's been such a frustration for me that I started writing my own as a side project a year or two ago, based on a using a standardized filesystem structure for packages instead of a manifest or configuration language. By leaning into the filesystem heavily, you can avoid a lot of language lock-in and complexity that comes with other tools. And with fingerprint-based addressing for packages and files, it's quite fast. Incremental rebuild checks for my projects with hundreds of packages take only 200-300ms on my low-end laptop with an Intel N200 and mid-tier SSD. It's an early stage project and the documentation needs some work, but if you're interested: https://github.com/somesocks/dryad https://somesocks.github.io/dryad/ One other alternative I know of that's multi-language is Pants(https://www.pantsbuild.org/), which has support for packages in several languages, and an \"ad-hoc\" mode which lets you build packages with a custom tool if it isn't officially supported. They've added support for quite a few new tools/languages lately, and seem to be very much an active project. reply 8n4vidtmkvmk 9 hours agorootparentNot loving the cutesy names (https://somesocks.github.io/dryad/docs/02-concepts/01-the-ga...). I want my build tool to be boring. reply 6keZbCECT2uB 20 hours agoparentprevI agree. In my opinion, if you can keep the experience of Bazel limited to build targets, there is a low barrier to entry even if it is tedious. Major issues show up with Bazel once you start having to write rules, tool chains, or if your workspace file talks to the Internet. I think you can fix these issues by using a package manager around Bazel. Conda is my preferred choice because it is in the top tier for adoption, cross platform support, and supported more locked down use cases like going through mirrors, not having root, not controlling file paths, etc. What Bazel gets from this is a generic solution for package management with better version solving for build rules, source dependencies and binary dependencies. By sourcing binary deps from conda forge, you get a midpoint between deep investment into Bazel and binaries with unknown provenance which allows you to incrementally move to source as appropriate. Additional notes: some requirements limit utility and approach being partial support of a platform. If you require root on Linux, wsl on Windows, have frequent compilation breakage on darwin, or neglect Windows file paths, your cross platform support is partial in my book. Use of Java for Bazel and Python for conda might be regrettable, but not bad enough to warrant moving down the list of adoption and in my experience there is vastly more Bazel out there than Buck or other competitors. Similarly, you want to see some adoption from Haskell, Rust, Julia, Golang, Python, C++, etc. JavaScript is thorny. You really don't want to have to deal with multiple versions of the same library with compiled languages, but you have to with JavaScript. I haven't seen too much demand for JavaScript bindings to C++ wrappers around a Rust core that uses C core libraries, but I do see that for Python bindings. reply sunshowers 20 hours agorootparent> You really don't want to have to deal with multiple versions of the same library with compiled languages, but you have to with JavaScript. Rust handles this fine by unifying up to semver compatibility diamond dependency hell is an artifact of the lack of namespacing in many older languages. reply 6keZbCECT2uB 16 hours agorootparentConda unifies by using a sat solver to find versions of software which are mutually compatible regardless of whether they agree on the meaning of semver. So, both approaches require unifying versions. Linking against C gets pretty broken without this. The issue I was referring to is that in Javascript, you can write code which uses multiple versions of the same library which are mutually incompatible. Since they're mutually incompatible, no sat-solve or unifyer is going to help you. You must permit multiple versions of the same library in the same environment. So far, my approach of ignoring some Javascript libraries has worked for my backend development. :) reply dwattttt 14 hours agorootparentRust does permit multiple incompatible versions of the same library in the same environment. The types/objects from one version are distinct from the types/objects of the other, it's a type error to try mix them. But you can use two versions of the same library in your project; I've done it by giving one of them a different name. reply lihaoyi 18 hours agoparentprevMy experience with Bazel is it does everything you need, and works incredibly well once set up, but is ferociously complex and hard to learn and get started with. Buck and Pants are easier in some ways, but fundamentally they still look and feel mostly like Bazel, warts and all I've been working on an alternate build tool Mill (https://www.mill-build.org) tries to provide the 90% of Bazel that people need at 10% the complexity cost. From a greenfield perspective a lot of work to try and catch up to Bazel's cross-language support and community. I think we can eventually get there, but it will be a long slog reply morepedantic 13 hours agoparentprevBrazil performs dependency resolution in a language-agnostic way. https://gist.github.com/terabyte/15a2d3d407285b8b5a0a7964dd6... reply rednafi 9 hours agoprevI like that Go decided to natively support this. But since it’s keeping the dev dependencies in the same go.mod, won’t it make the binary larger? In Python’s uv, the pyproject.toml has separate sections for dev and prod dependencies. Then uv generates a single lock file where you can specify whether to install dev or prod deps. But what happens if I run ‘go run’ or ‘go build’? Will the tools get into the final artifact? I know Python still doesn’t solve the issue where a tool can depend on a different version of a library than the main project. But this approach in Go doesn’t seem to fix it either. If your tool needs an older version of a library, the single go.mod file forces the entire project to use the older version, even if the project needs—or can only support—a newer version of the dependency. reply catlifeonmars 9 hours agoparent> won’t it make the binary larger? No. The binary size is related to the number of dependencies you use in each main package (and the dependencies they use, etc). It does not matter how many dependencies you have in your go.mod. reply rednafi 9 hours agorootparentAh, thanks. This isn't much of an upgrade from the `tools.go` convention, where the tools are underscore-imported. All it does is provide an indication in the `go.mod` file that some dependencies come from tools. Plus, `go tool ` is slower than `./bin/`. Not to mention, it doesn’t resolve the issue where tools might use a different version of a dependency than the app. reply kbolino 20 minutes agorootparentgo tool is only slower when (re-)compilation is needed, which is not often. You'd have to pay the same price anyway at some point to build the binary placed in ./bin. reply pragma_x 2 hours agorootparentprevExactly. You lose build isolation for those tools, but you have the convenience of shipping something tested and proven (ideally) alongside the project they support. At the same time, this mess all stays isolated from the parent environment which may be the bigger fight that devs have on their hands not everyone is using Nix or container isolation of some sort. I also see this as sugar for `go build` or even `go run`. Or as something way easier than the `go generate` + `//go:generate go run` hack. So we can look at this as a simple refinement for existing practices. reply imiric 21 hours agoprevThis seems handy, but often the tools run by `go generate` are outside of the Go ecosystem, or need to be binaries. So I think a general solution would work better, and not be limited to Go. There are plenty of tools in this space to choose from: mise, devenv, Nix, Hermit, etc. reply TheCondor 4 hours agoparentMise is right on the edge of being pretty killer. I’m bullish on it. It also includes a lot of nice to haves that you can declare, like k9s, which isn’t exactly a dev tool but becomes expected reply arccy 17 hours agoparentprevbetter motivation for rewrite it in go... but are there really that many tools you need in a go project not written in go? reply WuxiFingerHold 9 hours agoprevI don't understand why it's a good idea to couple tooling or configuration or infrastructure (e.g. Aspire.NET, which I'm also not convinced of being a good idea) so tightly with the application. An application should not need to be aware of how whatever tools are implemented or how configuration or infrastructure is managed. The tooling should point to the application as dependency. The application should not have any dependency on tooling. reply remram 21 hours agoprevSo it's just dev-dependencies? reply nikolayasdf123 16 hours agoparenta bit worse. it is all mixed up. to keep separate dependency tree for tools need use old approach with go.mod. it is actually even worse now. reply nikolayasdf123 10 hours agorootparentUPD: 1. it is single tree 2. BUT tools will not propagate through the dependency tree downstream due to go module pruning check this comment: https://github.com/golang/go/issues/48429#issuecomment-26184... official docs: https://tip.golang.org/doc/modules/managing-dependencies#too... > Due to module pruning, when you depend on a module that itself has a tool dependency, requirements that exist just to satisfy that tool dependency do not usually become requirements of your module. reply remram 4 hours agorootparent\"usually\"? reply silverwind 18 hours agoparentprevYes, except it does not support version ranges. reply verdverm 18 hours agorootparentGo uses Minimum Version Selection (MVS) instead of a SAT solver. There are no ranges in any go dependency specifications. It's actually a very simple and elegant algorithm for dependency version selection https://research.swtch.com/vgo-mvs reply 8n4vidtmkvmk 9 hours agorootparent> Minimal version selection assumes that each module declares its own dependency requirements: a list of minimum versions of other modules. Modules are assumed to follow the import compatibility rule—packages in any newer version should work as well as older ones—so a dependency requirement gives only a minimum version, never a maximum version or a list of incompatible later versions. That sounds like a non-starter. You almost never want to to unintentionally 'upgrade' to the next 'major' ver. There's also occasionally broken or hacked/compromised minor vers. reply arccy 8 hours agorootparentthankfully major versions in go have different names (module, module/v2, module/v3) enforced by tooling, so you'll never upgrade to the next major (except v0 > v1). reply jamietanna 21 hours agoparentprevYep, that's the intent reply the_gipsy 19 hours agoparentprevlol yea reply tumetab1 10 hours agoprevI appreciate that \"tools\" that are used to build the final version of a module/cli/service are explicitly managed through go.mod. I really dislike that now I'm going to have two problems, managing other tools installed through a makefile, e.g. lint, and managing tools \"installed\" through go.mod, e.g. mocks generators, stringify, etc. I feel like this is not a net negative on the ecosystem again. Each release Golang team adds thing to manage and makes it harder to interact with other codebases. In this case, each company will have to decide if they want to use \"go tool\" and when to use it. Each time I clone an open source repo I'm going to have to check how they manage their tools. reply puika 11 hours agoprevMy current approach has been setting GOBIN to a local project bin via direnv and go installing bins there. install commands themselves are cached by me with a naive checksum check for the install script itself when I run my commands. Therefore all `go install`s run in parallel if I edit the install script, and go decides what to reinstall or not. At this point I don't feel it's worth migrating to `go tool` having this setup, we'll see when it's stable reply nikolayasdf123 10 hours agoprevwhat is also concerning, Go team years ago did small vote, small survey of positive occurrences, and decided to enforce it globally for anyone. old design give people option to use `tools.go` approach, or other, or nothing at all. now they are enforcing this `tools.go` standard. Go looks to be moving into very restrictive territories. what about surveying opposing views? what about people who did not use `tools.go` what is going on in Google, Go team? reply nikolayasdf123 8 hours agoparentUPD: check this github comment. https://github.com/golang/go/issues/48429#issuecomment-26184... basically it go tool relies heavily on go module pruning, so transitive dependencies from tools are not propagated downstream. also, official docs say this: https://tip.golang.org/doc/modules/managing-dependencies#too... > Due to module pruning, when you depend on a module that itself has a tool dependency, requirements that exist just to satisfy that tool dependency do not usually become requirements of your module. reply arccy 8 hours agoparentprevyou don't have to use this feature if you don't like it... reply nikolayasdf123 8 hours agorootparentI was so concerned, because it seemed to me \"you will not have any other choice\". after digging deeper, it is alright reply gqgs 3 hours agoprevI don't love the pollution in the go.mod or being forced to have multiple files to track dependencies. Being able to run tools directly with go generate run [1] already works well enough and I frankly don't need see any benefits compared to it in this new approach. [1] https://github.com/golang/go/issues/42088 reply favadi 9 hours agoprevIt doesn't look like much of an improvement over `tools/tools.go` with blank imports like this: ``` //go:build tools package tools import ( _ \"github.com/xxx/yyy\" ... ) ``` reply pjmlp 13 hours agoprevThis has been a thing in dotnet tool for years, now. reply Cthulhu_ 3 hours agoparentI think it's great that there's prior art that can be used as examples, proof of value, and opportunities for learning. I commend .NET for investing the resources in researching and developing this feature. reply nikolayasdf123 11 hours agoprevproposal is closed and accepted by Go team. [~sigh] you can still leave comments in discussion issue: https://github.com/golang/go/issues/48429 reply karel-3d 9 hours agoprevI don't like this. When I install a tool, I want to use it with their dependency versions at the moment they released it. When I use `go tool`, it uses whatever I have in go.mod; and, in the opposit way, it will update my go.mod for no real reason. But some people do this right now with tools.go, so... whatever, it's a better version of tools.go pattern. And I can still do it my preffered way with `go install @version` in makefile. So, eh. reply globular-toast 11 hours agoprevA note for the author in case they are reading: \"i.e.\" means \"that is\", \"e.g.\" means \"for example\". You should be able to substitute these meanings and find the sentence makes sense. In all cases here you wanted \"e.g.\". reply dankobgd 7 hours agoprevSince i started using nix devShells this is kind of useless. What if i have 1 tool that isn't go tool so what do i do with it? so i have that one \"exception\" and here we go again... reply Cthulhu_ 3 hours agoparentThis is actually a valid downside of Go, in that it has its own set of tools for things like debugging and whatnot instead of being compatible with existing tools. reply jen20 6 hours agoparentprevAnd why is that tool always protoc? reply indulona 9 hours agoprevI wish Go Team would focus on performance rather thann adding new features that nobody asked for. The http stack is barely able to beat NodeJS these days, ffs. reply Cthulhu_ 3 hours agoparentThe Go HTTP stack is not optimized for raw performance though; there's a number of alternative HTTP stacks written in Go optimized for performance, like fasthttp (https://github.com/valyala/fasthttp). (source: https://www.techempower.com/benchmarks/) Likewise, the standard library NodeJS http stack will not be as performant as a performance optimized alternative. That said, if raw performance is your primary concern, neither Go nor NodeJS will be good enough. There's many more factors to consider. reply hu3 7 hours agoparentprevnodejs' v8 has millions of man hours spent in optimzation alone since half the internet frontend runs on v8, if not more. Go being garbage collected and still beating v8 is one hell of an achievement. If you need faster Go Http you're using the wrong tool. reply syvolt 21 hours agoprevI have tested it and probably will use it but the fact that it pollutes your go.mod's indirect dependency list (without any distinction indicating it's for a tool) is very annoying. reply cirwin 20 hours agoparentPrimary contributor to the feature here. We went back on forth on this a lot, but it boiled down to wanting only one dependency graph per module instead of two. This simplifies things like security scanners, and other workflows that analyze your dependencies. A `// tool` comment would be a nice addition, it's probably not impossible to add, but the code is quite fiddly. Luckily for library authors, although it does impact version selection for projects who use your module; those projects do not get `// indirect` lines in their go.mod because those packages are not required when building their module. reply syvolt 18 hours agorootparentThank you for working on it. It is a nice feature and still better than alternatives. I'm not a library author and I try to be careful about what dependencies I introduce to my projects (including indirect dependencies). On one project, switching to `go tool` makes my go.mod go from 93 lines to 247 (excluding the tools themselves) this makes it infeasible to manually review. If I'm only using a single feature of a multi-purpose tool for example, does it matter to me that some unrelated dependency of theirs has a security issue? reply arccy 7 hours agorootparentit probably doesn't, and good vulnerability scanners like govulncheck from the go team won't complain about them, because they're unreachable from your source code. now, do you care about some development tool you're running locally has a security issue? if yes, you needed to update anyway, if not, nothing changes. reply wakawaka28 18 hours agorootparentprev>If I'm only using a single feature of a multi-purpose tool for example, does it matter to me that some unrelated dependency of theirs has a security issue? How is anyone supposed to know whether there's an issue or not? To simplify things, if you use the tool and the dependency belongs to the tool, then the issue can affect you. Anything more advanced than that requires analyzing the code. reply syvolt 17 hours agorootparentWhat if I'm already using techniques, such as sandboxing, to prevent the tools from doing anything unexpected? Why bring this entire mess of indirect dependencies into my project if I'm just using a tool to occasionally analyze my binary's output size? Or a tool to lint my protobuf files? reply wakawaka28 16 hours agorootparentIf it's a build dependency, then you have to have it. If you don't like the size of the tool then take it up with the authors. I'm not a Go programmer by the way, this is all just obvious to me. reply syvolt 16 hours agorootparentThe functionality we're discussing can be used for tools that are not build dependencies. They may be important for your project and worth having contributors be on the same version but not part of the build. It will still add the dependencies of those tools as indirect dependencies to your go.mod file, that is what's being discussed. reply wakawaka28 6 hours agorootparentIf you use the tool to develop your project then it is basically a build dependency. That is a sweeping generalization, but it's essentially correct in most cases. reply verdverm 18 hours agorootparentprevIn addition, a good dependency security scanning tool can analyze reachability to answer this question for you reply syvolt 17 hours agorootparentReachability analysis on a tool that could be called by something outside of the project? We're talking about tools here after all anything that can run `go tool` in that directory can call it. The go.mod tool entry could just be being used for versioning. reply verdverm 17 hours agorootparentI'm speaking of tools and processes independent of this \"go tool\" stuff that we already use in our CI pipelines Big fan of Dagger over this go tool thing I generally loath the use of comments for things other than comments reply wakawaka28 6 hours agorootparentprevThat is a bit much to ask for IMO. In any case, the project may not be aware of how any given developer will use the tool. So who is to say that if you change the order of two parameters to the tool, the tool might not take a different path and proceed to hack your computer? You really don't want any of this problem. What you should ask for is for the tools' dependencies to be listed separately, and for each tool to follow the Unix philosophy of \"do one thing well.\" reply verdverm 1 hour agorootparent> for each tool to ... \"do one thing well.\" There is a lot of merit to this statement, as applied to `go tool` usage and to security scanning. Just went through a big security vendor analysis and POCs. In the middle I saw Filippo Valsorda post [1] about false positives from the one stop shops, while govulncheck (language specific) did not have them. At the same time, there was one vendor who did not false positive with the reachability checks on vulns. While not always as good, one-stop-shops also add value by removing a lot of similar / duplicated work. Tradeoffs and such... [1] https://bsky.app/profile/filippo.abyssdomain.expert/post/3ld... reply eadmund 6 hours agorootparentprev> it boiled down to wanting only one dependency graph per module instead of two Did you consider having tool be an alias for indirect? That would have kept a single dependency graph per module, while still enabling one reading one’s go.mod by hand rather than using ‘go mod’ to know where each dependency came from and why? I know, a random drive-by forum post is not the same as a technical design … reply caust1c 16 hours agorootparentprevHaving not looked at it deeply yet, why require building every time it's invoked? Is the idea to get it working then add build caching later? Seems like a pretty big drawback (bigger than the go.mod pollution, for me). Github runners are sllooooow so build times matter to me. reply cirwin 15 hours agorootparent`go tool` doesn't require a rebuild, but it does checking that the tool is up-to-date (which requires doing at least a bit of work). This is one of the main advantages of using `go tool` over the \"hope that contributors to have the right version installed\" approach. As the version of the tool required by the project evolves, it continues to work. Interestingly, when I was first working on the proposal, `go run` deliberately did not cache the built binary. That meant that `go tool` was much faster because it only had to do the check instead of re-running the `link` step. In Go 1.24 that was changed (both to support `go tool`, but also for some other work they are planning) so this advantage of `go tool` is not needed anymore. reply caust1c 1 hour agorootparentThanks for the explanation and contribution! Very much appreciated :-) reply arccy 20 hours agoparentprevThere's module graph pruning https://go.dev/ref/mod#graph-pruning reply movedx 20 hours agoparentprevWhat solution would you propose? reply rplnt 20 hours agorootparentAt least a comment if not its own section. reply jamietanna 21 hours agoparentprevYeah, I'm still rather excited about it, but less-so given it /does/ impact the `go.mod` for consumers reply 21 hours agoprevnext [10 more] [dead] gouggoug 21 hours agoparentI came to the comment section for this comment. Dark background, too many colors, inconsistent spacing, inconsistent font-size and/or family, some links appear fully pink with pink underline, some links aren't pink and only have the underline, inlineis blue, but large code blocks are the same color as regular text – on black background, etc. I had to stop reading unfortunately. reply jamietanna 21 hours agoparentprevSorry to hear that are there any particular tweaks you think would work to reduce the impact? Is it i.e. the blue used by code snippets? Or because there's also the diff syntax which has green/reds? reply throitallaway 20 hours agorootparentI'm not OP, but my brain also quickly noped out of reading that page. I can appreciate the care that went into the formatting of commands and links, but it's a bit much to parse all at once. I think monospaced/preformatted text usually looks best with a different background (like the dedicated code blocks towards the end.) Also on my browser the preformatted text is decently larger than the normal paragraph text. This combined with the blue color is a bit jarring. reply lelandfe 21 hours agorootparentprevChoosing colors is like making music. This color scheme feels discordant, like a jumble of loud notes. Maybe try looking at color palette creators online? reply jamietanna 20 hours agorootparentThanks this is based on the Srcery theme (https://srcery.sh/) but maybe needs some tweaks, as per some suggestions in the thread reply jonathrg 20 hours agorootparentprevMy first thought was Monokai (the default theme in Sublime Text 3 and earlier) reply throitallaway 20 hours agorootparentprevIMO keep it simple applies here. The page linked below does pretty much everything your page does (minus code diffs) and is MUCH easier to read. https://go.dev/doc/tutorial/getting-started#code reply jonathrg 21 hours agorootparentprev(not OP) some ideas: make the 's not go all the way to out the edge of the window. Make the diff colors and code colors less dramatically different from its surroundings. Increase the contrast of the default text. Use blue for links. Drop the orange reply 4ad 21 hours agorootparentprev> are there any particular tweaks you think would work to reduce the impact? Yeah, remove all CSS. I clicked on View > Page Style > No Style in Firefox and suddenly I could read it. Reader mode in Safari also worked. Reading mode in Chrome didn't work properly. reply wordofx 20 hours agoprevAre we just copying .NET now? reply movedx 20 hours agoparentWhat’s wrong with copying from other projects if they’re indeed offering good ideas worth copying? You say it as if the Golang community MUST only ever have unique ideas no one else has ever thought of — something that’s increasingly rare and unlikely. reply Cthulhu_ 3 hours agoparentprevTo be snide, .NET was copying Java, Java was copying C, etc etc etc. I don't understand your comment when you should know copying proven features is a good thing. reply purpleidea 19 hours agoprevgo.mod and the golang tooling is a horror show. I absolutely LOVE the language, but dealing with the tooling is horrendous. I should blog about the specifics, but if you want a short version: * not using posix args * obscure incantations to run tests * go.mod tooling is completely non-deterministic and hard to use, they should have just left the old-style vendor/ alone (which worked perfectly) and wrapped a git-submodules front-end on top for everyone who was afraid of submodules. Instead they reinvented this arcane new ecosystem. If you want to rewrite the golang tooling, I'll consult on this for free. reply verdverm 17 hours agoparentMVS, the algo for dep version selection, is deterministic, given the same inputs you will get the same outputs. Go has invested a lot of effort in creating reproducible builds through the entire toolchain https://research.swtch.com/vgo-mvs reply purpleidea 15 hours agorootparentThe _tooling_ is not reproducible. Take a not small golang project with some number of dependencies and there should be a single list of the latest versions for the entire project. And exactly what golang commands do you run to generate that list? It's totally broken. This is why so many tools cropped up like go-mod-upgrade and so on. Everyone downvoting obviously doesn't understand the problem. reply arccy 8 hours agorootparentthe versions in go.mod are an enforcement of the versions required by your dependencies, and those your module require. asking for it to be reproducible from scratch is like deleting package.json in a node project and asking it to magic all your >=Even the introduction of `go generate` long ago formalized this approach It did, but if you recall it came with a lot of \"We have no idea why you need this\" from Pike and friends. Which, of course, makes sense when you remember that they don't use the go toolchain inside Google. They use Google's toolchain, which already supports things like code generation and build dependency management in a far more elegant way. Had Go not transitioned to a community project, I expect we would have seen the same \"We have no idea why you need this\" from the Go project as that is another thing already handled by Google's tooling. The parent's experience comes from similar sized companies as Google who have similar kinds of tooling as Google. His question comes not from a \"why would you need this kind of feature?\" in concept, but more of a \"why would you not use the tooling you already have?\" angle. And, to be fair, none of this is needed where you have better tooling, but the better tooling we know tends to require entire teams to maintain it, which is unrealistic for individuals to small organizations. So, this is a pretty good half-measure to allow the rest of us to play the same game in a smaller way. reply teeray 4 hours agorootparent> if you recall it came with a lot of \"We have no idea why you need this\" from Pike and friends The blog post and design document both authored by Rob Pike at the time[0] contains none of that sentiment. The closest approach comes from the blog post which states: > Go generate does nothing that couldn’t be done with Make or some other build mechanism, but it comes with the go tool—no extra installation required—and fits nicely into the Go ecosystem. This, taken alone, would seem to support “we have no idea why you need this,” until you read the hope from the design document: > It is hoped, however, that it may replace many existing uses of make(1) in the Go repo at least. These are not words of someone who doesn’t understand why users would need this. Also, I am at a FAANG and my experience differs from the parent—`go tool` is sorely needed by my teams. [0] https://go.dev/blog/generate [1] https://go.googlesource.com/proposal/+/refs/heads/master/des... reply GauntletWizard 21 hours agoparentprev [–] I am both strongly of the opinion that this was already done much better in Bazel, and that the go-native version seems clean, clear, and simple and should probably be adopted by pure go shops. The digraph problem of build tooling is hardly new, though the ability to checksum all of your build tools and executables and mid-outputs to assure consistency is relatively new to feasibility. Bazel is a heavy instrument and making it work as well as it does was a hard problem even for Google. I don't know anyone making the same investment, and doubt it makes the slightest hint of sense for anyone outside the fortune 500. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Go 1.24 introduces a new `go tool` command and `tool` directive in `go.mod`, enhancing the management of project tools in the Go ecosystem.",
      "This update addresses issues with the `tools.go` pattern, such as performance impacts and dependency tree bloat, by allowing more efficient tool management and reducing unnecessary dependencies.",
      "While the `go tool` command improves performance by caching `go run` invocations, there are concerns about tool dependencies being treated as indirect, potentially leading to dependency clashes."
    ],
    "commentSummary": [
      "The introduction of \"go tool\" in Go 1.24 has led to debates about its impact on dependency management, with concerns about merging tool and project dependencies causing conflicts.",
      "Critics propose alternatives such as separate module files or using tools like Nix for improved version control.",
      "Supporters of Go's approach argue it offers simplicity and effectiveness, reflecting broader challenges in dependency management across programming languages."
    ],
    "points": 270,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1738010023
  },
  {
    "id": 42845933,
    "title": "I trusted an LLM, now I'm on day 4 of an afternoon project",
    "originLink": "https://nemo.foo/blog/day-4-of-an-afternoon-project",
    "originBody": "My Afternoon Project Turned Into Four Days of AI Lies, USB Chaos, and Hard Lessons 2025-01-26 by nemo TLDR AI isn’t a co-pilot; it’s a junior dev faking competence. Trust it at your own risk. I’m 4 days into an afternoon project. I was so sure I’d crush this one. I had a good plan and the stoke was high. Let me introduce Deskthang. It’s a thang for your desk. When I work, I want to put my phone in the other room, and only get the important notifications (thangs) in a different way. If my deployment pipeline fails, I want a globe on my desk to turn red and show me a gitlab logo. I do not want to check my phone or email or anywhere a distraction might find me. Quick backstory: I work full time++ doing boring enterprise software dev and rarely get to flex my engineering skills. While my title says engineer, I’d disagree. The Problem I’m Trying to Solve I always try to align multiple interests for a side project. I wanted to pull my electronics hardware box out of storage, I wanted to solve the notifications and focus issue for myself, and I wanted to see how scared I should be about AI taking my job. As they say, I was trying to get a few birds stoned at once. 1. I miss working with hardware. During COVID lock-downs I landed an R&D contract for a IoT Prototype. That R&D job was the most fulfilling work of my career. I worked with a small, scrappy team with some of my best friends. I was 3D printing models, soldering components, writing embedded C, and field-testing with mechanical engineers… Real engineers. We worked hard, often late into the night, and the collaboration felt more like playing StarCraft with the boiz than a 9-to-5. I’ve missed that deeply ever since. Recently, I’ve been inspired recently by @_MaxBlade and DeskHub and wanted to brush the dust off my electronics skills. 2. I hate the UX of MFA (Multi Factor Authentication). I use GitLab heavily for CI/CD with my personal Kubernetes projects. Knowing the status of my pipelines is crucial… broken builds could disrupt all 7 of my users! Logging into GitLab feels like getting stabbed in the spleen. Every time I log in (multiple times a day), I face captchas, authenticator apps, or waiting for email codes, followed by yet another captcha. I’ve tried pipeline notifications through Slack, Discord, and Telegram, but those apps are like productivity black holes. I don’t want my phone near me while working, or to open chat apps that derail my focus. Removing these distractions keeps me locked in. 3. I want to see how good these AI tools are. I want to figure out if AI is going to take my job. I’m skeptical it can replace what I do, but I like testing my assumptions. Sometimes AI surprises me; other times, it’s just a rabbit hole of wasted hours when I avoid doing real thinking. Recently, I used Claude Sonnet 3.5 to brute-force hundreds of React compile errors while upgrading a project from React 15 to 18. I threw package.json updates, deleted node_modules, and burned through a small fortune in AI tokens. To my surprise, we had a passing build by the end of the day. Work has been encouraging us to adopt an AI-first workflow and giving us unlimited tokens. It’s a wild experiment. This happened on a Friday. I wiped the sweat off my brow after a hard day’s prompting, and headed home early to start on my side project… The Plan Unlike me, my wife likes to leave the house and do things. I’ve spent a few years turning my garage into my favorite place to be. My wife and I have a deal where 1 day a month, She takes the kiddo and I am absolved of all responsibilities. I get a full day to lock in and build projects. From her perspective, I order doordash and turn into a degen who is unfit to father. From my perspective, I get to enjoy my favorite place and just tinker or play games or do whatever. These are the days I get to play mad scientist and feel most like myself. I look forward to it every month. My plan was to learn zig, brush off my hardware skills, build this project, write a blog post and make a video about it. Totally achievable. I wanted to wire up a Raspberry Pi Pico, a small 240x240 LCD display and some RGB LEDs. I was going to learn Zig and use it to send image data over USB to the pico which will put an image on the screen and change the LED color. I would set up webhooks from GitLab to call an API in my Kube cluster and setup my host Zig app to poll that same API for changes and send updates to the Pico. I really wanted to transmit the data over USB because I’ve never done that before. I’ve already used Bluetooth, LTE and Wifi and just wanted to do something new. The wiring is simple. Common patterns I was familiar with like SPI (Serial Peripheral Interface) for the display + some RGB leds. The TTY (TeleTYpewriter) serial data port on Linux /dev/ttyACM0 for USB communication with the Pico felt familiar because of how I had setup debug logging in the past. It looked like I had enough example repos collected that I could stitch a solution together. I did a little research each day and felt a little more sure each time. I’ve been using ChatGPT and Claude more and more to do initial research. I was at an AI hype peak and was bold enough to trust it… Since I do full stack web stuff on the daily, the api, webhooks and postgres are out of scope for the degen day. I was scoping the day’s work to Zig > Pico image transfer. 1. Setup Pico Organize the workspace Find a micro usb cable that supports data and not just charging… really why are they all power only?! Wire up a breadboard with Pico & display and LED Setup the C SDK for the raspi pico and a repo gitlab, gihub-mirror Push a build and see logs on cat /dev/ttyACM0 2. Setup Pico Display Put something on the screen during the boot loop. youtube link (12 sec) 3. Setup Host Zig Project Setup a host directory in repo Init zig project Send a message and see something on the screen youtube link (9 sec) 4. Image Transfer Yeet the raw rgb image data over USB It’s bidirectional safe right USB CDC is bidirectional safe TTY interface is built on top of USB CDC TTY is bidirectional safe because CDC is (no it’s not… thanks gpt) In the above image you can see the outright lie that broke me… USB CDC has separate TX (Transmit) and RX (Receive) buffers so it’s bidirectional safe. The same is not true for TTY which is bidirectional but less safe with a single buffer for TX and RX data. Timeline of Actuality (AI Woes) After a dozen duds, I found a data capable usb micro cable and everything went smoothly until the image transfer. I used Claude, Cline, and ChatGPT to AI-max my way to a buggy but working implementation. I sent commands from my terminal with Zig over USB to the Pico which read them and changed the screen. This only took a few hours and I was excited that the AI assisted dream was real. It’s not complex but I think it was faster than I could have done alone. I have no experience with zig besides hearing ThePrimeagen yap about it. I haven’t even read the docs. Multiple times, I found myself stopping Cline from starting completely new implementations of already solved issues. I didn’t catch everything though. When Cline blew through my API limits, I added Claude to my harem and ran both in parallel when possible. As I look through the code now, I realize that I’m lousy at multitasking and was gaslighting myself. The Image Transfer Disaster This is where my hubris came into play. In my mind, I pictured sending all the image data in one go, like an S3 upload. I imagined clean, raw data streaming over /dev/ttyACM0. It wasn’t clean. It wasn’t raw. It was chaos. I expected to see: pico heartbeat zig start image transfer zig [240x240 COLOR PIXELS] zig end image transfer pico heartbeat What I actually saw looked like this, but worse: pico heartbeat zig start ima% pico heage transfert pico heartbeat zig [240x240 COL pico heartbea OR PIXELtS] pico heartbeat zig end image transfer pico heartbeat It’s just like that interrupting cow knock knock joke. Completely unfunny and day ruining. Key Problems: 1. Buffer Conflicts: /dev/ttyACM0 was the battlefield. The same buffer was used for both logging and image transfer. If a log slipped in during the data stream… well good luck figuring out what the hell just happened. 2. Noise: Some weird corruption was happening. Maybe I wasn’t clearing buffers properly. Maybe the gods of USB communication just hate me. The bottom line? Neither the Pico nor my laptop could trust the data. Each system needed to learn to yield, and I needed to build the round-a-bout to force them to be polite and wait their turn. Packets, Protocols & State Machines, Oh my… I needed to get serious. So, naturally, I let Claude write some docs: Detailed a packet shape. Documented a checksum verification plan. Described data format for transfer. Denoted how to chunk and rebuild the image. Depicted the state machine transitions. Demonstrated command system. Designed a logging system that doesn’t break incoming commands. After delving down dem docs, I let AI run with the actual implementations. At this point, my “degen hat” came off, and I resumed my dad duties while letting Cursor and Cline play StarCraft with my codebase. This is the dream use case for AI, right? Just let it rip and come back to a perfectly functioning system. Let’s see just how close we get to the sun. Reality Check: AI tools are like interns who know how to Google really fast but don’t understand context. Cursor started changing core implementations for unrelated edits. Cline would randomly rewrite half the system without asking. By the time I noticed, my codebase looked like the aftermath of a spaghetti fight at a junior developer convention. Most of the codebase was actually unreachable. What did I learn? Like Icarus, my codebase is irrecoverable. A tangled heap of wing fragments and melted wax, dripping with half-baked ideas and unsupervised AI chaos. My grand vision of outsourcing grunt work to AI had sent me soaring, but the sun of reality burned away any hope of landing gracefully. Here’s what I’m taking away from this flaming descent. 1. AI is a tool, not a co-pilot AI is great for generating ideas or drafting code, but it doesn’t understand. It’s like giving a junior developer a chainsaw instead of a scalpel—it might finish the job, but you’ll spend twice as long cleaning up the mess. I learned that I need to stay firmly in the driver’s seat when tackling new tech. 2. Friction forces focus Having AI directly in my editor felt like playing with infinite cheat codes. It was too easy to let it run wild and harder to maintain control. Moving forward, I’m introducing deliberate friction. I will be using AI only in web interfaces or as a brainstorming tool. If I have to paste its suggestions into my code manually, I’ll be more mindful of the process and less likely to reach for it. 3. Mistakes teach better than shortcuts When I make mistakes, I learn. Debugging my own failures has always been one of the best ways to understand a new language or concept. Relying on AI to “fix” things for me short-circuited that learning process. As a result, I’m left with no deeper understanding of Zig than when I started. 4. Patience beats hubris Building something new, with unfamiliar tools takes time. The idea that I could fully implement my vision in a single “degen day” was overly optimistic, bordering on foolish. Sometimes, you have to respect the complexity of what you’re trying to achieve. Moving Forward Deskthang has grown from a casual afternoon project into a saga of overconfidence, AI misadventures, and lessons learned the hard way. For now, I’m shelving the AI driven shortcuts and committing to a rewrite on my next no-responsibilities day. I’ve picked up a pen and started writing docs by hand like it’s the stone age. I plan to work through some Advent of Code problems in Zig to actually learn the language before taking another crack at this project. Want to see if Deskthang ever works, or just enjoy the chaos as I fail forward? Subscribe below for a monthly email drop of my latest misadventures and skill issues. Together, we’ll learn how to coexist with AI, relearn the lessons I forget, and hopefully build something worthwhile in the process. LFG 🚀",
    "commentLink": "https://news.ycombinator.com/item?id=42845933",
    "commentBody": "I trusted an LLM, now I'm on day 4 of an afternoon project (nemo.foo)263 points by nemofoo 21 hours agohidepastfavorite191 comments potsandpans 20 hours agoCounterexample: Ive been able to complete more side projects in the last month leveraging llms than i have ever in my life. One of which I believe to have potential as a viable product, and another which involved complicated rust `no_std` and linker setup for compiling rust code onto bare metal RISCV from scratch. I think the key to being successful here is to realize that you're still at the wheel as an engineer. The llm is there to rapidly synthesize the universe of information. You still need to 1) have solid fundamentals in order to have an intuition against that synthesis, and 2) be experienced enough to translate that synthesis into actionable outcomes. If youre lacking in either, youre at the same whims of copypasta that have always existed. reply pfannkuchen 19 hours agoparentI’ve found LLMs to basically be a more fluent but also more lossy way of interfacing with stack overflow and tutorials. If a topic is well represented in those places, then you will get your answer quicker and it can be to some extent shaped to your use case. If the topic is not well represented there, then you will get circular nonsense. You can say “obviously, that’s the training data”, and that’s true, and I do find it obvious personally, but the reaction to LLMs as some kind of second coming does not align with this reality. reply biohcacker84 17 hours agorootparentThat matches my experience too. I wonder how fast they'll improve and if LLMs will hit a wall, as some AI experts think. reply janderson215 12 hours agorootparentIs it possible that you’re both using LLMs the same way you’d use SO and that’s the reason you see such similarities? The reason I ask is because it doesn’t not match my experience. It feels more like I’m able to Matrix-upload docs into my brain like Trinity learning to fly a helicopter. reply pfannkuchen 11 hours agorootparentI am using it like stack overflow in the sense that I’m solving a problem and I’m using it to answer questions when I’m in an unfamiliar or non-obvious place in the problem space. If I have a question about a first order language or framework feature or pattern, it works great. If I have a question about a second order problem, like an interaction between language or framework features, or a logical inconsistency in feature behavior, then it usually has no idea what’s going on, unless it turns out to be a really common problem such as something that would come up when working through a tutorial. For code completion, I’ve just turned it off. It saves time on boilerplate typing for sure, but the actual content pieces are so consistently wrong that on balance I find it distracting. Maybe I have a weird programming style that doesn’t mesh well with the broader code training corpus, not sure. Or maybe a lot of people spend more time in the part of problem-space that intersects with tutorial-space? I am not very junior these days. That being said I definitely do use LLMs to engage with tutorial type content. For that it is useful. And outside of software it is quite a bit better for interfacing with Wikipedia type content. Except for the part where it lies to your face. But it will get better! Extrapolating never hurt anyone. reply h0l0cube 12 hours agorootparentprev> I’m able to Matrix-upload docs into my brain like Trinity learning to fly a helicopter. So you're using it like Wikipedia? I find when learning something new (and non coding related) YouTube is infinitely better than an LLM. But then I prefer visual demonstration to tutorial or verbal explanation. reply almostdeadguy 1 hour agorootparentprevI'm sorry, what? reply EarthMephit 19 hours agoparentprevI find that LLMs are almost comically bad at projects that have a hardware component like RaspberryPi or Pico, or Ardunio. I think that its because often the libraries you use are niche or have a a few similar versions, the LLM really commonly hallucinated solutions and would continually suggest that library X did have that capability. I think because often in hardware projects you often hit a point where you can't do something or you need to modify a library, but the LLM tries to be \"helpful\" and it makes up a solution. reply jerf 16 hours agorootparentBased on my own modestly successful forays into that world, I have to imagine one problem the LLMs have in that space is terrible training data. A good three quarters of any search result you search for in that space will be straight-up out-of-date and not work on your system. Then you've got all the tiny variations between dozens of chipsets, and all the confidently wrong people on the internet telling you to do nonsensical things, entire ecosystems that basically poofed out of existence three years ago but are still full of all kinds of juicy search terms and Google juice... if I can hardly paw through this stuff with years of experience and the physical hardware right in front of me to verify claims with, I don't know how an LLM is supposed to paw through all that and produce a valid answer to any question in that space, without its own hardware to fiddle with directly. reply lostlogin 12 hours agorootparentYou’re making my eye twitch. The number of projects I’ve done where my notes are the difference between hours of relearning The Way or instant success. Google doesn’t work as some niche issue is blocking the path. ESP32, Arduino, Home Assistant And various media server things. reply chamomeal 17 hours agorootparentprevThey’re also pretty bad at typescript generics. They’re quite good at explaining concepts (like mapped types), but when push comes to shove they generate all sorts of stuff that looks convincing, but doesn’t pass the type checker. And then you’ll paste in the error, and they’ll just say “ok I see the problem” and output the exact same broken code lol. I’m guessing the problem is lack of training data. Most TS codebases are mostly just JS with a few types and zod schemas. All of the neat generic stuff happens in libraries or a few utilities reply swatcoder 16 hours agorootparentprevActually, it's because many of the people writing tutorials and sharing answers about that stuff don't know what the hell they're doing or grasp the fundamentals of how those systems work and so most of the source material the LLM's are trained on is absolute garbage. Public Arduino, RPi, Pico communities are basically peak cargo cult, with the blind leading the blind through things they don't understand. The noise is vastly louder than the signal. There's a basically giant chasm between expereinced or professional embedded developers that mostly have no need to ever touch those things or visit their forums, and the confused hobbyists on those forums randomly slapping together code until something sorta works while trying to share their discoveries. Presumably, those communities and their internal knowledge will mature eventually, but it's taking a long long time and it's still an absolute mess. If you're genuinely interested in embedded development and IoT stuff, and are willing to put in the time to learn, put those platforms away and challenge yourself to at least learn how to directly work with production-track SoC'a from Nordic or ESP or whatever. And buy some books or take some courses instead of relying on forums or LLM's. You'll find yourself rewarded for the effort. reply rcxdude 8 hours agorootparent>Presumably, those communities and their internal knowledge will mature eventually, but it's taking a long long time and it's still an absolute mess. I'm not sure they will. There's a kind of evaporative cooling effect where once you get to a certain level of understanding you switch around your tools enough that there's not much point interacting with the community anymore. reply bsder 16 hours agorootparentprev> Presumably, those communities and their internal knowledge will mature eventually, but it's taking a long long time and it's still an absolute mess. It won't because the RPi are all undocumented, closed-source toys. It would be an interesting experiment to see which chips an LLM is better at helping out with: RPi's with its hallucinatory ecosystem or something like the BeagleY-AI which has thousands of pages of actual TI documentation for its chips. It would be really nice if the LLMs could cover for this and circumvent where RPi's keep getting used because they were dumped under cost to bootstrap a network effect. reply jrmg 16 hours agorootparentprevI was just today trying to fix some errors in an old Linux kernel version 3.x.x .dts file for some old hardware, so that I could get a modern kernel to use it. ChatGPT seemed very helpful at first and I was super impressed. I thought it was giving me great insight into why the old files were now producing errors … except the changes it proposed never actually fixed anything. Eventually I read some actual documentation and realised it was just spouting very plausible sounding nonsense and confident at it! The same thing happened a year or so ago when I tried to get a much older ChatGPT to help me with with USB protocol problems in some microcontroller code. It just hallucinated APIs and protocol features that didn’t actually exist. I really expected more by now but I now suspect it’ll just never be good at niche tasks (and these two things are not particularly niche compared to some). reply maybesomaybenot 19 hours agoparentprevClaude is like having my own college professor. I've learned more in the past month with Claude then I learned in the past year. I can ask questions repeatedly and get clarification as fine as a need it. Granted, Claude has limits, but its a game-changer. > I think the key to being successful here is to realize that you're still at the wheel as an engineer. The llm is there to rapidly synthesize the universe of information. Bingo. OP is like someone who is complaining about the tools, when they should be working on their talent. I have a LOT of hobbies (circuits, woodworking, surfing, playing live music, cycling, photography) and there will always be people who buy the best gear and complain that the gear sucks. (NOTE: I\"m not implying claude is \"the best gear\", but it's a big big help.) I think the only problem with LLMs is synthesis of new knowledge is severely limited. They are great at explaining things others have explained, but suck hard at inventing new things. At least that's my experience with Claude: it's terrible as a \"greenfield\" dev. reply BalinKing 17 hours agorootparent> Claude is like having my own college professor. I don't use Claude, so maybe there's a huge gap in reliability between it and ChatGPT 4o. But with that disclaimer out of the way, I'm always fairly confused when people report experiences like these—IME, LLMs fall over miserably at even very simple pure math questions. Grammatical breakdowns of sentences (for a major language like Japanese) are also very hit-or-miss. I could see an LLM taking the place of, like, an undergrad TA, but even then only for very well-trod material in its training data. (Or maybe I've just had better experiences with professors, making my standard for this comparison abnormally high :-P ) EDIT: Also, I figure this sort of thing must be highly dependent on which field you're trying to learn. But that decreases the utility of LLMs a lot for me, because it means I have to have enough existing experience in whatever I'm trying to learn about so that I can first probe whether I'm in safe territory or not. reply satvikpendem 13 hours agorootparent> LLMs fall over miserably at even very simple pure math questions They are language models, not calculators or logic languages like Prolog or proof languages like Coq. If you go in with that understanding, it makes a lot more sense as to their capabilities. I would understand the parent poster to mean that they are able to ask and rapidly synthesize information from what the LLM tells them, as a first start rather than necessarily being 100% correct on everything. reply BalinKing 3 hours agorootparentOf course that's fair advice in itself, but the parent specifically equated them to a \"college professor.\" reply joseda-hg 15 hours agorootparentprevMajor in the context of Japanese is rough, I can see a significant drop in quality when interacting with the same model in say Spanish vs English For as rich a culture the Japanese have, there's only about 1XX million speakers and the size of the text corpus really matters here, the couple billion of English speakers are also highly motivated to choose English over anything else because Lingua Franca has homefield advantage To use LLM's efectively you have to work with knowledge of their weaknesses, Math is a good example, you'll get better results from Wolphram Alpha even for the simple things, which is expected Broad reasoning and explanations tend to be better than overly specific topics, the more common a language, the better the response If a topic has a billion tutorials online, an LLM has a really high chance of figuring out first try Be smart with the context you provide, the more you actively constrain an LLM, the more likely it is to work with you I have friends that just use it to feed class notes to generate questions and probe it for blindspots until they're satisfied, the improvements on their grade s make it seem like a good approach, but they know that just feeding responses to the LLM isn't trustworthy, so they do and then they also check by themselves, the extra time valuable by itself, if just to improve familiarity with the subject reply maybesomaybenot 12 hours agorootparentprevI think a lot of these people object to AI probably see the gross amounts of energy it is using, or the trillions of dollars going to fewer than half a dozen men (most american, mostly white). But, once you've had AI help you solve some gnarly problems, it is hard not to be amazed. And this is coming from a gal who thinks the idea of self-driving cars is the biggest waste of resources ever. reply BalinKing 3 hours agorootparent(EDIT: Upon rereading this, it feels unintentionally blunt. I'm not trying to argue, and I apologize if my tone is somewhat unfriendly—that's purely a reflection of the fact that I'm a bad writer!) Sorry, maybe I should've been clearer in my response—I specifically disagree with the \"college professor\" comparison. That is to say, in the areas I've tried using them for, LLM's can't even help me solve simple problems, let alone gnarly ones. Which is why hearing about experiences like yours leaves me genuinely confused. I do get your point about people disagreeing with modern AI for \"political\" reasons, but I think it's inaccurate to lump everyone into that bucket. I, for one, am not trying to make any broader political statements or anything—I just genuinely can't see how LLMs are as practically useful as other people claim, outside of specific use cases. reply maybesomaybenot 12 hours agorootparentprevLike I made very clear, it is great at some things and terrible at others. YMMV. /shrugs/ reply KerrAvon 18 hours agorootparentprevHow do you know it's accurate? reply maybesomaybenot 17 hours agorootparentSimple: one thing I'm learning about is RFCs for TCP/IP. I can literally go test it. It's like saying, \"How do you know it is right when it says 2+2=4\"? Some knowledge when taught is self-correcting. Other things I'm studying, like, say, tensor calculus, I can immediately use and know I learned it correctly. reply kortilla 14 hours agorootparentTCP/IP is a great example though of something you can get seemingly correct and then be subject to all kinds of failure modes in edge cases you didn’t handle correctly (fragmentation, silly windows, options changing header sizes, etc). reply maybesomaybenot 12 hours agorootparentThanks for telling me how wrong I am! I bet you're fun at parties. reply namaria 10 hours agorootparentThis is not a party, this is a technology forum and we prize being right reply maybesomaybenot 1 hour agorootparentSomeone made a subjective statement that I'm wrong, without knowing ANYTHING about me. Go blow, kiddo. reply selcuka 18 hours agorootparentprevThey are reasonably accurate, and no tutor is perfect. How do you know your college professor is accurate? reply malfist 18 hours agorootparentMy college professor has certifications and has passed tests that weren't in their training data. My college professor was also willing to say \"I don't know, ask me next class\" reply paulryanrogers 17 hours agorootparent> My college professor was also willing to say \"I don't know, ask me next class\" This is a key differentiator that I see in humans over LLMs: knowing ones limits. reply selcuka 16 hours agorootparentprev> My college professor has certifications and has passed tests that weren't in their training data. Granted, they are not (can't be) as rigorous as the tests your professor took, but new models are run through test suites before being released, too. That being said, I saw my college professors making up things, too (mind you, they were all graduated from very good schools). One example I remember was our argument with a professor who argued that there is a theoretical limit for the coefficient of friction, and it is 1. That can potentially be categorised as a hallucination as it was completely made up and didn't make sense. Maybe it was in his training data (i.e. his own professors). I agree with the \"I don't know\" part, though. This is something that LLMs are notoriously bad. reply maybesomaybenot 12 hours agorootparentprevYou can also test your professor's answers. I don't just walk around going \"Oh, Claude was right\", I'm literally using what I just learned and am generating correct results. I'm not learning facts like dates, or subject things, I'm learning laws, equations, theories, proofs, etc. (Like how to apply Euler's totient or his extended theories on factorization... there's only one \"right answer\"). Also, you method for attesting your professors accuracy is inherently flawed. That little piece of paper on their wall doesn't correlate with how accurate they are; it doesn't mean zero, but it isn't foolproof. Hate to break it to you, but even heroes are fallible. reply Eisenstein 17 hours agorootparentprevWhat do you consider 'not in its training data'? I just asked Claude a question I am pretty sure was not in its training data. * https://i.imgur.com/XjvImeT.jpeg reply mahogany 6 hours agorootparentIt is immediately wrong in Step 1. A newborn is not a 2:1 ratio of height:width. Certainly not 25cm width (what does that even mean? Shoulder to shoulder?). This is a perfect example of where not knowing the “domain” leads you astray. As far as I know “newborn width” is not something typically measured, so Claude is pulling something out of thin air. Indeed you are showing that something not in the training data leads to failure. reply philote 4 hours agorootparentBabies also aren't rectangles.. you could lay a row shoulder to shoulder, then do another row upside down from the first and their heads would fit between the heads of the first row, saving space. Edit: it also doesn't account for the fact the moon is more or less a sphere, and not a flat plane. reply selcuka 16 hours agorootparentprevThat's almost in the training data: https://www.quora.com/How-many-Humans-can-we-fit-on-the-Moon reply Eisenstein 12 hours agorootparentI guess coming up with a truly original question is tougher that it seems. Any ideas? reply ramses0 6 hours agorootparentAsk them what's the airspeed velocity of a laden astronaut riding a horse on the moon... Edit: couldn't resist, and dammit!! Response: Ah, I see what you're doing! Since the Moon has no atmosphere, there’s technically no air to create any kind of airspeed velocity. So, the answer is... zero miles per hour. Unless, of course, you're asking about the speed of the horse itself! In that case, we’d just have to know how fast the astronaut can gallop without any atmosphere to slow them down. But really, it’s all about the fun of imagining a moon-riding astronaut, isn’t it? reply lelanthran 4 hours agorootparentAn African horse or a European horse? reply satvikpendem 13 hours agorootparentprevDid you actually test the math done? Usually LLMs are terrible at math as, as I mentioned in another comment, they are language models, not calculators. Hopefully that changes when LLMs leverage other apps like calculators to get their results, I am not sure if Claude does that already or it's still in development. reply Eisenstein 12 hours agorootparentClaude has access to an analysis frame which takes javascript which it can use for calculations. reply XenophileJKO 18 hours agorootparentprevI would add though, that they can be very good at combining known concepts. Which can create a non-trivial set of \"new knowledge\". reply maybesomaybenot 18 hours agorootparentCreating new knowledge from current knowledge is called \"synthesis\" (ancient term, nothing modern). I'm hoping you're right, it would be amazing. reply onemoresoop 18 hours agoparentprevI find LLMs to be decent unblockers. I only turn to them form time to time though, unless Im in a playful mode and try poking out various ideas. As a coder I also ask for snippets when Im lazy. I tried growing a slightly larger solution a few times and it failed in dumb ways. It was clear it doesn’t really comprehend we do, it’s not aware it’s moving in circles and so on. All these things will probably see a lot incremental of improvents and as a tool will definitely stay but fundamentally LLMs can’t really think, at least the way we do and expecting that is also foolish. reply baxtr 20 hours agoparentprevIs it reasonable to assume that more senior devs benefit more from LLMs? reply adamtaylor_13 19 hours agorootparentI believe so. In my experience, you need to have that gut intuition (or experience) to say, “No way. That’s totally wrong.” Since AI will capitulate and give you whatever you want. You also have to learn how to ask without suggesting because it will take whatever you give it and agree. reply j_bum 19 hours agorootparentYep. I think a default state of skepticism is an absolute necessity when working with these tools. I love LLMs. I agree with OP them expanding my hobby capacity as well. But I am constantly saying (in effect) “you sure…?” and tend to have a pretty good bs meter. I’m still working to get my partner to that stage. They’re a little too happy to accept an answer without pushback or skepticism. I think being ‘eager to accept an answer’ is the default mode of most people anyway. These tools are likely enabling faster disinformation consumption for the unaware. reply simonw 19 hours agorootparentprevI think so. Junior devs can get plenty of value out of them too, if they have discipline in how they use them as a learning tool, not as a replacement for thinking about projects. Senior devs can get SO much more power from these things, because they can lean on many years of experience to help them evaluate if the tool is producing useful results and to help them prompt it in the most effective way possible. A junior engineer might not have the conceptual knowledge or vocabulary to say things like \"write tests for this using pytest, include a fixture that starts the development server once before running all of the tests against it\". reply jaredcwhite 15 hours agorootparentprevI wouldn't assume that at all. Most of the senior devs I talk to on a regular basis think commercial* LLMs are ridiculous and the AI hype is nonsensical. * I put commercial there as a qualifier because there's some thought that in the future, very specifically-trained smaller models (open source) on particular technologies and corpuses (opt-in) might yield useful results without many of the ethical minefields we are currently dealing with. reply RickS 19 hours agorootparentprevIMO experience provides better immunity for common hangups. Generated code tends to be directionally pretty good, but with lots of minor/esoteric failures. The experience to spot those fast and tidy them makes all the difference. Copilot helps me move 10x faster with tedious arduino stuff, but I can easily see where if I didn't have decent intuition around debugging and troubleshooting, there'd be almost zero traction since it'd be hard to clear that last 10% hurdle needed to even run the thing. reply dogma1138 20 hours agorootparentprevIt depends it think it’s less about how senior they are and how good they are at writing requirements, and knowing what directives should be explicitly stated and what can be safely inferred. Basically if they are good at utilizing junior developers and interns or apprentices they probably will do well with an LLM assistant. reply satvikpendem 13 hours agorootparentprevIt's the LLM paradox, seniors get more senior with them while juniors get more junior, creating a bimodal distribution in the future simply because juniors will start depending on them too much to learn how to code properly while seniors (who some may also exhibit the previous trait) will by and large be able to rapidly synthesize information from LLMs with their own understanding. reply jumpman500 19 hours agorootparentprevYa. I think people that have better technical vocabulary and an understanding of what should be possible with code do better. That’s usually a senior engineer, but not always reply Spivak 19 hours agorootparentprevYes, you essentially have an impossibly well read junior engineer you can task with quick research questions like, \"I'm trying to do x using lib y, can you figure that out for me.\" This is incredibly productive because in the answer is typically all the pieces you need but not always assembled right. Getting the LLM to pull out well-known names of concepts is for me the skill you can't get anywhere else. You can describe a way to complete a task and ask for what it's called and you'll be heading down arxiv links right away. Like yes the algorithm to find the closest in edit distance and length needle string in a haystack is called Needleman–Wunsch, of course Claude, everyone knows that. reply ahi 19 hours agorootparentOnce it gives me the names for the concepts I'm struggling with, I often end up finding the stackoverflow or documentation it's copy pasting. reply ern 18 hours agorootparentprevI had a couple of the most capable senior developers reach out to me to tell me how Github Copilot accelerated their productivity, which surprised me initially. So I think there's something to it. reply irthomasthomas 6 hours agoparentprevAnother data point. Plugin ideation to publication in 2 minutes, from a single prompt, albeit a multi-step shell prompt. 3 packages published in 24 hrs https://x.com/xundecidability/status/1884077427871342955 reply pinoy420 12 hours agoparentprevExactly this. OP, credit where credit is due, appears to be someone who “hacks things together” copy pasting solutions blindly from the internet with little intuition gained along the way. reply chamomeal 17 hours agoparentprevI agree with his point about asking AI to “fix” problems though. It’s really nice that you don’t have to fully understand something to use it, but that becomes a problem if you lean on it too much reply gsf_emergency_2 17 hours agoparentprevIme engineers who find LLM useful have misunderstood their reasons for existence outside of being salaried.. What is your main project ? Do you LLM that? ( I wager you're not a rust expert and should maybe reconsider using rust in your main project. FWIW asking LLM whether you should use rust ~ asking it about the meaning of life. Important questions that need answers, but not right away! (A week or 2 tops) If you need to synthesize the universe of information with LLM.. that is not the universe you want to live or play in reply dogma1138 20 hours agoparentprevIndeed LLMs are useful as an intern, they are at the “cocky grad” stage of their careers. If you don’t understand the problem and can’t steer the solution and worse has only limited understanding of the code they produce you are unlikely to be productive. On the other hand if you understand what needs to be done, and how to direct the work the productivity boost can be massive. Claude 3.5 sonnet and O1 are awesome at code generation even with relatively complex tasks and they have a long enough context and attention windows that the code they produce even on relatively large projects can be consistent. I also found a useful method of using LLMs to “summarize” code in an instructive manner which can be used for future prompts. For example summarizing a large base class that may be reused in multiple other classes can be more effective than having to overload a large part of your context window with a bunch do code. reply blast 16 hours agoparentprev> The llm is there to rapidly synthesize the universe of information That's a nice way of putting it. reply talldayo 20 hours agoparentprev> which involved complicated rust `no_std` and linker setup for compiling rust code onto bare metal RISCV from scratch. That's complicated, but I wouldn't say the resulting software is complex. You gave an LLM a repetitive, translation-based job, and you got good results back. I can also believe that an LLM could write up a dopey SAAS in half the time it would take a human to do the same. But having the right parameters only takes you so far. Once you click generate, you are trusting that the model has some familiarity with your problem and can guide you without needing assistance. Most people I've seen rely entirely on linting and runtime errors to debug AI code, not \"solid fundamentals\" that can fact-check a problem they needed ChatGPT to solve first place. And the \"experience\" required to iterate and deploy AI-generated code basically boils down to your copy-and-paste skills. I like my UNIX knowledge, but it's not a big enough gate to keep out ChatGPT Andy and his cohort of enthusiastic morons. We're going to see thousands of AI-assisted success stories come out of this. But we already had those \"pennies on the dollar\" success stories from hiring underpaid workers out of India and Pakistan. AI will not solve the unsolved problems of our industry and in many ways it will exacerbate the preexisting issues. reply simonw 19 hours agorootparentA tool that can \"write up a dopey SAAS in half the time it would take a human to do\" is a pretty incredible thing to add to your toolbox! reply talldayo 17 hours agorootparentIf the summary goal of your existence is to be the most delirious waste of resources that humanity has yet known, sure. It's the hammer and nail of spoiled burnouts everywhere that need a credible ruse to help them out of the bottle. Some of us are capable of wanting for things better than a coin-operated REST API. The kind of imagination used to put people on the moon, that now helps today's business leaders imagine more profitable ways to sell anime pornography on iPhone. (Don't worry, AI will disrupt that industry too.) reply tonyhart7 16 hours agorootparentI'm sorry, but generated REST API boilerplate has probably fed more people than putting people on the moon. reply HPsquared 19 hours agorootparentprevWhat it will do is to free up a lot of brainpower to think about those hard problems and empower people to try our their ideas. reply talldayo 17 hours agorootparentI used to think the exact same thing would happen when we paid Pakistani and Indian labor to do America's busywork. That was about 15 years ago, I no longer have the same enthusiasm you do. reply tonyhart7 16 hours agorootparentNow, you are paying a Taiwanese or American company to produce GPUs for you. This allows you to use open-source models like DeepSeek R1, significantly reducing your reliance on Indian tech labor reply sgc 14 hours agorootparentI believe they are saying we did not learn to think more deeply then, so why should we expect to learn how now. reply mythrwy 20 hours agoparentprevI've had both experiences strangely enough. reply gerdesj 17 hours agoparentprevnext [2 more] [flagged] potsandpans 17 hours agorootparentYou are very smart and cool. reply KronisLV 20 hours agoprevIn my experience LLMs will help you with things that have been solved thousands of times before and are just a matter of finding some easily researched solution. The very moment when you try to go off the beaten path and do something unconventional or stuff that most people won't have written a lot about, it gets more tricky. Just consider how many people will know how to configure some middleware in a Node.js project... vs most things related to hardware or low level work. Or even working with complex legacy codebases that have bits of code with obscure ways of interacting and more levels of abstraction that can be reasonably put in context. Then again, if an LLM gets confused, then a person might as well. So, personally I try to write code that'd be understandable by juniors and LLMs alike. reply winocm 20 hours agoparentIn my experience, a LLM decided to not know type alignment rules in C and confidently trotted out the wrong answer. It left a horrible taste in my mouth for the one time I decided to look at using a LLM for anything and it keeps leaving me wondering if I'd end up more time bashing the LLM into working than just working out the answer myself and learning the underlying reasoning why. It was so wrong that I wonder what version of the C standard it was even hallucinating. reply tasuki 3 hours agoparentprevLLMs are surprisingly good at Haskell (and I'm not). I hope for a rennaisance of somewhat more rigorous programming languages: you can typecheck the LLM suggestions to see if they're any good. Also you can feed the type errors back to the LLM. reply NitpickLawyer 20 hours agoparentprev> vs most things related to hardware or low level work. counter point: https://github.com/ggerganov/llama.cpp/pull/11453 > This PR provides a big jump in speed for WASM by leveraging SIMD instructions for qX_K_q8_K and qX_0_q8_0 dot product functions. > Surprisingly, 99% of the code in this PR is written by DeekSeek-R1. The only thing I do is to develop tests and write prompts (with some trials and errors) reply alfalfasprout 20 hours agorootparentA single PR doesn't really \"prove\" anything. Optimization passes on well-tested narrowly scoped code are something that LLMs are already pretty good at. reply dutchbookmaker 19 hours agorootparentI think DeekThink is something different though. It is able to figure out some things that I know do not have much training data at all. It is looking at the manual and figuring things out. \"That doesn't make sense. Wait, that can't be right. I must have the formula wrong.\" I just seen that in the chain of thought. reply 9cb14c1ec0 16 hours agorootparentNah, in my experience, if there is the slightest error in the first sentence of the chain of thought, it tends to get worse and worse. I've had prompts that would generate a reasonable response in llama, but turn out utter garbage in Deepthink. reply tonyhart7 16 hours agorootparentBut how is this any different from real humans? They are not always right either. Sure, humans can understand things better, but are we really going to act like LLMs can't get better in the next year? And what about the next 6 months? I bet there are unknown startups like Deepseek that can push the frontier further. reply alfalfasprout 27 minutes agorootparentThe ways in which humans err are very different. You have a sense of your own knowledge on a topic and if you start to stray from what you know you're aware of it. Sure, you can lie about it but you have inherent confidence levels in what you're doing. Sure, LLMs can improve but they're ultimately still bound by the constraints of the type of data they're trained on and don't actually build world models through a combination of high bandwidth exploratory training (like humans) and repeated causal inference. reply suddenlybananas 19 hours agorootparentprevat a certain point though, one wonders if you can trust people to accurately report how much is written by an LLM. (not even implying bad faith, but if you're constantly re-reading, selecting and re-combining snippets written by LLMs, it's not really \"written\" by LLMs in the same way that's implied). reply petercooper 3 hours agorootparentWe kinda went through this with images when Photoshop and similar tools appeared. I remember a lot of people asking questions in the late 90s/early 00s in particular about if an image were “real” or not and the distinctions between smart photography and digital compositions. Nowadays we just assume everyone is using such tools as a baseline and genuinely clever photography is now celebrated as an exception. Perhaps ditto with CGI and prop/set making in movies. Unless a director crows about how genuine the effects are, we assume CGI. reply NitpickLawyer 13 hours agorootparentprev> at a certain point though, one wonders if you can trust people to accurately report how much is written by an LLM. That's an interesting thought. I think there are ways to automate this, and some IDEs / tools track this already. I've seen posts by both Google and Amz providing percentages of \"accepted\" completions in their codebases, and that's probably something they track across codebases automatically. Also on topic, here's aider's \"self written code\" statistics: https://aider.chat/HISTORY.html But yeah I agree that \"written by\" doesn't necessarily imply \"autonomously\", and for the moment it's likely heavily curated by a human. And that's still ok, IMO. reply kitchenchem 17 hours agorootparentprevYeah I never know exactly what this means. The pr says for one variant it got in one shot and the other they said took re-prompting 4 to 8 more times. reply ryandrake 20 hours agoparentprevI use CoPilot pretty much as a smarter autocomplete that can sometimes guess what I'm planning to type next. I find it's not so good at answering prompts, but if I type: r = (rgba >> 24) & 0xff; ...and then pause, it's pretty good at guessing: g = (rgba >> 16) & 0xff; b = (rgba >> 8) & 0xff; a = rgba & 0xff; ... for the next few lines. I don't really ask it to do more heavy lifting than that sort of thing. Certainly nothing like \"Write this full app for me with these requirements [...]\" reply mercer 12 hours agoparentprevI've only started using LLMs for code recently, and I already tend to mentally translate what I want to something that I imagine is 'more commonly done and well represented in the training data'. But especially the ability to just see some of the stuff it produces, and now to see its thought process, is incredibly useful to me already. I do have autism and possibly ADD though. reply rooroobooragool 18 hours agoparentprevThat rhymes with my experience of trying to generate placeholder art using AI. Since it's just a placeholder I often ask for a funny twist but it's rarely ever anything like it. reply tashian 17 hours agoprevI used Claude to help me build a side project in 4 hours that I would never have built otherwise. Essentially, it's a morphing wavetable oscillator in React (https://waves.tashian.com). Six months ago, I tried building this app with ChatGPT and got nowhere fast. Building it with Claude required a gluing together a few things that I didn't know much about: JavaScript audio processing, drawing on a JavaScript canvas, an algorithm for bilinear interpolation. I don't write JavaScript often. But I know how to program and I understand what I'm looking at. The project came together easily and the creative momentum of it felt great to me. The most amazing moment was when I reported a bug—I told Claude that the audio was stuttering whenever I moved the controls—and it figured out that we needed to use an AudioWorklet thread instead of trying to play the audio directly from the React component. I had never even heard of AudioWorklet. Claude refactored my code to use the AudioWorklet, and the stutter disappeared. I wouldn't have built this without Claude, because I didn't need it to exist that badly. Claude reduced the creative inertia just enough for me to get it done. reply masklinn 11 hours agoparentWhat was your workflow for doing that? Just going back and forth in a chat, or a more integrated experience in a dedicated editor? reply tashian 3 hours agorootparentJust copy/paste from the chat window. I kept running into token limits. I came away from it wanting a much better workflow. That's the next step for me in learning AI... playing with different integrated editor tools. reply pieix 21 hours agoprev> AI isn’t a co-pilot; it’s a junior dev faking competence. Trust it at your own risk. This is a good take that tracks with my (heavy) usage of LLMs for coding. Leveraging productive-but-often-misguided junior devs is a skill every dev should actively cultivate! reply nrb 20 hours agoparent> Leveraging productive-but-often-misguided junior devs is a skill every dev should actively cultivate! Feels like this is only worthwhile because the junior dev learns from the experience; an investment that yields benefits all around, in the broad sense. Nobody wants a junior around that refuses to learn in perpetuity, serving only as a drag on productivity and eventually your sanity. reply simonw 19 hours agorootparentThat's somewhere that the AI-as-junior-dev analogy breaks down a little. There's still incredible accumulated value here, but it's at the other end. The more times you successfully use an LLM to produce working code, the more you learn about how to use them what they're good at, what they're bad at, how to effectively prompt them. reply krageon 4 hours agorootparentprevIt's worthwhile because you cannot do everything and often it is better to have someone far worse than you work on a problem than to just ignore it. reply codr7 20 hours agoparentprevWhat you're doing is sacrificing learning for speed. Which is fine, if it's a conscious choice for yourself. reply piva00 20 hours agorootparentI don't think GP was talking about themselves being a junior using LLMs, at least my interpretation was that devs should learn how to leverage misguided junior, and LLMs are more-or-less on the level of a misguided junior. Which I completely agree, I use LLMs for the cases where I do know what I'm trying to do, I just can't remember some exact detail that would require reading documentation. It's much quicker to leverage a LLM rather than going on a wild goose chase of the piece of information I know exists. Also it's a pretty good tool to scaffold the boring stuff, asking a LLM \"generate test code for X asserting A, B, and C\" and editing it to be a proper test frees up mental space for more important stuff. I wouldn't trust a LLM to generate any kind of business logic-heavy code, instead I use it as a quite smart template/scaffold generator. reply codr7 2 hours agorootparentAnd the end result is you won't learn the details, so you will become more and more dependent on your magic piano. reply piva00 1 hour agorootparentI know the details, I've been through the wading, thrashing around the docs, the books, I just can't recall the right incantation at that moment and a LLM is more efficient than searching the web. I still have the skills to search the web if the magic piano disappears. Don't know why you are trying to come up with a situation that doesn't exist, what's your point exactly against this quite narrow use-case? reply pieix 18 hours agorootparentprevThanks for explaining my intent, you nailed it. reply jdietrich 19 hours agoparentprevIt is quite remarkable that we are already at the stage where saying \"this AI is about as competent as an inexperienced college graduate\" constitutes criticism. It is entirely proper for people to be engaging sceptically with LLMs at their current level of capability, but I think we should also keep in mind the astonishingly rapid growth rate in their performance. LLMs were a toy two years ago, they're now a useful if flawed colleague, but what can we expect two years from now? reply acedTrex 19 hours agorootparentI mean 2 years ago they were at about the same place, theres been very little practical gain from gpt4 in my opinion. No matter the model the fundamental failure cases have remained the same. reply danielbln 10 hours agorootparentI disagree, context size alone has exploded from 8k to 200k now and that makes a huge difference. LLMs have also progressed significantly in many other metrics, code quality, understanding, etc. The recent reasoning models have upped the ante further, especially when combined with models that are good at editing code. reply jdietrich 19 hours agorootparentprevReasoning models like o1 or QwQ absolutely destroy 4o in coding, let alone GPT-4 circa 2022. reply tkgally 19 hours agorootparentMinor correction: GPT-4 was announced on March 14, 2023, less than two years ago. I don’t remember how much LLMs had been discussed as coding assistants before then, but it was Greg Brockman’s demonstration of using it to write code that first brought that capability to my attention: https://www.youtube.com/live/outcGtbnMuQ?si=oTMA02ns_BJDRS4c... Advances since then have indeed been remarkable. reply zitterbewegung 21 hours agoprevI have used LLMs as a tool and I start to \"give up\" working with it after a few tries. It excels at simple tasks, boilerplate, or scripts but larger programs you really have to know what exactly you want to do. I do see the LLMs ingesting more and more documentation and content and they are improving at giving me right answers. Almost two years ago I don't believe they had every python package indexed and now they appear to have at least the documentation or source code of it. reply XorNot 20 hours agoparentThe trouble is the only reliable use-case LLMs actually seem good at is \"augmented search engine\". Any attempts at coding with them just end up feeling like trying to code via a worse interface. So it's handy to get a quick list of \"all packages which do X\", but it's worse then useless to have it speculate as to which one to use or why, because of the hallucination problem. reply zitterbewegung 19 hours agorootparentYes it does work as an augmented search engine but, it does output working code you just have to prompt it better if the code is not that complex like a simple endpoint you just have to understand exactly what you want. reply powerset 20 hours agoprevI've had a similar experience, shipping new features at incredible speed, then waste a ton of time going down the wrong track trying to debug something because the LLM gave me a confidently wrong solution. reply williamcotton 20 hours agoparentWell that's kind of on you for not noticing that it was the wrong solution, isn't it? reply trinix912 20 hours agorootparentSometimes the solution is 99% correct but the other 1% is so subtly wrong that it both doesn't work and is a debugging hell. reply forgetfreeman 18 hours agorootparentWelcome to programming. reply nyarlathotep_ 20 hours agorootparentprevoften it's something you casually overlook, some minor implementation detail that you didn't give much thought to that ends up being a huge mess later on, IME reply cruffle_duffle 20 hours agorootparentprevI think the parents post happened to everybody, and if it hasn’t it will. The edge between being actually more productive or just “pretend productive” using large language models is something that we all haven’t completely figured out yet. reply mythrwy 20 hours agorootparentprevYa but you kind of get painted in corner sometimes. And sunken cost fallacy kicks in. reply th0ma5 17 hours agorootparentprevIt's on you however for not understanding the greater point? reply cherry_tree 19 hours agoparentprevSeems like LLMs would be well suited for test driven development. A human writes tests and the LLM can generate code passing all tests; ending with a solution that meets the humans expectations. reply drpossum 19 hours agorootparentI disagree because you're only considering the \"get code to make the test pass\". Refactoring, refining, and simplifying is critical and I've yet to see this applied well. (I've also yet to see the former applied usably well either despite \"write tests generate code\" being an early direction.) reply distortionfield 13 hours agorootparentprevThis is more or less how I use LLMs right now. They’re fantastic at the plumbing, so that I can focus on the important part the business and domain logic. reply ravroid 20 hours agoprevOne strategy I've been experimenting with is maintaining a 'spec' document, outlining all features and relevant technical notes about a project. I include the spec with all relevant source files in my prompt before asking the LLM to implement a new change or feature. This way it doesn't have to do as much guessing as to what my code is doing, and I can avoid relying on long-running conversations to maintain context. Instead, for each big change I include an up-to-date spec and all of the relevant source files. I update the spec to reflect the current state of the project as changes are made to give the LLM context about my project (this also doubles as documentation). I use an NPM script to automate concatenating the spec + source files + prompt, which I then copy/paste to o1. So far this has been working somewhat reliably for the early stages of a project but has diminishing returns. reply SparkyMcUnicorn 19 hours agoparentYou're describing functionality that's built into Aider. You might want to try it out. Aider also has a copy/paste mode to use web ui interfaces/subscriptions instead apis. I definitely use and update my CONVENTIONS.md files and started adding a second specification file for new projects. This + architect + \"can your suggestion be improved, or is there a better way?\" has gotten me pretty far. reply ravroid 19 hours agorootparentDidn't know about Aider going to give that a try, thanks! reply tanseydavid 21 hours agoprevI ask this question without a hint of tone or sarcasm. You said: \"*it’s a junior dev faking competence. Trust it at your own risk.*\" My question is simply: \"wouldn't you expect to personally be able to tell that a human junior dev was faking competence?\" Why should it be different with the LLM? reply latexr 20 hours agoparentObviously, it depends on context. When talking to someone live you can pick up on subtle hints such as tone of voice, or where they look, or how they gesticulate, or a myriad other signals which give you a hint to their knowledge gaps. If you're communicating via text, the signals change. Furthermore, as you interact with people more often you understand them better and refine your understanding of them. LLMs always forget and “reset” and are in flux. They aren’t as consistent. Plus, they don’t grow with you and pick up on your signals and wants. It’s incredibly worrying that it needs to be explained again and again that LLMs are different from people, do not behave like people, and should not be compared to people or interacted like people, because they are not people. reply appleorchard46 20 hours agorootparentInterestingly your description of social cues you expect to pick up on are the exact sort of social cues I struggle with. If someone says something, generally speaking I expect it to be true unless there is an issue with it that suggests otherwise. I suppose the wide range of negative and positive experiences people seem to have working with LLMs is related to the wide range of expectations people have for their interactions in general. reply layer8 20 hours agoparentprevNot instantly. You’d give the human junior dev the benefit of the doubt at first. But when it becomes clear that the junior dev is faking competence all the time (that might take longer than the four days in TFA — yes I know it’s not exactly comparable, just saying) and won’t stop with that and start being honest instead, you’d eventually let them go, because that’s no way to work with someone. reply sarob 2 hours agoprevYou are running the show and the LLM can act like many other roles to help you. Obscure or confusing topics, likely the LLM will be as bad at solving as any employee. Give it a plan. Follow up and make sure it’s on track. reply transcriptase 20 hours agoprevI’ve been able to do more far complex things with ESP32s and RPis in an evening without knowing the first thing about python or c++. I can also tell when it’s stuck in some kind of context swamp and won’t be any more help, because it will just keep making the same stupid mistakes over and over and generally forgetting past instructions. At that point I take the last working code and paste it into a new chat. reply superq 15 hours agoprevMost LLMs default to being sycophantic yes-men, but if you create a custom prompt, it can help mitigate any issues. I have a custom prompt that instructs gpt4o to get aggressive about attacking anything I say (and, importantly, anything it says). Here's my result for the same question: https://chatgpt.com/share/67984aa9-1608-8012-be93-a77728ab8e... reply thot_experiment 20 hours agoprevAs opposed to not trusting an LLM, and ending up on day 4 of an afternoon project? :P I've been doing that since way before LLMs were a thing. reply medhir 16 hours agoprevPerhaps being a PM for several years has helped, I’ve had great success speeding up my programming workflows by prompting Claude with very specific, well defined tasks. Like many others are saying, you need to be in the drivers seat and in control. The LLM is not going to fully complete your objectives for you, but it will speed you up when provided with enough context, especially on mundane boilerplate tasks. I think the key to LLMs being useful is knowing how to prompt with enough context to get a useful output, and knowing what context is not important so the output doesn’t lead you in the wrong direction. reply qiqitori 17 hours agoprevYour plan was to use USB, but to me it looks like you're pretty much just using serial via USB. That's completely fine of course! One cheap way to tackle your problem is to use a version of printf with locking, which is likely available in many microcontroller SDKs (it's also slow). (Or you could add your own mutex.) USB-CDC is cooler than that, you can make the Pico identify as more than just one device. E.g. https://github.com/Noltari/pico-uart-bridge identifies as two devices (so you get /dev/ttyACM0 and /dev/ttyACM1). So you could have logs on one and image transfers on another. I don't think you're limited to just two, but I haven't looked into it too far. You can of course also use other USB protocols. For example you could have the Pico present itself as a mass-storage device or a USB camera, etc. You're just limited by the relatively slow speed of USB1.1. (Though the Pico doesn't exactly have a lot of memory so even USB1.1 will saturate all your RAM in less than 1 second) reply nsavage 20 hours agoprevFunny enough, I posted an article I wrote here yesterday with the same sort of thesis. Different technologies (mine was Docker) but same idea of LLM leading me astray and causing a lot of frustration reply djray 3 hours agoprevThe OP misunderstands (perhaps deliberately or for humorous effect) what a co-pilot is. This is telling: \"I learned that I need to stay firmly in the driver’s seat when tackling new tech.\" Er, that's pretty much what a pilot is supposed to do! You can't (as yet) just give an AI free reign over your codebase and expect to come back later that day to discover a fully finished implementation. Maybe unless your prompt was \"Make a snake game in Python\". A pilot would be supervising their co-pilot at all times. Comparing AIs to junior devs is getting tiresome. AIs like Claude and newer versions of ChatGPT have incredible knowledge bases. Yes, they do slip up, especially with esoteric matters where there are few authoritative (or several conflicting) sources, but the breadth of knowledge in and of itself is very valuable. As an anecdote, neither Claude nor ChatGPT were able to accurately answer a question I had about file operation flags yesterday, but when I said to ChatGPT that its answer wasn't correct, it apologised and said the Raymond Chen article it had sourced wasn't super clear about the particular combination I'd asked about. That's like having your own research assistant, not a headstrong overconfident junior dev. Yes, they make mistakes, but at least now they'll admit to them. This is a long way from a year or two ago. In conclusion: don't use an AI as one of your primary sources of information for technology you're new to, especially if you're not double-checking its answers like a good pilot. reply cudgy 15 hours agoprevReally enjoyed reading your article. Haven’t laughed as much reading a tech article in quite some time. You should consider doing some YouTube videos as your communications style is very humble and entertaining. Made me wanna join in your garage and help out with the project :) reply Fourier864 18 hours agoprevFWIW, I fed in the same problematic prompt to all the current ChatGPT models and even the legacy/mini models enumerated a bunch of pitfalls and considerations. I wonder why/how it managed to tell the author everything was perfect? A weird one-off occurrence? reply cmdtab 20 hours agoprevToday, I needed to write a proxy[0] that wraps an object and log all method calls recursively. I asked claude to write the initial version. It came up with a complicated class based solution. I spent more than 30 minutes getting a good abstract to come out. I was copy pasting typescript errors and applying fixes it suggested without thinking much. In the end, I gave up and wrote what I wanted myself in 5 minutes. 0] https://github.com/cloudycotton/browser-operator/blob/main/s... reply dambi0 19 hours agoparentWould you have written it in five minutes has you not just spent 30 minutes ruling out wrong solutions? reply cmdtab 9 hours agorootparentYes. I wrote how to do it technically. Claude was able to come up with a solution that worked on second attempt. The problem was it didn’t work with typescript nicely. The approach overcomplicated anything that depended on this class. reply lbotos 18 hours agoprevNemo, are you using a self-hosted install or .com? If .com, email me (it's in my profile) and I can see if there is a reason your account is getting so heavily captcha'd. reply insane_dreamer 19 hours agoprev> junior dev faking competence A bit on a tangent, but has there been any discussion of how junior devs in the future are ever going to get past that stage and become senior dev calibre if companies can replace the junior devs with AIs? Or is the thinking we'll be fine until all the current senior devs die off and by then AI will be able to replace them too so we won't need anyone? 1. CS/Eng degree 2. ??? 3. Senior dev! reply wvenable 17 hours agoparentIt's definitely as good as junior dev at a lot of tasks but you always have to be in the driver seat. I don't ask junior devs to write functions one at a time. I give them a task, they ping me if they need something, but otherwise I hope I don't hear from them again for a while. I don't see AI replacing that. AI is a tool with the instant Q&A intelligence of a junior dev but it's not actually doing the job of a junior dev. That's a subtle distinction. reply dragonwriter 19 hours agoparentprevTraining will adapt to the widespread use of AI coding assistance if they are that universally useful, and people will come into the market as junior AI wranglers, with skillsets stronger than curent junior devs is some areas but weaker in others; current seniors will grumble about holes in their knowledge, but that's been the case with the generational changes in software development as the common problems people face at different levels have shifted over time. The details are new, but the process isn't. reply insane_dreamer 19 hours agorootparentNot if the goal is to replace the junior devs with AIs people won't be \"coming into the market\" because they won't be needed. Companies are not saving money by paying for AI tools if they continue to hire the same number of people. The only way it makes financial sense, and for the enormous amounts of money being invested into AI to reap profits, is if companies are able to reduce the cost of labor. First, they only need 75% of the junior devs they have now, then 50%, then 25%. reply dragonwriter 17 hours agorootparent> Not if the goal is to replace the junior devs with AIs people won't be \"coming into the market\" because they won't be needed. It won't happen all at once, and as tasks done by current juniors are incrementally taken over by AI, the expected entry skillset will evolve in line with those changes. There will always be junior people in the field, but their expected knowledgebase and tasks will evolve, and even if 100% of the work currently done by juniors is eventually AI-ified, there will still be juniors, they just will be doing completely different things, and going through a completely different learning process to get there. > Companies are not saving money by paying for AI tools if they continue to hire the same number of people. Companies which have a fixed lump of tech work (in practice, none, actually) will save money because they will hire fewer total workers because output per worker will increase, but they will still have people who are newer and more experienced within that set, because the More realistic companies that either make money with tech work or that apply internal effort to tech as long as it has net positive utility may actually end up spending more on tech, because each dollar spent gives more results. This still saves money (or makes more money), but the savings (where it is about savings, and not revenue) will be in the areas tech is applied to, not tech itself. reply tippytippytango 18 hours agoprevWhat I learned is you can’t outsource expertise to an LLM, after many similar experiences to OP. Don’t ask it for advice, ask it to teach you so you can make decisions like this on your own. Preferably ground questions with excerpts from human made documents. It seems to make less mistakes when explaining things, and those mistakes are more noticeable when they do happen. reply Havoc 17 hours agoprevThat’s a fun writeup. I’ve come to a similar conclusion for now at least it’s best applied at a fairly granular level. Make me a red brick wall there rather than „hey architect make me a house“. I do think OP tried a bit too much new stuff in one go though. USB plus zig is quite a bit more ambitious than the traditional hello world in a new lang reply dstainer 19 hours agoprevOne of the areas where I've struggled to get effective use out of the LLM's is with UI/UX. That isn't my primary area of expertise (backend) so it definitively could be operator error here, but I use tools like v0.dev and just can't quite get it to do what I need it to do. Anybody have any tools, workflows, suggestions for this? reply righthand 14 hours agoprevI spend a good portion of my time asking people to fix their LLM code now at work. It has made code reviews tiring. And it has increased pairing time significantly, making it a less fun activity. reply Joel_Mckay 14 hours agoparentWhen workmanship doesn't matter, than the ship is already sinking. It has been my experience 1 code clown can poison a project with dozens of reasonably talented engineers active. i.e. clowns often go through the project smearing bad kludges over acceptable standards to appear like their commit frequency means something. This is why most developers secretly dream of being plumbers. Good luck, =3 reply petarb 13 hours agoprev> My wife and I have a deal where 1 day a month, She takes the kiddo and I am absolved of all responsibilities. I get a full day to lock in and build projects. I love it! reply BigParm 21 hours agoprevLLM == WGCM = Wild Goose Chase Model reply Graziano_M 16 hours agoprevOP could have written the firmware in zig, too! https://github.com/ZigEmbeddedGroup/microzig reply anigbrowl 21 hours agoprevI've found them tobe quite a time saver, within limits. The blog post seemed scattered and disorganized to me, and the author admits having no experience with using LLMs to this end, so perhaps the problem lies behind their eyes. reply lxe 19 hours agoprevI'm developing an intuition to how and what to ask in order for the LLM's answer to be helpful. Once you start spinning your wheels, clear context, copy what you need, and start over. reply adsharma 18 hours agoprevA few mitigation ideas * Get AI to write tests * Use copy/paste. No IDE * Use python (not because it's better than zig) reply protocolture 18 hours agoprevI used very similar hardware to accomplish a very similar project (Notifications on a round screen) and the LLM was great for everything except UX. reply addaon 21 hours agoprevThere's not much actual LLM-generated text in this post to go by, but it seems like each of the tokens generated by the LLM would be reasonable to have high probability. It sounds like the developer here thought that the sequence of tokens then carried meaning, where instead any possible meaning came from the reading. I wonder if this developer would be as irritated by the inaccuracies if they had cast sticks onto the ground to manage their stock portfolio and found the prophecy's \"meaning\" to be plausible but inaccurate. reply ffitch 16 hours agoprevfor what it’s worth, my afternoon projects tend to take over four days even if no llm is involved reply raleighm 14 hours agoparentLaughed aloud at this one. Yup, same here. reply anaisbetts 20 hours agoprevContext matters a lot, copy-pasting snippets to a webpage is _way_ less effective than Cursor/Windsurf. reply DigitalSea 19 hours agoprevThis is like watching a carpenter blame their hammer because they didn’t measure twice. AI is a tool, it's like a power tool for a tradesperson: it'll amplify your skills, but if you let it steer the whole project? You’ll end up with a pile of bent nails. LLMs are jittery apprentices. They'll hallucinate measurements, over-sand perfectly good code, or spin you in circles for hours. I’ve been there back in the GPT-4 days especially, nothing stings like realising you wasted a day debugging AI’s creative solution to a problem you could've solved in 20 minutes. When you treat AI like a toolbelt, not a replacement for your own brain? Magic. It’s killer at grunt work like; explaining regex, scaffolding boilerplate, or untangling JWT auth spaghetti. You still gotta hold the blueprint. AI ain't some magic wand: it’s a nail gun. Point it wrong, and you’ll spend four days prying out mistakes. Sucks it cost you time, but hey, now you know to never let the tool work you. It's hopefully a lesson OP learns once and doesn't let it sour their experience with AI, because when utilised properly, you can really get things done, even if it's just the tedious/boring stuff or things you'd spend time Google bashing, reading docs or finding on StackOverflow. reply rglover 19 hours agoprev> AI is great for generating ideas or drafting code, but it doesn’t understand. It’s like giving a junior developer a chainsaw instead of a scalpel—it might finish the job, but you’ll spend twice as long cleaning up the mess. For anything remotely complex, this is dead on. I use various models daily to help with coding, and more often than not, I have to just DIY it or start brand new chats (because the original context got overwhelmed and started hallucinating). This is why it's incredibly frustrating to see VCs and AI founders straight-up gaslighting people about what this stuff can (or will) do. They're trying to push this as a \"work killer,\" but really, it's going to be some version of the opposite: a mess creator that necessitates human intervention. Where we're at is amazing, but we've got a loooong way to go before we can be on hover crafts sipping sodas Wall-E style. reply stuaxo 21 hours agoprevThe junior dev faking competence is useful but needs a lot of supervision (unlike a real junior dev we don't know if this one will get better). reply mordymoop 20 hours agoprevI am frankly tired of seeing this kind of post on HN. I feel like the population of programmers is bifurcating into those who are committed to mastering these tools, learning to work around their limitations and working to leverage their strengths… and those who are committed to complaining about how they aren’t already perfect Culture Ship Minds. We get it. They’re not superintelligent at everything yet. They couldn’t infer what you must’ve really meant in your heart from your initial unskillful prompt. They couldn’t foresee every possible bug and edge case from the first moment of conceptualizing the design, a flaw which I’m sure you don’t have. The thing that pushes me over the line into ranting territory is that computer programmers, of all people, should know that computers do what you tell them to. reply rglover 19 hours agoparent> computer programmers, of all people, should know that computers do what you tell them to. Right. The problem isn't that the tool isn't perfect, it's that you get a lot of excitable people with incentives pretending that it is or will soon be perfect (while simultaneously scaring non-technical people into thinking they'll be replaced with a chat bot soon). There are certainly luddite types who are outright rejecting these tools, but if you have hands-on, daily experience, you can see the forest for the trees. You quickly realize that all of the \"omg this thing is sentient\" or \"we can't let what we've got into the world, it's too dangerous\" fodder like the Google panic memo are just covert marketing. reply suddenlybananas 19 hours agoparentprev>The thing that pushes me over the line into ranting territory is that computer programmers, of all people, should know that computers do what you tell them to. are you claiming LLMs function like computer program instructions? like they clearly don't operate like that at all. reply mordymoop 17 hours agorootparentThey are closer to being deterministic machines that comply exactly with your instructions, for better or worse, than they are to magical pixies that guess what you must’ve actually meant. The implicit expectation demonstrated by many in the “loudly disappointed in LLMs” contingent seems to be that LLMs should just know what you meant, and then blame them for not correctly guessing it and delivering it. I think LLMs have uncovered what we have always known in this industry: that people are, by default, bad at communicating their intent clearly and unambiguously. If you express your intent to an LLM with sufficient clarity and disambiguation, it will rarely screw up. Often, we don’t have time to do this, and instead we aim for the sweet spot of sufficient but not exhaustive clarity. This can be fine if you are experienced with that particular LLM and you have a good feel for where its sweet spot actually is. If you miss that target, though, the LLM will not correctly infer your intended subtext. This is one of the things that requires experience. In fact, even the “same” LLM will change in its behavior and capabilities as it undergoes fine tuning. Sometimes it will even get worse at certain things. All of this is to say, of course, you’re right that it’s not a compiler. But I think people fail in their application of LLMs for much the same reason that novice coders fail to get compilers to guess what they intended. reply Dylan16807 14 hours agorootparent> They are closer to being deterministic machines that comply exactly with your instructions, for better or worse, than they are to magical pixies that guess what you must’ve actually meant. If those are your only two reference points, yes they're closer to the former. But the biggest problem is how much \"pixie that does something you neither wanted nor asked for\" gets mixed in. And I think a lot of the complaints you're saying are about lack of mind reading are actually about that problem instead. reply dambi0 19 hours agorootparentprevWhat part of the comment makes you think they are claiming that? reply suddenlybananas 19 hours agorootparentThe part I quoted? I don't really see how to interpret it any other way. reply dambi0 19 hours agorootparentThanks, I’m not thinking clearly at all. I had it in my head that programming instruction are not the only way that a computer might do as it’s told. For example, if we delete a folder in a UI by accident we shouldn’t be surprised the folder is gone. But it doesn’t really quite fit the parent analogy. Sorry about that. reply bigstrat2003 19 hours agoparentprevNobody is complaining that LLMs aren't perfect Culture minds. People disagree with the premise that they are useful tools given their current capabilities. Your portrayal of those with whom you disagree is such a strawman that it might as well be set to a soy-vs-wojak meme. reply mordymoop 19 hours agorootparentThey clearly are useful tools given their current capabilities. It just depends on what you’re using them for. You don’t use a screwdriver to drive nails, and you don’t go to HardwareNews to complain when your screwdriver isn’t working as a hammer. I’m currently using them to port a client-side API SDK into multiple languages. This would be a pain in the ass time consuming task but is a breeze with LLMs because the exact behavior I want is clearly defined and relatively deterministic, and it’s also straightforward to test that I’m getting what I intend. The LLM thus gets done in 3 days what would take me 3 weeks (or more) to do by hand. If the complaint is that it can’t do X, where X is something that would clearly require full AGI and likely true superintelligence — in this case expecting instantaneous, correct code that solves novel problems on the first try then I have to insist that people are actually expecting Claude to be a Culture Ship Mind, implicitly. They just don’t realize that what they’re asking for his hard, which is itself a psychologically interesting fact, I suppose. reply simonw 19 hours agorootparentprev\"People disagree with the premise that they are useful tools given their current capabilities.\" I will argue the opposite of that forever. They're very evidently useful, if you take the time to learn how to apply them. reply satvikpendem 13 hours agoparentprevUse uBlock Origin to block posts with the keywords you don't want in the title, like AI or LLM. reply antigeox 20 hours agoparentprev> I am frankly tired of seeing this kind of post on HN. You've been here since 2016 and this is the kind of posting that finally gets to you? How in the world have you avoided all the shitposts in the last decade? What is your secret? reply mordymoop 20 hours agorootparentIt’s the fact that it’s the same thing over and over. The 100th case of “Well I had a bad experience using LLM for coding!” is not interesting. reply antigeox 19 hours agorootparentThere are about 3-5 (5 being a strict upper bound in my experience over the last 17 years) takes on any given subject on HN that are regurgitated without thinking over, and over, and over again. I think an AI can be enlisted to basically catalogue the HN responses and package them so we don't have to discuss the same shit not for the 100th time, but for the 1000th time. reply j45 14 hours agoprevI doubt one experience, positive, or negative counts for them all. Learning how to build or create with a new kind of word processor is a skill unto itself. reply rekabis 17 hours agoprev> As they say, I was trying to get a few birds stoned at once. Imma gonna have to work this into a convo some day. Just to see the “wait, what??” expressions on people’s faces. reply turnsout 18 hours agoprevCounterpoint: I'm on day 26 of an afternoon project I never would have attempted on my own, and I'm going to release it as a major feature. Cursor & Claude got the boilerplate set up, which was half the mental barrier. Then they acted as thought partners as I tried out various implementations. In the end, I came up with the algorithm to make the thing performant, and now I'm hand-coding all the shader code—but they helped me think through what needed to be done. My take is: LLMs are best at helping you code at the edge of your capabilities, where you still have enough knowledge to know when they're going wrong. But they'll help you push that edge forward. reply 65 18 hours agoprevI have never found a use for LLMs for programming because I can find the (correct) answer much easier with a search. Perhaps the search engines just suck so hard these days people resort to LLMs. I use Kagi and GitHub to search and the results are much better. reply theodric 18 hours agoprev> \"From her perspective, I order doordash and turn into a degen who is unfit to father. From my perspective, I get to enjoy my favorite place and just tinker or play games or do whatever. These are the days I get to play mad scientist and feel most like myself.\" Most demeaning and depressingly toxic thing I've read today... reply tacoooooooo 20 hours agoprevthe \"AI lies\" takeaway is way off for those actually using these tools. Calling it a \"junior dev faking competence\" is catchy, but misses the point. We're not expecting a co-pilot, it's a tool, a super-powered intern that needs direction. The spaghetti code mess wasn't AI \"lying\", it was a lack of control and proper prompting. Experienced folks aren't surprised by this. LLMs are fast for boilerplate, research, and exploring ideas, but they're not autonomous coders. The key is you staying in charge: detailed prompts, critical code review, iterative refinement. Going back to web interfaces and manual pasting because editor integration felt \"too easy\" is a massive overcorrection. It's like ditching cars for walking after one fender bender. Ultimately, this wasn't an AI failure, it was an inexperienced user expecting too much, too fast. The \"lessons learned\" are valid, but not AI-specific. For those who use LLMs effectively, they're force multipliers, not replacements. Don't blame the tool for user error. Learn to drive it properly. reply latexr 20 hours agoparent> We're not expecting a co-pilot Microsoft’s offering is literally called “copilot”. That is exactly what they’re marketing it as. reply cruffle_duffle 20 hours agoparentprev“Experienced folks” in this case means folks who’ve used LLM’s enough to somewhat understand how to “feed them” in ways that make the tools generate productive output. Learning to properly prompt an LLM to get a net gain in value is a skill in it of itself. reply neilv 18 hours agoprev> TLDR AI isn’t a co-pilot; it’s a junior dev faking competence. Trust it at your own risk. A junior dev faking competence while plagiarizing like crazy. The plagiarizing part is why the junior dev from hell might not get fired: laundering open source copyrights can have beancounter alignment. reply groby_b 19 hours agoprev [–] I mean.... No design. Hardware & software. 2 different platforms. A new language. Zig. Unrealistic time expectations. A senior SWE would've still tanked this, just in different ways. Personally, I'd still consider it a valuable experiment, because the lessons learned are really valuable ones. Enjoy round 2 :) reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author embarked on a project called Deskthang, intending to create a desk device using a Raspberry Pi Pico, LCD display, and RGB LEDs, while testing AI's capabilities.",
      "AI tools like ChatGPT and Claude initially assisted but ultimately led to a buggy implementation, causing issues like buffer conflicts and data corruption.",
      "Key lessons learned include recognizing AI as a tool rather than a co-pilot, understanding the value of friction and mistakes in learning, and the importance of patience over overconfidence."
    ],
    "commentSummary": [
      "Large Language Models (LLMs) can be beneficial for simple tasks but may extend project timelines if relied upon for complex problems without proper oversight.",
      "They are effective at synthesizing information but may struggle with niche topics or new knowledge, requiring users to have strong fundamentals and experience.",
      "Users must maintain control by providing clear prompts and critically reviewing outputs to harness the full potential of LLMs effectively."
    ],
    "points": 263,
    "commentCount": 191,
    "retryCount": 0,
    "time": 1738013879
  },
  {
    "id": 42845681,
    "title": "Nvidia sheds almost $600B in market cap",
    "originLink": "https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS VIDEO INVESTING CLUB PRO LIVESTREAM Search quotes, news & videos WATCHLIST SIGN IN AI AT WORK AI AGE AI INSIGHTS AI EFFECT Nvidia sheds almost $600 billion in market cap, biggest one-day loss in U.S. history PUBLISHED MON, JAN 27 20254:08 PM ESTUPDATED MON, JAN 27 20255:26 PM EST Samantha Subin @SAMANTHA_SUBIN KEY POINTS Nvidia shares plunged 17% on Monday, resulting in a market cap loss of close to $600 billion, the biggest drop ever for a U.S. company. The sell-off, which hit much of the U.S. tech sector, was sparked by concerns about increased competition from Chinese AI lab DeepSeek. Data center companies that rely on Nvidia chips also plummeted, with Dell, Oracle and Super Micro Computer all falling by at least 8.7%. In this article NVDA Follow your favorite stocks CREATE FREE ACCOUNT Nvidia CEO Jensen Huang holds a Blackwell GeForce RTX 50 Series GPU (L) and a RTX 5000 laptop as he delivers a keynote address at the Consumer Electronics Show (CES) in Las Vegas, Nevada on January 6, 2025. Patrick T. FallonAfpGetty Images Nvidia lost close to $600 billion in market cap on Monday, the biggest drop for any company on a single day in U.S. history. The chipmaker's stock price plummeted 17% to close at $118.58. It was Nvidia's worst day on the market since March 16, 2020, which was early in the Covid pandemic. After Nvidia surpassed Apple last week to become the most valuable publicly traded company, the stock's drop Monday led a 3.1% slide in the tech-heavy Nasdaq. The sell-off was sparked by concerns that Chinese artificial intelligence lab DeepSeek is presenting increased competition in the global AI battle. In late December, DeepSeek unveiled a free, open-source large language model that it said took only two months and less than $6 million to build, using reduced-capability chips from Nvidia called H800s. Nvidia's graphics processing units, or GPUs, dominate the market for AI data center chips in the U.S., with tech giants such as Alphabet , Meta and Amazon spending billions of dollars on the processors to train and run their AI models. Analysts at Cantor wrote in a report Monday that the release of DeepSeek's latest technology has caused \"great angst as to the impact for compute demand, and therefore, fears of peak spending on GPUs.\" Read more DeepSeek coverage China's DeepSeek AI dethrones ChatGPT on App Store DeepSeek hit with large-scale cyberattack, says it's limiting registrations How China's new AI model DeepSeek is threatening U.S. dominance Nvidia hits new low for session on threat from DeepSeek AI model Buzz around Chinese AI model DeepSeek sparks massive Nasdaq sell-off Pro: The key chart levels to watch on Nvidia and other tech stocks amid DeepSeek rout The analysts said they \"think this view is farthest from the truth\" and that advancements in AI will most likely lead to \"the AI industry wanting more compute, not less.\" They recommend buying Nvidia shares. But after Nvidia's huge run-up — the stock soared 239% in 2023 and 171% in 2024 — the market is on edge about any possible pullback in spending. Broadcom , the other big U.S. chipmaker to see giant valuation gains from AI, fell 17% on Monday, pulling its market cap down by $200 billion. Data center companies reliant on Nvidia's GPUs for their hardware sales saw big sell-offs as well. Dell , Hewlett Packard Enterprise and Super Micro Computer dropped at least 5.8%. Oracle , a part of President Donald Trump's latest AI initiative, fell 14%. For Nvidia, the loss was more than double the $279 billion drop the company saw in September, which was the biggest one-day market value loss in history at the time, unseating Meta's $232 billion loss in 2022. Before that, the steepest drop was $182 billion by Apple in 2020. Nvidia's decline is more than double the market cap of Coca-Cola and Chevron and exceeds the market value of both Oracle and Netflix . CEO Jensen Huang's net worth also took a massive hit, declining roughly $21 billion, according to Forbes' real-time billionaires list. The move demoted Huang to 17th on the richest-person list. The sudden excitement around DeepSeek over the weekend pushed its app past OpenAI's ChatGPT as the most-downloaded free app in the U.S. on Apple's app store. The model's development comes despite a slew of recent curbs on U.S. chip exports to China. Venture capitalist David Sacks, who was tapped by Trump to be the White House's AI and crypto czar, wrote on X that DeepSeek's model \"shows that the AI race will be very competitive\" and that Trump was right to rescind President Joe Biden's executive order last week on AI safety. \"I'm confident in the U.S. but we can't be complacent,\" Sacks wrote. Nvidia is now the third most-valuable public company, behind Apple and Microsoft . WATCH: CNBC's full interview with Bernstein's Stacy Rasgon WATCH NOW VIDEO22:05 Watch CNBC’s full interview with Bernstein's Stacy Rasgon, Trivariate’s Adam Parker and Payne Capital’s Courtney Garcia MORE IN AI EFFECT DeepSeek sets stage for frenetic tech earnings season Jonathan Vanian Early Facebook investor Jim Breyer says Zuckerberg has been 'revitalized' by Meta's AI push Annie Palmer Trump had phone call with OpenAI's Sam Altman last week about AI infrastructure Kate Rooney READ MORE Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2025 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=42845681",
    "commentBody": "Nvidia sheds almost $600B in market cap (cnbc.com)242 points by mfiguiere 21 hours agohidepastfavorite26 comments ChrisArchitect 21 hours agoMore discussion: https://news.ycombinator.com/item?id=42839650 reply dang 16 hours agoparentComments moved thither. Thanks! reply KarmaArchitect 21 hours agoparentprevThank you. reply booleandilemma 20 hours agorootparentAre you two related? reply Netcob 9 hours agoprevThis is really complicated... NVIDIA has probably been overvalued for a while But what is happening now is clearly a panic GPUs are still needed for training and inference, and I guess people will figure that out in the next few days So far, all we have is claims from one source. Seems credible enough, and I'm sure there's massive optimization potential in training LLMs, but someone else should replicate the process first before it's taken as a fact reply Havoc 16 hours agoprevThis drop is wild to me. Everything about DS success screams the future of AI is bright. The advance puts more use cases into feasible space. And then number one supplier for it drops. DS is literally using NVIDIA chips for god‘s sake Markets aren’t rational. Or people don’t understand AI. Or both. Idk reply kfcjligmom 13 hours agoparentNobody wants to be the one left holding the bag reply dadbod 13 hours agoprevJensen: 1000$ Customers: 10$ Jensen: 500$ :( Customers: 20$ Jensen: 250$ Customer: 10$ + your jacket Jensen: :'(( reply pinkmuffinere 21 hours agoprev [–] Pet-peeve: I hate titles like this, “biggest loss in history” is meaningless unless it’s inflation-adjusted, and it almost never is. Is this a bigger loss in real terms? Or is this just a big number? I’m guessing it’s the second. Edit: I read the article, it’s definitely the second case. Is this _also_ the biggest loss in real terms? I don’t know. reply Terr_ 21 hours agoparentSimilarly, \"new record for highest-grossing film\", or \"new record for number of popular votes received.\" reply otherjason 19 hours agorootparentAnd \"our best iPhone ever.\" If it weren't your best iPhone ever, then I would be better off buying one of your older models. reply magicalhippo 20 hours agorootparentprevI like the \"fastest growing app/magazine/whatever\" when the competition has been there for 50+ years. reply lotsofpulp 21 hours agorootparentprev\"xyz company [in business that historically earns low single digit profit margins] reports highest profits\" reply paxys 21 hours agorootparentAnd \"record profits\" were actually less than previous year's profits + inflation. reply paulddraper 21 hours agorootparentprevHey, they clearly just keep making more and more popular movies. /s FYI for the curious: the highest inflation-adjusted lifetime grossing film is Gone with the Wind. reply walterbell 20 hours agoparentprevWould inflation change the ranking? > For Nvidia, the loss was more than double the $279 billion drop the company saw in September, which was the biggest one-day market value loss in history at the time, unseating Meta’s $232 billion loss in 2022. Before that, the steepest drop was $182 billion by Apple in 2020. reply seizethecheese 20 hours agorootparentNot compared to these recent losses, but compared to all history it’s very possible, the value of the dollar drops by orders of magnitude as you go back in time. reply amazingamazing 20 hours agoparentprevCounterpoint generally larger companies should be less susceptible to the type of volatility that leads to the title, so I still think it's newsworthy. Even if you were to change the title to largest percentage single-day loss amount among 3 trillion+ companies it would still be true (might even be true amount 2+ trillion as well) reply Retric 20 hours agorootparentInflation adjusted is still limited to the largest companies. It’s simply using 2025 dollars for companies rather than comparing 1950 or whatever dollars vs 2025 dollars. reply amazingamazing 20 hours agorootparentbut inflation adjusted doesn't make any sense. things are less volatile now, there's high speed trading, better knowledge, easier trades, more automation, regulation, etc. it wouldn't make any sense. even now compared to 2020 there has been a huge change in amount of retail investors. reply Retric 20 hours agorootparentThings aren’t less volatile in terms of extreme events. Look in depth at say the 2010 flash crash one of the all time great examples of very short term volatility. Similarly, longer term we still see huge shifts such as the COVID dip before all that money flooded the markets. reply amazingamazing 20 hours agorootparenti'm talking about the strictly among the biggest companies reply Retric 19 hours agorootparentI don’t have an intraday chart to look at, but DJIA which is just 30 large companies lost ~1.5 trillion in 2025 dollars in that flash crash. So on average each company lost ~50 billion over a few minutes only for them to recover almost as quickly. Though it wasn’t an even split due to differences in market cap etc and it’s not even a listing of the biggest companies. From what I recall multiple companies briefly took a 100+B hit, but again I don’t have the numbers on hand. reply dang 16 hours agoparentprevOk, we've taken that bit out of the title above. reply Andrex 20 hours agoparentprevIt's not meaningless to people who accept that the vast majority of these reportings always use non-adjusted numbers and usually call out when that isn't the case. reply coliveira 20 hours agoparentprev [–] Well, this is what the media does, it is a low hanging fruit that will attract readers. The reality is that NVDA price was way too high and would fall anyway. This was just a catalyst. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nvidia's market cap suffered a historic loss of nearly $600 billion, with shares dropping 17% due to competition concerns from Chinese AI lab DeepSeek.",
      "The sell-off impacted the broader U.S. tech sector, causing declines in companies like Dell and Oracle, and contributing to a 3.1% fall in the Nasdaq index.",
      "DeepSeek's new AI model, developed using Nvidia's H800 chips, has heightened competition fears, affecting Nvidia's stock despite its previous gains, and reducing CEO Jensen Huang's net worth by $21 billion."
    ],
    "commentSummary": [
      "Nvidia's market cap experienced a significant drop of nearly $600 billion, leading to debates about the company's valuation and whether it was overvalued.",
      "Despite the market reaction, Nvidia's GPUs continue to be crucial for AI-related tasks, underscoring their importance in the tech industry.",
      "The media's focus on large financial losses without considering inflation can be misleading, but Nvidia's decline is notable even among major corporations."
    ],
    "points": 242,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1738012390
  },
  {
    "id": 42852400,
    "title": "Janus Pro 1B running 100% locally in-browser on WebGPU",
    "originLink": "https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/",
    "originBody": "whoa there, pardner! Your request has been blocked due to a network policy. Try logging in or creating an account here to get back to browsing. If you're running a script or application, please register or sign in with your developer credentials here. Additionally make sure your User-Agent is not empty and is something unique and descriptive and try again. if you're supplying an alternate User-Agent string, try changing back to default as that can sometimes result in a block. You can read Reddit's Terms of Service here. if you think that we've incorrectly blocked you or you would like to discuss easier ways to get the data you want, please file a ticket here. when contacting us, please include your ip address which is: 52.234.6.33 and reddit account",
    "commentLink": "https://news.ycombinator.com/item?id=42852400",
    "commentBody": "Janus Pro 1B running 100% locally in-browser on WebGPU (reddit.com)156 points by fofoz 4 hours agohidepastfavorite17 comments ndr 3 hours agoThe image generation results are extremely poor, but it's exciting that it does _anything_ in the browser. reply vunderba 2 hours agoparentEven the full 7b model's results are relatively low-res (384x384) so its hard for me to imagine the generative aspect of the 1b model would be useable. Comparisons with other SoTA (Flux, Imagen, etc): https://imgur.com/a/janus-flux-imagen3-dall-e-3-comparisons-... reply littlestymaar 39 minutes agorootparentIt's still very impressive that it gets the cube order right! Also it looks like octopuses are suffering the “six finger hand” syndrome with their arms from all models. reply qingcharles 1 hour agoparentprevI actually had some pretty impressive results (and a few duds). I think we've lost sight of how amazing something like this actually is. I can run this on my low-end GPU in a web browser and it doesn't even tax it, yet it's creating incredible images out of thin air based on a text description I wrote. Just three years ago this would have been world-changing. reply jjice 3 hours agoparentprevI don't know a lot about image generation models, but 1B sounds super low for this kind of model, so I'm pretty impressed, personally. reply diggan 3 hours agorootparentIf I remember correctly, SD had less than 1B parameters at launch (~2 years ago?), and you could generate pretty impressive images with the right settings and prompts. reply refulgentis 1 hour agorootparentJanus Pro 1B is a multimodal LLM, not a diffusion model, so it's got a bit more things to pack in the parameters. It is super low parameter count, in an LLM context. reply salviati 2 hours agorootparentprevYep! Less than 1B in total [0]: > 860M UNet and 123M text encoder [0] https://github.com/CompVis/stable-diffusion/blob/main/README... reply jjice 3 hours agorootparentprevOh wow okay thank you for the context reply amelius 2 hours agoprevThe reason why this doesn't work on Firefox: https://news.ycombinator.com/item?id=41157383 reply pentagrama 2 hours agoprevHappy to have these models running locally on a browser. However, the results are still quite poor for me. For example: https://imgur.com/a/Dn3lxsU reply sdesol 1 hour agoparentIt's not too bad given that runs in your browser. I took your prompt and asked GPT-4o mini to elaborate on it and got this https://imgur.com/a/qmQ7ZHl The burger looks good. reply n-gauge 1 hour agoprevI like the local running of this and learning about how it works. Q:These models running in WebGPU all seem to need nodejs installed. I that for just the local 'server side', can you not just use a python http server or tomcat for this and wget files? reply andrewmackrodt 1 hour agoparentHad a peek at the repo and it looks to be a react frontend, so a JavaScript runtime is needed to \"bundle\" the application in a way browsers can consume. If you had the dist folder then I imagine you can use whatever web server you want to serve the static files. reply jedbrooke 3 hours agoprev [–] well it was a long shot anyway but it doesn’t seem to work on mobile. (tried on iOS safari on iPhone 11 pro) a 1B model should be able to run in the RAM constraints of a phone(?) if this is supported soon this would actually be wild. Local LLMs in the palm of your hands reply nromiun 3 hours agoparentI don't know about this model but people have been running local models in Android phones for years now. You just need a large amount of ram (8-12 GB), ggml and Termux. I tried it once with a tiny model and it worked really well. reply bla3 2 hours agoparentprev [–] This needed a 4 GB renderer process and about that much additional memory use in the GPU process for me, in Chrome. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Janus Pro 1B is a model running locally in-browser using WebGPU, showcasing the capability of executing AI models in a browser environment. Despite its low parameter count, which limits its capabilities, the model can run on low-end GPUs, highlighting its accessibility. While image generation results are inconsistent, the ability to run such models locally in a browser is a significant technological advancement, though it currently does not support mobile devices."
    ],
    "points": 156,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1738073061
  },
  {
    "id": 42855283,
    "title": "Berkeley Researchers Replicate DeepSeek R1's Core Tech for Just $30: A Small Mod",
    "originLink": "https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"xyzlabs.substack.com\",cType: 'non-interactive',cRay: '90933c7908e8fa72',cH: 'F09P0DTcCQiKfI3FWCiX5sYyR9cRrKtsREobavskTVQ-1738090940-1.2.1.1-9HC2vxJ_xrjQyDLLroW70_ZYbdXPZ3dOG1ozyQEujzafHOAqy9x340vi4Ofy_Ia6',cUPMDTk: \"\\/p\\/berkeley-researchers-replicate-deepseek?__cf_chl_tk=Zp9zsL4zxM0gUpB5DxREXe2xzKWjO44ZV3KQ0seo8ZM-1738090940-1.0.1.1-gTEG.ORwr8m2X4fTB_qgpp_CK3jPYnLeaZbW_5pqDDs\",cFPWv: 'g',cITimeS: '1738090940',cTTimeMs: '1000',cMTimeMs: '120000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/berkeley-researchers-replicate-deepseek?__cf_chl_f_tk=Zp9zsL4zxM0gUpB5DxREXe2xzKWjO44ZV3KQ0seo8ZM-1738090940-1.0.1.1-gTEG.ORwr8m2X4fTB_qgpp_CK3jPYnLeaZbW_5pqDDs\",md: \"llk_S2hyPDE_zHMFck_hABqgs07gE6Ej9Vy.Py7w2QM-1738090940-1.2.1.1-7GFU_q2k2dIkZRMY71suv8NQFNfHw7LLjUsGHfhdb7zFnYmv2W9fobuMV_AYJ39.LVGbzPeFpxNDyS4Gizw7HrhnpHeAitf6m3.gspiWsFpeStNh.GMjGh7RDHZz57unwFD25GYdZqappaylfyhR7l4DGSSsMonwfexPOjair7oMVOZXENM6w_3TEn6TWB.4APEJ.rDhzRTTZU49sd966IV.1QaTVVcEnuXNZ7uojKwRmW0C3kRHrXiq9LOg1mtQBGi1Ws5DXih3cWhFOXoeNc74VhruSAGTtJGCnC76r6W0NMr.HTSXiGO.UE8S4jPZs5HjeCePpa8MY_iyh0SBiFmCxCOFEL.4e4kWqnz2FbAUozaI1nw_md_sIssYVVs0X4fHsD.IzO7flV9hG.5nyd0MKoLY4VnemABKOgHaPSIVx_ZEqna.uZZDY6CX0adB_l3ko.2qXVlQfm51_7IyV5cSyZFATaYdSlH0RDt7u5.9sLGuyQWAaqI934Ga.DR.qZzw7rWdPGN4adrNVLSUfjFm_LgDzp9jUSXTw8y2_4jEkpZ98fHaONFP.fjZL3xEDWXY.UqFdI8UgQRlQZDPBPEfb.46QBvVs_56LER_P.945UT9rtQxhL9HQoLIWIfx54xLkc9a59W0QHs4NlwRPJu2KaJxXk0a4tjk._bR5vpm03dpEqa8vq9KZMRc0fkZIMoqPTSvojE36ki5kO0N2kKrTG4M3uNXo7ys4._Ow5X4ywMmvaSCdyfxTUgRiN_Ibb5xQVcULT9DldJCV9UNXZCUSvjztT_Ri_535EwwWXTEcl8IDBPpxHY0sm_cfWM8gjkp9vL8xxb0IboJEd6AcPOvXfOxIOEl14oG.Q7tN_bP9k9yo02g8ILseMxtEPpx9qDAvGG.PCbg7NiREszcspVoOUMpIgbKWXjxhTNZReOrRgdiNXY.aDt5t7g99buvUkUrHtuNDmh2HeAQvXRbivQgWyVKM7Yl6uP_CyOcU11pmkK.vYCw2GDCZveBGOi_0B81SSOEm9OArw_aCke0aX4vdF1ne_FKt3lLpavcYuUuy2t7o.MuD6PLv2XTkHjgg1QqkRqjtsuPdXHAKNwfUXwMcQYkHMEUDh6kihzqGYecqce6H2RuM5z8SHrJ4GVU.jVguJ6nRZReShajOosBtfUbi8YoRXKWHlWn8sJe2FbNDr7vAzXpidMQhX0hEDo8Ik5X5TeH0q0LhrlExR_S5SXNedZrd3VZpyCm.jkszort8L2T.3kWZ9ORc56mQaPWC7x4xzlLWR2I6_8yki0NVaeqexX113PHppCZ7h6vKfryFtEycdsEJI3qQChJlbnsjT.8TNpyHNjnZZ5W9MSNr90NKv38R5SpnvavHSfuvG6jkx1WM6EvHnwe9LINU2gCuU8_49H7BVYAaZsmNkaus.X0ZoPkX7WdYx0_qgHOlaLbJDWifvP8diBfHaaz0n446QomF7.DVaLQbcZ1CmDX5DVBN9W5T4rrzoQ0HzpbAKFb69ZJ9_EwBsbzAM2EiELW.324v3fTae0PqbvLCC0vWZdL5XffH4sgM7CAjg1SzztaDxD9DLLg8zhrMAhmHJtpFDPVrv23aWujIx2ZX0mSNUBekG4s_bUtbAO1Dwg.mXW33Y.p75DByQqH1FEwDu.jwWLV1U8bUh6EsjnX9qZ3C4PSdwzf0Nmg7tRZo9CTypAB8XdFkO71BX8lCpzdvNRh9.y85FPnfebTp6SP.959HiDWp2msP5k1FQSmuB8TleRbEXHJkEuWflXu7QrT4xGaVht.ns.7naDxyoY_SK3e.NuFeBAOlz4LOZXPTvp244x8ph9H8cnbN2uaBMMc_QRO\",mdrd: \"AWotqazH18mRmKfIyaedvDl7e1yUl4q0loJdg9Jbelc-1738090940-1.2.1.1-0j6DPYJ6BveaA7QC7IWuZCRnOrAd.otKEG3OkD_HfYMH3KCsxhWPV1J4gLpwDVAHouWuUFGjD8luv7d7p7XizI6blPhW38VFPrwX8qdntUMO9veOREIdS9vTUApHPLKoZHQ1v3lZpMJaxNlSl2g4u8STEpayJSJAkIfv.FzP5PwOVyXeKgiiivz9Ngm2wlqwIjPYoRo5lU2nJBY8vkSg8nCBLoG.B0wO9VnP9wlJG95Aoil6Uw.6keTL4ncvidRBpKVglTVrewwmJZmqpkpou3nsjJAUXnAecq0B8FCQtNq9ooCdjgQUUvLuy_OmwcyqEAQVCjpMavsaidjF29Z6rSwEwxvOH4YrZ9m6HoZ3F3YHnw27IP71KEAP.dzoxMjd24aFd_qCDcXbOnXnaYNgCtDb8Pxz.vETbZqaPMSXkAXDwQ4dySaDC4RS2q8oObghhr.UInbV.Pf_zC2QAhQXFP.afqXWErFX9Zvxm6d3dhcdjUnJuH608_Z2WGRKfUcWtrs49_dLAfdl80dqsNx3ysUANMEb8oZ1p.PPMEtLueEehECEkzI0cw.rWewSAE.NL2BZePua12aXbdyiPdqmXqrs3iLwNhEffTsXU5eR8irOt1UJHN0O8K30kNcfzxxQyKPDh8PCBJjjk2qugtCZ0ezoGrdYGxXy1EIg9tdqNimBUA2BlsbTWP.eUJn.G0VenwfCSN3jH9VBPNCKeFRSmGSGAeobCtIo_8vwRDiuf_vCgNvu9ttbeRZyCbz4189qt3DSIMsySz5C6rnUdqGOBqnMrf78UOBV5V8HV_w2GFm4EoL1qH2n4CdWXlZ6jZsEuoio1GtjH8Pw1AmIc6Fgd3IZVtMFDhCeduC8tqgWTnogchWvnh.Qzoj_mY7P.LftGAZ.N4BiFICJgB5YCEQu4Etl_R0rXmgibD3nQvftZzNHT3Ctr7qy8oS9pcCx99lOI38Jwn8tBbHbd7ozC4eFr899iH1Kkjerz.7uAU_2xUcoeQB63tAXSyAxKXV5nCT.np1dqPoe0AuG8RfmUyletZ66DsJEs7jyi5NWGPQECtPBkjoPq7pavzh615Jto6il95TBpFNJh3Oio.o0HheEIY3OZXcvRwWJF0oIPI.e6_4mw50iGFEIJnZQUJZ3fnCNouM9jfJ5517mBLvgiG3HAbD.R1q1mvEWfGwgTEiciA03HuCkmHLAZ7WbTeo.zJiv7V07W0PUif7ky0HupPLYOR4kmVH1U3p9SMyBkE.MszH8UkR4dTUeeXvuHJPKR5n5Fnev7FA0b9P9VFhVqBADzpcGaNpbP5c73mBJjAYQyZJ6ic9glScxHqTpCMtlJN1Gz4Bz8_cfVUcMkTZrRmceYiPwlp1AeKGMwHbtAgC7lSYVwgCgcmqluu03qlZNEfnYBBflOujHoDV2okR1HXv182XUC8UBLEcXfUYfUyJYtF7.frglLMgebMUFzl3UFUovfkJs0lYQX06Yd2lcz_cLJTtc.ZtHglmzmQKtPzvqejVZH9r_zTxNQp4y85hay4q7orTqKpElU_3ObAtUMuwvTxi1QdCJH2IQeA9JJl4rUz.44eRywDWifvSbCtZJbRJ6_sPY9EpzVUAKq_NH1AcfQjhajwQIhXKmqBpxeIbzdrsNcZJokvNcJwYqw9jm1FFiL8NHpLkUe_HsclKLBO6TISeHUgZGSCp4tKwYW_Q_ylXZvMDR5sAktT.cRkkGQwN6323GCYABREwcG_E0xIwLPoU41EXbAR_IkobJNVsO478YQBGvAfOOz_12rqBG1sZE7qJJ3tjkb_CJUj39oyPtVmYGP8dLu78nSMs6hjZocI50rhQd6DJC4eCKoxs9lpCEfj9kGtjPUq15FEt41EqqPrwm_Lx_7O9motxj.cpNwoSnDeTxphaQBPuC9o8BknrcMHmG0wFeb813xdUeyfCSUtJU1l89hJnMi7geOJXoOnlWsGsASncbj16qbzyC2Kl9O1u20itckF90Hmn691bZbQGc2nPjmOmXPTCvMemRc2X511NrR7RU2MtZDqll_4zAHRBStg0AgyzwQRGbOv4v4b5frJyDNVyqmOf0m0AWV8MzDfgNqOsp8F.eQAqqetwimuIdWmPQy_0u5ZjifRxd0nPEvEu1RfyJ1PtP4mobHQPSwf8DrsNLB8yzawQvHGT1NaCwNyza0GauLt2mXtq2KtJRxq_ES9r6dRbmT7lFLhcTzOqff62GoNA6Z_DXF8DMTJ4w_Fenwej9MxcgSrqpf.0lt.FZHX24IsBSaWcU8Uq239rC_ukeqRi_odRn_8ikQXeTgOBvcXx5CThXPtxxycMdiCfSHh3I1PRMD_7g5Tfq7yvgYDuLEzJPp00bsbRA1BKEjGblNDUZCTEmGAu1ImP3tp3bARLF6unxWlM_6hk\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=90933c7908e8fa72';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== 1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length window._cf_chl_opt.cOgUHash.length).indexOf('?') !== 1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/berkeley-researchers-replicate-deepseek?__cf_chl_rt_tk=Zp9zsL4zxM0gUpB5DxREXe2xzKWjO44ZV3KQ0seo8ZM-1738090940-1.0.1.1-gTEG.ORwr8m2X4fTB_qgpp_CK3jPYnLeaZbW_5pqDDs\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42855283",
    "commentBody": "Berkeley Researchers Replicate DeepSeek R1's Core Tech for Just $30: A Small Mod (xyzlabs.substack.com)153 points by semking 1 hour agohidepastfavorite57 comments ipsum2 54 minutes agoReposting the comment from https://news.ycombinator.com/item?id=42843959: This is blogspam of https://github.com/Jiayi-Pan/TinyZero and https://nitter.lucabased.xyz/jiayi_pirate/status/18828393705.... This also doesn't mention that it's for one specific domain (playing Countdown). See also https://news.ycombinator.com/item?id=42819262. reply semking 45 minutes agoparentI sincerely do not understand this comment so please clarify: are you saying I'm spamming? Is there an issue with the source? reply ipsum2 44 minutes agorootparentThe title of the post is misleading and the contents are blogspam and do not link to the original source. reply semking 38 minutes agorootparentI just added the links here since the substack failed to do it. And I left them a note on the sub to add the credits! Thank you for pointing this out! reply ipsum2 29 minutes agorootparentThanks! reply UncleOxidant 0 minutes agoprev\"TinyZero is a reproduction of DeepSeek R1 Zero in countdown and multiplication tasks.\" Does that mean that this has very limited utility (to certain math problems)? reply cluckindan 1 hour agoprevIf the current hubbub around DeepSeen is really because they ”created their model” with like $5M when previously ”creating a model” cost $500B, it is rather obvious that ”creating the model” with just $30 implies the meanings of the three ”creating a model” expressions are highly divergent. reply TheTaytay 1 hour agoparentSo far it is literally 3 different uses of the phrase “creating a model”. I could further obfuscate it by fine tuning the work of these researchers on a single step and claimed I “created a model” for $0.05. :( I feel like that’s what everyone who is naming their model “R1” is doing right now… This article is trying to make the headline sound as if they replicated the model, when they “just” replicated the theory of one of their findings. It’s a great result even without the author’s obfuscation. reply PaulHoule 50 minutes agorootparentThese models are highly specialized models that reason similarly to DeepSeek's new model except instead of reasoning over the general domain they do something specific like arithmetic. They're using a similar model and a similar training method to do something simple. It is to DeepSeek what DeepSeek is to ChatGPT-4o, and this kind of reduction to the simplest case is exactly what makes progress go. (e.g. ChatGPT-4 is a dead end because it is so expensive to train that you can't do training experiments. At $30 a model it is reasonable to train a model 1000s of times) reply nyrikki 21 minutes agoparentprevThat may be the current hubub, But how they built on other open concepts like Cold Start + RL + Rejection Sampling etc... is the big thing. I am not at all connected to whomever this is, but as I am on a \"confidential project for a confidential client\" it seems to simplify what I would overcomplicate anyway. https://youtu.be/Pabqg33sUrg Some of us need to improve domain specificity, and are more interested in targeted capabilities. DeepSeek-R1-Distill-Qwen-32B itself is pretty good to be honest, but it is more aspirational how easy it is to add reasoning to any base model IMHO. Those of us who remember Simon’s satisficing principle, realize that often complex problems sub-optimal solutions. We don't need, want or expect some central omniscient and omnipotent AGI, we want tools that are sufficient to solve real world problems. Obviously having good reliable domain specific CoT for cold start etc... is hard. But not nearly as hard as hitching the companies success to MS/OpenAI. While there is a lot of noise...I think some people are missing the forrest for the trees here. Deepseek is more about moving the decision to use OpenAI for some needs from a single source vendor to one of convenience for us. For those of us who are in the camp that OpenAI was never going to reach their \"AGI\" claims because of fundamental issues like the frame problem, this is huge. Ya...we won't be training our own models for some silly budget of ~$5M like the press is claiming. But we won't have to for some use cases... I think it will take time and someone on a project with not such a strict NDA to demonstrate just how much this 'completes' things in ways that RAG, functions etc... didn't. Or I just possibly got lucky with my needs? who knows. reply simlevesque 1 hour agoparentprevNot really, the two first examples you named are the same meaning. reply drawnwren 1 hour agorootparentThey aren’t though. $5M is the cost of a single training run. $500B includes the cost of operations, data center, a lot more failed runs because they weren’t sure that they’d were on the right path etc. reply onlyrealcuzzo 58 minutes agorootparentCompare to what you think a single run cost then. It was orders of magnitude more before DeepSeek. reply tomrod 37 minutes agorootparentSo far. And there might be orders of magnitude left to improve! Deepseek R1 is a training architecture improvement cool stuff! [0] [0] https://newsletter.languagemodels.co/p/the-illustrated-deeps... reply EA-3167 51 minutes agorootparentprevRight. It's like building a large model rocket and saying that you've cracked rocketry for a fraction of the cost that was required in the 1940's and 1950's. Well yes, yes you did, because all you had to do was follow the existing instructions, guidelines, and use easily available materials. You didn't go down any dead ends, didn't have to work your way from propellants like high test peroxide, dangerous hypergolics, and eventually develop solid rocket boosters. It's like making the generic of a drug someone else developed. reply dkjaudyeqooe 25 minutes agorootparentYou're more or less describing how all progress on anything, ever, happened. Even if you merely flipped a single bit and created AGI based on existing tech, you're still the legitimate creator of AGI. reply deadbabe 38 minutes agorootparentprevSo what’s your point, everyone developing a model should be forced to spend the same as what the first movers did? reply nightpool 23 minutes agorootparentNo, just that what DeepSeek did is not as valuable as what the first movers did, because it did not advance the state of the art nearly as much. It's a new cheaper way to go from Base LM > CoT \"reasoning\" LM. We already had CoT \"reasoning\" LMs, so while the new cheaper path to get to them is interesting, it's not necessarily groundbreaking either. Also, R1 only works with the \"cold start\" data that they distilled from o1, so it's not quite clear that it'll ever be able to exceed o1's capabilities. We already know it's much cheaper to distill new, smaller models from large already pretained and well-performing models—in fact, $5M sounds like a very expensive way to do so. So while these new techniques are probably going to have some impact, OpenAI is far from quaking in their boots reply skeaker 24 minutes agorootparentprevProbably just that it's not as impressive as it appears because it didn't innovate. Which is of course irrelevant since the innovative leap here were the optimizations that let them make their model with an order of magnitude fewer materials, regardless of whatever innovation costs OpenAI ate. reply bilbo0s 58 minutes agorootparentprevThe first two are not the same thing. DeepSeek never claimed to have trained the base models they used. Now maybe a lot of people inferred that they did, but that's not what they claimed. Their breakthrough was more along the lines of, \"given a model, we can train it to reason using a certain class of RL techniques\". Which is, to my mind, more useful in any case. But yeah, if there are people out there thinking they can train base models from scratch for USD6 Million with no data, they're likely to be disagreeably surprised when they make the attempt. reply btown 3 minutes agorootparentThere's a lot of conflicting information here. There are two papers: https://arxiv.org/pdf/2412.19437v1 for DeepSeek-V3-Base, which makes claims about 2.8M H800 GPU hours, and https://arxiv.org/pdf/2501.12948 for DeepSeek-R1, which was based on V3-Base but doesn't seem to quote the cost of its RL steps. https://newsletter.languagemodels.co/p/the-illustrated-deeps... is a really fascinating overview of what those new steps were! The innovation here seems to be that the creation of a cutting-edge reasoning model could be made from a relatively inexpensive base model, regardless of how much RL is needed. Part of this seems to be a novel methodology for generating large volumes of chain-of-thought training data; it's left ambiguous exactly how the bulk of this data was generated, though, and it could have been a long process of manual curation. I'd also take the assertion that they started R1 with V3-Base in its release form with a grain of salt. We'll see whether people can reproduce similar results. Either way, there's a new chain-of-thought model with open weights, and that's an incredible achievement regardless of the context and one that will be meaningful to future research. reply supermatt 49 minutes agorootparentprev> DeepSeek never claimed to have trained the base models they used Isn't that exactly what they have done? Maybe you are confused with the distilled models? DeepSeek-R1: \"DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base\" DeepSeek-V3: \"At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours\" reply chpatrick 45 minutes agorootparentAren't they based on qwen? I thought the clever bit with DeepSeek is a way to fine tune with reinforcement learning, not training a huge model from scratch. reply vineyardmike 34 minutes agorootparentNo… mostly. They developed their “zero” model, which (they claim) is a from-scratch model. Base models are typically not fine-tuned for particular applications (eg chat). They trained their Zero model into a Chat+Reasoning model, which is what is attracting news. They ALSO fine-tuned small Qwen models using their big models as a teacher (distillation technique). reply supermatt 31 minutes agorootparentprevdeepseek-r1 (reasoning) is a post-trained (not simply finetuned) deepseek-r3 (base model) by deepseek. qwen is a completely separate 3rd party model by alibaba. deepseek distilled (finetuned on the outputs of) r1 into qwen. They also did the same with llama. reply snake_doc 41 minutes agoprev@dang please link to either the GitHub https://github.com/Jiayi-Pan/TinyZero or the primary source twitter thread: https://x.com/jiayi_pirate/status/1882839370505621655 reply semking 40 minutes agoparentThanks, I just commented on their sub to give credit! reply nightpool 20 minutes agorootparentCommented where? I don't see any source links in the substack article. reply semking 11 minutes agorootparentGo to the bottom, you'll find my comment with the links: my name is Elie Berreby. reply highfrequency 1 hour agoprevFirst graph tells the story below a certain model size (500m params), reinforcement learning is close to useless. Above this (task-dependent) model size threshold, reinforcement learning basically works. I suspect this is what we saw play out with math/coding reasoning models until recently, the base models were not good enough for ~random output search to hit on a correct path with any reasonable frequency. Below this threshold of base model intelligence, the only efficient way forward was to collect plain supervised data (either through human labeled math problem solutions [1] or meticulous filtering of web text [2]. But as soon the base model (in this case Deepseek V3) breaks through and can actually solve a decent fraction of math problems, then reinforcement learning (plus other simple tricks like chain-of-thought prompting, simple ensemble voting, etc.) can easily juice the results through the following loop: 1) random search through different solution paths 2) identify the correct solution paths based on the final answer 3) train on the correct solution paths The exciting thing is that not only can RL bump up the performance of the current base model, but it can be used to generate new high-quality reasoning trace data, which was in painfully short-supply for training the initial models. This leads to a new wave of base models with better one-pass intuition, which leads to more efficient reinforcement learning search on harder problems, which leads to better training data... Note that this was basically impossible for non-LLM models in the past. You could always juice ImageNet classification performance with a simple ensemble of identically trained models, but that path didn't lead anywhere interesting because a juiced model didn't allow the creation of new synthetic data that was superior to the data it was trained on. The key difference is that LLMs not only output the solution but also output a solution path with all the intermediate steps and these searched-and-filtered solution paths are much more valuable than the vast majority of the model's initial training data. [1] https://arxiv.org/abs/2305.20050 [2] https://arxiv.org/abs/2402.03300 and https://arxiv.org/abs/2206.14858 reply whimsicalism 56 minutes agoprev'replication' requires matching benchmark performance, definitionally. more like 'demonstrates the technique generalizes' here. HN has really been inundated with blogspam recently reply aurareturn 1 hour agoprevThis is truly the biggest breakthrough from DeepSeek that an LLM can teach itself to reason, no human feedback needed. That’s nuts and brings forward the idea that an AI is close to self improvement. reply thorum 1 hour agoparentIt’s great, but it only works for problems where there is exactly one correct solution and it’s possible to automatically verify the solution like math and programming. So far these reasoning models have not shown much transfer learning of reasoning to other domains, and are often worse at non-math/code tasks than standard models. reply aurareturn 49 minutes agorootparentLLMs can verify math problems if you give it a calculator tool. Coding problems if you give it a Linux environment. So why can’t we give LLMs other virtual environments to verify if the solution is correct? For example, a stock simulator, a physics simulator, a driving simulator, etc. reply onlyrealcuzzo 56 minutes agorootparentprevThere are infinite solutions to coding problems. You could have lots of comments and pass statements and unnecessary conditionals. I don't think it matters that there is only one correct answer. It matters that you can verify reliably if the answer is correct enough. reply onlypassingthru 33 minutes agorootparentprevOnce the computer learns to apply the logic of Tic-Tac-Toe to Global Thermonuclear War, we will finally have the W.O.P.R.[0] [0]https://en.wikipedia.org/wiki/WarGames reply vintermann 53 minutes agorootparentprevWe can probably make it work at more nebulous problems by using a big LLM to judge the quality of the answers as well. It should be easier to recognise e.g. a really good poetic translation than to make one, and as long as that's true it could benefit from internal monologue \"reasoning\" in those domains as well. reply timr 36 minutes agorootparentThis has been done, and doesn't work especially well. See, for example, GANs, which are difficult to train and vulnerable to mode collapse. Not saying that it's impossible, but reinforcement learning of the form shown by DeepSeek was particularly well-established and robust. It's sort of ironic, actually...IIRC, OpenAI started in the world of this kind of reinforcement learning. reply datameta 58 minutes agorootparentprevNot sure if this a trivial or naive thought is that perhaps because in non-discrete ideas there is more granularity of information encoded compared to a numerical solution? Separatelt, but on a related note do we need analog or quantum computing to \"truly\" scale? reply JKCalhoun 54 minutes agorootparentprevSo, we have a very good left-brain model. reply segasaturn 1 hour agoparentprevThis makes ScaleAI obsolete right? reply EGreg 1 hour agoparentprevWe already knew that kind of stuff from AlphaZero vs AlphaGo AlphaGo Zero is a version of DeepMind's Go software AlphaGo. AlphaGo's team published an article in Nature in October 2017 introducing AlphaGo Zero, a version created without using data from human games, and stronger than any previous version.[1] By playing games against itself, AlphaGo Zero: surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0; reached the level of AlphaGo Master in 21 days; and exceeded all previous versions in 40 days.[2] Training artificial intelligence (AI) without datasets derived from human experts has significant implications for the development of AI with superhuman skills, as expert data is \"often expensive, unreliable, or simply unavailable.\"[3] Demis Hassabis, the co-founder and CEO of DeepMind, said that AlphaGo Zero was so powerful because it was \"no longer constrained by the limits of human knowledge\".[4] Furthermore, AlphaGo Zero performed better than standard deep reinforcement learning models (such as Deep Q-Network implementations[5]) due to its integration of Monte Carlo tree search. David Silver, one of the first authors of DeepMind's papers published in Nature on AlphaGo, said that it is possible to have generalized AI algorithms by removing the need to learn from humans.[6] Google later developed AlphaZero, a generalized version of AlphaGo Zero that could play chess and Shōgi in addition to Go.[7] In December 2017, AlphaZero beat the 3-day version of AlphaGo Zero by winning 60 games to 40, and with 8 hours of training it outperformed AlphaGo Lee on an Elo scale. AlphaZero also defeated a top chess program (Stockfish) and a top Shōgi program (Elmo).[8][9] Source: https://en.wikipedia.org/wiki/AlphaGo_Zero reply aurareturn 1 hour agorootparentIndeed. But DeepSeek is the first to release a working repro for LLMs as far as I know. reply simlevesque 1 hour agorootparentprevYeah but LLM and game AIs are very different. An AI can easily tell if it won the game but an LLM can't know by itself if what they said was useful. reply DannyPage 1 hour agoprevUnless I missed it, it seems strange that the article wouldn’t link to the Github repo for the TinyZero model. https://github.com/Jiayi-Pan/TinyZero reply semking 32 minutes agoparentYou are right! I commented here and on their substack to give the credit to the OPs! reply semking 39 minutes agoprevGuys I'm sorry but it appears the substack did NOT link to the original authors which is NOT acceptable! Credit GitHub: https://github.com/Jiayi-Pan/TinyZero Source on X: https://x.com/jiayi_pirate/status/1882839370505621655 reply semking 35 minutes agoparentI just left a comment on their Substack to give the credit to the OPs. reply nick3443 58 minutes agoprevWould it be correct to summarize that the general conceptual shift is optimizing MOEs on more specific smaller tasks? It smells like borderline overfitting to me for some reason. reply whimsicalism 55 minutes agoparentthe core insight is that you can train on extremely sparse deterministic reward signal and it just works that MoEs are better for a given compute budget has been known for a while. reply fp64 1 hour agoprev$30 is “less than a dinner for two”? reply jedberg 59 minutes agoparentFor a grad student. It's about 2 burritos in Berkeley. :) reply SubiculumCode 21 minutes agoprevSell Nvidia last week? Seriously. Or is it that now we can make smaller models more powerful, and then run more of them to get more work done. reply oytis 52 minutes agoprevIs it some kind of a joke? reply xigency 49 minutes agoparentThis reads like an AI hallucination. I'm willing to steak-out this ground even if I'm wrong because of the glaring lack of skepticism. I don't even know when dollars became a concrete compute measure. We used to use FLOPs before we were trying to pull headlines like it were a claw machine game. reply semking 41 minutes agorootparentThe title is click-baity if you ask me. I didn't change it. reply excalibur 34 minutes agoprev [–] Good to know that our AI overlords will be built as cheaply as possible. If there's one thing I can't stand about bondage it's inefficiency. reply Consider applying for YC's Spring batch! Applications are open till Feb 11. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Berkeley researchers have successfully replicated DeepSeek R1's core technology for just $30, focusing on specific tasks such as playing the game Countdown.",
      "The innovation involves using reinforcement learning, a type of machine learning where an agent learns by interacting with its environment, to enhance reasoning models, though its application is limited to areas with verifiable solutions.",
      "The discussion emphasizes the potential for AI self-improvement and its implications for future AI development, despite criticisms of the article's misleading title and lack of proper source links."
    ],
    "points": 153,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1738085791
  }
]
