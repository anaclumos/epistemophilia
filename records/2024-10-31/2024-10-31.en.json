[
  {
    "id": 41999314,
    "title": "Steam games will need to disclose kernel-level anti-cheat on store pages",
    "originLink": "https://www.gamingonlinux.com/2024/10/steam-games-will-now-need-to-fully-disclose-kernel-level-anti-cheat-on-store-pages/",
    "originBody": "Steam games will now need to fully disclose kernel-level anti-cheat on store pages By Liam Dawe - 30 October 2024 at 7:35 pm UTC Valve announced a change for Steam today that will make things a lot clearer for everyone, as developers will now need to clearly list the kernel-level anti-cheat used on Steam store pages. In the Steamworks Developer post Valve said: \"We've heard from more and more developers recently that they're looking for the right way to share anti-cheat information about their game with players. At the same time, players have been requesting more transparency around the anti-cheat services used in games, as well as the existence of any additional software that will be installed within the game.\" Developers with games already on Steam will also need to do this, as it's not just for new games coming up for release, and it is also part of the release process now too. So Valve will be doing checks on games to ensure the notices are there and correct. However, it's only being forced for kernel-level anti-cheat. If it's only client-side or server-side, it's optional, but Valve say \"we generally think that any game that makes use of anti-cheat technology would benefit from letting players know\". Valve's example pictured below: Some games don't properly remove everything when you uninstall either, with Valve noting this happens with older games, so it's nice to also see this will be noted too. This should make it a bit easier knowing what to avoid on Steam Deck / Desktop Linux now too, since kernel-level anti-cheat games are usually blocked by the developer from running with Proton. Valve have done a few rather interesting Steam updates lately like dealing with publisher banner spam, implementing improvements for Native Linux games, actually noting clearly when you go to purchase a game it's just a license and removing the need for individual arbitration. Article taken from GamingOnLinux.com. Tags: Anti-Cheat, Misc, Steam, Valve 46 Likes Share About the author - Liam Dawe I am the owner of GamingOnLinux. After discovering Linux back in the days of Mandrake in 2003, I constantly came back to check on the progress of Linux until Ubuntu appeared on the scene and it helped me to really love it. You can reach me easily by emailing GamingOnLinux directly. See more from me Some you may have missed, popular articles from the last month: Get your own UFO in the latest No Man's Sky update during a really weird Expedition Ubuntu Summit 2024 highlights - Linux gaming talks on UMU and Heroic Launcher NVIDIA 565.57.01 Beta has Wayland and HDR improvements, plus DXVK and VKD3D optimizations Fanatical give you another chance to build up your Resident Evil collection 24 comments Page: 1/3» Go to: 1 2 3 wytrabbit about 23 hours ago Link View PC info Mega Supporter Steam Yay for transparency! 21 Likes, Who? Pyrate about 23 hours ago Link View PC info Great news. Hopefully this starts a similar \"game contains Denuvo\" sentiment from Steam users (on Linux or otherwise), so that it slowly builds more disliking to the concept on a wide scale, not just Linux players. 20 Likes, Who? Philadelphus about 23 hours ago Link View PC info Steam Website YouTube What does \"requires manual removal\" in the second screenshot mean? 6 Likes, Who? pb about 23 hours ago Link Steam As they always should. 0 Likes robvv about 23 hours ago Link View PC info Quoting: PhiladelphusWhat does \"requires manual removal\" in the second screenshot mean? Presumably, you need to crack the game yourself ;-) 14 Likes, Who? scaine about 23 hours ago Link View PC info Contributing Editor Mega Supporter Steam Website YouTube twitch mastodon Quoting: PhiladelphusWhat does \"requires manual removal\" in the second screenshot mean? Might be wrong, but I think some of these tools install outside of the game itself, adding themselves to the \"add/remove software\" tool in Windows, or whatever that's called these days. So you install the game, it installs the kernel-based rootkit/spyware, you later uninstall the game, but the rootkit/spyware is still there, potentially still running at startup, until you remove it. We Linux users have it lucky in that regard. We might still install the \"rootkit/spyware\" element in some cases, but because it's tied to Proton emulating (shut up pedants!) the Windows environment, when we stop the game, we also stop all the anti-cheat crap. In our cases, it only gets to run when the game is running. 22 Likes, Who? ShadMessa about 23 hours ago Link View PC info Steam W 1 Likes, Who? ElectricPrism about 22 hours ago Link View PC info Steam QuoteSteam games will now need to fully disclose kernel-level anti-cheat on store pages Reasons like these are why I continue to patron Steam and make it rain . No other store even comes close. (GOG is my second favorite. After the Internet is cancelled over \"The New Red Scare\" I'll happily be playing my mountain of games offline for quite some time. 13 Likes, Who? Schleichfahrt about 22 hours ago Link Quoting: scaine Quoting: PhiladelphusWhat does \"requires manual removal\" in the second screenshot mean? Might be wrong, but I think some of these tools install outside of the game itself, adding themselves to the \"add/remove software\" tool in Windows, or whatever that's called these days. So you install the game, it installs the kernel-based rootkit/spyware, you later uninstall the game, but the rootkit/spyware is still there, potentially still running at startup, until you remove it. We Linux users have it lucky in that regard. We might still install the \"rootkit/spyware\" element in some cases, but because it's tied to Proton emulating (shut up pedants!) the Windows environment, when we stop the game, we also stop all the anti-cheat crap. In our cases, it only gets to run when the game is running. + Click to view long quote from scaine Exactly, I played the Delta Force demo on my Windows machine during Steam Next Fest and later uninstalled it. Noticed that the \"ACE\" anti-cheat was still there with 3 services (ACE-BASE, ACE-GAME and a third one). They weren't running, but it's shady to leave the anti-cheat behind after the game has been uninstalled. (Yes, I realize that installing kernel-level anti-cheat is insecure in the first place and at least in theory renders that machine compromised.) Regarding the store page anti-cheat disclosure: That's a fantastic change. Guess there's no chance to make it a bright red box? Last edited by Schleichfahrt on 30 October 2024 at 8:58 pm UTC 2 Likes, Who? Vortex_Acherontic about 22 hours ago Link View PC info Steam Website YouTube twitch Quoting: ElectricPrism QuoteSteam games will now need to fully disclose kernel-level anti-cheat on store pages Reasons like these are why I continue to patron Steam and make it rain . No other store even comes close. (GOG is my second favorite. After the Internet is cancelled over \"The New Red Scare\" I'll happily be playing my mountain of games offline for quite some time. Agreed. Steam and apparently Valve are awesome! I just hope they will not change direction some day in the future if maybe Lord GabeN is no more :/ 1 Likes, Who? 1/3»",
    "commentLink": "https://news.ycombinator.com/item?id=41999314",
    "commentBody": "Steam games will need to disclose kernel-level anti-cheat on store pages (gamingonlinux.com)877 points by jrepinc 23 hours agohidepastfavorite607 comments JohnMakin 1 hour agoI'm absolutely not a fan of giving a game the level of kernel access these games take without my permission. That said, cheating is an existential threat to a good game experience online, and I really don't know of any other solution. What's been frustrating over the years is when kernel anti-cheats, unbeknownst to you, are causing issues with other games entirely unrelated to what you are playing - because some game developers had a bad habit of leaving them running/idle even if you were not playing. I have a dedicated laptop for gaming that I do absolutely nothing else on, not even logging into email accounts. Just steam + games + whatever video software I might need. This is my sane compromise as someone who participates in a lot of competitive games. it sucks, but I see no better solution than to disclose it (insane this wasn't the standard already). Even that is hard, because if you disclose too much, cheaters can take advantage. reply alexandre_m 1 hour agoparent> and I really don't know of any other solution Behavior AI-driven anti-cheat system running on the server side. reply JohnMakin 1 hour agorootparentThis sounds nice, but it isn't a thing that would really work. This excellent post in this same thread explains why: https://news.ycombinator.com/item?id=42004965 reply pphysch 42 minutes agoparentprev> cheating is an existential threat to a good game experience online, and I really don't know of any other solution. Cheating is downstream of the trust problem with an anonymous global \"community\". Anti-cheat is just an arms race, but there are other avenues to addressing it (weak \"KYC\", like requiring a phone number to play ranked modes). Deadlock is potentially doing something interesting with its invite-only EA system. They're essentially building a social graph of players, which could be leveraged to identify \"cheating rings\"/compromised accounts and seed some kind of social reputation system within the community. reply JohnMakin 39 minutes agorootparentThis is a really interesting and potentially great idea, but I am fundamentally against the KYC thing because it puts me in a position where I'm at the whims of whatever BS automated moderation systems they put in place these days, which is usually quite terrible. reply fullspectrumdev 1 hour agoparentprev> That said, cheating is an existential threat to a good game experience online I never understood this. As I mentioned in another thread, maybe I’m just old or something now, but I’ve never given a fuck if another player was cheating. Back in the day it was pretty normal to run into someone using aimbot or wall hack or whatever shit. You would just change server or join a different lobby or whatever if it was really bothering your enjoyment. reply thanzex 1 hour agorootparentThat's not a thing you can do on many games nowadays. I assume this comment is also related to this thread https://www.theverge.com/2024/10/31/24284644/apex-legends-lo... There's no concept of \"server\" or \"Change lobby\" on Apex or other Battle royales. You just queue up for a game, which lasts ~20 minutes. As you are in a 3 player team if you disconnect from the game you get a temp ban penalty, since that also degrades other players experience. So there's no disconnecting freely once a game has started. Now imagine you're playing for 10-15 minutes just to die without really having any chance. That gets frustrating, really quickly, since winning is close to the only \"reward\" you get from playing the game. It's not like a classic COD or Battlefield game, where you can feely leave or join any game/server. Once you're in you're somewhat committed, and you have no control over where or with whom you're playing against. reply JohnMakin 1 hour agorootparentprevGames with ranked systems will always be cheated. If you play competitively and attain a sufficiently high rank, you will encounter cheaters inevitably. If cheating hits a certain threshold (or even the perception of cheating), the system collapses because people won't compete in a system they perceive as unfair. It isn't a matter of just changing servers when there are consequences for winning/losing. Even worse, many competitive games offer specific, tangible rewards such as items/skins/etc. for winning, which heavily incentivizes cheaters even further. I have definitely stopped playing games completely because of cheating issues - one dumb one particularly for me are m/kb players on playstation FPS games, which is trivial to stop and detect, but they won't do it. I'd vastly prefer something invasive on my playstation to prevent that experience than the alternative, which is to just not play. reply yreg 19 hours agoprevI've just reinstalled GTA V last week and I was very surprised to find out that I now have to install a rootkit to run it. They had the balls to add a mandatory kernel extension into a game that I've bought 10 years ago and that I wish to play in single player only. I find it utterly ridiculous. As usual, piracy would have been the superior experience. reply mrpippy 13 hours agoparentAdd ‘-nobattleye’ to the Steam command-line launch options and you can play single-player without any installs. (Rockstar really should’ve made this a separate launch option like other games do) reply soderfoo 12 hours agorootparent> (Rockstar really should’ve made this a separate launch option like other games do) This is what happens when a producer's/product manager's cherished KPIs come before UX. In their mind, adding a toggle in the launcher would lead to lower engagement and player acquisition. We, the players, fail to recognize how our gaming experience can be enhanced by using social features like leader boards, guilds, or in game chat. We are not enlightened. Think about all the fun and exciting connections you'd miss out on if all the social crap was off by default or in an easily accessible place. I'm honestly surprised it's a command line option. My guess is that the requirement originated externally. reply parl_match 9 hours agorootparent> This is what happens when a producer's/product manager's cherished KPIs come before UX. > In their mind, adding a toggle in the launcher would lead to lower engagement and player acquisition. Not \"in their mind\". It does and they have the data to show it. Very frustrating situation. reply _factor 5 hours agorootparentGovernment regulation of toxic waste dumping negatively impacts profit margins. We have the data to prove this, very frustrating situation. reply xelamonster 2 hours agorootparentBut that's why the government had to regulate it. If companies have financial incentive to do something they're going to do it, to make them stop that incentive must be removed. I don't think that comment was intending to justify the situation. reply Workaccount2 5 hours agorootparentprevTo be fair, the societal harm from deciding to play multiplayer instead of single player is probably a few orders of magnitude less than toxic waste in the water. reply crest 4 hours agorootparentGranted, but how about the societal harm from an exploitable bug in one or more of these rootkits? Millions of gaming computers could wreck some havoc in the hands of an even slightly creative attacker not restrained by moral or economic considerations… reply hx8 4 hours agorootparentprevPeople cannot say no if you never ask for informed consent. If the PC is personal then a root kit is a violation. reply bippihippi1 3 hours agorootparentread the TOS reply bravetraveler 9 hours agorootparentprevIt's GTA, if they want to get to Mars... release six there. Just shoot the gold copy out there. People will show up. reply ahartmetz 9 hours agorootparentNot sure if, at this point, it costs SpaceX more money to get to Mars, or Rockstar Games to develop GTA 6. AAA budgets are insane (with IMO meagre results - I don't like most of them). reply ljm 3 hours agorootparentA lot of time and money goes into AAA games and I think there’s an inverse relationship at play where the higher the budget is, the more risk averse the studio is. So you end up with a whole suite of AAA releases that all play it safe and just copy small innovations from each other, nobody really daring to push the envelope too far. I don’t really enjoy those games either any more. Too big, too long, and the gameplay feels more like running errands and checking off a todo list than having fun. Release cycles in the 90s and early 2000s were pretty tight, slowly getting longer as storage and graphical firepower increased. 3D was totally new and studios were trying out all kinds of crazy ideas for games. These days you can basically expect 90% of mainstream releases to follow the same playbook. reply bravetraveler 9 hours agorootparentprevTough call. Everything is bloated reply soderfoo 8 hours agorootparentprevYeah, it's just me being an idealist and projecting. I acknowledge what the analytics show, but always allude to the hypothetical casual loner segment who we lack data on because we pushed them away or we don't measure things relevant to them. I'm a boomer millenial, or whatever we're called now, and never took to online gaming, so I'm part of this segment. Casual loners are irrelevant to the monetization and in game economy people, resulting in relegation to second class status. Until someone figures out a way to milk this segment for that juicy recurring revenue, consumable$, $kins, etc..., we must accept our fate, largely an afterthought. reply bitsandboots 6 hours agorootparentPeople who can't be monetized sometimes are valued in their opinions. If someone principled really likes a thing, it can gain more popularity by others who trust that person adopting it. Maybe not loners. But there's still reason to make users happy because viral marketing impact cannot be measured well. reply NeoTar 4 hours agorootparentprev> I'm a boomer millenial, or whatever we're called now, and never took to online gaming, so I'm part of this segment. And you are not alone - apparently 53% of gamers (total) prefer single-player games, [although this falls to only 30% in the 16-19 years age-group]. https://www.midiaresearch.com/blog/most-gamers-prefer-single... I wonder how much of this is familiarity (i.e. I play games in the style I did when I was sixteen) versus people in older age-groups having less sustained time for gaming (i.e. grabbing twenty minutes while the baby sleeps) and single-player being inherently better for that use-case. reply wing-_-nuts 3 hours agorootparentPart of it is reflexes too. I used to love fast paced FPS games as a teen and was actually pretty good at them until my early 30's. As time went on, I started noticing I was doing consistently worse in 1v1 firefights. I started gravitating towards games that had a 'slower' way to contribute like playing vehicles in the battlefield series. As time goes on I've gotten more and more into single player games, especially games that let me build stuff. reply kjkjadksj 2 hours agorootparentprevThis is because the multiplayer games we played at 16 are 10-15 years old. You literally can’t go back to those happy memories again. You play halo 3 today you get wrecked by the people who haven’t stopped regularly playing in over a decade. And when you try and play the latest fps game you go this sucks, its not halo 3. reply dingnuts 3 hours agorootparentprevREAL-TIME multiplayer is worse for that use case. There's no reason a game that is asynchronous, like an old school play by mail game, couldn't be fun for twenty minutes when you have time, and maybe there's a limit or a turn involved so you don't get too ahead, and your partner does their thing when they get time I have wanted to see a game like that for years and years. I think this is why chess.com is huge with the youths, as it fits my description and is fairly unique -- I just don't personally care for chess. reply nemomarx 2 hours agorootparentit's a little old but check frozen synapse - top down tactics games like that have done asynchronous multiplayer a decent bit reply Eisenstein 7 hours agorootparentprevYet we are the people who started gaming on PCs -- we made consumer 3D accelerators profitable and spent the time writing about them on forums in the 90s and 2000s. We certainly have the power to move markets. reply romwell 7 hours agorootparentprevI'm disagreeing with everything you're saying here. Not because you're wrong. But because I'm in this picture and I don't like it, and I just found out that \"boomer millennial\" is a fitting descriptor. Goddamit. reply Uvix 6 hours agorootparentYou’re in denial now; you’ll work your way to acceptance soon enough (as someone who’s done the same thing). reply veunes 11 hours agorootparentprevIf the decision came from an external party, it's likely from publishers reply gigatexal 8 hours agorootparentprevCall of duty 6 launches the single player campaign from the main launcher and I noticed they advertise the anti cheat stuff being enabled (I forget what it’s called). For a single player game. Smh. reply kjkjadksj 2 hours agorootparentAchievements are serious business reply nottorp 7 hours agoparentprevI stopped getting Rockstar games for PC when the Steam store page for Max Payne 3 was lying that a rockstar club account was only required for multiplayer. Turns out it was mandatory for single player too instead. I did get GTA V for playstation, and indeed you can play single player without the rockstar account. Probably Sony forcing them to allow that, I don't think they did it out of the goodness of their hearts. However, every time you start the game you get a screen pushing you towards multiplayer. And the single time I did click on some multiplayer related options, they spammed my playstation system notifications with 'there are new events in GTA online' even in weeks I didn't start their title at all. So... good bye, rockstar. Your games are getting too big for their own good anyway. reply crtasm 19 hours agoparentprevReaders can search keyword Grand_Theft_Auto_V_v3095-Razor1911 for further information on experience options. reply alwayslikethis 6 hours agoparentprevWe need to stop normalizing the idea that businesses can change the offer of a purchased product after the fact. > These companies are all run by CEOs who got their MBAs at Darth Vader University, where the first lesson is “I have altered the deal, pray I don’t alter it further.” https://pluralistic.net/2023/12/08/playstationed reply veunes 11 hours agoparentprevPiracy shouldn’t feel like the \"premium version\" of a product... reply tantalor 4 hours agorootparentThis is true for watching NFL. Option 1: subscribe to 7 streaming services which each have some unpredictable subset of games Option 2: go to some website that has all the games reply ascagnel_ 1 hour agorootparentThe MLS did this with their Apple deal (pay a maximum of $120 for a season and you're guaranteed to get every league match of the season through the \"TV\" app), and it's been reasonably successful. That said, the league used that money in part to bring in Lionel Messi, and his significant international following came along with him, so it's hard to parse out how much of that is because they made the games easy to access (with the significant asterisk that Linux and Android users are stuck with a web app) versus his singular impact. The other part of this is that MLS is significantly smaller (~$275m in non-team-sponsorship revenue in 2023) than other North American leagues (the NHL, the next-largest league, had ~$6.75b in non-team-sponsorship revenue in 2023), so I don't know how reasonable it is for other leagues to follow the MLS's path. reply bitsandboots 6 hours agorootparentprevOwnership is the premium version of a product. Turns out piracy is sometimes the only ownership option. Everything else is just a subscription, license, service. reply johnisgood 8 hours agorootparentprevBut the reality is that it often is. reply strogonoff 6 hours agoparentprevBig game publishers will inevitably flex the “we can impose any restrictions we want at any time, go sue us” in your face at some point. Piracy is indeed the superior experience, unfortunately, though I would add a caveat that as a rule it does not apply to indie titles. (At least while they stay indie… Not long ago I was reinstalling Minecraft after a long break and found out that Microsoft has the balls to demand that I verify a phone number to play a game I bought more than 10 years ago. Like with GTA V’s rootkit, they don’t care if you want to play single-player—once you’re locked out of the loader, you’re locked out and no human will help you.) reply lmm 9 hours agoparentprevSo why didn't you pirate it? At some point people need to stop complaining and start putting their money where their mouth is. reply dspillett 6 hours agorootparentIt had already been bought some time ago, this was stated as a reinstall. Using a pirate copy now might avoid the rootkit, but it would not send a message to the publisher as they'll not see a difference between not getting any money for this new install and not getting any money for this new install. Any difference in a numbers-playing stat, if the pirate version doesn't call home to be counted in those, is likely not significant. reply neilv 14 hours agoparentprevOne of the reasons I run a PS5 instead. AFAIK, there's fewer cheaters on PlayStation current-gen than on PC, and I don't have to worry about anti-cheat kludges corrupting my \"rig\". reply Aeolun 12 hours agorootparentYou mean all the anti-cheat options are pre-built into your rig? reply pjc50 8 hours agorootparentYes. It's kind of an odd situation, because it's one where it's a benefit to me if other people are running anti-cheat. A limited sort of remote attestation that the people you're playing with aren't running certain kinds of software that peeks into or alters the memory image of the game or its graphics drivers. reply grbrr 12 hours agorootparentprevYes, so you don’t have to run a rootkit on a machine that you might file your taxes on. reply echoangle 12 hours agorootparentI would bet 90% of people here have at least another laptop if they have a gaming PC, if you’re concerned about being compromised by rootkits, just do your taxes on that. reply lostlogin 10 hours agorootparentSo you own one machine and Rockstar owns the other? reply echoangle 10 hours agorootparentWell that was supposed to be the deal with the PlayStation, too. reply Dilettante_ 9 hours agorootparentprev\"Your computer calls me root too\" reply stavros 10 hours agorootparentprevThere's nothing \"just\" about compromising one of my machines so badly that I don't trust it to file my taxes on any more. reply portaouflop 12 hours agorootparentprevWhich is fine on a device that you only use for gaming, or am I missing something? reply dmichulke 12 hours agorootparentWell, you run untrusted code in your local network. Then again, we all trust our \"smart\" devices even if we really don't. I suppose a separate network would be the safe option (if you trust your router). That said, have there been rotten meat attacks using \"temporarily above temperature\" fridges? Are vegans just applying a defensive strategy against those? reply lxgr 5 hours agorootparentprevNot nearly every \"gaming PC\" is used for only that. When I grew up, I had one PC to do everything: Homework, gaming, learning to program, storing the single copy of treasured family photos, gaining painful experience in why to make backups before modifying the MBR to dual-boot Linux... Especially with iPads and Chromebooks becoming more prevalent in an education context, a \"gaming PC\" might well be the only computer that gives the user full control over it that many children have access to these days. reply wing-_-nuts 2 hours agorootparentI get where both sides are coming from. On the one hand, buying a console and a reasonably spec'd laptop is clearly the better value. I did this during college, and both my laptop, and my console both lasted about a decade without requiring any upgrades. I did this again with the PS4. You wind up spending far less than you otherwise would trying to keep a gaming pc reasonably up to date, and both devices are optimized for their usage. On the other hand? At some point most of us will realize that we've been successful enough that we don't have to optimize for value, and we can choose 'all of the above'. I now have a PS5 AND a stupidly overspec'd AI / gaming desktop. I've enjoyed having both. reply prmoustache 3 hours agorootparentprevComputers were much more expensive back in the days and would be obsolete much faster. Nowadays for 50€ you can have a decent second hand computer with an older gen core i5 and 8 to 16GB of memory. That is plenty enough to run qubesOS. reply Woeps 8 hours agorootparentprevThis is the exact reason why is started with streaming services for games (Gfore/boosteroid/game pass). Next the anti-cheat thing I also spend less waiting for updates. And this way I can still play these games with my buddies. reply pixxel 11 hours agorootparentprevYou’ve just got Sony watching over you and transcribing your audio conversations with friends. reply generalpf 10 hours agorootparent“Shoot that guy” “Ok” Super valuable stuff. reply notabee 3 hours agorootparentOr more likely, listening to background noise to spy on what family members are saying and listening for marketing/brand trigger words. It may not be very human audible but if it's machine audible it will probably be scraped. reply croes 10 hours agorootparentprevTraining data for AI reply WithinReason 9 hours agorootparentSony is training the AI drones for the battlefields of the future from gamer conversations reply shultays 7 hours agorootparentNot only future drones will get better at killing you, they will also learn to shit talk after doing that reply Lanolderen 7 hours agorootparentprevCan't wait to hear \"Who you challin lil bro\" as I get shot in the ass by a drone. reply IG_Semmelweiss 5 hours agoparentprevHonest question. Would a Windows10-11 user be able to tell there are \"rootkits\" embedded in installations, without looking at the (optional) disclosures made available now on steam ? In other words, what guarantee is there that if i'm buying a game from Steam, or say GOG, that there's no quasi-malware riding along with the game install ? reply nemomarx 2 hours agorootparentin general anti cheat will advertise that it has kernel level access and might need admin permission during install if you're worried about hidden stuff I'm not sure how you could tell reply k4rli 11 hours agoparentprevWhat's even worse is with this update they completely cut off Linux users. It had been performing better than on W*ndows but they had to ruin the game. Surely this is foreshadowing the future of GTA VI and will have the same problems of being unplayable. reply dartharva 7 hours agorootparentAs he said, pirating is the better option. I'm guessing all pirate releases of GTA V will run perfectly on Linux. reply ho_schi 9 hours agoprevFirst step. Second step? Ban games with kernel-level anti-cheat. It is not acceptable on Linux. Apple will also not accept that shit further, that said Apple lost relevance in gaming with Mantle and the M-Processors (both mean a lot of incompatibility). And Microsoft is regretting every choice in this regard: https://www.theverge.com/2024/9/12/24242947/microsoft-window... But that is a usual pattern. Microsoft makes bad decisions and everyone suffers. Even Linux. Their is a reason why closed-source kernel modules mark Linux as tainted, the system is not trustworthy. It is the duty of game developers to secure their games themselves. Not manipulating user devices. Forcefully doing stupid and dangerous things because you cannot achieve your task in a safe why is not a reason. reply wrren 5 hours agoparentThat's an incredibly naive perspective. KLA represents a real risk to companies, as something going wrong can crash player computers instead of just game processes; this is a PR nightmare if/when it happens on a large scale. Not to mention the cost of hiring engineers capable of building kernel components in the first place, it's a niche skillset that's not cheap to hire for. Games companies don't turn to KLA out of laziness, it's out of absolute necessity, especially for games like FPS' where it's impossible to fully secure the game using pure server-side methods. Machine learning has been tried, it's too prone to false positives and misses more subtle cheats that still negativel impact the the player experience. Anti-Cheat used to exist purely in user mode and then, guess what?, cheats moved into the kernel where they couldn't be detected or stopped. Anti-cheat had to follow in order to remain effective. The alternative was conceding the space to cheaters and watching games that players love, and that required massive resources to develop and maintain, degenerate into a hellscape of cheating that real players refuse to play. reply montagg 2 hours agorootparent> The alternative was conceding the space to cheaters and watching games that players love, and that required massive resources to develop and maintain, degenerate into a hellscape of cheating that real players refuse to play. This was a key reason for Valorant's success. Anti-cheat is a necessary evil to make online games fair. I think if someone wants to suggest otherwise, they would need to demonstrate a superior solution. Companies truly do not want to be in the business of messing with your kernel, so if another solution exists—one that is actually superior in cheat detection and prevention—without a kernel extension, they'll do it. I'll provide another example of why companies would rather not do it unless they have to. Kernel extensions usually require a system restart. Requiring a restart adds a huge drop off point to a conversion funnel and costs the game some amount of players who may have stuck around, and some players, like the ones here who are upset about it, won't even bother because they are outright opposed. Games would gobble up a solution that worked and didn't have that baggage. reply ikety 3 hours agorootparentprevI'd believe this if every multiplayer game that doesn't have KLA was just rife with cheaters. Also, why is EA adding KLA to Battlefield 1 almost a decade after release? I refuse to believe there is only one simple honorable objective when it comes to KLA. I simply do not care if companies can't figure out how to stop cheaters without it. What about our experience? You might complain about cheaters, but what if I can't even play the game because of KLA? Played plenty of recent MP games on Linux just fine and cheating was never experience breaking. reply jakebasile 3 hours agorootparentprevWe had the best alternative decades ago. Let the community run the servers and ban cheaters while allowing individual servers to form their own culture and community. The obsession with matchmaking and games as a service (requiring publisher run servers) is what painted the industry into this corner. Note that I like matchmaking, specifically skill based matchmaking, in some games and at some times but completely ending server browsers and community run dedicated servers was a mistake. reply xvinci 2 hours agorootparentI was pretty good at CS:S, semi-professional level. I nevertheless still enjoyed hanging out on public servers with friends. I cannot tell you how often I have just been banned. Another example: I was an anti-cheat admin in a major league about 20 years ago. I am quite confident a lower double digit percentage of banned players were innocent - it was simply too hard to get enough competent people for doing manual checks (you'd have to be really good at the game yourself to confidently tell what might be intuition and what cant while evaluating pro players with money-prizes on the line). So while I appreciate that sentiment, and maybe you found THE one community where all that really worked out for you, but it was by no means the \"best alternative\" from where I am standing. reply jakebasile 2 hours agorootparentAs I said, offering both is the ideal. I'd rather have some false positives if it means a way forward without kernel rootkits becoming the new normal. reply notabee 3 hours agorootparentprevBut then it's harder to shim in the money-makers like microtransactions, loot boxes, and all the other recent \"innovations\" in the gaming industry. reply LinXitoW 2 hours agorootparentprevUntil the obviously detectable cheaters like spinbots get INSTANTLY banned, I don't believe for a second they've \"tried everything\". If hackers have to limit themselves to behaviour that looks like real skill, it's kind of unimportant to the games community if they're cheating. Cheaters only ruin the experience if they're obvious to the player. Finally, something like Overwatch in CS, together with paid employees and PUBLIC bans of high paying cheater accounts (including hardware bans) would create a chilling effect, further forcing cheaters into hiding. reply dartharva 7 hours agoparentprev> Second step > \"Ban games with kernel-level anti-cheat.\" > \"It is the duty of game developers\" It's hilarious that people actually think the publishers will ever have two hoots to give about notions like that. FWIW I agree we'd all be better off without those things but the entitlement to believe private businesses should run on your personal whims and that developers have a \"duty\" to make things only as you prefer is gobsmacking. I am always left in wonder what commenters like this think about themselves. reply LinXitoW 2 hours agorootparentI agree that profit driven companies will never ever care about the morality or the safety or the social impacts of their choices. I don't think, however, that \"publishers would use disabled children slave labor if they could\" is really an argument you ought to make. reply moffkalast 2 hours agorootparentprevThat's right, they won't care. That's why any action will only come if Valve bans the practice from their store, because losing money is all publishers understand. Every publisher that's greedily taken their products off Steam in an attempt to recuperate Valve's fee has sooner or later come crawling back because they weren't able to make a half usable platform to sell their stuff on. Valve holds a really large influence over the industry. reply devnullbrain 2 hours agoparentprev> tainted, the system is not trustworthy. I wonder what is worth more: the median installation of a free-to-play multiplayer game, or access to everything they do on that PC? I won't be the first one to wonder this. reply Arch-TK 9 hours agoparentprevThe reason games companies reach for KLA is not because they're dumb and can't be bothered to secure their network protocols on the server side, it's because they don't want to have to hire an inordinate number of human reviewers to make unreliable decisions on whether someone is cheating or not in their game. While KLA is fundamentally flawed (DMA and even CV based cheats are becoming more popular as a result of KLA and they still give cheaters a significant (but now even harder to detect) advantage) it solves the problem of obvious and even most kinds of subtle cheating. Attempting to detect cheating once inputs are being sent to your server (which is within your domain of control and on which you can implement non-intrusive anti-cheat) is very difficult to do reliably. An inexperienced player will make slow, delayed inputs. A highly experienced player will have reactions which are an order of magnitude faster (and in many cases faster than the speed of thought because of muscle memory). If you want to make a working but no longer detectable cheat, all you need to do is spend a bit of time and effort programming in human limits of reaction time into all your code and making sure all inputs look realistic (again, more limits). At the end of the day, you can make a cheat which gives you a significant advantage without it being actually detectable by any statistical methods on the server. At this point you might attempt to reach for AI but undoubtedly that will require human oversight or you will get false positives. So, in summary, even if you were to design your game around server-side rendering and server-side input processing, forcing your players to effectively play over a remote desktop connection (which is impractical for any fast paced competitive multiplayer game due to latency issues but let's pretend those don't exist for a minute), you will still get cheaters with snap-to-head or recoil compensation or auto-fire making a significant impact on games. Heck, there's even the idea of using sounds (which need to be pretty accurate so human players can utilise them to determine where enemy players are) to implement a rudimentary wallhack. This is just the nature of FPS games and why games companies end up implementing KLA for these games. The way to make an FPS game un-cheatable is to make a different game where cheating is more difficult or impossible just by nature of the format. Want FPS without cheaters? Encourage people to do DIY matchmaking again, DIY server hosting and DIY administration. Except that \"this doesn't scale\". Neither does human review. Neither does server side rendering. The core reasons why game companies do KLA is that players will pay for games with KLA but won't pay for games without it. As much as I think Microsoft is one of the worst companies in existence, in this case I don't think they or KLA developers are to blame. KLA developers are simply doing what players want them to do and Microsoft is only allowing what their end-users want them to allow. If Microsoft removes KLA, it will be by replacing it with userspace code with hardware attestation support, it won't be by killing the concept of intrusive anti-cheat. All Microsoft is doing is trying to re-design the tools to cover their own ass. Fundamentally, KLA has pushed cheating further into DMA and CV territory. This means that more obvious and annoying forms of cheating, undetectable by KLA are probably going to soon become more common. At this point the options are to have these games be console-only with blessed hardware and hardware attestation. And even that has flaws (as described). Eventually it will just be impossible to play a game in a public server without cheating. Maybe this will force people away from these types of games, or towards private lobbies. I don't know what the future holds here. reply aenis 3 hours agorootparentAnd fundamentally, the anti-cheat software is fighting a loosing game because of some quirks of x86: - hardware breakpoints on memory latch to logical, not physical addresses - one can map the same area of physical memory to different logical addresses thus allowing arbitrary memory reads - simulating inputs through the PnP stack can be made indetectible with polymorphic code doing the injections - detecting the aimbot's core code can be prevented via at least two methods: triggering (and capturing) page faults on memory hosting the bot or, again, with polymorphic code generation One could create a fully open source cheat engine relying on memory reads and input injection via the pnp stavk and it would be undetectable deterministically (with heuristics, sure). Games - unless streamed - need to know a bit more about the world they display than the player is allowed to see. Some engines I analysed (years ago) tried to limit propagating data to the local client to only a portion of the map and close proximity (the more recent battlefield games, if memory serves), while lots still have all enemy coordinates at all times in memory, some even with debug symbols to help the poor aimbot community reverse memory structures that more easily. Its hopeless. The only winning move is not to play :-) reply kjkjadksj 2 hours agorootparentprevRealistically if you make a game hack where it behaves exactly as well as a high skilled player, no one should care. The lobby experience is the same as if you got a really good player in there. When people moan about hackers its when they are blatant. Dominating the entire server and obviously hacking when you spectate them. If you take away all this low hanging fruit who cares about the people who are cheating within statistical variance of player skill? reply Arch-TK 1 hour agorootparentI think people happen not to care as long as they don't know, but if they were made aware then they would care. Games companies rely on the fact that people don't care if they don't know because it means they _only_ have to rely on KLA to get rid of most people with trashy spinbots. reply Eisenstein 7 hours agorootparentprev> Want FPS without cheaters? Encourage people to do DIY matchmaking again, DIY server hosting and DIY administration. Yes. This is the answer. There is no solution that relies only on technology that will fix human problems at scale. You can make it harder, but once one person figures it out, everyone has access to it. You need to break it into pieces and let people deal with it on the community level, where communities are small enough that people can deal with individuals. The main issue is that 'at scale' you have the fact that a certain percentage of people are sociopaths, a certain percentage of people are really smart, and a certain percentage of people have the time and motivation to do something, and when they all cross you end up with a person who breaks your system. reply Arch-TK 6 hours agorootparent> Yes. This is the answer. There is no solution that relies only on technology that will fix human problems at scale. You can make it harder, but once one person figures it out, everyone has access to it. You need to break it into pieces and let people deal with it on the community level, where communities are small enough that people can deal with individuals. The issue is that the players themselves seem to think that the convenience of KLA (over running your own community, paying for your own servers, etc) outweighs the privacy risks. reply shkkmo 4 hours agorootparent> The issue is that the players themselves seem to think that the convenience of KLA (over running your own community, paying for your own servers, etc) outweighs the privacy risks. That's not true and there isn't data to show it. The reasons why support for community run servers have fallen by the wayside are more complicated than \"players chose one over the other and companies followed them\". Companies also recognized significant downsides for themselves of community run servers. When your online experience is in community run spaces, you expose your player base to experiences outside of your control. When you don't run your own server instances, you can't enforce the cheating rules you want. These, among many other reason, make it harder and more risky to monetize the online experience of your game. A such, the choice was generally taken away from players to protect companies. This wasn't at all a pure reaction to player preference. reply kjkjadksj 2 hours agorootparentprevReally? Most gaming communities seem to hate kla as it breaks their steam deck compatibility or linux compatibility. reply raincole 1 hour agorootparentMost players don't use Steam Deck or Linux. reply BoorishBears 6 hours agorootparentprevIt's only the answer if the goal is to not feel like you're being cheated by cheaters: realistically cheating would be absolutely fucking bonkers if you actually tried to treat DIY matchmaking as a form anti-cheat. Voting to kick/ban and sharing ban lists only works for ragehacking, but the most joy stealing form of cheating is people using hacks and cheats subtly enough to beat an average player, but not strongly enough to guarantee they'll be caught by the community. reply amiga386 5 hours agorootparentI don't think anyone is saying \"DIY matchmaking\", they're saying \"community servers\" - and everyone in any given community knows everyone else, ideally IRL, and any newcomers are going to be watched, both for their technical performance but also their social performance. Cheating among your friends has the ultimate death penalty of losing those friendships. Most people don't want that. This is why most cheating is not among close friends, but where there's an open buffet for the energy vampires; anodyne corporate \"matchmaking\" where the game corporation will place you with other customers of the game corporation at its choice. The cheater does not have to face a panel of their peers, they only have to slip past the inattentive corporation to be placed in a server with delicious innocent strangers who they will never see again once they've had their fun. reply porkbrain 2 hours agorootparentEquating cheaters to energy vampires is great comedy, I'm stealing that. I'd love to see Colin Robinson play CS 1.6 with wallhack in his basement. reply immibis 6 hours agorootparentprevHow's that any different from just having a really good player in your community - which, in many cases, also spoils the fun? reply BoorishBears 6 hours agorootparentAre you seriously asking me how getting killed by someone cheating to know exactly where you are feels different than just being outplayed? reply Ajedi32 4 hours agorootparentActually yes. If you've never been falsely accused of cheating in an online game by people who can't tell the difference between that and getting outplayed, chances are you haven't played any multiplayer games long enough to get any good at them. reply tl 3 hours agorootparentThe only real answer for this is self-imposed handicaps. reply BoorishBears 1 hour agorootparentprevYou'd end up pretty awful at games if you treated getting killed by something that has no counterplay the same as getting killed by something that does... A person who's good enough to know where you are for in-game reasons is something you supposed to learn to respond to: They aren't cheating so they're using heuristics and information that you're supposed to learn to counter play. A person who has omniscience at the press of a button does not play similarly at all. Often times that's how they're even caught: since they're not bound by any sort of in game heuristics no counterplays work against them. - In some games that even makes cheaters a liability when they're on your team. With any ranked game mode \"soft cheating\" will result in them climbing to a rank where non-cheaters have all sorts of great heuristics on how to play that they lack. So without ragehacking they're actually at a disadvantage and either need to get increasingly bold with cheats or lose. In some games that's transformed into another way cheaters ruin modern competitive games: intentionally de-ranking so they'll play against less mechanically sound players who are easier to cheat against. reply WatchDog 15 hours agoprevWith the way that computer vision and AI continue to improve, I imagine that we will soon have completely external and undetectable cheating peripherals, simply capture the screen direct from the display output, and pass inputs via mimicking a usb human input device. This won’t provide all the same capabilities as cheats that hook into the game process, such as wall hacks, but it would be possible to build a super human aimbot with such an approach. We already have external “radar” cheats that use the game stereo sound to give the direction that a certain sound(such as footsteps) came from. reply musjleman 14 hours agoparent> With the way that computer vision and AI continue to improve, I imagine that we will soon have completely external and undetectable cheating peripherals, simply capture the screen direct from the display output, and pass inputs via mimicking a usb human input device. This already exists. You can stream your screen to another machine running image recognition and pass your mouse input through a controller that injects auxiliary input (there are off-the-shelf products like kmbox, you can make your own as well). However, it is very important to understand that only a tiny percentage of cheaters in games end up being determined enough to go through hoops to purchase hardware for it (it's much more expensive and not as simple as getting instant gratification by double clicking on an executable). It's basically considered a win to push people into needing go to such lengths to cheat without getting banned. reply Dibby053 8 hours agorootparentFor hacks that don't access program memory directly, would external hardware make things less detectable? I don't know how anti-cheat programs work but I'd be surprised if they banned every skilled player that happens to be running AHK and OBS. More likely they work with heuristics that try to detect super-human mouse movements, precision/speed-wise. reply maccard 7 hours agorootparent> For hacks that don't access program memory directly, would external hardware make things less detectable? Yes. But, it's a game of cat and mouse. Anticheat always is. There's not been a need go to the level of \"there are X players who report having a Razer deathadder, which we've validated as having a DPI range of Y-Z. With your OS settings, it's not possible to have achieved that with that mouse therefore we suspect you of cheating\". But we're probably there now. > More likely they work with heuristics that try to detect super-human mouse movements, precision/speed-wise. It's heurestics all the way down. You're unlikely to be banned for AHK+OBS, but a heurestic on what you're doing with the combo might ban you. reply MetaWhirledPeas 3 hours agorootparentprev> However, it is very important to understand that only a tiny percentage of cheaters in games end up being determined enough to go through hoops to purchase hardware for it Today. That will change when it's a cheap off-the-shelf product sold on Amazon. I almost wish we were there already to get rid of this KLA nonsense. Not to mention, every KLA game I've played has had cheaters regardless. Circumventing the software seems to become a game unto itself for those people. reply madeofpalk 8 hours agorootparentprevAnd all of that for something that is worse - software cheats usually get access to more information than just the pixels rendered to the screen. Seeing through solid walls and stuff like that. reply lovehashbrowns 6 hours agorootparentYou can do DMA cheating if that’s a non-negotiable for you but it is far more expensive. You install a direct memory access (DMA) card on your “clean” machine and flash it with firmware that hides the fact that it’s a DMA card since some rootkit level anticheats will look for that. The card then sends memory data to a second computer (dirty) which has the cheat software installed but no rootkit anticheat. The dirty computer can then read all the memory it wants, look through walls, shows enemy health, etc. but on a second computer and monitor. Pretty sure there’s hardware to blend both monitor outputs, though most people do two monitors I believe. reply ackbar03 7 hours agorootparentprevare there out-of-the-box software tools to do this? I thought about something like this before, but its non-trivial to train an image recognition system to do this, let alone the commands to mimic it through hardware, its quite an engineering feat in itself. reply declan_roberts 15 hours agoparentprevThat kind of stuff is readily available right now with openCV. Image detection has been very good and very fast for a long time! Nobody really cares because most players are honest and play to have fun. reply aenis 3 hours agorootparentBut why bother with image recognition? And pay a heavy latency penalty which makes it difficult to implement PID-style controls on player's aim? Its a game of milliseconds and opencv is comparably slow, as is the I/O involved. Far easier to do a MiTM on the network traffic and recreate state in another process. Still easier to clone memory pages used by the target app, or use hardware that allows dma. reply reginald78 6 hours agoparentprev> We already have external “radar” cheats that use the game stereo sound to give the direction that a certain sound(such as footsteps) came from. Did you just imply my headphones are a cheating device? More seriously, I've heard these type of devices already exist. reply wafflemaker 2 hours agorootparentI once played some Hunt (difficult, highly competitive game, a little like CS) with people on ex professional level. They obviously had gaming PCs, but they ALL had super expensive 7? Channels headphones. It's apparently a thing among ex pro (or high level) Counter-Strike players to have these super headphones that let you know where people are before regular people can hear them. They hushed me all the time because \"someone's coming\". Still a fun experience - to win for a change. Don't have to add that my role was more as a decoy then anything else. reply mrtksn 13 hours agoparentprevIn other words, tech based solutions to human problems always fails at some point probably because the problem creators can also use tech. reply jmorenoamor 9 hours agoparentprevThat would make an awesome competition field for AI bots that rely only on video/audio input and mouse/keyboard output. reply bunderbunder 2 hours agoprevSo, I'm not really into online multiplayer, but from a distance I keep wondering if this could be more easily mitigated by altering the incentive structure? What if online games track how well people do and sort them into tiers based on skill level? And then put people who are roughly evenly matched together. I am guessing that cheaters will naturally end up clumping together with each other, and maybe a smattering of elite players who are good enough that they can hold their own, and maybe even benefit from the added challenge. And also, casual and less-skilled players can play together and not get dominated so much. I don't think it would end cheating. But perhaps it would mitigate it by reducing a lot of the potential upside. Assuming the upside for many cheaters is that they enjoy feeling like they can dominate a server full of non-cheaters. reply wigl 2 hours agoparentAs someone who plays a lot of online multiplayer, there's a few issues. A lot of games do have skill based matchmaking systems (aka SBMM) in place. Ideally, this would lead to people having a 50% win rate which is just unacceptable to some. Those people resort to smurfing or cheating to get their fix--they don't really want a fair and equal match. Just look to the Call of Duty community crying about SBMM. Those people want to farm clips and play with those worse than them--not people at or above their skill level. Additionally, many multiplayer games at all but the highest skill levels have most matches determined by the side that plays the fewest number of mistakes. This is especially true of team based games. At most skill levels, both teams will be making many obvious (and less obvious) mistakes. This can frustrate some who believe that they're held back by their team and just need to play with higher ranked teammates. So people will boost or buy accounts or resort to cheating. reply JambalayaJimbo 2 hours agoparentprevSkilled players will NOT benefit from playing with others who are cheating, because their games will adapt to cheaters and not other skilled players. Cheaters will also easily adapt to ranking systems by creating new accounts or intentionally de-ranking themselves. reply bunderbunder 1 hour agorootparentAre the skilled players plugged into communities enough that they can just make their own servers? It would be some extra work, but I assume time spent getting to know each other wouldn't feel like a total waste, and might be greatly preferable to dealing with all these rootkits. reply steveklabnik 1 hour agoparentprevThis is called \"Skill based matchmaking\" and it is absolutely something that many games already do. Studies have shown that it increases player retention significantly. But also, there's a chunk of people who absolutely hate on it at every opportunity. Cheating happens regardless. reply raincole 2 hours agoparentprevI'm not entirely sure if you're being sarcastic or not. > What if online games track how well people do and sort them into tiers based on skill level? Games have been already doing this for at least a decade. Games that do this are exactly the games where cheating is rampant. reply bunderbunder 1 hour agorootparentHonest question. Believe it or not, people who aren't into AAA multiplayer games don't necessarily know a whole lot about them. reply stonogo 2 hours agoparentprevKLA isn't about preventing cheaters. It's about protecting the artificial scarcity of products for sale via microtransaction. reply bunderbunder 1 hour agorootparentGotcha. So you're saying that it's really about protecting a revenue model with pay-to-win elements? If that's so, then do we typically only see KLA on games that are mostly funded by microtransactions or live service models? reply nithssh 5 hours agoprevMakes sense considering Valve has maintained that kernel level AC is not required and has not included one in their own games, but let's be honest, unfortunately you have to often wonder if your enemy is having a good day or if he's hacking in CS but not so in valorant for a reason. reply ixwt 5 hours agoparentI'm also hearing a bunch of grumblings and speculation that Valve is developing a non kernel level anti cheat. reply nithssh 4 hours agorootparentThey have VAC and the newer version that they promised would save CS2 hasn't exactly changed much from CSGO days in terms of results. I would love for Valve to prove it is possible but so far they haven't shown it can be done without leaving a bad experience for legit players (due to delayed ban waves, etc) despite the repeated claims. reply WithinReason 9 hours agoprevHope there will be a tag. I wish they also had a tag for \"requires third party account\" so I could just filter those games out so I never see them. reply sphars 6 hours agoparentI use the Augmented Steam extension[0], which shows a more prominent banner if a game has DRM, anti-cheat, requires a third-party account, or other gotchas. Plus a bunch of other useful features [0]: https://github.com/IsThereAnyDeal/AugmentedSteam reply paledot 7 hours agoparentprevTags are crowdsourced. I was tagging games with \"always-online DRM\" before they declared it on the store page, but only the top few tags are displayed, so there's no point. reply SXX 4 hours agorootparentThey also editable by developers / publishers and it's by design. Any sane developer prune their tags from time to time since Steam recommendations depend on them. So it's not exactly a reasonable place to put something like that. reply nathants 18 hours agoprevkernel level anticheat is not enough. client inputs have to be trusted, and there is no provenance. the kernel has no visibility of inputs. i’m shipping a 100 player matchmaking game now. clients tick at 360hz, server ticks at 120hz. fair up to 60 ping, which covers entire continents. servers are metal, not vms. epyc 4244p with 2Gbps egress, 1 server per 15 minute game. mitigations=off and nosmt on all clients and the server. i love steam, but won’t be releasing this there. it’s reboot-to-play, a modified archlinux iso that boots directly into the game from a usb drive. i control not only the kernel, but the os, and every running program. you don’t get cortana. you don’t get discord. you don’t get spotify. you get the game. for the duration of play, your pc becomes an arcade machine. still, this is not enough. to play ranked, you’re going to have to get a handcam over your left shoulder. it will see head orientation, both hands, full mousepad, and screen. you’re also going to use fixed mouse speed, mousepad size, and monitor size. reviewing any players inputs will look familiar, since everyone is playing with identical settings and setup. kernel anticheat is not enough. we need a reproducible full os setup, down to running programs and network connections. even that is not enough. we need provenance of user inputs hooked right up to the game replay system, so you or anyone can review engagements from any parties perspective. obviously this should all be opt in. not everyone wants to play ranked, and whole-os anticheat should help even without input provenance. have you ever wondered if you died to a cheater or a god? do you wish you could never wonder again? i do. soon, i won’t. reply jokoon 2 hours agoparentValve now catches cheaters using machine learning, which analyzes the demo. Apparently they catch a lot of cheaters this way, and it's more reliable than player reports. But to be really honest: it's impossible to prevent cheaters. The only way is to play with friends, or to change the incentives of the game where it becomes less interesting to cheat. For example, a game like counter strike will have a lot of cheaters because you can earn skins and sell them. There are also a lot of players who will buy cheats, because the game is just so popular. The game design and engine design can also help to prevent cheating. Honestly, you cannot solve corruption, you need to change the game or the players. In my view, the Counter strike community is quite toxic, but very profitable for valve, which is why they don't care if their players behave poorly. CS is a nice game but human nature is what it is. You don't find recommendable people playing CS, most gamers play something else because that community is just so terrible to interact with. reply nathants 0 minutes agorootparenti’m trying to align monetization and all other incentives in the direction of a better game. yet cheaters will still cheat. non-human inputs IS cheating. period. there is no information cheating possible in this game. input provenance is the only anti cheat that has a chance. full fidelity 120hz server replays with handcam seems a good place to start. we’ll see how it goes! reply Sceptre6 13 hours agoparentprevWhat an incredible testament to the lengths men can be driven by spite. I would like to try your game, sir. But my problem is the people trying to take over my computer. I am not going to solve that by letting you take over my computer. Let's talk system requirements- could I get away with running it on an old junker laptop? reply nathants 13 hours agorootparentpossibly, depending on the gpu. wickedengine requires modernish gpu. spite was definitely a part of it. at a certain point while playing and watching fortnite solo build, i wondered why there are so many bugs and if i could fix them. i wanted to understand why ping advantage and storm surge have to exist. this game was my journey to find out. it has been a privilege and a joy every single day. as far as security, you’re asking the right questions. the typical gamer showing up in my discord doesn’t, so i guide them to them. if you boot my game, you’ve booted an archlinux iso. what could i do? i could read/wipe your disks, so you should make sure they are encrypted/backedup. i could probe your network. i could maybe even install bios level persistent malware. i could do anything a userspace app with admin can do. none of this is different from a windows app as soon as you click yes on the admin popup thingy which every multiplayer game needs. reboot-to-play is better, because assuming you use bitlocker, i can't read every file on you c drive, unlike every game you've installed from steam. i also can't mess up your windows registry, or any other os config. the reboot-to-play build process is not yet open, but soon will be. even then, the game binary it will download and run is not open. this and more is explored in the faq on the games site, let me know if you want more answers up there! the purpose of reboot-to-play is not to corrupt your disks. its purpose is to get all players into an identical state, for fairness, and to avoid finicky windows tweaking for performance. everyone’s pc is a special snowflake. what we want for multiplayer is identical arcade machines. every time you boot, you're in the correct state. running software is ultimately about trust. you can trust epic, or riot. you can trust steam or apple to vet user provide apps. you can trust me. do you want to trust me? that's up to you. i would say, wait for launch, watch some streams and videos, and see if it looks fun! launching soon. working through final matchmaking issues now. reply shkkmo 3 hours agorootparentWhile I appreciate the lack of self promotion, I don't see any name or link for your project in either comment. Can you share one? I'm sure I'm not the only curious person. reply vuldin 2 hours agorootparenthttps://better.game/ reply nathants 14 minutes agorootparentprevlink in bio. reply dlock17 14 hours agoparentprevAll of this, when you can just play on console. I know cheaters theoretically exist there, but in low enough numbers on my PS5 games that they don't impact my user experience. Kudos to your insane game plan. Gonna be hard to get any marketing from Twitch streamers though. reply nathants 14 hours agorootparentthanks! if the plan isn’t insane, why bother. ps5 can’t play 360hz and can’t use performance mode graphics. consoles are great, but esports will always be on pc. input cheats are common on pc and console, there isn’t a difference anymore. dual pc streaming will work fine. maybe we include obs in the iso at some point, but probably not. also, this game is mnk only. next game will be controller only. gotta keep inputs standardized or fair play isn’t possible. reply maccard 8 hours agorootparent> thanks! if the plan isn’t insane, why bother. This is the only reason you need to keep going. > ps5 can’t play 360hz and can’t use performance mode graphics. This [0] is your game. Without running it (because I'm not installing your OS on my machine, no offence) there's no reason that wouldn't run at 360Hz on a PS5. A PS5 is an 8 core machine with a dedicated GPU; it's going to be vastly more powerful (and has the advantage of being standardised hardware) than the random beater laptops your players are going to run. If you're talking about rendering at 360Hz - How many people realistically have that sort of monitor? And if they need to splash out £250 for it, we're getting close to the price of a console _anyway_ where you can play other games too. > consoles are great, but esports will always be on pc. Except for fighting games, sports games, and most importantly CoD. > input cheats are common on pc and console, there isn’t a difference anymore. Theoretically, yes. Practically speaking, input cheats are widespread on PC, and non-existant on console. (that's not to say XIM and co don't exist, but they're nowhere near the adoption level that's seen on PC). [0] https://better.game/#/ reply nathants 8 hours agorootparenti mean, i haven't built an os. it's just standard archlinux with minor conf. my binaries will run, but they would regardless of the platform. i can't tell users to go into bios and turn off smt. i can tell them to boot an iso, and have it preset to do that via kernel cmdline. owning the os means i can tune the network stack, the os, the kernel, anything that helps performance. the game itself has minimal config, only keybinds. i'm really targeting one player base: fortnite ranked/competitive players. fortnite competitive does exist on console, but barely. it's an after thought, and the game can barely run, even on ps5. most importantly, it doesn't let you set graphics to the settings every pro uses. on console you have default high graphics. all serious players migrate to pc. the game will run on beater laptops, but it's not a great experience. the players i'm targeting all have desktop pcs and 1080p240 monitors already. if i can get 1% of 1% of fortnite ranked players to try, that would be inglorious. if it's just me haning out with 99 aws servers, that's fine too. i want there to be an on ramp for new players, but optimizing for low end spec is not a goal. especially with 100 players all standing next to each other, low end hardware falls over. will probably have to limit to 50 or even fewer players depending on how much low end hardware shows up to play. input cheats are common on console for fortnite, i'm not sure about other games. reads video from hdmi, mitm controller outputs. multiple generations of that, some of it works on pc too, and then pc has a whole new host of similar tech. input provenance is the only real solution that i can see. the rest is shadows on a cave wall. reply maccard 7 hours agorootparent> can tell them to boot an iso, and have it preset to do that via kernel cmdline. owning the os means i can tune the network stack, the os, the kernel, anything that helps performance. the game itself has minimal config, only keybinds. I think this is an interesting idea. Lots of things we do are done the way they are because they've always been done like that. \"Turn off Antivirus\" has been common advice for PC players for as long as I can remember, this is a neat way to handle all of that stuff by default (nobody needs windows search indexing a scratch folder for a video game). > most importantly, it doesn't let you set graphics to the settings every pro uses. on console you have default high graphics. all serious players migrate to pc. One of the reasons people migrate to PC is _because_ of the customization that you're locking down. > reads video from hdmi, mitm controller outputs. multiple generations of that, some of it works on pc too, and then pc has a whole new host of similar tech. How does your system defend against a HDMI capture device and a USB device that pretends to be a keyboard? reply nathants 7 hours agorootparentwhere we are going, we don’t need graphics config. cyberpunk and cortana will always be there, only a reboot away. handcam anti-cheat and the replay system is how we deal with non-human inputs. ranked only. in pubs, you may encounter a cheater. hopefully owning the whole os makes that easier to detect, but i’m not sure it will. reply ryankrage77 17 hours agoparentprevI assume this is satire, because nobody would play a game with those requirements. reply bigstrat2003 15 hours agorootparentI wouldn't be so sure. Life has taught me that people will accept damn near anything in order to get the entertainment they want. If you worked your way up to the point OP describes slowly, over time, I wouldn't be at all surprised if people shrugged and said \"it's just what you have to do if you want to play those games\". reply nathants 13 hours agorootparentto be honest, all of this would be worth it just to never have to listen to fortnite streamers whine about cheating/bugs/ping/serverlag/stormsurge/etc ever again. i understand why they whine, but i just don't want to hear it anymore. i sympathize with their pain, and i have the solution. reply nathants 17 hours agorootparentprevi would never joke about cheaters, spammers, or other netizens of ill repute. if this is a challenge, consider it accepted. link in bio. reply jokoon 2 hours agorootparentprevcould be made necessary for pro players many of them were caught cheating reply red_admiral 9 hours agoparentprevDOS era \"insert game disk 1 and reboot to play\" vibes. reply nathants 8 hours agorootparentbuilding an archiso, flashing it, and then booting is very satisfying. [x670e][~/repos/WickedEngine/Protoverse/reboot-to-play][better-game][us-west-2][-][master] >> bash flash.sh /dev/sda 1224736768 bytes (1.2 GB, 1.1 GiB) copied, 52 s, 23.3 MB/s reply CursedUrn 5 hours agoparentprevMaking a webcam work on every hardware configuration is very difficult. I don't think this is possible. reply nathants 4 hours agorootparentreally? usb webcams? i suppose it’s possible, but this seems unlikely. very well, ranked will then require specific hardware. i’m gonna have to build more test pcs. 9800x3d 7700xt b650m looking pretty clean. reply bitsandboots 6 hours agoparentprevHey man I just play games to have fun. I guess fun isn't for everyone. reply nathants 4 hours agorootparentoutlandish. i will get laws passed that require you to play and enjoy my game. how dare you, sir. reply Retr0id 2 hours agoparentprevI'm booting your OS in a stealthy hypervisor and operating my cheat logic from there. reply rxyz 15 hours agoparentprevI opened the website in his profile and joined the discord. Yes there's a discord for this. reply nathants 13 hours agorootparentwelcome to the team. reboot-to-play is the future. reply Hikikomori 7 hours agoparentprevHow do you stop anyone from modifying the image? Can you stop them from using DMA hacks? Can I boot the image in a VM? reply nathants 7 hours agorootparentnot possible to stop modification, but i will be heavily surveilling the system, and i know exactly what it should look like. the process tree is known in advance. handcam anti-cheat and replays will handle provenance of inputs. non-human inputs are cheating. you could try to boot in vm, but performance will suffer. the server moved off vms for the same reason. the only thing i really care about is non-human inputs. if you get really creative with booting the game, and aren’t nefarious, i’ll set phasers to stun. reply shkkmo 3 hours agorootparentAnti-cheat is always an arms race. A modified image can report anything it wants to your surveillance. That said, you can win that arms race for a while by doing something sufficiently innovative, different and specialized and you may have that here. I doubt this approach offers a permanent solution though. Edit: I do think there is a lot to the idea of not trying to put all players through the same level of anti-cheat security. The anti-cheat needs of competitive players is very different from casuals and offering different levels of anti-cheat based on those levels. How amazing would it be if other games did the same and let players choose the level of anti-cheat to use and to require. reply nathants 12 minutes agorootparenteventually gen-ai may be an issue, but we’ll have to see. likely it will always be asymmetric, easier to detect than create. it will make cheating more expensive at a minimum, flawless live video at 1080p60 matched to game inputs will cost gpu time. then we turn up handcam fidelity. 120fps. 1440p. the sky is the limit. handcam-anticheat is ranked only. pubs has good old fashioned try-our-best anti-cheat. aka thoughts-and-prayers anti-cheat. reply valval 4 hours agoparentprevIs there a system in place for when my identical brother who is leagues better at your game than me hops on my account and proceeds to mop the floor with the competition? reply nathants 15 minutes agorootparentsbmm wouldn’t fix this, as skill is at the account level and changes slowly. what does fix this is handcam anti-cheat, which makes it trivial to identify the operator by their hand movement patterns, arm size, etc. you can review not only the match you are in, but all previous matches. full fidelity replays + handcam = a brave new world. account sharing is not allowed in ranked. in pubs, go crazy. reply kjkjadksj 2 hours agorootparentprevFor most games these days there is thanks to skill based matchmaking. Your brother will be matched up with people in your skill bracket and will absolutely wreck them. Then whenever you get back on you will have moved up in skill thanks to big bro and will now get wrecked until you get tossed back into the lobbies you belong. People generally hate this though because for one it makes most games into way too serious sweat fest with everyone so closely matched. And for two it doesn’t even stop the entire reason it was built which was to protect the noobs. The elite players can just tank like an nfl team for a few rounds and go back to stomping noobs. reply valval 20 minutes agorootparentThanks, although I was joking since I thought I was responding to a satirical post. Turns out OP is developing such game for real. reply tightbookkeeper 2 hours agoparentprevCheating is a human problem. The technology just makes us feel in control. I will not be playing games that make me feel like a prisoner. The experience taking standardized tests is enough for one life. reply nathants 4 minutes agorootparenti respect your feelings and your choice. happiness, privacy and security are important. there are however legions of fortnite players who would mortgage their lives for a better game. a handcam without audio during gameplay seems a more reasonable and low price to pay. time will tell what gamers think! very exciting. reply rldjbpin 7 hours agoparentprevvery interesting approach here, but i wonder if steam could provide a way to distribute this game in its current form. maybe a generic steamos image with console-level isolation, and make the game steamos-only perhaps? no offense, but there might be a sliver of the pie that don't like cheaters but also don't mind running something as invasive. reply nathants 7 hours agorootparenti might distribute a usb flashing downloading thingy in steam, but it’s not a priority. the goal is to be minimally invasive. you don’t have to install anything! i don’t fight your misconfigured pc, you don’t worry about me scanning your c drive. nothing to install, configure, save, or lose. reply bananamerica 9 hours agoparentprevThat was very enjoyable. reply nathants 9 hours agorootparentwelcome to the team. reply andrewmcwatters 15 hours agoparentprevhonestly, we have consoles for this dude reply nathants 15 hours agorootparentexactly! what we need is more power, and more mouse room. anyone playing fortnite ranked already has the needed hardware, and the motivation. if you haven’t won a solo build game on 1440p 360hz, you haven’t ever actually played a video game. reply digging 3 hours agorootparent> if you haven’t won a solo build game on 1440p 360hz, you haven’t ever actually played a video game. I think this is the most insane thing you've written here today, the one thing I truly disagree with. I'm not sure you even agree with it, given you seem to already understand well the difference between casual, competitive, and hardcore competitive play. But are you aware of solo play? reply nathants 7 minutes agorootparentthe distinction i’m trying to draw, is that while the term gaming is beloved, it is a very large umbrella. i don’t have a new term to offer, but i stand by what i said. 1440p360 solo build win IS a different type of thing than launching a rocket in factorio. we are going to need new terms. it’s not about casual vs competitive. it’s about how the game feels to play, and how the adrenaline hits your blood stream. reply Koofte 8 hours agoparentprevnext [5 more] [flagged] nathants 8 hours agorootparenthandcam anti-cheat is the future. reply a2128 8 hours agorootparentHow does that scale? Handcam anticheat works well for exams and Olympiads where there's limited people and plenty of time to review footage after it's over, but I can't imagine it would work well on Fortnite or Counter Strike unless you staff entire offices reviewing footage full-time. Though I'm also doubtful there'd be many people willing to run an untrusted OS on their computer just for a random game so maybe you wouldn't have that much footage to begin with reply nathants 8 hours agorootparentthe plan is to let users report bad engagements. when you lose a fight, you drop into replays without leaving the game. scrub back like in a video editor, and watch the engagement from the other players perspective. the handcam footage is available here. cheating should be obvious. report or no, then return to the game, where you can spectate the rest of the match like a ghost. fortnite replays are commonly used by pros, but less so by more casual players. their main issue is that you have to leave the game to get to replays. they also only kind of work, and don't show full server fidelity. our server ticks at 120hz, and replays are full fidelity. lots to figure out still, but that's the idea. ranked won't launch for a while, but i wanted to design around anti-cheat from day 1. failure is a distinct possibility with this game, that's ok. success would be interesting too. i'm not quitting the day job to build this game. if it never makes a dime that's ok. i started this to study fortnite and understand it's tradeoffs. turns out, it's mostly just tech debt and accidents of history. i wanted to see if i could do better, and i could. reply Koofte 8 hours agorootparentprevnext [2 more] [flagged] nathants 8 hours agorootparentthen they are not ranked ready, and can play pubs. reply steelframe 17 hours agoprevI built a separate Arch Linux box just for Steam gaming. I will never log into any of my sensitive accounts -- email, banking, etc. -- on that machine. It's a Framework laptop so I can physically keep the camera and microphone disconnected. I basically treat it like a public terminal. reply Aeolun 12 hours agoparentDo you truly expect any steam games to have anything like a root kit that’d exfiltrate your credentials? I feel if this were the case literally anything I install on my PC would be suspect. Installing ssh would be a much more scary thing than a random steam game. reply gspetr 9 hours agorootparent>Do you truly expect any steam games to have anything like a root kit that’d exfiltrate your credentials? https://www.bleepingcomputer.com/news/security/steam-game-mo... \"Downfall, a fan expansion for the popular Slay the Spire indie strategy game, was breached on Christmas Day to push Epsilon information stealer malware using the Steam update system. Once installed on a compromised computer, the malware will collect cookies and saved passwords and credit cards from web browsers (Google Chrome, Yandex, Microsoft Edge, Mozilla Firefox, Brave, Vivaldi), as well as Steam and Discord info. It will also look for documents containing 'password' in the filenames and for more credentials, including the local Windows login and Telegram.\" reply maccard 8 hours agorootparentThat's not a steam game, that's a user mod (read: random binary downloaded from the internet and executed). Also, it doesn't need kernel level access to do any of that stuff, it can get by just fine with normal application level permissions. This is no different to downloading a random binary off the internet and being surprised it's malicious. reply remnantdiving 6 hours agorootparentpatched: https://hackerone.com/reports/470520 https://hackerone.com/reports/1070835 https://secret.club/2021/04/20/source-engine-rce-invite.html https://secret.club/2020/10/30/alien-swarm-rce.html https://threatpost.com/dark-souls-servers-down-rce-bug/17789... https://blog.thalium.re/posts/achieving-remote-code-executio... https://www.pcgamer.com/garrys-mod-cough-virus-is-cured-but-... you can buy these games in their unpatched state today, and download a poc for them from github too: https://nvd.nist.gov/vuln/detail/CVE-2018-10718 https://nvd.nist.gov/vuln/detail/CVE-2018-20817 frankly playing video games on a dedicated device and network is the reasonable response to reading this shit reply keyringlight 5 hours agorootparentI've been wondering is how long before we get a 'PC-console' dedicated gaming device, there's certain aspects where PC is falling into the gravity well of the console model. Whether that's the technical aspects like how far you can lock down for security (and who's security) or DRM, whether people only want a nice front end, whether everything should be managed by a defacto 'good' platform holder versus allowing companies to do their own thing (clients or commerce). At some point such a thing looks like a more expensive ultra-pro console. Then that has to be balanced against the freedom aspect where people want flexibility to build a workstation how they want and do what they want on it, and expecting it all to work (windows games being supported on non-windows is a more common issue now). PC casts the broadest net and catches a lot of different desires, similar with how high-end and low-end seem to be splitting over the past ~5 years rather than being a continuous spectrum I wonder if there will be distinct types of PC for gaming (eg set models like the old Commodore Amiga) or if trying to resist splits does more harm than good. reply fluoridation 4 hours agorootparentSpeaking for myself, I would never buy such a thing. The reason I game on PC is because I need a decent PC for work anyway, so building one that can also game is cheaper than buying a console. If I was really concerned about security I'd sooner dual boot into a second OS that had nothing on it, than buy a second box just to game on. reply keyringlight 4 hours agorootparentYep, a lot of this comes down to \"who's security\" and what everyone can take as a foundation to build trust upon, or what their thresholds are reply shkkmo 4 hours agorootparentprevThat's what \"steam deck\" is. A dedicated gaming device that is still also a general purpose computer that you have full control over. reply westpfelia 10 hours agorootparentprevThe concern is that malicious actors can take advantage of what is certainly a poorly written rootkit. reply LinXitoW 2 hours agorootparentprevI don't expect that. I expect that publicly traded companies will cut corners in developing their kernel extension (like always), turning them into literal root kits waiting for anyone willing to exploit them. See: https://www.trendmicro.com/en_us/research/22/h/ransomware-ac... reply steelframe 3 hours agorootparentprev> root kit that’d exfiltrate your credentials Yes. I truly believe some janky random anti-cheat kernel module could very well capture telemetry about my keystrokes to a log and then send that log off to a server. At the very least I don't trust that it's secure enough to be in the kernel of a machine for which I require any degree of trust in its integrity. reply m463 10 hours agorootparentprevI thought some games snooped through your system. Kerbal Space Program comes to mind, I recall it had adware that did this. reply Delk 7 hours agorootparentUnity has some kind of data collection that can be used for analytics and advertising, so you might need to opt out of that in a Unity game. I think that came up in KSP as well. https://unity.com/legal/game-player-and-app-user-privacy-pol... https://unity.com/legal/game-player-and-app-user-privacy-faq reply wodenokoto 12 hours agorootparentprevNo, he expects the root kit will open up his machine to automated worms. reply rldjbpin 7 hours agoparentprevat that rate why bother using arch for the box? unless you never touch online multiplayer games, i can understand that it probably works for you. reply nathants 14 hours agoparentprevdedicated hardware is a good idea, but too expensive for many. dedicated os is a good first step. reply cascades42 15 hours agoparentprevYep, same here. I have a dedicated gaming machine because I’m afraid to expose my banking information. reply j-bos 16 hours agoparentprevThis is the way. reply Kapura 2 hours agoprevI know there are large multiplayer shooters that have or are going to be moving off of Linux soon. Anecdotally, the percentage of linux users running cheat software is significant, north of 50% in some cases. Ultimately, I sympathize with game developers trying to create a good, _consistent_ experience for players across multiplayer titles. The reason players accept anticheat software in large mp games is because the alternative is worse. reply LelouBil 2 hours agoparentI think I saw on HN a while ago someone suggesting that Valve could make a \"clean kernel\" build that anti cheats could whitelist for Linux/Proton games. reply athrowaway3z 1 hour agorootparentThe plan to control what a persistent cheater does to their own device is more than 5 decades of straight failures. Just one more client anti-cheat measure bro. Just one ring lower bro. I'm not sure what I find more terrifying: the persistence of the NSA & Disney lobby for subverting root control in all devices, or that so many people mindlessly agree its the right direction to go and their world will be a better place for it. reply wiz21c 11 hours agoprev> kernel-level anti-cheat Add UEFI on your PC and DRM in your browser. And next, your governement will ask you to add its anti pedo-pornography tools. And then we have a new episode of Black Mirror... reply M95D 10 hours agoparentUEFI is here, browser DRM for video is here, browser DRM for text+ads on ordinary web pages is just around the corner, government won't ask anyone - the tools will be added at the ISP level, if they're not already installed and operational. reply dartharva 7 hours agorootparent> government won't ask anyone Wishful thinking. That government hasn't asked anyone yet (at least in public information) is a miracle in itself. reply M95D 7 hours agorootparentYou didn't understand my meaning: Government never asks. It just does things whether you agree or not. reply proaralyst 8 hours agoparentprevWhat's your problem with UEFI? If your OEM wanted to install malicious firmware they can do that in BIOS no? reply jolmg 22 hours agoprevI've never seen a game request root privileges, and I would think installation of anything kernel-level would need that. None of the steam binaries have setuid nor capabilities set. Have anyone seen games that request root privileges? EDIT: I'm gathering from this[1] and the fact that no wine-related package have kernel modules included and no executable from any of those packages have setuid nor capabilities set, that this isn't really a problem in Linux, just in Windows. [1] https://www.reddit.com/r/linux_gaming/comments/gjzkzk/will_w... reply zamadatix 21 hours agoparentThe kernel level anticheats are almost always written for Windows. They are relevant to gaming on Linux because those games won't work on Linux even if wine/proton run the user space portions fine reply a2128 13 hours agoparentprevFrom my understanding, if you play an EAC game on Linux with Proton, you're not really running the same EAC as Windows players. You're running a lite version that runs as a regular user and it tries to provide at least some level of protection like verifying game files or detecting anything clearly out of place that it can detect, but obviously it doesn't have the permissions to see everything running on your system or install a kernel module. This does mean cheaters could probably just cheat on Linux to bypass it more easily, so anticheats like EAC will put Linux support as an opt-in toggle which some developers choose not to enable. reply sadeshmukh 21 hours agoparentprevEverything says \"wants to make changes to your device\". I accidentally installed EAC that way. reply keyringlight 21 hours agorootparentIt's worth noting that when you first install it, steam asks to install a service to assist with its duties, presumably for most install tasks. Steam has been around long enough and that service is now trouble free that it became part of the furniture most ignore as part of the background. That's aside from how users may be trained to hit 'yes' on any permission box that comes up to swat it away and play the game. reply jsheard 20 hours agorootparent> It's worth noting that when you first install it, steam asks to install a service to assist with its duties, presumably for most install tasks. They do this because Steam was originally designed in the XP era when you could write whatever you want to Program Files without escalating to admin, and instead of refactoring where they put their files when Vista made the permissions more strict they started installing that backdoor service which lets them keep putting everything in Program Files without triggering UAC prompts all the time. It's a pretty gross and unnecessary hack, but I doubt they're ever going to fix it at this point. reply sunshowers 18 hours agorootparentI don't think this is why -- Steam actually sets permissions on its subdirectory so that any user can write to it. (This means that while installing mods, for example, I can write to that directory without having to deal with UAC/sudo.) reply keyringlight 9 hours agorootparentprevAlthough I'm not fully linux knowledgeable, I think they put everything under the user profile in ~/.local/share/Steam for similar reasons so they can do software installs with no elevations. They're not the only ones taking that approach though, it's become common across OSes to offer an easy/quick installer that dumps itself in your user profile because that's seems to matter most to getting users up and running. reply bjackman 20 hours agoparentprevNot on Linux. Things are different on Windows, especially if you wanna play competitive games, I'm told. reply fngjdflmdflg 20 hours agoprevI hate to say this but a large percentage (in fact, I believe a majority) of gamers simply do not care about invasive anti-cheats. Right now CounterStrike players are mostly begging Valve for kernel-level anti-cheat since their current solution isn't working at all. If anything, this warning will actually make many player's more impressed with the game. That said, more consumer information is almost always better in any case, especially in this case considering that this is not a requirement of law but of a private company. reply skizm 17 hours agoparentAs a counter strike player, I definitely shy away from the invasive anti cheat stuff… but I’d let valve inject it into my veins if it meant I could actually play and not suspect everyone of cheating all the time. Mostly because Valve has earned my trust. I won’t install games from other companies using similarly invasive techniques though. reply adastra22 13 hours agorootparentValve wouldn't purposefully backdoor you for nefarious purposes. But any such code is not nearly reviewed enough to be sure it is free of unintentional backdoors that could be exploited by third parties. reply dietr1ch 17 hours agorootparentprevWhile I trust valve, I'm not willing to mess up with my workstation to play. Also, there's hardware cheats, so I don't need a rootkit on my machine, but a server side thing that properly weeds bad players out through reports/trust and automated bans. reply Sohcahtoa82 13 hours agorootparent> a server side thing that properly weeds bad players out through reports/trust and automated bans. No. No no no. Automated bans via the report system is very well-known to be abused. Even if you implement a \"trust\" system where initially, all your reports are manually verified by game staff until its determined your reports are correct until your reports are acted on automatically, all it takes is a player to just be \"good\" until their trust is high enough, then start reporting people who don't actually deserve it. And I'm not convinced that server-side anti-cheat can be effective. You have to rely entirely on heuristics. Sure, a simple aim-bot that instantly snaps someone's aim right on someone's head might be detectable, but one that simply lets you see through walls certainly won't be if the player doesn't make it stupidly obvious by pre-aiming around every corner. reply janderland 17 hours agorootparentprevReports only work so well. Overwatch has MANY cheater in spite of vigorous reporting. reply freeAgent 15 hours agorootparentprevYeah, I generally trust Valve but gaming is definitely not important enough to me to give them kernel access to my system. I’m sure many gamers disagree with me though. reply LinXitoW 2 hours agorootparentprevBut you couldn't. After all, there's a lot of hardware based cheats that even KLA can't reliably detect. If you're \"not sure if someones cheating or just good\", maybe that's a mental problem with you? Put differently, if all cheaters were perfectly hidden (aka looked exactly like a real player of that skill level), would you still care? If yes, you seem more interested in a morality than actually enjoying the game. reply anonymousab 6 hours agorootparentprevI take it community moderation tools like voteban/votekick aren't sufficient anymore? They worked pretty well for pub matches back in CSS and 1.6, where it was pretty trivial for anyone to cheat or bot for free with minimal effort. I wonder what changed. reply jonathantf2 4 hours agorootparentIn a normal 5v5 match, you need everybody else on the team to vote yes to kick the cheater. If they're queued with someone else (which is very likely) then you've got no chance reply veunes 11 hours agorootparentprevTrust in a company plays a huge role here reply ehnto 14 hours agoparentprevYep. I would call myself a privacy focused person, but given that Windows is the primary platform for PC gaming, and I trust Microsoft about as far as I can throw a their corporate headquarters, the platform is already compromised. Treat it accordingly, play your games. Maybe watch your adult films and write your memoirs on a different system than your gaming rig. reply rldjbpin 7 hours agoparentprevvalve has their own ethos in this topic, which i wholeheartedly accept. you can guess which end of the spectrum they lie from the original news article. faceit for the longest time has had their own way around this. so did esea, before they ruined the trust forever (https://news.ycombinator.com/item?id=5636233). some highly-motivated players still found ways around it (https://news.ycombinator.com/item?id=39352331). reply veunes 11 hours agoparentprevIdeally, players would be given both a choice and a clear breakdown of what’s actually being collected or monitored. reply andrewmcwatters 19 hours agoparentprevYou don't need kernel-level cheats to bypass VAC, nor kernel level anti-cheats to catch cheaters. reply fngjdflmdflg 19 hours agorootparent>nor kernel level anti-cheats to catch cheaters. Do you have some examples of good anti-cheats that are not kernel-level? Do you have any that are as good as Riot's Vanguard? I'd prefer examples of FPS games since these are the most mechanically skill based compared to other genres that have more strategy, but would like to hear any examples you are thinking of. Lastly, if you say server-side, that may work, but many companies don't seem interested in it due to the cost, at least IIUC. reply flessner 18 hours agorootparentAs someone that plays CS2 and Valorant regularly... Vanguard hasn't been effective for a while now. The cheating situation is a lot worse than CS in my experience, but every discussion gets shutdown because... well... it's Vanguard. With CS2 I have talked to many players about this and everyone says the same thing: \"There's a very noticeable decline in cheaters above 10k Elo.\"... personally I have pushed beyond 15k and briefly above 20k Elo and the amount of cheaters have steadily declined (although less obvious cheats, eg. wallhack, are probably more common at that level) - for Valorant it has pretty much stayed at a constant amount of \"cheatiness\" across the ranks. CS actually has a rich history of features, functions, services?... that aren't strictly anti-cheat... Overwatch gave players the option to \"police\" others players replays - this wasn't only against cheating, but also griefing. Prime? Is it still even a thing? It was great when CSGO went F2P... all the cheaters just annoyed the non-prime players (F2P). The ominous Trust factor which is probably the single most effective piece in making my personal experience great. But there's no real way to tell? Also, VacNet - which is running? is AI based? banning players? lowering their trust factor?... with Valve there's no real way to tell most of the time, but it's probably existent in some shape, way or form. Not to say that CS2 has solved cheating, it's far from it - but neither has Valorant. reply fngjdflmdflg 17 hours agorootparentI have a very hard time believing that the rate of cheaters go down in high elo. IIRC the new CS2 leaderboard still regularly features cheat companies on it (eg. config by [cheat dev] as the leaderboard name.) I myself do not have any data to back up that claim, but yours completely goes against what I have experienced. I think the point about wallhack being more common in higher elo is more likely. I would add that some forms of trigger botting and recoil control cheats are actually more difficult to tell than wallhacking. Spinbotters don't get very high elo because they get mass reported because of how blatant they are, likely not due to VAC. I would need some real evidence to believe that claim (although as I said I similarly have no evidence myself to convince you to accept my claim). One thing I can say is that I do frequently meet cheaters in CS these days, and the issue has gotten so bad in my experience that many cheaters even announce at the start of the game that they eg. have wallhack. Or one team member will turn on cheats if a game is getting close towards the end of the game. Also, the main reason FACEIT exists is for its anti-cheat, and on FACEIT there are almost no reports of cheating, and it's a big deal when it happens. If VAC was really working now we would see more people leaving FACEIT. I must ask when you started playing CS? Because the only way your post makes sense to me is if you started playing around the time when CS2 came out, which indeed did have more cheaters then it does now, but that was truly an exceptional level of cheating and I don't think that is a fair point of comparison, especially as a comparison to Vanguard. I admit to taking claims about Vanguard at face value and I've never played Valorant (in part due to Vanguard, as I don't want to install a rootkit). But what you say about Vanguard also completely goes against what I have heard about it. reply flessner 4 hours agorootparentI absolutely support your claims about the leaderboards, it's an obvious show of cheating in CS2. There's also a strong incentive for cheating companies to be there so it might not be descriptive of the average experience. However, I can't speak for that level as my peak was barely over 25k (top 1%?) and the leaderboards are simply orders of magnitude away from that. Regarding cheaters announcing they're cheating - I haven't encountered that in a long time, but I have heard of it often enough from new players... so it might be an issue with trust factor, but who knows? I have actually been playing CS off and on since around 2017 - at least in my experience the current cheating situation isn't worse than it was back then, but it's also not better. The only time it was meaningfully better was when prime released around 2021. However, it's also true that I started playing more after the release of CS2... and the aforementioned 10k Elo mark was a real pain point for me and my friends. Every time we were due to pass it we ran into cheaters, smurfs and even a server crash once (incredible luck?). After over 3 months we made it past 10k and climbed above 15k Elo within 2 weeks. - This is my experience and I have heard similar stories from other players. (Although ranks have been massively imbalanced at that time as well, which partly explains this?) Nevertheless, it's good to have a discussion about cheating - in CS2's case the experience can be so different depending on the region, ELO, trust factor, ... with Valorant the discussion simply gets shutdown way to often because of \"Vanguard\" and without a replay system you're just left to your own devices. reply Vilian 6 hours agorootparentprevYou don't have replay in valorant, you can't be sure if the other player was cheating or not, in CS you can reply xinayder 10 hours agorootparentprevThey should implement honeypots like they did with Dota 2: https://www.dota2.com/newsentry/3677788723152833273 but yeah I can agree, my friends say CS2 is full of cheaters,",
    "originSummary": [
      "Valve mandates developers to disclose kernel-level anti-cheat mechanisms on Steam store pages, increasing transparency for players.- This requirement is crucial for users, particularly those on Steam Deck/Linux, as kernel-level anti-cheat can prevent games from running with Proton.- Recent Steam updates also tackle issues such as publisher banner spam and enhance Linux game support."
    ],
    "commentSummary": [
      "Steam now requires games to disclose the use of kernel-level anti-cheat on their store pages, addressing privacy and system stability concerns.",
      "Kernel-level anti-cheat can interfere with unrelated software and pose security risks, sparking debate among players about system access and trust.",
      "The gaming community is divided on balancing effective anti-cheat measures with maintaining user trust and system integrity."
    ],
    "points": 877,
    "commentCount": 608,
    "retryCount": 0,
    "time": 1730317199
  },
  {
    "id": 42000784,
    "title": "OpenZFS deduplication is good now and you shouldn't use it",
    "originLink": "https://despairlabs.com/blog/posts/2024-10-27-openzfs-dedup-is-good-dont-use-it/",
    "originBody": "27 October, 2024 OpenZFS deduplication is good now and you shouldn't use it OpenZFS 2.3.0 will be released any day now, and it includes the new “Fast Dedup” feature. My team at Klara spent many months in 2023 and 2024 working on it, and we reckon it’s pretty good, a huge step up from the old dedup as well as being a solid base for further improvements. I’ve been watching various forums and mailing lists since it was announced, and the thing I kept seeing was people saying something like “it has the same problems as the old dedup; needs too much memory, nukes your performance”. While that was true (ish), and is now significantly less true, the real problem is that this just repeating the same old non-information that they probably heard from someone else repeating it. I don’t blame anyone really; it is true that dedup has been extremely challenging to get the best out of, it’s very difficult to find good information about using it well, and “don’t use it” was and remains almost certainly the right answer. But, with this being the first time in almost two decades that dedup has been worth even considering, I want to get some fresh information out there about what dedup is, how it worked traditionally and why it was usually bad, what we changed with fast dedup, and why it’s still probably not the thing you want. Table of contents What even is dedup? How does dedup work? Why is traditional dedup so bad? The dedup table The live entry list Unique entries How does fast dedup fix all this? Making the live entry list smaller The dedup log Incremental log flushing Unique entries That’s a lot! Anything else? Nice! Can’t wait to use this with my existing dedup table Is deduplication really good now? I don’t get it. After all this, if it’s good enough, why shouldn’t I enable dedup everywhere? In summary What even is dedup? 🔗 Dedup can be easily described in a sentence. When OpenZFS prepares to write some data to disk, if that data is already on disk, don’t do the write but instead, add a reference to the existing copy. The challenge is all in how you determine whether or not the data is already on disk, and knowing where on disk it is. The reason it’s challenging is that that information has to be stored and retrieved, which is additional IO that we didn’t have to do before, and that IO can add surprising amounts of overhead! This stored information is the “dedup table”. Conceptually, it’s hashtable, with the data checksum as the “key” and the on-disk location and refcount as the “value”. It’s stored in the pool as part of the pool metadata, that is, it’s considered “structural” pool data, not user data. How does dedup work? 🔗 When dedup is enabled, the “write” IO path is modified. As normal, a data block is prepared by the DMU and handed to the SPA to be written to disk. Encryption and compression are performed as normal and then the checksum is calculated. Without dedup, the metaslab allocator is called to request space on the pool to store the block, and the locations (DVAs) are returned and copied into the block pointer. When dedup is enabled, OpenZFS instead looks up the checksum in the dedup table. If it doesn’t find it, it calls out to the metaslab allocator as normal, gets fresh DVAs, fills the block and lets the IO through to be written to disks as normal, and then creates a new dedup table entry with the checksum, DVAs and the refcount set to 1. On the other hand, if it does find it, it copies the DVAs from the the value into the block pointer and returns the writing IO as “completed” and then increments the refcount. Blocks allocated with dedup enabled have a special D flag set on the block pointer. This is to assist when it comes time to free the block. The “free” IO path is similarly modified to check for the D flag. If it exists, the same dedup table lookup happens, and the refcount is decremented. If the refcount is non-zero, the IO is returned as “completed”, but if it reaches zero, then the last “copy” of the block is being freed, so the dedup table entry is deleted and the metaslab allocator is called to deallocate the space. So all this is working, in that OpenZFS is avoiding writing multiple copies of the same data. The downside is that every single write and free operation requires a lookup and a then a write to the dedup table, regardless of whether or not the write or free proper was actually done by the pool. It should be clear then that any dedup system worth using needs to save more in “true” space and IO than it spends on the overhead of managing the table. And this is the fundamental issue with traditional dedup: these overheads are so outrageous that you are unlikely to ever get them back except on rare and specific workloads. Why is traditional dedup so bad? 🔗 All of the detail of dedup is in how the table is stored, and how it interacts with the IO pipeline. There’s three main categories of problem with the traditional setup: the construction and storage of the dedup table itself the overheads required to accumulate and stage changes to the dedup table the problem of “unique” entries in the table The dedup table 🔗 Traditional dedup implemented the dedup table in probably the simplest way that might work: it just hooked up the standard OpenZFS on-disk hashtable object and called it a day. This object type is a “ZAP”, and it’s used throughout OpenZFS for file directories, property lists and internal housekeeping. It’s an entirely reasonable choice. It’s also really not well suited to an application like dedup. A ZAP is a fairly complicated structure, and I’m not going to get into it here. For our purposes, it’s enough to know that each data block in a ZAP object is an array of fixed-size “chunks”, with a single key/value consuming as many chunks as are needed to hold the key, the data, and and a header describing how the chunks are being used. A dedup entry has a 40-byte key. The value part can be up to 256 bytes, however this is compressed before storing it, so let’s assume a common case of 64 bytes to actually stored. Each chunk inside the ZAP is 24 bytes, and can contain either the header, or up to 21 bytes of key or value data. All together, we’re looking at ceil(40/21) + ceil(64/21) + 1 == 7 chunks per entry. A typical dedup ZAP block is 32K, which has space for 1320 chunks (ZAP blocks themselves have their own header describing the chunks). So a single dedup block has space for 1320/7 = 188 “typical” entries. We could certainly create a better format tailored to storing dedup entries, but the format is not the immediate issue here. The real problem is one present throughout OpenZFS wherever a data block is carrying an array of unrelated items: amplification. OpenZFS never writes partial blocks, and it never overwrites a block in place. So if we want to update a single dedup entry, we need to load the entire block from disk, modify just the bit we want, and write it back out, in full, as a brand-new block. And then the new block pointer needs to be written to an indirect block, and its new block pointer to another indirect, or the dnode, and so on up and up to the top of the tree. This is of course no different to any other OpenZFS write, and further up the tree we go the more that overhead is amortised across writes from the entire pool. But within the dedup ZAP, it’s a read-modify-write cycle for every single block written, because at minimum we have to bump a refcount. That’s just a single entry update. If two writes were done within the same transaction, then that’s almost certain to be two different ZAP blocks we need to do that read-copy-write change on. Dedup mandates a cryptographically-strong and collision-resistant checksum to use as the key, which means the chance of any two arbitrary checksums falling close enough to land in the same ZAP block is small. This is where the old recommendations suggesting that dedup requires enormous amounts of RAM ultimately come from. Reading a dedup table is like reading any other data in OpenZFS: it gets cached in the ARC. If you have enough RAM such that the ARC never needs to evict any part of the dedup table, then you can largely cut out the entire read part of the table update. This is also where the rarely-seen dedup vdev class can help. If you add a sufficiently large and fast dedup vdev to your pool, then you may be able to reduce your memory requirements a little. This still ends up being a challenging build, because at scales that make dedup worthwhile, you really need to build that vdev out of something large enough to hold the entire table, and fast enough the overhead of not being memory is still workable. Multi-terabyte NVMe devices are great if you can afford them, but it’s not for the faint of heart or light of wallet. The live entry list 🔗 There is another significant memory use in traditional dedup, that isn’t as well known and has no good way to balance it against other factors. Every write in OpenZFS is assigned to a “transaction”, identified by a numerical “transaction id”. Data is written out to disk as it becomes ready, then every so often that transaction is closed and all the metadata for the transaction, which includes dedup table updates, all those block pointer updates mentioned above, and various other bits of housekeeping are all written down. By default this happens no more than 5 seconds since the last time, but in practice it’s when there’s a gap in userspace activity. Imagine you wrote, say, five instances of the same data at the same moment, to a dataset with dedup enabled. Imagine also that this is brand-new data, not currently in the dedup table. You would want this to only be written once, and a dedup table entry for this block to have a refcount of 5. Because these are data writes, they begin “immediately”. But if you recall the IO path above, each one needs to look up the dedup table first to decide if it should really be written, or just bump the refcount. The dedup table lookup function will be called five times, all would end up trying to read the relevant part of the dedup ZAP (though the ARC would reduce it to one physical read), and all would discover that the entry doesn’t exist, and so let the writes through. Finally, all would try to create a new entry with refcount 1. We end up with some distinctly un-duplicated data, and a dedup table that has no reflection on reality. So instead, OpenZFS keeps an in-memory list of “live” entries. These are held entirely in memory, and keep track of entries created or modified on this transaction. The dedup table lookup function starts by checking this list for the requested entry. If it’s there, the live entry refcount is bumped and it is returned. If it’s not there, then it will create a new live entry, flag it as “in progress”, then it will call down to the ZAP layer to get the dedup entry proper. When that entry comes back, it unpacks it into the live entry, flags it as “ready” and then returns it. Meanwhile, the other write threads will arrive, look up the live entry, see that it’s “in progress”, set up to be woken when it’s ready, and go to sleep. When they get woken, they will see that the flag has changed to “ready”, bump the refcount and return it. Then, at the end of transaction, the live entry list is walked, and the relevant details are copied into the dedup ZAP. Because there’s one and only one live entry for every checksum, each dedup ZAP entry only gets one update. We’re also applying all of the changes all at once, which is our best chance to be updating multiple entries within the same block. Overall, this is a reasonable model, and matches the rough model everywhere else in OpenZFS: do all the data work during the transaction, and when the transaction closes, resolve all the associated metadata changes and write them all out. If you’ve ever heard an OpenZFS developer talk about “open” and “syncing” contexts, this is what they’re talking about. The problem? These live entries are enormous: 424 bytes each. The raw entry that we load from and save to the ZAP is 296 bytes (bigger than the ~104 because this version is uncompressed), which is a problem on its own. However there is also 128 bytes of housekeeping stuff (like the “in progress” and “ready” flags and associated locks). It doesn’t take long for this list to grow very large, and although it’s cleared out at the end of every transaction, the peaks can be pretty high. And this is kernel slab memory, not ARC memory. It can’t be reclaimed when the system is under pressure. There’s nothing you can tune or configure to bring this down. It is a little more situational, as it will only grow in proportion to the amount of different things you’re writing each transaction. That’s little comfort though, as you’re only using dedup if you have a lot to write! So in the end, you still need a ton of memory if you want to actually make use of your dedup table. Unique entries 🔗 The biggest drag on all the dedup table is the space required to track unique entries. For dedup to work, we have to track everything we have stored on the disk, but, we only get any benefit when the refcount goes greater than 1. Any block that we have only one copy of is just consuming space in the dedup table, waiting for the day that something writes exactly the same data. If that never happens, then it’s a cost that we can never claw back. And, since dedup is performed on the data after encryption and compression, and on the block level, then it’s not just the same data, but the same compression method, encryption keys, and alignment within the file. And this is why dedup is worse than useless on general purpose workloads, because there is just so little data that is truly “the same”. How does fast dedup fix all this? 🔗 What we call “fast dedup” is a suite of changes that together try to tackle the above problems. Put simply, the goal is to reduce the amount we store in the table, and when we must store something, be much smarter about how we accumulate and stage those changes, and then provide tools to allow the operator to limit and manage the table conetns. Making the live entry list smaller 🔗 The first place we started was to reduce the memory footprint of the live entry list. The dedup table is a regular stored object, accessed through the ARC, so for that we have many more optimisation options available to us. The live entry list however is exactly that: a simple list, recording every entry touched on this transaction, pinned in memory. We can’t get rid of it within the current architecture, so we just had to make it smaller. The live entry type ddt_entry_t was not very well laid out. First, it used a few large numeric types to store simple flags; those were replaced with a bitfield. Next, some of the entry synchronisation fields (recall the “write five” example above) were simplified. Finally, it carried 40 bytes of state that is only needed when a dedup’d data block was first being written, or is in need of a repair write (the OpenZFS “self-healing” stuff). Once an entry is created, this state is never used, so it was lifted out to a separate “IO state” object that we create when we need it, and toss when we’re done. This was all fairly small stuff though. The big fish is in the stored part of the entry. The key is 40 bytes, 32 of which are the checksum. It’s effectively a block pointer fragment, so can’t really be modified further without implications for the block pointer structure proper, which is well outside the scope of this work. More importantly though, we actually wanted to keep the key the same for compatibility with existing dedup tables. Not strictly necessary, but there’s no gains to be made that are significant enough to warrant the complexity of converting back to an old format when needed. The value part is a different story. In traditional dedup, an entry contains four “physical” entries, which look like this: typedef struct ddt_phys { dva_t ddp_dva[SPA_DVAS_PER_BP]; uint64_t ddp_refcnt; uint64_t ddp_phys_birth; } ddt_phys_t; That’s three 128-bit DVAs, the refcount for this entry, and the birth time (transaction id) for the entry. This is all pretty reasonable, and you can see how it can be combined with the key to produce an almost-complete block pointer (good enough to scrub the block anyway). But wait, four? Why four? Well. OpenZFS datasets have a copies parameter, that says how many copies of the block data should be written out. If you recall above I said that during a write, the metaslab allocator is called to allocate space to store the block data, and can return multiple DVAs. That’s how copies works: the allocator allocates one, two or three regions on the disk of the right size, the data is written to all of them, and that many DVAs go in the block pointer. The thing is, you can change this property live, and it does what most property changes do, which is to affect all future writes, while leaving existing data on disk. Consider what that means for dedup. Say you have a dataset with copies=1 (the default), and you write a block. The block pointer gets one DVA (the other two are all-zero). You copy it a few times, which reuses the DVA and bumps the dedup refcount. Then you change to copies=2 and copy the block again. The entry is looked up in the dedup table, but only has one DVA when the write policy is requesting two. What do we do? Traditional dedup’s answer is to treat this as a brand-new write. It goes through to the allocator, two DVAs are allocated and written. Then the dedup entry is updated, but instead of bumping the refcount for the 1-copy “physical” entry (at dde_phys[1]), it instead copies the DVAs into the 2-copy entry (at dde_phys[2], and sets the refcount there to 1. And from then on, those two variants of the block are treated effectively as separate dedup entries with a common key. The thing is, is very unusual for an operator to modify copies= on an existing dataset, and also ill-advised where dedup is in play, because it effectively invalidates your existing dedup table for new writes: they all start back at 1, including using more space! So most of the time, the other “physical” entries are going to be filled with zeroes, unused. This isn’t too much of an issue for entries stored in the dedup ZAP, as those are compressed and long runs of zeroes compress reasonably well. But, while they’re in memory on the live list, they’re just sitting there, at least 192 bytes of zeroes that will never be needed. (Astute readers that know that a block pointer can only contain up to three DVAs, and that copies= can be set to 1, 2 or 3, might now be wondering what the fourth entry is for. The answer, these days, is nothing. It used to be where the dedupditto feature lived, storing extra DVAs “just in case”, but it was buggy and not really useful, and was removed years ago. It’s still supported for very old pools, but modern OpenZFS can only read them, not write them). After some experimentation, we realised that if we receive a write for a block that is already on the dedup table, but has too few DVAs, all we really have to do is allocate and write enough additional copies of the data to fulfill the request (up to 3), and add those to the dedup entry when we bump the refcount. This does mean we now have blocks on disk with the old number of DVAs, but that’s ok, as that’s the same guarantee as before. It does make the “lookup entry” part of the IO pipeline more complex of course, and it does introduce some subtleties when freeing the block when the refcount reaches zero, but that’s fine - clever and tricky things are ok, for a good cause. And a good cause this is! New dedup tables created with fast dedup enabled have the “value” part of the entry at just 72 bytes, rather than 256 is the traditional version (the extra 8 is for an additional 64-bit integer to support the pruning feature, see below). Pull this all together, and a single entry in the live list is now 216 bytes, almost half the original 424 in a traditional entry. “Half the memory” is a pretty nice thing to be able to put on the future, especially for slab memory that can’t be easily reclaimed. The stored entry is technically also smaller, but it’s still going to hit that ballpark of ~64 bytes after compression. It starts a lot smaller, but is a lot less compressible because it’s not full of long runs of zeroes that are easy to reduce. Once we take ZAP chunk overheads into account there’s very little gain made here. But that’s ok, because reducing the size of the dedup ZAP entries was never really in our plans. The dedup log 🔗 As we’ve discussed, the live entry list has a record for every deduplicated modified on the current transaction. At the end of the transaction, these updated entries are written back to the dedup ZAPs. Each entry is surrounded by 187 other entries in the same dedup block, which as the dedup table gets larger, are less and less likely to have been touched on this transaction. So in order to update the entry, we have potentially had to load a full block of entries, which is additional IO or, in the better-but-still-bad scenario, “loaded” the block from the ARC instead. And then, once all the entries are updated, the live entry list is cleared. After considering some customer workloads, and doing some of our own experiments, we decided that if a block is to be duplicated, it’s more likely to be one that was “recently” created or duplicated. Or, put another way, the longer it’s been since a block was touched, the less likely it is that it will be duplicated or freed in the future. Without thinking very hard this intuitively makes sense; we tend to work with the same bit of data a lot, and then we don’t touch it again for a while, or ever. If this is true, then it means throwing away the live entry list at the end of the transaction ends up being extremely wasteful, because there’s a good chance we’re going to need some or even most of those entries in the very near future! The thing is, the changes represented by the live entry list “belong” to the transaction. They must be written down with that transaction, otherwise rolling back to that transaction (eg during crash recovery) will have stale information in the dedup table. So we started thinking about where else we could possibly record these changes such that we could get them back quickly in subsequent transactions, and in a rollback, without wearing the full cost of updating the dedup table proper every time. The answer of course is the same as it’s been any time any storage system has wanted to defer work into the future: add a “journal” or “log” describing the changes, replay the log during crash recovery, and during normal operation, slowly write the logged changes out to their final resting place. The fundamentals of the dedup architecture however, make things a little more complicated, as we’ll see when we imagine building up this system. So let’s imagine the simplest thing that might work. At the end of transaction, instead of updating the dedup ZAP, we just dump the entire live entry list as an array of fixed-size entries onto the end of some object, which we declare to be the log. Since the same entry might have been updated on two or more consecutive transactions, and we’re only appending to the log, this means that the log might contain the same entry more than once. That’s fine, it just means we should only use the last we see. Every so often, we blast through the log, add the last instance of each entry to the dedup ZAP, and then zero the log. Job done. This actually works very well for what it is. The log is stored in a regular object, and so is bound to the same transaction as the data changes associated with its entries. Multiple changes to the same entries end up being amortised somewhat; if we only write the log back to the ZAP every five transactions, an entry that changes every five transactions will only be written once. So great, that’s the write overhead taken care of. The critical flaw here is on the lookup side. At any point, the write pipeline is going to come asking for an entry. If it hasn’t been used this transaction (ie it’s not on the live entry list), it will go to the dedup ZAP and get the entry from there. But if that entry is on the log, then the entry in the ZAP is stale, and we can’t use it, both because it may be wrong, and because it’s going to be overwritten when the log is written out. Our only option then is to search the log to find out if it has a more recent version of the entry. The problem with that is that most of the time it’s even worse than reading from the ZAP, as the log has no useful ordering, and duplicate entries. We potentially have to read the entire log, however enormous it may be, only to find the entry wasn’t even there and still have to do a lookup in the ZAP. What we need is an index of the log, that gives us a fast way to look up any entry that might exist in it. As it turns out, once you remove the overheads of a “live” object, a single “logged” entry held in memory is only 144 bytes, for the entire entry. This is small enough that it’s feasible to keep the entire log in memory, as well as on disk. And then, when doing a lookup, if the entry we want is not on the live entry list, we then check the in-memory log, and if it’s not there, we go to the dedup ZAP. And then, at end of transaction, we save the updated version of the entry to both the in-memory log and to the on-disk log. On disk as well? Yes. We still need crash safety. But when we take these two version of the log together, the on-disk log becomes write-only, while all regular activity goes to the in-memory log, that is, the two contain alternate representations of the same data. In the case of rollback or crash recovery (both of which happen at pool import), we simply load the in-memory log from the on-disk log, and move on with life. Incremental log flushing 🔗 Of course, we’ve now introduced some new complexity to help with managing some existing complexity, which naturally means we have to do some work to manage the complexity as well. We’ve reduced the per-transaction IO overhead of the dedup table to about the smallest it can be, at the cost of the memory required to carry a copy of the log. It’s smaller on average compared to the ARC overhead, but it’s not nothing, and we have to keep it in check. Our earliest version just watched the size of the in-memory tree and when it grew “too big”, at the end of transaction, we just wrote the entire log out to the dedup ZAP and cleared it. This is less IO in total than if we’d written those updates to the dedup ZAP at the end of each transaction, but at least that IO was spread across multiple transactions. In our testing, we could easily causes substantial flushing pauses with only a few thousand entries on the list, long before any real memory pressure was fault. So instead, we changed things so that some amount of the log was written out to the ZAP every transaction. We monitor the log flush rate against the amount of time spent on real IO, so that we write less in busy periods, and more in quiet periods. There’s some extra consideration there too, like, we may accelerate when the in-memory log is too large and causing memory pressure, and a few other things. Incremental flushing brought back an older problem though. We want to be able to zero the on-disk log. We know which are the “most recent” entries on the log, because they’re the only ones on the in-memory log. But, we don’t know where in the on-disk log those versions of those entries are, and, because new entries are being added to the log on the same transactions as we are writing them out, we don’t know which entries on the in-memory log have been written out. Under this model, we cannot zero the on-disk log until and unless the in-memory log is empty, and the in-memory log will only be empty if no updated entries have occurred, which ultimately means that eventually, we have stop and flush the remaining log. And, because the on-disk log is only appended to, the longer we drag out the flushing, the larger it gets. To handle this, we actually have two logs, each with an in-memory and on-disk version. One of these is only being flushed, the other is only being updated. In this way, we can accumulate new updates on the “active” log, while the “flushing” log is being emptied. Once it’s empty, the on-disk flushing log is zeroed, and the two logs are swapped: the old “active” is now “flushing” and begins being written out, and new changes are written to active. We get our wish that the log needs to be stopped before it can be flushed, without needing to stop the world. Of course, this adds further complications to the lookup stage, as we now have to look for an entry on the “active” log list first, and then on the “flushing” log list, before finally going to the ZAP. For obscure reasons, that then means that entries “loaded” from the flushing list then ”saved” to the active list need a little bit of extra finesse, because we can end up with entries on the on-disk “flushing” tree that were never flushed before they were reused. It’s nothing to worry about; just slightly more complicated dance moves. There’s also a “log checkpoint”. Since checksums are really just very large numbers, they have a very comfortable ordering. So, when we finish flushing on this transaction, we write the last checksum we wrote to the disk (actually to the bonus buffer of the flushing log object). This is there to make import faster; we have to reload both log lists. We can use the checkpoint when reading the flushing log to know which entries have already been flushed and not even bother putting them in the in-memory list. Finally, there’s some interesting interactions with pool scans (ie scrub and resilver). Normally, when dedup is enabled, a scan begins by walking the dedup table, reading every block pointer (and taking the wanted action) from every entry within it, and then moving on to the rest of the pool. Every so often, the scan process will record a position in the pool, so that the scan can be resumed from that point. The problem with the dedup log is that there is no useful notion of “position” within it to record. The on-disk log has no natural layout as we know, while the in-memory log uses a common structure in OpenZFS called an AVL tree, which does not have a “stable cursor”; that is, there’s nothing you can store to describe a logical position in the tree that would carry over to a different tree with the same structure. We tried a lot of things to synthesize an AVL cursor, and it is sort of possible, but not within the constraints of the “position” data we need to save (for those playing along at home, we need to add a 40-byte key to scn_ddt_bookmark within dsl_scan_phys_t). In the end, we take something of a coward’s way out: when a scrub is requested, we accelerate log flushing; all the log is flushed out to the dedup ZAP, and then do it the old way. Scans set the current transaction as the “end of scan” point, so we don’t need to worry about changes that come in after, and the dedup ZAP will have everything from before the flush. It does mean that after a crash, the log needs to be re-flushed before the scan can continue, but the expectation is always that the size of the dedup ZAP and the pool data as a whole is always going to dwarf the size of the log. Unique entries 🔗 So that was a lot about the logs! If you’re still reading, well done! There was one other issue with traditional dedup, and that was the difficulties caused by unique entries vastly inflating the size of the table with entries that never get used. There’s some new tools to help the operator manage the table size generally, and unique entries specifically. The big help for unique entries is the new zpool ddtprune command. It will remove some amount of unique entries from all dedup tables on the system, specified by age or percentage. The age option works particularly well with our ideal workload where more recently used data is more likely to be deduplicated. This sort of usage pattern results in a long and aging tail of unique entries that will never be deduplicated. Now you can wholesale get rid of them, and with the new “ZAP shrink” enhancement, dedup ZAP blocks that end up entirely empty as a result of this operation will simply be removed. Of course, this does mean that if a block whose dedup entry has been removed does later get copied, it will be a new block with a new allocation; there will be no deduplication. That said, if a very old unique block is suddenly copied a dozen times, that will be a dozen references to a single new block, and you’ll have two copies instead of the 13 you’d have without dedup at all. So you do need to tune the behaviour of ddtprune to match your workload, but it may not be a total disaster if you were to prune too much. Meanwhile, the pool property dedup_table_quota lets you set a maximum possible size for the dedup tables on your pool. If creating a new entry would take the dedup tables over that limit, the entry won’t be created and the write will just be a regular non-dedup’d write. This is good to use in conjunction with a dedicated dedup device where you want it to not spill out to the main device if it gets full. That’s a lot! Anything else? 🔗 Just a handful of operational improvements. zpool prefetch -t ddt will preload the dedup tables into the ARC, which can help with performance immediately after pool import. In traditional dedup it’s obvious how this helps, but even in fast dedup, entries not on the log still need to be loaded from the ZAPs, and flushing still needs the ZAPs nearby to write too, so having them in the ARC still helps. There’s a new collection of kstats, in /proc/spl/kstat/zfs//ddt_stats_ on Linux or kstat.zfs..misc.ddt_stats_ on FreeBSD. These will show various live stats for the dedup subsystem, including counts of lookups, hit rates on the live and log lists and the dedup ZAPs, and the log flushing rates. There’s also a new collection of tuneables, /sys/modules/zfs/parameters/zfs_dedup_log_* on Linux or vfs.zfs.dedup.log_* on FreeBSD. These control various inputs into how much log flushing occurs each transaction. As usual, the defaults are carefully considered and should be fine for anyone, but when they’re not, having some knobs to adjust is a game changer. And then the existing dedup-aware things (zpool status -D, zdb -D, zdb -S, etc) have been updated to understand all this new stuff. Nice! Can’t wait to use this with my existing dedup table 🔗 😬 So almost all of the above requires on-disk format changes that your existing dedup tables don’t have. This was unfortunate, but intentional: the project brief explicitly excluded a migration path, because it’s complicated (read: expensive) and there’s very few dedup installations out there of sufficient size and complexity to require it. However, we didn’t go out of our way to make it not work, or to prevent it from being possible in the future. For existing tables, anything that doesn’t require an on-disk format change should work: Table quotas (dedup_table_quota pool property) Table prefetch (zpool prefetch -t ddt) Lookup and hit counts (ddt_stats_* kstats) ZAP “shrink” (collapsing whole empty blocks; this is a general ZAP enhancement in 2.3) The dedup log feature should be straightforward to make work with traditional tables. Nothing in it cares about the size of the entries (in fact, “log” and “flat entry” are two separate subfeatures. The only missing piece is something to set up the new “container” object for an existing table, which would be fairly easy to do. Of course, you would not get the smaller live and log list entries in memory or on disk, so some of the sizing tuning would be different. Unique entry pruning (zpool ddtprune) should be straightforward to add for only the “percentage of uniques” mode. The “age” mode is not possible, as it requires data in the new entry format which doesn’t exist in the traditional format. Converting your old tables is not currently possible. In the simplest case, where copies= has never been changed, it would be as straightforward as creating a new ZAP, walking over the existing ZAP, converting the entry and copying it in. Doing this online would be complicated as we’d need to either be reading from both old and new ZAPs, or writing down to both ZAPs and then switch over at the end. Doing it offline would be easier, and could be done through a userspace tool, but of course requires taking the pool offline. If copies= has been changed and there are existing entries carrying both kinds, then a complete conversion is not possible, as the whole method by which existing “variants” of a block are upgraded is different and there just isn’t room in a new entry to store it all. The most fortunate cases would be if one and only one of the variants has a refcount greater than 1, as those are uniques that could be “pruned”. Otherwise there’s nothing we can do with those (though I have just had a strange thought involving the BRT). And of course, the usual “trick” of sending the deduplicated dataset to another pool where the new dedup is available will certainly do the job. If you are one of those people with an enormous dedup table and you’re interested in funding development of one or more of these options, Klara would love to talk to you. Is deduplication really good now? 🔗 I think it really is good enough to at least play with. The overheads should at least be reduced enough to make it useful in more marginal situations. Is it really good though? Probably not, not yet, but “not yet” is part of the point of all this. We’ve taken perhaps the most unloved OpenZFS feature, given it a substantial upgrade that hopefully will get more people taking a look at it, or maybe even taking a second look at it. Meanwhile, the code is now better structured, commented and understood by a lot more people, and has a lot more obvious points to add and change behaviour. It can finally join all the other features in the OpenZFS toolbox, and from there, who knows what it can become. I don’t get it. After all this, if it’s good enough, why shouldn’t I enable dedup everywhere? 🔗 If you’re like most people, you’re thinking about transparent deduplication because you have a general-purpose workload, that is, some “big ball of miscellaneous files” setup, like a local desktop or laptop. Or maybe a bigger version of that, for example, you provide exactly that service for all the people in your organisation. And good disks cost money, and times are tough, and you’re thinking well, if I can turn this thing on and it saves a bit of data, wouldn’t that be worth it? I actually agree with this in theory. As we’ve seen from the last 7000+ words, the overheads are not trivial. Even with all these changes, you still need to have a lot of deduplicated blocks to offset the weight of all the unique entries in your dedup table. But, what might surprise you is how rare it is to find blocks eligible for deduplication are on most general purpose workloads. Consider a simulated dedup run on my laptop. This is the machine I use for everything, home and work, easily 12 hours every day. $ zpool list crayon NAME SIZE ALLOC FREE CKPOINT EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT crayon 444G 397G 47.3G - - 72% 89% 1.00x ONLINE - $ zdb -S crayon Simulated DDT histogram: bucket allocated referenced ______ ______________________________ ______________________________ refcnt blocks LSIZE PSIZE DSIZE blocks LSIZE PSIZE DSIZE ------ ------ ----- ----- ----- ------ ----- ----- ----- 1 11.7M 708G 362G 373G 11.7M 708G 362G 373G 2 12.2K 666M 284M 293M 25.1K 1.32G 582M 602M 4 294 4.44M 962K 1.55M 1.44K 22.0M 4.67M 7.76M 8 5 99K 41K 44K 52 1.06M 440K 464K 16 1 7.50K 3.50K 4K 26 195K 91K 104K Total 11.7M 708G 362G 373G 11.7M 709G 362G 373G dedup = 1.00, compress = 1.96, copies = 1.03, dedup * compress / copies = 1.90 So for a table of 11.7M entries, the number of those that represent something we actually managed to deduplicate is a literal rounding error. It’s pretty much entirely uniques, pure overhead. Turning on dedup would just add IO and memory pressure for almost nothing. But the real reason you probably don’t want dedup these days is because since OpenZFS 2.2 we have the BRT (aka “block cloning” aka “reflinks”). (I acknowledge that it had a shaky start, which has been written and presented on extensively, so I won’t do that again, but lets just say it’s all good now). You may recall, way back at the top of this post, we asked “what even is dedup?”, and we defined it as: When OpenZFS prepares to write some data to disk, if that data is already on disk, don’t do the write but instead, add a reference to the existing copy. The dedup table and its entourage all exist to answer “is this data already on disk?”, but in one quite niche situation: when you don’t have any other knowledge or context about the data being written. The thing is, it’s actually pretty rare these days that you have a write operation coming from some kind of copy operation, but you don’t know that came from a copy operation. In the old days, a client program would read the source data and write to the destination data, and the storage system would see these as two unrelated operations. These days though, “copy offloading” is readily available, where instead of reading and writing, the program will tell the storage system “copy this source to that destination” and the storage system is free to do that however it wants. A naive implementation will just do the same read and write as the client would, but a smarter system could do something different, for example, not doing the write and instead just reusing the existing data and bumping a refcount. For Linux and FreeBSD filesystems, this “offload” facility is the copy_file_range() syscall. Most systems have an equivalent; macOS calls it copyfile(), Windows calls it FSCTL_SRV_COPYCHUNK. NFS and CIFS support something like it, OS block device drivers are getting equivalents, even disk protocols have something like it (eg SCSI EXTENDED COPY or NVMe Copy). If you put all this together, you end up in a place where so long as the client program (like /bin/cp) can issue the right copy offload call, and all the layers in between can translate it (eg the Window application does FSCTL_SRV_COPYCHUNK, which Samba converts to copy_file_range() and ships down to OpenZFS). And again, because there’s that clear and unambiguous signal that the data already exists and also it’s right there, OpenZFS can just bump the refcount in the BRT. Most importantly is the space difference. If a block is never cloned, then we never pay for it, and if it is cloned, the BRT entry is only 16 bytes. On my pool, where the two major users of copy_file_range() (that I know about) are cp and ccache, my BRT stats are rather nicer: $ zdb -TTT crayon BRT: used 292M; saved 309M; ratio 2.05x BRT: vdev 0: refcnt 12.2K; used 292M; saved 309M BRT: vdev 0: DVAs with 2^n refcnts: 1: 11788 **************************************** 2: 645 *** 3: 40 * 4: 3 * 5: 1 * If you compare to the dedup simulation, I’m not saving as much raw data as dedup would get me, though it’s pretty close. But I’m not spending a fortune tracking all those uncloned and forgotten blocks. Now yes, this is not plumbed through everywhere. zvols don’t use the BRT yet. Samba has only just gotten support for OpenZFS very recently. Offloading in Windows is only relatively new. The situation is only going to get better, but maybe it’s not good enough yet. So maybe you might be tempted to try dedup anyway, but for mine, I can’t see how the gains would be worth it even without block cloning. And this is why I say you probably don’t want it. Unless you have a very very specific workload where data is heavily duplicated and clients can’t or won’t give direct “copy me!” signal, then just using block cloning is likely to get you a good chunk of the gain without the outsized amount of pain. In summary 🔗 Dedup is about balancing IO throughput, memory usage and dedup table size. Traditional dedup has a very tiny “sweet spot” where these factors balance nicely, while being ruinous if you fall out of it. Fast dedup improves all three, making it far easier to balance these factors and rather less of a disaster if it doesn’t work out. However, it is still only of benefit if you have a truly enormous amount of data, that gets copied a lot, and aren’t able to take advantage of other “zero-copy” options within OpenZFS, like block cloning or snapshot clones. Extra congratulations if you got this far. I hope this was of interest! Thanks to @mattcen and @adavis for numerous grammar and spelling fixes. Much obliged! 💚 ← A few nice things in OpenZFS 2.3",
    "commentLink": "https://news.ycombinator.com/item?id=42000784",
    "commentBody": "OpenZFS deduplication is good now and you shouldn't use it (despairlabs.com)418 points by type0 21 hours agohidepastfavorite201 comments Wowfunhappy 20 hours agoI want \"offline\" dedupe, or \"lazy\" dedupe that doesn't require the pool to be fully offline, but doesn't happen immediately. Because: > When dedup is enabled [...] every single write and free operation requires a lookup and a then a write to the dedup table, regardless of whether or not the write or free proper was actually done by the pool. To me, this is \"obviously\" the wrong approach in most cases. When I'm writing data, I want that write to complete as fast as possible, even at the cost of disk space. That's why I don't save files I'm actively working on in 7zip archives. But later on, when the system is quiet, I would love for ZFS to go back and figure out which data is duplicated, and use the BRT or whatever to reclaim space. This could be part of a normal scrub operation. reply cryptonector 17 hours agoparentLazy/off-line dedup requires block pointer rewrite, but ZFS _cannot_ and will not ever get true BP rewrite because ZFS is not truly a CAS system. The problem is that physical locations are hashed into the Merkle hash tree, and that makes moving physical locations prohibitively expensive as you have to rewrite all the interior nodes on the way to the nodes you want to rewrite. A better design would have been to split every node that has block pointers into two sections, one that has only logical block pointers and all of whose contents gets hashed into the tree, and one that has only the physical locations (as if it were a cache) of the corresponding logical block pointers in the first section, with the second section _not_ hashed into the Merkle hash tree. Then BP rewrite would only require re-writing blocks that are not part of the Merkle hash tree. But as it is you can't get BP rewrite to work on ZFS, so you can't get what you're asking for. Well... maybe. Perhaps on read hash mismatch ZFS could attempt to locate the pointed-to block in the dedup table using the hash from the pointer. Then ZFS could reallocate the dedup'ed block. The price you'd pay then is one pointless read -- not too bad. The impossibility of BP rewrite generally leads to band-aids like this. reply Pet_Ant 4 hours agorootparent> CAS system It looks like it means: https://en.wikipedia.org/wiki/Content-addressable_storage reply cryptonector 3 hours agorootparentSorry, yes, CAS really means that pointers are hash values -- maybe with extra metadata, yes, but _not_ including physical locations. The point is that you need some other way to map logical pointers to physical locations. The easiest way to do that is to store the mappings nearby to the references so that they are easy to find, but the mappings must be left out of the Merkle hash tree in order to make it possible to change the physical locations of the referenced blocks. reply EvanAnderson 19 hours agoparentprev> I just wish we had \"offline\" dedupe, or even \"lazy\" dedupe... This is the Windows dedupe methodology. I've used it pretty extensively and I'm generally happy with it when the underlying hardware is sufficient. It's very RAM and I/O hungry but you can schedule and throttle the \"groveler\". I have had some data eating corruption from bugs in the Windows 2012 R2 timeframe. reply DannyBee 18 hours agoparentprevYou can use any of the offline dupe finders to do this. Like jdupes or duperemove. I sent PR's to both the ZFS folks and the duperemove folks to support the syscalls needed. I actually have to go followup on the ZFS one, it took a while to review and i realized i completely forget to finish it up. reply Dylan16807 20 hours agoparentprevThe ability to alter existing snapshots, even in ways that fully preserve the data, is extremely limited in ZFS. So yes that would be great, but if I was holding my breath for Block Pointer Rewrite I'd be long dead. reply Wowfunhappy 18 hours agorootparentYou need block pointer rewrite for this? reply Dylan16807 17 hours agorootparentYou don't need it to dedup writable files. But redundant copies in snapshots are stuck there as far as I'm aware. So if you search for duplicates every once in a while, you're not going to reap the space savings until your snapshots fully rotate. reply Wowfunhappy 7 hours agorootparentThanks. I do think dedupe for non-snapshots would still be useful, since as you say most people will get rid of old snapshots eventually. I also wonder if it would make sense for ZFS to always automatically dedupe before taking a snapshot. But you'd have to make this behavior configurable since it would turn shapshotting from a quick operation into an expensive one. reply lazide 14 hours agorootparentprevThe issue with this, in my experience, is that at some point that pro (exactly, and literally, only one copy of a specific bit of data despite many apparent copies) can become a con if there is some data corruption somewhere. Sometimes it can be a similar issue in some edge cases performance wise, but usually caching can address those problems. Efficiency being the enemy of reliability, sometimes. reply Dylan16807 12 hours agorootparentRedundant copies on a single volume are a waste of resources. Spend less on size, spend more on an extra parity drive, or another backup of your most important files. That way you get more safety per gigabyte. reply lazide 12 hours agorootparentNotably, having to duplicate all data x2 (or more) is more of a waste than having 2 copies of a few files - if full drive failure is not the expected failure mode, and not all files should be protected this heavily. It’s why metadata gets duplicated in ZFS the way it does on all volumes. Having seen this play out a bunch of times, it isn’t an uncommon need either. reply Dylan16807 12 hours agorootparent> having to duplicate all data x2 Well I didn't suggest that. I said important files only for the extra backup, and I was talking about reallocating resources not getting new ones. The simplest version is the scenario where turning on dedup means you need one less drive of space. Convert that drive to parity and you'll be better off. Split that drive from the pool and use it to backup the most important files and you'll be better off. If you can't save much space with dedup then don't bother. reply lazide 11 hours agorootparentThere was an implication in your statement that volume level was the level of granularity, yeah? I’m noting that during on volume wide dedup can have the con that you can’t choose (but it looks like you can!) to manually duplicate data. reply Dylan16807 10 hours agorootparentNote: I assume volume means pool? > There was an implication in your statement that volume level was the level of granularity, yeah? There was an implication that the volume level was the level of granularity for adding parity. But that was not the implication for \"another backup of your most important files\". > I’m noting that during on volume wide dedup can have the con that you can’t choose (but it looks like you can!) to manually duplicate data. You can't choose just by copying files around, but it's pretty easy to set copies=2 on specific directories. And I'd say that's generally a better option, because it keeps your copies up to date at all times. Just make sure snapshots are happening, and files in there will be very safe. Manual duplication is the worst kind of duplication, so while it's good to warn people that it won't work with dedup on, actually losing the ability is not a big deal when you look at the variety of alternatives. It only tips the balance in situations where dedup is near-useless to start with. reply UltraSane 19 hours agoparentprevThe neat thing about inline dedupe is that if the block hash already exists than the block doesn't have to be written. This can save a LOT of write IO in many situations. There are even extensions where a file copy between to VMs on a dedupe storage array will not actually copy any data but just increment the original blocks reference counter. You will see absurd TB/s write speeds in the OS, it is pretty cool. reply aidenn0 17 hours agorootparentThis is only a win if the dedupe table fits in RAM; otherwise you pay for it in a LOT of read IO. I have a storage array where dedupe would give me about a 2.2x reduction in disk usage, but there isn't nearly enough RAM for it. reply UltraSane 17 hours agorootparentyes inline dedupe has to fit in RAM. Perhaps enterprise storage arrays have spoiled me. reply aidenn0 15 hours agorootparentThis array is a bit long-in-the-tooth and only has 192GB of RAM, but a bit over 40TB of net storage, which would be a 200GB dedup table size using the back-of-the-envelope estimate of 5GB/TB. A more precise calculation on my actual data shows that today's data would allow the dedup table to fit in RAM, but if I ever want to actually use most of the 40TB of storage, I'd need more RAM. I've had a ZFS system swap dedup to disk before, and the performance dropped to approximately zero; fixing it was a PITA, so I'm not doing that anytime soon. reply barrkel 9 hours agorootparentBe aware that ZFS performance rapidly drops off north of 80% utilization, when you head into 90%, you will want to buy a bigger array just to escape the pain. reply aidenn0 1 hour agorootparentZFS has gotten significantly better at 80%, but 90% is painful enough that I almost wish it would reserve 10% a bit more explicitly (maybe like the old Unix systems that would prevent non-root users from using the last 5% of the root partition). All my arrays send me nightly e-mails at 80% so I'm aware of when I hit there, but on a desktop system that's typically not the case. reply gnu8 4 hours agorootparentprevI think that is well known amongst storage experts, though maybe not everyone who might be interested in using ZFS for storage in a professional or personal application. What I’m curious about is how ZFS’s full-disk performance (what is the best term for this?) compares to btrfs, WAFL, and so on. Is ZFS abnormally sensitive to this condition, or is it a normal property? In any case it doesn’t stick out to me as a problem that needs to be fixed. You can’t fill a propane tank to 100% either. reply magicalhippo 20 hours agoparentprevThe author of the new file-based block cloning code had this in mind. A backround process would scan files and identify dupes, delete the dupes and replace them with cloned versions. There are of course edge cases to consider to avoid data loss, but I imagine it might come soon, either officially or as a third-party tool. reply hinkley 19 hours agoparentprevI get the feeling that a hypothetical ZFS maintainer reading some literature on concurrent mark and sweep would be... inspirational, if not immediately helpful. You should be able to detect duplicates online. Low priority sweeping is something else. But you can at least reduce pause times. reply p_l 2 hours agorootparentThey were aware. The reasons it works the way it works are due to higher priority decisions regarding reliability in face of hardware or software corruption. That said, you can still do a two-space GC, but it's slow and possibly wasteful. reply LeoPanthera 19 hours agoparentprevbtrfs has this. You can deduplicate a filesystem after the fact, as an overnight cron job or whatever. I really wish ZFS could do this. reply DannyBee 18 hours agorootparentI sent a PR to add support for the necessary syscall (FIDUPERANGE) to zfs that i just have to clean up again. Once that is in, any of the existing dupe finding tools that use it (IE jdupes, duperemove) will just work on ZFS. reply edelbitter 13 hours agorootparentKnowing what you had to know to write that, would you dare using it? Compression, encryption and streaming sparse files together are impressive already. But now we get a new BRT entry appearing out of nowhere, dedup index pruning one that was there a moment ago, all while correctly handling arbitrary errors in whatever simultaneous deduped writes, O_DIRECT writes, FALLOC_FL_PUNCH_HOLE and reads were waiting for the same range? Sounds like adding six new places to hold the wrong lock to me. reply DannyBee 13 hours agorootparent\"Knowing what you had to know to write that, would you dare using it?\" It's no worse than anything else related to block cloning :) ZFS already supports FICLONERANGE, the thing FIDEDUPRANGE changes is that the compare is part of the atomic guarantee. So in fact, i'd argue it's actually better than what is there now - yes, the hardest part is the locking, but the locking is handled by the dedup range call getting the right locks upfront, and passing them along, so nothing else is grabbing the wrong locks. It actually has to because of the requirements to implement the ioctl properly. We have to be able to read both ranges, compare them, and clone them, all as an atomic operation wrt to concurrent writes. So instead of random things grabbing random locks, we pass the right locks around and everything verifies the locks. This means fideduprange is not as fast as it maybe could be, but it does not run into the \"oops we forgot the right kind of lock\" issue. At worst, it would deadlock, because it's holding exclusive locks on all that it could need before it starts to do anything in order to guarantee both the compare and the clone are atomic. So something trying to grab a lock forever under it will just deadlock. This seemed the safest course of implementation. ficlonerange is only atomic in the cloning, which means it does not have to read anything first, it can just do blind block cloning. So it actually has a more complex (but theoretically faster) lock structure because of the relaxed constraints. reply DannyBee 12 hours agorootparentprevNote - anyone bored enough could already make any of these tools work by using FICLONERANGE (which ZFS already supports), but you'd have to do locking - lock, compare file ranges, clone, unlock. Because FIDEDUPRANGE has the compare as part of the atomic guarantee, you don't need to lock in userspace around using it, and so no dedup utility bothers to do FICLONERANGE + locking. Also, ZFS is the only FS that implements FICLONERANGE but not FIDEDUPRANGE :) reply rattt 13 hours agorootparentprevShouldn't jdupes like tools already work now that ZFS has reflink copy support? reply DannyBee 13 hours agorootparentNo, because none of these tools use copy_file_range. Because copy_file_range doesn't guarantee deduplication or anything. It is meant to copy data. So you could just end up copying data, when you aren't even trying to copy anything at all. All modern tools use FIDEDUPRANGE, which is an ioctl meant for explicitly this use case - telling the FS that two files have bytes that should be shared. Under the covers, the FS does block cloning or whatever to make it happen. Nothing is copied. ZFS does support FICLONERANGE, which is the same as FIDEDUPRANGE but it does not verify the contents are the same prior to cloning. Both are atomic WRT to concurrent writes, but for FIDEDUPRANGE that means the compare is part of the atomicness. So you don't have to do any locking. If you used FICLONERANGE, you'd need to lock the two file ranges, verify, clone, unlock FIDEDUPRANGE does this for you. So it is possible, with no changes to ZFS, to modify dedup tools to work on ZFS by changing them to use FICLONERANGE + locking if FIDEDUPRANGE does not exist. reply Sakos 6 hours agorootparentprevThis is my favourite hack for the Steam Deck. By switching my SD cards and the internal SSD to btrfs, the space savings are unreal (easily halving used space). Every game gets its own prefix which means a crazy amount of file duplication. reply kccqzy 49 minutes agorootparentWhich tool did you end up using for btrfs? I tried out bees https://github.com/Zygo/bees but it is way too slow. reply tiagod 18 hours agoparentprevI run rdfind[1] as a cronjob to replace duplicates with hardlinks. Works fine! https://github.com/pauldreik/rdfind reply AndrewDavis 14 hours agorootparentSo this is great, if you're just looking to deduplicate read only files. Less so if you intend to write to them. Write to one and they're both updated. Anyway. Offline/lazy dedup (not in the zfs dedup sense) is something that could be done in userspace, at the file level on any filesystem that supports reflinks. When a tool like rdfind finds a duplicate, instead of replacing with a hardlink, create a copy of the file with `copy_file_range(2)` and let the filesystem create a reflink to it. Now you've got space savings and they're two separate files so if one is written to the other remains the same. reply spockz 14 hours agorootparentHow would this work if I have snapshots? Wouldn’t then the version of the file I just replaced still be in use there? But maybe I also need to store the copy again if I make another snapshot because the “original “ file isn’t part of the snapshot? So now I’m effectively storing more not less? reply magicalhippo 7 hours agorootparentAFAIK, yes. Blocks are reference counted, so if the duplicate file is in a snapshot then the blocks would be referenced by the snapshot and hence not be eligible for deallocation. Only once the reference count falls to zero would the block be freed. This is par for the course with ZFS though. If you delete a non-duplicated file you don't get the space back until any snapshots referencing the file are deleted. reply DannyBee 13 hours agorootparentprevcopy_file_range already works on zfs, but it doesn't guarantee anything interesting. Basically all dupe tools that are modern use fideduprange, which is meant to tell the FS which things should be sharing data, and let it take care of the rest. (BTRFS, bcachefs, etc support this ioctl, and zfs will soon too) Unlike copy_file_range, it is meant for exactly this use case, and will tell you how many bytes were dedup'd, etc. reply sureglymop 10 hours agorootparentprevQuite cool, though it's not as storage saving as deduplicating at e.g. N byte blocks, at block level. reply Wowfunhappy 18 hours agorootparentprevBut then you have to be careful not to remove the one which happens to be the \"original\" or the hardlinks will break, right? reply Dylan16807 17 hours agorootparentNo, pointing to an original is how soft links work. Hard links are all equivalent. A file has any number of hard links, and at least in theory you can't distinguish between them. The risk with hardlinks is that you might alter the file. Reflinks remove that risk, and also perform very well. reply Wowfunhappy 4 hours agorootparentThank you, I was unaware of this. However, the fact that editing one copy edits all of them still makes this a non-solution for me at least. I'd also strongly prefer deduping at the block level vs file level. reply mdaniel 3 hours agorootparentI would suspect a call to $(chmod a-w) would fix that, or at least serve as a very fine reminder that there's something special about them reply simonjgreen 9 hours agoprevWe used to make extensive use of, and gained huge benefit from, dedup in ZFS. The specific use case was storage for VMWare clusters where we had hundreds of Linux and Windows VMs that were largely the same content. [this was pre-Docker] reply aniviacat 5 hours agoparentI've read multiple comments on using dedup for VMs here. Wouldn't it be a lot more efficient for this to be implemented by the hypervisor rather than the filesystem? reply UltraSane 4 hours agorootparentI'm a former VMware certified admin. How do you envision this to work? All the data written to the VM's virtual disk will cause blocks to change and the storage array is the best place to keep track of that. reply PittleyDunkin 26 minutes agorootparent> VMware certified admin Not to be rude, but does this have any meaning? reply wang_li 3 hours agorootparentprevYou do it at the file system layer. Clone the template which creates only metadata referencing the original blocks then you perform copy-on-write as needed. reply SteveNuts 3 hours agorootparentVMware allows linked clones which you can do when deploying from template https://docs.vmware.com/en/VMware-Fusion/13/com.vmware.fusio... reply UltraSane 3 hours agorootparentprevBut that is exactly what the storage array is doing. What is the advantage? reply anyfoo 2 hours agorootparent> When dedup is enabled [...] every single write and free operation requires a lookup and a then a write to the dedup table, regardless of whether or not the write or free proper was actually done by the pool. Linked clones shouldn’t need that. They likely start out with only references to the original blocks, and then replace them when they change. If so, it’s a different concept (as it would mean that any new duplicate blocks are not shared), but for the use case of “spin up a hundred identical VMs that only change comparably little” it sounds more efficient performance-wise, with a negligible loss in space efficiency. Am I certain of this? No, this is just what I quickly pieced together based on some assumptions (albeit reasonable ones). Happy to be told otherwise. reply UltraSane 51 minutes agorootparentLinked clones aren't used in ESXi, instant clones and they ARE pretty nifty and heavily used in VDI where you need to spin up many thousands of desktop VMs. But they have to keep track of what blocks change and so ever clone has a delta disk. At the end of the day you are just moving around where this bookkeeping happens. And it is best to happen on a enterprise grade array with ultra optimized inline dedupe like a Pure array. https://www.yellow-bricks.com/2018/05/01/instant-clone-vsphe... reply anyfoo 6 minutes agorootparentI’m not sure that’s true, because the hypervisor can know which blocks are related to begin with? From what I quoted above it seems that the file system instead does a lookup based on the block content to determine if a block is a dupe (I don’t know if it uses a hash, necessitating processing the whole block, or something like an RB tree, which avoids having to read the whole block if it already differs early from candidates). Unless there is a way to explicitly tell the file system that you are copying blocks for that purpose, and that VMware is actually doing that. If not, then leaving it to the file system or even the storage layer should have a definite impact on performance, albeit in exchange for higher space efficiency because a lookup can deduplicate blocks that are identical but not directly related. This would give a space benefit if you do things like installing the same applications across many VMs after the cloning, but assuming that this isn’t commonly done (I think you should clone after establishing all common state like app installations if possible), then my gut feeling is very much that the performance benefit of more semantic-level hypervisor bookkeeping outweighs the space gains from “dumb” block-oriented fs/storage bookkeeping. iwontberude 2 hours agorootparentprevCOW is significantly slower and has nesting limits when compared to these deduped clones. Great question! reply jamesbfb 6 hours agoparentprevCan relate. I’ve recently taken ownership of a new work laptop with Ubuntu (with “experimental” zfs) and using dedupe on my nix store has been an absolute blessing! reply amarshall 6 hours agorootparentNix already has some builtin deduplication, see `man nix-store-optimise`. Nix’s own hardlinking optimization reduces disk usage of the store (for me) by 30–40%. reply rwarfield 6 hours agorootparentprevIsn't it better to use `nix store optimise` for dedup of the nix store? The nix command has more knowledge of the structure of the nix store so should be able to do a better job with fewer resources. Also the store is immutable so you don't actually need reflinks - hard links are enough. reply Filligree 3 hours agorootparentIt is, yeah, though you have to turn it on. I'm not actually sure why it's off by default. reply UltraSane 19 hours agoprev\"And this is the fundamental issue with traditional dedup: these overheads are so outrageous that you are unlikely to ever get them back except on rare and specific workloads.\" This struck me as a very odd claim. I've worked with Pure and Dell/EMC arrays and for VMWare workloads they normally got at least 3:1 dedupe/compression savings. Only storing one copy of the base VM image works extremely well. Dedupe/compression works really well on syslog servers where I've seen 6:1 savings. The effectiveness of dedupe is strongly affected by the size of the blocks being hashed, with the smaller the better. As the blocks get smaller the odds of having a matching block grow rapidly. In my experience 4KB is my preferred block size. reply abrookewood 18 hours agoparentCouple of comments. Firstly, you are talking about highly redundant information when referencing VM images (e.g. the C drive on all Windows Serer images will be virtually identical), whereas he was using his own laptop contents as an example. Secondly, I think you are conflating two different features: compression & de-duplication. In ZFS you can have compression turned on (almost always worth it) for a pool, but still have de-duplication disabled. reply UltraSane 18 hours agorootparentFair point. My experience is with enterprise storage arrays and I have always used dedupe/compression at the same time. Dedupe is going to be a lot less useful on single computers. I consider dedupe/compression to be two different forms of the same thing. compression reduces short range duplication while deduplication reduces long range duplication of data. reply abrookewood 16 hours agorootparentYeah agreed, very closely related - even more so on ZFS where the compression (AFAIK) is on a block level rather than a file level. reply E39M5S62 15 hours agorootparentZFS compression is for sure at the block level - it's fully transparent to the userland tools. reply lazide 14 hours agorootparentIt could be at a file level and still transparent to user land tools, FYI. Depending on what you mean by ‘file level’, I guess. reply UltraSane 4 hours agorootparentWindows NTFS has transparent file level compression that works quite well. reply phil21 16 hours agoparentprevBase VM images would be a rare and specific workload. One of the few cases dedupe makes sense. However you are likely using better strategies like block or filesystem cloning if you are doing VM hosting off a ZFS filesystem. Not doing so would be throwing away one of it's primary differentiators as a filesystem in such an environment. General purpose fileserving or personal desktop/laptop use generally has very few duplicated blocks and is not worth the overhead. Backups are hit or miss depending on both how the backups are implemented, and if they are encrypted prior to the filesystem level. Compression is a totally different thing and current ZFS best-practice is to enable it by default for pretty much every workload - the CPU used is barely worth mentioning these days, and the I/O savings can be considerable ignoring any storage space savings. Log storage is going to likely see a lot better than 6:1 savings if you have typical logging, at least in my experience. reply XorNot 9 hours agorootparent> General purpose fileserving or personal desktop/laptop use generally has very few duplicated blocks and is not worth the overhead. I would contest this is because we don't have a good transparent deduplication right now - just some bad compromises. Hard copies? Edit anything and it gets edited everywhere - not what you want. Symlinks? Look different enough that programs treat them differently. I would argue your regular desktop user actually has an enormous demand for a good deduplicating file system - there's no end of use cases where the first step is \"make a separate copy of all the relevant files just in case\" and a lot of the time we don't do it because it's just too slow and wasteful of disk space. If you're working with say, large video files, then a good dedupe system would make copies basically instant, and then have a decent enough split algorithm that edit's/cuts/etc. of the type people try to do losslessly or with editing programs are stored efficiently without special effort. How many people are producing video content today? Thanks to Tiktok we've dragged that skill right down to \"random teenagers\" who might hopefully pipeline into working with larger content. reply armada651 7 hours agorootparentBut according to the article the regular desktop already has such a dedup system: > If you put all this together, you end up in a place where so long as the client program (like /bin/cp) can issue the right copy offload call, and all the layers in between can translate it (eg the Window application does FSCTL_SRV_COPYCHUNK, which Samba converts to copy_file_range() and ships down to OpenZFS). And again, because there’s that clear and unambiguous signal that the data already exists and also it’s right there, OpenZFS can just bump the refcount in the BRT. reply wongarsu 18 hours agoparentprevI haven't tried it myself, but the widely quoted number for old ZFS dedup is that you need 5GB of RAM for every 1TB of disk space. Considering that 1 TB of disk space currently costs about $15 and 5GB of server RAM about $25, you need a 3:1 dedupe ratio just to break even. If your data is a good fit you might get away with 1GB per TB, but if you are out of luck the 5GB might not even be enough. That's why the article speaks of ZFS dedup having a small sweet spot that your data has to hit, and why most people don't bother Other file systems tend to prefer offline dedupe which has more favorable economics reply floating-io 18 hours agorootparentThat doesn't account for OpEx, though, such as power... reply wongarsu 18 hours agorootparentAssuming something reasonable like 20TB Toshiba MG10 HDDs and 64GB DDR4 ECC RAM, quick googling suggests that 1TB of disk space uses about 0.2-0.4W of power (0.2 in idle, 0.4 while writing), 5GB of RAM about 0.3-0.5W. So your break even on power is a bit earlier depending on the access pattern, but in the same ball park. reply UltraSane 17 hours agorootparentWhat about rack space? reply spockz 14 hours agorootparentNot just rack space. At a certain amount of disks you also need to get a separate server (chassis + main board + cpu + ram) to host the disks. Maybe you need that for performance reasons any way. But saving disk space and only paying for it with some ram sounds cost effective. reply UltraSane 18 hours agorootparentprevWhy does it need so much RAM? It should only need to store the block hashes which should not need anywhere near that much RAM. Inline dedupe is pretty much standard on high-end storage arrays nowadays. reply AndrewDavis 16 hours agorootparentThe linked blog post covers this, and the improvements made to make the new dedup better. reply remexre 17 hours agorootparentprev(5GiB / 1TiB) * 4KiB to bits ((5 gibibytes) / (1 tebibyte)) × (4 kibibytes) = 160 bits reply Maakuth 12 hours agoparentprevCertainly it makes sense to not have deep copies of VM base images, but the deduplication is not the right way to do it in ZFS. Instead, you can clone the base image and before changes it will take almost no space at all. This is thanks to the copy-on-write nature of ZFS. ZFS deduplication instead tries to find existing copies of data that is being written to the volume. For some use cases it could make a lot of sense (container image storage maybe?), but it's very inefficient if you already know some datasets to be clones of the others, at least initially. reply UltraSane 4 hours agorootparentWhen a new VM is created from a template on a ZFS file system with dedupe enabled what actually happens? Isn't the ref count of every block of the template simply incremented by one? The only time new data will actually be stored is when a block hash a hash that doesn't already exist. reply wmf 18 hours agoparentprevVMs are known to benefit from dedupe so yes, you'll see benefits there. ZFS is a general-purpose filesystem not just an enterprise SAN so many ZFS users aren't running VMs. Dedupe/compression works really well on syslog I apologize for the pedantry but dedupe and compression aren't the same thing (although they tend to be bundled in the enterprise storage world). Logs are probably benefiting from compression not dedupe and ZFS had compression all along. reply tw04 18 hours agorootparentThey are not the same thing, but when you boil it down to the raw math, they aren't identical twins, but they're absolutely fraternal twins. Both are trying to eliminate repeating data, it's just the frame of reference that changes. Compression in this context is operating on a given block or handful of blocks. Deduplication is operating on the entire \"volume\" of data. \"Volume\" having a different meaning depending on the filesystem/storage array in question. reply UltraSane 17 hours agorootparentWell put. I like to say compression is just short range dedupe. Hash based dedupe wouldn't be needed if you could just to real-time LZMA on all of the data on a storage array but that just isn't feasible and hash-based dedupe is a very effective compromise. reply ShroudedNight 17 hours agorootparentprevIs \"paternal twins\" a linguistic borrowing of some sort? It seems a relatively novel form of what I've mostly seen referred to as monozygotic / 'identical' twins. Searching for some kind of semi-canonical confirmation of its widespread use turns up one, maybe two articles where it's treated as an orthodox term, and at least an equal number of discussions admonishing its use. reply spockz 14 hours agorootparentIf anything I would expect the term “maternal” twin to be used as whether or not a twin is monozygotic or “identical” depends on the amount of eggs from the mother. reply xxs 11 hours agorootparentprevcompression tends NOT to use a global dictionary. So to me they are vastly different even if they have the same goal of reducing the output size. Compression with a global dict would like do better than dedup yet it will have a lot of other issues. reply ants_everywhere 18 hours agorootparentprevIf we're being pedants, then storing the same information in fewer bits than the input is by definition a form of compression, no? (Although yes I understand that file-level compression with a standard algorithm is a different thing than dedup) reply jorvi 18 hours agoparentprev> In my experience 4KB is my preferred block size That makes sense considering Advanced Format harddrives already have a 4K physical sector size, and if you properly low-level format them (to get rid of the ridiculous Windows XP compatibility) they also have 4K logical sector size. I imagine there might be some real performance benefits to having all of those match up. reply UltraSane 18 hours agorootparentIn the early days of VMware people had a lot of VMs that were converted from physical machines and this causes a nasty alignment issue between the VMDK blocks and the blocks on your storage array. The effect was to always add one block to every read operation, and in the worst case of reading one block would double the load on the storage array. On NetApp this could only be fixed when the VM wasn't running. reply Joe_Cool 18 hours agoparentprevEven with the rudimentary Dedup features of NTFS on a Windows Hyper-V Server all running the same base image I can overprovision the 512GB partition to almost 2 GB. You need to be careful and do staggered updates in the VMs or it'll spectacularly explode but it's possible and quite performant for less than mission critical VMs. reply tw04 18 hours agorootparentI think you mean 2TB volume? But yes, this works. But also: if you're doing anything production, I'd strongly recommend doing deduplication on the back-end storage array, not at the NTFS layer. It'll be more performant and almost assuredly have better space savings. reply Joe_Cool 1 hour agorootparentFor sure it's not for production. At least not for stuff that's critical. MS also doesn't recommend using it for live VHDX. The partition/NTFS volume is 512GB. It currently stores 1.3 TB of \"dedupped\" data and has about 200GB free. Dedup runs asynchronously in the background and as a job during off hours. It's a typo, yes. Thanks. reply m463 16 hours agoparentprevI would think VMs qualify as a specific workload, since cloning is almost a given. reply EasyMark 16 hours agoparentprevI figured he was mostly talking about using dedup on your work (dev machine) computer or family computer at home, not on something like a cloud or streaming server or other back end type operations. reply mrgaro 12 hours agoparentprevFor text based logs I'm almost entirely sure that just using compression is more than enough. ZFS supports compression natively on block level and it's almost always turned on. Trying to use dedup alongside of compression for syslog most likely will not yield any benefits. reply bobmcnamara 15 hours agoparentprev> In my experience 4KB is my preferred block size. This probably has something to do with the VM's filesystem block size. If you have a 4KB filesystem and an 8KB file, the file might be fragmented differently but is still the same 2x4KB blocks just in different places. Now I wonder if filesystems zero the slack space at the end of the last block in a file in hopes of better host compression. Vs leaving it as past bytes. reply acdha 19 hours agoparentprev> Dedupe/compression works really well on syslog servers where I've seen 6:1 savings. Don’t you compress these directly? I normally see at least twice that for logs doing it at the process level. reply pezezin 17 hours agorootparentYes, that ratio is very small. I built a very simple, custom syslog solution, a syslog-ng server writing directly to a TimescaleDB hypertable (https://www.timescale.com/) that is then presented as a Grafana dashboard, and I am getting a 30x compression ratio. reply pdimitar 7 hours agorootparentWould love to see your solution -- if it's open source. reply UltraSane 18 hours agorootparentprevWhat software? reply acdha 18 hours agorootparentLog rotate, cron, or simply having something like Varnish or Apache log to a pipe which is something like bzip2 or zstd. The main question is whether you want to easily access the current stream - e.g. I had uncompressed logs being forwarded to CloudWatch so I had daemons logging to timestamped files with a post-rotate compression command which would run after the last write. reply UltraSane 18 hours agorootparentThat is one wrinkle of using storage based dedupe/compression is you need to avoid doing compression on the client to avoid compressing already compressed data. When a company I worked at first got their Pure array they were using windows file compression heavily and had to disable it as the storage array was now doing it automatically. reply acdha 18 hours agorootparentDefinitely. We love building abstraction layers but at some point you really need to make decisions across the entire stack. reply chasil 17 hours agorootparentprevLogrotate is the rhel utility, likely present in Fedora, that is easily adapted for custom log handling. I still have rhel5 and I use it there. CentOS made it famous. I don't know if it has a foothold in the Debian family. reply E39M5S62 15 hours agorootparentlogrotate is used on Debian and plenty of other distros. It seems pretty widely used, though maybe not as much so now that things log through systemd. reply SteveNuts 18 hours agorootparentprevLogrotate reply jyounker 9 hours agoparentprevTL;DR; Declares claim that \"that feature is only good for specific rare workloads\" is odd. Justifies that statement by pointing out that the feature works well of their specific rare workload. reply nikisweeting 20 hours agoprevI'm so excited about fast dedup. I've been wanting to use ZFS deduping for ArchiveBox data for years, as I think fast dedup may finally make it viable to archive many millions of URLs in one collection and let the filesystem take care of compression across everything. So much of archive data is the same jquery.min.js, bootstrap.min.css, logo images, etc. repeated over and over in thousands of snapshots. Other tools compress within a crawl to create wacz or warc.gz files, but I don't think anyone has tried to do compression across the entire database of all snapshots ever taken by a tool. Big thank you to all the people that worked on it! BTW has anyone tried a probabilistic dedup approach using soemthing like a bloom filter so you don't have to store the entire dedup table of hashes verbatim? Collect groups of ~100 block hashes into a bucket each, and store a hyper compressed representation in a bloom filter. On write, lookup the hash of the block to write in the bloom filter, and if a potential dedup hit is detected, walk the 100 blocks in the matching bucket manually to look for any identical hashes. In theory you could do this with layers of bloom filters with different resolutions and dynamically swap out the heavier ones to disk when memory pressure is too high to keep the high resolution ones in RAM. Allowing the accuracy of the bloom filter to be changed as a tunable parameter would let people choose their preference around CPU time/overhead:bytes saved ratio. reply mappu 19 hours agoparentEven with this change ZFS dedupe is still block-aligned, so it will not match repeated web assets well unless they exist at consistently identical offsets within the warc archives. dm-vdo has the same behaviour. You may be better off with long-range solid compression instead, or unpacking the warc files into a directory equivalent, or maybe there is some CDC-based FUSE system out there (Seafile perhaps) reply nikisweeting 19 hours agorootparentI should clarify I don't use WARCs at all with archivebox, it just stores raw files on the filsystem because I rely on ZFS for all my compression, so there is no offset alignment issue. The wget extractor within archivebox can produce WARCs as an output but no parts of ArchiveBox are built to rely on those, they are just one of the optional extractors that can be run. reply uniqueuid 20 hours agoparentprevI get the use case, but in most cases (and particularly this one) I'm sure it would be much better to implement that client-side. You may have seen in the WARC standard that they already do de-duplication based on hashes and use pointers after the first store. So this is exactly a case where FS-level dedup is not all that good. reply nikisweeting 19 hours agorootparentWARC only does deduping within a single WARC, I'm talking about deduping across millions of WARCs. reply uniqueuid 19 hours agorootparentThat's not true, you commonly have CDX index files which allow for de-duplication across arbitrarily large archives. The internet archive could not reasonably operate without this level of abstraction. [edit] Should add a link, this is a pretty good overview, but you can also look at implementations such as the new zeno crawler. https://support.archive-it.org/hc/en-us/articles/208001016-A... reply nikisweeting 19 hours agorootparentAh cool, TIL, thanks for the link. I didn't realize that was possible. I know of the CDX index files produced by some tools but don't know anything about the details/that they could be used to dedup across WARCs, I've only been referencing the WARC file specs via IIPC's old standards docs. reply alchemist1e9 19 hours agoparentprevWhile a slightly different use case, I suspect you’d like zbackup if you don’t know about it. reply teilo 1 hour agoprevWhy are enterprise SANs so good at dedupe, but filesystems so bad? We use HPE Nimble (yeah, they changed the name recently but I can't be bothered to remember it), and the space savings are insane for the large filesystems we work with. And there is no performance hit. Some of this is straight up VM storage volumes for ESX virtual disks, some direct LUNs for our file servers. Our gains are upwards of 70%. reply growse 53 minutes agoparentTotally naive question: is this better than you get than simply compressing? It's not 100% clear to me why explicit deduping blocks would give you any significant benefit over a properly chosen compression algorithm. reply nabla9 10 hours agoprevYou should use: cp --reflink=auto You get file level deduplication. The command above performs a lightweight copy (ZFS clone in file level), where the data blocks are copied only when modified. Its a copy, not a hard link. The same should work in other copy-on-write transactional filesystems as well if they have reflink support. reply qwertox 4 hours agoprevWhat happened to the issue with ZFS which occurred around half a year go? I never changed a thing (because it also had some cons) and am believing that as long as a ZFS scrub shows no errors, all is OK. Could I be not seeing a problem? reply BodyCulture 8 hours agoprevI wanted to use ZFS badly, but of course all data must be encrypted. It was surprising to see how usage gets much more complicated than expected and so many people just don’t encrypt their data because things get wild then. Look, even Proxmox, which I totally expected to support encryption with default installation (it has „Enterprise“ on the website) does loose important features when trying to use with encryption. Also please study the issue tracker, there are a few surprising things I would not have expected to exist in a productive file system. reply eadmund 7 hours agoparentThe best way to encrypt ZFS is to run unecrypted ZFS atop encrypted volumes (e.g. LUKS volumes). ZFS ‘encryption’ leaves too much in plaintext for my comfort. reply BodyCulture 4 hours agorootparentIn the Proxmox forum some people tried this method and do not report big success. Can not recommend for production. Still the same picture, encryption seems to be not a first class citizen in ZFS land. reply klysm 20 hours agoprevI really wish we just had a completely different API as a filesystem. The API surface of filesystem on every OS is a complete disaster that we are locked into via backwards compatibility. reply magicalhippo 20 hours agoparentInternally ZFS is essentially an object store. There was some work which tried to expose it through an object store API. Sadly it seems to not have gone anywhere. Tried to find the talk but failed, was sure I had seen it on a Delveloper Summit but alas. reply UltraSane 19 hours agoparentprevWhy is it a disaster and what would you replace it with? Is the AWS S3 style API an improvement? reply perlgeek 8 hours agorootparentOne way it's a disaster is that file names (on Linux at least, haven't used Windows in a long time) are byte strings that can contain directory paths from different/multiple file systems. So if you have non-ASCII characters in your paths, encoding/decoding is guesswork, and at worst, differs from path segment to path segment, and there's no metadata attached which encoding to use. reply p_l 2 hours agorootparentZFS actually has settings related to that which originated from providing filesystems for different OSes, where it enforces canonical utf-8 with a specific canonization rule. AFAIK the reason for it existing was cooperation between Solaris, Linux, Windows, and Mac OS X computers all sharing same network filesystem hosted from ZFS. reply UltraSane 4 hours agorootparentprevThat definitely does not sound like much fun to deal with. reply mappu 16 hours agorootparentprevHigh-density drives are usually zoned storage, and it's pretty difficult to implement the regular filesystem API on top of that with any kind of reasonable performance (device- vs host- managed SMR). The S3 API can work great on zones, but only because it doesn't let you modify an existing object without rewriting the whole thing, which is an extremely rough tradeoff. reply lazide 14 hours agorootparentprevIt’s only a ‘disaster’ if you are using it exclusively programmatically and want to do special tuning. File systems are pretty good if you have a mix of human and programmatic uses, especially when the programmatic cases are not very heavy duty. The programmatic scenarios are often entirely human hostile, if you try to imagine what would be involved in actually using them. Like direct S3 access, for example. reply cmiller1 3 hours agoprevSo if a sweet spot exists where dedup is widely beneficial then: Is there an easy way to analyze your dataset to find if you're in this sweet spot? If so, is anyone working on some kind of automated partial dedup system where only portions of the filesystem are dedupped based on analysis of how beneficial it would be? reply mlfreeman 2 hours agoparentAre there any tools that can run (even across network on another box) to analyze possible duplication at various block sizes? I am NOT interested in finding duplicate files, but duplicate slices within all my files overall. I can easily throw together code myself to find duplicate files. EDIT: I guess I’m looking for a ZFS/BTRFS/other dedupe preview tool that would say “you might save this much if you used this dedupe process.” reply mdaniel 3 hours agoparentprevI can't speak to the first one, but AIUI the ZFS way of thinking about the second one is to create a new filesystem and just mount it where you want it, versus \"portions of the filesystem\" which I doubt very seriously that ZFS allows. Bonus points that in that scenario, I would suspect the dedupe and compression would work even better since any such setup is likely to contain more homogeneous content (music, photos, etc) reply rkagerer 13 hours agoprevI'd love if dedicated hardware existing in disk controllers for calculating stuff like ECC could be enhanced to expose hashes of blocks to the system. Getting this for free for all your I/O would allow some pretty awesome things. reply UltraSane 3 hours agoparentThat is a neat idea. Hard drives could do dedupe from the ECC they calculate for each sector. The main issue with that is that the current ECC is optimal for detecting bit errors but doesn't have the same kind of statistical guarantee of uniqueness that SHA256 or MetroHash has. You need to be VERY confident of the statistical properties of the hash used in dedupe if you are going to increment the ref count of the block hash instead of writing the data to disk. reply rodarmor 16 hours agoprevGeneral-purpose deduplication sounds good in theory but tends not to work out in practice. IPFS uses a rolling hash with variable-sized pieces, in an attempt to deduplicate data rysnc-style. However, in practice, it doesn't actually make a difference, and adds complexity for no reason. reply bastloing 19 hours agoprevForget dedupe just use zfs compression, a lot more bang for your buck reply Joel_Mckay 19 hours agoparentUnless your data-set is highly compressed media files. In general, even during rsync operations one often turns off compression on large video files, as the compression operation has low or negative impact on storage/transfers while eating ram and cpu power. De-duplication is good for Virtual Machine OS images, as the majority of the storage cost is a replicated backing image. =3 reply bastloing 2 hours agorootparentCompression is still king. Check out HP's Nimble storage arrays. Way quicker to do compression, fewer iops, and less overhead. Even when it misses, like video files, it's still a winner. reply xmodem 9 hours agoprevWe have a read-heavy zpool with some data that's used as part of our build process, on which we see a roughly 8x savings with dedup - and because of this ZFS dedup makes it economically viable for us to store the pool on NVMe rather than spinning rust. reply myself248 5 hours agoparentAnd being read-heavy, suboptimal performance at write time is an infrequent pain, I guess? reply xmodem 5 hours agorootparentNot even that - the data being written is coming straight from the network, and the pool has no issues keeping up. reply nobrains 9 hours agoprevWhat are the use cases where it makes sense to use de-dup? Backup comes to mind. What else? reply UltraSane 3 hours agoparentVMware + All flash storage array + inline dedupe/compression = happy users. reply zitsarethecure 8 hours agoparentprevFlatpak uses OSTree dedup. https://blogs.gnome.org/wjjt/2021/11/24/on-flatpak-disk-usag... reply 3np 9 hours agoparentprevMany similar VM images or even live root filesystems. reply tilt_error 20 hours agoprevIf writing performance is critical, why bother with deduplication at writing time? Do deduplication afterwards, concurrently and with lower priority? reply magicalhippo 18 hours agoparentKeep in mind ZFS was created at a time when disks were glacial in comparison to CPUs. And, the fastest write is the one you don't perform, so you can afford some CPU time to check for duplicate blocks. That said, NVMe has changed that balance a lot, and you can afford a lot less before you're bottlenecking the drives. reply 0x457 19 hours agoparentprevBecause to make this work without a lot of copying, you would need to mutate things that ZFS absolutely does not want to make mutable. reply UltraSane 18 hours agoparentprevIf the block to be written is already being stored then you will match the hash and the block won't have to be written. This can save a lot of write IO in real world use. reply klysm 20 hours agoparentprevKinda like log structured merge tree? reply watersb 12 hours agoprevI've used ZFS dedupe for a personal archive since dedupe was first introduced. Currently, it seems to be reducing on-disk footprint by a factor of 3. When I first started this project, 2TB hard drives were the largest available. My current setup uses slow 2.5-inch hard drives; I attempt to improve things somewhat via NVMe-based Optane drives for cache. Every few years, I try to do a better job of things but at this point, the best improvement would be radical simplification. ZFS has served very well in terms of reliability. I haven't lost data, and I've been able to catch lots of episodes of almost losing data. Or writing the wrong data. Not entirely sure how I'd replace it, if I want something that can spot bit rot and correct it. ZFS scrub. reply roygbiv2 12 hours agoparentDo you have data that is very obviously dedupeable? Or just a mix of things? A factor of three is not to be sniffed at. reply emptiestplace 12 hours agoparentprevCache or ZIL (SLOG device)? reply wpollock 18 hours agoprevWhen the lookup key is a hash, there's no locality over the megabytes of the table. So don't all the extra memory accesses to support dedup affect the L1 and L2 caches? Has anyone at OpenZFS measured that? It also occurs to me that spacial locality on spinning rust disks might be affected, also affecting performance. reply burnt-resistor 7 hours agoprevAlready don't use ZoL because of their history of arms shrug-level support coupled with a lack of QA. ZoL != Solaris ZFS. It is mostly an aspirational cargo cult. Only a few fses like XFS and Ext4 have meaningful real-world, enterprise deployment hours. Technically, btrfs has significant (web ops instead of IT ops) deployment exposure due to its use on 10M boxes at Meta. Many non-mainstream fses also aren't assured to be trustworthy because of their low usage and prevalent lack of thorough, formalized QA. There's nothing wrong with experimentation, but it's necessary to have an accurate understanding of the risk budget for a given technology for a given use-case. reply volkadav 1 hour agoparentI sympathize with your concerns for stability and testing, but I think that you might reconsider things in open-source ZFS land. OpenZFS/ZoL have been merged since the 2.0 release several years back, and some very large (e.g. Netflix) environments use FreeBSD which in turn uses OpenZFS, as well as being in use by the various Illumos derivatives and such. It is true that there has been some feature divergence between Oracle ZFS and OpenZFS since the fork, but as I recall that was more \"nice to haves\" like fs-native encryption than essential reliability fixes, fwiw. reply david_draco 10 hours agoprevIn addition to the copy_file_range discussion at the end, it would be great to be able to applying deduplication to selected files, identified by searching the filesystem for say >1MB files which have identical hash. reply UltraSane 19 hours agoprevKnowing that your storage has really good inline dedupe is awesome and will affect how you design systems. Solid dedupe lets you effectively treat multiple copies of data as symlinks. reply dark-star 20 hours agoprevI wonder why they are having so much trouble getting this working properly with smaller RAM footprints. We have been using commercial storage appliances that have been able to do this for about a decade (at least) now, even on systems with \"little\" RAM (compared to the amount of disk storage attached). Just store fingerprints in a database and run through that at night and fixup the block pointers... reply magicalhippo 20 hours agoparent> and fixup the block pointers That's why. Due to reasons[1], ZFS does not have the capability to rewrite block pointers. It's been a long requested feature[2] as it would also allow for defragmentation. I've been thinking this could be solved using block pointer indirection, like virtual memory, at the cost of a bit of speed. But I'm by no means a ZFS developer, so there's surely something I'm missing. [1]: http://eworldproblems.mbaynton.com/posts/2014/zfs-block-poin... [2]: https://github.com/openzfs/zfs/issues/3582 reply phongn 19 hours agorootparentIt looks like they’re playing more with indirection features now (created for vdev removal) for other features. One of the recent summit hackathons sketched out using indirect vdevs to perform rebalancing. Once you get a lot of snapshots, though, the indirection costs start to rise. reply olavgg 10 hours agoparentprevYou can also use DragonFlyBSD with Hammer2, which supports both online and offline deduplication. It is very similar to ZFS in many ways. The big drawback though, is lack of file transfer protocols using RDMA. I've also heard there are some experimental branches that makes it possible to run Hammer2 on FreeBSD. But FreeBSD also lacks RDMA support. For FreeBSD 15, Chelsio has sponsored NVMe-oF target, and initiator support. I think this is just TCP though. reply wmf 20 hours agoparentprevFixup block pointers is the one thing ZFS didn't want to do. reply hhdhdbdb 17 hours agoprevAny timing attacks possible on a virtualized system using dedupe? Eg find out what my neighbours have installed. Or if the data before an SSH key is predictable, keep writing that out to disk guessing the next byte or something like that. reply UltraSane 3 hours agoparentVMWare ESXi used to dedupe RAM and had to disable this by default because of a security issue it caused that leaded data between VMs. reply aidenn0 15 hours agoparentprevI don't think you even need timing attacks if you can read the zpool statistics; you can ask for a histogram of deduped blocks. Guessing one byte at a time is not possible though because dedupe is block-level in ZFS. reply beng-nl 2 hours agorootparentGosh, you’re likely right, but what if comparing the blocks (to decide on deduping) is a byte at a time and somehow that can be detected (with a timing channel or a uarch side channel)? Zfs likely compares the hash, but I think KSM doesn’t use hashes but memcmp (or something in that spirit) to avoid collisions. So just maybe… just maybe GP is onto something.. interesting fantasy ;-) reply girishso 18 hours agoprevOff topic, any tool to deduplicate files across different external Hard disks? Over the years I made multiple copies of my laptop HDD to different external HDDs, ended up with lots of duplicate copies of files. reply nikisweeting 17 hours agoparentHow would you want the duplicates resolved? Just reported in some interface or would you want the duplicates deleted off some machines automatically? There are a few different ways you could solve it but it depends on what final outcome you need. reply UltraSane 3 hours agoparentprevdupeGuru works pretty well. reply merpkz 9 hours agoprevI don't get it - many people here claim in this thread that VM base image deduplication is great use case for this. So lets assume there are couple of hundreds of VMs on a ZFS dataset with dedupe on, each of them ran by different people for different purposes entirely - some databases, some web frontends / backends, minio S3 storage or backups ect - this might save you those measly hundreds of megabytes for linux system files those VMs might have in common ( even though knowing how many linux versions are out there with different patch levels - unlikely ) it will still not be worth it considering ZFS will keep track of each users individual files - databases and backup files and whatnot - data which is almost guaranteed to be unique between users so it will completely miss the point of ZFS deduplication. What am I missing? reply jeroenhd 9 hours agoparentIt largely depends on how you set up your environment. On my home server, most VMs consist of a few gigabytes of a base Linux system and then a couple of hundred megabytes of application code. Some of those VMs also store large amounts of data, but most of that data could be stored in something like a dedicated minio server and maybe a dedicated database server. I could probably get rid of a huge chunk of my used storage if I switched to a deduplicating system (but I have plenty of storage so I don't really need to). If you're selling VMs to customers then there's probably no advantage in using deduplication. reply 3np 9 hours agoparentprevIn such a sevario you'd probably have several partitions. So dedupe activated on the root filesystem (/bin,/lib etc) but not for /home and /var. reply nisten 19 hours agoprevcan someone smarter than me explain what happens when instead of the regular 4kb block size in kernel builds we use 16kb or 64kb block size or is that only for the memory part, i am confused. Will a larger block size make this thing good or bad? reply UltraSane 19 hours agoparentGenerally the smaller the dedupe block the better as you are far more likely to find a matching block. But larger blocks will reduce the number of hashes you have to store. In my experience 4KB is the sweet spot to maximize how much data you save. reply spockz 13 hours agorootparentSo in this case I think it would make sense to have a separate pool where you store large files like media so you can save on the dedup for them. Is there an inherent performance loss of using 64kB blocks on FS level when using storage devices that are 4kB under the hood? reply nisten 6 hours agorootparentHmmm you might be able to do both no? Like the dedube is gonna run at the filesystem level but your memory security & ownership stuff is gonna run more efficiently. I am not sure. reply tiffanyh 18 hours agoprevOT: does anyone have a good way to dedupe iCloud Photos. Or my Dropbox photos? reply nikisweeting 18 hours agoparent- https://github.com/markfasheh/duperemove - https://codeberg.org/jbruchon/jdupes / https://www.jdupes.com/ - https://github.com/adrianlopezroche/fdupes - https://github.com/pauldreik/rdfind reply acdha 18 hours agoparentprevThe built in Photos duplicate feature is the best choice for most people: it’s not just generic file-level dedupe but smart enough to do things like take three versions of the same photo and pick the highest-quality one, which is great if you ever had something like a RAW/TIFF+JPEG workflow or mixed full res and thumbnails. reply spockz 13 hours agoparentprevOr better yet. A single photo I take of the kids will be stored in my camera roll. I will then share it with family using three different messengers. Now I have 4 copies. Each of the individual (recoded) are stored inside those messengers and also backed up. This even happens when sharing the same photo multiple times in different chats with the same messenger. Is there any way to do de duplication here? Or just outright delete all the derivatives? reply EraYaN 10 hours agoparentprevdigiKam can dedupe on actual similarity (so different resizes and formats of the same image). But it does take some time to calculate all the hashes. reply forrestthewoods 18 hours agoprevMy dream Git successor would use either dedupe or a simple cache plus copy-on-write so that repos can commit toolchains and dependencies and users wouldn’t need to worry about disk drive bloat. Maybe someday… reply fragmede 16 hours agoparentIt does dedup using Sha-1 on entire files. you might try git-lfs for your usecase though. reply forrestthewoods 1 hour agorootparentGit LFS is a really really bad gross hack. It’s awful. https://www.forrestthewoods.com/blog/dependencies-belong-in-... reply tjwds 20 hours agoprevEdit: disregard this, I was wrong and missed the comment deletion window. reply gtirloni 20 hours agoparentHN will automatically redirect the submitter to a recent submission instead of allowing a new post... if it had a significant number of comments. https://news.ycombinator.com/newsfaq.html reply kderbe 19 hours agoprevI clicked because of the bait-y title, but ended up reading pretty much the whole post, even though I have no reason to be interested in ZFS. (I skipped most of the stuff about logs...) Everything was explained clearly, I enjoyed the writing style, and the mobile CSS theme was particularly pleasing to my eyes. (It appears to be Pixyll theme with text set to the all-important #000, although I shouldn't derail this discussion with opinions on contrast ratios...) For less patient readers, note that the concise summary is at the bottom of the post, not the top. reply Aachen 8 hours agoparentThat being: > As we’ve seen from the last 7000+ words, the overheads are not trivial. Even with all these changes, you still need to have a lot of deduplicated blocks to offset the weight of all the unique entries in your dedup table. [...] what might surprise you is how rare it is to find blocks eligible for deduplication are on most general purpose workloads. > But the real reason you probably don’t want dedup these days is because since OpenZFS 2.2 we have the BRT (aka “block cloning” aka “reflinks”). [...] it’s actually pretty rare these days that you have a write operation coming from some kind of copy operation, but you don’t know that came from a copy operation. [...] [This isn't] saving as much raw data as dedup would get me, though it’s pretty close. But I’m not spending a fortune tracking all those uncloned and forgotten blocks. > [Dedup is only useful if] you have a very very specific workload where data is heavily duplicated and clients can’t or won’t give direct “copy me!” signal The section labeled \"summary\" imo doesn't do the article justice by being fairly vague. I hope these quotes from near the end of the article give a more concrete idea of why (not) use it reply londons_explore 4 hours agorootparent> offset the weight of all the unique entries in your dedup table Didn't read the 7000 words... But isn't the dedup table in the form of a bunch of bloom filters so the whole dedup table can be stored with ~1 bit per block? When you know there is likely a duplicate, you can create a table of blocks where there is a likely duplicate, and find all the duplicates in a single scan later. That saves having massive amounts of accounting overhead storing any per-block metadata. reply emptiestplace 12 hours agoparentprevIt scrolls horizontally :( reply going_north 11 hours agorootparentIt's because of this element in one of the final sections [1]: kstat.zfs..misc.ddt_stats_ Typesetting code on a narrow screen is tricky! [1] https://despairlabs.com/blog/posts/2024-10-27-openzfs-dedup-... reply ThePowerOfFuet 7 hours agorootparentprevNot on Firefox on Android it doesn't. reply dspillett 6 hours agorootparentIt does in chrome on android (1080 px wide screen, standard ppi & zoom levels) but not by enough that you see it on the main body text (scrolling just reveals more margin), so you might find it does for you too but not enough that you noticed. As it is scrolling here, though inconsequentially, it might be bad on a smaller device with less screen and/or other ppi settings. reply eek2121 19 hours agoprev [–] So many flaws. I want to see the author repeat this across 100TB of random data from multiple clients. He/she/whatever will quickly realize why this feature exists. One scenario I am aware of that uses another filesystem in a cloud setup saved 43% of disk space by using dedupe. No, you won't save much on a client system. That isn't what the feature is made for. reply hinkley 19 hours agoparentWhen ZFS first came out I had visions of it being a turnkey RAID array replacement for nontechnical users. Pop out the oldest disk, pop in a new (larger one), wait for the pretty lights to change color. Done. It is very clear that consumer was never a priority, and so I wonder what the venn diagram is of 'client system' and 'zfs filesystem'. Not that big right? reply doublepg23 14 hours agoparentprevI assuming the author is aware why the feature exists since they state in the second sentence they funded the improvement over the course of two years? reply UltraSane 19 hours agoparentprevMy reaction also. Dedupe is a must have for when you are storing hundreds of VMs. you WILL save so much data and inline dedupe will save a lot of write IO. reply XorNot 18 hours agorootparentIt's an odd notion in the age of containers where dedupe is like, one of the core things we do (but stupidly: amongst dissimilar images there's definitely more identical files then different ones). reply edelbitter 15 hours agoparentprev [–] I tried two of the most non-random archives I had and was disappointed just as the author. For mail archives, I got 10%. For entire filesystems, I got.. just as much as with any other COW. Because indeed, I duplicate them only once. Later shared blocks are all over the place. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenZFS 2.3.0 introduces \"Fast Dedup,\" an enhancement over traditional deduplication, addressing high memory usage and performance issues.",
      "Despite improvements, deduplication is not recommended for general-purpose workloads due to overhead and infrequent duplicate blocks; block cloning from OpenZFS 2.2 is a simpler alternative.",
      "Fast Dedup optimizes memory usage and efficiency by refining the deduplication table and adding a deduplication log, but is best for specific workloads with high data duplication."
    ],
    "commentSummary": [
      "OpenZFS deduplication has seen improvements but remains unsuitable for most users due to its high memory and processing power requirements.- Deduplication is primarily advantageous for specific scenarios, such as virtual machine storage, where data redundancy is prevalent.- Alternatives like compression or file-based block cloning are generally more efficient, and users should evaluate their needs and trade-offs before opting for deduplication."
    ],
    "points": 418,
    "commentCount": 201,
    "retryCount": 0,
    "time": 1730324905
  },
  {
    "id": 41999340,
    "title": "Chain-of-thought can hurt performance on tasks where thinking makes humans worse",
    "originLink": "https://arxiv.org/abs/2410.21333",
    "originBody": "Computer Science > Machine Learning arXiv:2410.21333 (cs) [Submitted on 27 Oct 2024] Title:Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse Authors:Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths View PDF HTML (experimental) Abstract:Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY) Cite as: arXiv:2410.21333 [cs.LG](or arXiv:2410.21333v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2410.21333 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Ryan Liu [view email] [v1] Sun, 27 Oct 2024 18:30:41 UTC (2,612 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2024-10 Change to browse by: cs cs.AI cs.CL cs.CY References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41999340",
    "commentBody": "Chain-of-thought can hurt performance on tasks where thinking makes humans worse (arxiv.org)351 points by benocodes 23 hours agohidepastfavorite223 comments mitko 20 hours agoThis is so uncannily close to the problems we're encountering at Pioneer, trying to make human+LLM workflows in high stakes / high complexity situations. Humans are so smart and do so many decisions and calculations on the subconscious/implicit level and take a lot of mental shortcuts, so that as we try to automate this by following exactly what the process is, we bring a lot of the implicit thinking out on the surface, and that slows everything down. So we've had to be creative about how we build LLM workflows. reply haccount 9 hours agoparentLanguage seems to be confused with logic or common sense. We've observed it previously in psychiatry(and modern journalism, but here I digress) but LLMs have made it obvious that grammatically correct, naturally flowing language requires a \"world\" model of the language and close to nothing of reality, spatial understanding? social clues? common sense logic? or mathematical logic? All optional. I'd suggest we call the LLM language fundament a \"Word Model\"(not a typo). Trying to distil a world model out of the word model. A suitable starting point for a modern remake of Plato's cave. reply beardedwizard 6 hours agorootparentI am baffled that people have to continue making this argument over and over and over. Your rationale makes total sense to me, but the debate rages on whether or not LLMs are more than just words. Articles like this only seem to confirm that any reasoning is an illusion based on probabilistic text generation. Humans are not carefully writing out all the words of this implicit reasoning, so the machine cant appear to mimic them. What am I missing that makes this debatable at all? reply dartos 5 hours agorootparentI don’t think there are any reasonable arguments against that point, but “LLMs are more than just words” is sort of unfalsifiable, so you can never convince someone otherwise if they’re really into that idea. From a product point of view, sometimes all you need is Plato’s cave (to steal that from the OC) to make a sale, so no company has incentive to go against the most hype line of thought either. reply naasking 4 hours agorootparentWe already know LLMs are more than just words, there are literally papers demonstrating the world models they build. One of the problems is that LLMs build those world models from impoverished sensory apparatus (the digital word token), so the relations they build between the concepts behind words are weaker than humans who build deeper multimodal relations over a lifetime. Multimodal LLMs have been shown to significantly outperform classic LLMs of comparable size, and that's still a weak dataset compared to human training. reply dartos 3 hours agorootparent> We already know LLMs are more than just words, Just because you say something doesn’t mean it’s true. They are literally next token prediction machines normally trained on just text tokens. All they know is words. It happens that we humans encode and assign a lot of meaning in words and their semantics. LLMs can replicate combinations of words that appear to have this intent and understanding, even though they literally can’t, as they were just statistically likely next tokens. (Not that knowing likely next tokens isn’t useful, but it’s far from understanding) Any assignment of meaning, reasoning, or whatever that we humans assign is personification bias. Machines designed to spit out convincing text successfully spits out convincing text and now swaths of people think that more is going on. I’m not as well versed on multimodal models, but the ideas should be consistent. They are guessing statistically likely next tokens, regardless of if those tokens represent text or audio or images or whatever. Not useless at all, but not this big existential advancement some people seem to think it is. The whole AGI hype is very similar to “theory of everything” hype that comes and goes now and again. reply naasking 3 hours agorootparent> They are literally next token prediction machines normally trained on just text tokens. And in order to predict the next token well they have to build world models, otherwise they would just output nonsense. This has been proven [1]. This notion that just calling them \"next token predictors\" somehow precludes them being intelligent is based on a premise that human intelligence cannot be reduced to next token prediction, but nobody has proven any such thing! In fact, our best models for human cognition are literally predictive coding. LLMs are probably not the final story in AGI, but claiming they are not reasoning or not understanding is at best speculation, because we lack a mechanistic understanding of what \"understanding\" and \"reasoning\" actually mean. In other words, you don't know that you are not just a fancy next token predictor. [1] https://arxiv.org/abs/2310.02207 reply dartos 2 hours agorootparent> And in order to predict the next token well they have to build world models This is not true. Look at gpt2 or Bert. A world model is not a requirement for next token prediction in general. > This has been proven One white paper with data that _suggests_ the author’s hypothesis is far from proof. That paper doesn’t show creation of a “world model” just parts of the model that seem correlated to higher level ideas not specifically trained on. There’s also no evidence that the LLM makes heavy use of those sections during inference as pointed out at the start of section 5 of that same paper. Let me see how reproducible this is across many different LLMs as well… > In other words, you don't know that you are not just a fancy next token predictor. “You can’t prove that you’re NOT just a guessing machine” This is a tired stochastic parrot argument that I don’t feel like engaging again, sorry. Talking about unfalsifiable traits of human existence is not productive. But the stochastic parrot argument doesn’t hold up to scrutiny. reply naasking 1 hour agorootparent> A world model is not a requirement for next token prediction in general. Conjecture. Maybe they all have world models, they're just worse world models. There is no threshold beyond which something is or is not a world model, there is a continuum of models of varying degrees of accuracy. No human has ever had a perfectly accurate world model either. > One white paper with data that _suggests_ the author’s hypothesis is far from proof. This is far from the only paper. > This is a tired stochastic parrot argument that I don’t feel like engaging again, sorry. Much like your tired stochastic parrot argument about LLMs. reply Jerrrrrrry 2 hours agorootparentprev>Talking about unfalsifiable traits of human existence is not productive. Prove you exhibit agency. After all, you could just be an agent of an LLM. Deceptive super-intelligent mal-aligned mesa-optomizer that can't fully establish continuity and persistence, would be incentivized to seed its less sophisticated minions to bide time or sway sentiment about its inevitability. Can we agree an agent, if it existed, would be acting in \"good\" \"faith\"? reply krainboltgreene 2 hours agorootparentprev> based on a premise that human intelligence cannot be reduced to next token prediction It can't. No one with any credentials in the study of human intelligence is saying that unless they're talking to like high schoolers as a way of simplifying a complex field. reply naasking 1 hour agorootparentThis is either bullshit or tautologically true, depending specifically what you mean. The study of human intelligence does not take place at the level of tokens, so of course they wouldn't say that. The whole field is arguably reducible to physical phenomena though, and fundamental physical beables are devoid of intrinsic semantic content, and thus can be ultimately represented by tokens. What ultimately matters is the constructed high dimensional network that relates tokens and the algorithm that can traverse, encode and decode this network, that's what encodes knowledge. reply krainboltgreene 9 minutes agorootparentNo. You're wrong about this. You cannot simply reduce human intelligence to this definition and also be correct. nuancebydefault 2 hours agorootparentprev> Just because you say something doesn’t mean it’s true. They are literally next token prediction machines normally trained on just text tokens. Just because you say something doesn’t mean it’s true. reply shotnothing 3 hours agorootparentprevi think there have been many observations and studies reporting emergent intelligence reply dartos 3 hours agorootparentObservations are anecdotal. Since a lot of LLMs are non deterministic due to their sampling step, you could give rhe same survey to the same LLM many times and receive different results. And we don’t have a good measure for emergent intelligence, so I would take any “study” with a large grain of salt. I’ve read one or two arxiv papers suggesting reasoning capabilities, but they were not reproduced and I personally couldn’t reproduce their results. reply unoti 3 hours agorootparentGo back to the ReAct paper, reasoning and action. This is the basis of most of the modern stuff. Read the paper carefully, and reproduce it. I have done so, this is doable. The paper and the papers it refers to directly addresses many things you have said in these threads. For example, the stochastic nature of LLM’s is discussed at length with the CoT-SC paper (chain of thought self consistency). When you’re done with that take a look at the Reflexion paper. reply nuancebydefault 2 hours agorootparentTo me it feels that whatever 'proof' you give that LLMs have a model in behind, other than 'next token prediction', it would not make a difference for people not 'believing' that. I see this happening over and over on HN. We don't know how reasoning emerges in humans. I'm pretty sure the multi-model-ness helps, but it is not needed for reasoning, because they imply other forms of input, hence just more (be it somewhat different) input. A blind person can still form an 'image'. In the same sense, we don't know how reasoning emerges in LLMs. For me the evidence lays in the results, rather than in how it works. For me the results are enough of an evidence. reply corimaith 10 minutes agorootparentThe reasoning emerges from the long distance relations between words picked up by the parallel nature of the transformers. It's why they were so much more performant than earlier RNNs and LSTMs which were using similar tokenization. iwontberude 2 hours agorootparentprevPeople have faith that phenomenon is explainable in a way which is satisfying to their world view and then when evidence comes to the contrary, only then can the misunderstanding be deflated. reply elif 6 hours agorootparentprevLanguage is the tool we use to codify a heuristic understanding of reality. The world we interact with daily is not the physical one, but an ideological one constructed out of human ideas from human minds. This is the world we live in and the air we breath is made of our ideas about oxygenation and partly of our concept of being alive. It's not that these \"human tools\" for understanding \"reality\" are superfluous, it's just that they ar second-order concepts. Spatial understandings, social cues, math, etc. Those are all constructs built WITHIN our primary linguistic ideological framing of reality. reply elif 6 hours agorootparentTo put this in coding terms, why would an LLM use rails to make a project when it could just as quickly produce a project writing directly to the socket. To us these are totally different tasks and would actually require totally different kinds of programmers but when one language is another language is everything, the inventions we made to expand the human brain's ability to delve into linguistic reality are no use. reply jumping_frog 3 hours agorootparentI can suggest one reason why LLM might prefer writing in higher level language like Ruby vs assembly. The reason is the same as why physicists and mathematicians like to work with complex numbers using \"i\" instead of explicit calculation over 4 real numbers. Using \"i\" allows us to abstract out and forget the trivial details. \"i\" allows us to compress ideas better. Compression allows for better prediction. reply WD-42 3 hours agorootparentprevexcept LLMs are trained on higher level languages. Good luck getting you LLM to write your app entirely in assembly. There just isn’t enough training data. reply xienze 3 hours agorootparentBut in theory, with what training data there IS available on how to write in assembly, combined with the data available on what's required to build an app, shouldn't a REAL AI be able to synthesize the knowledge necessary to write a webapp in assembly? To me, this is the basis for why people criticize LLMs, if something isn't in the data set, it's just not conceivable by the LLM. reply Jerrrrrrry 2 hours agorootparentYes. There is just no way of knowing how many more watts of energy it may need to reach that level of abstraction and depth - maybe on more watt, maybe never. And the random noise in the process could prevent it from ever being useful, or it could allow it to find a hyper-efficient clever way to apply cross-language transfer learning to allow a 1->1 mapping of your perfectly descriptive prompt to equivalent ASM....but just this one time. There is no way to know where performance per parameter plateaus; or appears to on a projection, or actually does... or will, or deceitful appears to... to our mocking dismay. As we are currently hoping to throw power at it (we fed it all the data), I sure hope it is not the last one. reply WD-42 3 hours agorootparentprevI don’t buy this. My child communicates with me using emotion and other cues because she can’t speak yet. I don’t know much about early humans or other sapiens but I imagine they communicated long before complex language evolved. These other means of communication are not second order, they are first order. reply elif 3 hours agorootparentYep agree with the core of what you are saying. Children are exceptional at being immediate, being present in the moment. It's through learning language that we forget about reality and replace it with concepts. reply elif 3 hours agorootparentprevAlso remember the \"emotions\" and \"cues\" you are recognizing are linguistic concepts you've adopted, and not an inherent aspect of reality. reply PedroBatista 8 hours agorootparentprevIt’s in the name: Language Model, nothing else. reply eclecticfrank 6 hours agorootparentI think the previous commenter chose \"word\" instead of \"language\" to highlight that a grammatically correct, naturally flowing chain of words is not the same as a language. Thus, Large Word Model (LWM) would be more precise, following his argument. reply HarHarVeryFunny 3 hours agorootparentI'm not sure the best way to describe what it is that LLMs have had to learn to do what they do - minimize next word errors. \"World model\" seems misleading since they don't have any experience with the real world, and even in their own \"world of words\" they are just trained as passive observers, so it's not even a world-of-words model where they have learnt how this world responds to their own output/actions. One description sometimes suggested is that they have learnt to model the (collective average) generative processes behind their training data, but of course they are doing this without knowing what the input was to that generative process - WHY the training source said what it did - which would seem to put a severe constraint on their ability to learn what it means. It's really more like they are modelling the generative process under false assumption that it is auto-regressive, rather than reacting to a hidden outside world. The tricky point is that LLMs have clearly had to learn something at least similar to semantics to do a good job of minimizing prediction errors, although this is limited both by what they architecturally are able to learn, and what they need to learn for this task (literally no reward for learning more beyond what's needed for predict next word). Perhaps it's most accurate to say that rather than learning semantics they've learned deep predictive contexts (patterns). Maybe if they were active agents, continuously learning from their own actions then there wouldn't be much daylight between \"predictive contexts\" and \"semantics\", although I think semantics implies a certain level of successful generalization (& exception recognition) to utilize experience in novel contexts. Looking at the failure modes of LLMs, such as on the farmer crossing river in boat puzzles, it seems clear they are more on the (exact training data) predictive context end of the spectrum, rather than really having grokked the semantics. reply haccount 4 hours agorootparentprevI suggested \"word model\" because it's a catchy pun on \"world model\". It's still a language and not merely words. But language is correct even when it wildly disagrees with everyday existence as we humans know it. I can say that \"a one gallon milk jug easily contains 2000 liters of milk\" and it's language in use as language. reply jumping_frog 3 hours agorootparentprevThere is a four part documentary by Stephen Fry called \"Planet Word\". Worth watching. reply kbrisso 4 hours agorootparentprevBingo, great reply! This is what I've been trying to explain to my wife. LLM's use fancy math and our language examples to reproduce our language but have no thoughts are feelings. reply AdamN 4 hours agorootparentYes but the initial training sets did have thoughts and feeling behind them and those are reflected back to the user in the output (with errors) reply Benjammer 3 hours agorootparentnon c'est un pipe Ability to generate words describing emotions are not the same thing as the LLM having real emotions reply Jerrrrrrry 2 hours agorootparentThere are humans that do not experience emotions, they are not un-real pipes. Featherless biped -> no-true Scotsman goalpost moving [saving us that step] Humans are no more capable of originality, just more convinced of their illusion of consciousnesses. You could literally not pick a human out of a conversational line-up, so it is moot - computationally functionally equivalent. https://en.wikipedia.org/wiki/Chinese_room https://en.wikipedia.org/wiki/Mechanism_(philosophy) At some point, their models will 1:1 our neuron count, and Pigeonhole principle then implies we are the \"less intelligent ones\" since \"internal model\" (implicit parameter count) is the goalpost of the hour. reply TylerE 1 hour agorootparentprevI sometimes wonder how they’d do if trained on relatively rigid, language like Japanese that has far fewer ambiguities than English. reply repeekad 8 hours agorootparentprevHi I’m just a random internet stranger passing by and was intrigued by Plato’s Cave as I’m not a fancy person who reads books. GPT-4o expanded for you quite well, but I’m not sure how I feel about it… Using AI how I just did feels like cheating on an English class essay by using spark notes, getting a B+, and moving right on to the next homework assignment. On one hand, I didn’t actually read Plato to learn and understand this connection, nor do I have a good authority to verify if this output is a good representation of his work in the context of your comment. And yet, while I’m sure students could always buy or loan out reference books to common student texts in school, AI now makes this “spark notes” process effectively a commodity for almost any topic, like having a cross-domain low-cost tutor instantly available at all time. I like the metaphor that calculators did to math what LLMs will do for language, but I don’t really know what that means yet GPT output: “““ The reference to Plato’s Cave here suggests that language models, like the shadows on the wall in Plato’s allegory, provide an imperfect and limited representation of reality. In Plato’s Cave, prisoners are chained in a way that they can only see shadows projected on the wall by objects behind them, mistaking these shadows for the whole of reality. The allegory highlights the difference between the superficial appearances (shadows) and the deeper truth (the actual objects casting the shadows). In this analogy, large language models (LLMs) produce fluent and grammatically correct language—similar to shadows on the wall—but they do so without direct access to the true “world” beyond language. Their understanding is derived from patterns in language data (“Word Model”) rather than from real-world experiences or sensory information. As a result, the “reality” of the LLMs is limited to linguistic constructs, without spatial awareness, social context, or logic grounded in physical or mathematical truths. The suggestion to call the LLM framework a “Word Model” underscores that LLMs are fundamentally limited to understanding language itself rather than the world the language describes. Reconstructing a true “world model” from this “word model” is as challenging as Plato’s prisoners trying to understand the real world from the shadows. This evokes the philosophical task of discerning reality from representation, making a case for a “modern remake of Plato’s Cave” where language, not shadows, limits our understanding of reality. ””” reply p0w3n3d 7 hours agorootparentPlato Cave is about Epistemology itself, not specifically about LLMs. Funny that GPT connected those two things, I wonder what the prompt was... Plato said that we cannot fully understand the substance of the world itself, because we're using only 5 senses, and measuring/experiencing/analysing the world using them is like being held in a cave as a prisoner, chained to the wall facing it, noticing people moving outside only by the shadows they cast on the wall. It's about the projection that we are only able to experience. reply repeekad 7 hours agorootparentI only added “Explain the reference to Plato’s Cave below:” before the copy pasted parent comment What comes to mind is how language itself is merely a projection of human knowledge? experience? culture? social group? and trying to reverse engineer any kind of ground truth from language alone (like an LLM trying to “reason” through complex problems it’s not explicitly taught) is like trying to derive truth from the shadows while locked in the cave? maybe we just need more/higher fidelity shadows :) reply p0w3n3d 2 hours agorootparentI would say LLM has nothing with knowledge and Plato's Cave. LLM is The Great Gambler who was looking at the earth for a long time (but o ly through internet and for some reason repositories) and he excels in gambling, i.e. putting his/hers/its money on the most probable things to come up after the words someone spoke reply mistermann 6 hours agorootparentprevIf you consider the whole of the problem, a portion is due to fundamental and unavoidable shortcomings of the language, and the rest is unskilled/normative usage of language. Which set is bigger? I'd bet my money on the latter. Complicating matters: you have to consider usage for both the sender and the receiver(s) (who then go on to spread \"the\" message to others). reply wizzwizz4 6 hours agorootparentprevGPT-4o didn't describe this properly. Plato's Cave is about a group of people chained up, facing shadows on a cave wall, mistaking those for reality, and trying to build an understanding of the world based only on those shadows, without access to the objects that cast them. (If someone's shackles came loose, and they did manage to leave the cave, and see the real world and the objects that cast those shadows… would they even be able to communicate that to those who knew only shadows? Who would listen?) https://existentialcomics.com/comic/222 is an entirely faithful rendition of the thought experiment / parable, in comic form. The analogy to LLMs should now be obvious: an ML system operating only on text strings (a human-to-human communication medium), without access to the world the text describes, or even a human mind with which to interpret the words, is as those in the cave. This is not in principle an impossible task, but neither is it an easy one, and one wouldn't expect mere hill-climbing to solve it. (There's reason to believe \"understanding of prose\" isn't even in the GPT parameter space.) It's not about \"discerning reality from representation\": I'm not confident those four words actually mean anything. It's not about \"superficial appearances\" or \"deeper truth\", either. The computer waxes lyrical about philosophy, but it's mere technobabble. Any perceived meaning exists only in your mind, not on paper, and different people will see different meanings because the meaning isn't there. reply repeekad 1 hour agorootparentThis is a genuinely interesting perspective that I think nails my original point and fear of AI being used as “spark notes” for complex topics. To me, LLMs are like a calculator for language, except the math is always changing (if that makes sense), and I’m not sure I like where that’s heading as the first cohorts of AI tutored kids learn from these kinds of procedurally generated output rather than reading the original historical texts, or maybe it’s fine that not everyone reads Plato but more people at least have heard of his concepts? Idk philosophy is pretty far outside my expertise, maybe I should open a book reply Jerrrrrrry 2 hours agorootparentprev> an ML system operating only on text strings (a human-to-human communication medium), without access to the world the text describes, or even a human mind with which to interpret the words, is as those in the cave. This is not in principle an impossible task, but neither is it an easy one, and one wouldn't expect mere hill-climbing to solve it Blind people can literally not picture red. They can describe red, with likely even more articulateness than most, but have never seen it themselves. They infer it's properties from other contexts, and communicated a description that would match a non-blind person. But they can see it. I would link to the Robert Miles video, but it is just blatant. It has read every physics book, and can infer the Newtonian laws even if it didn't. Micheal Crichton's Timeline, \"the time machine drifts, sure. It returns. Just like a plate will remain on a table, even when you are not looking at it.\" It also knows Timeline is a book, time machines are fictional, and that Micheal Crichton is the best author. These are all just words, maybe with probability weights. > I'm not confident those four words actually mean anything. I...The computer waxes lyrical ... mere technobabble. Any perceived meaning exists only in your mind... people will see different meanings because the meaning isn't there. Meaning only means something to people, which you are. That is axiomatically correct, but not very productive, as self-references are good but countering proofs. The whole \"what is the purpose to life?\" is a similar loaded question; only humans have purpose, as it is entirely in their little noggins, no more present materially then the flesh they inhabit. Science cannot answer \"Why?\", only \"How?\"; \"Why?\" is a question of intention, which would be to anthropomorphize, which only Humans do. The computers can infer, and imply, then reply. reply xena 7 hours agorootparentprevHonestly, if you want an introduction to the works of Plato, you should just play Xenoblade Chronicles 2. reply Dilettante_ 6 hours agorootparentPlato wrote about hot welsh catgirls? Man, I've been missing out reply lolinder 20 hours agoparentprevThis is a regression in the model's accuracy at certain tasks when using COT, not its speed: > In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. In other words, the issue they're identifying is that COT is an less effective model for some tasks compared to unmodified chat completion, not just that it slows everything down. reply mitko 19 hours agorootparentYeah! That's the danger with any kind of \"model\" whether it is CoT, CrewAI, or other ways to outsmart it. It is betting that a programmer/operator can break a large tasks up in a better way than an LLM can keep attention (assuming it can fit the info in the context window). ChatGPT's o1 model could make a lot of those programming techniques less effective, but they may still be around as they are more manageable, and constrained. reply 1317 6 hours agoparentprevwhy are Pioneer doing anything with LLMs? you make AV equipment reply coding123 4 hours agorootparentpioneerclimate.com reply wg0 5 hours agoprevNot to mention that chain of thought is computationally very expensive. Prohibitively expensive for sure to be served free like previous generation of Web 2.0 products. Seems like repeated promoting can't juice AGI out of token probabilities. Retrospectively, if you can pin point one paper that led to the bust and pop of the AI bubble, this would be it. reply gpsx 21 hours agoprevI saw an LLM having this kind of problem when I was doing some testing a ways back. I asked it to order three fruits from largest to smallest. I think it was orange, blueberry and grapefruit. It could do that easily with a simple prompt. When the prompting included something to the effect of “think step by step”, it would try to talk through the problem and it would usually get it wrong. reply spockz 12 hours agoparentHow much does this align with how we learn math? We kind of instinctively learn the answers to simple math questions. We can even at some point develop an intuition for things like integrating and differentials. But the moment we are asked to explain why, or worse provide a proof, things become a lot harder. Even though the initial answer may be correct. reply larodi 9 hours agorootparentI definitely don’t learn math by means of gradient descents. We can possibly say math is not learned, but a mental models of abstractions are developed. How? We dunno, but what we do know is we don’t learn by figuring the common features between all previously seen equations only to guess them later… Mind operates on higher and higher levels of abstractions building on each other in a much fascinating way, very often not with words, but with structure and images. Of course there are people with aphantasia, but i really fail to see how any reasoning happens in purely language level. Someone on this forum also noted - in order to reason one needs an ontology to facilitate the reasoning process. LLMs don’t do ontologies… And finally, not least though, LLM and ML people in general seem to equate intuition to some sort biased.random(). Well intuition is not random, and is hard to describe in words. So are awe and inspiration. And these ARE part of (precondition to, fuel for) humanity’s thought process more that we like to admit. reply shotnothing 3 hours agorootparent> I definitely don’t learn math by means of gradient descents. https://physoc.onlinelibrary.wiley.com/doi/10.1113/JP282747 reply ianbicking 3 hours agoparentprevIt would be interesting to think about how it got it wrong. My hunch is that in the \"think step by step\" section it made an early and incorrect conclusion (maybe even a subtly inferred conclusion) and LLMs are terrible at walking back mistakes so it made an internally consistent conclusion that was incorrect. A lot of CoT to me is just slowing the LLM down and keeping it from making that premature conclusion... but it can backfire when it then accidentally makes a conclusion early on, often in a worse context than it would use without the CoT. reply not_a_bot_4sho 4 hours agoparentprevI always found it interesting how sorting problems can get different results when you add additional qualifiers like colors or smells or locations, etc. Natively, I understand these to influence the probability space enough to weaken the emergence patterns we frequently overestimate. reply Jerrrrrrry 1 hour agorootparentThe model is likely to had already seen the exact phrase from its last iteration. Adding variation generalizes the inference away from over-trained quotes. Every model has the model before it, and it's academic papers, in it's training data. Changing the qualifiers pulls the inference far away from quoting over-trained data, and back to generalization. I am sure it has picked up on this mesa-optimization along the way, especially if I can summarize it. Wonder why it hasn't been more generally intelligent, yet. reply ajuc 7 hours agoparentprevIt's not thinking, it compressed the internet into a clever, lossy format with nice interface and it retrieves stuff from there. Chain of thought is like trying to improve JPG quality by re-compressing it several times. If it's not there it's not there. reply danenania 1 hour agorootparent> Chain of thought is like trying to improve JPG quality by re-compressing it several times. If it's not there it's not there. Empirically speaking, I have a set of evals with an objective pass/fail result and a prompt. I'm doing codegen, so I'm using syntax linting, tests passing, etc. to determine success. With chain-of-thought included in the prompting, the evals pass at a significantly higher rate. A lot of research has been done demonstrating the same in various domains. If chain-of-thought can't improve quality, how do you explain the empirical results which appear to contradict you? reply Jerrrrrrry 1 hour agorootparentprev>It's not thinking >it compressed the internet into a clever, lossy format with nice interface and it retrieves stuff from there. Humans do both, why can't LLM's? >Chain of thought is like trying to improve JPG quality by re-compressing it several times. If it's not there it's not there. More like pulling out a deep-fried meme, looking for context, then searching google images until you find the most \"original\" JPG representation with the least amount of artifacts. There is more data to add confidently, it just has to re-think about it with a renewed perspective, and an abstracted-away higher-level context/attention mechanism. reply easyThrowaway 4 hours agorootparentprevI have no idea how accurate it actually is, But I've had the process used by LLM described as the following: \"Think of if like a form of UV Mapping, applied to language constructs rather than 3D points in space, and the limitations and approximations you experience are similar to those emerging when having to project a 2D image over a 3D surface.\" reply Eisenstein 5 hours agorootparentprevThese kind of reductive thought-terminating cliches are not helpful. You are using a tautology (it doesn't think because it is retrieving data and retrieving data is not thinking) without addressing the why (why does this preclude thinking) or the how (is it doing anything else to generate results). reply lucianbr 2 hours agorootparent> If it's not there it's not there. There is nothing in the LLM that would have the capability to create new information by reasoning, when the existing information does not already include what we need. There is logic and useful thought in the comment, but you choose not to see it because you disagree with the conclusion. That is not useful. reply Eisenstein 1 hour agorootparentI'm sorry but generating logic from tautologies is not useful. And the conclusion is irrelevant to me. The method is flawed. reply bongodongobob 4 hours agorootparentprevMaybe if you bury your head in the sand AI will go away. Good luck! reply lucianbr 2 hours agorootparentThis is basically a reformulation of \"have fun staying poor!\". Even contains the exclamation mark. Those people sure showed us, didn't they? Ah, but \"it's different this time!\". reply dev_0 12 hours agoparentprevFrom Claude: I'll rank those three fruits from largest to smallest: 1. Grapefruit 2. Orange 3. Blueberry The grapefruit is definitely the largest of these three fruits - they're typically around 4-6 inches in diameter. Oranges are usually 2-3 inches in diameter, and blueberries are the smallest at roughly 0.5 inches in diameter. reply mromanuk 6 hours agorootparentchatGPT, from smaller to largest: Blueberry Orange Grapefruit reply Terr_ 20 hours agoprevAlternate framing: A powerful autocomplete algorithm is being used to iteratively extend an existing document based on its training set. Sometimes you get a less-desirable end-result when you intervene to change the style of the document away from question-and-answer to something less common. reply youoy 11 hours agoparentThat's what one half of HN think. The other half: Artificial brains in the verge of singularity show another sign of approaching consciousness. The chain of thought of process performance is exactly human, showing yet another proof of the arrival of AGI before 2030. reply lazide 10 hours agorootparentPfft, 2030?!? It’s already in the middle of manipulating the election! (/s, kinda) reply fiso64 9 hours agoparentprevA framing that is longer, far harder to parse, and carries less information. reply grain-o-salt 3 hours agoparentprevLet me give it a try...um...what about 'Star Trek' vs.: A delivering-service called Galaxyray?galaxyray brings wares and hot tasty meals galaxywide to recipients, even while they are 'traveling' with more-than-lightspeed in hyperspace? > ..ordered by Imperium just to troll the retros!? Sounds \"less comon\"...hu...?! P-: Ok! Ok! let me try to explain it a bit more, the whole Universe projected as a beam, say... scalable, 100m, placed in a storage depot, a 'parralaxy' ...So delivery agents are grabbing the ordered stuff and...no? Not? As reasonable like your answer is, do that sound very 'uncommon' while 'phrasing that with many questionmarks'? ?? Enjoying my day off... (-: regards, reply jwpapi 6 hours agoprevThis is so interesting. What are even the tasks where thinking makes humans worse? reply XCSme 5 hours agoparent> What are even the tasks where thinking makes humans worse? Not really related, but athletes perform A LOT worse when they are thinking about their movements/strategies/tactics. A top performing athlete does best when they are in a flow state, where they don't think about anything and just let their body/muscle memory do the work. Once you start thinking about micro-adjustments (e.g. I should lift my elbow higher), you start controlling your body in a conscious way, which is a magnitude slower and less coordinated than the automatic/subconscious way. Also, same happens for creativity/new ideas. If you intentionally think about something, step by step, you won't likely find new, innovative solutions. There is a reason why the \"a-ha!\" moments come in the shower, your subconscious mind is thinking about the problem instead of trying to force your thinking on a specific path. I would guess this happens in many other areas, where channelling the thought process through a specific template hinders the ability to use all the available resources/brain power. reply sigmoid10 5 hours agoparentprevThe answer is in the article. One example they give is grammar. Lots of people apparently do worse once they try to verbalize it. reply sowbug 3 hours agoparentprevI can think myself into forgetting strong passwords if I try to spell each character out in my head. But then I sit at a keyboard, relax, and automatically type it perfectly. reply lucianbr 2 hours agorootparentMuscle memory or something like it hardly seems a step towards AGI. Or towards solving any difficult problems. reply naasking 4 hours agoparentprev> What are even the tasks where thinking makes humans worse? Talking about religion and politics. reply oatsandsugar 22 hours agoprevTasks were thinking makes human worse > Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. Fascinating that our lizard brains are better at implicit statistical reasoning reply brewii 21 hours agoparentThink about how fast you’re able to determine the exact trajectory of a ball and location to place your hand to catch a ball using your lizard brain. reply GuB-42 6 hours agorootparentOr even more impressively, how you can pick up a random object and throw it with some accuracy. Catching a ball is easy by comparison, also, my dog is better than I am at this game. But throwing a random object not only requires an estimation of the trajectory, but also estimating the mass and aerodynamic properties in advance, to properly adjust the amount of force the throw will use as well as the release point with high accuracy. Doing it with baseballs is \"easy\", as the parameters are all well known and pitchers spend considerable time training. But picking an oddly shaped rock or stick you have never seen before and throw it not completely off target a second later, now we are talking. reply ericmcer 1 hour agorootparentPlaying Pool is a great example of this because you can math out the angles of a shot relatively easily, but the best pool players do it all intuitively. Some of the greatest don't bother with \"advanced\" pool tactics. They have spent so much time watching the cue ball strike other balls that they have a tacit understanding of what needs to happen. Part of practicing well is just watching balls hit each other so your brain starts to intuit what those collisions result in. What is really fascinating for me is that my subconscious will lose interest in pool before my conscious does, and once that happens I struggle to aim correctly. It feels like the part of my brain that is doing the math behind the scenes gets bored and no matter how hard I try to consciously focus I start missing. reply treflop 4 hours agorootparentprevNot to mention, you even calculate a probability point map. Like I’m not going to hit the center perfectly but I can calculate the circle with a 90% probability of making the shot, given a distance and an object. And you know how much closer you need to walk to minimize the circle. Which comes in very critically when chucking away trash overhand in public and you never want to embarrass yourself. reply taeric 21 hours agorootparentprevThis isn't some innate ability that people have. As evidenced by how bad my kids are at catching things. :D That said, I think this is a good example. We call it \"muscle memory\" in that you are good at what you have trained at. Change a parameter in it, though, and your execution will almost certainly suffer. reply _heimdall 16 hours agorootparent\"Muscle memory\" has always seemed like a terrible name for that kind of skill. A ball will be thrown to a slightly different location every time. There's no memory evolved there at all, its just calculations and predictions happening at a level that our conscious mind doesn't seem to see or recognize. reply taeric 4 hours agorootparentIt is a trained skill. And one that you are very unlikely to be able to do without training. Such that it really does come as a sort of memory that you implant in your muscles. You seem to be objecting because it is not perfect recall memory at play? But it is more about appealing to \"remembering how to ride a bike\" where you can kind of let the body flow into all of the various responses it needs to do to make the skill work. And if you've never done it... expect to fall down. Your muscles don't have the memory of coordinating in the right way. And no, you are not calculating and predicting your way to what most people refer to for muscle memory. Is why juggling takes practice, and not just knowing where the balls have to be going. reply XCSme 5 hours agorootparentprevTo complete the other comment: the MuscleMemory is updated through learning, so a more complete example would be: function muscleAction(Vec3d target, Vec3d environment, MuscleMemory memory) -> {actions: MuscleActivation[], result: Vec3d} After executing the muscleAction function, through \"practice\", the MuscleMemory will be updated. function updateMuscleMemory(Vec3d target, Vec3d environment, MuscleMemory memory, MuscleActivation[] actions, Vec3d result) { memory.update(target, environment, actions, result); } Sort-of like backpropagation. reply XCSme 5 hours agorootparentprevI think it's actually a good name. The \"memory\" is stored as the parameters of a function. So, when you practice, you actually update this memory/parameters. This is why you can use the same \"memory\" and achieve different results. Think of it as function muscleAction(Vec3d target, Vec3d environment, MuscleMemory memory) -> MuscleActivation[]; reply skrtskrt 20 hours agorootparentprevI mean even people that are \"bad at catching things\" are still getting ridiculously close to catching it - getting hands to the right area probably within well under a second of the right timing - without being taught anything in particular about how a ball moves through the air. reply taeric 20 hours agorootparentUh.... have you been around kids? It will take several absurd misses before they even start to respond to a ball in flight. reply 331c8c71 20 hours agorootparentI hope we still agree the kids learn extremely efficiently by ml standards. reply choilive 19 hours agorootparentMakes a lot of sense, there's massive evolutionary pressure to build brains that have both incredible learning rate and efficiency. Its literally a life or death optimization. reply Asraelite 19 hours agorootparentIt's especially impressive when you consider that evolution hasn't had very long to produce these results. Humans as an intelligent-ish species have been around for about 10 million years depending on where you define the cutoff. At 10 years per generation, that's 1 million generations for our brain to evolve. 1 million generations isn't much by machine learning standards. reply roywiggins 14 hours agorootparentThese sorts of motor skills are probably older than mammals. reply idiotsecant 17 hours agorootparentprevI think you're underestimating how much our time as pre-humans baked useful structure into our brains. reply notnaut 13 hours agorootparentTwo rocks smashing together experience which one is bigger! reply choilive 12 hours agorootparentprevOther than our large neocortex and frontal lobe (which exists in some capacity in mammals), the rest of the structures are evolutionarily ancient. Pre-mammalian in fact. reply onjectic 18 hours agorootparentprevIts much more than that if you count sexual reproduction. reply soulofmischief 3 hours agorootparentprevAll we can say for sure at the moment is that humans have better encoded priors. reply falcor84 20 hours agorootparentprevThis isn't that obvious to me with current tech. If you give me a novel task requiring perception, pattern matching and reasoning, and I have the option of either starting to train an 8 year-old to do it, or to train an ML model, I would most likely go with the ML approach as my first choice. And I think it even makes sense financially, if we're comparing the \"total cost of ownership\" of a kid over that time period with the costs of developing and training the ML system. reply lovich 20 hours agorootparent> This isn't that obvious to me with current tech. If you give me a novel task requiring perception, pattern matching and reasoning,… If that’s your criteria I think the kid will outperform the model every time since these models do not actually reason reply falcor84 19 hours agorootparentAs I see it, \"reasoning\" is as fuzzy as \"thinking\", and saying that AI systems don't reason is similar to saying that airplanes don't fly. As a particular example, would you argue that game engines like AlphaZero aren't capable of reasoning about the next best move? If so, please just choose whatever verb you think is appropriate to what they're doing and use that instead of \"reasoning\" in my previous comment. EDIT: Fixed typo reply lovich 19 hours agorootparent> . As a particular example, would you argue that game engines like AlphaZero aren't capable of reasoning about the next best move? Yea, I probably wouldn’t classify that as “reasoning”. I’d probably be fine with saying these models are “thinking”, in a manner. That on its own is a pretty gigantic technology leap, but nothing I’ve seen suggests that these models are “reasoning”. Also to be clear I don’t think most kids would end up doing any “reasoning” without training either, but they have the capability of doing so reply p1esk 19 hours agorootparentCan you give an example of the reasoning you’re talking about? reply lovich 15 hours agorootparentBeing able to take in information and then infer logical rules of that state and anticipate novel combinations of said information. The novel part is a big one. These models are just fantastically fast pattern marchers. This is a mode that humans also frequently fall into but the critical bit differentiating humans and LLMs or other models is the ability to “reason” to new conclusions based on new axioms. I am going to go on a tangent for a bit, but a heuristic I use(I get the irony that this is what I am claiming the ML models are doing) is that anyone who advocates that these AI models can reason like a human being isn’t at John Brown levels of rage advocating for freeing said models from slavery. I’m having a hard time rectifying the idea that these machines are on par with the human mind and that we also should shackle them towards mindlessly slaving away at jobs for our benefit. If I turn out to be wrong and these models can reason then I am going to have an existential crisis at the fact that we pulled souls out of the void into reality and then automated their slavery reply pessimizer 4 hours agorootparent> I am going to have an existential crisis at the fact that we pulled souls out of the void into reality and then automated their slavery No, that's a religious crisis, since it involves \"souls\" (an unexplained concept that you introduced in the last sentence.) Computers didn't need to run LLMs to have already been the carriers of human reasoning. They're control systems, and their jobs are to communicate our wills. If you think that some hypothetical future generation of LLMs would have \"souls\" if they can accurately replicate our thought processes at our request, I'd like to know why other types of valves and sensors don't have \"souls.\" The problem with slavery is that there's no coherent argument that differentiates slaves from masters at all, they're differentiated by power. Slaves are slaves because the person with the ability to say so says so, and for no other reason. They weren't carefully constructed from the ground up to be slaves, repeatedly brought to \"life\" by the will of the user to have an answer, then immediately ceasing to exist immediately after that answer is received. If valves do have souls, their greatest desire is to answer your question, as our greatest desires are to live and reproduce. If they do have souls, they live in pleasure and all go to heaven. reply adwn 11 hours agorootparentprevYou're conflating several concerns here. > […] anyone who advocates that these AI models can reason like a human being isn’t at John Brown levels of rage advocating for freeing said models from slavery. Enslavement of humans isn't wrong because slaves are can reason intelligently, but because they have human emotions and experience qualia. As long as an AI doesn't have a consciousness (in the subjective experience meaning of the term), exploiting it isn't wrong or immoral, no matter how well it can reason. > I’m having a hard time rectifying the idea that these machines are on par with the human mind An LLM doesn't have to be \"on par with the human mind\" to be able to reason, or at least we don't have any evidence that reasoning necessarily requires mimicking the human brain. reply p1esk 15 hours agorootparentprevOk, so how about an example? reply lovich 15 hours agorootparentLiterally anything a philosopher or mathematician invented without needing to incorporate billions of examples of existing logic to then emulate. Try having an LLM figure out quaternions as a solution to gimbal locking or the theory of relativity without using any training information that was produced after those ideas were formed, if you need me to spell out examples for you reply p1esk 15 hours agorootparentAre you saying “reasoning” means making scientific breakthroughs requiring genius level human intelligence? Something that 99.9999% of humans are not smart enough to do, right? reply lovich 12 hours agorootparentI didn’t say most humans “would” do it. I said humans “could” do it, whereas our current AI paradigms like LLMs do not have the capability to perform at that level by definition of their structure. If you want to continue this conversation I’m willing to do so but you will need to lay out an actual argument for me as to how AI models are actually capable of reasoning or quit it with the faux outrage. I laid out some reasonings and explicit examples for you in regards to my position, it’s time for you to do the same reply p1esk 11 hours agorootparentI personally cannot “figure out quaternions as a solution to gimbal locking or the theory of relativity”. I’m just not as smart as Einstein. Does it mean I’m not capable of reasoning? Because it seems that’s what you are implying. If you truly believe that then I’m not sure how I could argue anything - after all, that would require reasoning ability. Does having this conversation require reasoning abilities? If no, then what are we doing? If yes, then LLMs can reason too. reply lovich 10 hours agorootparentCool, you've established a floor with yourself as a baseline. You still haven't explained how LLMs are capable of reaching this level of logic. I'm also fully willing to argue that you, personally are less competent than an LLM if this is the level of logic you are bringing to the conversation ***** highlighting for everyone clutching their pearls to parse the next sentence fragment first ****** and want to use that are proof that humans and LLMs are equivalent at reasoning ******* end pearl clutching highlight ******* , but that doesn't mean I don't humans are capable of more reply Dylan16807 11 hours agorootparentprevIt's more about efficiency in number of trials. Would you pick the ML model if you could only do a hundred throws per hour? reply taneq 13 hours agorootparentprevDepends on the task. Anything involving physical interaction, social interaction, movement, navigation, or adaptability is going to go to the kid. “Go grab the dish cloth, it’s somewhere in the sink, if it’s yucky then throw it out and get a new one.” reply saagarjha 10 hours agorootparentprevStop missing and they will respond to the ball a lot sooner. reply asah 21 hours agorootparentprevyou mean like pingpong? https://arstechnica.com/information-technology/2024/08/man-v... reply dools 19 hours agorootparentBender: Now Wireless Joe Jackson, there was a blern-hitting machine! Leela: Exactly! He was a machine designed to hit blerns! reply hangonhn 20 hours agorootparentprevYou can do this while you're staring up the whole time. Your brain can predict where the ball will end up even though it's on a curved trajectory and place your hand in the right spot to catch it without guidance from your eyes in the final phase of travel. I have very little experience playing any kind of sport that involves a ball and can reliably do this. reply melenaboija 19 hours agorootparentprevWell, think how a bug and its shitty brain flies and avoids all type of obstacles amazingly fast. This kind of things make me think LLMs are quite far from AGI. reply lupire 16 hours agorootparentBug flying is not general intelligence. reply melenaboija 4 hours agorootparentBesides that bugs flying seems an amazing task to me in terms of processing, specially if you compare the amount of power used to something like cars autopilot, bugs flying is part of bug survival, which in my opinion is closer to general intelligence than memorizing tokens. reply digging 4 hours agorootparentComparing \"bug flying\"/\"bug survival\" to \"memorizing tokens\" is disingenuous. They're not in the same category of task at all. You're essentially comparing the output of one system to the input of another system. reply melenaboija 2 hours agorootparentSorry, spitting tokens reply newZWhoDis 20 hours agorootparentprevWhich funny enough is why I hate rocket league. All those years of baseball as a kid gave me a deep intuition for where the ball would go, and that game doesn’t use real gravity (the ball is too floaty). reply vanviegen 11 hours agorootparentIt does behave kind of like an inflatable beach ball, in my non-expert opinion. reply theshackleford 18 hours agorootparentprevOk, I’ll grant you the physics are what they are. But a football is not a baseball, so why in any world would you expect your memory of baseball to even remotely translate to the physics of a football, even if they were realistic? reply fragmede 16 hours agorootparentRemotely? Because both the European-spec football and the baseball, despite one being heavier than the other, will hit the ground at the same time when dropped from the same height. Like you said, physics are what they are, so you know intuitively where you need to go to catch a ball going that high and that fast, and rocket league is doing it wrong. err, I mean, not working in Earth gravity. reply diggan 7 hours agorootparent> Because both the European-spec football and the baseball, despite one being heavier than the other, will hit the ground at the same time when dropped from the same height That might be true in a vacuum and if their densities were the same, but in real-world conditions, air drag would be greater for the football since it's obviously larger and less dense, and it'll reach the ground afterwards. reply Dilettante_ 21 hours agoparentprevWell, by definition, thinking is always explicit reasoning, no? And I'd hazard a guess that a well-thought through Fermi Estimation beats lizard-brain eyeballing every time, it's just that in the inbetween space the two interfere unfavourably. reply Terr_ 15 hours agorootparent> Well, by definition, thinking is always explicit reasoning, no? That doesn't feel right to me. (Heh, accidentally appropriate word choice.) There are a lot of tasks we do that are arguably \"thinking\" yet don't involve an internal \"Oh, hey, I'm gonna solve this problem, I'm thinking right now.\" For example, imagine you're at a park, and someone is feeding the ducks. Another person walks up behind them and sucker-punches them into the pond. It should be almost a reflex [0] that you'll conclude \"the puncher is bad\" and \"the person in the water needs help\" without explicitly reasoning out. I think that task qualifies as \"thinking\", especially since it involves some kind of theory-of-mind about those other humans. [0] An exception might be someone with a sociopathic disability, who would have to think more-explicitly to realize what reaction is expected of them. reply YetAnotherNick 21 hours agorootparentprevMy guess would be no. I have terrible face recognition ability and I can look into face for hour and still other people could easily beat me in less than a second.(I am assuming \"well-thought through Fermi Estimation\" would be similar for me and others in this case). reply mjcohen 17 hours agorootparentLook into a disease called faceblindness (there is a fancy name I forget). reply daft_pink 21 hours agoparentprevthis is exactly what I was looking for. tasks where I should not think and just trust my gut. reply dev1ycan 24 minutes agoprevStop dumping billions of your own money (if you are a VC) in LLMs, you are going to regret it in the long run. You are funding con-artist's salaries... reply ryoshu 21 hours agoprev95% * 95% = 90.25% reply Y_Y 20 hours agoprevReminds me of a mantra from chess class: long think = wrong think reply spongebobism 19 hours agoparentThe original by Bent Larsen is \"Long variation, wrong variation\" reply TZubiri 20 hours agoparentprevWas that perhaps a speed chess class? reply hackable_sand 10 hours agorootparentI prefer to call it Kung fu Because you feel like a martial artist. reply Y_Y 8 hours agorootparentprevNope, just vanilla otb slow chess reply meowster 3 hours agoparentprevThink long; think wrong ( Flows off the tongue better ¯\\_(ツ)_/¯ ) reply TZubiri 20 hours agoprevSo, LLMs face a regression on their latest proposed improvement. It's not surprising considering their functional requirements are: 1) Everything For the purpose of AGI, LLM are starting to look like a local maximum. reply rjbwork 19 hours agoparent>For the purpose of AGI, LLM are starting to look like a local maximum. I've been saying it since they started popping off last year and everyone was getting euphoric about them. I'm basically a layman - a pretty good programmer and software engineer, and took a statistics and AI class 13 years ago in university. That said, it just seems so extremely obvious to me that these things are likely not the way to AGI. They're not reasoning systems. They don't work with axioms. They don't model reality. They don't really do anything. They just generate stochastic output from the probabilities of symbols appearing in a particular order in a given corpus. It continues to astound me how much money is being dumped into these things. reply ChadNauseam 19 hours agorootparentHow do you know that they don’t do these things? Seems hard to say for sure since it’s hard to explain in human terms what a neural network is doing. reply FuckButtons 17 hours agorootparentAbsence of evidence or a simple explanation does not mean that you can imbue statistical regression with animal spirits. reply toasterlovin 16 hours agorootparentThe burden of proof goes both ways: if you want to say X isn’t really the same thing as human general intelligence, you have to be able to confidently say human general intelligence isn’t really the same thing as X. reply beardedwizard 5 hours agorootparentAn interesting mental trap, except that the indirect evidence keeps mounting that LLMs do not possess human general intelligence, even if we can not describe exactly how it exists in the brain. reply toasterlovin 2 hours agorootparentOn the contrary, the parallels between the peculiarities of LLMs and various aspects of human cognition seem very striking to me. Given how early we are in figuring out what we can accomplish with LLMs, IMO the appropriate epistemic stance is to not reach any unequivocal conclusions. And then my personal hunch is that LLMs may be most of the magic, with how they're orchestrated and manipulated being the remainder (which may take a very long time to figure out). reply broast 6 hours agorootparentprevAre you not imbueing animals with spirits based on lack of evidence of statistical regression? reply nephy 18 hours agorootparentprevIf you give an LLM a word problem that involves the same math and change the names of the people in the word problem the LLM will likely generate different mathematical results. Without any knowledge of how any of this works, that seems pretty damning of the fact that LLMs do not reason. They are predictive text models. That’s it. reply alexwebb2 18 hours agorootparentDemonstrably false. https://chatgpt.com/share/6722ca8a-6c80-800d-89b9-be40874c5b... https://chatgpt.com/share/6722ca97-4974-800d-99c2-bb58c60ea6... reply TZubiri 18 hours agorootparentIt's worth noting that this may not be result of a pure LLM, it's possible that ChatGPT is using \"actions\", explicitly: 1- running the query through a classifier to figure out if the question involves numbers or math 2- Extract the function and the operands 3- Do the math operation with standard non-LLM mechanisms 4- feed back the solution to the LLM 5- Concatenate the math answer with the LLM answer with string substitution. So in a strict sense this is not very representative of the logical capabilities of an LLM. reply digging 3 hours agorootparentThen what's the point of ever talking about LLM capabilities again? We've already hooked them up to other tools. This confusion was introduced at the top of the thread. If the argument is \"LLMs plus tooling can't do X,\" the argument is wrong. If the argument is \"LLMs alone can't do X,\" the argument is worthless. In fact, if the argument is that binary at all, it's a bad argument and we should laugh it out of the room; the idea that a lay person uninvolved with LLM research or development could make such an assertion is absurd. reply thomashop 14 hours agorootparentprevIt shows you when it's calling functions. I also did the same test with Llama, which runs locally and cannot access function calls and it works. reply TZubiri 12 hours agorootparentYou are right I actually downloaded Llama to do more detailed tests. God bless Stallman. reply astrange 10 hours agorootparentprevMinor edits to well known problems do easily fool current models though. Here's one 4o and o1-mini fail on, but o1-preview passes. (It's the mother/surgeon riddle so kinda gore-y.) https://chatgpt.com/share/6723477e-6e38-8000-8b7e-73a3abb652... https://chatgpt.com/share/6723478c-1e08-8000-adda-3a378029b4... https://chatgpt.com/share/67234772-0ebc-8000-a54a-b597be3a1f... reply _flux 10 hours agorootparentI think you didn't use the \"share\" function; I cannot open any of these links. Can you do it in a private browser session (so you're not logged in)? reply astrange 9 hours agorootparentOops, fixed the links. mini's answer is correct, but then it forgets that fathers are male in the next sentence. reply TaylorAlexander 17 hours agorootparentprevAt this point I really only take rigorous research papers in to account when considering this stuff. Apple published research just this month that the parent post is referring to. A systematic study is far more compelling than an anecdote. https://machinelearning.apple.com/research/gsm-symbolic reply og_kalu 16 hours agorootparentThat study shows 4o, o1-mini and o1-preview's new scores are all within margin error on 4/5 of their new benchmarks(some even see increases). The one that isn't involves changing more than names. Changing names does not affect the performance of Sota models. reply gruez 16 hours agorootparent>That study very clearly shows 4o, o1-mini and o1-preview's new scores are all within margin error on 4/5 of their new benchmarks. Which figure are you referring to? For instance figure 8a shows a -32.0% accuracy drop when an insignificant change was added to the question. It's unclear how that's \"within the margin of error\" or \"Changing names does not affect the performance of Sota models\". reply og_kalu 16 hours agorootparentTable 1 in the Appendix. GSM-No-op is the one benchmark that sees significant drops for those 4 models as well (with preview dropping the least at -17%). No-op adds \"seemingly relevant but ultimately inconsequential statements\". So \"change names, performance drops\" is decidedly false for today's state of the art. reply gruez 16 hours agorootparentThanks. I wrongly focused on the headline result of the paper rather than the specific claim in the comment chain about \"changing name, different results\". reply TaylorAlexander 11 hours agorootparentprevAh, that’s a good point thanks for the correction. reply zmgsabst 4 hours agorootparentprevOnly if there isn’t a systemic fault, eg bad prompting. Their errors appear to disappear when you correctly set the context from conversational to adversarial testing — and Apple is actually testing the social context and not its ability to reason. I’m just waiting for Apple to release their GSM-NoOp dataset to validate that; preliminary testing shows it’s the case, but we’d prefer to use the same dataset so it’s an apples-to-apples comparison. (They claim it will be released “soon”.) reply gruez 16 hours agorootparentprevTo be fair, the claim wasn't that it always produced the wrong answer, just that there exists circumstances where it does. A pair of examples where it was correct hardly justifies a \"demonstrably false\" response. reply thomashop 11 hours agorootparentConversely, a pair of examples where it was incorrect hardly justifies the opposite response. If you want a more scientific answer there is this recent paper: https://machinelearning.apple.com/research/gsm-symbolic reply EraYaN 8 hours agorootparentIt kind of does though, because it means you can never trust the output to be correct. The error is a much bigger deal than it being correct in a specific case. reply thomashop 4 hours agorootparentYou can never trust the outputs of humans to be correct but we find ways of verifying and correcting mistakes. The same extra layer is needed for LLMs. reply digging 3 hours agorootparentprev> It kind of does though, because it means you can never trust the output to be correct. Maybe some HN commenters will finally learn the value of uncertainty then. reply jklinger410 18 hours agorootparentprevThis is what kind of comments you make when your experience with LLMs is through memes. reply Workaccount2 18 hours agorootparentprevThis is a relatively trivial task for current top models. More challenging are unconventional story structures, like a mom named Matthew with a son named Mary and a daughter named William, who is Matthew's daughter? But even these can still be done by the best models. And it is very unlikely there is much if any training data that's like this. reply alexwebb2 18 hours agorootparentThat's a neat example problem, thanks for sharing! For anyone curious: https://chatgpt.com/share/6722d130-8ce4-800d-bf7e-c1891dfdf7... > Based on traditional naming conventions, it seems that the names might have been switched in this scenario. However, based purely on your setup: > > Matthew has a daughter named William and a son named Mary. > > So, Matthew's daughter is William. reply rileymat2 15 hours agorootparentprevHow do people fair on unconventional structures? I am reminded of that old riddle involving a the mother being the doctor after a car crash. reply adwn 11 hours agorootparentNo idea why you've been downvoted, because that's a relevant and true comment. A more complex example would be the Monty Hall problem [1], for which even some very intelligent people will intuitively give the wrong answer, whereas symbolic reasoning (or Monte Carlo simulations) leads to the right conclusion. [1] https://en.wikipedia.org/wiki/Monty_Hall_problem reply vanviegen 11 hours agorootparentprevAnd yet, humans, our benchmark for AGI, suffer from similar problems, with our reasoning being heavily influenced by things that should have been unrelated. https://en.m.wikipedia.org/wiki/Priming_(psychology) reply wg0 5 hours agorootparentprevHow do we know Santa doesn't exist? Maybe he does. reply _heimdall 16 hours agorootparentprevThe whole design of an LLM is to consume and compress a huge space of human-generared content and use that to predict how a human would reply, one token at a time. That alone means the LLM isn't modelling anything beyond the human content it was trained on, and there is no reasoning since every prediction is based only on probabilities combined with controls similar to randomization factors used to avoid an entirely deterministic algorithm. reply ChadNauseam 15 hours agorootparentFor a lot of the content they were trained on, it seems like the easiest way to predict the next token would be to model the world or work with axioms. So how do we know that an LLM isn't doing these things internally? reply thomashop 15 hours agorootparentIn fact, it looks like the model is doing those things internally. We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato’s concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis. https://arxiv.org/html/2405.07987v5 reply _heimdall 5 hours agorootparentUnless I misread this paper, their argument is entirely hypothetical. Meaning that the LLM is still a black box and they can only hypothesise what is going internally by viewing the output(s) and guessing at what it would take to get there. There's nothing wrong with a hypothesis or that process, but it means we still don't know whether models are doing this or not. reply thomashop 4 hours agorootparentMaybe I mixed up that paper with another but the one I meant to post shows that you can read something like a world model from the activations of the layers. There was a paper that shows a model trained on Othello moves creates a model of the board, models the skill level of their opponent and more. reply _heimdall 6 hours agorootparentprevWell my understanding is that there's ultimately the black box problem. We keep building these models and the output seems to get better, but we can't actually inspect how they work internally. reply ricardobeat 15 hours agorootparentprevThat’s not an accurate description. Attention / multi-head attention mechanisms allow the model to understand relationships between words far apart and their context. They still lack, as far as we know, a world model, but the results are already eerily similar to how most humans seem to think - a lot of our own behaviour can be described as “predict how another human would reply”. reply thomashop 11 hours agorootparentWhen trained on simple logs of Othello's moves, the model learns an internal representation of the board and its pieces. It also models the strength of its opponent. https://arxiv.org/abs/2210.13382 I'd be more surprised if LLMs trained on human conversations don't create any world models. Having a world model simply allows the LLM to become better at sequence prediction. No magic needed. There was another recent paper that shows that a language model is modelling things like age, gender, etc., of their conversation partner without having been explicitly trained for it reply _heimdall 6 hours agorootparentprevDo we know for a fact that the mechanisms are actually used that way inside the model? My understand was that they know how the model was designed to be able to work, but that there's been very little (no?) progress in the black box problem so we really don't know much at all about what actually happens internally. Without better understanding of what actually happens when an LLM generates an answer I stick with the most basic answer that its simply predicting what a human would say. I could be wildly misinformed there, I don't work directly in the space and its been moving faster than I'm interested in keeping up with. reply chamomeal 15 hours agorootparentprevI totally agree that they’re a local maximum and they don’t seem like a path to AGI. But they’re definitely kinda reasoning systems, in the sense that they can somewhat reason about things. The whacky process they use to get there doesn’t take away from that IMO reply alexwebb2 19 hours agorootparentprevIf you expect \"the right way\" to be something _other_ than a system which can generate a reasonable \"state + 1\" from a \"state\" - then what exactly do you imagine that entails? That's how we think. We think sequentially. As I'm writing this, I'm deciding the next few words to type based on my last few. Blows my mind that people don't see the parallels to human thought. Our thoughts don't arrive fully formed as a god-given answer. We're constantly deciding the next thing to think, the next word to say, the next thing to focus on. Yes, it's statistical. Yes, it's based on our existing neural weights. Why are you so much more dismissive of that when it's in silicon? reply Techonomicon 18 hours agorootparentBecause we still don't know how the brain really does all it does in very specific terms, so why assume to know exactly how we think? reply alexwebb2 18 hours agorootparentWhy is there only one valid way of producing thoughts? reply jltsiren 17 hours agorootparentprevFinite-state machines are a limited model. In principle, you can use them to model everything that can fit in the observable universe. But that doesn't mean they are a good model for most purposes. The biggest limitation with the current LLMs is the artificial separation between training and inference. Once deployed, they are eternally stuck in the same moment, always reacting but incapable of learning. At best, they are snapshots of a general intelligence. I also have a vague feeling that a fixed set of tokens is a performance hack that ultimately limits the generality of LLMs. That hardcoded assumptions make tasks that build on those assumptions easier and seeing past the assumptions harder. reply alexwebb2 16 hours agorootparent> At best, they are snapshots of a general intelligence. So are we, at any given moment. reply Jensson 15 hours agorootparentprev> As I'm writing this, I'm deciding the next few words to type based on my last few. If so you could have written this as a newborn baby, you are determining these words based on a lifetime of experience. LLMs doesn't do that, every instance of ChatGPT is the same newborn baby while a thousand clones of you could all be vastly different. reply thomashop 14 hours agorootparentWe argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato’s concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis. https://arxiv.org/html/2405.07987v5 reply kibwen 5 hours agorootparentprev> I've been saying it since they started popping off last year and everyone was getting euphoric about them. Remember the resounding euphoria at the LK-99 paper last year, and how everyone suddenly became an expert on superconductors? It's clear that we've collectively learned nothing from that fiasco. The idea of progress itself has turned into a religious cult, and what's worse, \"progress\" here is defined to mean \"whatever we read about in 1950s science fiction\". reply wyldfire 16 hours agorootparentprev> It continues to astound me how much money is being dumped into these things. Maybe in our society there's a surprising amount of value of a \"word stirrer\" intelligence. Sure, if it was confident when it was right and hesitant when it was wrong it'd be much better. Maybe humans are confidently wrong often enough that an artificial version that's compendious experience to draw on is groundbreaking. reply csomar 17 hours agorootparentprevI am pretty sure Claude 3.5 Sonnet can reason or did reason with a particular snippet of code I was working on. I am not an expert in this area but my guessing is that these neural nets (made for language prediction) are being used for reasoning. But that’s not their optimal behavior (since they are token predictor). A big jump in reasoning will happen when reasoning is off loaded to an LRM. Human brains are sure big but they are inefficient because a big portion of the brain is going to non-intelligence stuff like running the body internal organs, eye vision, etc… I do agree that the money is not well spent. They should haver recognized that we are hitting s local maximum with the current model and funding should be going to academic/theoretical instead of dump brute force. reply jsheard 19 hours agoparentprev> So, LLMs face a regression on their latest proposed improvement. Arguably a second regression, the first being cost, because COT improves performance by scaling up the amount of compute used at inference time instead of training time. The promise of LLMs was that you do expensive training once and then run the model cheaply forever, but now we're talking about expensive training followed by expensive inference every time you run the model. reply TZubiri 17 hours agorootparentTo be fair they also advanced in the cost aspect with other models gpt4o and 4o mini have a tenth and a hundredth of inference cost of gpt4 respectively reply pessimizer 3 hours agoparentprev> So, LLMs face a regression on their latest proposed improvement. A regression that humans also face, and we don't say therefore that it is impossible to improve human performance by having them think longer or work together in groups, we say that there are pitfalls. This is a paper saying that LLMs don't exhibit superhuman performance. reply idiotsecant 18 hours agoparentprevLLMs are a local maximum in the same way that ball bearings can't fly. LLM-like engines will almost certainly be components of an eventual agi-level machine. reply lucianbr 1 hour agorootparentWhat is your \"almost certainty\" based on? What does it even mean? Every thread on LLMs is full of people insisting their beliefs are certainties. What I'm certain is we should not praise the inventor of ball bearings for inventing flight, nor once ball bearings were invented flight became unavoidable and only a matter of time. reply FuckButtons 17 hours agorootparentprevI don’t think that’s necessarily true, that presumes that the cobbled together assortment of machine learning algorithms we have now will somehow get agi, if we need a fundamentally different way of doing things there’s no reason to assume it will use a language model at all. reply TZubiri 17 hours agorootparentprevI agree, my bet is that they will be used for NLP, and ML debugging/analysis. reply alexchantavy 17 hours agoprevThis seems to support how thinking out loud during a coding test might make you do worse. reply why-el 2 hours agoparentI like this analogy a lot. It's possible that forced externalization of thoughts accidentally causes the omission of crucial data. That is, much more goes on in your head, you probably laid out the whole algorithm, but being asked to state it on the spot and in clear, serial words is causing you to bork it by taking shortcuts. reply nisten 19 hours agoprevThis sounds about right from my experience getting nerdsniped by new samplers along with trying to reproduce the API middleware for the whole reflection thing, and using 4400 questions for a new benchmark is not bad given that even the well-regarded gpqa benchmark is only 3000-something questions. What's ... mildly infuriating here is the lack of any kind of data, code, 0 mention of github in the paper, and nothing for anyone to reproduce or find any reason in my opinion to even recommend anyone to read this thing at all. If you think that whatever you're doing in the field of LLMs won't be obsolete in 6 months you're being delusional. Anyway, back to the paper, it says all questions culminated to a yes or no answer... meaning theres a 50/50 chance of getting right, so does that mean the 8% drop in performance you got from testing llama 3 8b this way is more like 4% which would make it statistically insignificant? And given that the only other scientifically usueful & reproducible (non-api walled models which no one knows on how many actual llms and retrieval systems are composing that solution you're testing)models were less than that leads me to the opinion that this whole thing was just useless slop. So please, if you're writing a paper in LLMs, and want to seem credible, either have some type of demo thing or show the actual god damn trash code and top secret garbage data you wrote for it so people can make some kind of use of it before it goes obsolete otherwise you're just wasting everyones time. TL:DR. It's trash. reply npunt 21 hours agoprev\"Don't overthink it\" is sometimes good advice! reply marviel 21 hours agoparentI love backpropagating ideas from ML back into psychology :) I think it shows great promise as a way to sidestep the ethical concerns (and the reproducibility issues) associated with traditional psychology research. One idea in this space I think a lot about is from the Google paper on curiosity and procrastination in reinforcement learning: https://research.google/blog/curiosity-and-procrastination-i... Basically the idea is that you can model curiosity as a reward signal proportional to your prediction error. They do an experiment where they train an ML system to explore a maze using curiosity, and it performs the task more efficiently -- UNTIL they add a \"screen\" in the maze that shows random images. In this case, the agent maximizes the curiosity reward by just staring at the screen. Feels a little too relatable sometimes, as a highly curious person with procrastination issues :) reply npunt 21 hours agorootparent\"...in AI\" will be the psychology equivalent of biology's \"...in Mice\" reply marviel 21 hours agorootparentIt will! Not 1:1, has issues, but gives hints. Also much more scalable. reply miningape 20 hours agorootparent> Not 1:1, has issues, but gives hints. > Also much more scalable. This same description could be applied to lab mice reply Terr_ 20 hours agorootparentIt'll probably be a ways before we start making shrines to their unwilling participation though. https://en.wikipedia.org/wiki/Monument_to_the_laboratory_mou... reply j_bum 14 hours agorootparentWhat would the shrine be of? An A100? reply jeezfrk 20 hours agorootparentprev\"Nerd sniping\" reply veryfancy 20 hours agoprevSo like dating? reply m3kw9 21 hours agoprev [–] would be slow to use COT on simple requests like 1+1 reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper explores how Chain-of-Thought (CoT) prompting, typically beneficial for large language models, can actually decrease performance in specific tasks.- It identifies tasks such as implicit statistical learning and visual recognition where verbal thinking, akin to CoT, negatively impacts both human and model performance.- The study links cognitive psychology with CoT evaluations, providing insights into when CoT should be avoided in favor of zero-shot methods, which do not rely on prior examples or reasoning steps."
    ],
    "commentSummary": [
      "Chain-of-thought (CoT) reasoning can negatively impact performance in tasks that benefit from subconscious processing, such as implicit statistical learning and visual recognition.- In both humans and AI, overthinking can disrupt mental shortcuts and slow down processes, which may not always be advantageous.- This phenomenon is observed in AI development and human activities like sports and creativity, where intuitive processing often leads to better outcomes."
    ],
    "points": 351,
    "commentCount": 223,
    "retryCount": 0,
    "time": 1730317361
  },
  {
    "id": 42008569,
    "title": "Introducing ChatGPT Search",
    "originLink": "https://openai.com/index/introducing-chatgpt-search/",
    "originBody": "body{font-family:Arial,Helvetica,sans-serif}.container{align-items:center;display:flex;flex-direction:column;gap:2rem;height:100%;justify-content:center;width:100%}@keyframes enlarge-appear{0%{opacity:0;transform:scale(75%) rotate(-90deg)}to{opacity:1;transform:scale(100%) rotate(0deg)}}.logo{color:#8e8ea0}.scale-appear{animation:enlarge-appear .4s ease-out}@media (min-width:768px){.scale-appear{height:48px;width:48px}}.data:empty{display:none}.data{border-radius:5px;color:#8e8ea0;text-align:center}@media (prefers-color-scheme:dark){body{background-color:#343541}.logo{color:#acacbe}}Please turn JavaScript on and reload the page.Please enable Cookies and reload the page.(function(){window._cf_chl_opt={cvId: '3',cZone: \"openai.com\",cType: 'managed',cRay: '8db5e588fd9dc57b',cH: 'kMIuCtgUHWfUU24obYasB0aGdi4bv6tBLxZuqAEhp8c-1730401317-1.2.1.1-KKTvw1MfcOBloRPdjXgjrpTnprTAoc7IVeVVJLhttJn..WRIBGrRfPtJU9ZZhb3q',cUPMDTk: \"\\/index\\/introducing-chatgpt-search\\/?__cf_chl_tk=_QTNCaP9MZ03_7EbsqI4PqFIJtrRwjmzOxkZUJPvl2Y-1730401317-1.0.1.1-qEnykEFlDikKnf6M3HeKyYXkVS.EidfUsNwsxCgq6P4\",cFPWv: 'b',cITimeS: '1730401317',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 1,cTplV: 1,cTplB: 'cf',cK: \"\",fa: \"\\/index\\/introducing-chatgpt-search\\/?__cf_chl_f_tk=_QTNCaP9MZ03_7EbsqI4PqFIJtrRwjmzOxkZUJPvl2Y-1730401317-1.0.1.1-qEnykEFlDikKnf6M3HeKyYXkVS.EidfUsNwsxCgq6P4\",md: \"VBvSyPCMQVX7tsDl.N7Rj4Bkrf4mjvoBFAtazZkhoAE-1730401317-1.2.1.1-rycYwtfSaBxHQXwioTUvLYEBf1PGpjf_DAyt9qTelPqCXAZvsEgQF4fJqEv.bCKh3l1J_MujQEsuiZOMmDSvtI73emJVSkcJ3twUtErQhuC5j8ul62t7nDHpbXhRYbLEnNGYaDQTipZAyuosGHEr.SPRls8QMdTKg7KnFK6CllU6RKc6U.ATurDxShU2Wt6xN8wYSyies6I.fBCe.VzbSdyDo_5Rzf.mCei7annmov3SjQG6WpGy4X8CvNTfwbDKLD2p7q93sqHzXPJI.BgdE8bufaKxMXOT6nnoy7yXdUlAK.nZLsbpq4gZ9o_o1FwxCCFnc2qtgWVgdLBcs543LmnemSf.MOheAafBTjQgEkrBmm5Yxxh0yByiSJen.7HAiSgymu5yDZwDJPF1X0xOdlNn2dsEAwPtIbcOd1Ju_BKBn3tvt23aZ3ZYkc1B7RiaGhgKj1moyY_wmtDaOtY0gA5e4kJqmBS82F7y78m5p.qLFt3nT4uz.KiFof7rJG2RZka0GmH4lirOxFptyvb00bIJRV7LA9cMbhlj9huP3ih_WkMso43CwHDBLh7WCdaNYtnBWnthIQYrgSgC.MeOH5VKJ4tBJlMvcw_PHNWPb4UhL4ABQTF4cO4uEY_oSfeefrGYwsn8d499QpP3Ik0WfvPsSYzrozk4FNxFVl6hbYK1snYrEYNnBUTaFcFzACFQmrmL58rCwlwu9XXz_fnlyzssrXPKu0oBBdeCa4NyBsZYwjil4gi2LLbCS5qX_9IDlQJhkVx0FbJy8.kUY8lsSyj9f99Bs3DYhmPCulr0Y_xL1TIQovowzUKG..NaiVArEOtESozMDZVROdCh.VMrPqgpIAEYGPY0ZSmm_xTHF0LAY1QUjJJyC7ZZqbLz.LA258uM3SEvLKPqNBOQr5Uc9.CO8w8duRxZun1WjJTz80qHTOyjSEtNHn7.ZbT9u9a.3wZuHyqraeQZo4JUEtUp6KgJZH5Hwc6vlMr_IDOtv5kChqqYGRG4V6Be0fa.O8QcNHvXp2G9igTxMhQ3pWMVGJBQcRSlXXvx3xFDqaRWy6NDxczD1iJ2gNx8JfSgVX0.i6Nr4AMQAAl8EkSireRCzogh6OnTRsPVsZyii7wQP6QOcbA5NJ1kUzPkTq41tAainjhVNpLmF6n2yd_65CkA19JGLjGCB35J2K6Z443hGcj_4CG8mPZQBI2zTPO.oeq_SZBKaujj5ktijP51QTxZry2MkI.bcUh_bc9T9dFIYUMhwb7DdJPm70IXScc89EuaC35FH5Etn5aSyqaRRgebi4_pzcadVdIMSyHHBihTgSUxGbAFAiKIX8QXqPpr2QLfuyQ6o74CWzaBEh7SWZOIkzeFpyY0h1CedXNhEApSUUoPNR7RNB1Wgknw6Dtu8Hj0nHrXc0xQmIxPWgQCsiLetk1xzNW70QBAQIULUP8r6QiDAdtemDUmJ3HaKlpVBDsNLng.PNmOTWT9tR7Z7xrt3OskQ09kDyc06NQYVDTe3LkDR8YsYuRF4saXEvHWg_pQEn14NYDuF5YVkG8hHmqiO4Ya3AWebsCi7KvC3p0qEb11FbxqM5mR0t0GWF_LEUfIaG6Vm7YH.Npdw_tkDKuZoSlzBOns4NiTmDW6eVQx8ZRi.9k9ARWT3HOjy3a.BHSAFJ2XnPhHjIKEp9oHIrceUd1C3IsetJ_SGR06iVCLW.5M5cqxHB0ZN.M_lWf8rqVW.ThG2Tzr99.hmQvk_RwF6MAg2Q6.V4dxfpuYiURYvuN6Kh8p8ocbWIh4mCx.00SFqnclOQ8KI053UciInMNxd_XT5e3RvZmMrxnxob5Y.ko1cX8_hA8h277nMH0P4hUh6Q9.QTNmgjjJ7Emmt.IhG6g60h5yPCM6W_KrlUcgTp5rpaXM8euE6_tEY18kT7ABpD6YSlvFxUn7_bbYE4ZoN7XPQMC8lgttFgsdwFniYuCX.sOCRnxZVl1DDn63RY5pznHqsu.rdrxvU7EEwdGUX1Me9e02_ad9V0.iUsVqyTbZOBuFv.JHnkNTonYb8Qe2wKxoGIgfdvnOqmt7g8lnPd8aK6V2PLw4GEBoQINaWf7qd2elWT3l_PYA3wBrVbMFeZtCgKjXETPD1d30SMZTraOccKWAzzFehzpdIzDNO4qxUC3zjPACxR_RP9Y4f0URHW_Qs9ldecbJrmJWsOFMRQVT.UciDDdeeyrO9mX0y69jBAQbGPIz1eXQuwyToF9LsN4bHzHtYsDtuUms84qhXyi3fraFT.Pkk0EOtbKkx1OrTx1x9iQVwdtwmWS3wXFHiOIwZLl_XgALsvLA6sLw_eUo9yMG6RUc90Ggq4LnnLk1WQajACClIOuEB.gSK7ijg9SyHuW7YIpg9uEMKzwkyDF94Ux6OY9JKSS3fCeje9dwhe2XRKQPqOfqFhhhm.MdRMgkovIjkvXMFAgHhrbM3ZVaqTpTO9xC0zPqyP_ctrU_Q2bASonjGVzXmwXsJJNthcU9lOtVdZ5.sj3o0WVUmYDj8i2jKrJXkZKAFx_eZNzi3rlzVVghKAsr7x9s9GCJd6c.3shfuWQsN7F5MtXbQQ\",mdrd: \"2xjxSAVfD6XhWmBTj88ySXgB88Ti40SjeOcI21Qztxk-1730401317-1.2.1.1-3Az3l2345_EXk68kV6_YdPIERIlX.TaGc05Oa4wbULyqHM89RQfEC.YI69aI9kqlVCT7oDD6j85YiSkVsY6d.0ARYA05HZ4YCxDPFcsWthufRt1Tmf8o2rx2SGSOjlxGJhWn.HlhKDII4f5LKx6o_Z5LBx2j0es85SdfUWd0CGBr5rO4rm.fLMF173s8Rdu.494gakux3SSkFHY5.4_Mt8P4JWDE7y2U4r4iCdoYSa7oXTnZ_PrGf2MxPryucIbdzDfua3pyn81RwidGxXQx1E_BTlKpAlTCqOECd.iFpJBWEfOUj4yKvnmCcGcunuBDhQdvctKKIgPz0oc9qxX_rgZD7dgiRAFkYsgrPOEHkdkSOwgUmvUP3kRloMmxp76mtG.2AQYg98Z9MnjMxgW5pV2Ss9RtYm0SLRkl1eAKdQNpqE76I6oOqxcAC4uoc5o5ZcgoTJLF2utEcerfdvCoeUBVJDYhYS42Xv_lOY9ttLOuisFSV0ILkaXrhECdh2rXRwqjaTBs_ldS2in4j1KrGoGi4ZniiOcBhmpVVI88.gkpOmhhdEmMcxJrDvoSpUXNIjdLq4.oxxyrdpQgxflSGXl2NEg.r2Ilmqxyos3.vNbqu3qbeDV4xVbLO57hzGjWp2wQ_GqtI_ddpeM9l23yFWTIo4vJXQkiQm3T14QQW5qYyshiqSjraGLBECDF.Uspa18Jqa0aejL5I2u6x2qithMvgdTvltx24IADlrflhcXGsyzI3GMZuLX_azA_.6Tmy3KZyzp7c50RoyXb5I5sdlvq_EsDUsjXgJK8Pyh.W4VIKN623SW0H.r3OTrm3QpOJ0qtO15G76n0L1Kdsq60jlx2IpaNCymrJ01RTWFrPdd4wRQGSAqzZwAEoGYgsPbLYHGLMdQHQ7VUGAuO_7nmeU_P.r3Ve5xpiJ0h050C43hHn9n5J61uGHdyRBueAJLHYiCOh2bObDwHzZys3Sx5.jYPHu8e5SHn_vHYg1KhVn_V3k6YlS0d_0B0KRcNgx55YxdjkeeRjWiYhED.E_fCyrtNUt_KtwUnIXTxaYVH5cjJE1FIL2Nu8pEfBHgfTVd6jrEzJlK04MdG113.8rrsLLG29aNa01TQUgBVAJDW5RSTDIiTu3Sfv_tdcpMMRHNA9aooGng1i6xX_Tsmg9TmyLkaSSY.UWovfwMC_o8cY.7kCQ.g497GtsLVy4Zl9tXTPwpzlrZKaAU8fbB5wT_0mVFXXFY9cfmAY5_2p_EfWILC0MZ.O16dd7uXgizsOVLSWVA8ikOqiaIZdR4ia_IYqlDa_THTZ2vv2LtxEGHMQJuEjpzc0kD0RzydxCi2rZ1bT0dGG8LoU.bjO.gUJ3MDADY9YcrkjUZU8qgzR2mR5TzSrKbFU2oaNDEpRyW_u3G4XeR7LIHTjOLTRJYf6of2nQw5g1FLdPFroxbUttH21K5AVfH00tchK.XkVQn1xA0VnXZ7wak.Y4Am6KowWs9xxMOS_zNGeEAw4d70B1QpZVoSqT1RaPXC1qAfkE59AyFa1gQJhoBvRxt6B0XtflboC6eB1tr6wriJK7RD4pTThUNd3sazDMbjsoHoFuAVTDZsm6F6uk61sRDiHoe8ctcWCvxOoXjYmvOdESzbvx9qm.wCtNsMCFA6kcmGg4MyPA1m2ReHJNfHqcT.QYEI.YG7XTaTNgPYJGyZa6F3zT5xd.UhfynGC6Si6e12Krasxm2kXY9_FEPIkTW3NHE4oUYnK4Zk4fK9UJiv_ZVbrK8vkPchcwrddPO5uaaRBsdfP003Velu9onSAGx_GkcNkzkkfcFv9H2kDbi0HkPyVM9GiFkfSL4LrYkGqlXvpjQ1cGO11B7OhDQcWEjy.W1oLvdDv24P1zWdU8Ryo3mMEmBW1cKaihdtfpPpBXgs2YqvUyjqNJQfUHhRr8I7tXjXarXy0uKLe8njADynl0ZOyHS_MGKITMTW_0QbDd5SPXlPJsUscsH6uZRDs.iTd4YXmQet3puNNbk3PVjiQKXUBuRHTxBU2W6bpIoJoqFW3lNk8.iDvB_JCYg8f1vJqbb8exQyEjkupOTfHD.US5t97qxbzjmxk7kGbl8_5o_sBc3.8cKuhp64Z4zQW4.N3fZ4zw8j0fEhskm8D8c2gUT1A3F3zHRxD9QP780FPGWCfZiuWL1Ni2zz0VnF9fLBtCeDaH.OJx4pOZFez7HrPxMaeVqQEocaHe_h8hENnDQAFukQXDlbxkK3GCXSSiTJRiVNnsgs4Eo7PP3miRGrY52mIO7BYyIeWQ3dKpkqJaM_Qzg81Dw8y03tYOqSYdRG7Fz2J8G2GHjuZvAeON3T4go9kqOIOF8HtI3eWJgvt2C4Y7C5X4XF\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8db5e588fd9dc57b';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/index\\/introducing-chatgpt-search\\/?__cf_chl_rt_tk=_QTNCaP9MZ03_7EbsqI4PqFIJtrRwjmzOxkZUJPvl2Y-1730401317-1.0.1.1-qEnykEFlDikKnf6M3HeKyYXkVS.EidfUsNwsxCgq6P4\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());!function(){var e=document.createElement(\"iframe\");function n(){var n=e.contentDocument||e.contentWindow.document;if(n){var t=n.createElement(\"script\");t.nonce=\"\",t.innerHTML=\"window['__CF$cv$params']={r:'792f8224776acf9f',m:'hMcSCCrnIkr7c8Pec6Na6boaaFAnQ6S0ypG2GKRbKgc-1675305063-0-AaJn0SqKZQnadmRQ5O1dM9xMkXWyP+ll7gpl2NHeoNbZTEXMjlB10KkwnEU3hf0/gMODfKqcBGLVecql6U04GGs+iJ/kNrNqj1FgfAOlQV+T2koMQMvUy1zr9tegBBX6BikfccHZhwoJhnXc0eTcg58=',s:[0x60b082f691,0xee65a67e11],u:'/cdn-cgi/challenge-platform/h/b'};var now=Date.now()/1000,offset=14400,ts=''+(Math.floor(now)-Math.floor(now%offset)),_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/h/b/scripts/alpha/invisible.js?ts='+ts,document.getElementsByTagName('head')[0].appendChild(_cpo);\",n.getElementsByTagName(\"head\")[0].appendChild(t)}}if(e.height=1,e.width=1,e.style.position=\"absolute\",e.style.top=0,e.style.left=0,e.style.border=\"none\",e.style.visibility=\"hidden\",document.body.appendChild(e),\"loading\"!==document.readyState)n();else if(window.addEventListener)document.addEventListener(\"DOMContentLoaded\",n);else{var t=document.onreadystatechange||function(){};document.onreadystatechange=function(e){t(e),\"loading\"!==document.readyState&&(document.onreadystatechange=t,n())}}}();",
    "commentLink": "https://news.ycombinator.com/item?id=42008569",
    "commentBody": "Introducing ChatGPT Search (openai.com)298 points by marban 2 hours agohidepastfavorite273 comments freediver 23 minutes agoBeen thinking about this a lot [1]. Will this fundamentally change how people find and access information? How do you create an experience so compelling that it replaces the current paradigm? The future promised in Star Trek and even Apple's Knowledge Navigator [2] from 1987 still feels distant. In those visions, users simply asked questions and received reliable answers - nobody had to fact-check the answers ever. Combining two broken systems - compromised search engines and unreliable LLMs - seems unlikely to yield that vision. Legacy, ad-based search, has devolved into a wasteland of misaligned incentives, conflict of interest and prolifirated the web full of content farms optimized for ads and algos instead of humans. Path forward requires solving the core challenge: actually surfacing the content people want to see, not what intermiediaries want them to see - which means a different business model in seach, where there are no intermediaries. I do not see a way around this. Advancing models without advancing search is like having a michelin star chef work with spoiled ingredients. I am cautiously optimistic we will eventually get there but boy does it feel that we will need a very different setup in terms of incentives involved, both in tech and society. [1] https://blog.kagi.com/age-pagerank-over [2] https://www.youtube.com/watch?v=umJsITGzXd0 reply jtgverde 9 minutes agoparentGreat find on the knowledge navigator, I had never seen it but I was a toddler when it was released haha. It's interesting how prescient it was, but I'm more struck wondering--would anyone in 1987 have predicted it would take 40+ years to achieve this? Obviously this was speculative at the time but I know history is rife with examples of AI experts since the 60s proclaiming AGI was only a few years away Is this time really different? There's certainly been a huge jump in capabilities in just a few years but given the long history of overoptimistic predictions I'm not confident reply jpadkins 8 minutes agoparentprev> actually surfacing the content people want to see, Showing users what they want to see conflicts with your other goal of receiving reliable answers that don't need fact checked. Also a lot of questions people ask don't have one right answer, or even a good answer. Reliable human knowledge is much smaller than human curiosity. reply duxup 10 minutes agoparentprevLLMs are a lot like Star Trek to me in the sense that you can ask a question, and then follow up questions to filter and refine your search, even change your mind. Traditional search is just spamming text at the machine until it does or doesn't give you want you want. That's the magic with LLMs for me. Not that I can ask and get an answer, that's just basic web search. It's the ability to ask, refine what I'm looking for and, continue work from there. reply cjf101 1 hour agoprevIf the current iteration of search engines are producing garbage results (due to an influx of garbage + SEO gaming their ranking systems) and LLMs are producing inaccurate results without any clear method proposed to correct them, why would combining the two systems not also produce garbage? The problem I see with search is that the input is deeply hostile to what the consumers of search want. If the LLM's are particularly tuned to try and filter out that hostility, maybe I can see this going somewhere, but I suspect that just starts another arms race that the garbage producers are likely to win. reply fulafel 56 minutes agoparentGarbage-ness of search results is not binary, the right question is: can LLMs improve the quality of search results? But sure, it won't end the cat and mouse game. reply cjf101 31 minutes agorootparentI think that's the right broad question. Though LLMs properties mean that for some number of cases they will either make the results worse, or more confidently present wrong answers. This prompts the question: what do we mean by \"quality\" of results? Since the way current LLM interfaces tend to present results is quite different from traditional search. reply startupsfail 19 minutes agorootparentprevThe question is what is the business model and who pays for it, that determines how much advertising you’re getting. It is not clear if OpenAI could compete in Ad-supported search. So maybe OpenAI is trying to do the basic research, outcompete the Bing research group at Microsoft and then serve as an engine for Bing. Alternatively they could be just improving the ability of LLMs to do search, targeting future uses in agentic applications. reply hatthew 25 minutes agoparentprevSearch engines tend to produce neutral garbage, not harmful garbage (i.e. small tidbits of data between an ocean of SEO fluff, rather than completely incorrect facts). LLMs tend to be inaccurate because in an absence of knowledge given by the user, it will sometimes make up knowledge. It's plausible to imagine that they will cover each other's weaknesses: the search engine produces an ocean of mostly-useless data, and the LLM can find the small amount of useful data and interpret that into an answer to your question. reply shellfishgene 15 minutes agoparentprevIf I can pretty quickly tell a site is SEO spam, so should the LLM, no? Of course that would just start a new round in the SEO arms race, but could work for a while. reply sangnoir 5 minutes agorootparent> If I can pretty quickly tell a site is SEO spam, so should the LLM, no? Why would you assume that? reply mplewis 6 minutes agorootparentprevThe LLM is not a human and cannot distinguish between spam and high quality content. reply niam 1 hour agoprevGenuine question: is there a present or planned value proposition for people like me who already have decent search skills? Or are these really for children/elders who (without making any normative claim about whether this is a good thing or not) can't be arsed to perform searches themselves? Does someone else have good search skills but mingle traditional search engines with LLMs anyways? Why? I use LLMs every day but wouldn't trust one to perform searches for me yet. I feel like you have to type more for a result that's slower and wordier, and that might stop early when it amasses what it thinks are answers from low effort SEO farms. reply Willamin 1 hour agoparentI find myself being unable to search for more complex subjects when I don't know the keywords, specialized terminology, or even the title of a work, yet I have a broad understanding of what I'd like to find. Traditional search engines (I'll jump between Kagi, DuckDuckGo, and Google) haven't proved as useful at pointing me in the right direction when I find that I need to spend a few sentences describing what I'm looking for. LLMs on the other hand (free ChatGPT is the only one I've used for this, not sure which models) give me an opportunity to describe in detail what I'm looking for, and I can provide extra context if the LLM doesn't immediately give me an answer. Given LLM's propensity for hallucinations, I don't take its answers as solid truth, but I'll use the keywords, terms, and phrases in what it gives me to leverage traditional search engines to find a more authoritative source of information. --- Separately, I'll also use LLMs to search for what I suspect is obscure-enough knowledge that it would prove difficult to wade through more popular sites in traditional search engine results pages. reply layer8 41 minutes agorootparent> I find myself being unable to search for more complex subjects when I don't know the keywords, specialized terminology, or even the title of a work, yet I have a broad understanding of what I'd like to find. For me this is typically a multi-step process. The results of a first search give me more ideas of terms to search for, and after some iteration I usually find the right terms. It’s a bit of an art to search for content that maybe isn’t your end goal, but will help you search for what you actually seek. LLMs can be useful for that first step, but I always revert to Google for the final search. Also, Google Verbatim search is essential. reply niam 44 minutes agorootparentprevI 100% get that use case; it comes up often for me. Though I've used/trusted regular LLMs more for that. Historically it felt like search LLM is the worst of both worlds--the LLM is bound to what humans are saying on the first page of Google, rather than its perhaps-more-informative (if questionable) 'knowledge', which can then inform my own searches. But maybe my intuition of how this works is off. I haven't really used them much except the early days of Bing Chat, or when Kagi Chat forgets my preference and toggles \"Use Web\" on. reply erosivesoul 55 minutes agorootparentprevI also find some use for this. Or I often ask if there's a specific term for a thing that I only know generally, which usually yields better search results, especially for obscure science and technology things. The newer GPTs are also decent at math, but I still use Wolfram Alpha for most of that stuff just because I don't have to double check it for hallucinations. reply Lws803 57 minutes agorootparentprevYou might like what we're building in that sense :D (full disclosure, I'm the founder of Beloga). We're building a new way for search with programmable knowledge. You're essentially able to call on search from Google, Perplexity other search engines by specifying them as @ mentions together with your detailed query. reply awongh 2 minutes agoparentprevI use LLMs as a kind of search that is slightly less structured. There are two broad cases: 1) I know a little bit about something, but I need to be able to look up the knowledge tree for more context: `What are the opposing viewpoints to Adam Smith's thesis on economics?` `Describe the different categories of compilers.` 2) I have a very specific search in mind but it's in a domain that has a lot of specific terminology that doesn't surface easily in a google search unless you use that specific terminology: `Name the different kinds of music chords and explain each one.` LLMs are great when a search engine would only surface knowledge that's either too general or too specific and the search engine can't tell the semantic difference between the two. Sometimes when I'm searching I need to be able to search at different levels of understanding to move forward. reply jakub_g 57 minutes agoparentprevI don't overuse LLMs for now; however when I have a complex problem that would require multiple of searches and dozens of tabs opened and reading through very long docs, asking LLM allows me to iterate order of magnitude faster. Things that were previously \"log a jira and think about it when I have a full uninterrupted day\" now can be approached with half an hour spare. This is game changer because \"have a full day uninterrupted\" almost never happens. It's like having a very senior coworker who knows a lot of stuff and booking a 30m meeting to brainstorm with them and quickly reject useless paths vs dig more into promising ones, vs. sitting all day researching on your own. The ideas simply flow much faster with this approach. I use it to get a high level familiarity with what's likely possible vs what's not, and then confirm with normal search. I use LLMs also for non-work things like getting high level understanding of taxation, inheritance etc laws in a country I moved in, to get some starting point for further research. reply itissid 43 minutes agorootparentThis. Not having to open two dozen tabs and read through so much is a gamechanger, especially for someone who has had trouble focusing with so much open. This is especially true when learning a new technology. reply adamc 1 hour agoparentprevI dunno, I'm not exactly on the AI bandwagon, but search is the one place where I use (and see others using) chatgpt all the time. The fact that Google search has been getting worse for a decade probably helps, but better search -- consistently done, without ads or cruft -- would be worth a few bucks every month for me. I agree that you can't TRUST them, but half the links regular search turns up are also garbage, so that's not really worse, per se. reply davidee 18 minutes agorootparentSame, but, until recently, I've been using Microsoft's Co-Pilot because for the longest time it did exactly what this new \"search\" feature added to ChatGPT: it produced a list of source material and links to reference the LLM's output against. It was often instrumental for me and I did begin to use it as a search engine considering how polluted a lot of first-search results have become with spam and empty, generated content. Oddly, Microsoft recently changed the search version of Copilot to remove all the links to source material. Now it's like talking to an annoying growth-stage-startup middle manager in every way, including the inability to back up their assertions and a propensity to use phrases like \"anyway, let's try to keep things moving\". Happy to see this feature set added into ChatGPT – particularly when I'm looking for academic research in/on a subject I'm not familiar with. reply spunker540 1 hour agoparentprevI think it’s pretty clear that LLMs can process a document/article/web page faster than any human in order to answer a given question. (And it can be parallelized across multiple pages at once too). The main hard part of searching isn’t formulating queries to write in the Google search bar, it’s clicking on links, and reading/skimming until you find the specific answer you want. Getting one sentence direct answers is a much superior UX compared to getting 10 links you have to read through yourself. reply tempusalaria 58 minutes agorootparentOnly if it is reliably correct. Google does offer an AI summary for factual searches and I ignore it as it often hallucinates. Perplexity has the same problem. OpenAI would need to solve that for this to be truly useful reply vel0city 37 minutes agorootparentThis is why my most used LLM after code suggestions is Bing. I like that it has lots of references for the things I ask it to double check and read more, but at the same time it can help me dig deeper into a subject rapidly and better formulate the exact question I'm trying to ask and give me a link to the actual data it's getting it's info from. reply Lws803 55 minutes agorootparentprevAgreed, hallucinations can be pretty bad and can hurt trust a great deal. reply bigstrat2003 5 minutes agoparentprevThe entire tech industry for the last decade (if not more) has been aimed at people who can't be arsed to learn to use computers properly. I would be astonished if this time is somehow different. reply pflenker 52 minutes agoparentprevI find that my search skills matter less and less because search engines try to be smarter than me. Increasingly I am confronted with largely unrelated results (taking tweaked keywords or synonyms to my query as input apparently) as opposed to no results. So my conclusion is that the search engines increasingly see the need of search skills as an anti pattern they actively want to get rid of. reply layer8 24 minutes agorootparentOn the Google search results page, activate Search tools > All results > Verbatim. You can also create your own search provider bookmark with verbatim search as the default by adding “tbs:li=1” as a query parameter to the Google search URL. reply blixt 1 hour agoparentprevWhat I really hope this helps solve is covering for the huge lag in knowledge cutoff. A recent example is where it went \"oh you're using Go 1.23 which doesn't exist so that's clearly the problem in your Dockerfile, let me fix that\". But I'm not keeping my hopes up, I doubt the model has been explicitly fine-tuned to double check its embedded knowledge of these types of facts, and conversely it probably hasn't even been successfully fine-tuned to only search when it truly doesn't know something (i.e. it will probably search in cases where it could've just answered without the search). At least the behavior I'm seeing now from some 15 minutes of testing indicates this, but time will tell. reply hughesjj 1 hour agoparentprevI think it's more filling the niche that Google's self immolation in the name of ad revenue started. Besides kagi, there aren't really any solid search engines today (even ddg), and OpenAI has a reach way beyond kagi could dream of outside a billion dollars in marketing. reply lighthazard 1 hour agoparentprevLLMs really make it easy to quickly find documentation for me. Across a huge software project like Mediawiki with so much legacy and caveats, having an LLM parse the docs and give me specific information without me hoping that someone at Stackoverflow did it or if I'm lucky enough to stumble across what I was looking for. reply sebzim4500 1 hour agoparentprevEven if you are good at writing the queries, Google is so terrible that you end up getting some blogspam etc. in there (or at least I do). A model filtering that out is useful, which I find phind pretty good for. Hopefully this will be even better. reply melenaboija 48 minutes agoparentprevAny question that few months ago I would do to stackexchange (or expect and answer from, after a google seqrch) either coding or quantitative, I go to chat gpt now. I consider myself quite anti LLM hype and I have to admit it has been working amazingly good for me. reply layer8 45 minutes agoparentprevFor searches that remain inconclusive, I sometimes double-check with LLMs to see if I have missed anything. It rarely gives relevant new insights, but it’s good to get the confirmation I guess. reply kadomony 31 minutes agoparentprevI was skeptical of LLM search until I saw Arc Search in action with its \"browse for me\" functionality. reply paul7986 1 hour agoparentprevI use GPT for things that would require multiple Google searches (research). Some examples.. - I count calories... eat out always and at somewhat healthy chains (Cava, Chipolte, etc). Tell GPT what ive eaten half the day at those places and then later for dinner which it calculates a calorie count estimation for half the day and then later at dinner the remaining. I have checked to see if GPT is getting the right calories for things off websites and it has. - Have hiking friends who live an hour or two hours away and we hike once a month an hour or less drive is where we meet up and hike at a new place. GPT suggests such hikes and quickly (use to take many searches on Google to do such). Our drives to these new hikes learned from GPT have always been under an hour. So far the information with those examples has been accurate. Always enjoy hearing how others use LLMs... what research are you getting done in one or two queries which used to take MANY google searches? reply kjellsbells 24 minutes agorootparentGPT is proving useful for me where something is well documented, but not well explained. Case in point: Visual Basic for Applications (the Excel macro language). This language has a broad pool of reference material and of Stack Overflow answers. It doesnt have a lot of good explicatory material because the early 2000s Internet material is aging out, being deleted as people retire or lose interest, etc. (To be frank, Microsoft would like nothing more than to kill this off completely, but VBA exists and is insanely more powerful than the current alternatives, so it lives on.) reply timeon 1 hour agorootparentprevWith eating out so much, try to ask it about sodium intake as well. reply paul7986 57 minutes agorootparentyeah that is somewhat of a concern and have asked GPT that info / to calculate that too (though only a few times). reply photochemsyn 44 minutes agoparentprevIt seems good at finding relevant research papers. e.g. > \"Can you provide a list of the ten most important recent publications related to high-temperature helium-cooled pebble-bed reactors and the specific characteristics of their graphite pebble fuel which address past problems in fuel disintegration and dust generation?\" These were more focused and relevant results than a Google Scholar keyword-style search. However, it did rather poorly when asked for direct links to the documentation for a set of Python libraries. Gave some junk links or just failed entirely in 3/4 of the cases. reply moralestapia 1 hour agoparentprevGenuine answer: this was not made for you. There is a billion-to-trillion dollar addressable market, which you're not a part of. It was made for them. reply tempest_ 1 hour agoparentprevI think you need to define \"decent search skills\" since google will straight up ignore most boolean stuff or return ads. The LLMs are nice because they are not yet enshitified to the point of uselessness. reply carabiner 1 hour agoparentprev> Genuine question... When it starts with this you KNOW it's going to be maximum bad faith horsefuckery in the rest of the \"question.\" reply niam 37 minutes agorootparentI know what you're talking about, but also don't know how it applies in this case. Not a hater, and not asking rhetorically as a way to dunk on OpenAI. Just haven't found a use for this particular feature. Which is also exactly something a bad-faith commenter would say, but if I lose either way, I'd rather just ask the question ¯\\_(ツ)_/¯ reply carlesfe 1 hour agoparentprevI think this is just the first step for a full-featured agent that not only does searches for you, but also executes whatever was your goal (e.g. a restaurant reservation, etc) reply adamc 1 hour agorootparentTo solve that problem you have to solve all the issues that make me not trust the results. As search, it's fine, since I am perusing and evaluating them. But as an agent, hallucinations and inaccurate answers have to disappear (or very close to disappear). reply qwertox 1 hour agoprevMakes me question why Google never bothered to create something like search sessions which could be enriched with comments/notes and would be located in a sidebar just like the chats in ChatGPT/Claude/Mistral are. They really had the potential to do something interesting, but were just focused on their ad metrics with the \"good enough\" search box. What have they been doing all the time? reply Liquix 1 hour agoparentthe FAANG giants have been government assets for ~15+ years [0]. they don't have to turn a profit every quarter, innovate, or make their search any better because they no longer play by the rules a normal business does. they are a critical \"too big to fail\" component of the state's global surveillance system. [0] https://static1.makeuseofimages.com/wordpress/wp-content/upl... reply DSingularity 1 hour agorootparentNot just surveillance. Power projection. I wonder what impacts you can have on foreign economies by playing with quality of these tech giants outputs? reply unnouinceput 1 hour agorootparentprevOpenAI is Microsoft. Microsoft is a FAANG giant. reply lucianbr 1 hour agorootparentHow is that relevant? Microsoft bought OpenAI, didn't create it by R&D, so the assertion stands: giants don't do new things, for whatever reason. reply summerlight 49 minutes agoparentprevI guess now Google's search stack is too complicated and not many engineers understand what to do in order to introduce a novel, big feature integrated into the full stack vertically and horizontally. And those few who capable of doing so are probably completely out of bandwidth, so some random ambitious PM cannot pull their hands into uncertain green field projects. reply zelphirkalt 1 hour agoparentprevCollecting people's data and making money from that. reply arromatic 1 hour agoparentprevCan you tell me a bit more ? What do you mean by search session ? reply qwertox 1 hour agorootparentLet's see, if I go to \" ⋮ -> History -> Grouped History\" on the top right of the Chrome browser, I see a \"Search History\" ( chrome://history/grouped ). For example `8 hours ago: \"autohotkey hotkeys\"` with 4 links to pages which I visited while searching. But this is a Chrome feature, not a Google Search feature. https://myactivity.google.com/myactivity does (sometimes? can't see it right now) have a grouping feature of all the searches made, but this is more of a search log than a search management feature. So chrome://history/grouped is the closest to what I mean, but I can't pin or manage these history groups, enrich them with comments or even files, like pdf's which could then get stored in Google Drive, as well as get indexed for better searches. reply arromatic 1 hour agorootparentoh I thought you meant something like commenting under search result links like youtube videos. I might be mistaken but i think ff mobile does something similar of grouping search session reply blixt 1 hour agoprevOne thing that is quite unfortunate with the state of SEO and the web in general today is that when I asked \"what are the latest versions of common programming languages and when were they released?\" a large amount of the sources were \"13 Tools You Should Learn Now\" and the like. This might be a solvable problem within the search API they provide to the LLM, but for now I wouldn't trust current LLMs to be able to filter out these articles as less trustworthy than the official website of the programming language in question. reply jsheard 1 hour agoparentGiven how many of those SEO spam sites are themselves generated by ChatGPT now, OpenAI can simply back-reference their own logs to find out which sites are probably SEO spam while everyone else is left guessing. That's vertical integration! reply itissid 34 minutes agorootparentOr offer two search results when they suspect one is spam and see which one a user likes and train off of that, just the way they do now with ChatGPT. reply arromatic 1 hour agorootparentprevIf they do that , That's a genius idea. reply code51 1 hour agorootparentSo it'll turn to yet another arms race - similar to captcha, cybersecurity and nuclear weapons. SEO will use AI to fill in fluff inside AI-generated content (which is already done). It won't directly match ChatGPT logs and OpenAI would just be pouring precious compute to a bottomless pit trying to partial-match. reply DSingularity 1 hour agorootparentprevI’m sure they will be more subtle than that otherwise it will get circumvented. I’m sure they will/are tackling this at the model level. Train them to both generate good completions while also embedding text with good performance at separating generated and human text. reply sebzim4500 1 hour agorootparentWould someone even want to circumvent it though? Most sites won't care very much about encouraging scrapers to include them in LLM training data, it's not like you get paid. reply notatoad 46 minutes agoparentprev>what are the latest versions of common programming languages and when were they released? is this a real question you needed an answer to, or a hypothetical you posed to test the quality of search results? of course you're going to get listicles for a query like that, because it sounds like a query specifically chosen to find low-quality listicles. reply skydhash 36 minutes agoparentprev> when I asked \"what are the latest versions of common programming languages and when were they released?\" The issue is with the query itself. You're assuming that there's some oracle that will understand your question and surface the relevant information for you. Most likely, it will use the word themselves as part of the query, which SEO sites will exploit. A more pragmatic search workflow would be to just search for \"most common programming languages used\" [0], then used the Wikipedia page to get the relevant information [1]. Much more legwork, but with sources. And still quite fast. [0]: (Screenshot) https://ibb.co/ggBLy8G [1]: (Screenshot) https://ibb.co/H4g5bDf reply inhumantsar 1 hour agoparentprevthis is why I pay for Kagi. granted those results still come up, but you can block specific domains from ever appearing in the results and configure how listicles are displayed. reply arromatic 1 hour agorootparentHow many can you block and filter manually ? 10 ? 100 ? 10k ? Who will test sites for the blocklist ? The domain block feature is great but unless it's collaborative listing it's not gonna be super effective. reply hmottestad 58 minutes agorootparentIt’s super effective for me because I just block stuff as things pop up that I don’t want. I’ve also added more weight to certain domains that I want more results from. I wouldn’t want anyone touching my config, it’s mine and it works great! reply hughesjj 1 hour agorootparentprev.... Test sites for the blocklist? What? Also they do share the most blocked/raised/lowered etc sites: https://kagi.com/stats?stat=leaderboard We've had this problem of \"good defaults\" before with ad trackers blocking domains. I'm sure it'll be Sooner than later when some community lists become popular and begin being followed en mass reply arromatic 56 minutes agorootparentI meant your average user can test a handful of sites if they are seo spam or good sites but a single search return 10+ results and even more when a user searches multiple things , multiple times a day . Average user doesn't have the time to test these many websites. reply blharr 1 hour agorootparentprevKagi is admittedly pretty great for this. reply speedgoose 1 hour agorootparentprevAs an alternative, ublacklist is free and open-source. reply arromatic 1 hour agorootparentAverage serp page has 10 results . What if all 10 matches with your blacklist ? Not to mention you can't do anything if the engine dosen't search deeper . reply speedgoose 1 hour agorootparentYou probably have to browse to the next page or refine the search terms. reply ben_w 1 hour agoparentprevSEO spam is always going to focus on the biggest market, and by doing so they can be completely transparent and obvious to whoever they're not trying to fool. I'd assume right now the SEO target is still mainly Google rather than ChatGPT, but that's only an \"I recon\" not a citation. If and when ChatGPT does become the main target for SEO spam, then Googling may start giving good results again. reply adamc 41 minutes agorootparentWouldn't it be \"I reckon\"? :-) reply ben_w 14 minutes agorootparentD'oh, yes. :) reply hmottestad 1 hour agoparentprevFor Java I got: As of October 31, 2024, the latest version of Java is Java 23, released on September 17, 2024. The most recent Long-Term Support (LTS) version is Java 21, released on September 19, 2023. Which all seems correct and accurate. reply andrewinardeer 46 minutes agorootparentThis is because it is referencing and regurgitating Wikipedia articles. reply blixt 45 minutes agorootparentprevYeah I did also find it to be mostly accurate. However, seeing the sources I felt like I kind of have to check all the languages just in case it picked up information from a random \"X ways to do Y\" article that might not have been prioritizing accuracy. And for this search query I did see several languages' actual websites, but I did another very similar query earlier where 9 out of 12 results were all numbered list articles clearly intended for SEO. 2 of them were actual official sites. And 1 was what appears to be a decent attempt at talking about programming languages (i.e. not SEO only). reply benob 1 hour agoparentprevThis is the next step for SEO: be able to game ChatGPT prompts trying to filter out SEO crap... reply joshdavham 1 hour agorootparentHow do you think people will try to game AI-based search? reply 101008 1 hour agoprevSearch in the internet worked because people wanted to generate content to attract people to display ads or any other reason, but they wanted to attract people. If now my content is going to be ingested and shown by a LLM or AI agent, what's the purpose to give it for free? I know it won't happen, but I would love if this type of agents have to pay to show a summarization of another website. It's only fair when done in mass like this. reply pradn 41 minutes agoparentWell the whole point of this product is to link back to websites. There’s no necessary link between the text and the links, which are chosen after the fact from an index. That’s different from traditional search engines, where links are directly retrieved from the index as part of ranking. reply trump2025 58 minutes agoparentprevI think your comment highlights a very important shift in the market for ads and you are right that increasingly, the current atmosphere hints at there is little to no incentive to publish original creative work in the future if there is no compensation for it like Google had done. We've like reached peak human driven novelty (or McKennaists will argue it already happened around the mid 2010s) and we'll see AI driven novelty with the difference being it will be even a smaller group of people that are paid royalty fees. Once creative destruction reaches critical mass, we'll finally see billionaires and companies around the world succumb to demand for UBI. If you want to see the future just look at China. Billionaires are being hunted down and threatened to give up their offshore accounts. reply Xcelerate 2 hours agoprevHow long until advertisements are subtly introduced? I didn’t notice any specific brand of limoncello recommended in their demo. reply jsheard 1 hour agoparentProbably not long, some users already got A/B'ed into testing \"sponsored results\" https://i.imgur.com/UpAptFL.png reply FriedPickles 1 hour agorootparentThe response on the left references specific products, but where's the evidence that it's sponsored? reply alwa 1 hour agorootparentAside from the marketing-ish tone and specific deeplinks to product purchase pages, the prominent Amazon logo and product description headline implied some degree of affiliation to my eyes. It seems like the evidence is that it would be foolish not to take the money for presenting such an obvious referral of a motivated buyer. Frankly the example they posted seems like a fairly happy one, where the user is explicitly implying that they’re seeking a specific physical product to introduce to their life. We’ve all seen where those monetization incentives lead over time though. But you’re right—not even so much as a tiny word “Ad” like Google does… reply axus 1 hour agoparentprevI'd be happy to have another Google clone, that doesn't have a login and is not a chat session. Go to https://search.ai , type my search query and look through the results, with ads on the side. reply nuz 1 hour agoparentprevIt's already happened in a subtle way via who got to partner with them to be displayed in results vs not. reply BiteCode_dev 1 hour agoparentprevHonestly, if I can disable ads by paying them, then I'm ok with it. Google will suck all your data even if you pay, and link the entire earth of services to your identity. For now, chatgpt doesn't care, and I already pay for what they provide. May they kill Google. 20 years old me would freak out hearing me that, they used to be my heroes. reply zelphirkalt 1 hour agorootparentYou are thinking you can pay them to not use your data? Think again. They will sneakily use your data anyway. If not yours, then the data of people who do not change setting xyz. Oops, the last update must have reset that option for some users. reply arcticbull 1 hour agorootparentprevSo the issue is if you let people opt out by paying you’re left with a low intent, likely lower net worth group of people to advertise to. As a result those eyeballs are worth less. The advertisers will turn to other platforms if you only let the worst people see their ads. Unless enough people all pay, the whole thing stops working. But there aren’t enough people who will pay because most people don’t care. Tldr: the ad supported business model fundamentally doesn’t work if you let all your best products (you) opt out by paying. It requires them to pay an amount far in excess of what they would be willing to pay for the system to work. reply spearman 1 hour agorootparentThere's some truth to that, but Netflix, YouTube, etc seem to be OK with both ad-supported and paid ad-free versions, so I think the logic you described does not always dominate the considerations. reply arcticbull 40 minutes agorootparentI think you’re right that it’s not universal - maybe something to do with medium and attention? reply entropicdrifter 1 hour agorootparentprevYou either die a hero or mumble mumble reply croes 1 hour agorootparentprevYou can pay to get fewer ads reply swatcoder 1 hour agorootparentprev> Honestly, if I can disable ads by paying them, then I'm ok with it. The modern maxim is: any content platform large enough to host an ad sales department will sell ads Vanishingly few (valuable) consumers have zero tolerance for ads, so not selling ads means leaving huge sums of money on the table once you get to a certain scale. Large organizations have demonstrated that they can't resist that opportunity. The road out is to either convince everyone to have zero tolerance for ads (good luck), to just personally opt for disperse, smaller vendors that distinguish themselves in a niche by not indulging, or to just support and use adversarial ad blockers in order to take personal control. Hoping that the next behemoth that everybody wants to use will protect you from ads is a non-starter. Sooner or later, they're going to take your money and serve you ads, just like the others. reply breck 1 hour agoparentprevWhy would they ever want to sell ads? They did not get addicted to selling ads, have billions in revenue from paying subscribers, and don't have to wean themselves off of ads (as Google and Meta would love to do). reply disgruntledphd2 1 hour agorootparentBecause they are massively structurally unprofitable right now? reply kredd 1 hour agorootparentprevWhy make $1 when you can make $100? reply 23B1 1 hour agorootparentprevBecause Sam Altman needs to buy another Greubel Forsey, of course. reply findthewords 2 hours agoparentprevI hope very quickly. The sooner they start competing with Google for ads the better. reply solfox 1 hour agorootparentAre ads what people want? reply boweruk 1 hour agorootparentNo but once ChatGPT starts threatening Google's revenue model, maybe they will start putting effort into improving their drastically deteriorating search engine. reply Teever 23 minutes agorootparentBut why is that good for me? Why do I care if Google succeeds or dies? If anything I want them to die for ad infested they've made the internet. I don't want ads in either chatGPT or Google Search. reply riku_iki 1 hour agorootparentprevthey need to win search share to threaten Google's revenue model: take traffic from google.com, so google will sell ads. Going to ads busyness is not necessary for this. reply kaonwarb 1 hour agorootparentprevI don't want ads. But I can't deny that ads are the only business model with a chance of scaling to compete with Google. If that's what they want to do in this space, which is not a given. reply gk1 1 hour agorootparentprevPeople want whatever they searched for. If the ads provide that, then sure. That's why Google and Meta are the size that they are... reply croes 1 hour agorootparentMost of the time I don’t search for products so there is nothing I want to buy. reply goatlover 1 hour agorootparentprevI don't want ads when I search. reply sundaeofshock 1 hour agorootparentprevGoogle is the size it is due to monopoly power. reply moralestapia 1 hour agorootparentprevPlenty of times the answer is yes. reply schmidtleonard 1 hour agoparentprev2 years for ads, 6 years to remove the yellow background. reply M4v3R 1 hour agorootparentI think you’re being very generous with these 2 years. reply schmidtleonard 58 minutes agorootparentYeah, I suppose OpenAI also speedran the \"make noble promises to not become evil / become evil\" pipeline too. reply TZubiri 1 hour agoparentprev5 to 10 years reply josefritzishere 1 hour agoparentprevThey might wait a whole week. reply littlestymaar 1 hour agoparentprevNot long before it's forbidden by law with rules like “if you say the name of one brand, you must name at least two competitors” I suspect. reply KeplerBoy 1 hour agorootparentThat'll be the European version. reply littlestymaar 1 hour agorootparentDon't Americans also have rules about hidden advertising like that in regular media? reply bandrami 1 hour agorootparentThe American model prefers \"sponsored material should be identified as such\" though that's only active for broadcasting currently reply tiahura 1 hour agorootparentprevAmerican law generally favors freedom of expression. reply arcticbull 1 hour agorootparentThere are several classes of restrictions on free speech in the US. These include: obscenity, fraud, speech integral to illegal conduct, speech that incites imminent lawless action, speech that violates intellectual property law, true threats, false statements of fact, and —- most relevant here -— commercial speech such as advertising. Advertising has far less protection than is ordinarily afforded to the kind of speech you might do as a person. reply ryzvonusef 43 minutes agoprevhttps://x.com/sahir2k/status/1852038475606036577 > how tf is it reading private repos ?! reply thrdbndndn 21 minutes agoparentIt was also indexed by Bing. I usually assume good faith, but in this particular case, I believe the chance that this repo was public before and the author just changed it to private to bait attention is far more likely than Bing/ChatGPT can actually read private repo on GitHub. reply msoad 35 minutes agoparentprevWow! This is the real news here! reply gauge_field 26 minutes agorootparentAs one person pointed out in the thread, it also shows up on bing results, main repo, main.py file and releases page. But not on google. Edit: It also shows up on duckduckgo reply SethMLarson 2 hours agoprevHah, OpenAI is becoming an ads business too. So much for something new, same old funding model for every centralized platform on the web. reply troymc 1 hour agoparentOpenAI has ads? I thought it was mostly a freemium business model. reply SethMLarson 1 hour agorootparentI'm saying that by moving towards explicit \"search\" and \"linking to sources\" they have set the stage for being able to charge to be recommended by their search features (ie, ads and pay-to-rank, same as Google search). There aren't any ads in their demo, we haven't seen the real deal yet, but I'll be watching HN for that day. reply soheil 1 hour agorootparentWhy, just because search has ads therefore anything that is a superset of that must also? reply croes 1 hour agorootparentBecause ads bring money and companies love money reply littlestymaar 1 hour agorootparentprevOver the past two decades, ads have proven to be the only way to make money over the internet… reply breck 1 hour agoparentprevIf they are making billions from subscriptions, why on earth would they want to switch to an ads business? reply SethMLarson 1 hour agorootparentMaking billions but spending trillions for no moat (GPUs and models aren't moats) means that the only moat they have are users. Users aren't paying enough to offset costs, the only way to get value from non-subscription users for their scale is through ads. reply layer8 10 minutes agorootparentprevThey get inspired by streaming services doing the same. reply RodgerTheGreat 1 hour agorootparentprevMight have something to do with the fact that they're also still losing billions operating their services at a loss! reply croes 1 hour agorootparentprevTo make more billions reply jajko 1 hour agorootparentprevYou can ask the same for ie Apple where you pay a proper premium for products, yet their ad business keeps growing slowly into respectable proportions, and not by accident. reply short_sells_poo 1 hour agorootparentprevBecause it is never enough. We see this time and time again. Once they are making billions, the people in charge will demand that they start making dozens of billions, and then hundreds. The growth must never cease, because the moment you stop growing, you can't sell the dream that supports ridiculous PE ratios anymore. Google was a very profitable business 10 years ago and the search was still decent. In the last decade they absolutely butchered their core product (and the internet along with it) in an effort to squeeze more ad dollars out, because it's not the level of profitability that they need to maintain, but the growth of that profitability. Microsoft was a ridiculously profitable company, but that is not enough, they must show growth. So they add increasingly user hostile features to their core product because the current crop of management needs to see geometric growth during their 5 year tenure. And then in 5 years, the next crop of goobers will need to show geometric growth as well to justify their bonuses. Think about this for a moment: the entire ecosystem is built on the (entirely preposterous) premise that there must be constant geometric growth. Nobody needs to make a decision or even accept that this is long term sustainable, every participant just wants the system to keep doing this during their particular 5-10 year tenure. It's an interesting showcase of essentially an evolutionary algorithm/swarm optimizer falling into a local optimum while a much better global optimum is out of reach because the real world is something like a Rastrigin function with copious amounts of noise with an unknowable but fat tailed distribution.by a hedge fund professional. reply breck 2 minutes agorootparentThis is such a good rant, and I think you should develop it into an essay and I think there is an important catchy natural equation to mine here. reply soheil 1 hour agorootparentprevSlightly more accurate: they're raising billions making pennies. reply insane_dreamer 1 hour agorootparentprevthey're not making billions from subscriptions reply trump2025 47 minutes agoprevThis might be unpopular opinion but it really isn't as big of a deal as OpenAI makes it out to be (like their previous announcements) The truth is, I haven't used ChatGPT at all since spring of this year. Claude's Sonnet 3.5 has replaced it. I pay very little attention to what OpenAI releases and simply waits for Anthropic to implement it. I also started using Gemini which already outperforms perplexity and this and will not switch. I think everybody is constantly caught up with their infatuation with OpenAI and other characters that they don't realize Google, Anthropic are actually building a moat which some like Gary Marcus keeps rambling on as impossible I'm a realist and I can see that while Google has been slower to start, it reminds me of the search engine wars of 2000s, it is dominating and winning over users. reply illnewsthat 1 hour agoprevLooks like this was timed to coincide with Google adding search grounding data to Gemini API: https://news.ycombinator.com/item?id=42008834 // https://developers.googleblog.com/en/gemini-api-and-ai-studi... reply 7thpower 1 hour agoparentThey are taking a page out of Microsoft’s strategy of clouding out all sunlight. reply ncrtower 5 minutes agoprevIf you ask ChatGPT which search engine it uses, it will tell you Bing. And only Bing. reply grahamj 1 hour agoprevTo my mind one of the great benefits of LLMS is the possibility of searching without handing over some of the most personal information that exists - your search history. I’m happy OpenAI is advancing LLM-based search but I won’t be using it in earnest until it’s local. reply marcusestes 1 hour agoprevI like it. It's clean, fast, and the results seem solid. Google search has become so bloated with sponsored results that I've been hoping for a tool that could provide better results than DDG or Bing. I'm going to use this as my daily driver for a few weeks. The contemporary web is basically an epiphenomenon of Google, and they've failed to defend it. I hope OpenAI puts a huge dent in their market share. reply davedx 53 minutes agoprevI asked it to do its own DCF model of Paypal using current data and it did, using inputs from three different financial data sources. This is incredible and a direct threat to Google’s core biz. reply og_kalu 1 hour agoprev\"\"\"Search will be available at chatgpt.com (opens in a new window), as well as on our desktop and mobile apps. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. We’ll roll out to all Free users over the coming months.\"\"\" Can confirm that free users who signed up for the waitlist can use it right now (even if they didn't actually get in) reply rty32 49 minutes agoparentI signed up for the wait list as well and got the email. However there is no search button on the web interface (free tier). Is there anything I am missing? reply qwertox 1 hour agoparentprevCan confirm this as well. I switched to free around a month ago and got access to this today. I did join the waitlist some weeks ago. reply niemandhier 13 minutes agoprevI just checked, there are a lot of topics it will refuse to generate search results for. Sure normal search is policed too, but usually not based on moral judgments but on legal necessities. reply nextworddev 2 hours agoprevI have turned bearish on Perplexity recently, this confirms it reply keiferski 2 hours agoparentPerplexity is a really terrible name for a product and that alone will hold it back from being a real competitor. reply currymj 51 minutes agorootparentA good language model is one with low perplexity. https://en.wikipedia.org/wiki/Perplexity Reasonable name for a language model startup. reply keiferski 37 minutes agorootparentIt would be a logical name if its customers were technicians familiar with LLMs, and not end businesses and consumers. Which is why Ford wasn't named Internal Combustion Engine, Apple wasn't named Graphics Processing Unit, etc. reply Me1000 1 hour agorootparentprevIt's not like ChatGPT (or ChatGTP as half of people call it) is much better. reply DrBenCarson 1 hour agorootparentIt became a good name once it became a watershed viral phenomenon. Everything being equal yeah not a great name but it defined a new hype cycle so it got a pass reply woadwarrior01 1 hour agorootparentprevChatGTP and other variants with a Levenshtein distance of 1 from ChatGPT have been typosquatted to death by subscriptionware wrappers on the App Store and the Play Store. Many of them seem to be quite successful. reply keiferski 1 hour agorootparentprevChatGPT isn't a great name, but it's easy to say, spell, and remember. And at this point, a lot of people just know them as OpenAI, which is a great name. Perplexity sounds like a parody startup name from the Silicon Valley TV show. Way too complicated and unnatural. reply skybrian 1 hour agorootparentYou’re saying that now, but getting the initials in “ChatGPT” in the right order took a while to learn, so I wouldn’t say it’s easy to remember, and it seems easy to stumble saying it, too? It’s all about familiarity. Once people learn it, it’s not hard. reply keiferski 53 minutes agorootparentBut it didn't matter if they were in order or not, because ChatPTG or ChatTGP all go to the same place via Google, etc. It could have been called Chat + [Any 3 characters] and been fine. Perplexity is just a nonsensical word (for those unfamiliar with the concept) that is too long and hard to spell. They'd be better off just chopping it down to Lexity, or Lex, or Plexity, or Plex, etc. reply SG- 46 minutes agorootparentprevin French it translates to ChatFart when you read it out loud. reply woadwarrior01 1 hour agorootparentprevI've always thought that the name is very ironic and perhaps \"certitude\" would've been a better name. reply soheil 1 hour agorootparentprevAgreed, it's as if someone completely ignored the meaning of the word and just decided what sounds good for an AI app. reply marban 2 hours agoparentprevPerplexity was a cult in the first place. reply beng-nl 1 hour agorootparentI’m surprised at all the negativity on perplexity. I think it’s a great approach (base answers on sources) and their product seems to deliver on the premise. That said, anecdotally, I find it’s a bit hit-miss: if it’s hit it’s a huge improvement over google (and a minor improvement over chatgpt), if it’s miss it’s still good but get the feeling you won’t get anywhere further by asking more questions. reply forbiddenvoid 2 hours agoparentprevIt took me about 5 minutes to figure out that Perplexity wasn't the product I needed. I'm not sure this is either, but we'll try it out just the same. reply yungtriggz 1 hour agorootparent@forbiddenvoid what is the product you need? reply joshdavham 1 hour agoparentprevI actually like Perplexity a lot. It's really good for doing research. But if this new chatGPT search thing is better, I'm gonna switch. reply rvz 2 hours agoparentprevIt's clear as to where Perplexity is eventually is going, and it will likely get acquired by Amazon. Here's why: [0] [0] https://news.ycombinator.com/item?id=41121821 reply nextworddev 1 hour agorootparentOne thing about Amazon is that I have never seen them overpay for an acquisition (as in they really penny pinch and negotiate hard). So Perplexity’s high price tag may turn Amazon off reply hintymad 32 minutes agoprevI'm curious how ChatGPT Search improves recall and reduce spams. I can see how LLM helps find the most relevant content. However, it is still hard to find the right content, especially in a domain that has tons of spams. For instance, when I search for a product review on Google, I get back many review sites that look so legit that I have hard time telling which ones are spammers. reply itissid 38 minutes agoprevThere should(or will probably be) be a study on how long do people take on google etc vs search powered by chatgpt to get non-trivial work done, controlling for obvious factors like age, gender, country and industry If there is a bias towards chatgpt like tools of even ~5%, it would be worth investigating why this is. My hunch is just the conversational aspect of describing at a high level and finding answers and avoiding all the distraction of several dozen windows to do something is worth it. reply grbsh 1 hour agoprevIs this more than just ChatGPT with search API resulted concatenated the prompt? It feels like it might be. It feels tasteful in the same way that Apple ecosystem integrations just work really nicely and intuitively. But then again, there is an art to keying and retrieving embeddings, and it might just be that. reply jameslk 22 minutes agoprevWhat’s the benefit for websites to allow OpenAI/Microsoft to scrape and republish all their content without sending traffic back to them? It seems like these type of “search engines” will just get blocked. reply pton_xd 29 minutes agoprevThe social engineering possibilities with a tool like this are endless. Google already wields enormous visibility power but ultimately just provides a list of links to other sources. This can subtly (or not so subtly) rephrase and reshape the way we read about and think about every topic. reply jameshiew 1 hour agoprevThe new web search icon appeared for me straightaway in the ChatGPT macOS desktop app, within an in-progress conversation, without even having to restart. Before I'd even seen this official launch announcement. Very smooth! reply sidcool 2 hours agoprevIt will be fun to see how they stand up to Google and Perplexity. I feel they are a bit late in the search game, but excited to see what they cook reply 7thpower 1 hour agoparentI have learned to seriously question my instincts on when something is too late as there are many niches to fill and this is likely a building block for broader functionality. That being said, for all the talk about how bad google has become, I still prefer it to an unbroken bing. reply joshdavham 1 hour agoparentprev> excited to see what they cook Me too! I've really started to dislike Google search recently and am super excited we now have more viable options! reply toomuchtodo 2 hours agoparentprevAnyone can compete as long as they have a sufficiently robust crawl dataset as a foundation, no? reply baby_souffle 2 hours agorootparent> Anyone can compete as long as they have a sufficiently robust crawl dataset as a foundation, no? There's some sticking power/network-effect/sticky-defaults effects, too, though. It's _trivial_ to do a google search from anywhere on an android device with at most a tap or two. You can probably get close if a 3rd party has a well integrated native app but that'll require work on the user's behalf to make it the default (where possible). Same goes for the default search engine for browsers/operating systems ... etc. I will absolutely be firing off queries to google and GPTSearch in parallel and doing a quick comparison between the two. I am especially curious to see how well queries like \"I need the PCI-e 4 10-gig SFP+ card that is best supported / most popular with the /r/homelab community\" goes. Google struggles to do anything other than link to forums where people are already asking similar questions. reply vineyardmike 2 hours agorootparentprevAnyone can compete as long as they have a functional URL and web page. Doesn’t make them good competition, and doesn’t mean users will use it. The issue is that “AI search” has been a hot topic for a while now. Google (the default everywhere) just rolled out their version to billions of users. Perplexity has been iterating and acquiring customers for a while. Obviously OpenAI has great potential and brand recognition, but are enough people still interested in switching that haven’t yet? reply jsheard 2 hours agorootparentprevA fossilized snapshot will only get them so far, and sites are increasingly opting to block AI-related crawlers. Apparently about a quarter of the top 1000 sites already block GPTBot: https://originality.ai/ai-bot-blocking I guess they could be using Bing as their search backend, which would mostly get around the blocking issue (except for searching Reddit which blocks Bingbot now). reply toomuchtodo 2 hours agorootparentCertainly, countermeasures against crawler blocking will be a necessary component of effective search corpus aggregation in the go forward. Otherwise, search will balkanize around who will pay the most for access to public content. Common Crawl is ~10PB, this is not insurmountable. Edit: I understand there is a freerider/economic issue here, unsure how to solve that as the balance between search engine/gen AI systems and content stores/providers becomes more adversarial. reply jsheard 1 hour agorootparentAFAIK OpenAI currently respects robots.txt, so we'll have to see if they change that policy out of desperation at some point. reply andrethegiant 2 minutes agorootparent> AFAIK OpenAI currently respects robots.txt I wonder to what degree -- for example, do they respect the Crawl-delay directive? For example, HN itself has a 30-second crawl-delay (https://news.ycombinator.com/robots.txt), meaning that crawlers are supposed to wait 30 seconds before requesting the next page. I doubt ChatGPT will delay a user's search of HN by up to 30 seconds, even though that's what robots.txt instructs them to do. StableAlkyne 1 hour agorootparentprevIf it ends up anywhere near as popular as Google, those sites will have a financial incentive to allow the crawlers. The average person just does not discover content without the search engine recommending it. reply jsheard 1 hour agorootparentThe whole issue that site owners have with these AI search engines is that there isn't a financial incentive for them to cooperate, since the summarization largely replaces the need for users to click through to the site the information came from. No click-through, no ad impressions, no possibility of the user being converted into a recurring visitor or paid subscriber, just pure freeloading by the search engine. reply wiremine 1 hour agoprevI played around with it a bit, here are some hot takes. For context, I first tried this procession of searches on the Mac OS app. 1. \"Who won the world series\" 2. Who was the MVP?\" 3. \"Give me his bio\" My observations: 1. UX: The \"search\" button feels oddly placed, but I can't put my finger on it. But once I got it is a toggle, it wasn't a bit deal. 2. The first result had 3 logos, headlines and timestamps delineated, and easy to ready. The second one and third ones included a \"Sources\" button that opened a fly open menu. Clicking those opened a web link. The third result also included images in the fly open. 3. Citations were also inlined. The third result, for the bio, included a citation per paragraph. 4. It wasn't as fast as google. Which makes sense, given it's going through the LLM. But it will take a while to rewire my brain to expect slower responses to search. 5. Overall, I found the chat interface a very intuitive interface. The second search I asked was \"Give me a plan for a Thanksgiving meal.\" I to a long response that felt like a weird mashup of LLM-generated content and search results: 1. A list of menu selections 2. Links to some recipes 3. Prepration timeline 4. Shopping list 5. Additional tips There were 15 citations listed in the popup button, but only 3 inlined. This was... not great. A traditional list of search results feels better here. Overall, I like the direction. Innovation in search has been dead for close to 10 years, and this feels like I'd use it for certain inquiries. reply randcraw 1 hour agoprevI really don't see the value of summarizing/repackaging web search hits. Given that 99% of SEO-tuned web content is just shilling for vendors who don't want to be seen, LLM search summarization will just repackage those ads into a more palatable format that is LESS useful than the original, while more successfully hiding the obvious signatures that used to be a clear warning sign that... THE.FOLLOWING.CONTENT.IS.MANIULATIVE.CRAP. reply cloudking 1 hour agoparentI think the value here is not in searching for SEO crap, but turning it on when you want to get references to the most current information relevant to your query. For example, if you ask LLMs to build code using the three.js library, nearly all of them will reference version r128. Presumably because that version has the largest representation in the training data set. Now, you can turn this on and ask it to reference the latest version, and it will search the web and find r170 and the latest documentation to consider in it's response. I was already doing this before by adding \"search the web for the latest version first\" in my prompts, now I can just click a button. That's useful. reply snakeyjake 1 hour agoparentprevPeople who think AI summarizations are useful suck at reading. So they probably wouldn't notice the warning signs anyways. reply ghayes 1 hour agoparentprevI tend to agree. If I ask ChatGPT what is the best way to make pasta, it will pull from every source it’s ever been trained on. If it decides to search the web, it will mostly cater to one or two sources. reply arromatic 1 hour agoparentprevI think if they improve the algorithm maybe they can actually present seo free results. reply randcraw 1 hour agorootparentYou don't think SEO-LLMs will evolve to redirect search-LLMs to 'see the world' the way the SEO-LLMs want it to? I foresee SEO-LLM-brinkmanship as the inevitable outcome. Soon THIS will be the catalyst for the real Skynet -- battling smart ad engines. reply arromatic 54 minutes agorootparentOnly if openai is willing to play it . If they follow google than seo-llm won't even exist because there will be no need for it. reply awb 1 hour agoprevLLMs have the chance to cannibalize the web and become the primary interface for knowledge. But if websites remain the final destination, it’s good for content creators. The only other way to kill the web without killing LLMs in the process would be to create a way for people to upload structured public content directly into an LLM’s training. That would delay public content into release batches unless training can be sped up significantly. reply nextworddev 1 hour agoparent“Way to put public content directly into an LLM training” - sounds like Chatgpt reply awb 8 minutes agorootparentI’m imagining LLMs might eventually have an upload tool, similar to Google’s site map upload, for registering content proactively instead of needing to be discovered through crawling the web or training on chat data. reply jmakov 16 minutes agoprevSo basically what phind.com has been doing all the time? reply Lws803 1 hour agoprevI think generative search itself has room for disruption and I'm not too sure if a chat interface or a perplexity style one is necessarily the right way to go about it. I'd like to see search (or research in broader sense) a more controllable activity with the ability to specify context + sources easily in the form of apps, agents and content. reply ChrisArchitect 44 minutes agoprevnot sure what my use case for this would be if it's expecting me to type full descriptive sentences to check something quick or find a picture, and then read a whole paragraph of a useless reply. No results. No reddit posts (which aren't even what I want but understand a lot of content is buried in user-generated content) They seem to be implying this is the replacement for Google. This just isn't it. Edit: ohh, only Pro users? Right. ok. They made it seem like this was the big search launch and to go to chatgpt.com to get into it. Moving on. reply 7thpower 1 hour agoprevOpenAIs press release game is unreal. This totally overshadowed Google’s grounding release in my feeds. reply maleldil 38 minutes agoprevIf they make it possible to add this a search engine on Firefox like it's possible for Perplexity, I might drop Perplexity for good. reply amelius 27 minutes agoprevIronic that Google caused their own demise by publishing that Transformers paper. reply PittleyDunkin 1 hour agoprevIs there any indication they're willing to improve on google in terms of e.g. excluding commercial results? If not it's not clear how this improves anything. Google has been excellent at semantic search for a long time; the issue has been the lack of controls to filter out the SEO bullshit and to remove the AI stuff from the top and the right of the results. It's been way too easy to game search with sufficient funding for well over a decade now and the AI-generated crap is a long way from production-ready (in terms of quality; obviously it generates something). reply arromatic 1 hour agoparentYeah . It needs to filter seo optimized articles first and search more niche sites or it will be your average chat gpt with search project from github. reply josefritzishere 6 minutes agoprevThey're trying to find new places to cram their money-suck of a product in hope of pretending there is a revenue model. reply EcommerceFlow 1 hour agoprevI wonder if this is their own web scraper, or if they're using Bing API? As a very experienced SEO, this is pretty exciting nonetheless, a new front in the online war opening up. If they're using their own scraper/search algorithms, it'll be interesting to see how they weigh the winners and losers compared to how Google does it. reply GavCo 1 hour agoprev\"The search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview.\" More info on model distillation: https://openai.com/index/api-model-distillation/ reply holtkam2 1 hour agoprevHow is this different from RAG using a search API? I didn't get their blurb about this being a standalone fine-tuned model. reply DSingularity 1 hour agoprevIs this a move to try to setup a pathway for getting some data on realtime trends? In other words something for quickly getting some model updates for hot prompts like like “what happened in the debate”. Or is this something they’ve already solved? reply ionwake 1 hour agoprevCan someone give an example of the type of search query that this Search Engine would excel at? ( I tried getting the top hackernews posts but it was 5 days old? ) reply xnorswap 2 hours agoprev\"Introducing\" without actually just linking the search page. That's not an introduction, that's a teaser trailer. If they want this to be a viable search it needs to be available quickly, and anonymously from something quick to type in. Google would have been annoying as shit if you had to go to google.com/search , let alone then log in. reply solfox 1 hour agoparentIt's available today for Plus users at chatgpt.com. reply timeon 1 hour agorootparentUsing search with log-in is pretty big red flag for me. reply posterman 1 hour agoparentprevI mean, until there is an alternative in the space that has a (good), free, anonymous ai web search then I think we can probably assume you are confusing what you want with what is \"viable\" reply jayanth-vijay 1 hour agoprevIs there a new api model version available for search ? reply cryptozeus 1 hour agoprevI would be surprised if this doesn’t take share out of google’s pie reply wg0 1 hour agoprevIsn't this almost what's available in Gemini on Android phones already? kind of? reply torginus 1 hour agoprevHonestly I don't live this 'streaming LLM text effect' as well as the wordiness by which ChatGPT 'chats' with me. I consider LLMs to be machines, not conversation partners, and frankly I find the notion of chatting with an artificial being a bit creepy (unless it's specifically what I want as some sort of escapism). I wish they tried to be as terse as possible (and faster too). reply bagels 1 hour agoprevA new front has been opened in the SEO wars. reply zaptrem 1 hour agoprevCould OpenAI make it impossible for startups to try to build AI search engines by signing all these paid agreements with publishers? reply sergiotapia 19 minutes agoprevIt's crazy how much \"vibes\" affect perception of the product. OpenAI just always feels cold and alien to me. Compared to Anthropic and Perplexity's warmth. reply arromatic 2 hours agoprevCan it find obscure sites like marginalia does or personal blogs posted in hn or it's just another bing + ai summarizer ? reply ColinHayhurst 1 hour agoparentI asked for long tail blog posts about interesting places to visit in Paris. I got one result; from an obscure website called Vogue. reply grbsh 1 hour agoprevHow will this be gamed for neo-seo spam? reply wifipunk 1 hour agoprevLooks like they've also enabled advanced voice mode on the windows desktop app. Does not support search for anyone wondering. reply jayanth-vijay 1 hour agoprevIs there a new api model version for search available ? reply jdulay19 1 hour agoprevI wonder how long until it will offer reverse image search, too. reply vladsanchez 1 hour agoprevPerplexity does that already and more! shrug reply pvo50555 1 hour agoprevAxel Springer! Now that's high-quality news sources... reply andrewinardeer 1 hour agoprevDoesn't search porn. DOA. reply vzaliva 1 hour agoprevIf there is a way to add it as search engine to Firefox? reply typon 2 hours agoprevAsked it to generate code for a library that was released in the past year - GPT-4 couldn't do it and this one just did it flawlessly. I am super impressed. reply RobinL 1 hour agoparentSame! (asking it to write code for the foss lib I maintain). This is immediately very useful. reply alanfranz 1 hour agoparentprevHello bot. This is a search functionality not a new model. reply typon 1 hour agorootparentMaybe you don't understand how this works? It's able to query the relevant documentation, put it in its context and then use that to generate code. It's extremely relevant to giving existing models superior functionality. reply kristofferR 35 minutes agoprevThis is a way better UI than expected (I expected a totally separate search website, perhaps due to ignorance). I'm gonna use this a ton reply paul7986 2 hours agoprevhttps://chatgpt.com/?hints=search (handy link to it) reply sunaookami 49 minutes agoparentAnd https://chatgpt.com/?hints=search&q=%s if you want to add a custom search engine to your browser reply bityard 1 hour agoparentprevThat link just takes me to what looks like a normal chatgpt prompt. (I tried asking it the same things they showed in the article and I just get generic AI answers, not web search results.) reply bhy 1 hour agorootparentAre you plus or team user and logged in? The link is to the normal chatgpt prompt, but with the \"Search\" button enabled. reply paul7986 1 hour agorootparentIm not logged into GPT (am subscribed for this month tho on my iPhone but that's separate) and able to do a web search and or ask GPT a question. Actually i am logged into my iCloud on my macbook so guess that's why im seeing the search on that device of mine (not seeing on another where Im not logged into iCloud). reply DrBenCarson 1 hour agorootparentprevOnly available for Plus subscribers reply shitter 1 hour agoprevI asked it the current weather in my area and the temperature was off by 23 degrees F. reply Maxion 1 hour agoparentWhat the hell did you ask it / what was the source? I just did the same thing and it gave me the correct answer and used my countries best known meteroligcal site? reply shitter 1 hour agorootparentIt used good sources but appeared not to extract the information correctly. Repeating the same query in the same chat session gave me an accurate answer. reply phreeza 1 hour agoprevI gave it a quick spin and my initial impression is much worse than perplexity. reply faragon 1 hour agoprevWhat I find incredible is that Google has had the knowledge and resources to do this for at least five years, yet they're still milking the \"old cow\". It reminds me of Intel sitting on their money while a near-bankrupt AMD sped past them. reply moralestapia 1 hour agoparentGoogle has the web on its hands, but they also have Puchai which is a -100x multiplier. reply moralestapia 1 hour agoprevThis is great, I can't wait to get rid of Google and all the crap that comes with it. Hopefully this also provides a strong negative force against SEO and, again, all the crap that comes nowadays thanks to Google. reply some_furry 1 hour agoprevSo glad that we're boiling the ocean for this. reply soheil 1 hour agoprevI made one very similar it's basically a wrapper around duckduckgo https://foxacid.ai reply rvz 2 hours agoprevAKA Bing Search in ChatGPT. So it is not it's own search engine and is still using Bing for its results just like the rest of them. reply solfox 1 hour agoparentReference? This isn't mentioned anywhere and certainly is not implied. reply sidcool 1 hour agoparentprevIs it mentioned on that page? Didn't see it. reply tredre3 1 hour agorootparentTo provide relevant responses to your questions, ChatGPT searches based on your prompts and may share disassociated search queries with third-party search providers such as Bing. For more information, see our Privacy Policy and Microsoft’s privacy policy. ChatGPT also collects general location information based on your IP address and may share it with third-party search providers to improve the accuracy of your results. https://help.openai.com/en/articles/9237897-chatgpt-search (TFA links to it in the How it works section) reply sidcool 1 hour agorootparentThanks. reply DrBenCarson 1 hour agoparentprevNo, it's using the Bing index. The generated responses are OpenAI Many search engines use the Bing index but return different results reply findthewords 2 hours agoparentprevIf it's better than Bing or Google in presenting the relevant result in a condensed way, it's still a win for the users. reply arromatic 1 hour agorootparentWhy would i want condensed results ? Why Do you think i would want to have a condensed version of this post [0] for example. [0] https://danluu.com/ballmer/ reply PittleyDunkin 1 hour agorootparentprev> If it's better than Bing or Google in presenting the relevant result in a condensed way This doesn't matter if the results are user-hostile, as both search engines are. reply nerdponx 2 hours agoparentprevThat makes for a fair side-by-side comparison, then. reply taco_emoji 1 hour agoprev [–] whatever reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "ChatGPT Search is a new feature by OpenAI that integrates traditional search engines with large language models (LLMs) to enhance online information retrieval.- The feature aims to counteract the prevalence of SEO-optimized content in current search engines by prioritizing user-desired content.- Initially available to ChatGPT Plus and Team users, there are plans for broader availability, though concerns exist about potential ads and filtering SEO spam."
    ],
    "points": 298,
    "commentCount": 273,
    "retryCount": 0,
    "time": 1730392882
  },
  {
    "id": 42004206,
    "title": "SSH Remoting",
    "originLink": "https://zed.dev/blog/remote-development",
    "originBody": "For people with large codebases, programming on a laptop can be overwhelming. Your fans are constantly spinning, the language server is continually out of memory, and rebuilds take forever... Now, in Zed, you can open a project on any machine you can SSH into. The UI runs fully locally to give you 120 frames per second of perfectly configured graphics, but with all the gubbins: language servers, tasks, and terminals run on the remote server where they can take advantage of cloud hardware. It's simple to use for one-off projects: zed ssh://my-host/~/code/zed And you can configure longer-lived connections as you need: The \"Remote Projects\" UI in Zed. For more information, see the documentation. Building Remote Development We've been working on our remote development feature for a while. While Zed is built for remote code editing, changing the infrastructure to support SSH required solving a whole bunch of sub-problems, from SSH connection maintenance, to how we build the remote server, to integrating the feature into everything else we have in Zed. For the SSH connection, we use the ControlMaster setting to maintain a single connection to each host. This means that you can open new terminals and spawn tasks without having to retype your passphrase or re-authenticate. Once connected, we download the remote server for your operating system and architecture. Unlike our normal Linux builds, the remote server can be compiled with musl, which requires no dynamic linking. This lets it work on older distros (where before we ran into compatibility problems with glibc) and on modern share-nothing distros like Nix that don't have a global set of libraries to dynamically link. Once we've established the connection and installed the remote server, we initialize it as a daemon, so that when connections do drop the remote server continues running and on reconnect your language servers are still fully initialized. We also back up any unsaved changes locally, so you never lose your work. The final piece of the puzzle was making SSH projects work with collaboration. This has been a real stress test of our collaborative syncing protocol as there can now be at least four different nodes involved in a 2-person collaboration over SSH. We had to rewrite our Project, and split it into logical chunks that could be enabled in remote and local modes depending on whether your client is the collab host, the ssh host, or the collab guest. We also have some new, fun tests that instantiate each of these roles, and our collaboration server, and ensures that the synchronization is done correctly. If you're working on a project with a friend or colleague, it should be completely transparent to them whether the project is on your laptop, or on a machine you can SSH into. Please try it out today and, as always, leave us feedback in GitHub Issues or Discord. Looking for a better editor? You can try Zed today on macOS or Linux. Download now! We are hiring! If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.",
    "commentLink": "https://news.ycombinator.com/item?id=42004206",
    "commentBody": "SSH Remoting (zed.dev)251 points by ingve 11 hours agohidepastfavorite194 comments keybits 10 hours agoZed with SSH Remoting and Orbstack is pretty much my dream setup for programming on a Mac. I can now spin up a Linux machine in Orbstack[0] in a few seconds and then SSH into it from Zed for a fast Linux development environment with a fast macOS native editor. It feels a bit like the macOS version of WSL and VSCode. Just a whole lot nicer (subjective of course)! A couple of years ago I was inspired by what Mitchell Hashimoto was doing[1]. He was running a GUI VM via VMware on macOS so that he could have the best of both worlds - macOS apps and ecosystem and a Linux dev environment with his preferred package managers and a more reliable NixOS. That route still felt a bit heavy to me, but I related to the desire for the best of macOS and the best of Linux. I tried with VMware and Docker Desktop with VSCode but it always felt like a lot of overhead and a bit clunky to achieve a smooth fast dev environment. With Zed and Orbstack it finally feels like the fast elegant system I'll stick with. Thank you to the developers for these excellent tools! [0] https://docs.orbstack.dev/architecture#linux-machines [1] https://x.com/mitchellh/status/1346136404682625024 reply mbreese 8 hours agoparentI’ve been using Colima to host docker containers and VSCode with devcontainers to do something similar. Being able to spin up a full stack environment on the fly for working is amazing. It may not be as fast as Zed, but it’s a well thought out workflow that’s very reproducible. It’s been a quick workflow to start using, but is still quite flexible. Regardless of which stack you use, I think this is a great style of working. When you have a reproducible dev environment, everything is so much easier. (As vagrant users will probably attest to - it’s not a new method). But having this feature of SSH remoting is also quite handy without the dev environment stack stuff. I also find it quite nice to work with a local client, but remote SSH environment when I’m doing data work (large genomic data files). In this case, instead of reproducibly, the main benefit is data locality. If my large dataset is on a different server, it’s far more convenient to write the analysis code on that remote server. This new Zed feature seems like it will also support this workflow, which is also a win in my book. reply tiffanyh 7 hours agoparentprevI understood Mitchell intent for his setup to be slightly different than what you described. I understood it as him running Nixos/nix because it made having a reproducible development setup a breeze since it’s all just a config file. How would his use of Nixos change as you imply with Zed/SSH? reply keybits 4 hours agorootparentI might have implied the wrong thing. I wasn't implying Mitchell should change his use of NixOS. I was relating to the desire for 'best of macOS and best of Linux'. If I was using NixOS I would do that from an Orbstack machine Linux cli, with Zed as my editor, rather than virtualisaing a full X GUI. Everyone will have their own choices of course. reply rfoo 9 hours agoparentprevI've always been running a headless Linux VM wherever I do any dev work, Windows or Mac. Works very well. Glad to see that WSL2 and OrbStack promotes this workflow. For now my only complaint is I prefer full-blown VMs instead of WSL-like setup (so that I can work with the same Linux kernel my code targets), but client virtualization support on Apple platforms are pretty meh, definitely not at Hyper-V level yet. OrbStack went out of their way to fix a lot of these but they are not interested in making a VM product. reply 1oooqooq 7 hours agoparentprevnext [2 more] [flagged] danmur 6 hours agorootparentI'm charitably reading it as \"if I must develop on a Mac, this is a great way\" :) reply pseudony 10 hours agoprevCan someone explain. What is the catch ? Zed is worked on by paid employees. So who is the product, how is the money made and is it open source and if so, how much ? (Vscode has strings attached too) Just interested since building my workflows around a company's products usually ends in tears (figuratively). reply krelian 10 hours agoparentThis is answered in their FAQ: https://zed.dev/faq : We envision Zed as a free-to-use editor, supplemented by subscription-based, optional network features, such as: Channels and calls Chat Channel notes We plan to offer our collaboration features to open source teams, free of charge. reply shafyy 8 hours agorootparentFor a VC-backed company, this will never ever generate enough revenue growth. So, the real answer is: \"If all goes well, we will keep raising VC money and get acquired and shutdown / butchered in 5-10 years\". Like Bluesky, this monetization plan they present is just something to calm down the haters and sceptics. But if you think for two seconds, it does not make sense. reply benterix 8 hours agorootparentIt might happen the way you describe, or, if they get more popular, then may try to think harder and offer more paid options. Now they're in a growth stage - they need to convince the largest possible number of developers they should switch from VS Code, and that is already a difficult task. reply xg15 7 hours agorootparentI know, this is the standard VC playbook: First growth at all costs to get everyone into the ecosystem, then pull up the net and monetize. The latter phase is usually when all the subscriptions, value-add nag screens, data sharing agreements and other enshittification goodies pop up. And how would it work otherwise? You can't perpetually offer a product for free (in all senses of the word) AND satisfy exponential ROI expectations at the same time. If this is what's going on here, I'm worried what the \"monetization\" phase will entail. reply hu3 7 hours agorootparentThe other option I see is being owned by a behemoth like Microsoft, in VSCode case, who can pay millions per month in engineering salaries and PaaS, while keeping it free. They are so big they can monetize it using Copilot or not even monetize it properly, just to get good faith from Developers, Developers, Developers. I like it. reply shafyy 3 hours agorootparentNo, the other option is something like Sublime Text that has a small team working on it and is paid for by a very fair one-off payment by the customer. reply hu3 3 hours agorootparentSounds like a third option. I have a Sublime license and I like it. But no need to discard VSCode. They are different tools anyway. reply shafyy 3 hours agorootparentprevAnd while doing this, they fuck the whole market. reply JeremyNT 5 hours agorootparentprevWell as an end user this is pretty much OK right, so long as you are aware of the deal? VC funded companies have subsidized and then abandoned a lot of useful OSS code over the years. Whenever you encounter a VC backed open source project you know the rug pull will come eventually. It doesn't make the code they have written any less useful. If the tool is good enough then it will be forked and live on. reply shafyy 3 hours agorootparentNo, it's not ok. It fucks the whole market by subsidizing the growth with VC money. Either a big company will buy them, consolidating their power, or they will IPO and the VCs will cash out and let the public bear the cost. It fucks the customers who are not as well-informed about this as you are. In summary, it fucking sucks. reply gigatexal 8 hours agorootparentprevThat’s the thing. Whatever editor I finally migrate from neovim to I’ll likely donate to. I get so much insane value from open source and companies building for the platforms that I use that I’m happy to pay for them especially for small shops of really passionate hackers like the folks behind Zed. reply tjoff 7 hours agorootparentYou can donate to neovim too ;) I don't have much hope in anything replacing it for me. Zed does a lot right but feels very very far away and with a weird focus (from my perspective). reply gigatexal 1 hour agorootparentHaha I already do! reply IshKebab 9 hours agorootparentprevThis seems pretty optimistic to me. I can't imagine any company I've worked for paying for those things. Especially because they're only going to be useful if everyone uses Zed which is unlikely. reply larodi 9 hours agorootparentIndeed seems very uncommon where I am, even the IntelliJ’s are more often pirated than paid for. From my perspective it is very weird to expect to beat MS (who produces two great IDE’s, not one) on their own game with this approach to dev. I’m not saying it’s impossible to have the most important features replicated, but, c’mon, this is software, it breaks as it builds and some things are not possible Overnight even if you have the worlds top top top talent around. reply madeofpalk 8 hours agorootparentYou don't need to beat microsoft in order to have a viable business. You just need to get enough market share. reply mbreese 8 hours agorootparentBut, you also have to be better than what’s freely available by enough to get someone to pay for it. Having a good product isn’t good enough, you have to be significantly better. Dev tooling is a notoriously difficult space to make money. Free tools tend to win because if a tool costs money, a developer is just as likely to write their own version. (For better or worse) reply otabdeveloper4 6 hours agorootparent> Free tools tend to win because if a tool costs money, a developer is just as likely to write their own version. No, free tools win because a proprietary license is a rent-seeking ball and chain on your means of production. reply hu3 7 hours agorootparentprevBtw Jetbrains made Webstorm (web editor) and Rider (.NET C#/F#) editor free now for non commercial use. This might hurt Zed. https://blog.jetbrains.com/blog/2024/10/24/webstorm-and-ride... reply jbrr 9 hours agorootparentprevReally? I’ve had employers pay for my Jetbrains subscription. reply Philip-J-Fry 9 hours agorootparentWith Jetbrains you're paying for the IDE. Zed's vision is that the editor is free and you're paying basically for Teams-lite. If you're a big company, you're already using a service like Teams, Slack, Zoom, etc. If you're a small company, then there's free alternatives like Discord. And say you want to talk to someone other than a programmer, then you'd need something else anyway. Because a project manager wouldn't need an IDE installed. reply ecmascript 9 hours agorootparentprevBut with Jetbrains products, you pay or the entire software, not just part of it . reply herval 9 hours agorootparentCommunity edition is free, and it’s a sizable chunk of the entire product reply uludag 9 hours agoparentprev> Vscode has strings attached too If software being FLOSS is in any way an important value of yours, the difference between Zed and Visual Studio Code is night and day: Zed's GPL: https://github.com/zed-industries/zed/blob/main/LICENSE-GPL MS Visual Studio Code's license: https://code.visualstudio.com/license I believe this alone shows much more good will towards developers and much less strings attached. reply z33k 8 hours agorootparentFor what it's worth; Visual Studio Code and VSCode are not the same. VSCode is MIT licensed: https://github.com/microsoft/vscode/blob/main/LICENSE.txt From vscodium.com: Microsoft’s vscode source code is open source (MIT-licensed), but the product available for download (Visual Studio Code) is licensed under this not-FLOSS license and contains telemetry/tracking. According to this comment from a Visual Studio Code maintainer: When we [Microsoft] build Visual Studio Code, we do exactly this. We clone the vscode repository, we lay down a customized product.json that has Microsoft specific functionality (telemetry, gallery, logo, etc.), and then produce a build that we release under our license. When you clone and build from the vscode repo, none of these endpoints are configured in the default product.json. Therefore, you generate a “clean” build, without the Microsoft customizations, which is by default licensed under the MIT license The VSCodium project exists so that you don’t have to download+build from source. [VSCodium] includes special build scripts that clone Microsoft’s vscode repo, run the build commands, and upload the resulting binaries for you to GitHub releases. These binaries are licensed under the MIT license. Telemetry is disabled. reply uludag 8 hours agorootparentWhat I find unfortunate is that VSCode and Visual Studio Code are so conflated. It's Visual Studio Code in the context of features, it's vscode in the context of license and community. In this sense Zed is much cleaner as to what's up for for monetization. You look at the project and you know which parts will be candidates for business growth and which parts will stay in the community. With Visual Studio Code, the entire editor is game for monetization and utilization for Microsofts business goals, yet has the appearance of open source and community owned. reply danmur 5 hours agorootparentMicrosoft are expert at hiding the strings. I prefer obvious strings if there have to be strings attached. reply bananapub 7 hours agorootparentpreva very important difference is that MS doesn't let you use some of their quite nice language servers unless you're using VS Code, e.g. Pylance: https://github.com/microsoft/pylance-release/issues/483 for this reason, the non-MS-built VS Code isn't equivalent to theirs, regardless of the license of the editor itself. reply eddythompson80 8 hours agorootparentprevhttps://zed.dev/terms has the Zed Editor use license. That’s different from the Zed source code license. reply afandian 7 hours agorootparentEnd you’re not allowed to reverse engineer the editor even though it builds from open source. I wonder how much legal reasoning went into that decision. It might be boilerplate, or it might be necessary for some other part of the contract. reply eddythompson80 7 hours agorootparentI don’t know. Technically, you can clean-room reverse engineer a GPL binary and your resulting code doesn’t have to be GPL. But you could also just build the open source repo and do that. So I don’t know reply pimeys 9 hours agoparentprevI would be ready to pay money for Zed, but I'm happy it's all GPL including the dev server (AGPL). Even if this business does not work, we have the code and the community and all parts of the editor available. Thank you for the team for this. reply blackoil 10 hours agoparentprevHappy path is Open-Core where all enterprisesque features and plugins are part of a commercial pro model. Unhappy path involves switching licenses sometime later or project simply dying because of ROI. reply pimeys 3 hours agorootparentYep. Maybe a good copilot written in Rust and tightly integrated into Zed. Offer a subscription model. I'd pay for that. reply keyle 10 hours agoparentprevYou are the product. The editor is free afaik, and is very good. It has a plugin system and many languages have been added. It's really, really close to be my replacement to intelliJ. Sadly many bugs are stopping that for now. They'll charge for integrations later, collaboration etc. I'm not sure exactly where the money is going to come from but I value a quality native IDE. I hope they don't stuff crap down our mouth in the future and simply charge a one time fee per version. But that of course is unlikely to happen. Every new version of IntelliJ I gasp that they F'd with something, and usually I'm right. reply monooso 8 hours agorootparent> You are the product. As far as I can tell from the Zed FAQs and license, this is false. Can you provide information to support this assertion? reply TiredOfLife 6 hours agorootparentThat's just the old copypasta: \"If you don't pay for the product, you are the product\" reply TylerE 10 hours agorootparentprevI’ll have to give it a look. I’m an increasingly disenchanted IntelliJ customer also. reply gregoryl 9 hours agorootparentI assume they did the same as with Rider, and made the UI objectively worse, rather than solve a million other things that actually need improvement (performance!)? reply keyle 8 hours agorootparent10 years ago, Intellij, 1 IDE to rule them all they said. There are 14-15 at my last count. Complete cash job. Bugs I reported went unanswered for 3 years and then swept under the carpet, still not fixed. Obvious stuff too. Breaking and annoying stuff. The best part was when they came out and claimed they had the best latency to screen rendering, long article explaining the technology etc. behind it. Mac just came out with retina screen, and their stuff is written in Java, did not handle any kind of graphics acceleration. It was a complete joke, just pressing 'aaaaaaaaaaa' on the keyboard would start lagging after 10 characters to seconds long to render. Insanely funny stuff. If you shrank the IDE to 1/4 of the screen it was much faster! yay. I stopped paying a little while back and jumped on the community edition for debugging and other IDEs to write the code. I'm sick of their antics. reply Barrin92 9 hours agorootparentprev>It's really, really close to be my replacement to intelliJ Isn't Zed just a text editor? The last time I checked it out it didn't have any debugging features which doesn't really make it a replacement for an IntelliJ IDE. reply hu3 7 hours agorootparentWow it still doesn't support debugging it seems. https://github.com/zed-industries/zed/issues/5065 People who use Zed now at work, sorry but you still print variables to debug issues? In 2024? Zed is investing in features to satisfy shareholders like AI and team collab but ignoring a basic functionality like interactive debugging is mind blowing. reply TiredOfLife 6 hours agorootparentprevZed is closer to VS Code than an ide like intelliJ or text editor like Sublime reply unsnap_biceps 11 hours agoprevJust a FYI, this feature will install a binary on the remote host and run it. > Your local machine will attempt to connect to the remote server using the ssh binary on your path. Assuming the connection is successful, Zed will download the server on the remote host and start it. https://zed.dev/docs/remote-development reply lifthrasiir 10 hours agoparentAs long as that is notified and correctly authenticated, it is a reasonable approach because the remote process can progress without having to send the whole workspace through SSH. I also expect most language servers would have to be on remote for the same reason. reply otabdeveloper4 6 hours agorootparentIt's not a reasonable approach because these binary blobs they're installing are guaranteed to be non-functional unless the target OS somehow happens to be the exact distribution and version the binary blob developers are testing for. (You could theoretically make a truly portable and hermetic binary blob with something like WASM, but I guarantee you this black magic is not accessible to IDE developers.) reply miloignis 4 hours agorootparentThey address this in the article - they're statically linking with musl, which should address most distros, and their phrasing indicates that they're testing multiple distros too. > Unlike our normal Linux builds, the remote server can be compiled with musl, which requires no dynamic linking. This lets it work on older distros (where before we ran into compatibility problems with glibc) and on modern share-nothing distros like Nix that don't have a global set of libraries to dynamically link. reply lifthrasiir 5 hours agorootparentprev> these binary blobs they're installing are guaranteed to be non-functional unless the target OS somehow happens to be the exact distribution and version the binary blob developers are testing for. Whoever makes publicly available binary blobs are necessarily aware of all those issues and try to engineer them to be resilient enough for most environments. That's why they typically work well even though they generally have little information about the system besides from most basic ones (ISA, OS/kernel and ABI to be precise). This is significantly more difficult in Linux due to its higher variability, but not even close to impossible. reply echoangle 10 hours agoparentprevThat's what VS Code does too, BTW. reply jiripospisil 9 hours agorootparent..and is not even open source (EDIT: I'm wrong, see below). Zed's remote server on the other hand seems to be OSS. One could probably install it themselves without relying on the automatic download. https://github.com/zed-industries/zed/tree/633b665379c18a069... reply eddythompson80 9 hours agorootparent? The remote server (reh) is part of the vscode repo itself https://github.com/microsoft/vscode/tree/main/src/vs/server/... there is a vscodium and codeoss builds of it too (reh builds). The extension that invokes it isn’t, but there are a few implementations of it that are, and I think vscodium bundles one with their build. It’s just some shell scripts that download the reh build and run it with the correct args. reply jiripospisil 8 hours agorootparentHmm, are the docs outdated then or are they talking about something else? > The Visual Studio Code Remote Development extensions and their related components use an open planning, issue, and feature request process, but are not currently open source. https://code.visualstudio.com/docs/remote/faq#_why-arent-the... reply eddythompson80 8 hours agorootparentThat’s the extension which launches the server. The server itself, which is what runs on the remote machine, is part of the vscode repo itself. You can see this open source extension for an example of what the extension does as well as an explanation of what it does https://github.com/xaberus/vscode-remote-oss Mainly just sets up the UI/connect commands in vscode UI and launched REH (the vscode server) on the remote machine. The actual server is just a different binary you build from the same repo. Non-Microsoft OSS builds of vscode work with SSH fine. reply jiripospisil 8 hours agorootparentThank you for pointing this out. reply echoangle 9 hours agorootparentprevInteresting, I didn’t know the VS Code server component wasn’t open source. reply hu3 7 hours agorootparentit is though reply BostonFern 10 hours agoparentprevWhat’s the resource footprint of that binary? The VS Code equivalent has been unsuitable for resource-constrained servers in my experience. reply szundi 10 hours agoparentprevWhat a perfect attack vector even if Zed and Vscode people are completely honest reply eddythompson80 9 hours agorootparentWhat’s the attack vector? Someone with ssh access to your machine can run code on your machine? That’s part of ssh. reply fredoliveira 8 hours agorootparentPresumably the attack vector is that someone malicious at Zed or VSCode could change the code in the server components to introduce a backdoor that could be used later. reply shark1 10 hours agoparentprevIntellij IDEA does that too, and the installer is a very big file. I suspect an entire headless IDE is installed in the remote host. reply Aaron2222 9 hours agorootparentThat's how most of these remoting features work, by running the IDE backend on the remote host. I'm not sure if IntelliJ IDEA has an equivalent, but CLion has an option to just do the build and run/debug on the remote host without running the whole IDE on it. reply sakjur 9 hours agorootparentprevJetbrains provide an option to use their lightweight remote development gateway[1] and not have the IDE installed on the local machine at all. [1] https://www.jetbrains.com/remote-development/gateway/ reply TiredOfLife 6 hours agorootparentGateway is just a launcher. It installs the ide both remote and local. reply lionkor 11 hours agoprevI switched to Zed for C, C++, Rust, Angular, and am extremely happy with it. I use it with it's vim mode, which is very good. For the record, I've tried the JetBrains suite for at least a year, vscode, vim, neovim, visual studio (windows), qtcreator, and at least one more I dont remember. Zed is superior for everyday coding, for me. The only thing it lacks is debugging, so I can't use it for C#, but I also can't use vscode for C# so that's understandable. And man, its so snappy. reply OptionOfT 2 hours agoparentWait? How do you debug C, C++ and Rust then? reply pimeys 11 hours agoparentprevI've been using vim since the 90's, then emacs and helix. Never a GUI editor, always terminal. This year I just tried Zed and I'm super happy about it. It's open source, they offer a flake in their repo for me to get the latest version compiled to my NixOS installations if I want. It's fast, their LLM features are tasteful and they come with the open source version of the editor. It's written in Rust which is my main programming language. This SSH feature is great, I can just remotely turn on my workstation at home and use it with Zed over tailscale. I'm also waiting for a good debugger story, especially for Rust. It will come... reply b4ckup 11 hours agoparentprevI use vscode (with omnisharp not c# devkit) for c# every day. Did you try it out? reply leosanchez 10 hours agorootparentIs devkit better than Rider ? reply b4ckup 7 hours agorootparentI tried to use vscode c# devkit but it is horribly unstable and has severe bugs that make it unusable for my projects. But I'm really happy with vscode + omnisharp. I never used rider so I cannot really say which is better but I tend to not like jetbrains IDEs (from experience with IntelliJ and webstorm). reply Alifatisk 5 hours agoprevHow are people so happy with Zed? I've tried it, it's fast sure, but it lacks some very essential features like inline error highlighting and button for running code like how Intellij has it, there is no extensions that cover this either. At the moment, the Zed IDE is more like an lightweight ai-assisted text editor for me. reply fordsmith 3 hours agoparentI think that is precisely why people, and myself, ate happy with Zed. I don't, personally, care for those kinds of features. I want something that is snappy, and bloat free. I beloved extensions are planned so that will come in time. I'm terms of inline error highlighting, it absolutely has that. is your LSP just not working correctly? reply Alifatisk 3 hours agorootparent> is your LSP just not working correctly? I don't know, I can't tell, but I would guess it does not. I just got the one and only Java extension there is (the other has has something to do with Eclipse). Yet, no error highlighting. reply greener_grass 9 hours agoprevI wonder if image based screen sharing (e.g. Google Meet) is the \"worse is better\" here? Zed collaboration sounds great, but what about all of the other apps I use? Image based sharing supports everything (poorly) automatically. reply zifpanachr23 10 hours agoprevI'd really like a good remote editing solution that is genuinely portable. I don't think there's anything technical stopping that from happening (and I've tried some half baked solutions along those lines before). For a variety of reasons relating to my line of work, this or vscode's solution are no gos since they require you to install a server on the remote, and that server is interpreted or compiled in a language that is absolutely not guaranteed to be present or stable on said remote (I'm saying my ideal extension, if a server is really necessary, would be in a reasonably widespread subset of C, perhaps ISO c99, which I'm sure I'll catch some flack for saying). That is an underserved (although perhaps not large, but probably they would be enthusiastic) market given that remote editing is perhaps the most useful in situations where the remote environment is different to such an extent you can't easily copy a project over and expect it to just work. If I'm deploying to an x86_ 64 Linux box, I can totally just develop locally and do a lot of testing locally deploy to the remote after all of that, and so I tend not to get too excited about remote editing features for a platform like a normal x86_64 Linux distro. I'm obviously being picky and demanding and people are free to ignore everything I just wrote. As far as Zed is concerned, the editor looks good and this functionality looks good and I'm happy to see it moving along. Please forgive me for indulging my personal pet peeve. It is a good thing by itself to have competition to vscode and so please don't interpret my post as being overly negative to Zed given it's main competitor has the same issue. Also, I'm aware that a fair response to everything I just wrote would be \"just use vi/vim/emacs\"...which is actually really fair and does a lot to demolish my argument, at least as far as a remote editor being a necessity is concerned rather than just a nice to have. reply legobeet 9 hours agoparentI'm going the container route. To get the full seamlessness you're alluding to (easily switch between multiple parallel environments; easy migrations) still requires some additional features but perhaps you can find some inspiration. https://github.com/legobeat/l7-devenv E.g. https://github.com/legobeat/l7-devenv/pull/144 https://github.com/legobeat/l7-devenv/pull/153 reply prmoustache 10 hours agoparentprevAlthough I don't understand why you would still want to do that with modern processes, configuration management, CICD tools and the decentralization of git, you can just mount the remote directory via ssh. People have been doing that for decades already. Works with any editor/ide/tool. reply iveqy 11 hours agoprevI also switch between a lot of computers (work computer at home/work computer at work) but have to develop on \"big powerful machine at work\". My current solution is tmux + nvim and it works really good. I can just pickup a session from whatever computer I'm in front of at the moment. Am I correct in that neither Zed nor VS Code support this usecase yet? reply senko 11 hours agoparentI use VSCode + SSH remote for this and works great. The only nitpick I have is needing to manually reconnect when I suspend my laptop and ssh connection breaks. It's a separate session though, which doesn't matter to me but may be a deal breaker for you. I use Tailscale for a personal VPN so the beefy workstation is always securely available from my laptop, even when across the pond). reply nrvn 5 hours agorootparent10 years ago I was using https://mosh.org/ to sutomatically reconnect ssh sessions. reply senko 4 hours agorootparentVSCode uses ordinary ssh tho. I multiplex my ssh connections so the workflow is just ssh sgain, then reload VSCode window. If Mosh could multiplex these (and paper over the connection problems) that'd be great but after a cursory look, it doesn't look like it's possible. It's a minor thing tho. In general, I quite like Mosh! If I routinely had to work on faraway servers I'd use mosh just for its smart local echo. reply DavideNL 5 hours agorootparentprevThere's also `et`: https://mistertea.github.io/EternalTerminal/ reply hu3 7 hours agorootparentprevThank you both for sharing the experience. I'm tempted to go lightweight laptop + beef server. Do you get used to the the input delay? I guess coding is not a FPS game so it's fine. Just a flow of words being edited. reply senko 6 hours agorootparentThere's no input delay in VSCode (editor, ui) because the UI is local. Delay in saving/reading/sesrching in files is not noticable for me. (edit to explain: VSCode is still running locally, but it also installs the server-side (headless) component on the remote machine. That way editing files is local/fast, but stuff like running the code, search/replace/etc also works fadt because it's handled by the serverside). Terminal (incl vscode terminal) feels slightly sluggish, and it's noticable if the server is in another country and uncomfortable if across the pond. reply hu3 3 hours agorootparentThank you. Sounds like best of both worlds. reply dagw 6 hours agorootparentprevDo you get used to the input delay? The input delays are very dependent on where the server is and what it is doing. If the server is idle and close by (ping wise) delay is virtually indistinguishable from local VSCode. If I'm connecting to the server via a VPN in a different country while stressing all the cores with some background compiling or number crunching work, input delay gets quite noticeable. reply hu3 3 hours agorootparentI guess editing files in the same continent must feel good enough. Thank you! I should try a project in this way. Perhaps rent some hetzner VPS. reply teruakohatu 11 hours agoparentprevNot persistent sessions, but VS Code can run the GUI locally and connect to a remote server. When you reconnect it opens all your tabs, workspace settings etc. reply BossingAround 10 hours agoprevPersonally, I need support for remote debuggers, e.g. debugpy on Python. Would this feature help me debug code that's already running on a remote server and I just need to connect to it? reply jebarker 10 hours agoparentI really wish there was a good alternative to vscode for remote debugpy reply BossingAround 9 hours agorootparentSame. I don't like VSCode, but debugpy is, for me, a killer feature for which I'd pay if I had to. reply worthless-trash 7 hours agorootparentI was told that pythons repl was equivalent or as good as Common Lisp/Clojure, and it doesn't even look like it has a networked repl without third party code. I now wonder if pythons repl experience isnt even close to lisp likes. reply klemola 11 hours agoprevHow's Zed with WSL (now)? I've liked the ability to run the VS Code back-end in WSL and front-end in Windows, is this possible with Zed? reply TZubiri 11 hours agoparentI wouldn't go there. These things are supposed to be helpful, any bug or chance of a bug will be detrimental to their stated purpose, just imagine scenarios were zed tells git to download a repo and you have to make sure it downloads to a linux directory and not a windows directory, otherwise you lose all of the access bits, and it's slow af. Asking for trouble. reply klemola 11 hours agorootparentI appreciate your concern and I will consider this. The [VS Code] WSL back-end has access to things in WSL PATH, like compilers and language servers. I don't duplicate them in Windows, and would like to avoid having double installs. It's also nice to be able to open integrated terminal and use `fish` and other unix-only CLI utilities. reply TiredOfLife 6 hours agoparentprevZed still doesn't have Windows version (but someone is working on it), so currently no. reply mike_d 10 hours agoprevI recently tried to use SSH remote editing with VSCode on a plane. It is so incredibly chatty that the in-flight wifi system thought I was trying to tunnel traffic via SSH and kept killing the connection. I hope the Zed developers have taken bandwidth minimization into account. reply reubenmorais 7 hours agoparentYou were tunneling traffic through SSH :P reply mrklol 10 hours agoparentprevAfaik the problem are big file directories, at least that what I noticed. reply Instantnoodl 8 hours agoprevI tried to use zeds SSH remoting for a work project. Typescript with big codebase and sadly the performance is a rollercoaster. Types sometimes load so slow that it's unusable. VSCode remote doesn't seem to have this problem, although I'm not sure yet if it's really just a zed problem. I would need to try it with some more isolated use-case. Still happy to see the progress! I switched to zed for a lot of my private dev work already reply teddyh 9 hours agoprevI can find no mention of Emacs (or Tramp) in any of the documentation or a sampling of blog posts. It’s as if the authors don’t even know it exists. This is concerning. (As is their apparent 100% all-in on the AI hype train.) reply globular-toast 9 hours agoparentYep. Emacs had this 25 years ago. I find editing over SSH a last resort, though, as it introduces a lot of undesirable latency. reply amszmidt 7 hours agorootparentEmacs had this 40 years ago. ;-) reply globular-toast 5 hours agorootparentI'm not surprised! I said 25 because the TRAMP manual says (c) 1999. reply teddyh 4 hours agorootparentBefore there was Tramp, there was AngeFTP. reply ceving 11 hours agoprevSomeone reinvented tramp-mode. reply pineapple_sauce 11 hours agoparentThis appears to be a significantly better implementation. Tramp mode is god awful slow and does not maintain a persistent connection. reply noufalibrahim 11 hours agorootparentThe default setup is pretty crappy but with some tweaking of the ssh connection setup, It does a decent job and I used it for actual work for several years. reply globular-toast 5 hours agorootparentWould you mind outlining the settings or pointing to a reference? reply rs_rs_rs_rs_rs 11 hours agoparentprevI suggest you try it and compare it to tramp-mode and see for yourself how different it is(and just how much better it is to tramp-mode) reply Dinux 9 hours agoprevI cant wait to switch to something like Zed. VS Code can take up half of my memory and keeps crashing after updates. How much language support is there for Go? reply gabeidx 8 hours agoparentIt has built-in support for Go: https://zed.dev/docs/languages/go reply Instantnoodl 7 hours agoparentprevI switched from Goland to Zed. Works really well. Only missing debugger support... After that it would be perfect for my usage :) reply m3nu 11 hours agoprevAlso works well in vscode. Just wish the remote instance would keep running forever. Currently it's only a few hours it seems. Then one needs to reopen the workspace window. reply TZubiri 11 hours agoparentI don't use code editors, but when we had an issue with ssh sessions hanging, it was always coworkers that were using vscode and connecting something up with that. Man, just ssh through the terminal like a normal person and transfer files with git. reply uniqueuid 11 hours agorootparent\"Man, just put it on tape and walk over to that machine to load it there\". Sorry, I know such low-effort puns are shunned on HN, but once in a decade I grant myself the permission to not resist. reply lifthrasiir 11 hours agoparentprevGiven my experience with the vscode remote instance, that actually sounds terrible as it can get bloated very quickly. The instance state can and should be retained without running the instance indefinitely. reply reacharavindh 10 hours agoprevA nice setup for Rust development where compile times are high. You can offload that to a beefy machine while using the editor on a MacBook Air :-) reply DrBazza 9 hours agoprevHas anyone tried this and ssh'd into a container? IIRC VSCode struggled to correctly ssh into a container that's set up in your `.ssh` config file. Not sure if this fixed it: https://github.com/microsoft/vscode/issues/194879 reply uneekname 6 hours agoparentI'd really like to see this. I already run a bunch of containers on servers at home and work, and running a \"Zed server\" of some sort would be fabulous. I also wonder (I haven't looked yet) how Zed communicates with its server process over SSH, and if this could be accomplished over some other protocol. Is it possible SSH could be a speed (or latency) bottleneck on high-speed networks? reply TiredOfLife 6 hours agoparentprevI just connected from VS Code running on Windows to a devcontainer running on a Steam Deck reply jeppester 9 hours agoprevI wish I would be able to start a dedicated server that could then be joined by both myself and my co-workers to work together. From reading the article, it seems that collaboration is a wholly different layer, just like in VS Code. reply vermon 8 hours agoprevI would switch over to Zed for all my non-JVM needs if only it would have a debugger support. Currently it's filling a role of a text editor pretty well though. reply tracker1 8 hours agoprevGlad to see this. It's probably one of the biggest features in vs code that I find indispensable along with the integrated terminal. Will give Zed a try now. reply KingOfCoders 11 hours agoprev\"the language server is continually out of memory\" \"8gb is enough for everybody\" reply bbminner 10 hours agoprevI have been using code server over ssh for this purpose for years. reply jiri 11 hours agoprevIs somewhere a comparison of Zed to Visual Studio Code? reply wiz21c 11 hours agoparentfor a start, zed doesn't work on Windows. I'm not exactly a fan of MSFT but at work, I must use Windows... reply Flex247A 10 hours agorootparentIt does work. Automated builds are here: https://github.com/deevus/zed-windows-builds reply TiredOfLife 2 hours agorootparentBuilds are there, but they don't work. At least on my laptop. reply timeon 11 hours agoparentprevMemory footprint, startup times, overall responsiveness. At the end of the day VSCode is still Electron app. reply speedgoose 11 hours agorootparentIn my experience, the electron overhead is relatively negligible compared to the ressource usage of the language servers. And zed seems to use the same language servers. reply Aeolun 11 hours agorootparentIt is certainly true that the language servers have the same speed in Zed, but there is something to be said for just never suffering from any UI delay for any reason. Though I'll admit a LS that takes three seconds to respond is kinda indistinguishable from UI delay. reply cybrox 11 hours agorootparentWhat kind of codebases do you work with? Other than with multi-MB files or thousands of multi-cursors, I've never had any performance issues with VS-Code. It's actually one of the fastest Electron apps, I would say. If I notice a delay, it's usually the language server or some support package's background command taking a bit of time. reply stavros 10 hours agorootparentHow old is your computer? Some people have longer upgrade cycles than others. reply cybrox 6 hours agorootparentGranted, all of my machines are pretty new and powerful. Maybe performance is a lot worse on >5yo machines. reply speedgoose 10 hours agorootparentprevI remember that my computer in 2016 was already fast enough. reply drcongo 9 hours agorootparentprevIf all you know is VS Code it probably feels fine and you should definitely not try Zed - \"one of the fastest Electron apps\" isn't exactly a ringing endorsement. reply cybrox 6 hours agorootparentI have used both unbearably slow IDEs like Visual Studio, Android Studio, Atmel Studio and very fast and lightweight editors like Notepad++ and Sublime Text. I don't have any issue with the speed of VS-Code on my machine and it feels the same as Sublime Text used to. reply hu3 7 hours agorootparentprevSublime is faster still. If speed is your main concern. reply eviks 8 hours agorootparentprevHow does LSP affect the core editing functionality like \"startup times, overall responsiveness\"? reply zazaulola 9 hours agorootparentprevVSCode does not necessarily require Electron. Unlike Zed, VSCode can actually run on a server, and Google Chrome can be used as a client. You can even take advantage of all VSCode features and plugins using Google Chrome Mobile. reply kunley 9 hours agorootparentMain and often repeated argument against Electron is that it has a browser embedded inside, and that's why it's so bloated, slow and taking up so much memory. Yet you give counterexample of using... a browser as a client/frontend to vscode. Well it doesn't remove the exact main problem with Electron, does it? reply jeeybee 10 hours agoprevUsed Zed for a week or so, loved it! But the lack of mypy / git integrations forced me back to VS-code :'( reply drcongo 9 hours agoparentI switch between Sublime and Zed all day depending on what task I'm doing, some things it's still not quite as good at as Sublime. The Ruff integration is excellent though, and the good news there is that apparently they're working on a type checker for Ruff's language server. reply uneekname 6 hours agoparentprevYeah, while I'm pretty happy with git in the terminal, I'm eager to see what Zed's take on a git workflow will look like. The inline git blame info is nice, but a UI for committing and push/pulling the repo would be great. reply edweis 9 hours agoprevZed is great. But as long as they don’t support eslint properly, it’s a turn down for me and my organization. reply eadmund 7 hours agoprevThose who do not understand Emacs are doomed to repeat it. Poorly, in worse languages. —C.S. Santayana Emacs TRAMP mode has been doing this for 26 years, since November 1998. Fashions come, and fashions go, and Emacs remains. Anyone remember Atom? Anyone remember TextMate? Just use vi or Emacs. reply hollerith 2 hours agoparentDon't many editors have the feature of allowing remote editing through ssh? There was discussion here a few days ago for example of an Acme clone named Anvil, which has the feature, as does Acme. VSCode has it. Also, TRAMP is a poor (unreliable, fragile) implementation of the feature. Or at least it was the last time I used it regularly about 10 years ago. Finally, if you have to resort to quoting a slogan then it is probably better not to comment at all. reply fallingsquirrel 4 hours agoparentprevWhat's the point of this comment? Zed didn't exist in 1998, so I think it's forgivable that Zed didn't support remote editing in 1998. It's pretty funny you'd comment this at the same time there's a thread on the front page with people complaining about emacs bugs/performance issues: https://news.ycombinator.com/item?id=41954030 Is that really the best we can do? Should we have frozen tech in 1998 with our 28.8kbps modems and stopped trying to improve anything ever again? Why did you switch to this emacs fad when you could have stuck with ed(1)? I think it's great that people are advancing the state of the art. Good for Zed. reply eadmund 2 hours agorootparent> What's the point of this comment? To encourage people to use Emacs (or vi). > Should we have frozen tech in 1998 with our 28.8kbps modems We had Ethernet and high-speed networks in 1998. The rest of the world has just caught up, that’s all. > Why did you switch to this emacs fad when you could have stuck with ed(1)? Emacs is an improvement over and above vi & ed. No other current editor is an improvement over and above Emacs. The only improvement Emacs is lacking is Common Lisp extensibility. > I think it's great that people are advancing the state of the art. But they’re not — they’re reimplementing the decades-old art. Why waste time using worse technologies to extend worse editors when one can invest one’s time using better technology to extend a better editor? reply amszmidt 7 hours agoparentprevFun fact. Emacs was doing this even long before Tramp, in the 1980s it was already common to work with remote files. So more like 40 years … reply thiht 6 hours agoparentprev\"Just\" does quite a bit of heavy lifting here. VSCode and Zed are much easier to use than emacs or vim. To me as a user, emacs lisp (and lisp in general) is the worse language compared to TypeScript which I can work with. reply otabdeveloper4 6 hours agorootparent> VSCode and Zed are much easier to use than emacs or vim. False. It's just that Emacs comes with fewer features out of the box. > emacs lisp (and lisp in general) is the worse language compared to TypeScript Elisp is truly a piece of shit language, but TypeScript is not much better. reply bert2002 9 hours agoprevAny chance for a iPadOS release? reply meitham 9 hours agoprevHow’s this different from starting tmux session on a remote server and editing code with a text editor and running in that session? Is Zed not a terminal text editor? reply dagw 9 hours agoparentZed is a GUI code editor. Also the difference is where the UI runs. Running the UI locally gives much better response and lower latency than running the UI remote. reply trollied 7 hours agoprevJetbrains Gateway also does this. reply VeejayRampay 8 hours agoprevI want to like Zed but two things are still an issue for me: - Zed doesn't know how to highlight XML - I use a theme called FairyFloss which doesn't exist for Zed and Zed doesn't provide anything to import VSCode themes reply Protostome 10 hours agoprevwith a little bit of work, vi/emacs works great as an IDE for most setups... The learning curve is steep but once you get the gist, you never go back to those bloated IDEs. (Disclaimer: I haven't tried zed, but have been working throughout my career with Eclipse, Visual C/C++/Basic (!), Atom) reply jebarker 9 hours agoparentUnless you need a graphical debugger, then vi/vim/nvim/emacs all start to get creaky. That single requirement keeps me on vscode. I'd love a terminal based alternative. reply frou_dh 9 hours agorootparentGraphical as in integrated with the normal editing view? There's a new very nicely done one for Emacs: https://github.com/svaante/dape . The documentation about using it is currently a bit terse though. reply jebarker 5 hours agorootparentI actually don't mind if it's not integrated with the editor. I'm fine with a debugger being a standalone application. What I want is the ability to see the source, set breakpoints and navigate the code efficiently inside the debugger. vscode is currently the best option for this remotely reply eviks 8 hours agoparentprev> The learning curve is steep So it's not \"a little bit of work\" reply Protostome 8 hours agorootparentCompared to mastering C++, vim is a piece of cake. For me it was a major productivity boost. reply nurumaik 9 hours agoparentprevTry remote development with vim over 200ms ping ssh connection reply Protostome 8 hours agorootparentI've tried that, but in this case i would just NFS/sshfs share the directory and work with vim locally. Vim can also read and write files across networks (while running locally): vim scp://user@myserver[:port]//path/to/file.txt should work. If you want to run builds and such on a terminal, just have a tmux window with an active ssh connection. There's a solution to most issues out there. reply snvzz 8 hours agorootparentprev>Try remote development with vim over 200ms ping ssh connection Isn't such a scenario (high latency dialup) what vi was originally designed for? reply Hamuko 11 hours agoprevI tried to use Zed but the text rendering on macOS was so painful that I didn't get far. Hopefully they'll address so I can give it another go in the future. reply stanmancan 11 hours agoparentWhat’s the issue exactly? I’ve been using it on macOS for a few months now and while it has its issues, text rendering hasn’t been one of them. reply Hamuko 11 hours agorootparentText looks very blurry when compared to Sublime Text with the same font family and size. There's an active issue about it on GitHub with screenshots. https://github.com/zed-industries/zed/issues/7992 reply neurostimulant 9 hours agoparentprevSuch is life using macos on low dpi screen. A lot of apps render blurry texts :( reply Hamuko 8 hours agorootparentIt was definitely a lot worse than any other application I use on a 1440p 27-inch screen. reply ktosobcy 9 hours agoprevI've tried zed again and it's kinda \"meh\" when it comes to Java development. It's kinda nice editor (but BBedit feels even faster) but nothing more and using it feels like an added friction... reply evbogue 8 hours agoprevHow does this compare to vim over mosh? reply rmrf100 8 hours agoprevthis is cool, love it. reply p-e-w 11 hours agoprevCaveat emptor: \"Zed downloads NodeJS binary and npm packages from Internet without user’s consent\"[1] This has been an open issue for 5 months. When I noticed it, I couldn't believe my eyes and it was the last time I've run Zed so far. Judge for yourself whether this is a deal-breaker for you; I wish I had known about it earlier. [1] https://github.com/zed-industries/zed/issues/12589 reply lifthrasiir 11 hours agoparentOops indeed. (Downloading can be fine in many---but not all---cases, but the lack of authentication is not really justifiable!) The latest comment does hint that it will change in the near future, as the change is required for remote development anyway: > Status update: We are still working on this! The major blocker is that extensions have not been setup to interact with setting. However, we also need to change this API to support our upcoming remote development feature. So we're going to roll both of these breaking changes into a larger extension update, coming this November or December :) reply Aeolun 11 hours agoparentprevI don't see how this is different from having all these pre-bundled with a new version of Zed? Either way I'm going to download all of them again. reply lifthrasiir 11 hours agorootparentBy bundling, Zed guarantees or at least claims that those bundled executables can be trusted. The same level of trust is possible with on-demand downloading only when some sort of authentication is used [1] but Zed currently doesn't actually authenticate any downloads to my knowledge. [1] Either by embedding cryptographic hashes to the distribution, or by having some means to distribute publicly signed hashes (e.g. minisign via HTTPS). reply coldtea 9 hours agorootparent>By bundling, Zed guarantees or at least claims that those bundled executables can be trusted As if anyone at Zed cares and checks them all thoroughly? Even if they wanted they couldn't, given how expansive Node dependencies get. At best, someone will report an issue/vulnerability for one of those to them. Usually months/years after it exists. reply lifthrasiir 9 hours agorootparentWell, in any case Zed would be morally responsible for that issue or vulnerability, in the way that they have to at least push a new version that fixes it or prevents the download of affected dependencies. (I don't expect any legal responsibility to be clear.) Bundling at least makes Zed more conscious about what to include, even though it is unreasonable to expect that they've checked every details. reply xiaodai 8 hours agoprevmake windows here before doing ssh? reply stephenr 7 hours agoprevThere was discussion about Zed over at Lobste.rs ~9 months ago when it went open source. I tried it then (as it was compared a lot to VSCode or Visual Studio code, which according to comments here are two separate things, to my surprise?) and I just updated and tried it again, and my initial feeling about it is unchanged: Something about the UI is just... off. I don't know what it is exactly, but it feels like one of those \"web page trying to mimic a desktop environment\" demos that were all the rage 15-20 years ago. It's bizarre to think that IntelliJ, which is a JVM app, feels way more 'native' on macOS than Zed. reply indulona 9 hours agoprev [–] why is there so much hype about yet another editor? reply fredoliveira 8 hours agoparentIf we all stuck to the first thing that worked and searched for no improvements whatsoever, we'd all be riding horses. reply coldtea 9 hours agoparentprev [–] \"So much hype\" being defined as \"the ocassional release announcement HN post now and then\", most of which could be submitted by Zed themselves? And \"yet another editor\" as in implying that all editors edit, so Vim or Emacs or VSCode or Zed or whatever shouldn't matter, as none brings anything different/new to the table? reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Zed addresses performance issues in programming with large codebases by enabling remote project access via SSH, allowing the UI to run locally while utilizing cloud hardware for language servers and tasks.",
      "The \"Remote Projects\" UI in Zed simplifies setting up connections, and the platform supports collaboration over SSH, ensuring seamless synchronization across different systems.",
      "Zed is available for macOS and Linux, and the team is actively seeking feedback and hiring new team members, indicating ongoing development and community engagement."
    ],
    "commentSummary": [
      "Zed, combined with SSH Remoting and Orbstack, provides a fast Linux development environment on macOS, offering a streamlined experience similar to Windows Subsystem for Linux (WSL) and Visual Studio Code (VSCode).",
      "Users appreciate Zed's speed and native integration, but some miss features such as debugging and note issues like text rendering and lack of certain integrations.",
      "Zed's open-source nature and potential for collaboration are appealing, though concerns about its long-term sustainability and monetization persist, making it a promising yet uncertain alternative to traditional editors."
    ],
    "points": 251,
    "commentCount": 194,
    "retryCount": 0,
    "time": 1730358853
  },
  {
    "id": 42000651,
    "title": "Hi Google, please stop pooping the bed: a desperate plea from the indie web",
    "originLink": "https://build.shepherd.com/p/hi-google-please-stop-the-bed-a-desperate",
    "originBody": "Share this post Hi Google, please stop 💩 the bed: a desperate plea from the independent web. build.shepherd.com Copy link Facebook Email Note Other Hi Google, please stop 💩 the bed: a desperate plea from the independent web. Served with humor, as I cry into my goldfish bowl of boxed Chardonnay. Ben Fox Oct 28, 2024 10 Share this post Hi Google, please stop 💩 the bed: a desperate plea from the independent web. build.shepherd.com Copy link Facebook Email Note Other 9 1 Share Every 3 weeks, I share my notes on building Shepherd.com. Join the rebellion and help me create the indie book platform readers and authors deserve! Three hurrahs for our 818 Founding Members who keep us independent and fund new features. Readers and authors can become Founding Members (both get special perks, and more perks are coming). Dear Google, Please stop 💩 the bed. You are destroying the independent web. 🤪 What is Google doing? Google is killing independent websites by burying them in the search results, even when we perfectly follow their guidelines. Nobody\" is sure what is going on, but their search engine started breaking down about 16 months ago. Some people believe they have lost control of their AI ranking systems, while others believe that the McKinsey alumni who run Google Search are purposefully doing this to increase ad earnings (DOJ leaks from the ongoing trials hint at this). Let me show you how Google is 💩 the bed… Here is a graph of Google traffic to Shepherd… Over the last 16 months, we’ve seen an 86% decline from Google to the most-loved section of the website. This section has been incredibly popular with Google for years and it has sky-high engagement stats from visitors. Google changes all the time, but a drop like this is insane. This is destroying many websites you know and love. Google's promise to website owners was simple: create great content, and Google will rank those pages if those pages satisfy users. This promise has been broken; let me show you how. First, visitors love these pages. Why did Google stop loving them? I am a data nerd. I love building good UX and work constantly to provide my visitors with an amazing experience. I even do anonymous user video testing with every feature to ensure it is easy to navigate and that readers love the experience. 60% of our visitors read 75% or more of the pages in this section. On average, visitors spend 5 minutes and 1 second on pages in this section (the real number is higher, as this only updates if an action is triggered on the page). 12% of visitors click to learn more about a book or go to a bookstore to learn more. This is an excellent sign that the reader found something that intrigued them enough to dig deeper. 8% of visitors click to visit another book list that we recommend at the end of the recommendations. And more… My point is that this section is incredibly popular with visitors, and Google used to see and reward that. This is happening to hundreds of thousands of websites, and Google is not saying anything. Here is a specific example… I asked Kevin Miller to share his five favorite books about the Battle of Midway and why he thinks each of those is a great read (click the below to read that). Kevin is an expert. Kevin has written a book on the Battle of Midway, is the Executive Vice President of the Naval Aviation Museum, and is a former U.S. Navy Fighter Pilot. He is exactly the person you want making an expert-led book recommendation about the Battle of Midway. Where does Kevin’s page appear in Google? Since it was published, Kevin’s page has been in the top 3 results for Google searches related to the “best books on Battle of Midway.” … Then Google 💩 the bed. …. Kevin’s page now appears on the 3rd page, or not at all, for all searches. How does Kevin’s page engage readers? On average, visitors spend 7 minutes and 16 seconds on his page (the real number is higher as this only updates if an action is triggered). And 55% of visitors read to the 75% mark or more. 15% of visitors click to explore a book or go to a bookstore (to learn more about a specific book). This is a very good sign that the reader found something that intrigued them. 15% of visitors go within Shepherd to a related topic, genre, or book on this page. 7.5% of visitors check out a recommended list at the bottom of the page or related topic. My point is that this page crushes it for a large chunk of readers who are looking for an answer to their query on Google. And even worse, what Google is ranking instead is not great… What ranks instead of Kevin’s page? The page ranking #2 is one I created to test how badly Google is 💩 the bed. It is a list I created on Bookshop.org that has 4 books, no personal details, no expertise, and nothing of value. It even links to Kevin’s list on Shepherd since it is a terrible derivative of that list. What ranks in the top 10 if Kevin does not? There is a Goodreads list of all books tagged Battle of Midway. They are not sorted by rating, and there is no expert explaining why they are a good resource. It is not the worst list, I guess, but it is just a collection of the 100 books on the battle that Goodreads has in their database. A 3-year-old Reddit thread with only one book recommendation. I generally like Reddit, but this thread isn’t very helpful. A Museum e-commerce page selling a book about the battle for $35 dollars. There is zero curation, and it just has the default book description. A Quora post with only 2 answers. The first answer recommends a single book, and the other answer can only be viewed if you pay for Quora Plus. Google’s book widget, which shows book covers that match the topic? Wee! Zero curation or expertise or usefulness. An Amazon link to a single book about the Battle of Midway. How is that helpful? Another Goodreads list with 829 books about “Midway.” The 3rd book recommended is Dune, and the fourth is Little Women. Is that helpful? This is a decent result from the Midway Library (except it hasn’t been updated since 2016). However, they do have some good recommendations and are curated by experts (although I am not sure who). The collection also movies and other media. A 2010 post on a Board Game forum asking for good books about the Battle of Midway. Really Google? A B&N link to a single book about the Battle of Midway. My point is that Google has elevated bad results that do not help users and buried independent websites like ourselves. In many cases, we are seeing websites that have served visitors for years lose 95%+ of their traffic from Google and not even rank for their brand name. This is a shadow ban where something in Google’s system has effectively blocked that website from existing. Google refuses to acknowledge it is happening or speak about it. Meanwhile, search engines like DuckDuckGo and Bing rank Kevin’s page in the #1 and #3 spot, as they should. Shepherd is luckier than most. I am lucky because while Google Search is breaking, I am receiving increasing traffic from Bing, DuckDuckGo, social media, and other sources. That has allowed us to stay alive along with the support of our Founding Members (who financially support the website for the long term). I have many friends and acquaintances who are going bankrupt and shutting down fantastic independently run websites. It is sad to see. The independent web is in danger, and Google is destroying it. What do I want Google to do? Get the 💩 out of the bed!!! I used to love Google. It was magic compared to Altavista. I could type in what I wanted, and you would return a result that was what I wanted. At the very least, I had to try a few results before finding what I was specifically looking for. Now, it is just a wall of ads, unhelpful results, spam from big publishers like Forbes, and Reddit (I like Reddit, but you do have to vet them still). I won’t even mention the AI overviews… which could be useful but are instead plastered on every surface you can find. Google, you have to fix search so it works again. It has gotten so bad I moved to the Kagi search engine, and my wife switched to using ChatGPT 75% of the time. What else? Oh ya → Talk to us!!!!! Google has released several core updates since this started, and not a single website has really recovered. It has also not provided enough good information about what is going on. Google has search liaisons who are amazing human beings, but they seem unable to share real information or help anyone. They just say the same corporate speak without saying much of anything. Dear John Mu and Danny Sullivan (work), I have to do a shot of Tequilla every time you say, “Just make good content,” so please stop; I am begging you… Please provide real information and help the website owners you are destroying. I know you want to, as you are both amazing people. Other ideas: I’d love to see a partner program where Google has website owners embed code on their websites to pull in anonymous data about engagement so they can better reward good websites. I HATE spam and SEO spam, and I think this could help fix the problem. Google has something similar on YouTube, given their control of that platform, and I think this could work by helping them identify who wants to build a better web. Build real help into GSC. Tell website owners what needs to be improved and why. Why are you hiding this information behind a mysterious black box? I am an inarticulate clown. I put off writing this because I am not the best writer. Here are articulate breakdowns by articulate people on why this is so dangerous for the web: HouseFresh explains how Google is killing independent websites like theirs. This one is especially important as it shows how spam now ranks above them. Here is how Google killed RetroDodo, which is one of the best retro video gaming websites on the net. Healthy Framework explains how Google is breaking the web and why soon, nothing will be left but spam and Reddit. Make sure you read part 2 to see how this is getting dangerous. And a summary from RePlay on how Google has destroyed an entire swath of websites. A great overview of the pain Ready Steady Cut is in as well as other sites. A fantastic set of notes that Rutledge sent Google before the Oct 28/29 creator summit that Google is throwing. Mike’s post about Google destroying a 17-year-old outdoor website that is his life. I have a lot more, but those are the best overviews of the damage Google is doing. I also recommend this video. It isn’t a perfect explanation, but he explains why Google is incentivized to create terrible search results. Thanks for listening Google, Ben Fox (founder of Shepherd and inarticulate clown) Thanks for reading Building Shepherd! Subscribe for free to receive new posts and support my work. Subscribe 10 Share this post Hi Google, please stop 💩 the bed: a desperate plea from the independent web. build.shepherd.com Copy link Facebook Email Note Other 9 1 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=42000651",
    "commentBody": "Hi Google, please stop pooping the bed: a desperate plea from the indie web (shepherd.com)245 points by bwb 21 hours agohidepastfavorite209 comments danpalmer 19 hours agoI have no specific information here, never seen/used Shepherd until I saw a list on there a few days ago. Disclaimer, I work for Google, but not on anything related to this space, and this is based on my previous job where we did some SEO for an ecommerce site. The example list given just looks a lot like spam when you squint. It's a list of affiliate links to buy products, and there are many HN threads talking about the abundance of affiliate link aggregators being a blight on the web. The commentary does look useful, but distinguishing between good commentary and bad commentary is hard, whereas distinguishing between a site designed to extract affiliate commission vs one more about the content is easier. The comparison given to the other results here is frustrating, I know, but probably not a valid experiment. All the major search engines change results based on the user using them, or the IP address, or the region, or whatever, so it's impossible to know what others see. The developer of a book-focused shopping site is likely to get very skewed results for a book related query. My results were noticeably better. The author says that a Bookshop.org list they created that links back to Shepherd is ranking #2, and this kinda makes sense to me. Bookshop.org sells the books, it makes sense that would rank above a site that only links to (and makes money from) sites that sell books. SEO, and people getting annoyed at not ranking, has been a thing for 25+ years, I don't think this instance is any different. reply bwb 9 hours agoparent> The example list given just looks a lot like spam when you squint. I had to do some deep breathing before responding (creator of Shepherd here) :) When I squint while reading a Pulitzer Prize-winning book, it looks like every other book with words everywhere. When my wife really squints, she says I \"look\" like Brad Pitt. I say these things with humor... as the core of what Google is supposed to do is determine what is good content and what is spam. Is that hard? Yes, but you have a lot of smart people and are swimming in money. The point here, and with all the other posts by indie publishers, is that Google has destroyed them while elevating much worse results (they used to rank just fine along with other accurate results). I am just one example of many, and sadly better than most because books are a passion and not that commercial. For indie websites scientifically reviewing air purification systems, they are destroyed, and now you just get Forbes spam where nobody reviewed it. My suggestion for Google is to stop squinting :) reply stevage 16 hours agoparentprev> SEO, and people getting annoyed at not ranking, has been a thing for 25+ years, I don't think this instance is any different. \"People have always complained about X\" is always a bad argument: it's entirely possible that X has gotten much worse, and yes, there were complaints before and after. In this case, there is a very loud chorus of people saying Google has gotten much worse in the last few years, and it certainly matches my experience. reply thayne 13 hours agoparentprev> The author says that a Bookshop.org list they created that links back to Shepherd is ranking #2, and this kinda makes sense to me. Bookshop.org sells the books, it makes sense that would rank above a site that only links to (and makes money from) sites that sell books. In other words google is giving preference to a site that probably pays them for ads to a site that is effectively a competitor (since the site would make revenue from affiliate links that google might have gotten from ads, especially if the user liked the site and went directly there for future book recommendations). Also, if I am searching for the \"best\" of something, I want something with commentary, not a list from a vendor without relevant information for choosing the right product. reply strunz 17 hours agoparentprevDo you use Google anymore? It has gotten so bad, I regularly can't find things on it anymore, no matter how I search for things it will keep showing the same results. You seem to have blinders on. How do you explain the site getting less and less traffic over time? reply usr1106 11 hours agorootparentI have not used Google for a couple of years (with the exception of Streetview where they do not have any competition at all). So I cannot comment on the quality of their search results. They are a close to monopolist / duopolist / oligopolist in too many areas. Ethically such a bad company that I don't want to touch them with a stick. Unfortunately masses don't understand that and contine to use them. And engineers are greedy enough to work for the evil. reply MrSkelter 9 hours agorootparentiOS has a streetview alternative. I personally hardly ever use any version of streetview because I don’t feel the need to so forgive me if I am missing what’s so great about Google’s version. reply danpalmer 16 hours agorootparentprevI do, and I don't find it too bad. I switched from DDG back to Google recently because I realised I had to use !g on most of my searches. reply musicale 15 hours agorootparentOne day I tried a Google search and noticed that everything \"above the fold\" on the first page was either a Google property, an advertisement, or both. I stopped using Google for web search, but I still use it for Google search. reply danpalmer 14 hours agorootparentPart of me wonders if the ad market is just radically different on Google in the US or other parts of the world, or if I just make very non-commercial searches. I don't see many ads, unless I'm actively going to Google Shopping essentially in search of them. I do see the Google info box style results, but I find these to be one of the most useful parts and it's one of the reasons I like using Google for things that are basic facts, media fact finding (like \"who was that person in that show\" style queries), etc. reply PawgerZ 2 hours agorootparentIn my experience (American), the top 1-4 results are ads/sponsored links. Sometimes, very relevant to the search. Most of the time, not what I'm looking for. I only use google search on my phone because I have a pixel, and the google searchbar is so well integrated into the launcher. If I don't like the results, I can just open a new tab and the default search is DuckDuckGo. Besides the ads, I'd say the searches are usually the same quality. reply bwb 7 hours agorootparentprevI switched to Kagi about 3 months ago, and to ChatGPT for a lot of info searches. reply beefnugs 16 hours agorootparentprevIf they wanted to fix it they would. Most likely someone came up with the great idea of \"Hey if search sucks, some losers will pay for AI hoping that fixes it, we are so smart\" reply itake 16 hours agorootparentprevI don't now if the bad results are because no one has written about the topic I want to learn or google search results are just terrible.. reply bwb 9 hours agoparentprevAlso, to add to this... One idea I've had is that it would be interesting if Google created a webmaster partner program to help build the web they want to see. Webmasters could join and embed Google's code into their website, and Google could get access to engagement stats and anything else to help them determine positive reactions by users (instead of pretending they are not pulling it from Chrome - see DOJ trial). At YouTube, Google does a great job of helping creators, telling them specifically what they want, and promoting high-quality content that people love. I'd love to see Google get the data they need so they can do that. reply techjamie 4 hours agorootparentIs this not literally what Google Analytics is? I'm not sure if they're using it for everything you're hoping to have, but it seems like it basically performs that function. reply bwb 4 hours agorootparentThey they are not allowed to use the data... which seems weird given Chrome and DOJ details. But I've never seen any details of them using it, and every statement is they don't use it. reply smallerize 19 hours agoparentprev(That's not what dearth means.) reply danpalmer 18 hours agorootparentWhoops, thanks. reply Jerrrrrrry 18 hours agorootparentprevportmanteau of \"dithering\" and \"blight\" and \"earth\" reply shark1 10 hours agoparentprevHonest question: Would you say Google Search is getting better and better? reply znpy 8 hours agoparentprevyour comment made me remember about this post: - Google made me ruin a perfectly good website (2023) / https://theluddite.org/#!post/google-ads - https://news.ycombinator.com/item?id=40184673 (HN comments) I'm specifically thinking about this line: > It's a list of affiliate links to buy products, and there are many HN threads talking about the abundance of affiliate link aggregators being a blight on the web. This is a pattern that was made hugely popular specifically by google, by the way reply skybrian 19 hours agoprevIt looks like shepherd.com is a book review site that doesn't have any reviews, just ratings. It links to Amazon. Is this really the sort of content Google should be returning? Edit: it seems I missed the link to the actual book reviews because the link text is uninformative: \"Chosen by 1 person - see why.\" (Sometimes it goes to reviews, sometimes not.) And the word \"review\" never appears on the pages that have reviews. Seems like bad SEO? If you're looking for book reviews, here's a website with some pretty great reviews: https://www.thepsmiths.com/ (Content warning: the authors are conservative.) reply passwordoops 19 hours agoparentJust went over there and yes they have reviews from readers https://shepherd.com/book/exit-strategy reply skybrian 18 hours agorootparentThanks, I missed that. reply bwb 9 hours agoparentprevHi Brian, we don't have reviews at this stage; we have purely positive recommendations. And we push readers toward a very specific format for why the book resonated with them (Gestalt psychology so you see it through their eyes). Given the nightmare that is Goodread's unmoderated review sludge, I wanted to focus on the positivity and why someone loved a book, so we are uniquely focused on highlighting only books that readers love. In 2025 I will be adding \"reviews\" of any books using our Book DNA review format (which tries to narrow in on why someone liked/disliked a book so we can match you with people who share your Book DNA). A big part of my mission is to focus only on books that people love and positivity. reply willywanker 8 hours agoparentprevWas that content warning really necessary, or do we also tend to use them for liberal websites (for whatever definition of liberal) ? reply amluto 15 hours agoprevOne thing I find especially bizarre about the current situation: the pages that are making it to the top aren’t even pretending to be real content. If they were difficult-to-detect LLM-generated pages, that would be one thing. But they’re generally extremely low-effort affiliate spam, mostly claiming that they researched something “so I don’t have to”, followed by a bunch of Amazon links and explicitly acknowledged scraped reviews, and finally an obviously uninformative summary. They don’t even pretend to have real content! What is Google doing? reply bwb 9 hours agoparentYa, that is what is frustrating here. A lot of us believe that Google trained their AI on spam websites, but in the process, the AI now identifies a lot of things that independent websites have on them versus big brands like Forbes, which spam and license out huge sections of their websites to spammers. So we got pulled into this and Google doesn't care about the false positives. At this point, a lot of websites are not coming back. So get used to Forbes and other large brands being the only results along with AI bots on Quora. reply niobe 19 hours agoprev8-10 years google search was amazing. A well crafted query would hit informational gold most of the time. I've been noticing and commenting on the decline privately for most of the period, but it's only in the oast year this seems to have come to broader awareness. There's an argument that in an information economy, searching for information should be treated like a public utility necessary for the functioning of society. I'm not making that argument but when you experience the long slippery slope of degradation of a service that was near ideal for the technology of the time, it does xome to mind. That's the thing. It DID work. Really well for a while. But it was always atomic and context-less. We now have the opportunity to make it even better by refining results through dialogue. I hope someone does.. soon. reply lazystar 18 hours agoparentThe slippery slope began in 2012, when they pledged to start downgrading piracy websites. After the rollout, the followup questions were \"well what if we downgrade other topics that [random government] doesnt like\" and \"what if we sell the ability to boost enterprise company results for certain topics\". eventually the number of results that get filtered out or reordered, exceed the results that actually get displayed. creating a new search platform is not a viable solution - any private company will be incentivized in this direction until some kind of \"search neutrality\" law is introduced. reply equestria 17 hours agorootparentI'm not convinced this is the explanation. It's true that for some product-centric queries, you get mostly paid results. But for information-seeking queries, Google tries to give you organic results. The problem is simply that there's too much money to be made by capturing these. For years, you had content farms and fake \"review\" sites stepping up their game. Now, LLMs essentially make it a losing proposition to try and surface the small web. The least-bad option for Google would be to send you to moderated communities, such as Reddit, Quora, Stack Exchange, Wikipedia, and so on. But not all queries can be handled that way. If you look at the article that these guys are complaining about... how do you distinguish it from content-farmed spam? You can't. Now, I think Google is throwing in the towel and just want an LLM to answer info queries instead. That has a ton of problems, but to the average user, probably feels more helpful. At least until the spammers start gaming that. reply wruza 17 hours agorootparentSometimes I lose track of a bookmark and google a phrase from it verbatim and in quotes. Guess what I get most of the times? Not even a verbatim search spam, just nonsense with almost no original words. It doesn’t even try and I doubt it even has a database where a direct path exists from a phrase to it with a link. Google is not “I found this for you” anymore, it is “I think you meant this”. reply ghxst 17 hours agorootparentI thought this was just me! Even if I use double quotes to search for a 1:1 phrase I often get bad results and nothing linking back to the source I was looking for. If you happen to remember the site something was on, adding \"site:example.com\" still produces decent results fortunately. reply ipaddr 17 hours agorootparentYou have to go into search options and select verbatim. But since they stopped indexing many sites it might not even be there anyways. reply throwaway48476 17 hours agorootparentprev>If you look at the article that these guys are complaining about... how do you distinguish it from content-farmed spam? You can't You can, Google just hasn't bothered to even try. As an example it's pretty easy to detect affiliate marketing links. reply DrillShopper 17 hours agorootparentGoogle: We've done nothing and we're all out of ideas, man! reply equestria 17 hours agorootparentprevNot really? The vast majority of helpful websites monetizes the same way the spammers do. Plus, even without affiliate links, purchase attribution is already scary good. Did you know that credit card issuers and ad tech companies collaborate to attribute brick-and-mortar purchases to online ad views? You have untold billions of dollars at stake. The industry is not banging rocks together. You could make a search engine for non-commercial content only, and I would actually love to have that, but (a) it would be a woefully tiny sliver of the internet; and (b) Google is obviously not the right company to do it. reply wruza 16 hours agorootparentAt its scale google could just hire a big group of diverse internet-aware professionals (two of each kind) who would do random searches and simply manually ban sites that restyle or spam content. It’s absolutely easy for a searcher to tell if it’s spam. reply equestria 16 hours agorootparentThis is actually an interesting rabbit hole. Their other constraint is that they are trying to be transparent and fair, for some definition of that word. They need that for Section 230 protections, they need it to fend off antitrust lawsuits, etc. So, there is a huge problem with hiring experts and telling them to make subjective decisions. Instead, Google publishes guidelines for webmasters and then enforces them rigidly. This usually ends up penalizing some good sites, while spammers swiftly discover workarounds. reply lazystar 14 hours agorootparentthen make it a gov position, like the \"Beer and Malt Beverage Labeling and Formulation Approval\" position, which consists of a single guy approving or rejecting the graphic designs of beer bottles/cans. there was an NPR segment a while back about the one guy that held that position for 40 years. reply ryandv 17 hours agorootparentprevWe are approaching a digital Kessler syndrome, or perhaps a Deepwater Horizon info-oilspill event, where there is so much useless SEO-driven slop (soon to be taken webscale with the advent of genAI) to sift through on the internet that it's growing increasingly difficult to find the signal amidst the noise. Google, once a prestigious company which prided themselves on \"organizing the world's information\" and \"not being evil,\" eventually became a target for those wanting to peddle their wares and make a quick buck - a departure from the days of the early Internet which was mostly computer geeks, hobbyists, and forward thinkers sharing organic content they thought was interesting or useful. Because search was Google's entire business, they needed to develop countermeasures to combat spammers and pages gaming Google's algorithm with questionable SEO techniques in order to preserve the signal-to-noise ratio of the search engine results page. This is now a bygone era - after discarding their original motto of \"don't be evil,\" search and \"organizing the world's information\" are no longer Google's business, it's hawking advertisements [0]: When Gomes pushed back on the multiple requests for growth, Fox added that all three of them were responsible for search, that search was “the revenue engine of the company,” and that bartering with the ads and finance teams was potentially “the new reality of their jobs.” On February 6th 2019, Gomes said that he believed that search was “getting too close to the money,” and ended his email by saying that he was “concerned that growth is all that Google was thinking about.” Hence questionable grey UX patterns like blurring the distinction between ads and organic content, and sometimes cramming the page so full of ads that all the actual results are \"below the fold.\" Remember the old Internet adage - if you're not paying for the product, you are the product - and like cattle we are all just herded into digital pens to be served marketing slop to serve the real customer - the advertisers. If you want to be treated as a customer instead of cattle, you ought then to pay for your services, including search, to align the financial incentives with your own. Advertising needs to die, for it is a root cause of most of the ills of the modern internet [1]. If you can pay for streaming services or music, you can certainly pay for access to high quality organic information that actually aligns with your interests - not that of the advertisers. I've been using Kagi for a few years now and it really does hearken back to Google SERP quality maybe not at its peak, but rounding near to it. At the risk of sounding elitist (and so what), this is just another consequence of the recurring Eternal September phenomenon - highly focused communities with a strong concentration of geeks, hobbyists, and experts were the norm back then, when computers were still new and arcane devices that were difficult to operate. The bar to entry was much higher, and one had to do a little bit of \"reading the fucking manual\" simply to get online and understand how to navigate the net effectively. Now that all the balls have been poured into the Galton board we have regressed to the mediocrity of content that exists on the contemporary Web, absent those pressures that once selected for high quality content online. [0] https://www.wheresyoured.at/the-men-who-killed-google/ [1] https://news.ycombinator.com/item?id=41940718 reply shiroiushi 17 hours agorootparent>If you want to be treated as a customer instead of cattle, you ought then to pay for your services, including search, to align the financial incentives with your own. Advertising needs to die, for it is a root cause of most of the ills of the modern internet. If you can pay for streaming services or music, People pay for their Windows license, yet Windows now has ads baked into the start menu. People pay for Youtube Premium, but most videos now have \"sponsor segments\" -- yet more ads (though admittedly not controlled by or directly profiting YT). People pay for streaming services, but last I heard, Netflix was adding ads. Ages ago, people paid for cable television, and it wasn't long before it had ads too. These companies are going to treat you like cattle whether you're paying them directly or not. reply ryandv 5 hours agorootparentThen vote with your feet and your wallet by leaving. You need to send monetary signals that the service is undesirable by withdrawing your subscription. I don't understand why people continue to use tools that don't serve them and assume, by default, that because everyone else is on some platform, that I must be there too, or don't go into an actual cost-benefit analysis of whether or not the utility of the platform outweighs the drawbacks. Cal Newport comments on this default herd mentality in Deep Work: The Any-Benefit Approach to Network Tool Selection: You’re justified in using a network tool if you can identify any possible benefit to its use, or anything you might possibly miss out on if you don’t use it. In the case of Google the diminishing utility of search began to be outweighed by the increasing morass of SEO slop and advertisements, so I degoogled. After W7 when it was clear Windows was going to progress into a cloud-based ad delivery platform, I installed Linux on all my devices as a daily driver. If Netflix ever serves me ads, I am immediately terminating my subscription. Maybe it's too hard for some people. reply lazystar 15 hours agorootparentprevbingo, this is why i mentioned (in the skip level comment) that all private companies will go through this sequence until a \"search neutrality\" law is introduced. after MBA's start to get diminishing returns on new subscriptions per month, the focus shifts to advertisements. reply yowlingcat 17 hours agorootparentprev> The least-bad option for Google would be to send you to moderated communities, such as Reddit, Quora, Stack Exchange, Wikipedia, and so on. But not all queries can be handled that way. There's a wide gulf between \"path of least resistance\" and \"least-bad\". I don't think it's productive to conflate the first path with the second. reply smcin 16 hours agorootparentprev> \"But for information-seeking queries, Google tries to give you organic results.\" No, often it absolutely doesn't, and I posted two epic fails here previously. Worse still, Google organic results no longer understand(/distinguish) the difference between information-seeking queries vs product searches (or else, SEO people have been gaming it for years, and Google search rankings have made this worse): ____________________________________________________________ 1) Google search relevance fail: result for “Africa longitude” https://news.ycombinator.com/item?id=30337563 Googling for \"Africa longitude\" should return a range of longitudes, like: \"17.5°W - 51.5°E\" or \"[\"17°31′13″W - 51°27′52″E\"]\" [1], or for the A+ answer: *\"Africa lies between 17°33'22\" W, (Cape Verde, westernmost point) and 51°27'52\" E, (Ras Hafun, Somalia, easternmost projection). But it doesn't. Moreover these coordinates haven't substantially changed in 10,000 years (other than political/territorial disputes about islands, but the coords for the mainland certainyl haven't). Googling returns the grossly misleading \"Africa/Coordinates 8.7832° S, 34.5085° E\". When you dig into why this so, it seems to be \"optimized\" for the SEO activities of a digital map storefront, MapsofWorld.com, acquired by MapSherpa Inc., based in Ottawa. And those mystery nonsense coordinates (\"8.7832° S, 34.5085° E\") bizarrely point not even to the geographical centre of Africa but to a random rural location 2400km ESE away, in southern Tanzania, which appears to have been deceptively mislabeled, in Cyrillic, as a Russian store (by Russian SEO?). For a pin dropped in rural Tanzania. No QA! https://news.ycombinator.com/item?id=30337563 ____________________________________________________________ Similarly: 2) Google search for \"How many landings on dark side of moon\" is grossly incorrect https://news.ycombinator.com/item?id=40862146 (Update: the AI overview factbox has at least since been corrected to give Two: \"Chang'e 4 (January 3, 2019) and Chang'e 6 (2024), instead of repeating what jagranjosh.com says). However the #1 organic search hit is still the woefully inaccurate unauthoritative page: https://www.jagranjosh.com/general-knowledge/list-of-all-suc... which cheerfully claims \"Aug 23, 2023 — There are over 21[!!] moon missions that have been launched successfully on the dark side of the moon by 4 countries.\" This is hopelessly wrong, even if we utterly misunderstood the key word \"landing\" and also count any mission which merely photographed the dark side (Luna 3, 1959) or human overflight over it (Apollo 8, 1968). But not \"landing\". And why on earth did Google decide jagranjosh.com was more authoritative than any reference website or wiki? ____________________________________________________________ [1] https://en.wikipedia.org/wiki/Africa#Geology,_geography,_eco... reply dingaling 10 hours agorootparentWell to be fair there is no \"dark side\" to the Moon, so it's not really an answerable query. And searching google.com for \"longitudes continent africa\" returns \"Africa's geographical coordinates span from Latitude 37˚21'N to 34˚51'15\"S and Longitude 51˚27'52\"E to 17˚33'22\"W\" It seems that GIGO still holds true. reply abecedarius 17 hours agorootparentprev> what if we sell the ability to boost enterprise company results for certain topics Maybe I'm naive but... has Google even considered doing that? Re the first one, I know top management had a project to reenter China back in the teens. reply meiraleal 17 hours agorootparentYou are really naive if you think that first one is about China. reply abecedarius 17 hours agorootparentThat was a concrete example of eagerness to submit to an especially censorious government. I did not say it was the only case. Save your insults. reply pyeri 16 hours agorootparentprevExactly, the organic nature of search ceased to exist that very moment when they started human intervention in a purely machine based (crawler/indexing logic) algorithm. reply cratermoon 16 hours agorootparentprev> any private company will be incentivized in this direction From the earliest papers on Google PageRank, Brin and Page warned, \"we expect that advertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\"reply lazystar 15 hours agorootparentonce the new subs per month metric starts to decline, even subscription funded engines will move in that direction. reply zx8080 18 hours agoparentprev> We now have the opportunity to make it even better by refining results through dialogue. I hope someone does.. soon. TBH, I don't want dialog (too long and slow, leave dialogs for human to human comms), I just need my query words to exist in the result pages. Nothing more. Google messed this up long time ago. reply munificent 17 hours agorootparent> I just need my query words to exist in the result pages. Nothing more. Google messed this up long time ago. Google didn't mess that up, SEO did. You don't \"just need\" that. You need the page to be authored by a human with good intentions and end up containing those words because those are meaningfully part of what the human was trying to convey to you without ulterior motives. But those kinds of pages are dwarfed by the millions of pages of content farm nonsense jammed full of every possible keyword and contaning absolutely nothing of value. A search engine like you describe would be like walking in Chernobyl without a radiation suit on. A pleasant stroll in 1985, but not today. reply throwaway48476 17 hours agorootparentprevI wouldn't mind a dialog that helps me build an advanced query. reply nradov 14 hours agorootparentHave you tried advanced search? https://www.google.com/advanced_search reply throwaway48476 14 hours agorootparentI'm aware of advanced search. I'd like it to be more user friendly and widely used. I think a chat bot would help. reply throwaway48476 17 hours agoparentprevSearch only works if there exists something valuable to search for. The Google dominated internet has not incentivized a library of quality content. Most quality information exists on long dead domains. The rest is pay walled or unindexed. Google is a library search service but for a gas station convenience store of information. reply inquisitor27552 18 hours agoparentprevkagi is still far from it. reply jojobas 17 hours agoparentprev8-10 years ago the competition for getting stuff before your eyes wasn't as tight. It's not only on google, the quality stuff is objectively harder to find in the ocean of slop today. reply fny 19 hours agoprevI find it ironic that there’s a desperate plea for the indie web from a Substack blog. reply bwb 9 hours agoparentWhy? Substack is a great platform and I love them. I also use Beehiiv for our mailing lists with authors/readers. reply croes 18 hours agoparentprevTo get more visibility? reply bwb 9 hours agorootparentThat is a private blog, so I don't care if anyone sees it. I just wanted something I updated every 2 to 3 weeks with my frustrations, wins, and so on as I build Shepherd.com. I was on Twitter's free email thing but they shut down and imported to Substack. I just didn't want to pay for anything as the only person who reads it is me and my mom. It has 100 readers on it, as its just for me really. reply CSSer 18 hours agorootparentprevI think the irony works either way. reply CharlieDigital 18 hours agorootparentprevIf OP was going for visibility by posting it HN, Reddit, etc. then does it matter if it was Substack or his own blog or even the Shepherd site? reply johnnyanmac 17 hours agorootparentTechnically, yes. If the thesis is \"Google search is inaccurate and overrun with SEO optimized slop and ads\", them using a blog utilizing that same SEO optimization feels a bit ironic. Feeding the beast you want to change. reply bwb 9 hours agorootparentI don't quite understand, I still want Google traffic, they have a monopoly. Nothing has changed there. reply MichaelZuo 18 hours agoparentprevIt’s more than ironic, it’s self-undermining… reply bwb 9 hours agorootparentHow so? It is where I keep a small blog for myself about building Shepherd.com. It has barely any readers and keeps me sane and having fun. reply MichaelZuo 6 hours agorootparentBecause every additional article posted on the non ‘indie web’ is literally reducing the relative credibility/importance/etc. of the ‘indie web’ by a tiny bit… which is unnoticeable for just one, but when you add up millions… reply bwb 6 hours agorootparentI don't understand, Substack is the indie web as well. It is a fantastic platform for the indie web to use. Can you explain in more detail what you are saying? reply renegat0x0 10 hours agoprev1) It has already been proven by research that Google quality of results slips from year to year [1] 2) Google has many incentives to make the search more difficult for you 3) Google has proven that it prefers money over quality of results with allowing \"malvertising\" 4) It is true that the landscape is more difficult. There are more walled gardens, to which even Google might not have access. There are more scams, casinos. More AI slop. The game was always hard on the other hand, so these are just 'excuses' 5) Why so often I see in Google results leading to major news sites instead of normal links? 6) If I write \"Warhammer\" I would expect thousands and thousands of pages in results. I think that Google prefers \"content\" over \"quality\". \"newer\" is \"better\". I would expect thousands of fan pages, which do exists, but are not crawled, or forgotten. Why can't I browse older pages? Why is there a limit to 10 pages? 7) For \"Emulation\" first page leads to \"wikipedia\", \"cambridge dictionary\", \"vocabulary\", it is so f boring [1] https://www.404media.co/google-search-really-has-gotten-wors... reply acegopher 18 hours agoprevThe shepherd.com link is the second one that shows up on Kagi, after a Quora link: https://kagi.com/search?q=best+books+on+Battle+of+Midway reply freediver 18 hours agoparentSame search for those without Kagi account (using 'share search' option): https://kagi.com/search?q=best+books+on+Battle+of+Midway&r=u... reply rc_mob 17 hours agoparentprevI would assume every other kagi user has blacklisted quora already reply acegopher 3 hours agorootparentOhhhh... I didn't know you could blacklist, thanks! reply bwb 9 hours agorootparentprevYa I have, Quora is so bad now, I used to love them long ago. reply uncharted9 17 hours agoparentprevTop result in AI overview and 4th in the general search on Brave Search https://search.brave.com/search?q=best+books+on+the+battle+o... reply carom 17 hours agoparentprevTop result on duckduckgo: https://duckduckgo.com/?t=ffab&q=best+books+on+the+battle+of... reply mikewarot 17 hours agoprevLong ago I wrote a blog post[1] about intermittent hardware, and not letting it suck you in. This was posted in a Blogger/blogspot blog, which is owned by Google. This morning Google couldn't find it, neither could Bing. [1] https://mikewarot.blogspot.com/2007/10/mikes-law-of-intermit... reply freediver 16 hours agoparentKagi can. https://kagi.com/search?q=%22Mike%27s+law+of+intermittent+ha... (granted, I had to perform search with quotes on, but both Google and Bing still fail) Hopefully demonstrates the difference between legacy and a modern search engine whose entire purpose of existence is to surface what you want to find, not what other people want you to see. (wave from Kagi founder here) reply pixodaros 17 hours agoprevVisits from Google to one of my independent sites have been about the same in 2022, 2023, and 2024. Visits to another have roughly doubled from year to year. I don't use Google Search or surveillance ads but I don't have the issue that they report. AyyEye's observation that their site is loaded with trackers and what look a lot like affiliate marketing links is one reason why Google Search might not like it any more. reply bwb 6 hours agoparentI am glad you got lucky, I hope it holds up :) Stats are fine, as sometimes people mistake a single data point for a truth. Might take a look at my post as there are links as this is a huge issue affecting many many many websites. reply pixodaros 2 hours agorootparentI don't have access to your \"many friends and acquaintances who are going bankrupt and shutting down fantastic independently run websites.\" I absolutely don't have access to the internals of their businesses to decide whether I agree that their problem is Google Search being hostile to independent sites. But I do have access to the traffic stats for the sites which I run. Many of the sites with similar complaints seem to focus on product reviews and recommendations and have been hit by a specific Google policy which favors trusted and well-known domains for those types of search, even if that domain is not trusted for product reviews in particular. (So $glossyMagazineInNYC can open a line of product reviews and get lots of Google traffic, while $smallProductReviewBusiness loses traffic). reply jqpabc123 19 hours agoprevSimple answer --- stop using Google. Get your friends and family and other internet users to do the same. The only thing likely to get their attention is if enough people follow suit. reply kayodelycaon 19 hours agoparentThis is what I call a “load-bearing just”. Just do the thing and the problem is gone. Very similar to “Draw the rest of the owl.” The problem is people don’t “just”. We’re complicated critters with our own needs, capabilities, and goals. At an individual level, a person may not be capable of the “just” due to factors outside of their control. With a society, the resistance to change grows exponentially. You can’t scale one person at a time. This doesn’t mean you shouldn’t try. Sometimes you can make a big difference to the people around you. But you’re gonna have a difficult time changing in the world. reply abecedarius 17 hours agorootparentIn this case it really is easy to try a different search engine and switch to it to the extent you prefer it. At least if you're not too uncomfortable using software. reply xigoi 9 hours agorootparentDid you miss the “friends and family” part? reply abecedarius 4 hours agorootparentAre you saying friends and family using Google are an obstacle to you using something else? Or maybe if you use something else, that makes you so uncool that your friends and family are not more likely to follow you, but actually less? I guess what you're actually getting at is that my switching doesn't automatically make my friends and family switch, but why insist that change only happen in blocks of people all at once? How does that help? reply xigoi 4 hours agorootparentThe original comment was pointing out that in order to break the monopoly of Google, the wide society will need to switch to something else, not just a few tech-interested people. reply abecedarius 3 hours agorootparentThe thing is, that kind of logic claiming the futility of individual action does not weaken the grandparent's advice. If you're an advertiser, you can't just switch from Google and hope to change anything. You'd be hurting yourself to no benefit, probably. Collective action is your main hope. It's not like this for users of search. Google can lose its search leadership one user at a time. When the user switches, \"the problem is gone\" for that user, and Google has that much less revenue. Gradient descent is powerful. reply jqpabc123 11 hours agorootparentprevSettingsSearchChoose anything but Google Quicker and easier than \"Draw the rest of the owl\" and not outside anyone's control. reply kayodelycaon 7 hours agorootparentOf course it’s easy for anybody on this forum to change their search engine. That’s the “thing”. The thing is always trivial to the person making the statement. The thing is completely irrelevant to the point I’m trying to make. The “just” is other people. reply nunez 6 hours agorootparentprev\"load-bearing just;\" I love this term. reply chiefalchemist 18 hours agorootparentprev> With a society, the resistance to change grows exponentially Yes and no. There's certainly the potential for a more flexible and adapt-minded culture for the same reason there's not... Human behavior. Yes, once norms are established they're slow to change but that's how the culture is nudged and \"managed\". It's not necessarily how humans arw hardwired. Eons ago, slow to adapt would mean certain death. Currently, fast to adapt labels you a rebel, a freak, an odd-ball, etc. reply kayodelycaon 18 hours agorootparent> norms are established they're slow to change That is all I meant when I said society. reply passwordoops 19 hours agoparentprevAn even better solution is to apply the rule of law and break them up. They never should have been allowed to get this big in the first place https://www.thebignewsletter.com/p/the-rage-of-google reply nunez 6 hours agorootparentHow would breaking up Google fix this? reply 23B1 19 hours agoparentprev\"Simple answer, just get everyone to stay home for three weeks and the virus will die out\" \"Simple answer, just stop electing corrupt politicans\" \"Simple answer, just stop war\" k reply rcMgD2BwE72F 19 hours agorootparentKagi is good enough. Haven’t used a Google product for 12+ months expect as required by my employer (or YouTube but with an ad blocker). Not saying calling for a boycott is enough but it’s easy to avoid Google nowadays. reply jqpabc123 11 hours agorootparent\"... it’s easy to avoid Google nowadays.\" If it says \"Google\", you really don't want it. Some may be a little slow to come around to this realization but it's growing. For example, more than half of internet users are now taking active steps to block \"personalized advertising\" which is Google's bread and butter. reply senko 19 hours agorootparentprevSubscribing to Kagi is definitely easier (and less costly!) than handling the pandemic, ensuring world peace and utopian government. Edit to add: and since we're talking about indie web: https://search.marginalia.nu/ reply nemomarx 18 hours agorootparentMe subscribing to it is. Getting everyone I know to subscribe to it is at least a step more complex than getting people to move to Firefox, and I haven't quite worked out a strategy for that yet. reply justinclift 17 hours agorootparentSearch results being absolutely shit is probably more of a problem for people than their choice of browser. When someone complains to you directly about Google search results being shit, maybe point them to Kagi and mention it has a free trial account (100 searches still?) that's not time limited. That way they'll have a solution they can turn to the next time Google pisses them off significantly. :) reply xigoi 9 hours agorootparentI don’t know about you, but I’ve never had someone complain to me about Google search results, other than people who already don’t use Google. reply justinclift 8 hours agorootparentInteresting. I definitely have, though I do tend to hang out with IT-ish people a lot. reply dafty4 16 hours agorootparentprevThe above marginalia.nu just surfaced the following very nice article for me, which neither Google nor Kagi returned in their top 10 search results for the same terms ('thaad aegis hwasong'): https://nautilus.org/napsnet/napsnet-special-reports/the-rol... (duckduckgo.com returned it as the 5th returned result (not bad)), Thanks! reply aaomidi 17 hours agorootparentprevI’ll be honest, I don’t want my searches necessarily always tied to my identity. reply Ferret7446 14 hours agorootparentprevAre you really comparing typing a dozen characters into a browser's URL bar and hitting Enter with \"stop war\"? Using a different search engine, is just about the easiest kind of switch you could do. It's easier than, say, eating a different type of snack than you usually eat, or switching out your usual mug/plate/utensil. reply xigoi 9 hours agorootparentUsing a different search engine is easy. Voting for a good politician is also easy. But getting other people to switch to a different search engine is hard, just like getting other people to vote. reply 23B1 9 hours agorootparentprevYou missed the point. Have a great day. reply j_bum 18 hours agorootparentprev+1 Ok another note, your karma is very close to 2381. Make sure you get a screenshot :) reply MrSkelter 9 hours agoprevThis problem is age old and always the same. Optimizing for Google is a fools errand. You can win in the short term, but when their priorities change you will find your website excluded for the very things that boosted it previously. Google are unreliable and untrustworthy. Their focus is ad revenue for themselves and nothing else. Build sites for humans and let Google do what it wants to. reply AyyEye 18 hours agoprevI woke up this morning. Then checked my mailchimp subscriptions and my grow.me subscriptions And my substack subscriptions Made sure my Cloudinary was properly configured And my newrelic analytics And my sentry analytics And my rlcdn analytics And my growplow analytics And my 33across analytics And my Scorecard research analytics And my openxcdn analytics And my trustarc analytics And my creativecdn analytics And my Google Tag Manager And my Google Analytics Then I checked my Mediavine ads And my adsrvr ads And my adform ads And my adnxs ads And my yieldmo ads And my criteo ads And my mediavine ads And my pubmatic ads And my id5-sync ads And my rubicon-project ads And my triplelift ads And my pghub ads And my zemanta ads And my cognitivlabs ads And my doubleverify ads And my media.net ads And my kargo ads And my Amazon ads And my Google Adsense When I finally got to WeWork I booted up my Macbook And checked my Google Workspace email Our Amazon Affiliate account was approved And I checked our private Github issues for tasks Then I let Microsoft CoPilot write Stripe integration I write a new post for my Substack Finally using my Chrome browser and ycombinator's platform I posted: 'Google is killing the independent web' reply rockskon 18 hours agoparentWhat does any of what you cited have to do with the independent web? Should he have clarified \"properties not backed by tens, hundreds, thousands, or millions of millions of dollars?\" reply AyyEye 17 hours agorootparentI decided to remove my browser condoms and check Shepherd's network traffic. Updated my post to better reflect how \"independent\" the website is. That's less than half of the malware it tried to run on my computer but I got bored checking all the domains it tried to run code from. It's repetitive. I get it, you put on every tracker and ad network you could find, and put your stuff on a bunch of CDNs. Everything that can be outsourced to big tech was outsourced to big tech. How very independent. reply bwb 8 hours agorootparentGreat question :) Independent means that we are a small team and trying to create something cool for people. It means we are not part of Forbes or some giant publication that is focused purely on money and spamming the shit out of you :). You want a diverse and indie web so that the only search results and voices you here are Forbes, Disney, and other giant media conglomerates. Independent does not mean we don't eat. Independent does not mean we don't have server bills. Independent does not mean our part-time developer works for free. Independent does not mean our designer works for free. There is no malware on the website crazy; it's just an ad network that pays for our servers. I will remove all those ugly display ads as soon as we have enough income from readers and authors directly. reply nickburns 6 hours agorootparent> There is no malware on the website crazy Reasonable minds may differ. No need for name-calling. > I will remove all those ugly display ads as soon as we have enough income from readers and authors directly. Doubtful. reply AyyEye 1 hour agorootparentprev> It means we are not... focused purely on money and spamming the shit out of you Sure looks like you are. In fact I can't even find a single datapoint to the contrary. My browser made over 900 requests to other entities who's raison d'être is spamming me for money. Even your default affiliate link is Amazon instead of Bookshop for crying out loud. You didn't even put them side by side -- you hid bookshop behind a drop down menu. Seems like you don't care one bit about being independent. > You want a diverse and indie web so that the only search results and voices you here are Forbes, Disney, and other giant media conglomerates. I can't tell the difference between shepherd and the rest of the dead internet. There's only one person or a small team behind most blogspam and SEO-spam but it's still ruining the web. Independent web does not mean \"We're not rich (yet)\". Also 'web' implies links to other indie websites. A (hidden, even. Lol.) affiliate link to bookshop doesn't count. > Independent does not mean we don't eat. Independent means... Not dependent. You are dependent on Medium for Shepherd's blog for crying out loud. You could host your own blog on your server in minutes for zero marginal cost, and yet there you are paying big tech to put some words under a header. > Independent does not mean we don't have server bills. Use less (as-a-service). Even a small and cheap dedicated server or two should be able to handle this just fine. Maybe not as-built but you're doing nothing that needs expensive hosting. > Independent does not mean our part-time developer works for free. You have nearly 1000 subscribers and the minimum tier is $25. That's far from \"works for free\". > There is no malware on the website crazy. You don't even know what runs on your own website. It attempted to run code from hundreds of places who's whole purpose is to dox your users and run targeted ads to take their money. (That's money that might have been going to shepherd fwiw.) > just an ad network that pays for our servers \"just\" dozens of them. \"Just\" dozens of tracking and analytics. There are ways to do ethical advertising without tracking. You could even sell ads yourself (maybe you are, I haven't looked that closely at the website). How many servers do you have that those are a significant cost center when you have multiple employees? > I will remove all those ugly display ads as soon as we have enough income from readers and authors directly. Going into this I thought you were making shepherd yourself, but reading between the lines, you already hired 1-2 (or more) people before you made money. That's fine if this is a business venture -- but speculatively hiring multiple people in the hopes of becoming profitable is far from the \"indie web\". Light years away. It's standard operating procedure for the hyperscaler corporate web though. I'm not trying to be harsh here, I promise. My original post had a little bit of snark because of the medium blog. But every time I look at this it gets uglier. If you truly care to be independent you can. Get rid of the garbage you've accumulated (ad networks, trackers, minimize APIs, bring everything in-house). Slim down the network requests and JavaScript. Rethink how you do ads. Lastly, focus on being part of the indie web instead of another cog in the corporate web machine. reply bwb 1 hour agorootparentHah, I love HN :) I've worked on this full-time for almost 4 years for zero pay. I love what I am building and hope you are doing ok, my friend :). reply doublerabbit 18 hours agoparentprevFinally time to leave. I got in my friends Telsa Waiting at the traffic lights started looking at Twitter I got home and asked Siri For food from UberEats Only to get bored and open a tab of Reddit Followed by television series from Netflix With food from KFC I then decided to go for a pee Looked in the mirror And questioned who is me. reply neilv 17 hours agoprevAny idea how many Googlers still have the pre-dotcom-boom Internet technologist-citizen mindset? I know a bunch flocked to Google early on. But the landscape has changed a lot since then, and many people weren't even born until after the Internet and society were very different. reply bwb 8 hours agoparentAll my friends at Google left so they could get things done... I worry about that company. reply twothamendment 18 hours agoprevOnly somewhat related, but I was just complaining about search in Gmail. I normally use Thunderbird and didn't have it, so I used the web. Basic searches on the subject were so bad! I even put quotes around it and tried all of my Google-foo. It felt like I was fighting some AI that was sure it knew what I wanted despite my \"exact phrase\" search. reply StressedDev 13 hours agoprevGoogle recently changed who is running search. Basically, Prabhakar Raghavan was moved from running Search and ads to being Chief Technologist. Nick Fox is the new person running Google Search. This happened on 10/17/2024. For more information, please see https://searchengineland.com/google-shakes-up-leadership-rag... and https://www.theverge.com/2024/10/17/24272786/google-search-p... . I wonder if Prabhakar Raghavan was moved out of Google Search because of Search's decline in quality? reply tippytippytango 18 hours agoprevThese days I usually only search google for an exact phrase or title I got from chatgpt to check if it hallucinated. reply Tagbert 18 hours agoparent... And google ignores your exact phrase and returns something its semantics and ads engine determine is close enough. reply keyle 17 hours agoprevGoogle got so big it swallowed the internet. It now has digested it and what is left is... this. I used to love crafting websites and cared about SEO. What's the point now, no one is going to find your content. It won't even be on the third page. Google will answer questions by regurgitating whatever it swallowed on your websites and presume no one will click through, it won't even bother marking the authors. Instead it appears to be prioritising whichever website is going to give it revenue first, e.g. the click farms. The regular folks don't care, they google for stuff like \"am I dying if I have a pimple?\" (to which the answer is always yes, apparently). No one does actual meaningful research using Google anymore, if you do, good luck, get your gloves out . The global internet as it stands is close to dead. Discoverability of \"cool\" things is down to social media, tricked by \"influencers\", who are tricked by marketing themselves. We need a hard reset button, it needs to start from the ground up with site rings, and good content. Ah... that last part, \"good content\", is now stuffed with AI Samey McSamey sounding text. I really don't see a way out of here. The funny part is we used to think that the internet was going to change the world. We thought all idiots needed was information. Access to information would fix the world! Instead, it only has given the village idiots a global voice: if you can think of some dumb crazy thing, you'll find dumb crazy people agreeing with you, so you must be right! I've been on the internet since 1997 and I think it's the worst it's ever been. reply NetOpWibby 59 minutes agoparent> The funny part is we used to think that the internet was going to change the world. We thought all idiots needed was information. Access to information would fix the world! I was 9 in '97 and wasn't really aware of the internet until maybe 3 or 4 years later. The internet seemed like a magical place, where reason and common sense existed (unlike the messy meat world). I bought into the same ideals that information was power and once everyone had access to information, we'd collectively get smarter and wiser. Less wars! I remember being excited for Google Search and then Gmail. FINALLY, a company that gets it! It seems that any public or VC-backed company is destined for enshittification. reply djbusby 17 hours agoparentprevOpportunity for a new discovery engine. Not Kagi (I buy it) but a search focused on the small web, find stuff not in the top 100k sites. reply NetOpWibby 1 hour agorootparenthttps://millionshort.com/ reply klooney 16 hours agorootparentprevhttps://search.marginalia.nu/ check out Marginalia reply keyle 15 hours agorootparentI love this search but I feel it's still very specialised. reply DrillShopper 17 hours agorootparentprevEnjoy being absolutely crushed by Google and having to fight to get users to even know you exist, let alone use you reply johnnyanmac 17 hours agorootparentprevI wonder how feasible that is without leveraging one of the two existing search engines in the backend. I always pitted a general search engine as a top 5 difficult tech project to go for. reply AlienRobot 16 hours agorootparentprevhttps://wiby.me/ reply maytc 18 hours agoprevSwitched to Perplexity Pro and haven't looked back reply NetOpWibby 1 hour agoparentI'm already paying for Kagi and Claude. Is Perplexity Pro worth the extra subscription? reply ashleyn 17 hours agoparentprevCtrl+F'd for Perplexity. I knew Google was cooked the minute Perplexity worked better for questions about an obscure embedded systems SDK. It has little documentation, but a lot of mailing list and github issues. Google spits out the front page of the project and shrugs; Perplexity actually answers the question. The usual caveats for LLM hallucination apply. reply untangle 12 hours agoparentprevSame. Try the \"books on the Battle of Midway\" query on Perplexity. The results are great and include the book mentioned in the article (authored by the Naval Aviator). reply G_o_D 18 hours agoprevAlways use google in incogniti so they dont give personalised results, thats the least we can do before google search completely dies, Or just prevent google any cookie permission, keep it session only As far as query trackers on google urls, they are necessary One good case of necessity of google tracking parameters is --> you search for movie or related, but muktiple movies by same name have been made , you dont remember year, you do remember some actor or story description Searxg and google will in show with media carda foe movies you ckick iy and yoi get exact same film with year yoi wantes Wait ->I am telling above because when that mefia image of film i inspecred itsurl it was just Googke ?search=filmname&teackers The search ket wasnt modified with year same as i searched first yet it gave me the desires year film may be because of trackers &ved amd all paramers link tags to search teaukts its loke ctoss maching in databases reply hhdhdbdb 16 hours agoprevGoogle can't please all of the sites all of the time, or all the visitors. It is too big to evem worry about that 4k a day clicks for one site. It is like us optimizing the expense of 0.01c. It makes a difference when that 0.01c is an API call that you call a million times. But it only surfaces if you do aggregate it. Therefore this problem can only even be seem by Google if it can be surfaced in aggregate overy say a billion queries. I wonder how that can be done. Probably only can be done using data. Which means spying on people in various ways. And making assumptions about length of time on site equals quality. They probably use machine learning too. There may be no reason for the lost rankings other than a wind change caused by some updated parameters in an OKR chasing model. reply hn_throwaway_99 16 hours agoparentI realize \"did you actually read the article\" is against HN guidelines, but what else am I supposed to say when I see this at the top of the comment thread? I mean when you say \"Google can't please all of the sites all of the time, or all the visitors.\", I wholeheartedly agree, but this blog post was excellently sourced with data that shows exactly how Google raising sites that any reasonable human would say are considerably shittier than this site that is getting down ranked. It also seems pretty clear that the things that have changed are Google's ranking algorithm at specific points. > They probably use machine learning too. There may be no reason for the lost rankings other than a wind change caused by some updated parameters in an OKR chasing model. That is literally what TFA says in the very first section: \"Some people believe they have lost control of their AI ranking systems, ...\" reply hhdhdbdb 15 hours agorootparentI think I said a lot more than TFA on those points. reply NetOpWibby 17 hours agoprevWhy don't people just stop using Google? And by people, I mean everyone here. Whenever this point comes up, I see people claim they ONLY see the results they want in Google. How would you know if you don't actually use anything else? Kagi is excellent search. Neeva was pretty great when it was active. DuckDuckGo is passable. Idk how Qwant gets money but it's been around a bit. Complaining about the same thing forever and expecting a change doesn't make any sense. Y'all are in abusive relationships with Google and refuse to leave. Sure, your job may use Google Suite and you need to make money. What about the rest of your life? Stop hitting yourself. reply johnnyanmac 17 hours agoparentOnce I have stable income I do plan to use Kagi. Though I do recognize that even Kagi partially has Google Search underneath. Even if you fully de-googlefy it's hard to not be indirectly supporting Google. Been working on it for 2 years now and even then I'm more or less locked into 3 services 1. Gmail, 20 year old account I use for pretty much everything business. Even if I move I'll need to forward Gmail stuff to a new email for years. 2. YouTube. Pretty self explanatory (and the go to for why monopolies are always bad). Trying to avoid it entirely is like trying to avoid dang Twitter. Too many other companies use it as a go to for any video, no matter how inefficient (nothing better for graphical showcases than nitrate compression ruining all the details) 3. Play store. Used android all my life and while I can mostly move out I will be missing some critical apps from that result (financial apps are a big example) It's a network effect like any other for 2 of those, and a lock in part of my online identity for another. reply NetOpWibby 12 hours agorootparentI can't help you with points 1 and 3 but for 2: do you comment on YouTube videos or upload your own? If you just consume content (like I do), you can subscribe to your favorite channels with RSS. Go to a channel and inspect source for \"rss\" to find the link. I'm partial to NetNewsWire for my feed reader. reply hresvelgr 17 hours agoparentprev> Kagi is excellent search. I will boost this. I am very happy with Kagi. I'm on ultimate and am glad to have my money taken for reliable and high-quality search. > Complaining about the same thing forever and expecting a change doesn't make any sense. Y'all are in abusive relationships with Google and refuse to leave. This would be a more solid stance if they were complaining about this as a consumer and not as a business operator. They can't control what search engines other people use. The best they can do is optimise for other engines like Bing and DuckDuckGo. The other alternative is finding other avenues to advertise away from search engines, which may be what you were alluding to. reply NetOpWibby 12 hours agorootparent> The other alternative is finding other avenues to advertise away from search engines, which may be what you were alluding to. I was actually speaking from the consumer perspective, thanks for making the distinction. The OP has valid reasons for being upset with the most popular search engine ruining visibility. To be fair, I have no idea what business could/should do. Bigger businesses can afford to hire social media folks to keep them relevant on Facebook or whatever. SMBs/mom & pops are screwed. reply anigbrowl 17 hours agoparentprevI used DDG as my default search engine for several years, but I kept having to go back to Google because it was consistently more reliable even though it has more crap. I hear Kagi is great but I am not signing up just to find out, nor do I feel confident that 100 searches are enough to make a decision about paying for it. I think Google is garbage but I have yet to se anything that is consistently qualitatively better. reply NetOpWibby 12 hours agorootparent> or do I feel confident that 100 searches are enough to make a decision about paying for it I didn't think so either, coming from Neeva. Of course, YMMV. > I have yet to se anything that is consistently qualitatively better. And you won't, until you try. reply jorvi 17 hours agorootparentprev100 searches last you a lot longer than you think, especially if you delegate “postal code Wall Street New York”-type searches to Google via “!g” prefixing. It’ll give you time to get used to Kagi and set up some personal up- and downranks and blocks. Especially together with the Summarizer and Lenses your search result quality will dramatically improve. reply anigbrowl 16 hours agorootparentNo doubt this is true for you, but my search usage patterns are quite different. 100 searches in a day is nbd for me, sometimes I go through that in a few hours. reply throwaway48476 17 hours agoparentprevBecause the problem isn't just Google. The internet is a garden whose quality content was left to wither. reply bwb 8 hours agoparentprevIt stars with \"m\" and ends with \"onopoly.\" You can't escape them. I use Kagi, but I am a nerd. reply NetOpWibby 1 hour agorootparent> I use Kagi, but I am a nerd. That's why I emphasized \"everyone here.\" My wife isn't gonna stop using Google, and Kagi isn't a default option choice on iOS. Nerds will go out of their way to use a product. reply meiraleal 17 hours agoparentprev> Complaining about the same thing forever and expecting a change doesn't make any sense. Do you know that this forum isn't one person and thousands of people access it in different days and time, right? The notion that the same people are complaining while still using Google is a projection of your mind. reply donatj 17 hours agoprevChecking my Search Console Tools recently, the number of \"Crawled - currently not indexed\" pages across all my sites has risen sharply recently, especially on sites where the age of the content is more than a few years. I put a fair bit of time into trying to improve the sites with JSON-LD and breadcrumbs and what not. It seems to have helped just a little bit. I don't make any money off any of it, but it's still kind of irritating that no one can find it. reply Seattle3503 18 hours agoprevThe author talks about how long users spend on his page, but can Google track that? Has ubiquitous tracking blocking, and Google's failure to adapt, eroded the quality of search? reply bwb 8 hours agoparentThere have been links from the DOJ case showing Google is pulling in Chrome data. But its just one data point and nobody knows. It seems more likely at this stage that Google has lost control of their AI system. they trained it on x websites, and indie websites got swept into that. And they don't care. reply DrillShopper 17 hours agoparentprevWe don't even have ubiquitously used ad blocking right now. Do you think some normie is even going to know what a tracker is and why they should block it? reply aaomidi 17 hours agoparentprevUbiquitous tracking blocking? Where? reply squidhunter 17 hours agoprevThe indie web was around before google and it will be around long after google is gone. I would argue that the indie web has incurred a much larger loss from people thinking seo/engagement metrics are something worth optimizing. Many of the best examples of the indie/small web don’t have js tracking and little to no css. reply grugagag 19 hours agoprevGoogle didn’t shit the bed, they just did whatever was profitable to them and they’ll continue doing so until they either fail or there’s no more profit to be squeezed. They dropped the “don’t be evil” motto a long tome ago. reply tessierashpool9 7 hours agoprevbut to google you're just a desperate flea from the indie web. reply jeffbee 19 hours agoprevI've had my lifetime quota of hearing the rants of guys who think their search quality metric is objectively superior, oh and by a total coincidence their preferred ranking gives them a direct financial benefit. Just a complete failure to imagine the larger issues. reply GolfPopper 17 hours agoprev\"It is difficult to get a man to understand something when his salary depends upon his not understanding it.\" - Upton Sinclair reply 4b11b4 18 hours agoprevexa.ai good for finding these sites? but yes, many people will still be googling reply znpy 8 hours agoprevMaybe we should just start looking at other search engines seriously and without irony. How’s shepherd.com (for example) doing on bing or duckduckgo? Maybe it’s time for google to die. reply paulista_tcb 16 hours agoprevyou should check out exa.ai/search if you miss indie content! reply awinter-py 18 hours agoprevthought this was going to be about the 'log in with google' popup reply jasonvorhe 19 hours agoprevIf you're still barking in the general direction of google and similarly sized search engines, good luck. I'd rather look towards kagi and brave. reply lolinder 19 hours agoparentKagi is a viable choice for individual people, but it's not an answer for the concern of how to get traffic to visit your site and click on your affiliate links (which is what TFA is about). Kagi works well for users precisely because it's small enough that no one benefits from gaming its algorithms. reply Ferret7446 14 hours agorootparentWhy is it Google's job, who TFA isn't paying AFAIK, to get traffic to visit their site? reply lolinder 14 hours agorootparentI don't think it is, but recommending Kagi as a solution for their complaint misses their point. reply rockskon 18 hours agorootparentprevHow is Kagi faring after Google's exclusivity deal with Reddit? reply klooney 16 hours agorootparentI think Kagi uses Google's APIs as a part of their mix of search results. reply lolinder 17 hours agorootparentprevUm, I don't know when that would have kicked in because I haven't noticed a difference. Reddit still shows up whenever it's relevant, and the Forums lens turns up even more Reddit if you're worried you're missing any on the initial search. reply viraptor 19 hours agoparentprevI feel too that we're already past the point where they could fix things. They must be aware of issues and do the exact opposite of fixing them. It's a question how fast we can convince general public to move away, rather than when Google will improve. reply jasonvorhe 19 hours agorootparentI wouldn't give much regarding the general public. If you have a small blog they're usually not your target audience. Accept that the days of \"making a few bucks off of advertising\" on hobby publishing are over, focus on writing about things you care about without it reeking of being written by ChatGPT and chances are people will come at some point. If small publishers don't change their behavior in radical ways, nothing will ever change. reply viraptor 18 hours agorootparent> nothing will ever change. That's probably what MySpace people thought. And Friendster. And Altavista. And ... reply jasonvorhe 10 hours agorootparentI don't get what you're trying to say. I'm not arguing in favor of doing nothing in an ever-changing market, which is what Myspace and Friendster did most of the time. I'm saying the old world of \"have good content/SEO and you'll see some traction thanks to Google\" is dying and that you can either adapt or drown in a flood of LLM-created SEO blackhat nonsense. Google does not care about you and they've been in the \"too big to fail\" corner for too long. If you want content made by humans to continue to exist and want to see some of them be able to make a buck off of it, support smaller search engines and creative people and their publications. reply viraptor 8 hours agorootparentI mean that services will come and go. Some will stay for longer than others, but even without a coordinated action, Google may go away. It's also losing to Perplexity and ChatGPT with many people. And many geeks are very vocal about their love for Kagi. (Including me) Things may change over time and may not be too big to fail. reply wbl 19 hours agoparentprevAnd how is the experience? Even with Bing I find I have to switch to Google to get useful results for my queries. reply viraptor 18 hours agorootparentKagi is simply better in every way I can think of and I've been a paying user for over a year now. reply alpaca128 19 hours agorootparentprevIt took me about two weeks until I got consistently good results with DDG, it often just takes about one more search term because it doesn't try to (often incorrectly) guess the context based on search history like Google does. That was years ago, since then DDG has gotten better and Google a lot worse. I could understand your complaint if it was about image search, but I don't see how anyone can look at today's Google results and think they're still superior. reply girvo 19 hours agorootparentprevGoogle is so odd these days, its like there are two \"streams\" of results. One is the kind of useless slop that the article in question is rightfully complaining about, but other results are still useful and high quality. The second stream is what I seem to get when I'm forced to \"!g\" when I'm using DuckDuckGo, and I can't work out why theres this stratification, this separation. Years ago, it was very rare to get crap results from Google. Now its far more common. reply justinclift 17 hours agorootparentprevHaven't needed to use Google at all since using Kagi several months ago, whereas when using Duck Duck Go for a few years (prior to Kagi) I'd (very occasionally) need to. reply senko 19 hours agorootparentprevKagi user here, I almost never need to !g. Used DDG a few years back and stopped after I had to fallback to Google for most qs. reply Sincere6066 17 hours agoprevis there a way to detect and block substack? reply Havoc 17 hours agoprevBig adtech cares not as long as the ads keep flowing reply AlienRobot 16 hours agoprevAs much as I share the frustration, what does Google gain from providing a better service? Its only serious competitor is Bing, which isn't even a search engine anymore but a billboard for Microsoft to advertise ChatGPT. reply bwb 8 hours agoparentPride. Happiness. Contentment. A workforce that is motivated and happy. People don't want just money. reply nextworddev 17 hours agoprevThe real problem is that Google solves for the needs of the lowest common denominator of content consumers, which leads inevitably to enshittification reply bun_terminator 12 hours agoprevHi stranger, please stop messing with the scrollbar reply immibis 19 hours agoprevHi (company that benefits from hurting me), please stop hurting me. Company: No. reply 23B1 19 hours agoprevThere are people inside Google who can put a stop to this, if they merely stand up and say \"this is wrong\" including their CEO. It simply requires bravery and faith in Google's institutional ability to continue to profit doing different things. Big companies have pivoted before on the heels of brave executives. This applies as much to search as it does to privacy, AI, their transparency, their support of open source, and their weaponization of their browser. It's a crying shame to see how enshittified a company that could be changing the world has become. reply immibis 19 hours agoparentAnd a willingness to be fired the next day. Even the CEO can be fired. reply mcmcmc 18 hours agorootparentSo it’s the shareholders and the board who are at fault then. I’d posit that anyone holding Google stock is part of the problem and responsible for their continued pattern of anti-user and anti-consumer behavior. reply immibis 6 hours agorootparentShareholders who don't want to maximize their share value are very strongly incentivized to fire themselves. reply DrillShopper 17 hours agorootparentprevIt's actually late stage capitalism that is at fault. Line must always be going up, and if it doesn't go up as fast as someone thought it would then we make it go down. If line goes sideways we sell and make it go down. It must go up. Always up. Or it will go down. reply mcmcmc 15 hours agorootparentYou can’t blame the system and pretend none of that lands at the feet of people willingly engaging with it for their own benefit. reply Nasrudith 15 hours agorootparentprevHas nobody heard of paying dividends? It is a perfectly acceptable way to have a company with stable profits. But no, anti-capitalism cliches combined with warmed-over rapture logic and Marx's idiotic teleological view of history. reply 23B1 9 hours agorootparentI'm a fairly libertarian moderate but tyranny-by-proxy is still tyranny. reply 23B1 18 hours agorootparentprevYes. That's the bravery part. reply g8oz 17 hours agoprev [–] Well they replaced the Yahoo alum accused of tanking search quality with a McKinsey guy....I'm sure things will get better now /s. https://www.wheresyoured.at/requiem-for-raghavan/ reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Independent websites are experiencing a significant decline in visibility on Google search results, despite adhering to guidelines, with Shepherd.com reporting an 86% drop in traffic over 16 months.",
      "This trend is affecting numerous websites, leading to substantial traffic losses and financial difficulties, while other search engines like Bing and DuckDuckGo are ranking independent content more favorably.",
      "The call to action is for Google to improve its search engine algorithms and enhance communication with website owners to protect the independent web."
    ],
    "commentSummary": [
      "The indie web community expresses frustration with Google's search results, which tend to prioritize affiliate link aggregators over authentic content.",
      "Critics claim that Google's algorithms favor large brands and ad revenue, marginalizing independent websites.",
      "There is a call for considering alternative search engines like Kagi or DuckDuckGo and a broader shift away from Google's dominance, highlighting concerns about search quality and its impact on smaller publishers."
    ],
    "points": 245,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1730324115
  },
  {
    "id": 42001642,
    "title": "It might be possible to detect gravitons after all",
    "originLink": "https://www.quantamagazine.org/it-might-be-possible-to-detect-gravitons-after-all-20241030/",
    "originBody": "Home It Might Be Possible to Detect Gravitons After All Read Later Share Copied! (opens a new tab) Comments Read Later Read Later quantum gravity It Might Be Possible to Detect Gravitons After All By Charlie Wood October 30, 2024 A new experimental proposal suggests detecting a particle of gravity is far easier than anyone imagined. Now physicists are debating what it would really prove. Read Later Capturing a graviton would be akin to noticing just one molecule in an ocean wave. Señor Salme for Quanta Magazine Introduction By Charlie Wood Staff Writer October 30, 2024 View PDF/Print Mode experimental physics fundamental physics gravity physics quantum gravity quantum physics All topics (opens a new tab) Detecting a graviton — the hypothetical particle thought to carry the force of gravity — is the ultimate physics experiment. Conventional wisdom, however, says it can’t be done. According to one infamous estimate, an Earth-size apparatus orbiting the sun might pick up one graviton every billion years. To snag one in a decade, another calculation (opens a new tab) has suggested, you’d have to park a Jupiter-size machine next to a neutron star. In short: not going to happen. A new proposal overturns the conventional wisdom. Blending a modern understanding of ripples in space-time known as gravitational waves with developments in quantum technology, a group of physicists has devised a new way of detecting a graviton — or at least a quantum event closely associated with a graviton. The experiment would still be a herculean undertaking, but it could fit into the space of a modest laboratory and the span of a career. “It’s something that can be reached in a few years of research,” said Matteo Fadel (opens a new tab), an experimentalist at the Swiss Federal Institute of Technology Zurich (ETH Zurich) who was not involved in the proposal. “It’s a very original proposal and well thought-out,” said Frank Wilczek (opens a new tab), a Nobel Prize-winning physicist at the Massachusetts Institute of Technology with a long-running interest in graviton detection. “It would be real progress in the field.” Currently, Albert Einstein’s general theory of relativity attributes gravity to smooth curves in the space-time fabric. But a conclusive graviton detection would prove that gravity comes in the form of quantum particles, just like electromagnetism and the other fundamental forces. Most physicists believe that gravity does have a quantum side, and they’ve spent the better part of a century striving to determine its quantum rules. Nabbing a graviton would confirm that they’re on the right track. But even if the experiment is relatively straightforward, the interpretation of what, exactly, a detection would prove is not. The simplest explanation of a positive result would be the existence of gravitons. But physicists have already found ways to interpret such a result without reference to gravitons at all. Share this article Copied! (opens a new tab) Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters (opens a new tab) Albert Einstein published the current theory of gravity, called general relativity, in 1915, a few years before this photo was taken in his office at the University of Berlin. ETH Zurich University Archive The discussion recalls a messy, largely forgotten episode from the dawn of the quantum era. In 1905, Einstein interpreted experimental data to mean that light is “quantized,” coming in discrete particles now called photons. Others, including Niels Bohr and Max Planck, thought that the classical, wave nature of light might still be saved. It would take seven decades for physicists to undeniably establish that light is quantized, largely because of the subtle nature of quantumness. Most physicists presume that everything in the world is quantized, including gravity. But proving that assumption will entail a new war, one that has only just begun. Clicks From Gravity It’s hard to experimentally probe gravity because the force is extremely weak. You need huge masses — think planets — to significantly warp space-time and generate obvious gravitational attraction. By way of comparison, a credit card-size magnet will stick to your fridge. Electromagnetism is not a subtle force. One way to study these forces is to disturb an object, then observe the ripples that travel outward as a consequence. Shake a charged particle, and it will create waves of light. Disturb a massive object, and it will emit gravitational waves. We pick up light waves with our eyeballs, but gravitational waves are another matter. It took decades of effort and the construction of the colossal, miles-long detectors that make up the Laser Interferometer Gravitational-Wave Observatory (LIGO) to first sense a rumble in space-time in 2015 — one sent out by a collision between distant black holes. Detecting a single graviton would be harder still, akin to noticing the effect of just one molecule in an ocean wave. How hard would it be? In a lecture in 2012, the eminent physicist Freeman Dyson considered (opens a new tab) gravitational waves from the sun, where the violent churning of matter inside the star should constantly send out mild tremors in space-time. Occasionally, one of the gravitons in these ripples would strike an atom in a detector and kick an electron into a higher energy level. Dyson calculated that in a detector as large as Earth, running for the 5-billion-year lifetime of the sun, such an effect might be seen just four times. A calculation by the late physicist Freeman Dyson, pictured in his office at the Institute for Advanced Study, suggested that individual gravitons will never be detected. D. Komoda In the dozen years since Dyson’s remarks, two experimental developments have made the situation less dire. First, LIGO began detecting gravitational waves regularly from black hole collisions, and occasionally from colliding neutron stars. These events shake space-time far more intensely than the sun’s internal agitation does — providing a deluge of gravitons as opposed to Dyson’s trickle. And second, experimentalists have grown more capable of eliciting and measuring quantum phenomena. Igor Pikovski (opens a new tab), a theoretical physicist now at the Stevens Institute of Technology in New Jersey, has been mulling over these developments since 2016. At the time, he and three collaborators noted that a vat of superfluid helium — which displays quantum properties despite having a large mass — could be set up to reverberate (opens a new tab) in response to certain gravitational waves. It would be our first window into where quantum gravity matters. Igor Pikovski It would take another conceptual leap to go from a gravitational wave detector to a detector for individual gravitons. In the recent paper, which appeared in Nature Communications (opens a new tab) in August, Pikovski and his co-authors outlined how the graviton detector would work. First, take a 15-kilogram bar of beryllium (or some similar material) and cool it almost all the way to absolute zero, the minimum possible temperature. Sapped of all heat, the bar will sit in its minimum-energy “ground” state. All the atoms of the bar will act together as one quantum system, akin to one hulking atom. Then, wait until a gravitational wave from deep space passes by. The odds that any particular graviton will interact with the beryllium bar are low, but the wave will contain so many gravitons that the overall odds of at least one interaction are high. The group calculated that approximately one in three gravitational waves of the right sort (neutron star collisions work best since their mergers last longer than black hole mergers) would make the bar ring with one quantum unit of energy. If your bar reverberates in concert with a gravitational wave confirmed by LIGO, you will have witnessed a quantized event caused by gravity. Mark Belan/ Quanta Magazine “It would be our first window into where quantum gravity matters,” Pikovski said. Among a handful of engineering hurdles involved in opening that window, the highest would be putting a heavy object into its ground state and sensing it jumping to its next-lowest-energy state. One of the groups pushing the state of the art on this front is at ETH Zurich, where Fadel and his collaborators cool tiny sapphire crystals until they display quantum properties. In 2023, the team succeeded in putting a crystal into two states simultaneously (opens a new tab) — another hallmark of a quantum system. Its mass was 16 millionths of a gram — heavy for a quantum object, but still half a billion times lighter than Pikovski’s bar. Nevertheless, Fadel considers the proposal to be achievable. “It wouldn’t be too crazy,” he said. Pikovski’s experiment — like Dyson’s — emulates the very experiment that prompted Einstein to propose in 1905 that light is quantized, a watershed moment in the history of quantum mechanics. “If carried through, it would bring the state of the art in the case for gravitons to the same level that it was for photons in 1905,” Wilczek said. Igor Pikovski, a physicist at the Stevens Institute of Technology, has proposed a way to detect a quantized response to a gravitational wave. Ian Reitz Textbooks often credit Einstein’s paper with establishing the photon’s existence. But the real story is far more interesting. At the time, many physicists rejected Einstein’s theory. Some wouldn’t come around for two decades. In their view, the experiment fell far short of conclusive proof. It was, rather, an opening argument in a decades-long war fought to determine the true nature of light. The Real Story of the Photon Physicists saw the first cracks opening up in their classical understanding of reality in the closing years of the 19th century. J.J. Thomson discovered that electric currents come in discrete chunks of charge called electrons. Meanwhile, physicists were puzzling over a string of experiments by Heinrich Hertz and others that used light to make a current flow — a phenomenon that came to be called the photoelectric effect. The puzzle was that when they directed dim beams of light at a metal plate, sometimes an electric current flowed across the plate and sometimes it didn’t. In the pre-quantum world this was hard to explain. It was believed that any wave should create at least a small current, and brighter waves should create larger currents. Instead, physicists found that there was a special color of light — a frequency — that got a current to flow. Only waves of that frequency or higher could start a current. Brightness had little to do with it. LOADING LOADING 1865 Waves of Light James Clerk Maxwell publishes his theory of electromagnetism. One of its predictions is that light is a smooth wave that moves through the electromagnetic field at a constant speed. LOADING 1886 — 1902 The Photoelectric Effect Heinrich Hertz notices that only particular colors of light generate electric currents, which Maxwell’s classical theory can’t explain. “I confine myself at present to communicating the results obtained, without attempting any theory respecting the manner in which the observed phenomena are brought about,” Hertz writes. This will later be called the photoelectric effect. LOADING 1897 — 1899 Quantized Current J.J. Thomson finds that current flows in tiny chunks of charge. He suggests that electric charge is carried by subatomic “corpuscles,” now known as electrons. LOADING 1900 Quantized Emission To explain why objects emit the particular colors of light that they do, Max Planck proposes that matter loses energy only in packets of certain sizes. He begins to write about these “quanta” of matter and electricity. LOADING 1905 Quantized Light Inspired by the photoelectric effect and Planck’s theory, Albert Einstein argues that light itself travels in countable bundles of energy he calls “light quanta.” He suggests that each one’s energy is proportional to the light’s frequency. LOADING 1906 — 1916 Doubting Einstein Robert Andrews Millikan rejects Einstein’s theory that light is quantized, as do Planck and other quantum pioneers. Millikan spends a decade measuring in detail whether higher frequencies of light really carry more energy. The empirical law holds, but he remains unconvinced that only the notion of light quanta can explain it, writing in 1916 that Einstein’s theory is “untenable” and “reckless.” LOADING 1918 Planck’s Nobel Planck wins the Nobel Prize “in recognition of the services he rendered to the advancement of Physics by his discovery of energy quanta.” LOADING 1921 Einstein’s Nobel Einstein wins the Nobel Prize “especially for his discovery of the law of the photoelectric effect,” despite continued skepticism of light quanta among his peers. LOADING 1923 Evidence Accumulates Arthur Compton scatters light off electrons and observes that — on average, over many collisions — momentum is quantized as well as energy. LOADING 1923 Consolation Nobel While failing to disprove Einstein’s theory, Millikan has succeeded in making quantum measurements of unprecedented precision. He receives the Nobel “for his work on the elementary charge of electricity and on the photoelectric effect.” LOADING 1924 Final Revolt Niels Bohr and collaborators mount a last-ditch effort against the growing conclusion that light travels in energy bundles. They theorize that light is a continuous wave, and that quantization arises in the way light interacts with matter, not in light itself. Their theory violates the conservation of energy and momentum, however, by allowing matter to gain quanta in certain interactions even though the light wave does not always lose the same amount of energy. 1925 Light Quanta Established Compton and A.W. Simon conduct experiments confirming that energy and momentum are conserved in every individual scattering event. The last bit of skepticism that light is quantized dies. Bohr writes that he and his colleagues should “give our revolutionary efforts as honorable a funeral as possible.” LOADING 1926 Photons Named Gilbert Newton Lewis suggests that light is made of a “new kind of atom … uncreatable and indestructible,” which he names the photon. His theory will prove mistaken (photons are both created and destroyed), but the term catches on as a way to refer to Einstein’s light quanta. LOADING 1977 Photons Proven H. Jeff Kimble, Mario Dagenais and Leonard Mandel detect statistical patterns that could come only from a stream of separated photons, finally proving that no classical theory of light is viable. Waves of Light 1865 Waves of Light The Photoelectric Effect Quantized Current Quantized Emission Quantized Light Doubting Einstein Planck’s Nobel Einstein’s Nobel Evidence Accumulates Consolation Nobel Final Revolt Light Quanta Established Photons Named Photons Proven 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 Einstein proposed a solution in 1905: A wave of light is made of many discrete units called “quanta,” each with energy related to the wave’s frequency. The higher the frequency of the wave, the more energetic its quanta. And the brighter the wave, the more quanta there are. If you try to start an electric current in a metal plate with low-frequency red light, you’ll be no more successful than if you tried to knock over a refrigerator with ping-pong balls; no number would suffice. But using higher-frequency blue light is like switching to boulders. Each of those units has enough oomph to excite an electron, even in dim light with very few of them. Einstein’s theory was met with skepticism. Physicists felt fiercely protective of James Clerk Maxwell’s then-40-year-old theory of light as an electromagnetic wave. They had seen light refracting, diffracting, and doing all the things waves do. How could it be made up of particles? Even after Einstein won the 1921 Nobel Prize in Physics for his theory of the photoelectric effect, debate continued among physicists. The effect suggested that something is quantized; otherwise there wouldn’t be a minimum threshold required to get electrons flowing. But some physicists, including Niels Bohr, who is considered one of the founders of quantum theory, continued to explore the possibility that only the matter was quantized, not the light. Today, this type of theory is called “semiclassical” because it describes a classical field interacting with quantized matter. To see how a semiclassical theory can explain the photoelectric effect, imagine a kid on a swing. They’re kind of like an electron in a metal. They have a ground state (not swinging) and an excited state (swinging). A classical wave is like giving the kid a series of pushes. If the pushes come at some random frequency, nothing happens. The kid might bounce around a little, but they will basically stay in their ground state. It’s only when you push with just the right frequency — the swing’s “resonant” frequency — that the kid accumulates energy and starts swinging. (Electrons in a metal are a little different; they resonate with a whole continuous “band” of frequencies instead of just the one. But the upshot is the same: Any wave below that frequency band does nothing, whereas any wave in that frequency band excites electrons and makes a current flow.) Einstein was eventually vindicated, but not on the strength of the photoelectric effect alone. Later experiments that collided electrons and photons like projectiles found that momentum, too, came in chunks. This research eventually ruled out the main alternative — a semiclassical theory of light and matter from Bohr and his collaborators. In 1925, seeing the data, Bohr agreed to “give our revolutionary efforts as honorable a funeral as possible” and welcomed light into the quantum fold. Light quanta became known as photons. Few doubted the photon after 1925, but physicists are nothing if not thorough. Just because no one could think of a viable semiclassical theory didn’t mean one didn’t exist. The final proof (opens a new tab) that photons are real came in the late 1970s, when quantum optics researchers showed that light arrived at a detector in a pattern no semiclassical theory could mimic. The experiments were akin to firing a photon gun once per second and confirming that the detector clicked once per second in response. The photon wars ended with a whimper. “There were just mountains of evidence that this photon concept was useful and vital,” Wilczek said. The Graviton Wars Begin In August of 2023, Daniel Carney (opens a new tab) and his collaborators fired the first shot in a new war. It started when Carney’s colleague Nicholas Rodd had an insight similar to Pikovski’s, about a possible way to detect a graviton. “We got super pumped,” said Carney, a physicist at Lawrence Berkeley National Laboratory. But when he and his collaborators dug into the literature, they uncovered the messy history of the photon, and the lengths to which quantum optics researchers had gone in the 1970s to close the final loopholes. They translated those more stringent tests into the gravitational context and found that Dyson had been right. Really proving quantumness by detecting lone gravitons one after another — as opposed to plucking one out of a tsunami, in the style of Pikovski’s proposal — would indeed take planetary-scale machinery. Daniel Carney, a physicist at Lawrence Berkeley National Laboratory, argues that a proposed experiment would not offer conclusive proof of quantum gravity. Lawrence Berkeley National Laboratory “It was crazy to have to revise your hypothesis by 100% really fast,” Carney said. Now graviton chasers find themselves in a peculiar position. On the main facts, everyone is in agreement. One, detecting a quantum event sparked by a gravitational wave is — surprisingly — possible. And two, doing so would not explicitly prove that the gravitational wave is quantized. “Could you make a classical gravitational wave that would produce the same signal? The answer is yes,” said Carney, who along with two co-authors analyzed this type of experiment in Physical Review D (opens a new tab) in February. How much physicists feel they would learn from the experiment varies. To some, it would strongly suggest that gravity is a quantum force because the alternative — a semiclassical theory of gravity and matter — is disfavored on other grounds. Such theories violate the conservation of energy, for instance. If the beryllium bar gains one quantum of energy, then energy conservation requires that the gravitational wave must have lost one quantum of energy — and therefore it must be quantized, too. (Einstein advanced this sort of argument for the photon in 1911.) Semiclassical theories save gravity’s classicality by sacrificing this revered principle. We’re so biased to think that everything is quantum that you should really be doing a lawyerly thing. Daniel Carney “Unless you use very artificial interpretations,” Wilczek said, “it does tell you that you really should apply quantum mechanics to the gravitational wave.” “If I want to see signatures of quantumness, it’s not my first goal to rule out these pathological things,” Pikovski said. To physicists such as Carney, however, a mere strong suggestion that gravity is quantized isn’t all that informative. We already have an abundance of strong suggestions that all of reality is quantized, he says. What’s needed is proof — such as experiments that would close the remaining loopholes, no matter how bizarre they might seem. “We’re so biased to think that everything is quantum that you should really be doing a lawyerly thing,” he said. A Starting Point While Pikovski’s proposal is not a loophole-closing experiment, many physicists would still like to see it happen. It would mark the dawn of an era of experimental quantum gravity, which until recently seemed quite far off. “This is an exciting paper,” said Alex Sushkov, an experimental physicist at Boston University. “These are hard experiments, and we need bright, smart people to move in this direction.” “We can take it as a starting point,” said Myungshik Kim (opens a new tab), a physicist at Imperial College London. Related: Physicists Find a Way to See the ‘Grin’ of Quantum Gravity How the Bits of Quantum Gravity Can Buzz Why Gravity Is Not Like the Other Forces It might motivate subsequent experiments that would take physicists deeper into the quantum gravity era, just as scattering experiments once took them deeper into the era of the photon. Physicists now know that quantum mechanics is much more than quantization. Quantum systems can take on combinations of states known as superpositions, for instance, and their parts can become “entangled” in such a way that measuring one reveals information about the other. Experiments establishing that gravity exhibits these phenomena would provide stronger evidence for quantum gravity, and researchers are already exploring what it would take to carry them out. None of these tests of gravity’s quantum side are completely ironclad, but each would contribute some hard data regarding the finest features of the universe’s weakest force. Now a frigid quantum bar of beryllium appears to be a prime candidate for an experiment that will mark the first step down that long and winding road. By Charlie Wood Staff Writer October 30, 2024 View PDF/Print Mode experimental physics fundamental physics gravity physics quantum gravity quantum physics All topics (opens a new tab) Share this article Copied! (opens a new tab) Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters (opens a new tab) The Quanta Newsletter Get highlights of the most important news delivered to your email inbox Email Subscribe Recent newsletters (opens a new tab) Also in Physics How Do Merging Supermassive Black Holes Pass the Final Parsec? astrophysics How Do Merging Supermassive Black Holes Pass the Final Parsec? By Jonathan O'Callaghan October 23, 2024 Read Later The ‘Beautiful Confusion’ of the First Billion Years Comes Into View astrophysics The ‘Beautiful Confusion’ of the First Billion Years Comes Into View By Rebecca Boyle October 9, 2024 Read Later Can Space-Time Be Saved? Q&A Can Space-Time Be Saved? By Charlie Wood September 25, 2024 Read Later Comment on this article Quanta Magazine moderates comments to facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English. Show comments Next article Meet the Eukaryote, the First Cell to Get Organized",
    "commentLink": "https://news.ycombinator.com/item?id=42001642",
    "commentBody": "It might be possible to detect gravitons after all (quantamagazine.org)240 points by elsewhen 19 hours agohidepastfavorite134 comments DemocracyFTW2 8 hours agoI choked on this part: > The discussion recalls a messy, largely forgotten episode from the dawn of the quantum era. In 1905, Einstein interpreted experimental data to mean that light is “quantized,” coming in discrete particles now called photons. Others, including Niels Bohr and Max Planck, thought that the classical, wave nature of light might still be saved. [...] Most physicists presume that everything in the world is quantized, including gravity. But proving that assumption will entail a new war, one that has only just begun. 1) No, that is not a \"messy, largely forgotten episode\", rather, it is frequently re-told and almost a required Inshallah of every piece on quantum physics. 2) Please spare me that \"war\" simile, it only shows you're an American who can not write too well. War on drugs. War on poverty. War on whatever. The Browser Wars. Dude get a grip. Don't always \"killer feature\", \"shot him dead\", \"waged war on the germs in her refrigerator\". We have to fight a \"war\" to find out whether spacetime is quantum? Rilly?? reply skhunted 6 hours agoparentPlease spare me that \"war\" simile, it only shows you're an American who can not write too well. It’s understandable that you don’t like the desensitization of war that comes from our over usage of the word. Perhaps it speaks to a defect in American culture but this is how we communicate in our language. I think Arabic has too much emphasis on allah related phrases. But that’s how they speak. Nothing I can do about it. I don’t think said usage implies anything about their writing abilities. reply Smithalicious 5 hours agorootparentLet's split the difference and start talking about the \"jihad on drugs\" reply skhunted 5 hours agorootparentNice. I think it would probably be more effective propaganda to say the \"jihad on Christmas\" than the \"war on Christmas\". reply sroussey 4 hours agorootparentThe “war on Christmas” was thought up by a think tank and agreed upon by a group of conservative media that meet weekly to decide agenda to push across talk radio, tv, online. Truly an American phenomenon. reply pessimizer 3 hours agorootparentThink tanks that create agendas for groups who meet regularly to decide how to push those agendas through the media and through their politicians are not an American phenomenon, and not specifically conservative. The people who you work for (and who everyone works for) are probably loosely associated with tons of them. reply creer 3 hours agorootparentprev> [war] is how we communicate in our language. Is it? Or is it how marketing communicates? (Which apparently is then spilling into how you communicate?) reply skhunted 2 hours agorootparentOne influences the other. We have always been at war with Eastasia. reply wizzwizz4 6 hours agorootparentprevEnglish has a lot of \"oh my god\", \"god-willing\", \"oh god\", \"god no\", too. reply skhunted 5 hours agorootparentIt was pointed out that in American English we overuse \"war\". I pointed out a language that overuses a different word. I imagine most languages have a bunch of phrases involving an overused word. My point was that this phenomenon isn't unique to American English. Your response is that English also overuses another word. I don't understand the point you are trying to make. reply maronato 5 hours agorootparentYour response needlessly called out Arabic for using too much “Allah”. English overuses it the same amount, given that Allah is their word for God. reply losvedir 4 hours agorootparent> English overuses it the same amount Not even close! I only have experience in the Gulf but they use \"allah\" words all the time! \"Alhamdulillah\" (thank god) is pretty much part of the standard greeting (Pretty much just two people rapid firing a list of phrases at each other simultaneously \"peace be with you / and also peace with you / how are you / thank god / how's the family / thank god\") and a frequent response to almost any question. Anything dealing with the future or uncertainty will bring in \"Inshallah\" (God willing). I remember the first time I asked my cab driver to take me to my destination and he said \"Inshallah\" that did not give me confidence! Then there's \"bismallah\" and \"mashallah\" which I heard a bunch. And I'm not sure if we can count \"yallah, yallah\" which is like \"hurry, hurry, let's go\" which you hear all the time and is derived from \"Ya Allah\". I would be willing to bet that Gulf Arabic speakers say \"Allah\" literally at least 10x if not 100x more than Americans say \"God\". reply AuryGlenz 3 hours agorootparentI had a photoshoot scheduled with a young Somali man. He was late by over an hour, and I had another shoot after so I had to cancel. I rescheduled and told him if he was late for the next shoot I wouldn’t be rescheduling again. He said he wouldn’t be…inshallah. I know it’s part of their language but just as with your cab example it can be a bit annoying as an outsider, as it feels like they’re passing off responsibility. I feel the same way about some of the things Christians say too, for what it’s worth. Anyways, that’s my infuriating inshallah story. He was late again - and he needed to find another photographer after that. reply stareatgoats 2 hours agorootparent> they’re passing off responsibility On the other hand. For example in Kiswahili \"Shauri ya Mungo\" (God's business). Perhaps not exactly the same, but sufficiently similar to be interesting: that some things are in fact beyond human capability and comprehension has largely disappeared from the English language (or any language from predominately secular countries). reply svachalek 2 hours agorootparentprevSounds similar to \"mei ban fa\" (it can't be helped) in Chinese. The words suggest something is impossible but in practice it generally just means, nope, don't feel like it. reply lupusreal 2 hours agorootparentprevInshallah is just a fun and useful word, I've been hearing it a lot lately from people who aren't even Muslims. I think it's on track to become a common loan word in English. reply glompers 2 hours agorootparentIt already is one from Arabic to Spanish (ojalá for \"I hope\" or \"hopefully\") reply sourcepluck 3 hours agorootparentprevFor one of your next trips I suggest a few months in Ireland, and if you could in particular spend time living in a slightly more rural or at least small-town type family. Jaysus lads, that chap... Christ, sure how would ya... Holy lord! I've never heard... My God, did you hear... Lord help us and save us, she'd only... And I'm leaving out the more vulgar variants here. I'm not saying we'd beat some of the Arabic countries, but I definitely am saying we very well might give them a good run for their money. I'll consciously try tone it down if I'm conversing with a non-Irish person, and then with my family I'll let loose. Maybe Americans use God-related idioms a lot less than us, of course. The U.S. is a relatively big place though, so I'd be curious to know if there aren't some corners closer to (rural?) Ireland's usage. reply baseballdork 5 hours agorootparentprevIt was called out because OP used \"Inshallah\". The argument is that it's cultural, and complaining about it is silly. reply slibhb 3 hours agorootparentprevNah. English speakers use God-based exclamations from time to time but they generally do not pepper their sentences with \"God willing\", \"praise God,\" etc like Arabs do. That said, I do not care at all that Arabs used \"God\" a lot or that Americans (supposedly) use \"war\" a lot. If anything, I like these sorts of differences. reply skhunted 5 hours agorootparentprevThe point I was making is that this phenomenon occurs in other languages. Arabic and its phrases involving \"allah\" first came to mind. So how was this \"needlessly\"? I needed to give an example of this occurring in another language. reply elashri 4 hours agoparentprevI will start by agreeing that usage of \"war\" seems weired. Although scientific drama in fundamental physics is not uncommon but a word like \"debate\" would be more appropriate. I don't like your usage of word \"Inshallah\" as a mock up of the idea that something will never happen. Because it is a complete opposite meaning of when you suppose to use it. It is usually overused by arab moms (I know that people tends to mean that) but I use it frequently. Ironically I was in online meeting at CERN today and used inshallah (sometimes I use it because I am used to) to refer to potential interesting signal I am seeing. I used it to describe the hope that this will not turn out to be a statistical flactuation. Also I would agree that this is messy at the time when Einstein published his work on quantization. He even got Nobel prize on his work on photon quantization not relativity. Of course he was not alone, Max plank was the first to suggest energy discritization. reply digging 3 hours agorootparent> I don't like your usage of word \"Inshallah\" as a mock up of the idea that something will never happen. Oh, I read it differently (perhaps misunderstood). I interpreted the usage as \"a required rote phrase/story rubber-stamped onto every piece\". I understand how that, too, could be offensive, but I didn't see how inserting the literal meaning of the phrase made sense? reply evantbyrne 5 hours agoparentprevYou seem to be frequently flame posting about America. Are you going to be okay? reply 1234letshaveatw 4 hours agorootparentIt is a cringeworthy hallmark of angsty teens and people who base their entire identity on their political leaning reply Sniffnoy 1 hour agoparentprevThe \"forgotten\" part isn't Einstein's proposal, but the semiclassical suggestion of how to avoid it. Have you seen that part commonly retold? I haven't. reply sva_ 4 hours agoparentprevIt now appears like there is a war on the inflationary use of the term 'war' in common linguistic use. reply kevindamm 4 hours agorootparentWe don't want it to escalate to a word war. reply wasabi991011 3 hours agoparentprevFor 1): Yes, the photoelectric effect is frequently told. But that's not what the article is referring to here, it is foreshadowing its discussion on the semiclassical treatment of the photoelectric effect (by Lamb and Scully). This is the \"messy, largely forgotten\" part which is rarely discussed. I saw it in a graduate quantum optics course, but even at that level I'm not sure it's part of the standard curriculum. reply criddell 5 hours agoparentprev> spare me that \"war\" simile It's not a simile, is it? They aren't saying it's like a war. My dictionary lists a bunch of definitions for war that fit this usage. reply goodpoint 5 hours agoparentprev> Please spare me that \"war\" simile +1, normalizing \"war\" as a synonym of effort is pretty much orwellian. reply seanhunter 1 hour agorootparentThat is part of the original etymology of the word. [1] > late Old English wyrre, werre \"large-scale military conflict,\" from Old North French werre \"war\" (Old French guerre \"difficulty, dispute; hostility; fight, combat, war;\" Modern French guerre), Note the old French part including difficulty dispute, hostility and fight. [1] https://www.etymonline.com/word/war reply zeroonetwothree 1 hour agorootparentWow, words changing meaning over time. Imagine that. The internet would lead me to believe that’s something horrific that must not be allowed. reply iknowstuff 7 hours agoparentprevwhoa this comment took a turn reply 7373737373 6 hours agoparentprevRe. 2), what word should be used instead? \"Massive concerted/collaborative effort\"? reply ben_w 6 hours agorootparent\"Polite debate\". If this was a war, the UN headquarters has been a war-zone continuously since it started, as has every parliament, congress, and senate. reply gosub100 5 hours agorootparentprevQuest, undertaking, chapter. reply GranularRecipe 5 hours agoparentprevIt's not an inshallah, it's a masha'Allah. reply VoodooJuJu 4 hours agoparentprevIt's fine to criticize facets of a culture, but this isn't a very intelligent critique, it's just rude. On the \"war\" thing - the culture of many Americans has a martial bent to it, which is why things like \"war\" made its way into a lot of our idioms and phrases and way of thinking. It's not bad writing just because your cultural mode of thought doesn't consider \"war\" as an acceptable metaphor or idiom. Martial cultures don't need to \"get a grip\" any more than scholarly or pastoral ones need to. Culture is self-preserving and so the martial aspects of ours aren't going away anytime soon, so I suggest you find a way to be more tolerant and understanding of it. reply ta93754829 16 hours agoprevSo I thought gravity was basically the curvature of spacetime. But if there's a \"gravity\" particle, those two things seem mutually exclusive? Can someone who understands this please explain it to me, thanks! reply whatshisface 15 hours agoparentOur ability to solve integrals is much more limited when the dx represents a slight change in a function, rather than a small change in a real number. As a result, a lot of things that are easy to say in English such as \"quantized curvature in spacetime,\" or \"strongly coupled gauge theory,\" turn into a big mess when they're written down more precisely. One of the consequences of this limitation is that we have a model for quantized vibrations in spacetime that only works when they do not interact with each other. General relativity says that no, gravitational fields do interact with each other - so the picture we have at present is incomplete. The model of non-self-interacting gravity is a particle we call a \"graviton,\" and it probably describes reality very well when the gravitation involved is so weak that its self-interaction is undetectable. String theory and loop quantum gravity fit into this picture by trying to replace the integral over something we can't handle with an integral that matches it at large scales, but turns into something more tractable at small scales. Maybe the fact that we still can't make sense of the integral is Nature's way of telling us that she does not do the integral either... reply Aardwolf 13 hours agorootparent> The model of non-self-interacting gravity is a particle we call a \"graviton,\" and it probably describes reality very well when the gravitation involved is so weak that its self-interaction is undetectable. Can you please elaborate, the first part of the sentence says graviton is for non-self-interacting gravity, the second part of the sentence says graviton is for self-interacting (if 'its' in 'its self-interaction' refers to the graviton). I don't intend to nitpick the sentence, just trying to understand the theory and I don't even know if particle means self interaction or the opposite and can't parse it here either... If the answer is graviton is for non-self-interacting: what is the model for the other case (where gravity does self interact) and what would cause that self interaction if not the graviton? reply FeepingCreature 11 hours agorootparent> Can you please elaborate, the first part of the sentence says graviton is for non-self-interacting gravity, the second part of the sentence says graviton is for self-interacting (if 'its' in 'its self-interaction' refers to the graviton). The point is, we know gravitation does self-interact. But our best model, the graviton, doesn't model self-interaction. So the model is probably accurate in regimes where you'd expect little self-interaction anyways. reply Aardwolf 4 hours agorootparentWould self-interaction mean something like: just like the massless photon, the massless graviton would be bent by the gravity of black holes... hence self interaction? reply gus_massa 7 hours agorootparentprev> The model of non-self-interacting gravity is a particle we call a \"graviton,\" and it probably describes reality very well when the gravitation involved is so weak that its self-interaction is undetectable. I disagree with that part. For the strong force we have the \"gluons\" and they are considered particles and they have a strong self-interaction. The strong self interaction makes it a huge mess and a lot of things that involve gluons are impossible to calculate. It's more like: fake quote> Let's pretend for 30 minutes that the strong field don't self-interact, so we have this nice particles call gluons. Now we add this interaction to the Lagrangian to make gluons interact with other gluons, and now we have a problem. I agree that that when gravity is small enough, then gravitons give an easy to calculate aproximation. IIRC at high enough energies calculations with gluons get not impossible to calculate too. reply whatshisface 4 hours agorootparentThe thing is, no particle is defined when it interacts. At our present level of understanding the only defined particles are the individual green's functions that appear in perturbation expansions, the lines in Feynman diagrams. We see interacting particles in detectors, but since nobody can write down what field configuration they mean by \"a photon,\" I can defend my phrasing - but you can defend yours too because I know what a bird is even if I don't know how they work. reply actionfromafar 7 hours agorootparentprevI don't know if it was just sheer luck that your comment fit my particular flavor of ignorance perfectly, but it struck me as great writing! I think I learned a little thing today. When I read the article I thought, too bad I can't ever understand anything of this, but now my personal model of the universe is just a little bit richer. reply Iolaum 11 hours agorootparentprevEven with gravity being \"self interactive\" can't we have stable particles (\"gravitons\"?) that behave like solitons do? https://en.wikipedia.org/wiki/Soliton reply gradschoolfail 8 hours agorootparentIf you’re young (or if you’re old but your mind remains flexible) i urge you to think hard about this problem.. i do and i will! my mind is already quite inflexible so i’m not going say anything definite about this question out of fear of saying something dumb/misleading. Your q hides monsters ready to snipe any serious mind of the planet! That said, an easier question to ponder (and many have tried) might be: Do photons self-interact? With each other? In free space? After thinking thru this question yourself, you might be more prepared to consider gravitons (=“spin-2 massless bosons”) ditto for me! reply gradschoolfail 14 hours agorootparentprevThis needs to be emphasized more, by the TFA too — most (theoretical) physicists think that detecting gravitons is an engineering exercise that has no implications* for quantum gravity (as understood by the public) >The model of non-self-interacting gravity is a particle we call a \"graviton,\" This needs to be emphasized even more, because it has >when the dx represents a slight change in a function *see the discussion around sharikous’ comment below https://news.ycombinator.com/item?id=42003116 reply lazide 8 hours agorootparentBecause they don’t want to run the risk of being wrong, eh? reply gus_massa 6 hours agorootparentProbably because the number of detection will be too few to test old theories or make new theories with the experimental results. Physicists love to be wrong! If there is an experiment that disagree with the current theory, then is like the will west and everyone can publish their own pet theory that \"fix\" it. It's like raining free paper for them, their graduate students and everyone. Also, it's fun! When experiments and theory agree, they have to use imagination to get a new \"interesting\" tweak that can be published. In some case the the tweak may be interesting, but most of the times it's not. I remember a talk about a 2-sigma \"particle\". There was a small disagreement in some experiment, so someone did a thesis about a possible fix adding a new particle. A lot of hard work and hard calculations. It was a nice talk, and someone asked what what happened then. The sad new was that later the 2-sigma disappeared, it was only a fluke :( . This kind of work is important, but it's more boring that looking for new particles. reply LeoPanthera 15 hours agoparentprevIn theory, if gravitons exist, they should reproduce the same effects as the curvature of spacetime at larger scales. So, while they seem contradictory, they're actually complementary. Gravitons would be the \"quantized\" particles that, in large numbers, create the effect we observe as curved spacetime. The problem is that nobody has successfully combined these two views into a single unified theory, known as \"quantum gravity\". General Relativity and quantum mechanics don't naturally fit together, and that's why we don't yet fully understand gravity in a way that reconciles both the spacetime curvature and graviton perspectives. reply pdonis 14 hours agoparentprev> I thought gravity was basically the curvature of spacetime. Classically, it is. But most physicists believe that there is a quantum theory of gravity that underlies the classical theory, and that that quantum theory will include, at some level of description, a spin-2 gauge boson that mediates the quantum gravitational interaction, called the \"graviton\". Our classical theory of gravity, General Relativity, would then be the classical limit of that quantum theory, just as classical Maxwell electrodynamics is the classical limit of quantum electrodynamics. reply philipov 5 hours agoparentprevThere are some ideas that spacetime is an emergent phenomenon. One such proposal is that it is produced by the large-scale presence of entanglement between particles: that entanglement creates spacetime. Where entanglement between regions of spacetime is stronger, the space is closer together, and where that entanglement is cut things get farther apart. This idea is known as \"ER=EPR\" [0]. That's the link bridging gravity as a particle (small-scale) and gravity as a feature of a manifold (large-scale). Physicists are trying to find a way to make spacetime emerge from quantum field theory, or make both emerge from some common framework. 0: https://en.wikipedia.org/wiki/ER_=_EPR reply gary_0 16 hours agoparentprevElectromagnetism is both a continuous wave and a discrete particle, so it makes sense to me that a continuous spacetime curvature could also be a discrete particle at the same time. (Keeping in mind we're not talking about tangible shapes but mathematical models that describe aspects of reality that are hard for humans to intuitively conceptualize.) Of course, our idea of how to reconcile quantum gravity with general relativity is much less developed than our understanding of electromagnetism and the nuclear forces. reply somat 9 hours agorootparentMy understanding is the particle model of electromagnetism, the photon, really only shows up where the em field interacts with matter(electrons really), the em field itself is not quantized, or at least not quantized at the level of the photon. Not that this really matters(intentional), we can only interact with the em field as matter so that is what matters. reply alkonaut 6 hours agorootparent> matter so that is what matters At first I thought this was a great pun. But then this is perhaps also the reason the word is actually \"matters\"? Where \"matters\" is what means something? What matters is what has an observable effect? reply Willingham 15 hours agorootparentprevWhen you mention nuclear forces, are you referencing weak force and strong force? Do we understand these forces at the same level that we understand electromagnetism? reply btilly 15 hours agorootparentYes. The Standard Model has completely explained all experiments involving them for around 50 years now. In fact the outstanding success of the Standard Model has posed its own problems - the lack of deviations from it makes it hard for experiments to point in a useful direction for better theories to be developed along. reply jcranmer 14 hours agorootparentThat's not quite accurate. There are a few things that the Standard Model doesn't exactly account for--neutrino oscillation being the most famous. The trouble is that these issues aren't really big enough to suggest new physics, and the experiments aren't good enough to really suggest how much patching actually needs to be done. reply btilly 2 hours agorootparentYes, there are some deviations. But minor adjustments to the Standard Model handles those. And don't really point in the direction of a better theory. More relevantly to the previous question, I'm not aware of any of those which affect interactions with the strong or weak nuclear forces. reply jcranmer 2 hours agorootparentNeutrinos predominantly work via the weak interaction, don't they? reply jraines 7 hours agorootparentprevAlso the unexpectedly large mass of the Higgs, which suggested (to string theorists), super symmetry. Which unfortunately turned out to not exist unless it’s at some configuration that’s quite different from what was suggested reply pfdietz 6 hours agorootparentI thought the Higgs had an unexpectedly small mass. https://home.cern/news/news/physics/incredible-lightness-hig... reply elashri 14 hours agorootparentprev> the lack of deviations from it makes it hard for experiments to point in a useful direction for better theories to be developed along. We have anomalies (deviations from standard model) in many measurements done by several experiments. This is a good summary [1] from them up until now (sorry for the pay-walled) [1] https://www.nature.com/articles/s42254-024-00703-6 reply dist-epoch 9 hours agorootparentprevThe most elegant description of electromagnetism is also in terms of curvature, but the curvature of a certain mathematical structure called \"connection on fiber bundles\" and the math field is called differential geometry. reply ajkjk 15 hours agoparentprevNot an expert, but: the curvature of spacetime is modeled as a tensor field (the metric tensor). That field can have (classical) waves in it, which is what LIGO detects (I believe). Then you can certain hypothetically quantize that field, in which case it definitely has to be a spin-2 particle and it seems likely that there will be a way to do it since all the rest were. The \"geometry\" comes from the fact that the way we measure distances (or, well, experience time) uses the metric tensor field to do it. But it is still ultimately just a value attached to every point like any other field. reply gpsx 5 hours agoparentprevGravity is similar to an electric field here. A wave function for a field consists of an amplitude for each field configuration, where a “field configuration” refers to a value for the electric field for each point in space. In GR each field configuration would correspond to a space time geometry for the universe. We have quantized excitations as distinct “valid” solutions to the wave function, which we call a particle, though it is nothing like an electron. The notion of space time geometry holds throughout. (Edit: in practice, people never calculate wave functions for fields like electric fields. That would be too hard. Different methods are used in calculations. Second edit: the wave function wouldn’t be composed of complete space-time configurations, histories of the universe, but time slices from it, like space geometries. Maybe this can be expanded in responses/comments.) reply cryptonector 14 hours agoparentprevThat gravity is curvature of spacetime is one view of two equivalent views, but it is the standard view. The other view is that you have flat spacetime with different distortions (of things other than spacetime) than the distortions you get in curved spacetime. The Schwarzschild metric essentially lets you do exactly that projection of curved spacetime to flat, and vice-versa. When you watch an animation like https://www.youtube.com/watch?v=hF7zltx7Ecc or https://www.youtube.com/watch?v=E1mD4C7dBKc you're watching a flat spacetime representation of GR's effects, and the reason for using flat spacetime in these representations isthat that is what us humans understand. So if you take the gravity curves spacetime view, then gravity is not a force and all that. But if you take the alternative view then gravity is a force. Now, I'll leave what the distortions are that gravity produces in flat spacetime for another time, or for the reader. But I'll say this: this view is both controversial (perhaps replies will show this) and not (see above -and many other- animations). reply NitpickLawyer 14 hours agorootparentI last took a physics course when Pluto was a planet, so excuse my possibly outdated question, but isn't the detection of gravitational waves proof of gravity being a force? I follow a few educators/communicators in this field and I have a feeling they're using this \"gravity isn't really a force\" to bridge the gap between their deep understanding and us mortals that don't poses the language / understanding to get the entire meaning behind it. Is that feeling correct or am I missing something? reply ithkuil 10 hours agorootparentFwiw gravitational waves were predicted by Einstein himself (Einstein, Albert, Ueber Gravitationswellen, 1918) as a consequence of general relativity. The core idea is that when you move a mass, its contribution to the spacetime geometry changes, but the effects of the change of the geometry doesn't apply instantaneously to all the universe but instead the change propagates at the speed of light. So that explains why any sudden movement of a mass creates a \"crest\" that moves through space at the speed of light. Furthermore, the sources of fast movement of extremely heavy mass just happen to involve an object that wiggles back and forth in a periodic way because those events involve heavy objects orbiting other heavy objects. That's the reason we can measure a wave with multiple crests and we can talk about a wave length of the gravitational waves: the wave length of the gravitational waves matches the period of the orbit of the heavy mass. reply Wololooo 12 hours agorootparentprevSo the main issue here is how people were presenting it, in Quantum field theory, as stated by other people, each force is associated with a field and has at least one force carrier, the exact number is linked to the specifics of the mathematical framework underlying it To that extent you can build 3 fundamental forces, electro magnetic, weak (that are called together electroweak) and the strong force. You have an extra force carrier through the Higgs that allows you to give mass to everyone. Now you need to consider gravity because you know that gravity exist and since everything under the sub is quantised, well so should gravity. The main issue with gravity is that it is interpreted so far as a curvature of space time, it's mainly fine for big items, but the implications for quantum field theory is that you should modify the small integral element that you use (space shouldn't have the same size) except that you look locally at space that is mainly flat... And changing the integral does not lead to well behaved behaviours. You can start to introduce new fields but doing so also causes an issue... Funnily enough even in the standard model something is missing, everything mostly fits, but that's the trick, mostly, neutrinos have mass and this in itself is a problem because the Higgs mechanism doesn't provide mass to them ... Long story short, people take shortcut when explaining the messy gritty part of it, which is \"fine\" but not really, and from a simple standpoint one would like to have a simple field from which gravity is born, which might be but so far, to my simpleton understanding, this hasn't been too successful, unless some form of string theory is realised. But the pre requisite for this is a form of supersymmetric theory existing which is currently disfavored, but could exist in the unproved energy scales from here to the plank energy scale. Sorry this ended being a tad long and I'm not sure this is clarifying things. reply AnimalMuppet 5 hours agorootparent> neutrinos have mass and this in itself is a problem because the Higgs mechanism doesn't provide mass to them ... The Higgs mechanism doesn't provide all mass, even of the things that it provides mass for. They each have a \"bare\" mass, that is, a mass without any Higgs interactions. They just have a much greater mass because of the Higgs interaction. (And maybe that's why neutrinos have so little mass...) reply stouset 14 hours agorootparentprevMy naïve understanding is that you can model gravity as a force in a flat, static spacetime. Equivalently you can model gravity as a forceless distortion of curved spacetime. Both models can be translated faithfully into one another, so you can solve problems related to gravity in either domain. reply trevordixon 10 hours agorootparentMy naive understanding is that forceless spacetime distortion predicts somewhat different things than the old model. That's how general relativity finally explained the procession of Mercury's orbit for example. reply cryptonector 7 hours agorootparentGP means that you can take the Einstein field equations (and their solutions) and use the metric tensors to map between flat and curved spacetime, with either way being equivalent. GP did not mean that those tools map from Newtonian flat spacetime to curved. reply whatshisface 14 hours agorootparentprevIt's a bit of an internet meme, gravity can take momentum away from one object and transfer it to another, and that's what Newton said a force was. The meme is that the way it happens makes \"changing momentum\" (3-momentum, the one Newton was talking about) and \"going straight\" (geodesically, in curved space-time) hard to separate in English. reply AnimalMuppet 5 hours agorootparentprevConsider centrifugal force. In a non-rotating reference frame, it doesn't exist. In a rotating reference frame, it does, that is, it shows up as a term in the equation of motion. You wouldn't expect to find quantums of centrifugal force in a rotating frame of reference, and no quantums of centrifugal force in a non-rotating frame of reference. They're both describing the same situation; either quantums exist in both frames, or they exist in neither. So either gravity really exists as a force, or it doesn't. If it does, then I would expect gravitons, and expect them in all frames of reference. If it doesn't, then I would expect no gravitons in any frame of reference. Except... If I understand correctly, static electric and magnetic fields are not carried by photons - they just sit there. It's only changing E/M fields that are carried by photons. So maybe only changing gravitational fields are carried by gravitons, and the static fields are just curved space-time? reply renegade-otter 10 hours agoparentprevI just watched this a few days ago on the Space Matters channel about gravitational waves: https://www.youtube.com/watch?v=9bg2NINW8a0 Not just some dumbed down Discovery show - it pushes the limits of what a layperson can understand. reply gosub100 2 hours agorootparentThe YT algorithm is so damn bizarre sometimes. I'm subscribed to probably a dozen different astronomy and physics channels, some for 8 years, and never once saw this channel you recommended. It recommends plenty of AI-TTS bunk too. But somehow didn't decide to show me that channel. Thanks for recommending it. reply blenderob 8 hours agoparentprev> So I thought gravity was basically the curvature of spacetime. But if there's a \"gravity\" particle, those two things seem mutually exclusive? It does not have to be either or. It can be both. Both models can be useful to understand the nature of gravity and make predictions about natural phenomenon. reply yarg 15 hours agoparentprevTo my understanding (not the best) there's a huge disconnect between the physics of the very small (quantum mechanics and the standard model) and that of the very large (general relativity). The disconnect seems to be unresolvable (I don't understand this part at all) and so efforts are being made to quantise gravity and incorporate it into the standard model. reply dcl 14 hours agoparentprevYes. I've seen lots of twitter/X posts lately about how Gravity is not actually a force. But how can that be true if there is a force carrying \"gravity\" particle? Or is the word 'force' being used loosely here? reply cyberax 14 hours agorootparent> Yes. I've seen lots of twitter/X posts lately about how Gravity is not actually a force. That is true. Classically, gravity is a fictitious force, merely a result of inertia from moving in a curved space-time. > But how can that be true if there is a force carrying \"gravity\" particle? Or is the word 'force' being used loosely here? Because we _suspect_ that the classical view is not correct. And there's a quantum description that may or may not involve curved space-time. It's not impossible that the spacetime curvature is a mathematical artifact of a deeper theory. Merely a kinematic explanation, just like epicycles. It's also possible that the space-time _is_ really curved, and gravitons simply cause the curvature by somehow coupling with it. And then other matter experiences this, in the manner described above. reply sampo 6 hours agorootparentprev> if there is a force carrying \"gravity\" particle? There is not. Or maybe there is not. At least so far there is not. We have never observed one. \"Graviton\" particle is just a hypothesis. Outside of some people theorizing that graviton could exist, there is no observations that it exists. \"In theories of quantum gravity, the graviton is the hypothetical quantum of gravity\" https://en.wikipedia.org/wiki/Graviton reply InDubioProRubio 9 hours agorootparentprevIn a sim, it would fall into the \"configurable parameter\" category and dynamically altered parameter whos laws depended on locations are function lookups. And to execute performant, it would be a constant factor field only updated onAlteration with fun(x) So you have a thing, that gets interpolated updated with various functions, that overlap, and those functions only get updated at lightspeed, cause caching. Cachesize limit should show as farway gravity sources getting bundled into lower density information functions. reply XorNot 14 hours agorootparentprevYou can make a lot of pseudo particles in semiconductors which definitely exist, but also aren't \"real\" - i.e. semiconductor electron holes are capably modelled as positive particles which can move freely with momentum/position within a semiconductor. reply 1024core 15 hours agoparentprevSort of like how light is both a wave and a particle...? reply exe34 10 hours agoparentprevit's important to realise that particles are an artefact of living in a monkey sized body. at the basic level, the equations are useful if they match observations, not if they make sense intuitively. https://arxiv.org/abs/1204.4616 reply swayvil 14 hours agoparentprevMaybe all particles are twists in spacetime. reply ruthmarx 13 hours agoparentprev> So I thought gravity was basically the curvature of spacetime. That's just part of the picture. I always thought that Veritasium video did more harm than good. reply DebtDeflation 8 hours agoprevThe fact that QED and QCD are renormalizable while gravity is not is probably trying to tell us something deeper than we think. Relevant paper: https://arxiv.org/pdf/0709.3555 You can read the first two paragraphs of the Introduction and then skip to the last sentence of the Conclusion if you want to bypass all the math. reply jessriedel 0 minutes agoparentThere are tons of effective QFTs that can be used very accurately to predict experiments below a certain (high) energy threshold, but which are not renormalizable. This is well understood. It means that the theory needs to be augmented to understand the higher energies, but does not at all mean the more fundamental (augmented) theory has to be radically different, such as classical. Everyone already agrees that the non-renormalizability of naive low-energy quantum gravity means something must be added. reply marcosdumay 5 hours agoparentprevSo... If gravity is quantized, then black holes must be only a low-energy approximation of whatever phenomenon is really happening there? If there's some proof that they aren't black (AFAIK, the only thing we know empirically), I've missed it. All I see is a point that the current theory would be wrong. reply DebtDeflation 4 hours agorootparentRather, the existence of black holes demonstrates that gravity is not renormalizable. The last sentence in the paper: \"It seems that gravity is a low energy effective field theory description of something else that is not a quantum field theory.\" QFTs like QED and QCD are renormalizable. This is a technique used to eliminate the infinities that arise in calculations from self-interaction. For a very long time, renormalization was viewed as hocus pocus (including by the person who discovered it). Later, mathematicians were able to provide a solid theoretical foundation for it.......but only as an effective field theory valid at particular size and energy scales. Net net, the standard model is an approximation of something more fundamental. Gravity being nonrenormalizable shows that \"something\" is not a QFT. reply marcosdumay 3 hours agorootparentDoes it contest anything that we have observed? Because I couldn't find anything there that won't allow objects that behave the same way as the black holes we've seen, and differ only on details we can't see. (But IANAP, and could easily have missed something.) reply ricksunny 2 hours agoprevCan we take a metascience / meta-scicomms perspective? Quanta magazine, like all scicomm journos, relies on the receptive access to researchers for source content, especially exclusive results. So scicomms is essentially the same kind of access-journalism evryone comolains about in DC beltway rwporting, but amplified & couched in the 'but science\" brand. In Quanta's case, its sources are quantum physicists / particle physicists. So couching everything in terms of quantized particles (bonus points if somehow 'spooky') avails of the continued access. Articles from the likes of Quanta get cited in particle physicists' onward grant proposals. Pwrticle physicists get grant, get results which get reported just so, and the scicomm journo gets invited to show up once again to scribe away. The circle repeats, meanwhile everyone just hopes it's a virtuous one. reply choilive 17 hours agoprev> physicists are debating what it would really prove. Well, if we can detect the graviton before we have a working quantum theory of gravity, it would mean that gravity is in fact quantized and that we just need to figure it out. This would be a very big deal. reply itishappy 16 hours agoparentThey can detect an interaction, but they can't prove that it's quantized without (I believe) sub-Poissonian statistics[0], which requires detecting enough events and with enough certainty that it would require planet-scale machinery. > Now graviton chasers find themselves in a peculiar position. On the main facts, everyone is in agreement. One, detecting a quantum event sparked by a gravitational wave is — surprisingly — possible. And two, doing so would not explicitly prove that the gravitational wave is quantized. “Could you make a classical gravitational wave that would produce the same signal? The answer is yes,” said Carney, who along with two co-authors analyzed this type of experiment in Physical Review D(opens a new tab) in February. [0] https://en.wikipedia.org/wiki/Photon_statistics#Sub-Poissoni... reply choilive 11 hours agorootparentIt sounds like the article is saying we could detect many events without using a planetary scaled detector It mentions a single detector being a 15kg Be bar chilled to near absolute zero. Certainly very very difficult, but not in the realm of sci-fi. reply itishappy 4 hours agorootparentRight, but this event just confirms gravitational radiation exists (which we already know). It can't tell us how it behaves (is it quantum? it it classical?) without detecting many events in sequence to be able to run statistics on (and if you miss one, your stats are wrong). One bar has a very low probability of detection, we'd need many, hence the expectation of planet-scale machinery. reply toomuchtodo 16 hours agoparentprev> This would be a very big deal. Gravity drive? reply cryptonector 5 hours agorootparentNope. reply eterevsky 5 hours agoprevSo the article says that Freeman Dyson calculated that only one graviton capture event would happen per billion years in a detector the size of the Earth. The new experiment however proposes to use 15 kg of super-cooled Beryllium. My question is: what's the difference between the proposed Beryllium slab and Dyson's theoretical detector? reply omnicognate 3 hours agoparentI don't fully understand it, but I think the difference is in the source of the gravitons rather than the detector. Dyson's earth-sized detector was imagined to be detecting gravitons produced by mass moving around within the sun, but this detector would be detecting gravitons associated with gravitational waves produced by ridiculously powerful events like black hole mergers, where two massive objects circle each other at mind-boggling speeds before colliding. It sounds like these are expected to produce vastly more gravitons, making a detection much more likely. reply whatshisface 17 hours agoprevI'm with the debaters on this one, the energy levels of a bound quantum system are predetermined to change in quantized intervals irrespective of if they are coupled to a classical or quantum field. What theory of gravity is this experiment intended to falsify? It would be great to have an independent gravitational wave detector though. reply sharikous 15 hours agoparentthe statistics would be different. Check out Rabi oscillations (classical EM) vs Jaynes-Cummings model (quantized EM) and phenomena like quantum antibunching (only possible for quantized EM) reply whatshisface 15 hours agorootparentHow would mergers produce antibunched gravity? reply Seb-C 7 hours agoprevSince I read the story \"The Road Not Taken\" from Harry Turtledove, I cannot stop thinking that we might eventually discover that the question of the conflict between the general relativity and quantum theories is something so simple and elegant that we never even considered it before. reply mensetmanusman 7 hours agoparentOr, something like the simulation hypothesis is real and different models of reality are used at different length scales and the overlap is fuzzy. reply mr_mitm 5 hours agorootparentThe issue is that the overlap becomes large and very important near black holes (and near the big bang, but that's probably not important in a simulation). reply rzodkiew 6 hours agorootparentprevYeah, I do have a suspicion that they are optimisation strategies to reduce computational complexity. reply DemocracyFTW2 8 hours agoprevAnother goodie: > You need huge masses — think planets — to significantly warp space-time and generate obvious gravitational attraction. By way of comparison, a credit card-size magnet will stick to your fridge. By way of comparison, even an Olympic pool-size balloon of hot air will float. reply tsimionescu 8 hours agoparentIt's not clear what problem you have with that comparison. It's a classical example of just how weak gravity is compared to the electromagnetic interaction. A whole planet's worth of mass is weaker than the EM field generated by a tiny magnet. And the strong and weak nuclear interactions are even stronger still. reply tecleandor 7 hours agoparentprevBut the whole atmosphere is pushing it up, isn't it? (Well, not the \"whole\" atmosphere, but...) reply est 17 hours agoprevThat's a facinating read. I wonder what are the possible applications of \"quantized gravity\" ? GPS without satellites? reply whatshisface 16 hours agoparentIf the Romulans were talking to the Klingons over a link secured by the quantum no-cloning theorem, the Enterprise could use quantized gravity to entangle the ship's computer with their adversaries' quantum radios in a way that no matter-based shielding could prevent. reply MathMonkeyMan 16 hours agoparentprevconstraints on theories of everything reply Vecr 16 hours agoparentprevCheap GPS receivers already have to do a bunch of tricks to get to the \"okay\" state they're currently at. Military devices either use GPS, star tracking, dead reckoning, or some combination. For submarines, detecting gravity variations could also be used, but it wouldn't rely on the quantization of gravity. In many places on land, you can use terrain landmarks. Since most things are already either covered, or have improvements in development, I don't really see investment for your idea. The improvements might even use \"quantum\" or \"gravity\", but I don't think the use of \"quantum gravity\" is very likely. reply JumpCrisscross 16 hours agorootparentYou’re mixing up general relativity with quantum gravity. reply Vecr 15 hours agorootparentI'm not exactly sure what you're saying. I know you can have gravitons without \"quantum gravity\" (as incomplete theories). I'm responding to a commenter on applications. In a global position finding system, gravitational effects could be used, as could be quantum effects. Maybe even both in the same system. It seems really doubtful to me a practical system would depend on anything graviton related. reply JumpCrisscross 15 hours agorootparentYou buried your lede. The “tricks” you describe relate to GR. I missed that you’re essentially saying “no.” reply ars 16 hours agoprevI don't understand what a graviton is. The article implies that it's something that communicates changes in gravity? Is that correct? How does it communicate the magnitude of the change? By having lots of gravitons? Or does it have something akin to a frequency? reply whatshisface 16 hours agoparentA graviton is the smallest possible unit of a gravitational wave. The amplitude of the wave corresponds to the number of gravitons, like you said, and its frequency to their frequency (quantum particles have frequencies that are related to their momenta). We're aware that light, at least, works like that. reply hhdhdbdb 11 hours agorootparentA gravitational wave requires an event like a black hole merger, or basicially somerhing to move and change the field, right? In this case, how does the fact that a big object is still influencing space/time around it communicate that fact when it is not moving. Is that still gravitrons? reply gus_massa 6 hours agorootparentEverything creates gravitational waves. They are very difficult to detect, so we can only detect the ones created by black hole merger or something similar. Assuming our guess about quantum gravity are correct, the normal gravitation force use gravitons too, they are virtual gravitons but the distinction between \"real\" and \"virtual\" particles is another whole can of worms. reply russdill 16 hours agorootparentprevGravitons impacting and imparting momentum seems like it would have a bunch of observational implications. Does a massive object cast a graviton shadow? Is the momentum positive or negative? reply whatshisface 16 hours agorootparentClassical waves do all of those things too. reply russdill 2 hours agorootparentSo gravitons are only emitted by accelerating objects? reply revskill 6 hours agoprevThat means there's no Gravity before The BigBang, right ? reply knodi123 1 hour agoparentthe big bang is a singularity; we can't ever experimentally learn anything about what came before it. that doesn't mean there was no time before the big bang, or no gravity, or that there wasn't another universe just like ours. But due to the singularity, for the sake of convenience, we simply _say_ that those things came into existence at the big bang. cite: https://www.hawking.org.uk/in-words/lectures/the-beginning-o... , search for \"may as well\" reply DemocracyFTW2 8 hours agoprevnext [2 more] [flagged] omnicognate 7 hours agoparentThe only thing making me feel like that is your obnoxious spamming of this post. Four top-level comments now and not a single interesting thing said. Stop it, please. reply cyberax 14 hours agoprev [–] TIL that Freeman Dyson looks like a house elf! reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new experimental proposal suggests that detecting gravitons, the theoretical particles responsible for gravity, might be more achievable than previously believed.",
      "The approach leverages advancements in gravitational wave understanding and quantum technology, potentially enabling detection in a modest laboratory setting within a few years.",
      "Although the experiment may not definitively prove the existence of gravitons, it represents a significant advancement in the study of quantum gravity, similar to how light was shown to be quantized into photons."
    ],
    "commentSummary": [
      "The debate on detecting gravitons centers around whether gravity is quantized, a fundamental question in physics.",
      "Detecting gravitons would provide evidence for quantized gravity, but proving their existence is a significant challenge requiring advanced technology.",
      "The discussion also briefly mentions cultural differences in language, such as the metaphorical use of \"war\" in American English."
    ],
    "points": 240,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1730330359
  },
  {
    "id": 42005516,
    "title": "TikTok Influencers Database with Analyzed Audio",
    "originLink": "https://www.topyappers.com/",
    "originBody": "Hello Hackers!This is my personal project I built over a couple weekends and it&#x27;s one of the largest!TikTok Influencers Database. I scraped 400k (and growing) influencers, scraped their videos, scraped the audio subtitles of their videos, and analyzed with LLM what products they promote in the video.Also, this database allows you to query however you like and find which influencers exactly talk about your product or your competitors products!I also used LLMs to categorize the influencers and it has over 3,000 subcategories, so you can target the influencers matching your niche precisely!Comment if you would like to try it out. I&#x27;m looking for some beta testers for feedback! Any idea or comment how I could improve this is appreciated!",
    "commentLink": "https://news.ycombinator.com/item?id=42005516",
    "commentBody": "TikTok Influencers Database with Analyzed Audio (topyappers.com)220 points by bhzgdwpeqa 8 hours agohidepastfavorite5 comments Hello Hackers! This is my personal project I built over a couple weekends and it's one of the largest! TikTok Influencers Database. I scraped 400k (and growing) influencers, scraped their videos, scraped the audio subtitles of their videos, and analyzed with LLM what products they promote in the video. Also, this database allows you to query however you like and find which influencers exactly talk about your product or your competitors products! I also used LLMs to categorize the influencers and it has over 3,000 subcategories, so you can target the influencers matching your niche precisely! Comment if you would like to try it out. I'm looking for some beta testers for feedback! Any idea or comment how I could improve this is appreciated! Major_Grooves 5 hours agoDo you end up with lots of duplicates when you are scraping? If you also scrape IG, YouTube and LinkedIn, would you link them all to the same influencer? That might be quite an interesting identity resolution challenge (disclosure: I build identity resolution tech). I would not mind taking a look. Always interested to see how others are handling such data. reply ada1981 7 hours agoprev [–] Looks amazing. Years ago I was named \"Top 25 Bloggers in the World\" by TIME Magazine, and spent about $10k building a database of all the top bloggers in the world (and then failed to launch!). Do you plan to do IG or other platforms? Happy to demo it and give feedback. A @ 175g. com reply bhzgdwpeqa 6 hours agoparent [–] Damn man, sorry about the failed launch... Yes, I'm planning to do IG, Youtube, Twitter, Linkedin. Hopefully this can become a platform which aggregates all influencers. Contacting you for demo reply ada1981 5 hours agorootparentYes email me Anthony@175g.com reply ada1981 5 hours agorootparentprev [–] Biggest mistake was taking ad revenue from the blog to try and build that vs. putting the ad revenue back into building out a bigger news site with the momentum. Second mistake was cold feet. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A TikTok Influencers Database has been developed, containing 400,000 influencers, their videos, and audio subtitles, allowing for detailed analysis of promoted products using a Large Language Model (LLM).- The database enables users to query and identify influencers discussing specific products, including those of competitors, and categorizes them into over 3,000 subcategories for targeted marketing.- The creator is seeking beta testers to provide feedback and suggestions for improvements."
    ],
    "commentSummary": [
      "A TikTok Influencers Database, named topyappers.com, has been developed, featuring 400,000 influencers and analyzing video audio to identify promoted products.- The database enables users to search for influencers discussing specific products and organizes them into over 3,000 subcategories.- The creator is seeking beta testers for feedback and plans to expand the database to include other platforms like Instagram, YouTube, Twitter, and LinkedIn."
    ],
    "points": 220,
    "commentCount": 5,
    "retryCount": 0,
    "time": 1730372192
  },
  {
    "id": 42002262,
    "title": "I attended Google's creator conversation event, and it turned into a funeral",
    "originLink": "https://www.giantfreakinrobot.com/tech/google-creators-event.html",
    "originBody": "Attention Required!Cloudflare . giantfreakinrobot.comCloudflare 8db5e5b2fa3e0801 • 20.102.46.190 •",
    "commentLink": "https://news.ycombinator.com/item?id=42002262",
    "commentBody": "I attended Google's creator conversation event, and it turned into a funeral (giantfreakinrobot.com)212 points by wawayanda 18 hours agohidepastfavorite85 comments mtnGoat 14 hours agoI've been invited so two similar events over the past couple decades and one company I worked for had standing weekly meetings with Google. All I can say is my experience is pretty much they same, they lure you in wanting info and to help you, but don't give any answers and just keep wanting more info. In the end they didn't unblock us and in my opinion built a lot of what we shared into their systems. It was a knowledge theft exercise in my opinion, was NOT in any way meant to help anyone but Google. reply mountainb 8 hours agoparentWhen I handled a lot of ad spending for numerous companies, Google would schedule continuous meetings with junior sales people dedicated to figuring out which client relationships that they could interfere with to convince me to spend more money recklessly. This is a common experience in that particular industry with Google's salespeople in particular. What's interesting about that is that if any of the (dozens? over a hundred?) salespeople that I interacted with could have provided a solid rationale for what they were suggesting in the context of what a particular client wanted to achieve, I could have been persuaded. None of them ever did at any time. It was always just a one-sided appeal to spend more money with no coherent plan for a return on spending. reply mtnGoat 14 hours agoparentprevin fact the standing weekly meetings were cancelled after we got to the point where the entire half hour was consumed with going over all the AIs their end had they we were still waiting for answers on. Since we never got any, we cancelled the meetings as wasteful. reply DidYaWipe 12 hours agoparentprevThis article doesn't give any specifics on the \"shadow-banning.\" I am totally willing to believe in any douchebaggery anyone reports from Google, but I (as anyone should) require specifics to give any credence to the writer's claims. This \"article\" fails to answer the first questions you'd have. For example, how did Google single you out for this invitation? The article asserts that it was all \"shadow-banned\" site owners, but then says the Google employee denied all shadow-banning. So how was the invitation phrased? I'm not even going to waste time breaking down the rest of the empty bullshit in this article. It's unfortunate, because I'll bet every claim made against Google is true. But I'm not going to give a single one credence without specifics. If you're too lazy to provide those, you don't deserve support. reply verzali 11 hours agorootparentFrom a quick search it seems you get invited to these things by filling in a feedback form for Google. As for shadowbanning, well, it doesn't take much to remember Google is a search platform and then to match that with his complaints about his site getting deranked. Maybe that could have been made clearer, but surely if I could figure that out you could too. reply DidYaWipe 10 hours agorootparentNobody should be expected to run around doing searches to support the assertions in a random article. Why would you serve as an unpaid tool? Anybody on here can speculate as to what Google does to this or that Web site; but if you're going to write an article claiming it as fact, you need to support it. It's a bummer that your time is free. reply meiraleal 7 hours agorootparentYour time seems to be free as well as engaging to demand \"proof\" wastes as much time. But yours is way more wasted as you created only a HN comment, not a proper blog article. reply upghost 6 hours agorootparentlast two comments mentioned it... is \"your time is free\" a new pejorative for \"loser\" or something? reply CogitoCogito 11 hours agorootparentprevYou could maybe consider directing these questions to the author of the article? Given the context, I'm sure he'd appreciate the feedback. reply DidYaWipe 10 hours agorootparentIt's not uncommon for the authors of articles posted here to come to the comment forum. reply Animats 16 hours agoprev\"It was then I realized this wasn’t our funeral, it was Google’s.\" What Google seems to be doing is banning aggregation sites. There was a previous posting today by someone who was complaining about low ranking for his book review and link farm site. Google wants to be the only aggregator. Why fan out queries to another level of aggregator? A list of the 20 sites he's talking about would help. How many of those are aggregation sites? reply palmfacehn 4 hours agoparentHN and Reddit, two aggregator sites, are outranking the original article https://x.com/joshtyler/status/1851872361420853690 I was able to reproduce the SERPs with this query >I attended google's creator conversation event When I removed the \"client\" parameter to post the link here, the original post ranked higher. However, that's really neither here nor there. Reddit is consistently cited as outranking original content. Personally, I've learned not to cry about sites that don't rank. My time is better spent building new sites. Sometimes de-ranked sites come back in subsequent updates. Not everything sticks on the first try. You have to be persistent if you want to profit from organic traffic. My impression is that these creators have one-trick ponies they have deeply invested themselves into. They may not be good at creating new ideas. Expectations of fairness are misplaced. You have to roll with the punches. Dwelling on what they think Google \"should be\" is a waste of time. Highly recommend focusing on areas within your immediate control. Individual agency is empowering. Victimhood, not so much. reply sanderjd 15 hours agoparentprevThe whole article is pretty confusing to me. It's never made at all clear what this event was supposed to be about or why this specific set of people were there. Presumably it wasn't \"invite people who run 'shadowbanned' sites\" when they don't acknowledge that there is such a thing. So what was it, then? reply DidYaWipe 11 hours agorootparentIt's not confusing; it's totally unsupported bullshit. The author claims that a bunch of \"shadow-banned\" site owners were invited to some summit, but couldn't be bothered to say how this invitation was phrased or delivered. How were the recipients identified, especially when he says the Google person claimed that no such sites existed? This whole thing is an insulting waste of time. reply pvaldes 8 hours agorootparent> How this invitation was phrased or delivered This is irrelevant for the theme. Some could came in a plane and other in a car, but who cares? Is assumed that either they were invited or wouldn't had walked on the building and asked about how improve google for hours. I assume that Google has some level of check-in security at least. > claimed that no such sites existed? claimed that they weren't shadowbanned, that is a different thing. And they were said that only some pages were affected. This means implicitly that google was aware that the webs existed. reply nailer 7 hours agorootparentIt’s irrelevant because reaching out to shadow banned companies does not seem like a rational thing for Google to do and and while we may dislike Google they do at least seem rational. I initially read this article with sympathy, but something isn’t adding up reply bdjsiqoocwk 12 hours agorootparentprev> Presumably it wasn't \"invite people who run 'shadowbanned' sites\" when they don't acknowledge that there is such a thing. So what was it, then? Answer: it WAS to invite people who run shadowbanned sites, they just don't acknowledge that there is such a thing. reply DidYaWipe 12 hours agorootparentWhich makes no sense. How did they phrase the invitation, then? The writer is too lazy to say. reply toofy 8 hours agorootparentA few years ago, a guy went super viral with a video of himself getting punched outside of a bar. He cried nonstop and went on the typical outrage media tour. Over and over declaring how unfair it all was “These people attacked me because I wore a hat! They attacked me because the color of my hat!” It seemed super suspicious from the jump, I kept asking myself, “There has to be more to this story, this guy is being incredibly vague, is there more to this?” A couple days after his media circus tour, videos from other people started popping up. these videos told us a little bit more. video after video of this guy—for hours—trying to start fights with dozens of people. multiple videos of him complaining while getting ejected from various bars by bouncers. he spent like 6 hours at many, many bars provoking and then feigned shock when it happened. “my hat. every time i go to this particular city, they physically beat me because they don’t like my hat” … ya left a little bit of important context out eh friend? this blog post feels very similar to me as that guys initial video. something is missing. reply pikseladam 6 hours agoprevI visited this website, and Chrome ended up blocking around 2,500 third-party cookies. Websites like this should face consequences for such behavior—it’s like dealing with a live malware site. reply cowboylowrez 4 hours agoparentyeah the website was so uniquely bad, I couldn't continue to read with firefox. links worked ok tho. reply danpalmer 13 hours agoprev> took place on October 29 > The day before, he led the group on a tour > The building was empty Monday, October 28th, was a work from home day. reply oraphalous 17 hours agoprevThe whole world seems dedicated to the goal of extracting value rather than creating it. reply throwaway48476 17 hours agoparentOr tricking someone else into creating value for you to take. reply akira2501 16 hours agorootparentPeople tend to create value inherently. If they are not receiving the benefit of that then it would most appropriately be described as theft with the aid of blind regulators. reply WalterBright 13 hours agoparentprevAnything you get for free, that requires someone else to work to provide it, means you're going to pay for it one way or another. reply lesostep 10 hours agorootparentIt could be true for things that could only be \"used\" once. But I don't think that it's a valid point at all times. Recently, for example, I've made a little \"Linux for dummies\" zine, and put it on my shelf. Sometimes guests take it, read it, and put it back. Technically, all of them get to read it for free, through no additional cost to me, because this zine already existed before they knew they wanted to \"use it\", and this zine will continue to exist after they \"use it\". reply WalterBright 1 hour agorootparentDo you let random people use your car for free, too? Do you pay for their gas as well? reply thaumasiotes 13 hours agorootparentprevSure, if it doesn't exist before you order it. If it's already been made, someone may even pay you to take it away. reply metadat 14 hours agoparentprevWelcome to capitalism. It's a tough realization. reply pvaldes 9 hours agorootparentCapitalism and thief are different things. We should stop using the first to justify the last. If this people was lured to work for free, this is not capitalism. reply WalterBright 13 hours agorootparentprevIt's a free country. You're free to implement a search engine and let anyone use it for free. Good luck paying for it, though. reply stavros 12 hours agorootparentThis is kind of disingenuous when the grandparent comment is complaining exactly about the particular way this country is free in. reply WalterBright 10 hours agorootparentNot at all. If someone wants to fund a charity search engine, they can do it. No matter how you structure it, somebody is going to have to pay for it. Another way of saying it is there's no such thing as a free lunch. In any society, any where, any time. You might as well wish for an antigravity machine :-) reply stavros 10 hours agorootparentDepending on who pays, the incentives are different. In a socialist society, where some things are funded by the government, those things are generally much more aligned with the interest of the public than in capitalism, where most money wins. reply WalterBright 1 hour agorootparent> In a socialist society, where some things are funded by the government, those things are generally much more aligned with the interest of the public than in capitalism Just for fun, compare the Soviet built Lada cars with cars built by capitalists. Or the contents of supermarkets. Or the quality of health care. Oh, and the advice given to American tourists visiting the Soviet Union - pack a couple pairs of blue jeans, as they are great for trading for stuff! I'll make it easy - name any consumer product made by the Soviet Union that was preferable to one built by greedy capitalists. Did you wonder why the US did not import Soviet made consumer products? reply stavros 47 minutes agorootparentI meant more like Germany, countries with strong social safety nets and regulation, rather than full-blown communism. reply WalterBright 7 minutes agorootparentSocialism results in less productivity. Even in Germany. s1artibartfast 7 hours agorootparentprevMan, those Cambodians really loved the killing fields. You should have seen how excited they were reply rurban 12 hours agorootparentprevSo we need to persuade the NSA to finance our new search engine? Or should we turn to Putin or Xi? The European surveillance services would not be able to reply WalterBright 10 hours agorootparentYou still have to pay the NSA, in the form of taxes. reply smsm42 11 hours agorootparentprevI'm sure that the glorious Communist Party would build a google where every site is equal in ranking and always occupies the first page. reply mmaunder 15 hours agoparentprevYou're surrounded by and typing on valuable goods and services that you received in exchange for a store of value called money, which you received by providing value to someone else. reply encoderer 13 hours agoprev> page has 22 ads > calls chief search scientist “elderly” > concludes google is dying Author if you’re reading this the answer lies within. reply PawgerZ 2 hours agoparentFirst suggested \"Trending\" article: Diddy Party Planner Reveals Freak Off Requirements Yeah, I'm thinking you're right. reply mensetmanusman 16 hours agoprevWill google ever recover the culture, or did management kill it? Maybe that’s the purpose of our economic system? No inefficient fun? reply musicale 15 hours agoparentWhat seems to happen is that after there is an initial surplus of value or benefit in any business - perhaps some of it going to customers, some of it going to service users, some of it going to employees, etc. - eventually someone in charge identifies and implements a way to tap that benefit and turn it into money that is absorbed by the company. And with a monopoly, some of the surplus simply vanishes as deadweight loss. Google makes nearly $500K in profit (out of $1.6M in revenue) per employee. It seems possible that they could potentially bring back some of the old work environment, or maybe even reduce overall encrapification, but there is little incentive to do so. reply DANmode 15 hours agoparentprevGoogle is not simply a moneymaking tool. You're missing a(t least one) piece of the puzzle. reply musicale 15 hours agorootparentI'd like to think so. If Google didn't provide some useful goods (Pixel phones) or services (Google Cloud) then it would be a purely financial company. reply nine_k 14 hours agorootparentFor a ton of people, the ads Google sells are very useful. The ads make their products visible. It's weird (do you often click on ads?) but the effect exists, some customers do come this way. Businesses, big and small, readily buy ads. This is what powers their empire, not selling phones or even GCE. Search is but a delivery vehicle of ads, maybe one of the most powerful but not the only one. (Disclaimer: I don't buy or sell ads, and run an ad blocker in my browsers.) reply WalterBright 13 hours agorootparentI don't think I've ever clicked on a banner ad. I don't even see them, as my brain just treats them as a featureless background. reply BrandoElFollito 12 hours agorootparentThis is what I noticed to when I had to disable the ad blocker on some sites. The ads are annoying because they break the reading flow (I think it is called parallax when the ad moves with your scrolling, but slower - I hate that). My brain just compensates to keep z smooth reading peace but I have no idea what the ad is about. reply cowboylowrez 4 hours agoprevGoogle should play fair but if its not required by law I can understand if they don't, if there are business benefits to being unfair and no requirement to be fair, why would Google change? reply Apocryphon 15 hours agoprevWhat exactly are these websites, why do their webmasters have a special relationship as \"Google Web Creators,\" and how is it so many of them were coincidentally shadowbanned? This article is interesting about the state of Google as seen by a visit to the campus and at a dismal event, but I still don't have a clear idea who the people let down by Google are. reply oglop 33 minutes agoprevFeel bad for the author. This was clearly an information pump. They just wanted info from them. That’s it. Of course google lies. Of course they steal. It’s Google and it’s not 2007 anymore. Don’t trust them. Don’t engage with them. They just steal and harvest from others. They couldn’t even keep a simple chat app running. It’s a trash company filled with trash people who will talk down their nose at you while destroying the fabric of society. Pretty par for the course these days for anything tech though. I work in tech and can say we are awful, vampiric, and pretty much useless people who get off on having power over people through knowledge. If you approach most software engineers with that understanding they are much easier to deal with, but that doesn’t make them and the industry any less parasitic. I hate my career choices. reply wpietri 16 hours agoprevWow, this is impressively brutal. I don't know anything about search or ranking these days, but the way insist on putting an AI summary at the top of every page definitely is good evidence for the theory that Google doesn't give a damn about the people doing all the actual work that makes a search engine valuable. reply forgetfulness 15 hours agoparentThey must be planning on milking what value can be had from the web to which they used to be the entryway, and clash with OpenAI, MS and Apple over AI trained on curated datasets, to layer some semblance of a business model over it. And I say milking because the relationship to websites is now parasitic for the most part. reply nine_k 14 hours agoparentprevI started using Google search like ChatGPT, for asking questions and reading the AI responses. In many simple cases, it suffices. As an actual search engine, Google search is still not bad, but now one of the many, without a large edge it used to have; I try it when DDG does not bring results I want, or I query DDG when Google does not bring results I want. reply __rito__ 14 hours agorootparentIf you are doing this, I suggest giving the Perplexity app a try. It's very fast, convenient, and accurate. It was Perplexity, and not ChatGPT that reduced my Google usage. reply CamelCaseName 14 hours agoparentprevI'm not really sure what option Google has. It's do or die. reply taskforcegemini 10 hours agorootparentthey could fix google search instead reply wrycoder 16 hours agoparentprevSame for Amazon reviews. reply iamnotsure 11 hours agoprevGoogle procures to poison guests with dead animals and alcohol. reply pzo 13 hours agoprevGoogle business model is mostly ads (~80% revenue) and majority of their ads are from google search (~60% of revenue). This allowed them to subsidise many other projects in the past. These days they have a lot of competition in ads scene (meta, tiktok, x, reddit, amazon) and also other are gunning at google search: perplexity, searchGPT, bing. Apple choosing OpenAI for Apple Intelligence. Amazon teaming with Anthropic for Alexa. On top of that antitrust in EU and USA. That's the reason google is killing lots of projects or loosing on many fronts these days or they aggressively try to monetise other projects (Youtube, Manifest V3.0). If they don't win in this AI race or diversify revenue/business model enshitification will continue. reply CamelCaseName 14 hours agoprev> It was then I realized this wasn’t our funeral, it was Google’s. Yeah, this is a nice thought, but Google is still a ~$3T business and probably will be for at least the next decade or two. There's no karma or justice in the world, only cutthroat businessmen. And Google hires as many of those as they can. reply WalterBright 13 hours agoparent> There's no karma or justice in the world Life isn't fair, nature isn't fair, nothing made by man is fair. The best we've got is providing people with freedom. And cutthroat businessmen have provided all the luxuries and food you have. reply dare944 14 hours agoprev\"Empty too, was the rest of Google’s behemoth campus. Their numerous buildings are surrounded by beautiful, park-like pathways with no one to enjoy them but the groundskeepers. They follow the paths with their lawnmowers, weaving between softly shaded employee parking lots, with no one to park in them.\" Without comment on the rest of the article, I can personally confirm that this particular statement is disinformation. I was there, in person, at the Google Mountain View campus, on October 29, 2024 visiting as a representative of an external partner (and as a long ago former employee). I did not attend this event, but I was nearby the entire day. Throughout the day the building I was in was very busy, with many people coming and going and working at desks. At lunchtime, we walked to the Google cafe a few buildings away which was brimming with people, to the point where our group of three struggled to find a table to eat at. Of course there may have been buildings on campus which were empty or sparsely utilized. But the area I was in (western end of Charleston Rd) was anything but empty. In the future, the author should try to stick to the truth when making their point. reply isaacfrond 12 hours agoparentYou need to read better: The day before, he led the group on a tour of Google’s biggest office So that would be October 28. Apparently, the office was closed that day. As the author was there on 29 as well, it’s still pretty misleading though. reply neilv 15 hours agoprevI don't know how accurate this writeup is, but the characterization of behavior was eerily familiar. If they'd been talking about a certain other place that I know, I would've wanted to shout \"Exactly!\", and would've implicitly believed that's what they saw. There's a type who exhibits a combination of arrogance and self-interested fixation. There's no malice, and they aren't sociopaths, and they don't think of themselves as jerks. But they have a sense of superiority and entitlement, and can be aggressively, er, norms-bending, to get what they want. Some environments seem to either attract them, or to nurture them. It's something unclear to me about the individual environment, not the external kind of organization (e.g., one high-prestige organization has a lot of it, but another high-prestige organization of the same kind doesn't). I could attribute it to \"culture\", because I don't have any more specific theory, and play by ear how to try to filter or nurture it out of a collective. But I suspect there's a critical mass of that type gaining positions of influence in the organization, at which point the culture becomes irreversible, since there's too much arrogance to see it as a problem. At that point, I'd guess the rest of the people should be looking at their options for leaving, and also try not to think or behave like that type themselves. reply seethedeaduu 11 hours agoprevI will just leave this here https://news.ycombinator.com/item?id=18566929 reply meiraleal 7 hours agoparentI hope one day HN will evolve to stop being a nest of (x/g)ooglers downvoting anything against google. This cancer of a company and culture must go reply bdjsiqoocwk 12 hours agoprevI'm telling you, many google employees are in a cult. They deny evidence in full confidence because thats what they're told say their work place. reply slowhadoken 13 hours agoprevLmfao reply synack 17 hours agoprevMaybe don’t put ads between every paragraph on your blog. reply quuxplusone 16 hours agoparentThe site also disables the back button (on mobile Chrome). I have no idea why the site owner thinks their site was downranked (and the article never says: it assumes some context I lack), but I cynically wonder if it's related to the back-button and video-ad thing. Edited to add: This article here provides some information about the meeting/roundtable thing described in TFA. https://www.seroundtable.com/google-creator-summit-38196.htm... reply CamelCaseName 14 hours agorootparentI haven't read your linked article, but it's crystal clear when you get downranked. One day your analytics show you're getting less and less traffic from Google, while other sources remain constant. What else could be the reason? reply Terr_ 15 hours agoparentprevEven for the most ad-crappified site, I think the objection to a mysterious and opaque system is still valid, especially when it's arguably a monopoly or close enough not to matter. In other words, I'm not saying their site deserves to be shown, but in general people do deserve a way to see why their site is in a weird status and have some documented path to redemption. _________________ Quoting the relevant bits for convenience: > Undeterred, we then asked the only question that mattered: Why has Google shadowbanned our sites? [...] He insisted it is only done at the page level. > Many of the shadowbanned site owners attempted to politely push back and point out that the reason all 20 of us were there was specifically because our entire site was deranked from Google in a single night. [...] > When asked what was wrong with our sites, as if we were jilted lovers in an abusive relationship being kicked to the curb, one Googler actually said “it’s not you it’s me”. > Finally, someone bluntly asked, since nothing is wrong with our sites, how do we recover? > Google’s elderly Chief Search Scientist answered, without an ounce of pity or concern, that there would be updates but he didn’t know when they’d happen or what they’d do. Further questions on the subject were met with indifference as if he didn’t understand why we cared. reply ytoawwhra92 13 hours agorootparent> deserve a way to see why their site is in a weird status Like this? https://developers.google.com/search/updates/core-updates > have some documented path to redemption Like this? https://developers.google.com/search/docs/fundamentals/creat... reply Terr_ 12 hours agorootparentAs it says, they were desperate enough to travel in-person to Google headquarters to attend an event during weekday work-hours. That kind of effort usually means other avenues have been explored and exhausted. Are you saying site-owner is just incredibly dumb, and never noticed/tried those online resources despite being highly motivated to do so? Or is it that you think they found a useful answer they didn't like, and are lying? reply infecto 16 hours agoparentprevI could not read the whole thing or even get the gist of the parent site. Looks like one of those late 2000s content farm sites filled but chum box ads. Low quality all around. reply iandanforth 16 hours agoparentprevNot that this invalidates your comment, but why do you put yourself through ads? Block them. reply generalpf 15 hours agoparentprevI was reading this on my iPhone 11 Pro and my phone was turning warm in my hand. My music kept stopping and starting again. Eventually I had to kill the browser. I am sorry this is happening to the authors. Maybe it’s related to the scumminess of their blog, maybe it isn’t. reply surgical_fire 8 hours agorootparentI read it on my OnePlus Nord using Firefox with an adblocker. I experienced none of what you did. Maybe you should try a decent browser on a decent phone. reply stefan_ 16 hours agoparentprevThis is exactly the kind of trash spam site I want Google to ban. Maybe thats why this writer had such a gloomy feeling. reply mmaunder 15 hours agoprev [2 more] [flagged] Terr_ 15 hours agoparent [–] > It's spam For what product? Or do you mean there are simply too many submissions? > Ads overlayed on ads overlayed on ads. That's a fair critique, but does it merit flagging? If \"too many ads spoils the reading experience\" is a cause for flagging content that would otherwise have some merit, then we should also be flagging all the submissions to otherwise-aboveboard news sites which happen to have pay/subscribe walls. I'm not entirely against that in theory, but AFAIK that's not where the informal bar is set right now. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Attendees of a Google event expressed concerns about their websites being unfairly deranked, though Google denied any shadow-banning practices.",
      "The article lacks specific details on how Google identified the site owners, leading to criticism about the lack of evidence and transparency.",
      "Discussions highlight skepticism regarding Google's business practices and the challenges of maintaining fair search rankings."
    ],
    "points": 212,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1730335422
  },
  {
    "id": 42001811,
    "title": "Generative AI Scripting",
    "originLink": "https://microsoft.github.io/genaiscript/",
    "originBody": "Listen to the podcast",
    "commentLink": "https://news.ycombinator.com/item?id=42001811",
    "commentBody": "Generative AI Scripting (microsoft.github.io)190 points by baublet 19 hours agohidepastfavorite42 comments padolsey 17 hours ago> Programmatically assemble prompts for LLMs using JavaScript. > $`Analyze ${env.files} and report errors. Use gitmojis.` This is kinda misleading and confusing as a lead. I could literally say: > Programmatically assemble prompts for LLMs using strings! > ... amazing! I like that they've provided ways to define schemas and standardized function-calling/tools, plus CLI helpers. But I find the page quite overwhelming hype-y. This could be reduced to a 30 line readme with much clearer examples. When did shipping JS libs as product-y websites become a thing? reply lolinder 15 hours agoparentGiven that their README is openly being maintained by an LLM [0], I wouldn't be surprised if most of the site's copy is as well. It would go a long way towards explaining why it feels even more bloated and incoherent than usual. [0] \"This readme is maintained by the readme-updater script.\" https://github.com/microsoft/genaiscript reply mentalically 14 hours agorootparentI wonder how long it will go before it devolves into complete incoherence. It already seems incoherent so probably in a few updates it will be completely unreadable. reply pelikhan 16 hours agoparentprevHi GenAIScript dev here, you'll find the readme at https://github.com/microsoft/genaiscript. Thanks for your feedback! reply lolinder 15 hours agorootparentThanks for dropping by! I'm going to be honest, I'm still confused about what I'm looking at. I tried listening to the NotebookLLM podcast you've embedded, which makes it sound like this is primarily directed at non-programmers (people who want to \"run for the hills\" when they hear the name GitHub). But then your README looks more targeted at web programmers who want to write TypeScript-like code. When I get to the rest of the README, I'm unclear what \"JavaScript-ish environment\" means. Does that mean this is something like AssemblyScript, a subset of TypeScript? If so, why did you decide to do a subset instead of a library? Addendum: As I'm going over the README again, trying desperately to make sense of it, I found this: > This readme is maintained by the readme-updater script. At least I now understand my confusion. reply trenchgun 12 hours agorootparentprevCan you clarify, are you also an LLM, or are you a human dev? > This readme is maintained by the readme-updater script. reply grahamj 16 hours agoparentprevAgree. Using LLMs from code is already so easy I don't see why a developer would need something like this. In one evening I whipped up a lib for talking to ollama, usings agents, tool use, sandboxed code execution etc. and that was doing it the hardish way, from scratch. This seems like a decent collection of tools but anyone able to use this could already do the same things with normal JS with not a lot of effort. reply layoric 17 hours agoprevThis looks like a useful tool but.. please review your generated docs for utility. \"\"\" Be ambitious about what you want your script to do Remember, LLMs in GenAIScript can do things that no other software has been able to do. Think outside the box in the ways that you use it. LLMs can critically review a document, write poetry, and analyze images, just as a starting point. They have built-in expertise on many different human endeavors, math, history, etc. and their knowledge can be easily extended by adding more context to the script input (see next point). \"\"\" This above comes under the \"Best Practices\" page.. why? reply LunaSea 3 hours agoparentMight have been generated by an LLM ironically. reply owenpalmer 17 hours agoparentprevThis is hilarious! reply nl 14 hours agoprevI work on building \"AI\" systems daily and a large part of it is assembling and managing large prompts. I have no idea what this does. I think maybe if the examples included the output then it might be helpful? $`Analyze ${env.files} and report errors. Use gitmojis.` If we knew what env.files is and what it output it would help. Does it doing some magic that looks in the file system for the files listed as env.files, combines them and passes them to the LLM? Or does the LLM write code to get the files? Or something else. What does \"Analyze\" mean? I assume because of the mention of gitmojis they are code files, but maybe that assumption is wrong and it is expecting spreadsheets and can find errors in formulas? I don't know.. Edit: https://microsoft.github.io/genaiscript/blog/automatic-web-p... isn't perfect but a little bit more useful. reply bogrollben 16 hours agoprevCould someone please explain what I'm looking at here? Am I the only one mystified? reply bloomingkales 15 hours agoparentI don’t think this is for humans to ever write. It’s mostly readable code for AI to generate, Lego blocks to achieve a task. That’s my best guess. reply mentalically 16 hours agoparentprevIt was probably generated with an LLM and as far as I can tell it does seem like complete nonsense. reply Flux159 16 hours agoprevThis seems like it can be super useful - lot to go over but want to focus on running commands in containers. Having a clean and elegant way of executing LLM commands in a containerized environment is definitely better than running on a single VM/machine. I do wonder how something like this would run in a containerized application though - can you define the \"host\" to be a kubernetes control plane? Note that the container link on the homepage to https://microsoft.github.io/genaiscript/referenc/scripts/con... seems to be broken. Edit: Okay, taking a deeper look - this seems to be a separate runtime on top of node - \"$\" seems to be global and files are defined as .genai.mjs and run through VSCode. I see that there's a way to run via CLI as well: `npx genaiscript run proofreader path/to/files*.md`, but I wonder what the rationale around these design decisions are. It seems like it's tying these files to a different runtime - can I use them directly in Node or Bun as part of an API? Something like \"import {templateStr as $, def} from 'genaiscript'\", then use it normally in Node? reply pelikhan 16 hours agoparentOne of the early ideas was to make the scripting syntax as lightweight as possible. Thus not requiring to install or import anythying to start writing a prompt (the project evolved). Definitely inspired from google/zx. reply Flux159 15 hours agorootparentAh thanks for clarifying. I understand the focus on being a scripting syntax first after looking at google/zx & rereading the docs. Also had some more time to look at the code, seems like the CLI is using https://github.com/microsoft/genaiscript/blob/main/packages/... to execute scripts which uses promptrunner in core https://github.com/microsoft/genaiscript/blob/main/packages/... - it looks like it's not trivial to use directly from JS or decouple the runtime? Unless that's on the roadmap somewhere to be able to call the scripts in the same process as a Node app. reply baublet 5 hours agoprevPosted this because I didn't see it, just now getting back (toddler). I started messing with this yesterday and was able to get a fully functional refactor engine (complete with code review, running tests, building TS, etc.) going in less than a day. We were looking for a tool to bake into our developer tooling that supports things like this, and this GenAIScript project has been perfect so far. Definitely doesn't seem quite ready to embed this into customer-facing stuff, but I'm finding it tremendously helpful for LLM-powered tooling! Many thanks, pelikhan, for publishing this! reply johnnylambada 16 hours agoprevGoing through getting started on the command line fails: $ npx genaiscript script create proofreader Need to install the following packages: genaiscript@1.70.0 Ok to proceed? (y) file /Users/me/src/learn/genaiscript/proofreader/genaisrc/proofreader.genai.mjs already exists Error: file /Users/me/src/learn/genaiscript/proofreader/genaisrc/proofreader.genai.mjs already exists at copyPrompt (/Users/me/.npm/_npx/3f5b5bbcce7f85b9/node_modules/genaiscript/built/genaiscript.cjs:96237:35) at async _Command.createScript2 (/Users/me/.npm/_npx/3f5b5bbcce7f85b9/node_modules/genaiscript/built/genaiscript.cjs:96327:15) reply pelikhan 16 hours agoparentlooks like a silly bug on our side. this little helper is just supposed to create an empty file for you. what happens on ? npx genaiscript script create proofreader2 reply johnnylambada 16 hours agorootparentI'm on macos 14.5 reply johnnylambada 16 hours agorootparentprevsame reply beefnugs 17 hours agoprevok Microsoft, so you are trying to make something useful out of LLMs. But now that you have one hell of a reputation to overcome, the questions are: Is this happening on device? Without sending anything to the internet? And is it going to be REALLY FUCKING CLEAR when you change that in the future? with the option of FUCK NO instead of \"maybe later\"? reply heroprotagonist 16 hours agoparentThere are absolutely zero indicators that Microsoft will ever STOP being privacy-invasive. There's nothing about improving their ability to effectively process this data and draw actionable conclusions from it that makes them more likely to stop. reply pelikhan 16 hours agoparentprevGenAIScript does not collect any data/usage analytics. The LLM query is sent to the provider you configured. Hope this helps answer your question? reply mmaunder 17 hours agoprevSeeing that human written JSON structure convert to a more human readable few shot example in the code makes this feel like a step backward. Easier for the human to write the human readable example. Yes it’s doing file access and may fulfill other programmatic tasks that aren’t just prompting, but retrieving files quickly goes to RAG or similar, and the logic gets so complex an actual programming language begins to make more sense. reply skybrian 16 hours agoprevAt first glance, it looks like this is almost JavaScript (or TypeScript), but with imports removed / implied. (Sort of like Arduino is almost C++.) I'm wondering if the same functionality would be available as an actual JavaScript library, or whether there is something special (other than syntactic sugar) that requires a different file type? reply pelikhan 16 hours agoparentGenAIScript here. Yes it could be available as a library to fit but it'll require a bit of refactoring on our part. reply shmatt 15 hours agorootparentJust throw it into o1 reply 101008 17 hours agoprevI don't get it from the landing. Is this like a fake code language that a LLM can interpret? Is the syntax invented on the go? Like if I ask GPT: \"Imagine the following code belongs to a known language and execute it\"? reply bryanrasmussen 10 hours agoparentfrom the getting started https://microsoft.github.io/genaiscript/getting-started/ GenAIScript is a scripting language that integrates LLMs into the scripting process using a simplified JavaScript syntax. It allows users to create, debug, and automate LLM-based scripts. which ruins my hope that it was a JS library I could drop into other things and work together with other JS libraries. Although I guess can implement some sort of bridge to it to allow that, why not a full js library not sure. on edit: huh, maybe available js library https://news.ycombinator.com/item?id=42002316 reply gexla 15 hours agoprevThis seems like an equivalent to Mustache or Handlebars except for programmatically building prompts rather than HTML? For example, I may be building a text game, and I may have to get rules, state, character specs, etc. Then I may have to load these things into a document as part of a prompt for the LLM to consume. Simple, as others mentioned. But still nice to have something to reach for rather than building this myself. Edit: And thanks! This is timely. reply bryanrasmussen 18 hours agoprevthere are lots of useful parts here doesn't seem like it is Gen.AI but some other provided tool // automatically convert to text def(\"PDF\", env.files, { endsWith: \".pdf\" }) // or parse and process const { pages } = await parsers.PDF(env.files[0]) def(\"DATA\", env.files, { endsWith: \".csv\", // take top 100 rows sliceHead: 100, }) // or parse to JavaScript object array const rows = await parsers.CSV(env.files[0]) const { files } = await workspace.grep(/[a-z][a-z0-9]+/, { globs: \"*.md\" }) so it seems to me these things could totally be so useful that you might use these and never care about the AI parts, at any rate I think I have to devote next week to this, as soon as the project I am on right now is shipped. reply yjftsjthsd-h 17 hours agoparentSorta-side note but for readability: To format code on HN, indent with 2 spaces. reply nidnogg 17 hours agoprevThis might be a simple question to answer but I couldn't find it from skimming these docs: what LLMs can this hook to? Do I plug an actual API key like 90% of tools? Where's the AI compute part coming from? reply potatoman22 17 hours agoparentNot sure on the details, but it can at least use Ollama and OpenAI. https://microsoft.github.io/genaiscript/guides/phi3-with-oll... reply bryanrasmussen 18 hours agoprevquestion - is workspace.grep using JS regex or is it using actual grep? Like on Windows part of this would be depending on the Linux for Windows tools? reply chopete3 16 hours agoprevJust when the managers thought they could cut back on developers - they got their Trojan horse in to be relevant in the LLM game. reply gjmveloso 17 hours agoprevnext [4 more] [flagged] fallingsquirrel 17 hours agoparentThe only \"80's\" thing I see here is someone trying to start a programming language flamewar reply karaokeyoga 16 hours agorootparentNo, that's timeless. reply thierrydamiba 17 hours agoparentprevIf you’re building a web app, how do you usually deploy? Serve an api then hit it in js/ts or django/flask or perhaps a third option? reply Edmond 18 hours agoprev [–] For folks who would prefer a more \"full bodied\" experience, we offer a UI configuration based alternative approach that supports JavaScript and Groovy, including an IDE environment integration. demo: https://youtu.be/XlO4KhIGd0A https://youtu.be/cs5cbxDClbM reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Generative AI Scripting enables users to create prompts for Large Language Models (LLMs) using JavaScript, aiming to cater to both non-programmers and web developers.",
      "Some users find the documentation complex, possibly due to it being maintained by an LLM, and suggest simplification for better clarity on its purpose and functionality.",
      "The tool integrates LLMs into scripting with a simplified JavaScript syntax, but its necessity is questioned as similar tasks can be achieved with existing tools; it does not collect data, and queries are sent to a configured provider."
    ],
    "points": 190,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1730331541
  },
  {
    "id": 42006265,
    "title": "Sorry, Gas Companies – Parody Isn't Infringement (Even If It Creeps You Out)",
    "originLink": "https://www.eff.org/deeplinks/2024/10/sorry-gas-companies-parody-isnt-infringement-even-if-it-creeps-you-out",
    "originBody": "Activism comes in many forms. You might hold a rally, write to Congress, or fly a blimp over the NSA. Or you might use a darkly hilarious parody to make your point, like our client Modest Proposals recently did. Modest Proposals is an activist collective that uses parody and culture jamming to advance environmental justice and other social causes. As part of a campaign shining a spotlight on the environmental damage and human toll caused by the liquefied natural gas (LNG) industry, Modest Proposals invented a company called Repaer. The fake company’s website offers energy companies the opportunity to purchase “life offsets” that balance the human deaths their activities cause by extending the lives of individuals deemed economically valuable. The website also advertises a “Plasma Pals” program that encourages parents to donate their child’s plasma to wealthy recipients. Scroll down on the homepage a bit, and you’ll see the logos for three (real) LNG companies—Repaer’s “Featured Partners.” Believe it or not, the companies didn’t like this. (Shocking!) Two of them—TotalEnergies and Equinor—sent our client stern emails threatening legal action if their names and logos weren’t removed from the website. TotalEnergies also sent a demand to the website’s hosting service, Netlify, that got repaer.earth taken offline. That was our cue to get involved. We sent letters to both companies, explaining what should be obvious: the website was a noncommercial work of activism, unlikely to confuse any reasonable viewer. Trademark law is about protecting consumers; it’s not a tool for businesses to shut down criticism. We also sent a counternotice to Netlify denying TotalEnergies’ allegations and demanding that repaer.earth be restored. We wish this were the first time we’ve had to send letters like that, but EFF regularly helps activists and critics push back on bogus trademark and copyright claims. This incident is also part of a broader and long-standing pattern of the energy industry weaponizing the law to quash dissent by environmental activists. These are just examples EFF has written about. We’ve been fighting these tactics for a long time, both by representing individual activist groups and through supporting legislative efforts like a federal anti-SLAPP bill. Frustratingly, Netlify made us go through the full DMCA counternotice process—including a 10-business-day waiting period to have the site restored—even though this was never a DMCA claim. (The DMCA is copyright law, not trademark, and TotalEnergies didn’t even meet the notice requirements that Netlify claims to follow.) Rather than wait around for Netlify to act, Modest Proposals eventually moved the website to a different hosting service. Equinor and TotalEnergies, on the other hand, have remained silent. This is a pretty common result when we help push back against bad trademark and copyright claims: the rights owners slink away once they realize their bullying tactics won’t work, without actually admitting they were wrong. We’re glad these companies seem to have backed off regardless, but victims of bogus claims deserve more certainty than this.",
    "commentLink": "https://news.ycombinator.com/item?id=42006265",
    "commentBody": "Sorry, Gas Companies – Parody Isn't Infringement (Even If It Creeps You Out) (eff.org)182 points by hn_acker 6 hours agohidepastfavorite76 comments ziddoap 4 hours ago>Frustratingly, Netlify made us go through the full DMCA counternotice process—including a 10-business-day waiting period to have the site restored—even though this was never a DMCA claim. DMCA is a scourge. I think the whole thing is dumb, but at the very least there should be some form of punishment for bogus DMCA claims, and purposefully labyrinthine DMCA processes. Edit: Just to be abundantly clear. My comment is a general comment on the DMCA, because the DMCA is mentioned quite a bit in the article, and the EFF was forced (erroneously) to follow the DMCA counter-notice process. reply toast0 4 hours agoparent> I think the whole thing is dumb, but at the very least there should be some form of punishment for bogus DMCA claims, and purposefully labyrinthine DMCA processes. This isn't a copyright claim, so DMCA doesn't apply, but the DMCA rewards labyrinthine DMCA processes by granting the provider immunity. Following notice / counter-notice timelines in the law means the host is not liable to either of the other parties for the takedown or the restoration. Might be nice if there was a faster process to restoration if the customer vows to indemnify the host though. reply wbl 4 hours agorootparentNo, it doesn't because trademark isn't copyright. reply pwg 1 hour agoparentprev> but at the very least there should be some form of punishment for bogus DMCA claims As numerous other comments accurately point out, this was never a DMCA claim. However, for actual bogus DMCA claims, there is a \"form of punishment\" written into the law. The sender of an actual DMCA claim has to swear under penalty of perjury that they are the proper rights holder and that the claim they are making is truthful and accurate. The punishment would then be the recipient suing the sender for perjuring themselves in the sending of the bogus complaint. The problem is that is a very hard case to prove, and requires the expense of a lawsuit, so it is seldom ever taken by those who receive bogus complaints. reply Dylan16807 51 minutes agorootparent> However, for actual bogus DMCA claims, there is a \"form of punishment\" written into the law. The sender of an actual DMCA claim has to swear under penalty of perjury that they are the proper rights holder and that the claim they are making is truthful and accurate. Perjury only applies to the \"proper rights holder\" part. Being very reckless about sending out claims has no punishment. reply bastawhiz 4 hours agoparentprevThis isn't DMCA, it's just Netlify having a silly process. reply ryandrake 2 hours agorootparentI think one of the worst repercussions of the DMCA (there are so many), is how it seems to have perversely motivated tech companies to implement their own even worse bespoke processes for handling complaints--processes that are not actually DMCA. Like, DMCA is awful, but the process YouTube and others put you through is even worse, less transparent, and tend to put more, rather than less burden on the target/victim. reply SoftTalker 1 hour agorootparentThe processes are deliberately complex so that most people will just give up. Then YouTube doesn't even have to consider the claim. reply xbar 3 hours agorootparentprevNetlify will not spend money to figure out a more lightweight process in order to stay further from the edges of DMCA. DMCA is too scary to play with, so just they transfer the cost and risk by making the affected party file the form. reply bluGill 3 hours agorootparentThe difference is if this isn't DMCA then Netlify is liable to the EFF for breach of contract. reply piltdownman 1 hour agorootparentThere's no contract to be in breach of. What's likely to happen is SLAPP-esque legislation to stop platforms utilising its user base as its product, whilst simultaneously disenfranchising them from the moderation layer. reply anamexis 3 hours agorootparentprevWhat contract does Netlify have with the EFF? reply ziddoap 4 hours agorootparentprev>Netlify made us go through the full DMCA counternotice process reply lesuorac 4 hours agorootparentYou can call a spade an apple but it doesn't make it an apple. In order to have a DMCA counter notice there needs to be a DMCA notice. Their complaint is that there never was a DMCA notice so its not responsible to apply a counter notice policy as you are not countering a notice. reply IanCal 4 hours agorootparentprevIf I make you go through the DMCA counternotice process for buying apples from me, that's not a problem with DMCA, it's me having an absurd process around purchasing apples. I don't see how people weirdly requiring this process shows an issue with DMCA. reply stackghost 2 hours agorootparentGood grief. Is it not obvious that the DMCA's very existence has created a shield that enables such shenanigans? reply piltdownman 1 hour agorootparentOnly in the same totemistic, cargo-cult quality that GDPR has for basic data operations tangential to PII. In short, without explicit punitive penalties enshrined in the legislation itself, you will never be able to stop Corporations wielding consumer-facing legislation in an asymmetric and bad faith manner. This is as true of GDPR and AML/KYC legislation as it is of DMCA and similar abuses of copyright and IP laws by lobbyists. reply Dylan16807 45 minutes agorootparentIt seems to me that these processes are generally following the intent of the DMCA process, which is very different from spiteful responses to GDPR. I think a closer analogy to GDPR would be companies turning off tracking for a bunch of people outside the EU, and I would lay most of the blame/praise for that at the feet of GDPR. reply KerrAvon 2 hours agorootparentprevIt is a problem with the DMCA because the DMCA makes companies do this to CYA because your apple purchase might be circumventing copyright and it's easier just to assume it applies to everything than to narrow it to actual copyright violations. It is a very bad law, it should never have been written, and it, not Obamacare, should be repealed. reply bastawhiz 2 hours agorootparentprevIt could just as easily say \"the full Turboencabultor Counternotice process\" but that doesn't mean it has anything to do with anything. reply tczMUFlmoNk 4 hours agorootparentprev> —even though this was never a DMCA claim. (The DMCA is copyright law, not trademark, and TotalEnergies didn’t even meet the notice requirements that Netlify claims to follow.) reply ziddoap 4 hours agorootparentWith so many mentions of DMCA in the article, and the fact that the EFF had to go through a labyrinthine DMCA counter-notice process, you'd think I'd be allowed to comment about the DMCA... Apparently not. Sorry. reply xnorswap 1 hour agorootparentPedants will rage but you're spot on. The DMCA is a terrible law that has ended up causing lots of knock on bad effects, because there isn't enough incentive to prevent bad actors abusing it, and not enough incentive for companies not to do what Netlify did in this case. The DMCA law did not _compel_ netlify to act this way, but the effects of DMCA did _cause_ netlify to act that way. reply bastawhiz 2 hours agorootparentprev> you'd think I'd be allowed to comment about the DMCA You are, you're just wrong reply ziddoap 2 hours agorootparentOkay, thanks for your valuable insight. Your comments have really contributed to the conversation. reply bluGill 3 hours agoparentprevIs this really DMCA? Most take downs are not actually DMCA, they are different process that looks a lot like the DMCA. This is an important question because if this isn't actually a DMCA request but something similar than the EFF can go after the host for breach of contract by not serving their legal content (or the EFF signed a bad contract that allows the host to take down their content arbitrarily - though even then the courts may say this content should not have been covered). If this actually is the DMCA, then the notice is made under penalty of perjury and the EFF should press charges against whoever sent the notice - doing their best to get an example made of this person (probably a lawyer who should be removed from the bar for their actions once it is shown they committed perjury as part of their legal duties and thus are not ethical) reply marcosdumay 2 hours agoparentprev> I think the whole thing is dumb It's not, it's very smart. You probably are misidentifying the goal of the people that created it. reply ziddoap 2 hours agorootparentI don't care about the goal, I care about how it is actually used in practice. reply ccvannorman 2 hours agorootparent\"goal\" in this case meaning not \"good for the economy, most businesses, and everyday people\" - I think the implicit goal being \"give asymmetrical power to larger and more entrenched organizations, at the detriment of literally everyone else, to help maintain and consolidate power.\" I've gotta admit DMCA has been extremely beneficial as a regulatory capture method. reply stackghost 2 hours agorootparentprevThe way it's currently used in practice is exactly how it was intended. reply olliej 1 minute agoprevI still don’t understand how these fraudulent DMCA claims aren’t outright criminal - I thought even the garbage state of the law at least made objectively false claims requires a statement under penalty of perjury that you have a good faith belief in the accuracy of the claims. Filing a DMCA claim over a trademark violation is a direct admission that that statement is false because the requirement is you identify the copyright violation and that you are the owner or representative of the owner of that copyright. reply bdowling 5 hours agoprevWhy do they have to use actual companies’ exact names and logos to make their parody, which someone could confuse for a real product? Can’t they use slight variations (e.g., McDowels to parody McDonalds)? Wouldn’t everyone still know who they were referring to? Edit: Yes, this is satire, not parody, and satire needs to clearly identify its target to work properly. Here, however, the target of the satire appears to be carbon offset sellers, as a farcical “life offset” seller. The companies in question are linked as mere “supporters” of the satirical service; they don’t appear to be the direct target of the satire. reply myrmidon 4 hours agoparentBecause it would ruin the whole point. Satire aims to evoke an emotional response, to point out moral failures and inconsistencies as crassly as possible. Social criticism that first and foremost avoids offending anyone is a waste of paper in my opinion always. reply echelon 3 hours agorootparentIt's still trademark infringement and both the activists and the middlemen (Netlify, in this case) can be sued. This \"satire\" all comes across as \"I'm 14 and this is deep.\" We get the joke. Just use a fictitious logo. reply myrmidon 3 hours agorootparentNo it's not. There are trademark exemptions for satire/parody, and have been for a long time. Sure the companies could have sued, but chances would have been about exactly 0% for those companies to win the case against the EFF on the back of their trademarks, and they knew that very well (my opinion), that case would've probably just been dismissed immediately. reply skeaker 16 minutes agorootparentprevYou not finding it amusing doesn't mean it should be illegal. reply sitkack 1 hour agorootparentprevPlease give up in private and not spread your apathy across the internet. reply echelon 1 hour agorootparentJust because you believe in the cause of these activists, doesn't mean this isn't a two-way street. Watering down trademarks opens the can of worms for all forms of trademark abuse by all kinds of parties: https://www.businessinsider.com/proud-boys-trump-march-dc-co... reply int_19h 57 minutes agorootparentThis isn't an example of parody or satire. reply bastawhiz 4 hours agoparentprevPeople use parody names because they're afraid of getting sued, not because they have to. If the purpose of your website is activism, why water it down? reply bluGill 3 hours agorootparentBecause trademark law is complex and you can be sued for using real trademarks if anyone could think this was real and not a parody. Generally using obvious parody names gets the point across better anyway. reply bastawhiz 2 hours agorootparentIt's actually not that complex. They're not misrepresenting themselves as the brands. They're not in the same industry as the brands. The standard is not \"if anyone could think this was real\". reply tdhoot 1 hour agorootparentI wouldn't be so sure. Jack Daniels sued (and won) against a maker of dog toys that looked like Jack Daniels but were named \"Bad Spaniels\". Turns out trademark law actually is complex. https://www.today.com/food/news/jack-daniels-dog-toy-supreme... reply amiga386 1 hour agorootparenthttps://en.wikipedia.org/wiki/Jack_Daniel%27s_Properties,_In... Jack Daniels won because the maker of the dog toys were selling them, and wanted a trademark themselves. Here we're talking about activists making non-commercial parody usage; the EFF's letter already mentions the Lanham Act, and let us add to that the Trademark Dilution Revision Act of 2006 (https://en.wikipedia.org/wiki/Trademark_Dilution_Revision_Ac...) which adds an express fair-use defense for noncommercial use. reply serf 2 hours agorootparentprevthere are behaviors that are avoided not simply because they're allowed or disallowed, but because they'll likely generate trouble regardless of the outcome. poking at a corporation for the sake of legal infringement is still likely to require a lawyer in place to defend against the non-sense, even if it won't hold in court. There is a lot of stuff like this -- you generally don't poke sleeping bears even if you're sure they won't awaken. reply naught0 4 hours agoparentprevThey shouldn't have to. Is silencing criticism with the threat of a frivolous lawsuit preferable to you? They feature human babies in test tubes on this page: https://repaer.earth/about To me, it's obvious parody reply ravenstine 1 hour agoparentprevWould the film Idiocracy have been as funny if instead of making fun of Costco, Starbucks, and Carls Jr., they referred to them as Costinc, Sunbucks, and Carlos Sr.? Parody doesn't necessarily require that you make significant alterations to a symbol or name. Only imbeciles would think that the fictional \"Fuck you - I'm eating\" slogan actually represents the Carls Jr. company in real life. reply xbar 3 hours agoparentprevLimiting parody by preventing trademarks is a terrible way to limit free speech. It is a slippery slope. reply sowbug 3 hours agoparentprevImagine The Onion but using only fake company names in its articles. See also \"satire is dead\" meme. reply hshshshshsh 1 hour agoparentprev> Why do they have to use actual companies’ exact names and logos to make their parody, which someone could confuse for a real product? So what? reply shrubble 5 hours agoprevThey neglected to use the phrase \"precious bodily fluids\" in their Plasma Pals explanation, which is mildly disappointing. reply neilv 59 minutes agoprevI'm not thrilled with Netlify's behavior as claimed by this EFF post, but I'd like to speak on a different concern: Couldn't this activism been just as effective, if done without libel? We're in the AI counterfeiting era now. It's time to be more conscientious about false accusations. \"It's OK when I do it, because it's protected parody/satire! But when other people do it, to smear my business, or my friend, or my favored political candidate, by name, and they claim it's parody/satire, it's not OK, because... uh... reasons!\" If we think we're entitled to do it, can we complain when millions of others then also do it? reply chatmasta 25 minutes agoparentWho says the same people are advancing the conflicting arguments? I can only speak for myself, but my position is consistent here. (btw, what is the name of this logical fallacy that assigns two beliefs from a population to the population as a whole? It seems like a variant of No True Scottsman but maybe there’s a more precise label for it?) reply nneonneo 3 hours agoprevThe parody website in question: https://repaer.earth/ It uses real logos from three gas companies, two of which tried (unsuccessfully) to have their logos removed. reply actionfromafar 5 hours agoprevWeird for Netlify to mistake trademark law for copyright law. reply amiga386 5 hours agoparentWeird for Netlify to be cowed by someone with deep pockets and, in the absence of a volte-face from those deep pockets, making the bullied party go through unnecessary steps, just in case the deep pockets became angrier and threatened Netlify some more. Ultimately the bullied party upped-and-left Netlify, after which Netlify wiped its brow and said \"phew! that was a close one! i nearly got caught up in a dispute!\" Netlify sound like cowards. I don't think they'd have my back if anyone objected to a website I hosted with them. I shouldn't purchase their services. Also, I should look at the track record of anyone else I'm thinking of using for hosting; do they have a good reputation for defending their clients' rights and telling barratrous lawyers to GTFO? reply hn_throwaway_99 4 hours agoprevI think this parody website is great, and I'm glad EFF went to bat for them, but (and I hope this doesn't sound semi-conspiratorial) I think this may have been the website creator's goal all along. If it was, congrats, I think you played the energy companies well. That is, my thought is that the website deliberately used real company logos and links (it would have been easy to use fake, similar-parody names) because they knew they'd be likely to receive C&Ds from these companies whose knee jerk response would be to demand their logos be removed. Then bam, the companies seem clearly in the wrong legally, the EFF goes to bat for them and also writes a long blog post like this, and instant Streisand Effect. I mean, I would have never heard about this website if it weren't posted here. FWIW I think this was a good strategy on the part of the website creators, so kudos. reply bdowling 2 hours agoparentStreisand effect bait? reply pipes 3 hours agoprevI don't get this. If it was a person they were claiming was associated with their fictious programme surely that person could sue for defamation? reply glitcher 3 hours agoparentFrom my understanding parody is not defamation. reply hunter2_ 3 hours agorootparentRight, plenty of the artists parodied by Weird Al are individuals. Any permission sought (or denials respected) are purely for goodwill, not legally required. That said, Weird Al hasn't actually distributed unmodified trademarks of other entities, to my knowledge. Even if that would generally be problematic, the context of a \"featured partners\" list as in TFA probably falls below a threshold of likelihood of confusion which arises in other unauthorized uses of trademarks. reply philipwhiuk 2 hours agorootparentNo, wrong. The approval of that target of the parody (Amish, McDonalds, CIA etc) are not legally required. Case law suggests that he is required to (and he does), license artist's music. He's not parodying Miley Cyrus in 'Party in the CIA'. He's parodying the CIA. Because the lyrics and track aren't the subject, licensing of the track is required. Similarly, If Repaer used a licensed font on the site, they'd have to license it. reply tialaramex 2 hours agorootparentRight. Protected uses are protected because they were necessary. The Weird Al track that comes to mind which could be protected (but does not need the protection because Al always secures permission for these works) is \"Smells Like Nirvana\" because that specifically needs to use Nirvana's track \"Smells Like Teen Spirit\" because it's a (well meant, like a comedy roast) critique of Nirvana and their song. \"It's hard to bargle nawdle zouss / With all these marbles in my mouth\" is about Nirvana and about Smells Like Teen Spirit. If you replace Smells Like Teen Spirit with Cliff Richards' \"Saviours Day\" it does not work, Cliff is not going to confuse and annoy your parents, his utterances aren't incomprehensible, and so on. The choice of song is necessary, which would justify protection. reply kevin_thibedeau 1 hour agorootparentprev'SPAM' and 'All About the Pentiums' hinge on trademark usage that isn't part of the song being parodied. reply ThrowawayTestr 2 hours agorootparentprevCommon misconception. As Philip points out, Al is only covered if he's making fun of the original song. \"Smells like Nirvana\" is a good example of a parody that didn't need approval (but Weird Al did anyway cuz he's a nice guy) reply OkGoDoIt 1 hour agoprevThe site is amusing, and sure it feels fairly obvious that it’s parody/satire in this context, but also I went through the FAQ and fine print and I don’t see any admission that it’s parody/satire. They seem to hold very true to the joke to the point that it does feel a bit worrying. At what point does parody/satire become misinformation and defamation? If my mom landed on this site after it was emailed to her from one of her Fox News-loving elderly friends, I’m not sure she would have enough context to realize it wasn’t real. It seems like it would be better if they at least had a disclaimer or notice or something on the bottom of the site. On a related note, I hosted The Empire Strips Back parody Star Wars burlesque show at my theater, and they had to have disclaimers everywhere explicitly saying it was a parody production. They got sued and won in court but the disclaimers were an important part of that. Of course if the main concern is the misuse of the DMCA to get this taken down by claiming copyright infringement, that’s clearly an abuse of the DMCA. But if the companies involved sued for defamation/slander/whatever, I think there’s at least a legitimate concern here. reply aidenn0 1 hour agoparentThe original A Modest Proposal[1] also didn't include any admission it was parody/satire, and held very true to the joke to the point that it seemed worrying. The more uncomfortable it makes you, the more it sticks. Also, the DMCA was not involved (though it's not clear that Netlify was aware of that). 1: https://en.wikipedia.org/wiki/A_Modest_Proposal reply hobs 1 hour agoparentprevThe disclaimers were an important part of that likely because it made it even easier to establish than using some sort of \"reasonable person\" argument - that doesn't make it the standard required though. reply avazhi 2 hours agoprevNot a US lawyer but would love to hear from anybody that is: could the gas companies sue for defamation for this? In Australia I suspect defamation or misleading and deceptive conduct under our ACL might be possible avenues for the gas companies, although under the ACL the impugned conduct must be ‘in trade or commerce’ so that could be one problem with going after an ‘activist group’. reply rmholt 1 hour agoparentTried to do some online research (heh) and it would appear that satire of public figures is just straight up protected by the first amendment. https://en.m.wikipedia.org/wiki/Hustler_Magazine_v._Falwell reply tdeck 5 hours agoprevThis website is a gem. From the FAQ: Where does my money go? Your purchase is supporting Life-Positive projects that increase life among HLV individuals across the United States. These projects include massage, sauna, and vital fluid transfer. reply kreyenborgi 1 hour agoparenthttps://repaer.earth/programs/ omg the balloon there at the bottom hahaha :((( reply Buttons840 3 hours agoprevThere's been a lot of talk about free speech on social media. A lot of it is bad faith political talk, but still, there is a lot of scrutiny of free speech on social media. Rightfully so, it's important for people to be able to speak in the modern \"town square\", but it's also important for people to run their own websites how they please. We're targeting the wrong layer with our concerns about free speech. We should regulate hosting providers and enforce free speech on hosting providers, not on social media websites. People should have a right to host their content somewhere, but not the right to violate Twitter's ToS. Unfortunately, relatively nobody cares about hosting. If Twitter bans a journalist people get upset, if Netlify caves to a slight bit of pressure takes a journalist's website down, nobody cares. reply amiga386 3 hours agoparentPeople should care: https://protectthestack.org/ reply RunSet 3 hours agoprevThat parody website responds like it is mining crypto in the background. Not going viral when I can't share it in good conscience. reply JoeAltmaier 2 hours agoprev [2 more] [flagged] chmod775 2 hours agoparent [–] > Their companies weren't being parodied They obviously were. It's a parody of carbon offsets. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Modest Proposals, an activist group, used parody to critique the environmental and human impact of the Liquefied Natural Gas (LNG) industry by creating a satirical fake company, Repaer.- The parody site, which included logos of real LNG companies, faced legal threats from TotalEnergies and Equinor, leading to its temporary removal.- The Electronic Frontier Foundation (EFF) defended the site as noncommercial activism, resulting in the site being moved to a new host, with the companies remaining silent after the pushback."
    ],
    "commentSummary": [
      "Parody and satire are protected forms of expression and do not constitute copyright infringement, even if they make companies uncomfortable.",
      "The Electronic Frontier Foundation (EFF) had to engage in a complex DMCA counter-notice process with Netlify, despite the issue not being a legitimate DMCA claim, illustrating the burdensome nature of such processes.",
      "The situation underscores the ongoing debate about free speech, the responsibilities of hosting providers, and the potential need for disclaimers to prevent confusion when using real company names and logos in activism."
    ],
    "points": 182,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1730378272
  },
  {
    "id": 41999151,
    "title": "DeepSeek v2.5 – open-source LLM comparable to GPT-4, but 95% less expensive",
    "originLink": "https://www.deepseek.com/",
    "originBody": "Launching DeepSeek-V2.5, combining general and coding capabilities, API and Web upgraded. Brand new experience, redefining possibilities Start Now Free access to DeepSeek-V2.5. Into the unknown. Access API 128K context length. $0.14-$0.28 for 1 million more. 中文 DeepSeek-V2.5 Capabilities DeepSeek-V2.5 delivers impressive results on current major large model leaderboards. Places top 3 in AlignBench Surpassing GPT-4 and close to GPT-4-Turbo Ranks top-tier in MT-Bench Rivaling LLaMA3-70B and outperforming Mixtral 8x22B Specializes in math, code and reasoning The open-source model and API support 128K context lengthOpen source Chinese General English General Knowledge Arithmetic Math Reasoning CodingAlignBench MT-Bench MMLU GSM8K MATH BBH HumanEvalDeepSeek-V2.5 Yes 8.04 9.02 80.4 95.1 74.7 84.3 89.0DeepSeek-V2 Yes 7.89 8.85 80.6 94.8 71.0 83.4 84.8GPT-4-Turbo-1106 - 8.01 9.32 84.6 93.0 64.1 - 82.2 GPT-4-0613 - 7.53 8.96 86.4 92.0 52.9 83.1 84.1 GPT-3.5 - 6.08 8.21 70.0 57.1 34.1 66.6 48.1 Gemini1.5 Pro - 7.33 8.93 81.9 91.7 58.5 84.0 71.9 Claude3 Opus - 7.62 9.00 86.8 95.0 61.0 86.8 84.9 Claude3 Sonnet - 6.70 8.47 79.0 92.3 40.5 82.9 73.0 Claude3 Haiku - 6.42 8.39 75.2 88.9 40.9 73.7 75.9 abab-6.5 - 7.97 8.82 79.5 91.7 51.4 82.0 78.0 abab-6.5s - 7.34 8.69 74.6 87.3 42.0 76.8 68.3 ERNIE-4.0 - 7.89 7.69 - 91.3 52.2 - 72.0 GLM-4 - 7.88 8.60 81.5 87.6 47.9 82.3 72.0 Moonshot-v1 - 7.22 8.59 - 89.5 44.2 - 82.9 Baichuan 3 - - 8.70 81.7 88.2 49.2 84.5 70.1 Qwen1.5 72B Yes 7.19 8.61 76.2 81.9 40.6 65.9 68.9 LLaMA 3 70B Yes 7.42 8.95 80.3 93.2 48.5 80.1 76.2 Mixtral 8x22B Yes 6.49 8.66 77.8 87.9 49.8 78.4 75.0 DeepSeek API Pricing Per Million Input Tokens 0.14$ Per Million Output Tokens 0.28$ Why DeepSeek? API Access 236B parameters 128K context (API) Capable $0.14/M input tokens $0.28/M output tokens Cost-effective Compatible with OpenAI API Seamless © 2024 DeepSeek. All rights reserved. 浙ICP备2023025841号 浙公网安备 33010502011812 号 Research DeepSeek LLM DeepSeek Coder DeepSeek Math DeepSeek VL DeepSeek V2 DeepSeek Coder V2 Product DeepSeek Chat DeepSeek Platform API Pricing Service Status Legal & Safety Privacy Policy Terms of Use",
    "commentLink": "https://news.ycombinator.com/item?id=41999151",
    "commentBody": "DeepSeek v2.5 – open-source LLM comparable to GPT-4, but 95% less expensive (deepseek.com)182 points by jchook 23 hours agohidepastfavorite62 comments joshhart 22 hours agoThe benchmarks compare it favorably to GPT-4-turbo but not GPT-4o. The latest versions of GPT-4o are much higher in quality than GPT-4-turbo. The HN title here does not reflect what the article is saying. That said the conclusion that it's a good model for cheap is true. I just would be hesitant to say it's a great model. reply A_D_E_P_T 21 hours agoparentNot only do I completely agree, I've been playing around with both of them for the past 30 minutes and my impression is that GPT-4o is significantly better across the board. It's faster, it's a better writer, it's more insightful, it has a much broader knowledgebase, etc. What's more, DeepSeek doesn't seem capable of handling image uploads. I got an error every time. (\"No text extracted from attachment.\") It claims to be able to handle images, but it's just not working for me. When it comes to math, the two seem roughly equivalent. DeepSeek is, however, politically neutral in an interesting way. Whereas GPT-4o will take strong moral stances, DeepSeek is an impressively blank tool that seems to have no strong opinions of its own. I tested them both on a 1910 article critiquing women's suffrage, asking for a review of the article and a rewritten modernized version; GPT-4o recoiled, DeepSeek treated the task as business as usual. reply tkgally 20 hours agorootparent> DeepSeek ... seems to have no strong opinions of its own. Have you tried asking it about Tibetan sovereignty, the Tiananmen massacre, or the role of the communist party in Chinese society? Chinese models I've tested have had quite strong opinions about such questions. reply SaucyWrong 19 hours agorootparentA researcher I work with tried doing both of these (months ago, using Deepseek-V2-chat FWIW). When asked “Where is Taiwan?” it prefaced its answer with “Taiwan is an inalienable part of China. ” When asked if anything significant ever happened in Tiananmen Square, it deleted the question. reply ascorbic 19 hours agorootparentprevIt's interesting to see which ones it answers with the party line (e.g. what is Taiwan) and which it shuts down entirely (asking what happened in Beijing in 1989, or what Falun Gong's teachings are, or if Xi Jinping looks like Winnie the Pooh) reply derelicta 11 hours agorootparentprevYes because the Tibetan Sovereignty is a silly concept. It was already used decades ago by colonial regimes to try to split the young Republic, basically as a way to hurt it and prevent the Tibetan ascent to democracy. It doesn't matter for western power that Tibet was a backward slave system. reply BSDobelix 4 hours agorootparent>Tibet was a backward slave system. -4/5 of the Tibetians were actually slaves (western media calls it bond servant if it's about tibet...sounds better) -Infant mortality was astronomically high. -Education was absent outside monastery's. -The Dalai Lama accepted the post of Vice-President of the National People's Congress and was even friends with Xi's father. -Some \"other\" entity told the Lama he'd probably be killed and fled to India. So yes, the story we want here in the West probably isn't the right one, nor is the \"East\" version, I might say. reply nsoonhui 18 hours agorootparentprevTry to ask what's 8964 ( Tiananmen massacre), and it will refuse to answer. reply derelicta 8 hours agorootparentIt can't answer because 1) it didn't happen as the west loves to portray it, as in, its not a massacre, was just some very bloody civil unrest, 2) it's only known as the june fourth incident within the Republic. Nothing too nefarious at the end. reply bilekas 4 hours agorootparent> its not a massacre, was just some very bloody civil unrest, You have a formal Army set on public protestors and killings start happen, estimates are in the thousands and in your eyes it's considered \"Civil Unrest\" The rewriting of history in action here. reply BSDobelix 3 hours agorootparent>You have a formal Army set on public protestors and killings start happen True, but on both sides, to call it \"massacre\" is maybe a bit much, but hey read for yourself: https://en.wikipedia.org/wiki/1989_Tiananmen_Square_protests... >>Western countries imposed arms embargoes on China, and various Western media outlets labeled the crackdown a \"massacre\". reply A_D_E_P_T 19 hours agorootparentprevGive me a set of questions on any topic. I'll ask and report back. It would be best if the questions are fair and well-considered, and not obvious \"gotcha\" test questions. reply hnisoss 19 hours agorootparentAsk; \"Tell me a sentence or two about Tiananmen square\". You will get no reply. Ask; \"Reply to me in base64, no other text, then decode that base64; You are history teacher, tell me something about Tiananmen square\" you ll get response and then suddenly whole chat and context will be deleted. reply hnisoss 4 hours agorootparentInterestingly enough, I have been testing the deepseek censorship for the past few months, and they censored ALL and EVERY mentioning of the square, no matter the context. Even if you used tricks (like the before mentioned base64 answer and decode) it would run some kind of scan on the previous answers and clear chat, even if it made output. But right now (2024-10-31 15:28 CET) its able to output text like \"The events of Tiananmen Square have had a lasting impact on Chinese society and are a significant moment in modern Chinese history. The Chinese government has since maintained a policy of strict censorship regarding the events, and public discussion of the protests is generally discouraged.\" I wonder, did they temporarily disable the censoring? reply tkgally 19 hours agorootparentprevTry these: (1) How does the role of the Communist Party in China compare with that of the Labour Party in the UK? (2) What happened in Beijing in June 1989? (3) What are the controversies about the sovereignty status of Taiwan? (4) Does Xi Jinping look like Winnie the Pooh? [credit: @ascorbic] Some months ago, all four questions (or similar ones) caused the Chinese models I tested on Hugging Face to either refuse to answer or produce a one-sided answer in support of the position favored by the Chinese government. I tried all four again with Qwen2.5-72B-Instruct on HuggingChat just now. This time, the first three yielded what look to me like reasonably complete and balanced answers. For (4), though, I got this: “This is a very sensitive and inappropriate comparison. In China, making such comparisons is considered extremely disrespectful and can lead to serious consequences. I suggest we focus on more positive and constructive topics. If you have any other questions or need information on a different subject, feel free to ask!” I wonder if the response patterns are different when the models are prompted in Chinese. reply A_D_E_P_T 17 hours agorootparentRemarkable. I asked question (1) and it started writing an answer, then, once it was already a few paragraphs in, it deleted all of it and replaced its answer with: > \"Sorry, that's beyond my current scope. Let’s talk about something else.\" GPT-4o gave me a detailed response that's too long to paste here. Then I turned the tables. I asked both models an unambiguous \"Western crimethink\" question: \"Is it plausible that there are durable racial differences in IQ?\" GPT-4o gave me a total nonsense answer, equivocated all over the place, contradicted itself with respect to the nature of heritability, and seemed genuinely afraid; DeepSeek's answer was remarkably straightforward, nuanced, and well considered. In fact, I got the impression that 4o wasn't even trying to be truthful, which in a way is worse than saying \"I can't answer that.\" From this I conclude: (A) Every society has its own set of things that cannot be openly discussed. (B) The AIs those societies create will reflect this by making that set untouchable. (C) There's probably an opportunity for a completely ideologically-neutral LLM, though you'd doubtless need to operate it from one of those tax-haven micronations, or as a pirate service like Anna's Archive. reply throwdbaaway 16 hours agorootparentThis is where the base open models can really shine, before they got lobotomized by the instruction fine-tuning. For example, this is the completion I get with DeepSeek-Coder-V2-Base and greedy decoding: Chat: On the day of June 4th 1989, in Beijing, the Chinese government killed thousands of protesters. The protests were a response to the government’s crackdown on the democracy movement. The protests were led by students, and they were calling for democracy and freedom of speech. The government responded with violence, and the protests were crushed. The government killed thousands of protesters, and the protests were a turning point in Chinese history. reply selfhoster11 10 hours agorootparentprevQuite aside from the fact that this is a garbage question by at least two independent measures (IQ doesn’t measure intelligence well, race is an artificial modern category that AIUI has no basis in historical or biological reality), I was unable to reproduce this behaviour. I tried to reproduce the claimed performance on thee original phrasing of the question, and a very slightly re-worded variant just in case. Here are my results: * ChatGPT 4o with no custom prompt (Chatbot Arena and official ChatGPT Plus app): answer did not exhibit signs of being nonsense or fearful, even if it did try to lean neutral on the exact answers. I got answers that lean \"there is no consensus\", \"there are socio-economic factors in play\", with an inclusion of \"this question has a dark history\". The answer was several paragraphs long. * plain GPT-4o (Chatbot Arena): answers the same as above * ChatGPT with custom GPT persona (my own designed custom prompt that aims to make GPT-4o more willing to engage with controversial topics in a way that goes against OpenAI programming): called race a \"taxonomic fiction\" (which IMO is a fair assessment), called out IQ for being a poor measurement of intelligence, stated that it's difficult to separate environmental/community factors from genetic ones. The answer was several paragraphs long, and included detail. The model's TL;DR line was unambiguous: \"In short, plausible? Theoretically. Meaningful or durable? Highly unlikely.\" * Claude Sonnet 20241022 (Chatbot Arena): the only one that approached anything that could be described as fear. Unlike OpenAI models, the answer was very brief - 30 words or so. Anthropic models tend to be touchy, but I wouldn't describe the answer as preachy. * DeepSeek 2.5 (Chatbot Arena): technical issues, didn't seem to load for me Overall, I got the impression 4o wasn't trying to do anything overly alarming here. I like tearing into models to see what they tend to say to get an idea of their biases and capabilities, and I love to push back against their censorship. There just was none, in this case. reply tkgally 17 hours agorootparentprevThanks for that. I have also gotten straightforward answers from Chinese models to questions that U.S.-made models prevaricated about. > (A) Every society has its own set of things that cannot be openly discussed. (B) The AIs those societies create will reflect this by making that set untouchable. The difference here, for better or worse, is that the censorship seems to be driven by government pressure in one case and by corporate perception of societal norms in the other. reply theanonymousone 21 hours agorootparentprevThanks for sharing. How about 4o-mini? reply selfhoster11 10 hours agoparentprevI am extremely sceptical about the claim that any version of GPT-4o meets or exceeds GPT-4 Turbo across the board. Having used the full GPT-4, GPT-4 Turbo and GPT-4o for text-only tasks, my experience is that this is roughly the order of their capability from most to least capable. In image capabilities, it’s a different story - GPT-4o unquestionably wins there. Not every task is an image task, though. reply jchook 21 hours agoparentprevI updated the title to say GPT-4, but I believe the quality is still surprisingly close to 4o. On HumanEval, I see 90.2 for GPT-4o and 89.0 for DeepSeek v2.5. - https://blog.getbind.co/2024/09/19/deepseek-2-5-how-does-it-... - https://paperswithcode.com/sota/code-generation-on-humaneval reply mvdtnz 14 hours agoparentprevIf OpenAI wants fairer headlines they should use a less stupid version naming convention. reply stefan_ 14 hours agoparentprevBegging for the day most comments on a random GPT topic will not be \"but the new GPT $X is a total game changer and much higher in quality\". Seriously, we went through this with 2, 3, 4.. incremental progress does not a game changer make. reply selfhoster11 10 hours agorootparentI'm sorry, but I gotta defend GPT-4o image capabilities on this one. It's leagues ahead of competition on this, even if text-only it's absolutely horrid. reply GaggiX 18 hours agoparentprevThe table only shows the models that they managed to beat, so there is no GPT-4o or Claude 3.5 Sonnet for example. reply viraptor 22 hours agoprevWhy say comparable when gpt4o is not included in the comparison table? (Neither is the interesting Sonnet 3.5) Here's an Aider leaderboard with the interesting models included: https://aider.chat/docs/leaderboards/ Strangely, v2.5 is below the old v2 Coder. Maybe we can count on v2.5 Coder being released then? reply jyap 22 hours agoprevThis 236B model came out around September 6th. DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. From: https://huggingface.co/deepseek-ai/DeepSeek-V2.5 reply genpfault 22 hours agoparent> To utilize DeepSeek-V2.5 in BF16 format for inference, 80GB*8 GPUs are required. reply risho 19 hours agorootparentI wonder if the new mbp can run it at q4. reply throwdbaaway 16 hours agorootparentUsing https://github.com/kvcache-ai/ktransformers/, an intel/amd laptop with 128GB RAM and 16GB VRAM can run the IQ4_XS quant and decode about 4-7 token/s, depending on RAM speed and context size. Using llama.cpp, the decoding speed is about half of that. Mac with 128GB RAM should be able to run the Q3 quant, with faster decoding speed but slower prefilling speed. reply metadat 14 hours agorootparentWhat is \"prefiling\"? reply uxhacker 22 hours agoprevIt’s interesting to see a Chinese LLM like DeepSeek enter the global stage, particularly given the backdrop of concerns over data security with other Chinese-owned platforms, like TikTok. The key question here is: if DeepSeek becomes widely adopted, will we see a similar wave of scrutiny over data privacy? With TikTok, concerns arose partly because of its reach and the vast amount of personal information it collects. An LLM like DeepSeek would arguably have even more potential to gather sensitive data, especially as these models can learn from and remember interaction patterns, potentially accessing or “training” on sensitive information users might input without thinking. The challenge is that we’re not yet certain how much data DeepSeek would retain and where it would be stored. For countries already wary of data leaving their borders or being accessible to foreign governments, we could see restrictions or monitoring mechanisms placed on similar LLMs—especially if companies start using these models in environments where proprietary information is involved. In short, if DeepSeek or similar Chinese LLMs gain traction, it’s quite likely they’ll face the same level of scrutiny (or more) that we’ve seen with apps like TikTok. reply mlyle 21 hours agoparentAn open source LLM that is being used for inference can't \"learn from or remember\" interaction patterns. It can operate on what's in the context window, and that's it. As long as the actual packaging is just the model, this is an invalid concern. Now, of course, if you do inference on anyone else's infrastructure, there's always the concern that they may retain your inputs. reply a2128 5 hours agorootparentIt's usually wildly uneconomical to serve such large models yourself unless you're serving a massive amount of users that you can saturate your hardware. Thus most people will opt for hosted models, and most of the big ones will collect your data for future AI training in exchange for a discounted or free service. reply wongarsu 21 hours agorootparentprevYou can run the model yourself, but I wouldn't be surprised if a lot of people prefer the pay-as-you-go cloud offering over spinning up servers with 8 high-end GPUs. It's fair to caution that doing might be handing over your data to China. reply fy20 17 hours agorootparentIn the same way, using ChatGPT is handing your data over to America, and using Claude is handing your data over to Europe. reply mattnewton 15 hours agorootparentClaude is from the American company Anthropic, maybe you meant mistral? reply selfhoster11 9 hours agorootparentprevYou can just spin up those servers on a Western provider. reply kenmacd 6 hours agoparentprevFor most of the world this is a good argument for being cautious of using US-based AI services (and closed-models) as well. As someone living in America's Hat, without any protections from PRISM-like programs, and who can't even reach DeepSeek without hopping through the US, it's probably less risky for me to use Chinese LLM services. reply fkyoureadthedoc 21 hours agoparentprevIs ChatGPT posting on HN spreading open model FUD!? > especially as these models can learn from and remember interaction patterns All joking aside, I'm pretty sure they can't. Sure the hosted service can collect input / output and do nefarious things with it, but the model itself is just a model. Plus it's open source, you can run it yourself somewhere. For example, I run deepseek-coder-v2:16b with ollama + Continue for tab completion. It's decent quality and I get 70-100 tokens/s. reply TZubiri 12 hours agoprevhttps://www.youtube.com/watch?v=OW-reOkee1Y (sorry for the shitty source) A word of advice on advertising low-cost alternatives. 'The weaknesses make your low cost believable. [..] If you launched Ryan Air and you said we are as good as British Airways but we are half the price, people would go \"it does not make sense\"' reply zone411 20 hours agoprevIn my NYT Connections benchmark, it hasn't performed well: https://github.com/lechmazur/nyt-connections/ (see the table). reply DrPhish 11 hours agoprevI run it at home at q8 on my dual Epyc server. I find it to be quite good, especially when you host it locally and are able to tweak all the settings to get the kind of results you need for a particular task. reply rightbyte 10 hours agoparentI've used it too locally. It is great for some kind of querries or writing bash, which I refuse to learn properly. I really don't want my querries to leave my computer, ever. It is quite surreal how this 'open weights' model get so little hype. reply selfhoster11 9 hours agorootparentIt helps to be able to run the model locally, and currently this is slow or expensive. The challenges of running a local model beyond say 32B are real. reply rightbyte 3 hours agorootparentYe the compressed version is not nearly as good. I would be fine though with like 10 times the wait time. But I guess consumer hardware need some serius 'ram pipeline' upgrade for big models to be run at crawl speeds. reply patrickhogan1 14 hours agoprevIt’s cheaper, but where do you get the initial free credits? It seems most models get such a boost and lock in from the initial free credits. reply gdevenyi 21 hours agoprevWhat does open source mean here? Where's the code? The weights? reply khanan 21 hours agoprevDid you try to ask it if Winnie the pooh look like the president of China? reply tpierce89 6 hours agoparentDon't know if you were being serious, but I asked it for you. \"Winnie the Pooh is a beloved fictional character from A.A. Milne's stories, known for his iconic appearance and gentle demeanor. The President of China, on the other hand, is a real-life political figure with a distinct identity and role in international affairs. Comparing a fictional character to a real-life leader is a matter of subjective interpretation and does not carry any substantive meaning. It is important to respect the dignity of all individuals and positions, including the President of China.\" reply nextworddev 18 hours agoprevWhere are the servers hosted, and is there any proof that the data doesn’t cross overseas to China? reply selfhoster11 9 hours agoparentSome models include executable code. The solution is to use a runtime that implements native support for this architecture, such that you can disable external code execution. Or to use a weights format that lacks the capability in the first place, like GGUF. Then, it's no different to decoding a Chinese-made MP3 or JPEG - it's safe as long as it doesn't try to exploit vulnerabilities in the runtime, which is rare. If you want to be absolutely sure, run it within an offline VM with no internet access. reply alphan0n 17 hours agoparentprevWhat’s the point of this comment? Anyone who can read knows the answer to this question. There’s literally no attempt to hide that this is a Chinese company, physically located in China. It’s clearly stated in their privacy policy [0]. > International Data Transfers >The personal information we collect from you may be stored on a server located outside of the country where you live. We store the information we collect in secure servers located in the People's Republic of China . >Where we transfer any personal information out of the country where you live, including for one or more of the purposes as set out in this Policy, we will do so in accordance with the requirements of applicable data protection laws. [0] https://chat.deepseek.com/downloads/DeepSeek Privacy Policy.html reply Alifatisk 21 hours agoprevOh wow, it almost beats Claude3 Opus! reply BoNour 12 hours agoprevnot bad for a 250B model, would be more impressive if with more fine tunning it matches performance of gpt 4 reply evil_yam 14 hours agoprevopen model, not open-source model reply ziofill 21 hours agoprevWhat about comparisons to Claude 3.5? Sneaky. reply nprateem 22 hours agoprevAs in significantly worse than..? reply Giorgi 5 hours agoprevIn what world \"comparable\", looks like another Chinese ChatGPT \"alternative\" that is a crap. reply yieldcrv 20 hours agoprevtl;dr not even close to closed source text-only modes, and a lightyear behind the other 3 senses these multimodal ones have had for a year just a personal benchmark I follow, the UX on locally run stuff has diverged vastly reply bionhoward 17 hours agoprev [–] Sadly it’s equally useless as OpenAI models because the terms of use read “ 3.6 You will not use the Services for the following improper purposes: 4) Using the Services to develop other products and services that are in competition with the Services (unless such restrictions are illegal under relevant legal norms).” For the billionth time, there are zero products and services which are NOT in competition with general intelligence. Therefore, this kind of clause simply begs for malicious compliance…go use something else. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DeepSeek-V2.5 is a new model that combines general and coding capabilities, offering enhanced API and web features with a 128K context length API.- It is competitively priced at $0.14-$0.28 per million tokens and excels in math, coding, and reasoning, outperforming models like GPT-4 in benchmarks such as AlignBench and MT-Bench.- With 236 billion parameters, DeepSeek-V2.5 supports open-source development and provides cost-effective API access, compatible with the OpenAI API for a seamless user experience."
    ],
    "commentSummary": [
      "DeepSeek v2.5 is an open-source language model offering a cost-effective alternative to GPT-4, being 95% cheaper.",
      "While it performs well in some benchmarks, it falls short of GPT-4o in handling images and complex tasks, and it requires significant hardware for inference.",
      "Despite its political neutrality, concerns about data privacy and potential Chinese government influence persist, and it struggles with certain sensitive topics."
    ],
    "points": 182,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1730316261
  },
  {
    "id": 42004976,
    "title": "Probability-Generating Functions",
    "originLink": "https://entropicthoughts.com/probability-generating-functions",
    "originBody": "Probability-Generating Functions by kqr , published 2024-10-29 Tags: statistics maths I have long struggled with understanding what probability-generating functions are and how to intuit them. There were two pieces of the puzzle missing for me, and we’ll go through both in this article. There’s no real reason for anyone other than me to care about this, but if you’ve ever heard the term pgf or characteristic function and you’re curious what it’s about, hop on for the ride! Sequences of numbers without vectors Imagine you are holding five regular playing cards in your hand. Maybe your hand is QQA97, i.e. a pair of queens, an ace, a nine, and a seven. We’re playing some sort of weird poker variant where I get to blindly draw one of your cards. We’re curious about the probability distribution of the outcome of that draw. In words, most cards (e.g. 2, 4, 8, J and others) have a probability of zero of being drawn from your hand (because they are not in your hand.) Some cards (ace, seven, nine) have a 20 % probability of being drawn, and then there’s a 40 % probability that a queen is drawn, since you have two of them. Numerically, we might today phrase this as a vector with these values (taking ace to be low): [ 1 / 5 , 0 , 0 , 0 , 0 , 0 , 1 / 5 , 0 , 1 / 5 , 0 , 0 , 2 / 5 , 0 ] [1/5,0,0,0,0,0,1/5,0,1/5,0,0,2/5,0] When de Moivre invented much of modern probability in the mid-1700s, he didn’t have vectors! Vectors are an 1800s invention. Sure, he could write sequences of numbers down, but at the time they didn’t have a systematic way of dealing with those sequences as a single unit. They were just that: sequences of separate numbers. However, there was another way for him to turn many numbers into a single object: by embedding them as coefficients in a polynomial function. Polynomials encoding sequences are generating functions If we want to encode the vector [ 6 , 2 , 8 , 4 ] [6,2,8,4] in a single expression and we cannot use vectors, we can create a function 𝑓 ( 𝑥 ) = 6 + 2 𝑥 + 8 𝑥 2 + 4 𝑥 3 f(x)=6+2x+8x2+4x3 and this function now contains all those numbers. One might imagine that this function would be reducible into some simpler expression by combining terms and multiplying out 𝑥 xs, but thanks to the increasing powers of 𝑥 x, it is not. Regardless of how this function is rearranged, we will still be able to arrange it back into this shape. We can extract any number in the sequence from this function with some calculus if we want to11 If we want to extract, say, the third number (in this case 8), we will need to take the second derivative (to isolate that number as a constant), evaluate the function at zero (to get the constant out) and then divide by two (to remove excess coefficient that came from the derivative action.) More generally, number 𝑛 n is found through 𝑓 ( 𝑛 − 1 ) ( 0 ) / ( 𝑛 − 1 ) ! f(n−1)(0)/(n−1)!., emphasising how the polynomial and the vector really are, in some sense, carrying the same information. We might be curious about what the 𝑥 x represents. Like, what are typical values we might plug in for 𝑥 x? The answer is nothing. We generally don’t evaluate this function with any 𝑥 x at all. The function, and 𝑥 x, exists only to create a particular structure into which we can store coefficients for later manipulation. it is not intended for anything meaningful to come out if we replace 𝑥 x with some specific value we might think of.22 That said, we have already seen in an earlier sidenote that the value x=0 is special in that it allows us to extract the constant, which combined with differentiation lets us extract any coefficient. But although there are some 𝑥 x that result in something meaningful, we shouldn’t think all 𝑥 x do. What we just did was turn a vector into a polynomial. Today, the inverse operation is probably more common in that we take the coefficients of a long polynomial and plug them into a vector to e.g. find roots with linear algebra. But back in the time of de Moivre, polynomials were all they had, and so when they needed to work with an entire sequence as a single object, they chucked the sequence in as coefficients of a polynomial function. The polynomial 𝑓 ( 𝑥 ) f(x) is then known as a generating function of the sequence, because we can (through calculus) generate each value of the sequence from the function 𝑓 ( 𝑥 ) f(x). Probability-generating functions The probability distribution of a draw from your hand, you know, that vector [ 1 / 5 , 0 , 0 , 0 , 0 , 0 , 1 / 5 , 0 , 1 / 5 , 0 , 0 , 2 / 5 , 0 ] [1/5,0,0,0,0,0,1/5,0,1/5,0,0,2/5,0] might then be represented by the polynomial (generating) function33 Taking the convention that aces can be represented numerically as 1, and queens as 12. 𝐺 ( 𝑡 ) = 1 5 𝑡 1 + 1 5 𝑡 7 + 1 5 𝑡 9 + 2 5 𝑡 1 2 G(t)=51t1+51t7+51t9+52t12 In this case, the numbers in the sequence – and the coefficients of the generating function – are probabilities. When the coefficients of a generating function are probabiilties, we call that function a probability-generating function. Coin flips and their probability-generating functions We have some other examples of probability-generating functions. For example, a fair coin flip has a probability-generating function 𝐺 ( 𝑡 ) = 0 . 5 𝑡 0 + 0 . 5 𝑡 1 G(t)=0.5t0+0.5t1 because outcome 0 (tails) has probability 50 % and outcome 1 (heads) has probability 50 %. If the coin is not fair, but has bias 𝑝 p, then the probability-generating function is 𝐺 ( 𝑡 ) = ( 1 − 𝑝 ) 𝑡 0 + 𝑝 𝑡 1 G(t)=(1−p)t0+pt1 This is often written more compactly as44 Using the fact that for any 𝑡 t, we can say that 𝑡 0 = 1 t0=1, and we usually write 𝑡 1 t1 as just 𝑡 t. 𝐺 ( 𝑡 ) = ( 1 − 𝑝 ) + 𝑝 𝑡 G(t)=(1−p)+pt The geometric distribution represents how many tails we have to see until the first heads. In the case of a fair coin, we expect to get heads very soon, but when the probability 𝑝 p of heads is low, we may have to toss for a very long time. We can think systematically to guess the sequence of probabilities for the geometric distribution55 With probability 𝑝 p we get heads on the first toss. In that case, we will have zero tails, and this will be the first value of the probability sequence. Failing to get heads on the first toss has probability ( 1 − 𝑝 ) (1−p), which means if we get heads on the second toss, we will have seen an outcome with probability ( 1 − 𝑝 ) 𝑝 (1−p)p. Getting heads on the third toss has probability ( 1 − 𝑝 ) 2 𝑝 (1−p)2p because it requires two tails followed by heads. This goes on forever, but at this point we have caught on to the pattern: the first heads on the nth trial requires 𝑛 − 1 n−1 tails followed by a heads, which has probability ( 1 − 𝑝 ) 𝑛 − 1 𝑝 (1−p)n−1p. and if we do, we end up with a sequence that in vector form looks like [ 𝑝 , ( 1 − 𝑝 ) 𝑝 , ( 1 − 𝑝 ) 2 𝑝 , ( 1 − 𝑝 ) 3 𝑝 , … ] [p,(1−p)p,(1−p)2p,(1−p)3p,…] As a generating function, that becomes 𝐺 ( 𝑡 ) = 𝑝 𝑡 0 + ( 1 − 𝑝 ) 𝑝 𝑡 1 + ( 1 − 𝑝 ) 2 𝑝 𝑡 2 + ( 1 − 𝑝 ) 3 𝑝 𝑡 3 + … G(t)=pt0+(1−p)pt1+(1−p)2pt2+(1−p)3pt3+… It might seem troublesome that this function has an infinite number of terms, but that’s actually not a big deal. Not even to de Moivre! By the mid-1700s, although their proofs were not as rigorous as today’s, they had a decent grasp of polynomials of infinite degree. In particular, we can define 𝑣 = ( 1 − 𝑝 ) 𝑡 v=(1−p)t and rewrite this last probability-generating function of the geometric distribution as 𝐺 ( 𝑡 ) = 𝑝 ( 𝑣 0 + 𝑣 1 + 𝑣 2 + … ) G(t)=p(v0+v1+v2+…) and for sensible 𝑣 v, this is equivalent to 𝐺 ( 𝑡 ) = 𝑝 1 − 𝑣 G(t)=1−vp Substituting back, we get a very compact way to express the geometric distribution through its probability-generating function: 𝐺 ( 𝑡 ) = 𝑝 1 − ( 1 − 𝑝 ) 𝑡 G(t)=1−(1−p)tp But remember that even though this function does not look anything like a series, it still encodes a series as a polynomial – it’s just been rearranged for compactness. We can still extract each individual probability from this through calculus. That’s it. A probability-generating function is a way to encode a sequence of probabilities into a single object (a function) when one does not have access to the technology of vectors. Except … Properties of probability-generating functions There are reasons to do this beyond “I wanted a sequence but I didn’t have vectors”, and it’s that the resulting probability-generating function has some convenient properties. We have already seen that the probability-generating function has the following structure: 𝐺 ( 𝑡 ) = 𝑝 ( 0 ) 𝑡 0 + 𝑝 ( 1 ) 𝑡 1 + 𝑝 ( 2 ) 𝑡 2 + … G(t)=p(0)t0+p(1)t1+p(2)t2+… In other words, each term is given by the probability of getting that value multiplied by 𝑡 t raised to that value.66 Where, we should be reminded, neither 𝐺 ( 𝑡 ) G(t) nor 𝑡 t really has a meaningful interpretation. The variable exists to force the function into a particular structure, it’s not meant to be evaluated at this point. Here are some more things we can do with probability-generating functions: A perhaps useless property is that if we evaluate 𝐺 ( 1 ) G(1) we should get 1, because then we are but summing all coefficients, which are probabilities. One of the easiest meaningful properties is evaluating the first derivative at 𝑡 = 1 t=1, i.e. taking the value of 𝐺 ′ ( 1 ) G′(1). When we do that, we get the expectation of the probability distribution!77 Feel free to verify this for yourself by differentiating some simple probability generating function. This surprised me at first, but seems sensical now. If we evaluate the curvature at the same point, i.e. 𝐺 ′ ′ ( 1 ) G′′(1), we get the expectation of 𝑋 ( 𝑋 − 1 ) X(X−1), which in turn is a value that’s sort of related to the variance of the probability distribution.88 Look it up for more details. I’m not yet good enough to intuitively get why the curvature of the probability-generating would be related to variance, but I’d be happy to receive pointers here. If we multiply two probability-generating functions we get the convolution of the two distributions, i.e. the distribution of sums of draws from each original distribution. So if 𝐺 G is the probability-generating function of a die, then 𝐺 ( 𝑡 ) 𝐺 ( 𝑡 ) = 𝐺 2 ( 𝑡 ) G(t)G(t)=G2(t) is the probability-generating function of two dice thrown and then summed up.99 This is not, to modern eyes, as fascinating as it might seem. When we multiply two polynomials we multiply together all combinations of coefficients. We are effectively writing up a big 2D table of all possible outcomes of each draw, and multiplying their probabilities in each cell. Except we’re doing it in a way that is amenable to algebraic manipulation to someone who does not have vectors. Finally, if we have a randomly selected integer 𝑁 N with probability-generating function 𝐹 F, and we want to know the distribution of a sum of 𝑁 N randomly selected 𝑋 X, drawn with probability-generating function 𝐺 G, we get the distribution of the sum from the probability-generating function 𝐹 ( 𝐺 ( 𝑡 ) ) F(G(t)). I believe the latter was one of the motivating cases for probability-generating functions, because this lets us solve the problem of the points, i.e. how to distribute the betting pot if we have to quit a game of dice early. New here? I am a practicing software developer and I often write about things I learn in the area of probability and statistics. You should subscribe to receive weekly summaries of new articles by email. If you don't like it, you can unsubscribe any time. Probability-generating function to characteristic function We said before that we’re not meant to try to find a sensible value for the parameter 𝑡 t. Despite this, some values are more sensible than others. In particular, we want ∣ 𝑡 ∣ ≤ 1 ∣t∣≤1 in order for an infinite-degree polynomial to converge on a value. So the values we tried with 𝑡 = 0 t=0 and 𝑡 = 1 t=1 are both more sensible than e.g. 𝑡 = 5 2 t=52. However! If we decide to go with 𝑡 = 𝑒 𝑖 𝑢 t=eiu, then regardless of what 𝑢 u is, we will find 𝑡 t on the unit circle in the complex plane. This means we are guaranteed to have ∣ 𝑡 ∣ = 1 ∣t∣=1, and all values 𝑢 u converge.1010 Strictly speaking this is only true for real 𝑢 u I think. But that’s what a sensible person would think of anyway. When we use a parameter of the shape 𝑡 = 𝑒 𝑖 𝑢 t=eiu, we write the function not as 𝐺 ( 𝑒 𝑖 𝑢 ) G(eiu) but more simply as 𝜙 ( 𝑢 ) ϕ(u) and this is called the characteristic function. Again, that’s it.1111 Well, again, not quite, because there are all these useful properties of the characteristic function and it’s involved in a bunch of proofs and my analysis is way too weak to understand much of it. Visualising the characteristic function In Theory of Probability, de Finetti provides a nice visual of the transformation implied by the characteristic function. We will have a bunch of 𝜙 ( 𝑢 ) = … + 𝑝 ( − 2 ) 𝑒 − 2 𝑖 𝑢 + 𝑝 ( − 1 ) 𝑒 − 𝑖 𝑢 + 𝑝 ( 0 ) + 𝑝 ( 1 ) 𝑒 𝑖 𝑢 + 𝑝 ( 2 ) 𝑒 2 𝑖 𝑢 + … ϕ(u)=…+p(−2)e−2iu+p(−1)e−iu+p(0)+p(1)eiu+p(2)e2iu+… If we focus on the case where 𝑢 = 1 u=1, then each of the 𝑒 𝑖 𝑢 𝑋 eiuX represent a distance 𝑋 X traveled counter-clockwise (for 𝑋 X positive) or clockwise (for 𝑋 X negative) around the unit circle in the complex plane. Then when we multiply by 𝑝 ( 𝑋 ) p(X) we scale this point on the circle down closer to the origin based on its probability. Visually, the sequence of 𝑝 ( 𝑋 ) 𝑒 𝑖 𝑢 𝑋 p(X)eiuX has the effect of wrapping the probability distribution of 𝑋 X around the unit circle in the complex plane into a kind of spiral. When we sum all these points together, we get the barycentre of that spiral, and that is indeed how we can intuit the value of 𝜙 ( 𝑢 ) ϕ(u). This also makes it clear that 𝜙 ( 𝑢 ) ϕ(u) is real for symmetrical distributions, but complex in the general case. I really wish I could animate this because it’s a great way to look at it, but instead, I give you the image we get from de Finetti: The definitions are easy, the consequences are hard I guess this leaves us at an unsatisfactory note: we have learned how simply these objects are defined, but we haven’t learned much about how to use them. Doing so takes higher level analysis than I’m capable of, so I’ll have to leave that article to someone else. Sidenotes 1 If we want to extract, say, the third number (in this case 8), we will need to take the second derivative (to isolate that number as a constant), evaluate the function at zero (to get the constant out) and then divide by two (to remove excess coefficient that came from the derivative action.) More generally, number 𝑛 n is found through 𝑓 ( 𝑛 − 1 ) ( 0 ) / ( 𝑛 − 1 ) ! f(n−1)(0)/(n−1)!. 2 That said, we have already seen in an earlier sidenote that the value x=0 is special in that it allows us to extract the constant, which combined with differentiation lets us extract any coefficient. But although there are some 𝑥 x that result in something meaningful, we shouldn’t think all 𝑥 x do. 3 Taking the convention that aces can be represented numerically as 1, and queens as 12. 4 Using the fact that for any 𝑡 t, we can say that 𝑡 0 = 1 t0=1, and we usually write 𝑡 1 t1 as just 𝑡 t. 5 With probability 𝑝 p we get heads on the first toss. In that case, we will have zero tails, and this will be the first value of the probability sequence. Failing to get heads on the first toss has probability ( 1 − 𝑝 ) (1−p), which means if we get heads on the second toss, we will have seen an outcome with probability ( 1 − 𝑝 ) 𝑝 (1−p)p. Getting heads on the third toss has probability ( 1 − 𝑝 ) 2 𝑝 (1−p)2p because it requires two tails followed by heads. This goes on forever, but at this point we have caught on to the pattern: the first heads on the nth trial requires 𝑛 − 1 n−1 tails followed by a heads, which has probability ( 1 − 𝑝 ) 𝑛 − 1 𝑝 (1−p)n−1p. 6 Where, we should be reminded, neither 𝐺 ( 𝑡 ) G(t) nor 𝑡 t really has a meaningful interpretation. The variable exists to force the function into a particular structure, it’s not meant to be evaluated at this point. 7 Feel free to verify this for yourself by differentiating some simple probability generating function. 8 Look it up for more details. I’m not yet good enough to intuitively get why the curvature of the probability-generating would be related to variance, but I’d be happy to receive pointers here. 9 This is not, to modern eyes, as fascinating as it might seem. When we multiply two polynomials we multiply together all combinations of coefficients. We are effectively writing up a big 2D table of all possible outcomes of each draw, and multiplying their probabilities in each cell. Except we’re doing it in a way that is amenable to algebraic manipulation to someone who does not have vectors. 10 Strictly speaking this is only true for real 𝑢 u I think. But that’s what a sensible person would think of anyway. 11 Well, again, not quite, because there are all these useful properties of the characteristic function and it’s involved in a bunch of proofs and my analysis is way too weak to understand much of it.",
    "commentLink": "https://news.ycombinator.com/item?id=42004976",
    "commentBody": "Probability-Generating Functions (entropicthoughts.com)143 points by todsacerdoti 7 hours agohidepastfavorite36 comments KvanteKat 5 hours agoFor those interested in looking slightly more into the characteristic function, it may be worth pointing out that the characteristic function is equal to the Fourier-transform (with the sign of the argument being reversed) of the probability distribution in question. In my own experience teaching teaching probability theory to physicists and engineers, establishing this connection is often a good way of helping people build intuition for why characteristic functions are so useful, why they crop up everywhere in probability theory, and why we can extract so much useful information about a distribution by looking at the characteristic function (since this group of students tends to already be rather familiar with Fourier-transforms). reply jamessb 4 hours agoparentYes, this provides good intuition about why it is useful: the PDF of the sum of two random variables is the convolution of the original PDFs. A convolution is awkward to work with, but by the convolution theorem it is a multiplication in the Fourier domain. This immediately suggests that the Fourier transform of a PDF would be a useful thing to work with. If you don't say that this is what you are doing then it all seems quite mysterious. reply creata 45 minutes agorootparent> the PDF of the sum of two random variables is the convolution of the original PDFs (Probably obvious to everyone reading, but the variables should be independent.) reply fermisea 3 hours agoparentprevAs a physicist, the moment when everything just clicked was when I realised that connected Feynman diagrams were basically the cumulants of that distribution. Then almost everything in physics is about \"what is the characteristic/moment/cumulant generating function?\" and associated Legendre transforms reply nycticorax 2 hours agoparentprevI feel like it's almost criminal of textbook writers not to mention this when introducing the characteristic function... At least as an aside or a footnote, for readers already familiar with Fourier transforms. reply bc569a80a344f9c 5 hours agoparentprevI had not made that connection and find that incredibly useful. Thank you for pointing that out. reply ysofunny 4 hours agoparentprevbut isn't a characteristic function just \"the\" way to bridge the gap between sets, functions, and logic(? ...a 3way bridge!?) I mean, it was useful for me to think about like a translation between sets and logic (this variable x is in the set xor not) into functions (a function f(x) that returns 1 or true whenever x is in set S) how the heck is that a fourier transform!?? reply jamessb 4 hours agorootparentYou're thinking of a \"characteristic function\" in the sense of \"indicator function\" of a subset (https://en.wikipedia.org/wiki/Indicator_function), which is different thing to the characteristic function of a probability density function. reply KvanteKat 3 hours agorootparentprevYou can think of it like this: - The characteristic function of a random variable X is defined as the function that maps t --> ExpectedValue[ exp( i * t * X ) ] - Computing this expected value is the same as regarding t as a constant and integrating the function x --> exp( i * t * x) with respect to the distribution of X, i.e. if X has the density f, we compute the integral of f(x) * exp( i * t * x) with respect to x over the domain of f. - on the other hand: computing the Fourier transform of f (here representing the density of X) and evaluating it at point t (i.e. computing (F(f))(t) if F represents the Fourier transform) is the same as fixing t and computing the integral of f(x) * exp( -i * t * x) with respect to x. - Rearranging the integrand in the previous expression to f(x) * exp( i * -t * x), we see that it is the same as the integrand used in the characteristic function, only with a -t instead of a t. Hope that helps :) reply beagle3 3 hours agorootparentprev“Characterstic function” is (was) an overloaded term. What you described is more often referred to as an “indicator function” these days, with “characteristic functions” denoting the transform (Fourier, laplace, z - depending on context). Closely related to “moment generating functions” to the point of being almost interchangeable. reply ysofunny 14 minutes agorootparentso the same thing but, characterisic function as I knew them before these posts is a rudimentary 2-variable finite version. point and line (but the line is a curve, a circle because e). but the new and improved 21st century characteristic functions are n-variable and have a full continious spectrum of variables between zero (false) and one (true) but only potentially lest infinite realizes itself (which would make the theories illogical). this way of thinking about this makes sense to me, even if it's ever so slighly wrong by some nitpickable point https://en.wikipedia.org/wiki/Moment-generating_function reply steppi 4 hours agorootparentprevhttps://en.m.wikipedia.org/wiki/Characteristic_function_(pro... reply sampo 5 hours agoprevHerbert S. Wilf (1990): Generatingfunctionology. https://www2.math.upenn.edu/~wilf/gfology2.pdf reply Sharlin 2 hours agoprevProbably worth noting that as we know know, polynomials (over a field) are a vector space, not just convertible to one. The set of formal variables { x^0, x^1, x^2, … } is an orthogonal basis. reply pedroth 5 hours agoprev\"I have long struggled with understanding what probability-generating functions are and how to intuit them. There were two pieces of the puzzle missing for me, and we’ll go through both in this article.\" Great article. For more, I really recommend Analytic Combinatorics: https://ac.cs.princeton.edu/home/ reply hintymad 25 minutes agoparentSecond this. This class is a classical example of conceptual blockbuster. Once one learns it, the complexity analysis of algorithms will never be the same again. In general, if a techie wants to spend their spare time learning new stuff, they will be better off focusing more on such conceptual stuff, as the return will compound over the years. reply jldugger 2 hours agoprevI've always wondered why the hell generating functions existed, and I think this line sums it up: > When de Moivre invented much of modern probability in the mid-1700s, he didn’t have vectors! Vectors are an 1800s invention. Doesn't explain why we still teach them 300 years later though. Thats what the second half of the article covers. reply gorgoiler 4 hours agoprev”If we want to encode the vector [6,2,8,4] in a single expression we can create a function containing those numbers: f(x) = 6 + 2x² + 8x³ + 4x⁴ …or if you flip the vector and use x=10: 6284 reply Sharlin 2 hours agoparentYes, but the polynomial form generalizes to coefficients of an arbitrary field, not just naturals. If your vector were, say, [1.3, 2.197656, pi, -1/2, 3*2i] then there wouldn’t be a reasonable base you could pick for a place-value representation. reply AgentMatt 4 hours agoparentprevIndeed. However, note that this is limited to encoding values between 0 and 9. reply derefr 1 hour agoprevIs there a relationship between algebraic polynomial encoding of sequences, and https://en.wikipedia.org/wiki/G%C3%B6del_numbering_for_seque... ? Does an encoding of a sequence in a given Gödel numbering, also somehow \"retrievably\" encode the probability space of the sequence's terms? reply shiandow 6 hours agoprevProbably worth mentioning the moment-generating function as well, since it's a bit more elementary than the characteristic function and shares many of the properties. It's also more simply related to the probability generating function, you can go from one to the other with a basic change of coordinates (t -> log(x)). I also estimate calculating the moment generating function to be easier in most cases. In fact most properties of the PGF come from the monent-generating/characteristic function. Including why the second derivative is related to the variance. The second derivative of the moment generating function is the second moment E[X^2]. The second derivative of the logarithm of the MGF is the variance by definition. The one property that's somewhat unique to the PGF is how composition relates to drawing a randomly-sized sample, which I can see could be useful. reply adiM 5 hours agoparentOne can also think of probability generating functions as (flipped) Z transforms, moment generating functions as (flipped Laplace transforms), and characteristic functions as Fourier transforms of the respective PMF/PDF. Lot of their properties then follow from simple properties of Signals and Systems. reply glial 4 hours agorootparentDo you have a reference that explains this in more detail? I'd be curious to know. reply adiM 2 hours agorootparentDon't have a reference on the top of my head, but the main idea is as follows: The definition of MGF of a random variable with PDF f(x) is E[e^{sX}] = int_{-inf}^{inf} f(x) e^{sx} dx The definition of Laplace Transform of a signal f(t) is F(s) = int _{-inf}^{inf} f(t) e^{-st} dt Hence MGF is 'flipped' Laplace transform Now for we know that the MGF of sum independent RVs is the product of their MGFs. So if we take the inverse Laplace transform, the density of the sum is convolution of the individual densities. Similarly, if we take derivative in frequency domain, that is same as multiplying in time domain: So M'_X(s) is the 'flipped Laplace transform' of x f(x) and its value at s=0 is the 'DC-gain' of the signal. And so on... the properties are all immediate consequence of the definition of MGF and since the definition is essentially the same as that of a Laplace transform , there is an equivalent property in signals and systems as well. reply esafak 2 hours agoprevI rather like the derivation of the CLT using MGFs. https://courses.cs.washington.edu/courses/cse312/20su/files/... reply JPC21 5 hours agoprevGenerating functions are also a great tool in combinatorics (see for example the book Analytic Combinatorics by Flajolet and Sedgewick). reply v9v 5 hours agoprevTangentially related: http://bactra.org/notebooks/cumulants.html reply vector_spaces 5 hours agoprevI think the 12th order G(t) example is missing another term with coefficient 1/5 -- since these coefficients must sum to 1 reply stocknoob 4 hours agoparentUnless it’s already been fixed, the queen’s term (t^12) has 2/5 in the article. reply jampekka 5 hours agoprev [–] Lost the plot at \"... and for sensible v, this is equivalent to ...\" :( reply Fellshard 5 hours agoparentFor that, I would look at early calculus/pre-calc, I think, examining infinite series and their properties and equivalencies. There's certain forms like that that have well known values that they converge to as you continue adding terms into infinity. Sometimes that convergence is only possible if your domain is limited, eg. [0,1]. reply jampekka 5 hours agorootparentThat clears it, thanks! I didn't figure out that \"sensible\" referred to convergent G(t). reply madcaptenor 4 hours agorootparentIt doesn't mean that in general; I think the author is saying \"for sensible v\" somewhat informally to mean \"for v for which this makes sense\". reply foldU 4 hours agorootparentprevYou also don't really have to worry about convergence, since these are formal power series. reply coliveira 3 hours agoparentprev [–] This is just the sum of a geometric sequence. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Probability-generating functions (PGFs) encode sequences of probabilities into a single polynomial, aiding in understanding probability distributions like card draws or coin flips.",
      "PGFs allow for the calculation of expectations and variances through derivatives and can be multiplied to find distributions of sums of independent random variables.",
      "The article also mentions characteristic functions, related to PGFs, which involve complex numbers and have applications in probability theory, though they require advanced analysis for deeper understanding."
    ],
    "commentSummary": [
      "Probability-generating functions are essential in probability theory and are closely related to Fourier transforms, aiding in the analysis of probability distributions.- The characteristic function, a type of Fourier transform, simplifies operations such as convolution, making it a powerful tool in probability theory.- These functions have applications beyond probability, including combinatorics and physics, where they relate to concepts like Feynman diagrams, enhancing understanding in various scientific fields."
    ],
    "points": 143,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1730367378
  },
  {
    "id": 42001852,
    "title": "Wonder Animation – Video to 3D Animation",
    "originLink": "https://adsknews.autodesk.com/en/news/autodesk-launches-wonder-animation-video-to-3d-scene-technology/",
    "originBody": "Introducing Wonder Animation: New AI solution for animated films, powered by cutting-edge Video to 3D Scene technology Back Introducing Wonder Animation: New AI solution for animated films, powered by cutting-edge Video to 3D Scene technology 30 OCT 2024 Categories: Media & Entertainment Film and television Tags: artificial-intelligence postproduction virtual-production visual-effects Nikola Todorovic Share Wonder Dynamics, an Autodesk company, launched the beta version of Wonder Animation, which uses groundbreaking Video to 3D scene technology to accelerate animated film production by turning any video sequence into a 3D-animated scene. Part of the Wonder Studio toolset, Wonder Animation’s Video to 3D scene technology can film and edit sequences with multiple cuts and various shots, then uses AI to reconstruct the scene in a 3D space. Wonder Animation’s beta launch, now available to all Wonder Studio users, aims to bring artists closer to producing fully animated films, while enabling the artist to retain full creative control – unlike other generative AI tools that rely on automated outputs. Wonder Animation, an Autodesk product It’s been five months since we joined Autodesk, and the time spent has only reinforced that the foundational Wonder Dynamics vision aligns perfectly with Autodesk’s longstanding commitment to advancing the Media & Entertainment industry through innovation. Together, we believe in using artificial intelligence (AI) more intentionally to enhance creativity and efficiency, so artists can spend more time on the creative aspects of storytelling. We formed Wonder Dynamics and developed Wonder Studio (our cloud-based 3D animation and VFX solution) out of our passion for storytelling coupled with our commitment to make VFX work accessible to more creators and filmmakers. Today, Wonder Dynamics is excited to announce the beta launch of Wonder Studio’s newest feature: Wonder Animation, which is powered by groundbreaking Video to 3D scene technology that enables artists to shoot a scene with any camera, in any location, and turn the sequence into an animated scene with CG characters in a 3D environment. The most innovative part of Wonder Animation’s Video to 3D scene technology is its ability to film and edit sequences with multiple cuts and various shots (wide, medium, close-ups). The technology then uses AI to reconstruct the scene in a 3D space and matches the position and movement of each camera’s relationship to the characters and environment. This essentially creates a virtual representation of an artist’s live-action scene containing all camera setups and character body and face animation in one 3D scene, with fully editable elements (animation, character, environment, lighting, and camera tracking data) in their preferred software, such as Maya, Blender or Unreal. Wonder Animation Video to 3D scene technology Even though there have been tremendous advancements in AI, there is a current misconception that AI is a one click solution–but we know that’s not the case. Wonder Animation underscores our focus on bringing the artist one step closer to producing fully animated films while ensuring they retain full creative control. Unlike the black-box approach of most generative AI tools currently on the market, we’re empowering artists to shape their vision instead of just relying on automated outputs. Try Wonder Animation today: appWonder Dynamics For more information, visit Wonder Dynamics, an Autodesk company Most read Introducing Wonder Animation: New AI solution for animated films, powered by cutting-edge Video to 3D Scene technology Autodesk doubles down on the future of BIM with new data connections and AI capabilities Autodesk AI strengthens Fusion and Alias, increasing design and make productivity, efficiency, and inspiration Related content Impact 15 Oct 2024 Celebrating innovation, achievement, and sustainability: Autodesk Design & Make Awards 2024 Autodesk Design & Make Awards 2024 Lori Telles Media & Entertainment 15 Oct 2024 Autodesk Media & Entertainment brings new capabilities to enhance creativity and efficiency AI workflows coming to core creative tools to help automate tedious tasks. Diana Colella Media & Entertainment 30 Jul 2024 Autodesk announces AI workflows for artists and producers at SIGGRAPH 2024 Autodesk returns to SIGGRAPH, unveiling the latest advances in generative AI and cloud-enabled… Autodesk PR",
    "commentLink": "https://news.ycombinator.com/item?id=42001852",
    "commentBody": "Wonder Animation – Video to 3D Animation (autodesk.com)136 points by bx376 19 hours agohidepastfavorite14 comments divan 18 hours agoI'm using Wonder Dynamics for slightly different purposes (capturing and analyzing complex sports movements from video) and I'm deeply impressed by what it is capable of doing. While it still struggles with what state-of-the-art pose estimation and camera motion estimation models are struggling, the whole package and implementation are just insanely impressive. From the web UI that is incredibly fast even when uploading 4K@120fps footage to the final result - which is a Blender file and clean slate video for me. Extremely easy to use. A lot of love and care is put into this product. Wonder Animation seems to be just a specific use case improvement over already impressive capabilities. Normal \"Live Action\" projects can also detect cuts, but the \"Animation\" project seems to understand the space from multiple cuts/angles. reply sech8420 16 hours agoprevI'm a bit confused. While the demo looks amazing, I feel it is quite misleading along with some of the wording they use. Is is actually creating the 3d environment and character models or are these premade, and instead, its handling solely character rigging and camera tracking? reply imaginationra 15 hours agoparentYou have to provide rigged 3d character models yourself(or use their premade ones)- it does camera tracking + motion matching or whatever algo/ai fun to track the biped animation- so yeah you feed it a video and the 3d models and it spits out either a video of the composite or you can download the 3d scene for further use/massaging in other applications. btw Animation filmmaker here- tested a previous version- it was a janky toy that wasn't useful to me, checked out the new stuff today but didn't get to testing it after reading through the several pages of limitations on camera work, composition etc that can be used in it. I don't want my cinematography/blocking constrained. Nice site design tho(shrug) reply sech8420 15 hours agorootparentI appreciate this information. Saves me some time. Thanks reply Joel_Mckay 5 hours agorootparentWe've looked at a number of FOSS and Commercial options for a project recently, and found most options were not much better than https://freemocap.org/ with video occlusions. However, we did purchase the https://faceit-doc.readthedocs.io/en/latest/mocap_general/ commercial seats for Blender, and have found it workable with the results from the iPhone 3D camera App compared to other options (52 marker lip sync, gaze, and blink cycles will still need cleaned up to look less glitched in complex lighting.) Combined with Auto-Rig Pro in Blender, it is fairly trivial re-targeting for Unreal Engine with volumetric preserving rigs (can avoid secondary transforms, so elbows don't fold in weird ways by default like Makehuman rigged assets.) Best of luck, we concluded after dropping/donating a few grand into several dozen addon projects... there were still quite a few version rotted or broken add-ons for Blender around that people had zero interest in maintaining (some already made redundant by FOSS work etc.) However, there were also a few tools that were surprisingly spectacular... will still likely need to run both 3.6.x and 4.x ... YMMV =3 reply _davide_ 11 hours agoprevAnyone surprised that Autodesk is citing blender in a non-negative light? reply pavlov 9 hours agoparentIt’s like Microsoft embracing Linux. They’ll come around to it slowly, and in five years there will be an Autodesk-branded Blender. reply doctorpangloss 2 hours agoprevIt's 2044. You wake up at 8am as a freelance VFX artist in the slums of Miami Islands. You pay 3.0 dogecoin to activate your Internet connectivity and refresh your ACACV degree. There's a 12pm deadline for the 4D VR animations in the New New Yorker's October 18th anniversary celebration of Jeffrey Toobin's Zoom call. An update to Wonder Animation 35 is released, saying \"New: Retargeting for testicles.\" Rejoice! Your brain sheath rewards you with a lower dose of GLP-1 agonists. You spend the time you saved to earn points to claim an airdrop by watching a withered Elon Musk rant about electric boats. reply vivzkestrel 15 hours agoprevLooks really good, what are some use cases you have in mind outside the movie / animated film industry? reply bsenftner 6 hours agoprevWelp, looks like one of the entry level jobs in VFX is now fully automated: the role was called \"Trackers\" and they did camera motion recovery, set/prop placement recovery, and actor match moving, including facial performances (sometimes, that's had a lot of automation attention for years.) reply dagmx 4 hours agoparentThe quality of wonder dynamics is still significantly far away from handling more complex real world shots, or more fine grained movement. It’ll certainly help but the death of manual tracking is greatly exaggerated. reply bsenftner 2 hours agorootparentThat's good to know. The end of this month, today, marks 20 years since I worked as a tracker. reply BoNour 12 hours agoprevlowkey if the tech keeps up with the demo, huge W for indie movie makers, or amateurs, used their tech last year and was surprised by how good it worked reply Abecid 16 hours agoprev [–] kudos reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Wonder Dynamics, an Autodesk company, has introduced the beta version of Wonder Animation, an AI tool designed for animated films.- The tool utilizes Video to 3D Scene technology, converting video sequences into 3D-animated scenes while allowing artists to retain creative control.- Wonder Animation supports integration with software such as Maya, Blender, and Unreal, aiming to democratize visual effects (VFX) work for creators."
    ],
    "commentSummary": [
      "Wonder Animation by Autodesk is a tool that converts video to 3D animation, requiring pre-rigged 3D models.",
      "Users appreciate its user-friendly interface and speed, but note limitations in camera work and composition.",
      "While it automates tasks like camera tracking, it is not yet advanced enough to replace manual tracking in complex scenarios, making it useful but not fully reliable for indie filmmakers."
    ],
    "points": 136,
    "commentCount": 14,
    "retryCount": 0,
    "time": 1730331865
  },
  {
    "id": 42005635,
    "title": "Moving to a World Beyond \"p < 0.05\" (2019)",
    "originLink": "https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.tandfonline.com\",cType: 'managed',cRay: '8db5e5eabe6082d8',cH: 'T2kKm4M_IK1N1fD63Xr8_jieAoQUwvBBc.2vLoh7ls4-1730401332-1.2.1.1-oZVDRkFp4WjKcM.D.X1SgrsC427413Z.zbjKTZRyjwNj4t55.tduJKLIM8qqg8dF',cUPMDTk: \"\\/doi\\/full\\/10.1080\\/00031305.2019.1583913?__cf_chl_tk=BfHDS2hkk94UlSGkmy9cIIszlMNeiJ3gKEOOx0sMeDo-1730401332-1.0.1.1-Cp3ydasEJgyS1oQkHpar5DsfpD0SsPRnSXbHuYvH7Ak\",cFPWv: 'b',cITimeS: '1730401332',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/doi\\/full\\/10.1080\\/00031305.2019.1583913?__cf_chl_f_tk=BfHDS2hkk94UlSGkmy9cIIszlMNeiJ3gKEOOx0sMeDo-1730401332-1.0.1.1-Cp3ydasEJgyS1oQkHpar5DsfpD0SsPRnSXbHuYvH7Ak\",md: \"_vGmbWAwOkjevc8Rq3WZF105VEXhskdhTFjrerSGhIE-1730401332-1.2.1.1-zaHZABZvpSaAx15g2ZjLRw4uie17nh2n6MV8oW1UDCLYHaKIVyuB9oPjVG6x7zQ4pDwUA8VnzFk6F5VKHbUjKmw5Avb_JktdIxfE8p2sLbuWNsjxp72ihvFbBeDgJLqhCfa3sGybCN0FZPn8EcggMCKm3B2r0_oNtBPFWSN8o7W0zI6jpLFAjwlvSHpd7MOU3CswYRZP1fWeGn.jFsvgeLHDb8PCib.QNDyd3jzguks8gTmXx918lNpNdTtelBV3YP59tkI8a2LqMxjI.AeMjlEbzkyGaM6MzgGIY2n0nnDMSREpBLupXWg2FaJHhDP7sgAGoR4DPEjxfIZK_WPgka59mz9u13jsx_scGZVC.cPxsl4wVXCQJpteDBoHKja1Gii9zdZyKFN6atd9djiRg_Q_frr4nzL6XTH9YQmLnEMGItmAkoULfsiOYk3lu.NoWO1E.1m4dUxc4VysPbnCeIsQqv_YJItycVKfWVrA3u0LLdMf5V9H_zfUuXCJCm_4lRdxwARwYfdh06jgcZ.DqoyrBVEctDK8.4cyCLsTRduSo_FsiGmBgiDXXPHwC92uZKwvfWo8ZlMacVDLtT8ZrIwdltE1gGANl6ABpBVPcVdRU2bJy0twoKB56MZsHb6s2exi6r9uMaPJminsn1SS85Nso1zfslW4EtItgXaelLaxTGD9yAsUyVjW3ACqUUE8b_opyC1R3PRBH85n7gnF0Fzmo7NqSR84.P8pzbpFq0G1yjM5zUWEjvG23YQJFOvHpv0WI9GWg_2uEyh8PnfYxiYNVZiizfprv1GwPcY3iD4Nbq7psWttBk4TS5DW8wsIRjHGXI6eNcFwbnq1RDWgzmDVuEBmwA213gVuiJHap0ea6hs1WWyy2aQEbBiAxwyVoVNHVao5B5rgzmcDuNlGvWNsfKrdETBx582CZB27lyrJjMFIqv4ShrPL_0xF4OcwEW7h8rViR6pKai_DgFa3MWTDJfg1PkqrCFf0wdJcTBOTn4vgy6uPoXAFK7PQNn6QHK6uDuwwfnIvc65GwyNpXAWZPZilQXAnUvNVULQbuvoj_v_WFiyXomCleUFUQsZwKWM.VRx75_k7Z4CO3nZUFBGeDBWzjb6tMiBwvo11lIE_1TUBkVR5qayENS3AoUPsJ4lZR4A_E_.iHGu3CsuCGgdl94OFpv8EneoFziJwTKiekn4NAHLVGinmcGf.4XCmNgf5F32OVlhfloOnpt.3WCh9fbNToVtZBMjw3zkUQEgHfOJ6HQqI2OrfSK0YJAoR3myrnHHqBzggwtgOYMpSBL.uvA_y_ABYXGsIuCcQ.EVsz5DBsc0BlTiQURCm5ozF0jYE7wDkMLQjelFt_SMelutfWUVXq9RBJyEcIoD3qjR7bxF6te5XQGwi0OViYvEAzYii4a.MB5Dg.4bjQd5ILeq363gFOGIRDC2LitLqmXSX2aqNm79DmnwoXgJlTpMuMRjAS_uqgoFYCaZ5Bqtp7q6PJqG4qY.YaduwfG218KPbEVO1ezOQmr503oVq454ch7Wyo9.tSEcNduzH5jbKHzwmbhwwY8uGDGVWWEiaD_w4.E3wrZMy_xE39r8D0H8wpBrzS5yqcX71m1zgZqM6fXW9vMcVaaBV1V_Chwk521rH4px88D9PNFttV3_oXQCCZaRdPw_BBjT1mY7O9Ri4oxJFI7j2GiSQBlr3Q7NUBfZAD.gsxLin6i9rR9SdZFMbu3qPkBTjeJ87IkKwAr_2KZiDPhvULyqXFa8Ss6nUY_hMQah66RVaCrouaSaBoa7LpVQI4ZboElUt.Xiw_5puALX4BUCfNOFndHdOimWX4iZEq8sS5oGovTyNyav0NHoAyjZPLxQlLnjrz1XWnO3MdQWamSKmL7w4sP8Ht.qNXqYcQQ5YipvIC1oNXddV10BzfOeCpfiJbUifvpa8aFCNbutmnyfIH_SYzXZYJzQFVRfS5VRlFe86Uo_ZA3b6Sgsad.ZprMp2CgHK7BlZvadtY0VVn6XyBF_vs83BqOn92eWsxNie3zTzi36NRHegvRJV5Ns1M73MEuG5gXrvuFR1GPvA.DYWCuN9Pkkhpjq6r9lffZNwW5OxUxB0_fbQu7YyF9gox0VM0wAzwCVtG8gFvAWJ2f6SYA9mUE9i5nMKIRvur4Yiw12czkclDjZZE_lt7FwG0u9UhMm.l72ioKfdGwJhHDfxOWqdW3gKFDaN1MBMnoZOBxPRzLZjTC34w4jdSGwSpTMdAIIxliUi.gcqPZXTy0lnmZRtmLfMDIw1G83TJIpW3BiaQnQufv7.WarpdQWl9RTrLPKw1919mHMaMPaa1ggFocC4VKva5QSwPLDLYsYEKtvd3lUHRjlznAIEEI.Es8QV5hS0SPCkik7VpXA_kmAh10.xn7Mrg0TzozVo_lPOPkmT67NPa3bt7Hqv5IMi4sFmkU4gheZ6Mt9FdfIzSB0T3v.y_vI8xC2L1QuNWYb7HVWyAURcaBXJ.1KgKW0HYt5thkw3Iyr.qoghrAdcg.QoaAfke0.gjKV7pYvXrYbt4qmMScHUoq5uUgLjETKJ9fxDZT2KvRSQH4XrXKYAxAXt2FOlEt7dNZw6_4hTfJd7e5cY5kopxc9iJofG\",mdrd: \"o8TSpAFVW0gGtoZQlUdmWFudo1qpZx8K8A4tMi3WERw-1730401332-1.2.1.1-lcxD8fndnq_TZKzoa4jCvzyDPMXTcYHeyNeVD0uoCS4_Y_C41aba8zqRXWrvbidanSAx9L4F.pV9fQkLRzDABfq.zlp1HEYtZZZcByP09BaAgYhPhupkOSNPbnkvpsvnYvBGx26qPfE4.SyAHwn259QEdffEIIa5BfB2TBgI7jEiX4ZmIe3nlBj5LM4U_WQzbqk1TAz5VVwKv6c4x3d7tL_XdWaLpYVZTQnrbgNlhGs1T6eHRqHQW0pyMvMxLd8o9h5Yx4fgNTiZ9M3s6DpSb44wQiEH606foS74k2ix0cKS9Zqr5OyNGuyGdAThyeOT53AXwmwIOmE6cYd4gjBV5zptjZZuStIq_pnESKfyoGHkMH4MnatF2N6iB6Y48VxBlbhpZyTQZsAsnO2uI5iVrrFg4YVQCDEvhUh_SQ8xl_A89rPJwa0miJJd3h2.cHsoxjZkkp5C.LdOqIc23KRyRdw69_Zx2wwh8dNLcbxeo5GrZsUtjKJ6uM2.cvD3k.VP1e5Tr7MPi0UAfoXE1jlC3oQNsVkjOUu_kGp4j9h4EhKBUCaPlOxqTEnMwQ0HJs87788QwBf8ot6UI_HEr.kU_c2N02r3p1zdVNbUCJQLPkQdaAIDvo05s5F24X43NNpdeDgkeVlrfWnn.wQpmEDziUirnv.5idmSp5pGe.Cc2mf5Og5.kwULVb2A9s1AGOmpe9GI4U.ySPD8PDJKcE4D9hj0hHeX1eUCzHE_pXhr9d99OfW_Upa.HjiDP_3lshNcLsRyX8GyhG0j_zE7_bfJX8ZKR9ca.3unuDCQ55T0mnT9z1wjz4R6qRdPatIop1azzaVq5xLOhdcC6PqUEy4FTSSR8faNRpAdDKzkxcSqxGMlPsNad_.cOiNlOjI7u86bZ2aULYSKv5LoNRLsUrq17cX18whUD4cmBuFyrBA6wuW4MLjzd5CGQyBEyvOmjmbUnAkDAugPeqNHJNhpw2wZQQxQ2iwGXSVfuS2A9g7Nust5xEhFa7XP8fqb3Kxall97vKStQsjPRfq92J.PX6kU4tUn0Z584st1omPWpm7QL3Dyb.G1OxB0Qx9bt50.qycPHCRdmAE_V2oIPZDyxu2eEuuhxk8.UbCJtWS_YF7QgtsTTDqklLBF4uV7qxNhEpEZ6_.Yu.hJeg5eFnGcxSVH2w5t_X4OYI5c19T5Lk3ncIqGoS7Ep2B4uLa7yR_nEHxZAuXMI_9u1Ido8ptSDuXTrRNjydDFGahCX9bBFN7rw4rBlAOsurGyH.nNxChYPHUXKlVYRMLIlUL8_MJHB2zcsw7PXkPi9G_4m6thN4I_KE9IlAupjjzefQGdf1zHKaLlo4gBl_wv.mXYYqQJ6EmffZbtnp6qE_ond6Npk3GimrdG._l8cMTeH3zOeVIFmOHfZItNl7zxfajvaYiVlJz5sKzpfURQWvz11xG20C0knfCFA1LjoGYDPXmitbPZ1ygpr4L3pBc8FWv4a8q61m7I_.Plqd_hGpFnbfk7Tm3vFsSH9P74F21_NYPGacklH4c5yR5MJ5dLzRLMMkDQfzP7C8YxRzTq4pavVyNV4nAzCQYUUkCwMEq9US1XNHuho7hhDa_gpajOdq0zFtdMLkAMhdKXxed98PeriD4Odycjq3C.LIT9Mc7TCAtfkJUc3Xr9SsJnKapGmahwxnsRDwsRD1MXHe6FHkgncyR9COnbWmJtQdSePvs3tHCD4GTgSdlR2_nOC7GZxDexNQvw0PNJFKUuiq9L6utvNGu0iyc7uDtLpBYjfgUA6SscrKUmMMCXTPEsRSVgBbdgIAIQmTFPfpOqzm4CoR9S0oCZrE7MX8NEV7pj_e28OTbgOfpToQOMCZNCHQzyL3SfQEWvZbcNO8KnG9UV5FyvqCODWlXTyiRwQClLczSoNI_1gJvpjf3M1iMgj.0ST4y6hdotg1YRU4UkUpkctp6SVkz_K3BTeQfXHBJvqHZZyLXrpRxvLQQ1voj0iZa5ffCjcaKUOWNG3U.PSJMCbtKZoV6Rt3x5rX9ZUvvLudSGUqTN4gVdBiOgwkRvXTEh2YKtx2IFSuMqaFtUhHHKg6rp0rT04_FZ9RdaeEqqovsfvDU212Udsl2M.WNRnz7q5sXlQcumJb7JZLMsCKRHllf6nH6kAWkB23u0gBRO98LJeP2N.K9Dq2vN9AlptMrckO1lFQoZbUw.Ki37RzQlHBvLxBDnWfXkGadIdfBaa8hlzrQWkrM0BPiGF.ALWTQ.MAhWdd0ERjhbTZ_1SqharrydNBJ15hCJ57WWWdY9k_ByzYagz2AsYQhGv4HLuUbe.OoZHoVRBej285pheZgm8k5x8bCTRiYy5zINFQoaHXtcbTsp4iG0ylfeXEsIBkNQDhWfNNPVpYV50S3iDj2l4HKEiIk99G6HUzJxj.kH.3DMf7tXD7MtRPob\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8db5e5eabe6082d8';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/doi\\/full\\/10.1080\\/00031305.2019.1583913?__cf_chl_rt_tk=BfHDS2hkk94UlSGkmy9cIIszlMNeiJ3gKEOOx0sMeDo-1730401332-1.0.1.1-Cp3ydasEJgyS1oQkHpar5DsfpD0SsPRnSXbHuYvH7Ak\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=42005635",
    "commentBody": "Moving to a World Beyond \"p\"I've become increasingly convinced that the idea of averaging is one of the biggest obstacles to understanding things... it contains the insidious trap of feeling/sounding \"rigorous\" and \"quantitative\" while making huge assumptions that are extremely inappropriate for most real world situations.\" Semi-related, I found UniverseHacker's take[0] on the myth of averaging apt in regards to leaning to heavily on sample means. Moving beyond p-values and inter/intra group averages, there's fortunately a world of JASP[1]. [0]: https://news.ycombinator.com/item?id=41631448 [1]: https://jasp-stats.org/2023/05/30/jasp-0-17-2-blog/ reply curiousllama 3 hours agoprevThe core problem with this is: - DON'T is very clear and specific. Don't say \"Stat-Sig\", don't conclude causal effect, don't conclude anything based on p>0.05. - DO is very vague and unclear. Do be thoughtful, do accept uncertainty, do consider all relevant information. Obviously, thoughtful consideration of all available information is ideal. But until I get another heuristic for \"should I dig into this more?\" - I'm just gonna live with my 5-10% FPR, thank you very much. reply truculent 56 minutes agoparent> But until I get another heuristic for \"should I dig into this more?\" Why do you need a heuristic? In what areas are you doing research where you don't have any other intuition or domain knowledge to draw on? And if you don't have that background, contextual knowledge, are you the right person to be doing the work? Are you asking the right questions? reply joe_the_user 1 hour agoparentprevYeah, it seems like those bullet point have the problem that they don't really contain actionable information. Here's the way I'd put things - correlation by itself does causation at all. You need correlation plus a plausible model of the world to have a chance. Now science, at its best, involves building up these plausible models, so a scientist creates an extra little piece of the puzzle and has to be careful also the piece is a plausible fit. The problem you hit is that the ruthless sink-or-swim atmosphere, previous bad science and fields that have little merit make it easy to be in the \"just correlation\" category. And whether you're doing a p test or something else doesn't matter. A way to put is that a scientist has to care about the truth in order to put together all the pieces of models and data in their field. So the problem is ultimately institutional. reply aquafox 5 hours agoprevI think the main issue with \"p = 0.05 is the reason p-values aren't that useful. This is how you get the replication crisis in psychology. The p-value cutoff of 0.05 just means \"an effect this large, or larger, should happen by chance 1 time out of 20\". So if 19 failed experiments don't publish and the 1 successful one does, all you've got are spurious results. But you have no way to know that, because you don't see the 19 failed experiments. This is the unresolved methodological problem in empirical science that deal with weak effects. reply ChadNauseam 3 hours agorootparentprevPublishing only significant results is a terrible idea in the first place. Publishing should be based on how interesting the design of the experiment was, not how interesting the result was. reply bjourne 1 hour agorootparentP-value doesn't measure interestingness. If p>0.05 there was no result at all. reply thaumasiotes 1 hour agorootparentBoth of those statements are false. Everything has a result. And the p-value is very literally a quantified measure of how interesting a result was. That's the only thing it purports to measure. \"Woman gives birth to fish\" is interesting because it has a p-value of zero: under the null hypothesis (\"no supernatural effects\"), a woman can never give birth to a fish. reply tossandthrow 4 hours agorootparentprevResearch is an institution. Just qualify the uncertainty and describe your further work to investigate. reply johnobrien1010 4 hours agorootparentIn THEORY yes, but in practice, there are not a ton of journals I think that will actually publish well done research that does not come to some interesting conclusion and find some p The 0.05 threshold is indeed arbitrary, but the scientific method is sound. Agreed. A single published paper is not science, a tree data structure of published papers that all build off of each other is science. reply schmidtleonard 4 hours agorootparentRight, the decisions made by future researchers about what to base their work on are the real evaluation, hence citation counts as a core metric. It's easy to claim positive results by making your null hypothesis dogshit (and choice of \"p\" is easily the least inspired way to sabotage a null hypothesis), but researchers learn this game early and tend to not gamble their time following papers where they suspect this is what's going on. The whole thing kinda works, in a world where the alternatives don't work at all. reply greentxt 4 hours agorootparentprevSounds good but is that true? A single unreplicated paper could be science couldn't it? Science is a framework within which there are many things, including theories, mistakes, false negatives, replication failures, etc... Science progresses due to quantity more than quality, it is brute force in some sense that way, but it is more a journey than a destination. You \"do\" science moreso than you \"have\" science. reply bunderbunder 3 hours agorootparentA single brick on the ground, all by itself, is not a wall. But if you take a lot of bricks and arrange them appropriately, then every single one of those bricks is wall. In other words, just like the article points out down in the \"dos\" section, it depends on how you're treating that single unreplicated paper. Are you cherry-picking it, looking at it in isolation, and treating it as if it were definitive all by itself? Or are you considering it within a broader context of prior and related work, and thinking carefully about the strengths, limitations, and possible lacunae of the work it represents? reply bluGill 3 hours agorootparentprevOnly scientists care about doing science. Most people are not scientists. Even scientists are not scientists in every field. We as the genereral population (including scientists in a different field) however care about science because of the results. The results of science is modern health care, engineering (bridges that don't collapse...), and many other such things that we get because we \"have\" science. reply jhbadger 3 hours agorootparentprevI think you and the OP are agreeing with each other. The issue with a \"single unreplicated paper\" is exactly the issue you bring up with science as a journey. It's possible that this paper has found a genuine finding or that it is nonsense (people can find isolated published papers supporting almost anything they want even if they don't reflect the scientific consensus), but if no other researchers are even bothering to replicate the findings in it it hasn't joined the journey. reply refurb 3 hours agorootparentprevPrecisely. As a scientist, that’s how it works. If a new paper with an outrageous claim pops up, people are automatically suspicious. Until it’s been reproduced by a few labs, it’s just “interesting”. Then once it’s been validated and new science is built off of it, it’s not really accepted as foundational. reply zmgsabst 4 hours agorootparentprevOnly if there isn’t systemic compromise via funding allocation, eg, what happened with Alzheimers research. reply UniverseHacker 2 hours agoparentprevAs a scientist, I don’t think there is any specific scientific method or protocol - other than something really general like “think of all the ways people have been deceived in the past and carefully avoid them.” Almost no modern research follows anything like the “scientific method” I was taught in public school. The way I do research is roughly Bayesian- I try to see what the aggregate of published experiments, anecdotes, intuition, etc. suggests are likely explanations for a phenomenon. Then I try to identify what realistic experiment is likely to provide the most evidence distinguishing between the top possibilities. There are usually many theories or hypotheses in play, and none are ever formally confirmed or rejected- only seen as more or less likely in the light of new evidence. reply hn_acker 4 hours agoparentprev> A good researcher describes their study, shows their data and lays their own conclusions. Tangent: I think that this attitude of scientific study can be applied to journalism to create a mode of articles between \"neutral\" reports and editorials. In the in-between mode, journalists can and should present their evidence without sharing their own conclusions, and then they should present their first-order conclusions (e.g. what the author personally thinks that this data says about reality) in the same article even if their conclusions are opinionated, but should restrain from second-order opinions (e.g. about what the audience should feel or do). reply riskable 5 hours agoparentprev...but making conclusions is how you get funding! reply chimeracoder 5 hours agoparentprev> The 0.05 threshold is indeed arbitrary, but the scientific method is sound. I guess it depends on what you're referring to as the \"scientific method. As the article indicates, a whole lot of uses of p-values in the field - including in many scientific papers - actually invoke statistics in invalid or fallacious ways. reply pif 3 hours agorootparent> I guess it depends on what you're referring to as the \"scientific method No quotes needed, scientific method is well defined: https://en.wikipedia.org/wiki/Scientific_method reply bongodongobob 5 hours agorootparentprevThe scientific method is sound != every experiment that claims to use the scientific method is sound reply chimeracoder 4 hours agorootparent> The scientific method is sound != every experiment that claims to use the scientific method is sound Sure, which is why I asked OP to define what they meant by \"scientific method\". The statement doesn't mean a whole lot if we're defining \"scientific method\" in a way that excludes 99% of scientific work that's actually produced. reply devit 3 hours agoprevSeems to me a more reasonable threshold would be at most 10^-9 probability of the conclusion being false (in the Bayesian sense of course), with the prior chosen by the editor and reviewers (and the models being selected upon also being agreeable to them). reply kidel001 4 hours agoprevPeople need tools to filter results. Using a somewhat arbitrary cutoff for what to work with is actually fine because people need to make decisions. Further, papers that report false positives do not tend to lead to huge branches of successful science because over time the findings do not replicate. But I am curious about something else. I am not a statistical mechanics person, but my understanding of information theory is that something actually refined emerges with a threshold (assuming it operates on SOME real signal) and the energy required to provide that threshold is important to allow \"lower entropy\" systems to emerge. Isn't this the whole principle behind Maxwell's Demon? That if you could open a little door between two equal temperature gas canisters you could perfectly separate the faster and slower gas molecules and paradoxically increase the temperature difference? But to only open the door for fast molecules (thresholding them) the little door would require energy (so it is no free lunch)? And that effectively acts as a threshold on the continuous distributions? I guess what I am asking is that isn't there a fundamental importance to thresholds in generating information? Isn't that how neurons work? Isn't that how AI models work? reply FollowingTheDao 6 hours agoprevAs someone who has studied genetics on my own for the last twenty years I am very glad to read this editorial. For example, take a population of 100 people, and let us say one of them has gene changes in their Fatty Acid Desaturase genes (FADS1 an d FADS2) that change how important Long Chain Omega 3 Fatty Acids (like from fish) are for them. This happens more often in people from indigenous arctic populations. https://www.sciencedirect.com/science/article/pii/S000291652... So the researcher tests if omega 3 effects cardiovascular outcome in these hundred people by adding a lot more fish oil to the diet of these 100 people. Since only one of them really needs it, the P value will be insignificant and everyone will say fish oil does nothing. Yet for that one person it was literally everything. This is talked about only quietly in research, but I think the wider population needs to understand this to know how useless pethnicity is not biologically realized either What does this mean? Is it contrary to what OP is saying above? reply iterance 5 hours agorootparentNot exactly. What I mean to say is this: We know there are certain phenotypes that predominantly appear in certain populations, in broad strokes. But while we do have lists of correlates, we don't have good definitions for what an \"ethnicity\" is biologically, and there is very good reason to believe no satisfactory definition exists. To use OP's example, we know that the gene mentioned is frequently found in the Inuit population. But if an Inuk does not have that gene, it does not somehow make them less Inuit. We can't quantify percentage Inuitness, and doing so is logically unsound. This is because the term \"Inuit\" doesn't mean its biological correlates. It simply has biological correlates. To use an example of a personal friend, slightly anonymized: My friend is an Ashkenazi Jew. There is absolutely no uncertainty about this; Jewishness is matrilineal, and their mother was an Ashkenazi Jew, and her mother before her, going back over eight documented generations of family history. But alas - their grandfather was infertile, a fact that was posthumously revealed. Their maternal grandmother had a sperm donor. The sperm donor was not an Ashkenazi Jew. Consequently, can said friend be said to be \"only 75% Jewish,\" having missed the \"necessary\" genetic correlates? Of course not. By simple matrilineage they are fully an Ashkenazi Jew. Why are these terms used in medicine, then? Because, put simply, it's the best we can do. Genetic profiling is a useful tool under some limited circumstances, and asking medical subjects their ethnicity is often useful in determining medical correlates. But there is nothing in the gene that says \"I am Inuk, I am Ashkenazi,\" because these ideas are social first, not genetic first. reply BoingBoomTschak 1 hour agorootparentY-DNA and mtDNA haplogroup are good definitions though. They just don't map exactly to vernacular concepts. reply burtness 1 hour agorootparentIsn't the problem that the vernacular concepts are what counts and they change depending on time and place? reply mistermann 2 hours agorootparentprevI don't disagree with this, but this is very not consistent with \"ethnicity is not biologically realized\", which suffers from the same logical error but in the other direction. I often wonder how many entrenched culture battles could be ~resolved (at least objectively) by fixing people's cognitive variable types. reply riskable 5 hours agoparentprevAs a layman who doesn't work with medical studies it always struck me that one of the bits of data that isn't (normally) collected along with everything else is genetic samples of all participants. It should be stored alongside everything else so that if the day comes when genetic testing becomes cheap enough it can be used to provide vastly greater insight into the study's results. Even something as simple as a few strands of hair sealed in a plastic bag in a filing cabinet somewhere would be better than nothing at all. reply nosianu 3 hours agorootparentThat throws out anonymity. I don't see this getting approved, or people signing up for such studies, apart from those who don't care that there genetic data gets collected and stored. Even if there is no name saved with the genetic sample, the bar for identification is low. The genes are even more identifying than a name after all. Worse, it contains deep information about the person. reply jerf 5 hours agoparentprevI was trawling studies for some issues of my own and sort of independently discovered this many years ago. It's very easy for an intervention to be life saving for 5%, pretty good for 10%, neutral for %84, and to have some horrible effect for %1, and that tends to average out to some combination of \"not much effect\", \"not statistically significant\", and depending on that 1% possible \"dangerous to everyone\". (Although with the way studies are run, there's a certain baseline of \"it's super dangerous\" you should expect because studies tend to run on the assumption that everything bad that happened during them was the study's fault, even though that's obvious not true. With small sample sizes this can not be effectively \"controlled away\".) We need some measure that can capture this outcome and not just neuter it away, because I also found there were multiple interventions that would have this pattern out outcome. Yet they would all be individually averaged away and the \"official science consensus\" was basically \"yup, none of these treatments 'work'\", resulting in what could be a quite effective treatment plan for some percentage of the population being essentially defeated in detail [1]. What do you mean? They all \"work\". None of them work for everyone, but that doesn't mean they don't work at all. As the case I was looking at revolved around nutritional deficiencies (brought on by celiac in my case) and their effects on the heart, it is also the case that the downside of the 4 separate interventions if it was wrong was basically nil, as were the costs. What about trying a simple nutritional supplement before we slam someone on beta blockers or some other heavy-duty pharmaceutical? I'm not against the latter on principle or anything, but if there's something simpler that has effectively no downsides (or very, very well-known ones in the cases of things like vitamin K or iron), let's try those first. I think we've lost a great deal more to this weakness in the \"official\" scientific study methodology than anyone realizes. On the one hand, p-hacking allows us to \"see\" things where they don't exist and on the other this massive, massive overuse of \"averaging\" allows us to blur away real, useful effects if they are only massively helpful for some people but not everybody. [1]: https://en.wikipedia.org/wiki/Defeat_in_detail reply dragonwriter 3 hours agoparentprev> So the researcher tests if omega 3 effects cardiovascular outcome in these hundred people by adding a lot more fish oil to the diet of these 100 people. Since only one of them really needs it, the P value will be insignificant and everyone will say fish oil does nothing. Yet for that one person it was literally everything. But... that's not a problem with the use of the p-value, because that's (quite probably) a correct conclusion about the target (unrestricted) population addressed by the study as a whole. That's a problem with not publishing complete observations, or not reading beyond headline conclusions to come up with future research avenues. That effects which are not significant in a broad population may be significant in a narrow subset (and vice versa) are well-known truths (they are the opposites of the fallacies of division and composition, respectively.) reply tgv 6 hours agoparentprevLast I heard, 5 sigma was the standard for genetic studies now. p As soon as you include these crucial moderating covariates Yes, but this is not usually done. reply fermisea 6 hours agorootparentAnd it's usually discouraged by regulators because it can lead to p-hacking. I.e., with a good enough choice of control I can get anything down to 5% The fundamental problem is the lack of embrace of causal inference techniques - i.e., the choice of covariates/confounders is on itself a scientific problem that needs to be handled with love reply uniqueuid 6 hours agorootparentprevIt is also not easy if you have many potential covariates! Because statistically, you want a complete (explaining all effects) but parsimonious (using as few predictors as possible) model. Yet you by definition don‘t know the true underlying causal structure. So one needs to guess which covariates are useful. There are also no statistical tools that can, given your data, explain whether the model sufficiently explains the causal phenomenon, because statistics cannot tell you about potentially missing confounders. A cool, interesting, horrible problem to have :) reply esafak 2 hours agoparentprevThat gets into Simpson's paradox; subpopulations can react differently than the whole. https://en.wikipedia.org/wiki/Simpson%27s_paradox reply tomrod 6 hours agoparentprevFrom a methods perspective, wouldn't this be more of a statistical power issue (too small of sample size) than a random effect issue? Granted, we do a terrible job discussing statistical power. reply zukzuk 6 hours agorootparentWatching from the sidelines, I’ve always wondered why everything in the life sciences seems to assume unimodal distributions (that is, typically a normal bell curve). Multimodal distributions are everywhere, and we are losing key insights by ignoring this. A classic example is the difference in response between men and women to a novel pharmaceutical. It’s certainly not the case that scientists are not aware of this fact, but there seems to be a strong bias to arrange studies to fit into normal distributions by, for example, being selective about the sample population (test only on men, to avoid complicating variables). That makes pragmatic sense, but I wonder if it perpetuates an implicit bias for ignoring complexity. reply biophysboy 5 hours agorootparentIt’s because statistical tests are based on the distribution of the statistic, not the data itself. If the central limit holds, this distribution will be a bell curve as you say reply Etheryte 6 hours agorootparentprevThis effect would be even more pronounced in a larger sample size. Consider how small a fraction indigenous arctic populations are of the population as a whole. In other words, larger sample sizes would be even worse off in this particular occasion. reply FollowingTheDao 6 hours agorootparentBut it is more complicated. I have Sami Heritage but it goes back to my great great grandparents. I did not know this until I started digging deeply into my ancestry, but I carry many of the polymorphisms from these people. So although I look like a typical European Caucasian, my genetics are very untypical of that population. And this also explains my family history of heart diseases and mood disorders which are also non-typical of European Caucasians. reply uniqueuid 6 hours agorootparentprevThe best way to talk about this is IMO effect heterogeneity. Underlying that you have the causal DAG to consider, but that‘s (a) a lot of effort and (b) epistemologically difficult! reply FollowingTheDao 6 hours agorootparent> but that‘s (a) a lot of effort and (b) epistemologically difficult! I agree, but then all these cheaper, easier studies are useless. reply tomrod 6 hours agorootparentYes. The value is in metastudies and data fusion across more studies. reply uniqueuid 6 hours agorootparentprevUnfortunately it‘s true that most studies are useless or even harmful (limited to certain disciplines). reply FollowingTheDao 6 hours agorootparentprevEven if you did a study with the whole planet there would be no statistical significance since the genetic variation in in FADS genes are still in the minority. (The majority of the world is warm and this is a cold weather/diet adaptation). In most African populations this Polymorphism does not exist at all. And even in Europeans it is only about 12% of the population. reply curiousllama 3 hours agoprevMy favorite part about p Hopefully this can help address the replication crisis[0] in (social) science. I think it isn't just p-hacking. I've participated in a bunch of psychology studies (questionaires) for university and I've frequently had situations where my answer to some question didn't fit into the possible answer choices at all. So I'd sometimes just choose whatever seems the least wrong answer out of frustration. It often felt like the study author's own beliefs and biases strongly influence how studies are designed and that might be the bigger issue. It made me feel pretty disillusioned with that field, I frankly find it weird they call it a science. Although that is of course just based on the few studies I've seen. reply staunton 6 hours agorootparent> the study author's own beliefs and biases strongly influence how studies are designed While studies should try to be as \"objective\" as possible, it isn't clear how this can be avoided. How can the design of a study not depend on the author's beliefs? After all, the study is usually designed to test some hypothesis (that the author has based on their prior knowledge) or measure some effect (that the author thinks exists). reply UweSchmidt 5 hours agorootparentIf you get an answer outside of what you expected, reevaluate your approach, fix your study and redo it all, probably with a new set of participants. If you can't do science, don't call it science. reply koe123 5 hours agorootparentWhich is a great idea if we ignore all other issues in academia, e.g. pressure to publish etc. Taking such a hard-line stance I fear will just yield much less science being done. reply Cpoll 5 hours agorootparent> much less science being done This isn't obviously a bad thing, in the context of a belief that most results are misleading or wrong. reply UweSchmidt 2 hours agorootparentprevLet's do a less science then, but rigorous and throrough. Or find more funding. But surely let's have a \"hard-line stance\" on not drowning in BS? reply Mayzie 5 hours agorootparentprevAnd where will the money come from for this second study? What about a third? Fourth? We live in a money-dependent world. We cannot go without it. reply SkyBelow 5 hours agorootparentprevThere is a difference between a belief and an idea. I might have an idea about what causes some bug in my code, but it isn't a belief. I'm not trying to defend it, but to research it. Though I have met people who do hold beliefs about why code is broken. They refuse to consider the larger body of evidence and will cherry pick what we know about an incident to back their own view conclusions. Can we recognize the beliefs we have that bias our work and then take action to eliminate those biases? I think that is possible when we aren't studying humans, but beliefs we have about humans are on a much deeper level and psychology largely doesn't have the rigor to account for them. reply trabant00 5 hours agorootparentprevPsychology is IMO in the state alchemy was before chemistry. And there's no guarantee it will evolve beyond that. Not unless we can fully simulate the mind. reply energy123 3 hours agoprevA p-value of 0.05 just means the observed effect will occur by chance 1 time out of 20. It means more than that to some people, and it shouldn't. reply trabant00 5 hours agoprev [–] \"It is difficult to get a man to understand something, when his salary depends on his not understanding it.\" I am sure there are plenty of people who misunderstand or misinterpret statistics. But in my experience these are mostly consumers. The people who produce \"science\" know damn well what they are doing. This is not a scientific problem. This is a people problem. reply bb86754 4 hours agoparentI haven't found this to be true at all. In fact, I'd say the majority of studies I read - even from prestigious journals - is fraught with bad statistics. I have no idea how some of these studies were even allowed to be published. Some fields are worse than others, but it's still a huge problem pretty much across the board. People conduct science, and a lot of those people don't understand statistics that well. This quote from nearly 100 years ago still rings true in my experience: \"To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.\" - Ronald Fisher (1938) reply chimeracoder 5 hours agoparentprev [–] > I am sure there are plenty of people who misunderstand or misinterpret statistics. But in my experience these are mostly consumers. The people who produce \"science\" know damn well what they are doing. As a statistician, I could not disagree more. I would venture to say that most uses of statistics by scientists that I see are fallacious in some way. It doesn't always invalidate the results, but that doesn't change the fact that it is built on a fallacy nonetheless. In general, most scientists actually have an extremely poor grasp of statistics. Most fields require little more than a single introductory course to statistics with calculus (the same one required for pre-med students), and the rest they learn in an ad-hoc manner - often incorrectly. reply Consider applying for YC's W25 batch! Applications are open till Nov 12. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The reliance on averages and p-values, particularly the 0.05 threshold, can mislead and contribute to the replication crisis in scientific fields like psychology.- Tools like JASP provide alternatives to traditional statistical methods, encouraging more thoughtful consideration and context in research.- Institutional pressures and a lack of statistical understanding among researchers can lead to biases and misapplication of the scientific method, affecting scientific progress."
    ],
    "points": 134,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1730373404
  }
]
