[
  {
    "id": 41612154,
    "title": "Sanding UI",
    "originLink": "https://blog.jim-nielsen.com/2024/sanding-ui/",
    "originBody": "Sanding UI 2024-09-03 One of the ways I like to do development is to build something, click around a ton, make tweaks, click around more, more tweaks, more clicks, etc., until I finally consider it done. The clicking around a ton is the important part. If it’s a page transition, that means going back and forth a ton. Click, back button. Click, right-click context menu, “Back”. Click, in-app navigation to go back (if there is one). Click, keyboard shortcut to go back. Over and over and over. You get the idea. It’s kind of a QA tactic in a sense, just click around and try to break stuff. But I like to think of it as being more akin to woodworking. You have a plank of wood and you run it through the belt sander to get all the big, coarse stuff smoothed down. Then you pull out the hand sander, sand a spot, run your hand over it, feel for splinters, sand it some more, over and over until you’re satisfied with the result. With software, the fact is that sometimes there are just too many variables to know and test and smooth out. So I click around, using the UI over and over, until I finally cannot give myself any more splinters. Just the other day, I was working on a list of radio options, pretty standard-fare stuff: Create awith an associated . Put them on the same row, center align them with a gap between the control and the label. Etc. As an oldie who used to leverage floats in CSS, I’m still amazed when I can use flexbox to do this stuff — it’s so easy!Foo.container { display: flex; flex-direction: row; align-items: center; gap: .5rem; }As I was doin’ my thang — clicking around a bunch, trying to get some splinters — I realized there was a dead spot in the UI, a place between the radio and the label where clicking didn’t toggle the control like I expected. “What the?” I thought. “I’ve got myandand associated for attribute, why isn’t this working?” Then I thought, “gap in my flex display must be the culprit!” Sure enough, it was. While flexbox had made it super easy to add some visual spacing between the control and its label, that spacing had become a dead zone of interaction even though it wasn’t my intention! There’s probably a million different ways to solve this problem — because CSS is awesome — but I just removed the gap and added some padding to my label, then voilà! Putting padding on the label, instead of the containing flexbox, made the whole thing clickable without a deadzone. A bunch more clicking around and things were working as expected. It’s a small thing, but lots of small splinters lead to an agonizing experience. So my recipe is: sand it, feel the grain, get a splinter, sand again, and repeat until smooth. REPLY Email Mastodon Twitter",
    "commentLink": "https://news.ycombinator.com/item?id=41612154",
    "commentBody": "Sanding UI (jim-nielsen.com)921 points by roosgit 23 hours agohidepastfavorite295 comments aetherspawn 16 hours agoThis is so lost in Agile. Engineers should get the time to “sand” their products, but we just don’t. If QA doesn’t make a ticket for the space between, it’ll never get fixed. The customer probably notices this kind of a thing but it’s a miracle if the customer bothers to report it, and another miracle if it eventually turns into a ticket, and another miracle if someone prioritises it enough to spend time fixing it. [In fact most companies have such opaque issue boards that as a customer I get so frustrated when I find a small issue or bug and have to spend like 50 hours back and forth to prove it’s a bug and actually get a ticket put in the tracker.] reply corytheboyd 9 minutes agoparentWhen it’s something this trivially small, why don’t you just… do it? So long as you can demonstrate your ability to deliver on “real” work, why not just knock these tiny things out too if you can? Work it into an adjacent change if that comes up. “But my manager and PM and CEO all say this is ILLEGAL TO DO” always seems to be the answer… but if your company isn’t completely rotten to the core with toxic waste, I don’t think anyone is going to axe you for making small microscopic incremental product improvements… reply crazygringo 16 hours agoparentprevWhat does agile have to do with anything? You think waterfall explicitly provided time to test out the UI and \"sand\" it? This is a process that generally requires a product manager to choose to prioritize, together with a capable UX engineer and/or designer. That prioritization can be inserted into any development methodology if you want it. Agile is irrelevant here. reply 8n4vidtmkvmk 12 hours agorootparentI don't think you even need PMs and UX to be involved here. Let the eng get a little bored and they'll find stuff to fix. The way it actually works though is we set some arbitrary impossible deadline, rush to meet it, creating a wake of tech debt, launch, and then straight on to the next thing. reply mlnj 7 hours agorootparentThe best way to build products I've found is to 1. hire passionate engineers. 2. give engineers enough room to breath and collaborate on the product. Every project that uses Agile defeats 2. I'm not saying that Agile is the reason for this but after 15 years of seeing this repeatedly I'm very wary of places which place Agile as a component of their culture. reply magicalhippo 2 hours agorootparentprevEven better, let the devs just watch the users use the product. I often join support when they have a call with a customer demonstrating an issue. I also sometimes get to join support when they're on-site for training or similar. Every time I watch users use our product, I learn so much about what to do and what not to do. reply meiraleal 5 hours agorootparentprev> Let the eng get a little bored and they'll find stuff to fix. That is the opposite of all engineers I worked with. Engineers easily learn that they should click in the specific part of the element and will forget it is an issue and use it like that for 5 years if nobody point that the issue is making the UX terrible for most users. reply Hasu 5 hours agorootparentWhere deviance is normalized, sure. If that dev didn't have to rely on 52 \"weird tricks\" and half-assed hacks just to keep their local environment running, they might care more about the quality of the product. But when everything around you has the grime of \"just do enough to get it barely working\", that's the game. The will to have nice things is a cultural value that some companies just don't have, and if they don't have it for themselves, they won't have it for their customers. reply lol768 5 hours agorootparentprevI think that says more about the engineers you've been working with than anything else. reply meiraleal 4 hours agorootparentsure but in some of the cases we were scaling products to millions of users. And it would work great but don't expect engineers to create good UI/UX paying attention to the \"small\" details. That won't happen naturally. reply swiftcoder 2 hours agorootparentMaybe hire some UX-focussed engineers then? I can't help thinking that the \"fungible engineer\" concept has done a lot of harm to tech hiring and team structure - of course if you take a guy who most is excited about database optimisation and put him on UI work he won't be passionate about it. reply richardlblair 6 hours agorootparentprevAgile isn't entirely irrelevant, it's also not the main issue. It's a cultural one. If someone tells me they work on an 'Agile Development' team my immediate perception is that they are a culture of cargo culting and bike shedding, without putting a ton of thought or care into their product, process, or users. These systems are designed to maximize output, not the quality of the output. Management is likely out of touch with the demands of creating a high quality product. This leads to misalignment with the development team and probably the business needs. Most businesses need a higher quality product than they have. Some don't, though. In those circumstances it doesn't really matter - I recommend avoiding these places like the plague. reply Viliam1234 2 hours agorootparentBusiness is trying to maximize money. It seems like quality has much less of an impact on sales than many of us would wish. I am not sure why it is like that, but as long as it is... it is a financially rational decision to throw unpolished products on the market as fast as possible. It would be easy to blame the customers. But let's look in the mirror -- how do I make purchase decisions as a customer? Actually, not a good example, because I usually don't buy software. OK, I buy Windows, but... I don't feel like I have a choice between more polished and less polished versions of Windows. Other than that, I use free software. When I use software at work, someone else made the decision and I had zero input. And that kind of software usually sucks (now I am thinking of Confluence, Jira, and other user-hostile monstrosities). So I guess a part of the answer is that if you sell software to business, there is no need to make it nice, because the people who decide whether the company buys it are not the ones who will be stuck with using it. reply __MatrixMan__ 2 hours agorootparentI wish users more aggressive about their software. At least once a week I hear something along the lines of: > I'm not a technology person, so I can't make X do Y It would be so much better if the go to response was: > This technology sucks, so I can't make X do Y. And if it can't even manage to make Y easy, then I'm sure not going to trust it with Z, and I'm going to tell my friends that they shouldn't either. If the business types need to be reminded that the quality problem is hurting them, then we should coordinate among ourselves to ensure that it hurts even more until they notice. reply noisy_boy 7 hours agorootparentprevAgile is relevant here because it has been adopted almost universally as a shovel to push more things on the programmer's back without much care to the quality of the changes being done. Sure, it is not to be blamed as per Agile manifesto but how many companies are adhering to the manifesto? It is how it is used, not what it was used for. reply jurgenkesker 10 hours agorootparentprevAgree, we have some time in sprints to do whatever we want, and then you finally can fix those minor annoyances that are probably the users annoyance too. But in general I think more time should be there reserved for it. It's now once in x sprints. When working on my own apps it's really obvious that non focused time leads to lots of improvements. Instead of only the business wishes. In that regard it seems that the potential of a dev is a bit diminished when you don't have time to do things according to your own priorities. reply raincole 9 hours agorootparentprevIt's just a general romanticization of how programmers work: if we have more time we'll fix everything. No we don't. reply ungamedplayer 9 hours agorootparentYou do not speak for me. I think but can't prove that you don't even speak for most programmers. reply meiraleal 5 hours agorootparentHe is not speaking for me but all enterprise projects I worked was like that. When devs (me included) run out of high priority tasks we ask or are directed to more high priority tasks, not fix the minor label/input issue, or the border of the disabled button in resolutions smaller than 400px, or whatever the PM/Scrum master didn't prioritize. Tired of bad management, developers just do what master Jira tell us to do. I do my best to fix all open bugs in my own software, tho. reply osigurdson 4 hours agorootparentprevI legitimately question if waterfall was ever practiced beyond a very small niche of projects. reply bravetraveler 9 hours agorootparentprevI think the key to your post is that waterfall or agile is irrelevant if you're getting work done and setting priorities Window dressing in the virtual space! Spending too much time/effort worrying about this nears sabotage, IMO reply xorcist 10 hours agorootparentprev\"Waterfall\" is not a development process. But plenty of the processes that give space for planning, also give space for QA. So surely agile, in the broad sense of development processes, is relevant to the comparison. reply ahoka 9 hours agorootparentA development process is not either Agile or “waterfall” anyway. reply exe34 12 hours agorootparentprevI agree that it's not agile, it's the environment that led to agile/scrum being adopted: not that it leads to better products, but that it gives management more control over every decision of how time is spent. essentially they can arbitrarily reduce the time/budget you have, and hire standard code monkeys, etc, to get something made. I think in a company like Steve Job's Apple, where it needs to look perfect (within his tastes), you'd have the time to polish the UI even with agile/scrum - one of the acceptance criteria will be \"I spent 5 minutes kicking it and I didn't get any splinters\". and then later on when Steve gets a splinter, he'd yell at you for a bit and then create a ticket. reply JimDabell 5 hours agorootparent> I agree that it's not agile, it's the environment that led to agile/scrum being adopted: not that it leads to better products, but that it gives management more control over every decision of how time is spent. essentially they can arbitrarily reduce the time/budget you have, and hire standard code monkeys, etc, to get something made. This is the exact opposite of agile. Direct from the Agile Manifesto: > Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. > Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. > The best architectures, requirements, and designs emerge from self-organizing teams. reply Viliam1234 2 hours agorootparentPerhaps the problem is that there are few self-organizing teams in companies. Managers needs to justify their salaries. reply exe34 4 hours agorootparentprevmay I refer you to Karl Marx? Communism is perfect, it's just been implemented wrong. reply ibash 3 hours agoparentprev> The customer probably notices this kind of a thing but it’s a miracle if the customer bothers to report it, and another miracle if it eventually turns into a ticket, and another miracle if someone prioritises it enough to spend time fixing it. I report a lot of bugs. But it seems like a lot of customer support people view their jobs as “protect engineers from bug reports and deflect responsibility”. That’s if you get a response at all. reply Vegenoid 1 hour agorootparentI worked as a sysadmin in a large physical services company. We had in-house software that integrated with almost all levels of our operations, most employees were using this software throughout their day. The whole IT staff, including the team of ~10 devs who built and maintained this software, worked in-person on the same floor. L1 support was constantly escalating issues that the sysadmin team could not assist with, because they had to do with this software. Either bugs, or new corner cases, or something changed, or they didn’t know how to do something. We would tell them again and again “we cannot help with operating this software” (it was outside our scope of responsibilities and knowledge - our job was to make sure the computers, servers, and network were all functioning properly). Despite the team of devs sitting 10 meters away, support would never, ever, talk to them. I think this was probably a dictate from management. It made no sense to me - these support staff were constantly using and helping people use this software, discovering the problems with it and the ways people wanted to use it, and all that feedback just died with them. The devs never interacted with the users of the software at all. You can probably guess how user-friendly that software was, and how much the users liked it. reply Viliam1234 2 hours agorootparentprevIn my experience, it can make a huge difference if the developers are allowed to talk directly to the users of their product. The users tell them which parts of the product hurt them most, and the developers find a way to fix that. If instead the communication is something like \"the end users give information to their manager, their manager gives information to our analyst, our analyst gives information to our manager, our manager creates Jira tasks for us\", there is often a lot of information lost at every step. For example, once my team made a web application that allowed users to edit some forms. When we asked how many rows there will on a form, we got an answer \"five, on average\". So we made forms that supported unlimited number of lines, tested them with about 10 rows, everything worked, we considered our job well done. One day, we met a guy who actually used the software. He complained about how it sucks, that validating or saving the form takes forever, that he sometimes loses data because of a timeout, etc. It turned out that although most of the forms contained about five rows, some of them actually contained thousands of rows. And yes, with over ten thousand rows in a form, on a bad day the web application lost the data because of a timeout. The developers were quite shocked. We complained about the analysis, but the analyst insisted that the average number of rows per form was about five, so the analysis was not wrong. (Technically correct; the best kind of correct.) Had we known this in advance, we certainly would have chosen a different web framework, but now it was too late to rewrite everything from scratch. So we just did what we could at this moment, some ugly hack like validating only 1000 rows at a time, so the end user had to push the validation button multiple times for very long forms or something like that, but at least he didn't get a timeout. The hack took about a week to implement and test, and the end user was happy, because it was a huge improvement over the previous situation. The management still insisted that developers meeting with the end users were wasting time. There were Jira tasks waiting to do, no time for chat. reply swiftcoder 2 hours agorootparentprevIn most companies customer support doesn't have any authority to retask engineers. Maybe if a customer reports a security issue, or a truly application-breaking bug. Otherwise all that customer feedback just gets rolled up into a slide deck once per quarter, and ignored reply __MatrixMan__ 2 hours agorootparentA neat quirk of having an open source saas product is that really it's customer support that's the product (after all, you can get the software for free). I think it makes for a higher percentage of tickets that are directly related to user experience--although there's still plenty of make-sales-happy type tickets too: gotta put a really shiny bezel around the open source thing. reply playingalong 55 minutes agorootparentprevYeah. In some rare cases there's a direct feedback button in the product itself. Then usually you don't get the response, but at least your remarks are read by someone. Or at least that's the impression I get. As a positive example - check out the new commit view page in GitHub which they are currently rolling out. There's a Feedback button which goes to a *public* discussion page with voting and comments. One can tell they are really into listening the feedback. And that's something. At least one of the miracles for free. reply 2024user 7 hours agoparentprevThis is so true. I'm the lead developer for a product and we have loads of small issues all over the place. It is not a great feeling to be responsible for something while also not having any power to fix it. From the business point of view, why would they spend time and money on fixing these things unless it's hurting sales or brand? Long term it likely does affect the brand but most people will have moved role/company in 5 years so don't care. reply dustingetz 6 hours agorootparentit is hurting sales and brand, the underlying problem is that to dramatically increase performance requires recomping both the tech team and the management and likely also the executives whose decisions shape and limit the org in the first place. In any org, the #1 priority for everyone involved is to keep their salary flowing, which means not replacing themselves with someone better. reply madeofpalk 1 hour agoparentprevI don’t know what we’re calling “agile”, but it’s explicitly supposed to capture this. You use the product, get feedback (from either yourself or users), and iterate on it. It sounds like you have organisational failures that prevent you from getting the feedback or iterating on it. reply bl4ckm0r3 12 hours agoparentprevpriority matters though, if you want to go full Apple with UI/UX then you need to lock the UI/UX for years, like Apple does, and refine every little thing to bring a fully ecstatic UX to the user. Or accept that most users won't notice that or wouldn't mind it behaving like that and build new features that will bring useful behaviours to the app (and revenue to the company). If you have to spend 50 hours to explain why it's a bug, most likely only you care about it. In some cases it may matter to be THAT precise with UI/UX and it's cool, not saying i don't appreciate quality but you can't have everything. One thing I'd like to point out is that most of the time it isn't even a matter of priority, lots of dev energy gets wasted in useless refactors and picking \"the right library/framework\" and building the \"next outdated design library\", instead of being used to improve things ;) reply martinsnow 10 hours agorootparentApple does many things right but things like Mail.app got thrown to the side for years. There's still many odd utilities here and there that have not seen any love like grapher. reply necovek 12 hours agoparentprevThe idea behind \"agile\" is to recognize when something isn't working and improve it. You obviously have a process that does not serve your customers' needs: work with your team to fix it. If you have SCRUM ceremonies, a retrospective is where you can raise it, but really, any time works (retrospectives are to purposely look at the past few weeks, but things you notice along the way, look to solve along the way). reply bartread 10 hours agorootparentStop drinking the koolaid, will you? I didn't get into software development to spend my entire time focussed on fixing broken processes. I'd bet almost none of us did. I got into it because I like building software and, more than that, I like building high quality software. At the end of the day it comes down to this: I've worked a bunch of different places over 25 years. I've seen a lot of different processes but, certainly for the past 17 or 18 years, mostly some flavour of agile that most closely aligned with Scrum. It's not been that great anywhere, and there are a handful of places it's been outright terrible. Reality check: if agile never gets any better than \"not that great\" then maybe agile is the problem. reply madeofpalk 1 hour agorootparentFixing “agile” is not the point. Find a process (and iterate on it) that helps you ship good software. I think there’s a bunch of stuff in the agile toolbox that’s helpful. Pick and mix that works for you. Maybe the business just doesn’t value shipping good software all that much - plenty don’t. reply mlnj 6 hours agorootparentprevEvery single time this gets brought up we see the Agile defenders come up with \"It's not agile, it's literally anything else.\". All I can say is if this is the common denominator... reply rrr_oh_man 21 minutes agorootparent\"It's not real communism...\" reply 8n4vidtmkvmk 12 hours agorootparentprevI don't buy this. Not serving the customer's needs? Is this the paying customer's #1 feature request/bug fix? It's probably some poor random sap that has to use the software and notices it. There's different kinds of software. The software I work on now, the advertisers are the real customers, not the users of the app. So the users have basically zero buying power unless they stop using the app and we need to attract them back, but a small bug like this isn't going to do that. The other kind of app you sell to a company. They want a good app that meets their business needs, but the ones making purchasing decisions still aren't the Frontline staff that have to use it. And there's no way a bug like this is making it up their internal chain and then over to the vendor. And even if all of that happens, I have trouble believing this would be prioritized in a sprint. The only way anything gets fixed is if by some miracle an eng with the power to fix it either notices himself or if the app is popular enough, someone tweets about it and he happens to read it. It'll never make it through the formal chain. I know this because as an eng who would rather do some sanding then add more useless features... Well, then the PMs wouldn't have anything to do. reply Viliam1234 2 hours agorootparentprevFun fact: most companies doing \"scrum\" actually don't have retrospectives. (I understand that this goes completely against the textbook idea of scrum, but there is always the textbook and \"the way we do scrum at our company\", and those two often have very little in common.) reply devjab 11 hours agorootparentprevThe idea behind agile is to sell you training and later consulting once your organisation fails to adopt it in any meaningful way because it’s principles are so vague your culture, will, get it wrong. It’s saying that the people behind the agile alliance and so on aren’t actually working in software engineering. Many haven’t since 20 years before the birth of Python. They’re also famous for handling any form of criticism with “you didn’t understand our principles”. Which to be fair is often completely correct, but maybe it’s because those principles are horrendous? What it has lead to is an industry full of pseudo-jobbers. As others point out… your software engineers, can, do the work if you let them. Even if you don’t, you have no guarantee that your added personal actually catches errors like the ones in this article. Because human testers usually aren’t part of the team in any meaningful way. reply ahoka 9 hours agoparentprevIt’s more of the artifact of ticket centric development, so I would blame Jira and the like before “programming self help” methodologies. reply abbadadda 5 hours agoparentprev> [In fact most companies have such opaque issue boards that as a customer I get so frustrated when I find a small issue or bug and have to spend like 50 hours back and forth to prove it’s a bug and actually get a ticket put in the tracker.] Agree wholeheartedly. Back & forth emails, screenshots, Q&A (what version are you on?), etc. The number of times I make it to checkout on the last step and something breaks on a certain version of a browser reply jagged-chisel 6 hours agoparentprevI feel like this goes along with the recent Artisan Software discussion on HN. Companies don’t want Software Artisans. reply 7bit 9 hours agoparentprev> The customer probably notices this kind of a thing but it’s a miracle if the customer bothers to report it, and another miracle if it eventually turns into a ticket, and another miracle if someone prioritises it enough to spend time fixing it. Man, Discord does have a \"posts\" feature that works similar like a forum. If you draft a text there, the HOME/END keys are all messed up and you can't select text with shift, or move by word holding CTRL (I don't remember the specifics at the moment). I have reported that a couple of times over the past 3 years, because that makes the drafting text *extremely* difficult and frustrating. As a web dev myself, I wonder how this even broke in the first place. Meaning, I wonder what kind of incompetency is needed to break something, that works out of the box. Anyhow, this should be a fix that cannot possibly take longer than 30 minutes to fix and would immediately make the user experience 1000 times better for everyone. Yet, the bug is still there 3 years after reporting it. I also reported a bug in Teams, where you cannot use the HOME/END keys in the phone number input, to Microsoft, through Premiere Support. The reply was: This works as designed. I am not surprised that customers don't report these kind of bugs any longer, because neither the employees/developers, nor the company doesn't give a shit anyway. reply whazor 12 hours agoparentprevYou can recommend adding analytics to all your flows. Then recommend some of these sanding tasks in order to improve metrics such as drop-off rate. Managers are more inclined to say yes when there is data involved. reply exe34 12 hours agoparentprev> QA doesn’t make a ticket for the space between, it’ll never get fixed. it'll be a low priority ticket with a very large tshirt size because the product manager doesn't want it done and the newbie who estimated it doesn't know what's going on, so it'll take a very long time to figure out. reply remus 6 hours agoprevI think being a big user of whatever you're building is incredibly useful for finding these kinds of issues. If you're a big user as well as a dev then you will often stumble on these little things before a user does, and you are also perfectly placed to fix these issues before users can stumble on them. I suspect this is why small teams with strong ownership can be so effective. If you feel ownership of a thing then you feel users' pain when they hit these little paper cuts, and it becomes a point of personal pride to fix these things and make the UX as smooth as possible. reply kizer 4 hours agoparentAlso why companies dogfood and have internal betas for products (when this is possible; i.e., you’re not making something for enterprises or other kinds of customers). The sense of ownership you’re talking about may not be as direct but stake in success of the product is there. reply xyst 16 hours agoprevI wonder what’s the most polished or “sanded” UI out there? You would think FAANG would have a half decent UI and UX with the amount of money they have. But anybody that has used Amazon.com or AWS, GCP, or even Azure would beg to differ. Personally, off the top of my head. The most polished UI/UX has to be “mcmaster.com”. I can find anything I need in what seems like a couple minutes. Compare this to big box stores like “Home Depot”, “Lowe’s”. I can spend 10-15 minutes just trying to find the right size of screw, board, or whatever using their bloated sites. On mobile it’s even worse. reply mrighele 1 hour agoparent> You would think FAANG would have a half decent UI and UX with the amount of money they have. There is no point in sanding something that someone else is using hammer and chisel on. FAANGS are the companies of continuously delivered websites, self-updating evergreen software, churners of framework-du-jour that are deprecated sooner than you can say \"FAANG\". Even if took the time to sand something, it would be replaced by something else the following day. reply TheAceOfHearts 13 hours agoparentprevFastMail has one of the best-feeling web apps I've ever used. It's incredibly snappy and I never encountered any bugs while using it. They raised the bar for what I thought a web app could achieve. reply corytheboyd 16 minutes agorootparentAgree! Only bummer is when I want to create calendar events and reference email details, or vice versa. On desktop it’s easy enough to have two tabs, but on mobile, it’s a pain. I get why it is the way it is, but we are talking about exceptional UX here, and I think there is room for improvement. reply dsissitka 11 hours agorootparentprevI think the only thing I really dislike about Fastmail's UI is they've hidden the \"Report phishing\" and \"Report spam\" links and they're in two completely different places. reply tombh 2 hours agorootparentYes! It doesn't make sense. It took me ages to remember where to go for each. reply RockRobotRock 8 hours agorootparentprevTo clarify: There's a menu bar for the entire email chain, and an \"Actions\" button for each mail message. Report Spam is only available in the former, and Report Phishing is only available in the latter. I agree, it's annoying! Maybe someone at Fastmail will see this. reply martinsnow 10 hours agorootparentprevThat's an unfortunate decision reply yawnxyz 13 hours agorootparentprevrecently switched all our mail to FastMail and that took 1ish hour... I was ready to spend a couple of days to wrestle switching mail providers. Guess they really live up to their promise reply vishnugupta 8 hours agorootparentprev+1 to Fastmail. More so after I discovered their shortcut keys. reply jmb99 6 hours agoparentprevRockAuto is my favourite website. Incredibly simple and utilitarian, but also quite powerful. You can easily drill down or search for parts, depending on what is more intuitive to you. Price comparison is automatic, and grouped into useful price/quality categories. You can see year/make/model compatibility for a part number once you’ve found it, as well as a brief description, at least 1 picture (usually more), and determine whether it will be shipped from the same warehouse as other parts in your cart. It does all this with 0 friction, from one page, blazing fast on any platform. I end up ordering almost all my car parts there, not because they necessarily sell the best parts but because it’s just so easy to. reply Cannabat 4 hours agorootparentI used RockAuto for the first time recently. The site is super dense with info and functionality, but never once did I feel lost or overwhelm. Neither did I feel like anything was missing. Everything I needed to see and do was exactly where it should be. It's so rare for the screen to disappear like that. It's so transparent that I _almost_ can't say it's a joy to use (it's a joy to use!). reply donatj 15 hours agoparentprevI can name multiple Facebook features that have been broken for months. Temporary profile pictures most annoyingly of all. They stopped working close to a year ago. They simply never change back. No one's sanding anything there. reply waterproof 14 hours agorootparentThe incentive structure doesn’t encourage it. Nobody gets a promotion for going back and fixing issues; it’s all about new initiatives and boosting metrics with new ideas. reply qingcharles 13 hours agorootparentprevIf you look at the HTML output of FB, IIRC it is about 100 levels deepthat cause me physical pain. reply djhn 13 hours agorootparentThe thing that gets me is how slow all of it is. And I’ve programmed against facebook APIs, which are ludicrously performant! Hundreds of megabytes of data in single digit milliseconds total roundtrip. reply kristiandupont 12 hours agorootparentThey do this in an attempt to combat ad blockers. It's an arms race that is sadly bound to create cruft like this. reply egeozcan 7 hours agorootparentexactly, one example is there are a lot of programatically absolute positioned divs to break ad-blockers or make them very inefficient, while the performance of the app suffers from many tricks like these as well. reply dpkirchner 13 hours agorootparentprevThe \"feeds\" screen in the app has been throwing errors for at least a year (\"Feed isn't available right now\"). I only try to use the screen because a while back it was the only way to view content in the correct order. reply duckmysick 13 hours agoparentprevLinear (of linear.app) has a highly polished UI. In fact they had a dedicated period for fixing just usability issues: https://linear.app/changelog/2022-12-01-polishing-season-202... https://web.archive.org/web/20231003205004/https://linear.ap... reply boundlessdreamz 2 hours agorootparentLinear has the best UI/UX of all the web apps I have used. After Gmail and Google Maps, I don't recall any other web app wowing me as much as Linear. reply 8n4vidtmkvmk 12 hours agorootparentprevI wish they'd make more apps. I use Linear but with a team of 1, it's not exactly the best fit for me. reply bosky101 13 hours agoparentprevThat time of the year when everyone can see there is craftsmanship in the tiny details. https://littlebigdetails.com is exactly that reply userbinator 13 hours agoparentprevI wonder what’s the most polished or “sanded” UI out there? Windows 2000. Everything newer has been slowly downhill. reply rollcat 4 hours agorootparentI really wish we had something resembling a \"native\" UI toolkit, but for the web. Just throw together a couple s or es with absolutely zero additional JS/CSS, but have it actually look & work moderately decent, the way SwiftUI does. CSS is too powerful, OOB HTML is too basic/ugly. reply ahoka 8 hours agorootparentprevI would say Project Chicago was maybe one of most consistent UI projects ever executed. Too bad they kept reinventing parts of it, so now we have a mishmash of different UX paradigms, some better, some worse, but certainly inconsistent. At least it’s not macOS though. reply exe34 12 hours agorootparentprevamen. and at some point Gmail was perfect, but they kept fixing it, so now it's getting bloated, slow and occasionally just does random things. reply kizer 4 hours agorootparentprevThere is SO much bloat in all the “modern” UI “culture”. Reinventing things over and over again. Creating entire frameworks for tiny, simple things. And the worst part of web UIs is that (though there have been efforts to address this) there is low regularity between the experiences especially compared to native UI apps where you are purposely restricted to a set of controls which look and behave the same across all apps that use them. reply 7bit 9 hours agorootparentprevWindows 3.1 was better. Windows 2000 introduced a lot of point and click, which just distracts from what you actually want to do on a computer. reply tikhonj 14 hours agoparentprevI've listened through some of the developer commentary on Half Life 2 and Half Life Alyx and the amount of user testing coupled with attention to detail really impressed me. The same can't be said for Steam though :/ reply skykooler 16 hours agoparentprevMy biggest issue with mcmaster's website is that it doesn't provide any sort of navigation hierarchy - if you go into, say, the \"rounded head screws\" subcategory, there's no option to get back to the general \"screws\" category besides the browser's navigation buttons. reply etrautmann 15 hours agorootparentYep, this is indeed annoying but infrequently pointed out. reply djhn 13 hours agorootparentprevI don’t know - I could argue that the user’s browser should be the preferred way to navigate, duplicating it’s functionality is redundant and adds clutter to the interface. It’s at least a defensible position. reply edflsafoiewq 12 hours agorootparent\"Back\" is the not the same thing, since you didn't necessarily come from the \"Screws\" category page. reply fuzzy2 9 hours agoparentprevMobile games, especially those with microtransactions. They're highly incentivized to offer a satisfying user interface. So as to get more money, of course. reply parpfish 3 hours agorootparentThey don’t optimize for satisfying interfaces, they optimize for driving engagement. I find the aesthetics of free to play games very stressful and unsatisfying (lots of notifications and popular to distract you), but they ARE effective at getting me to click into menus to make those nuisances go away reply SSLy 10 hours agoparentprev> I wonder what’s the most polished or “sanded” UI out there? premium iOS apps. Procreate {,Dreams}, Photomator, Overcast, Crouton, Mela, Carrot Weather, Apollo. reply peheje 13 hours agoparentprevIdk. I just visited the site (McMaster) for like 2 minutes and found a few annoying things. I filtered for cotton (o rings). Nothing happens after click for 4 secs. Then it chooses something else to filter on. Next the animation to get the filtering menu is bugging out. And dragging it down triggers a site refresh. reply Already__Taken 4 hours agoparentprevThe best UI is no UI. Anyone who tries to design for increased engagement isn't who you're looking for. I'd look to study lots of internal tools that don't get marketed or outside influences. That would be interesting to find out. Where's the crossover from just enough resources to make it exist and enough resources to leave it as \"finished\" reply arendtio 13 hours agoparentprev15 years ago, one would have said google.com But I think asking for a UI toolkit/framework is more helpful. Otherwise, you optimize for straightforward use cases, like entering a search box. reply ccakes 15 hours agoparentprevimo GitHub has to be up there. I think some of the recent changes to search and making the code view more IDE-like are steps backward from a “polish” perspective, but still useful features reply robin_reala 13 hours agorootparentGitHub specifically has the issue mentioned in this blog post! It annoyed me so much I had to file it with them three years ago, and it’s still not fixed: https://github.com/orgs/community/discussions/7506 reply masklinn 13 hours agorootparentprevCouldn’t disagree more. GitHub has jank from the api to the ui. reply geysersam 13 hours agorootparentprevI like the IDE \"defined here/used here\" features but I wish the page would be faster. It can be quite painful to read code on GitHub. The code view is also broken and unusably slow in some mobile browsers (Firefox) when scrolling horizontally reply mattlondon 10 hours agorootparentThis sort of problem (speed) is a much overlooked secondary functionality requirement IMO. You might get amazing UX mocks and wireframes and designs etc from the UX team, and the mockups from Figma or whatever may have user-tested really well, but if there is huge latency in the real implementation then that is a usability-killer IMO, regardless of how polished the UI is. reply dclowd9901 2 hours agoparentprevAs far as app UIs go, I just started using Duolingo and their app seems to be tremendously well polished. reply dgellow 11 hours agoparentprevAs something I’m using on a daily basis, Linear has one of the best product UI I experienced. It is extremely polished and snappy reply 8n4vidtmkvmk 12 hours agoparentprevLinear app is quite polished. Stripe is also quite excellent but not entirely bug free. I think they have a bit more surface area though. reply 2024user 7 hours agoparentprevHEY! mail was great when I used it reply cocoflunchy 12 hours agoparentprevProbably Netflix? reply blovescoffee 15 hours agoparentprevmcmaster is the most polished ui/ux you know? To each their own but this is a very hackernews comment - it's very much a site for engineers but I can't agree it's a polished UI or UX. To me: Sense of hierarchy is off, accessibility is meh, there's an enormity of information per page, there's poor use of color and spacing... it could be worse but I can imagine this site giving my designer friends a seizure. reply ddtaylor 19 hours agoprevThe goto tactic for this specific `` problem is:Foo reply dmix 16 hours agoparentBootstrap moved fromin 4.0 to+in 5.0 for radios/checkboxes [1]. I was curious about why but my guess is that it adds some simplicity for theming when repositioning/padding either the label or input. [1] https://getbootstrap.com/docs/5.3/forms/checks-radios/ reply abdusco 8 hours agorootparentI think it's for material design inspired focus animations. :has selector wasn't a thing, so you have to use + sibling selector to target the label of a focused input input:focus + label {} label:has(input:focus) {} reply parasti 8 hours agorootparentprevThey more than likely made that decision to be able to style the label based on input state using the + sibling selector. I use that trick for literally every visible input now. reply Ciantic 4 hours agorootparentprevIncidentally Bootstrap 5.3 seems to have the same problem as the article describes. There is a gap which doesn't do anything if clicked, right between the radio button and the label. reply robin_reala 13 hours agoparentprevEven with nesting you still need for/id attributes to make it accessible to common voice command software: https://www.tpgi.com/should-form-labels-be-wrapped-or-separa... reply melchizedek6809 6 hours agorootparentNot sure I agree with the conclusion of that article, according to it, only 2 screen readers don't support nested labels, I couldn't find statistics on how prevalent these are, but there are a lot of alternative screen readers one could use which might support nested labels since they're not mentioned there (I've mainly heard of JAWS, which isn't mentioned there), so it doesn't seem to be an inherent limitation of assistive technology, just a bug in some (popular?) screen readers. reply robin_reala 5 hours agorootparentVoiceControl and Naturally Speaking aren’t screen readers: they’re voice command software. They’re designed for people with mobility problems, not vision problems. There’s no inherent limitation here that couldn’t be solved by bug fixes, but they’re the two major pieces of assistive tech in that sector so can’t be dismissed without dismissing people who need that functionality. reply melchizedek6809 2 hours agorootparentFair enough, so to test things out I've enabled Voice Control and tried whether it makes a difference how the elements were arranged: At least with Chrome, it does not make a difference! It correctly determined the label and I could just tell it to click on that particular checkbox. Since Dragon Naturally Speaking doesn't seem to have a trial, as well as having a broken shop page you can't order from, I can't give it a test, but that articles advice seems rather questionable to me. reply rafram 11 hours agorootparentprevThat’s the voice control software’s problem. reply robin_reala 11 hours agorootparentAt the end of the day it’s your users’ problem. Ideally their assistive tech would work, but if it doesn’t and you’ve got an easy fix then you should implement that fix. reply 7bit 9 hours agorootparentI disagree. Of course you could fix that. But that would mean that you can fix a thousand and one issues that's caused by other softwares erroneous content handling. Which leads to nothing but bloat and introduces just more code that could come with more bugs. If your software works and is designed as intended, then sometimes it stays the users problem. Unfortunate, but otherwise problems will stay forever. reply robin_reala 8 hours agorootparentYour software isn’t working, because the user cannot use it. Your UI is always a combination of whatever hardware and software come together to give an experience. Everyone chooses to marginalise certain segments of users’ technologies – I don’t support IE any more of course – but I typically won’t drop support for technologies that are vital assistive technologies for that segment of users. (If you need a personal story to bring it home, the only way my dad, a programmer in the 70s-90s, could continue to use computers when he suffered from MND / ALS, was to use voice control software.) reply wruza 4 hours agorootparentYou’re basically enabling for these shitty variants of voice control software to exist. Html, css and js allow dor more granularity in how ui works, but using them this way is just absurd. reply robin_reala 4 hours agorootparentshrug They exist regardless, and the users have no real other options. I’ve found in my career that devs tend to fall into the “caring about the effect” and “caring about the artefact” buckets, and I guess you’re in the second category? There’s no one right place to be – without people in the second we‘d have less effective solutions overall – but I hope your team is also balanced by people in the first. reply raincole 9 hours agorootparentprevAnd unless you're google or facebook, people will not change their voice control software for your website. reply notduncansmith 17 hours agoparentprevThis was my first thought. The entire text label should toggle the radio or checkbox, not just the box and the padding. reply dclowd9901 2 hours agoparentprevJust a note that in a framework like react, this will introduce an error that propagates when people attempt to use something like “google translate” on your site. You’d need to wrap the “Foo” in an element to mitigate. reply brailsafe 19 hours agoparentprevBack in the day I used to think this was taboo for some reason, but maybe it was only for XHTML to enforce one-to-one label -> input association. Flexbox seemed a bit redundant, since even with the non-nested syntax I'd think it would lay out inline and you can just add some padding in the same way. reply 8n4vidtmkvmk 12 hours agorootparentThe alignment is not exactly the same when you just put them side by side. Flex can center the radio with the text a bit nicer. Otherwise it sits above the text baseline I believe. reply mattlondon 10 hours agoparentprevI think that would not have worked with the grid CSS approach. I guess it is kinda beside the point though as it was just an example to illustrate the point. reply pentagrama 19 hours agoprevIs so important have people with that spark to see and fix those little UX issues, a good analogy used on UX design is papercuts for the user, not critical but it degrades user satisfaction. To the author I will add that that radio button is not following the convension of a dot for the selected state instead of a check. Users may think at first sight that multiple/no selection is possible. reply ix101 17 hours agoparentOne I've experienced on GitHub and Jira is dragging to select text on a dialog, if you release outside the dialog the mouse up event dismisses the pop up which is probably a side effect of being able to click outside the dialog to dismiss it. reply wruza 4 hours agorootparentThat’s classic “web ui”, the consequence of lowering the absraction level without providing and forcing developers to use useful mechanisms. So everyone just goes mindlessly with events which are badly targeted by design. I’d say that desktop is an order of magnitude better, but a kde installation I have to work with also doesn’t register clicks on buttons sometimes. Because for the sake of ui-ness they used flat elements instead of buttons and forgot making them down-upable anywhere within to click. So when you move-quick-and-click it registers (I guess) drag instead due to the movement, and drag is a no-op. Allowing clueless developers to use lower level and normalizing lower level graphics is a huge mistake these platforms make. The web is basically built with this in mind, that’s why it sucks. 20 years ago you couldn’t even imagine clicking around in a desktop app to see if radio works. People would literally laugh at you. reply kchr 7 hours agorootparentprevThis could probably be fixed by tracking whether the mousedown event was started inside or outside the dialog, and only close the dialog if the mousedown started outside it. reply wruza 4 hours agorootparentIt’s called a cursor grab and in the web exists as el.setPointerCapture(). https://developer.mozilla.org/en-US/docs/Web/API/Element/set... reply perfunctory 4 hours agoprevAlan Kay was right classifying most software engineering as pop culture. It's 2024 and we are still fiddling with spaces around radio buttons, a problem that should have been solved decades ago. reply dclowd9901 2 hours agoparentI don’t get this take. “Still fiddling with spaces around radio buttons.” It’s design. Design is unique to the creation. We still fiddle around with spacing around radio buttons because one spacing doesn’t work for every design. Unless you’re talking about the clicking dead zone, which I would argue is more a problem with not using the right cursor than the dead zone the gap introduces. reply two_handfuls 15 hours agoprevMeanwhile, some other UI uses square boxes for radio buttons. Or have a highlighted button but the enter key doesn't activate it. Or there are three different menus all behind a different symbol (ellipsis, hamburger, kebab). There is a lot of variance in quality. Those of you who polish your UI: you are appreciated. Truly. reply selimco 15 hours agoparentI think space is usually the key that performs a click on focused buttons, not enter. reply meindnoch 10 hours agorootparentIt's up to the user agent. On one platform it's the spacebar, on another it's the return key. Of course, fake controls written in JS wouldn't be able to do this. Which is why it's the wrong thing to do. reply two_handfuls 14 hours agorootparentprevI was talking about the default button (typically blue, activated with enter), not the currently focused button (outlined, space). reply qingcharles 13 hours agoparentprevSquare or squircle seems to moving towards the standard for radio buttons now. Even Apple plays this game :( reply two_handfuls 34 minutes agorootparentFile each as a bug, fight the good fight. reply youssefarizk 5 hours agoprevI'm always torn whether this is a good use of time or not. If you're an early stage startup, it feels like shipping features (that work) quickly is your biggest differentiator, not how nice your UI is. I guess this is true if you're doing something in a not so saturated field, but understand that if you're in a saturated space, you probably do need the design to be natural os as to set yourself apart. reply hombre_fatal 4 hours agoparentThe good use of time is that OP learned that labels should wrap their inputs, so the next 10,000 times they write a control on a form, they'll just do it off the bat. That this post has 800 upvotes is just a reminder of the caliber of UI/UX experience the average HNer has, especially when you see them disparage UI development as something unworthy of their time / expertise. reply TimTheTinker 3 hours agorootparent> That this post has 800 upvotes is just a reminder of the caliber of UI/UX experience the average HNer has Or maybe it's our collective frustration with processes and expectations that make it very difficult to be a craftsman--and that make web app UI bad in general. I remember my boss being frustrated with how long it was taking me to build a set of filtering dropdown components from scratch (one of the most difficult to get right). I stuck with it and put in some extra hours to really get all the small details right. Later that year, he asked me to tech-lead a newly forming team of UI engineers. A year later, everyone called it great work and no one remembered how long it took. reply ludwigvan 4 hours agorootparentprevDid they? My understanding is they replaced gap with padding, didn’t necessarily wrap the input with the label (which they should as you suggest) reply wruza 4 hours agorootparentYep, they used flexbox to nicely do… absolutely nothing useful beyond blogspam. Next up “how I made my ui responsive but then heroically stopped labels wrapping away from radio marks”. There’s a reason, even if accidental, why writing custom controls was sort of a black magic in traditional ui. And that reason is, most people are clueless about how fragile ui actually will be in their hands. reply ryandrake 2 hours agorootparentI guess what I learned from the article is how crazy it is that developers are still putting UIs together with text labels and padding and flexbox-this and align-that. What kind of stone age shit is this? Back in the '90s we were putting UIs together with controls already polished and perfected by the OS vendor, presumably backed by man-years of UX research from those companies. The controls themselves had styling, behavior, event handling, and so on baked in. Fast forward to today's web development, where we've regressed to drawing text inside rectangles and trying to handle click events on those rectangles, and/or wrestling with half-baked \"frameworks\" that poorly do some subset of what OS-provided controls did 30 years ago?? UI development seems like cooking in a clay pot over a fire that you had to start with flint. reply TimTheTinker 37 minutes agorootparentThe root cause here is that unlike native OS controls, most native web controls are primitive as heck. So companies that don't want to look like Craigslist end up either using an off-the-shelf UI controls library (not a bad decision, in my opinion), or building their own. Along with the second option, a new set of disciplines is emerging: the design system designer and engineer. Since companies have grown so accustomed to building their own bespoke design language (instead of using the one the OS ships with), it's doubtful that a well-designed set of modern, native web controls (that ship with browsers) would be able to compete with the notion of \"having a bespoke design system\". Also, white-label libraries (like Radix UI) are increasingly appearing that handle all the implementation details and leave the appearance to be defined. reply wruza 1 hour agorootparentprevAbsolutely agree, there’s so much to lose with completed ui controls. It’s akin to the situation when you disassemble a device for the first time, then reassemble it and it’s always some parts left on the table. Styling and customization are useful and interesting topics in ui. But instead of thinking it through, most libs today just throw a ball of wires at the developer who is clueless and often couldn’t care less even. reply steve_taylor 4 hours agoparentprevPeople tend to underestimate the competitive advantage of a nice UI. reply parpfish 3 hours agorootparentAbsolutely. Having well sanded features doesn’t necessarily remove bugs directly drive conversion, but it gives the entire product an established professional “feel. Even though your customers feel it, they won’t explicitly articulate it reply Viliam1234 1 hour agorootparentprevIs there actually a competition where the UI could decide the winner? I mean, it is probably very rare to have two products that have the same features, and the only difference is how nicely done they are. For example, MediaWiki is in my opinion 100x better than Confluence, but Confluence has some extra features that I don't care about but managers do (stuff like easily attaching PowerPoint presentations to web pages), so the managers decide that the company uses Confluence. And I keep silently screaming in frustration every time I lose a part of text during editing, or the links break when a typo in a page name is fixed (because there is no option to add a redirect), etc. The extra feature often wins, when the salesman describes the product to the manager who makes the buying decision. That's why we have the \"checkbox features\" that most end users don't care about, but it allows the product to seem better in comparison. Feature creep is how stuff gets sold. The situation of two featurewise identical products competing only on better UI would probably be highly unstable, even if it happened somehow. There are network effects, so if one product starts winning, most people will switch to that product because \"that's what everyone else is using\". The other product will be left without money to pay for development, and will go out of business. Afterwards, the winning product does not have to care about their UI anymore. reply throwaway743 4 hours agoparentprevIt's not about how nice the UI is is, but rather how responsive it is and if it's providing your users with enough feedback. If it's not, it will lead to your users being confused and/or frustrated which will lead to them looking for an alternative tool. If you're product is the only one in town, then they're just left to deal with it until a competitor pops up with a better (whether real or perceived) UI. reply cal85 44 minutes agorootparentPlease elaborate a bit on real vs. perceived UI! reply Ylpertnodi 4 hours agorootparentprev>(whether real or perceived) UI Wouldn't they be the same? If i perceive some ui as awful, it's real(ly awful). A program I've used since the 80s has progressively been 'flattened' and is now a complete pain to use - to such an extent I often prefer to fire up ye olde atari emulator. To me - the pain is real, and perceived. reply enraged_camel 4 hours agoparentprevEliminating “friction” is a huge part of maximizing conversion and retention, both of which are important metrics for early stage startups. It’s only once you achieve significant momentum that you can afford to let these types of UX annoyances slide. The risk is that once you do that, the normalization of deviance makes your product end up like Jira. reply dexwiz 21 hours agoprevThis is my strategy in bug bashes, and it generates way more tickets than anyone who has a multidimensional Cartesian matrix of test case combinations. It’s good to know those tests cases to start, but random testing quickly outpaces planned testing when trying to find small issues. Also planned testing is often happy path or expected errors. Sanding like this finds edge bugs much faster. reply gwervc 18 hours agoparentFrom time to time, especially when too tired to work on a full feature, I do some random click here and there, and try stuff I usually don't in my game. I always discover some issues or little improvements than can be made. A lot would indeed not come up using planned testing. reply mdavid626 7 hours agoprevPut the input >intofoobar foobar and allowing a mark pseudoelement to participate in alignment? Or at least forcing everyone to use the input-in-label variant? Nobody. But they split it, and now people without clear understanding how ui should work do it wrong by design and invent Monte Carlo methods to check if it works. And it seems some crappy screenreaders don’t even recognize the proper form of it, adding salt to the cut. reply DustinBrett 18 hours agoprevI've been \"sanding\" my personal website (https://dustinbrett.com) for nearly 4 years now, and it feels like it could go on forever. Luckily I enjoy working on it. reply tkzed49 17 hours agoparentI really appreciate this. Most of the time when I see a \"desktop OS on webpage\" it feels half-assed and honestly overplayed. This on the other hand is super tight and polished! reply fhdsgbbcaA 14 hours agoparentprevOk, be honest: how many hours put into this were during 9-5, and on the boss’ dime? I hope all of them. :-) reply InvaderFizz 4 hours agoparentprevThis is really fun. A usability note, at least on Safari on iOS, you have to put the protocol for a web page to load. If you just put www.cnn.com without the https, it never loads. reply idreyn 17 hours agoparentprevthis is very smooth and scratches an itch I didn't know I had -- to use a windowed OS on my phone reply steve_adams_86 17 hours agoparentprevThis is so fun to explore. You've done some great work on this! It's inspiring. A lot of fun to think about how you've implemented everything. reply bschmidt1 17 hours agoparentprevLooks incredible. Snappier than real Windows tbh reply dgellow 11 hours agoparentprevI wish Windows was that snappy! Incredible work, kudos! reply MaxikCZ 10 hours agorootparentMeanwhile we get a loading splash screen when opening a calculator. reply high_priest 18 hours agoparentprevWarning: Prepare for a jumpscare with authors face. Fun page reply darepublic 16 hours agoparentprevvery nice. though you cannot have > 1 nesting of dustinbrett.com afaict reply ustad 12 hours agorootparentOn my iphone I can! Tested to 3 levels deep. reply sibeliuss 15 hours agoparentprevNice man! reply jay-barronville 17 hours agoparentprevThis is so fun. I love it. Well done! reply meowface 16 hours agoparentprevThis is cool and very polished, but my advice (which may not resonate with you) is that you should channel this attitude into some user-facing app instead of letting it basically go to waste on a personal website most people won't see or won't use for more than a second or two. reply fhdsgbbcaA 14 hours agorootparentIt may surprise you, but some people like to program computers for reason other than money. Makes no sense right? Weirdos, all of them! reply 8n4vidtmkvmk 11 hours agorootparentYes, but you can make free apps for a wider audience. Or even just useful apps for yourself. Can do what pleases you though. I sometimes work on intentionally useless apps just to try things out, but the idea is always to carry those ideas over to some app that has some purpose. reply exe34 11 hours agorootparentdo you think makers of abstract art should do something more useful? how about people in marketing? landlords? reply 8n4vidtmkvmk 53 minutes agorootparentI personally don't care much for abstract art, but if your goal is to produce an art piece, then sure, that's a purpose. I don't know if OP was going for \"art\" or just having fun with the tech. I wouldn't count it as art because it's very much a copy of an existing piece of software, not an original piece. reply exe34 24 minutes agorootparenthttps://en.wikipedia.org/wiki/Replicas_of_Michelangelo%27s_D... reply toast0 3 hours agorootparentprevAll of those things are useful. With no abstract artists, art museums will have bare walls (which is fine, if the wall is the one they gave to me... but nobody wants to look at a bare wall and pretend its art for a whole museum!) Marketting seems useless sometimes, but when Pepsi spent their marketting budget on community projects, they lost a ton of sales. My local ice rink has no marketting and nobody in the county knows it exists, even people who would like to do ice skating or ice hockey; a smidge of marketting would be super useful; other rinks in neighboring counties have doubled or more the number of kids playing hockey in the past few years since we got a local NHL team, but ours struggles to get a single full team at most age groups. Landlords are not well liked on HN, but seems to me having a place to live on a month to month basis was pretty handy before I had the ability to make a long term commitment to a single place. I know some people buy a place to live for college, but an off campus rental seemed a lot more sensible to me. And similar when moving for work if you're not sure you'll be there long term, or you want out of a hotel before you're sure of what neighborhoods you like. reply meowface 12 hours agorootparentprevI definitely didn't necessarily mean an app that had a way to make money. I mean exposing the work to more people who would get fun or utility out of the painstaking effort put into it. The world could use this sort of taste in a game, an activity, a tool, etc. reply exe34 11 hours agorootparentprevI think it's perfect. Art should be completely useless and yet spark joy - this one does both! reply d_burfoot 4 hours agoprevSomeone should do a YouTube channel where they \"sand\" popular software products and point out these kinds of subtle UI bugs. reply gnomespaceship 4 hours agoparentOne person I saw do this is Mia: https://www.tiktok.com/@heymiadotco If you sort videos by popularity there's a lot I enjoyed where she does a \"UX roast\" of SaaS or streaming websites reply masklinn 4 hours agoparentprevYou could probably do an entire channel out of just YouTube. reply VeejayRampay 4 minutes agoprevwe need to stop pretending that CSS is awesome though, it's been in use for about twenty five years now, keeps reinventing itself and still fails at simple things (as exhibited in this example) reply razodactyl 5 hours agoprevYou know what.... this was an amazing post. There was a lesson, analogy, and a very to-the-point example. reply dclowd9901 2 hours agoprevBike shedding kind of: I think the real failure here is not using a “cursor: pointer” directive. Easy to tell what’s clickable when your cursor changes based on what’s clickable. reply socialentp 17 hours agoprevThis is totally what I’ve been doing all day. I call it “digital puttering”. It’s where much of the beauty and craft of something is developed. It requires a craftsperson to not just “call it done and move on”, but instead to be intrinsically motivated to spend time with the creation intimately, rolling it around in your hands/brain. Guiding a vine here and there, plucking a leaf or two… until it ‘feels’ right. reply djsamseng 3 hours agoprev> So I click around, using the UI over and over, until I finally cannot give myself any more splinters. I’d take this with a grain of salt (pun intended). There’s a lot of bugs that you cannot reproduce without certain permissions or a particular environment. Let alone the race conditions or user setup. In my experience, most bugs would not have been uncovered using this brute force approach. A few tests using your understanding of the code and critical thinking goes a lot further in my opinion. reply linux2647 3 hours agoparent“Sanding” shouldn’t be the only approach to testing an app. Developers should test using a variety of techniques. Some bugs are discovered through unit or integration tests, others by brute force, others still from end users reply ChrisMarshallNY 17 hours agoprevThat's pretty much what I do. TestFlight records how many sessions I run, on the release-ready app. I use TestFlight from very early on. It always shows thousands of sessions for me. The next-highest tester is often only tens or hundreds. But that number is dwarfed by how many sessions I run in the simulator. It tends to result in apps that folks like using. The biggest danger is that I get so familiar with the UI, that I don't understand its [lack of] discoverability for those unfamiliar with it. I can easily design inscrutable UI. reply Sardtok 4 hours agoprevI was on a site the other day, a hotel or flight website. I think it was a review form. There were checkboxes, where the checkbox wasn't clickable, only the labels. I was sure the whole thing was frozen, but happened to find some other UI controls were responding. So I tried the labels. I've come across the reverse scenario quite a few times, where the label isn't clickable, but this variant was new to me. reply smokel 11 hours agoprevThis is a great example of where unit testing does not apply. I see many developers get caught up in rituals, and polishing (and monkey testing for that matter) seem to go against a reproducible approach, and are therefore frowned upon and even ignored. Still, it is a much more powerful technique to get something both working and user friendly. Investing in developers to spot that something is 3 pixels off, or the basic idea that different users have different tastes, can be very productive. reply dataviz1000 11 hours agoparentIt is also very easy to develop systematic automated ways to do this with tools like Playwright and Nut.js. reply handsclean 11 hours agorootparentOk, how would you use Playwright and Nut.js to discover the OP’s “splinter”? Note I’m specifically asking about discovery, not testing for it once you already know what to look for. reply thom 7 hours agorootparentI've actually been thinking lately about property based testing for UIs. In this particularly case, there should be an invariant for each entry in a radio button list that the selectable area covers the entire bounding box from button to the end of the label. There are many such invariants you could imagine - every paragraph of text should be selectable by a click and drag, menu drop downs shouldn't hide as long as the mouse is within its area etc. Build up a big enough suite of these tests and you could quite easily integrate them in Storybook or beyond. Probably not something you want to run on every save, but an asynchronous process running somewhere recreating this \"sanding\" activity would be a worth way of saving time and improving quality. reply smokel 5 hours agorootparentIf I understand you correctly, you mean to build up one test suite, and apply it to many diverse applications? I'm afraid that will be pretty hard to accomplish, given that such requirements are not easy to distill from the user interface itself, and impossible to obtain from the codebase (which, by definition, would contain bugs that you'd like to catch). Perhaps LLMs or a \"user interface foundation model\" might come in handy to find these implicit requirements, and run tests on the application. reply thom 7 hours agorootparentprevObviously this won't catch everything human QA can, but where an invariant can be expressed (navigating then going back should result in being in the right place, as mentioned in the article) it seems good to try and capture it. reply dcre 18 hours agoprevIt’s an important insight that the state space for UI is very large and that is why intuition is especially useful — it’s rarely feasible to account for all possibilities analytically. This is true to some extent in all areas of software development, but I think for UI dev moreso than most. reply jamamp 21 hours agoprevI think the article has good sentiments about it. Actually using your application a lot helps polish it down a ton. However, wouldn't putting the input inside of the label (before the label text) be a better solution than fiddling too much with CSS and flexbox? It's more foolproof to ensure clicks within the label activate the input, and eliminates the need for the \"for\" reference. reply tshaddox 20 hours agoparentThat’s what I generally do as well, but sometimes I don’t like how it leads to empty space that is part of the clickable area. This will happen if you have a label tag with the label text above the input (and the label text is much narrower than the input widget). This isn’t a huge problem, but it always bugs me. reply philo23 20 hours agoparentprev> However, wouldn't putting the input inside of the label (before the label text) be a better solution The one potential downside to doing it the way you describe is (assuming the same CSS flexbox layout) now all the white space on the right side of the label acts the same as clicking the radio/checkbox. Which is almost like the opposite problem to the original issue. This might actually be a good thing for some designs/contexts, but not always. For example, on mobile it might lead to miss-clicks while trying to scroll past the s reply bastawhiz 17 hours agorootparentThat's only true if you let your labels be as wide as the parent container. > on mobile it might lead to miss-clicks while trying to scroll past the s You can scroll on mobile by swiping over the text of a label itself without activating the input; this isn't generally a concern. reply philo23 5 hours agorootparentWell with the CSS in the post they would end up as wide as their parent. If you made it an inline flex box then yes, that wouldn’t be an issue. > You can scroll on mobile by swiping over the text of a label itself without activating the input; this isn't generally a concern. Generally speaking yes, but there’s still a chance of triggering it by touching the whitespace by mistake. Whereas if it wasn’t the full width it just wouldn’t be possible to begin with. reply lelandfe 21 hours agoparentprevlabel>input instead of label+input. This is called an implicit label - time was, there were concerns about screen readers that couldn't interpret them. I don't know how bad that is in practice: https://a11ysupport.io/tests/html_label_element_implicit ...but it does look worse than explicit: https://a11ysupport.io/tests/html_label_element_explicit reply swatche 21 hours agorootparentThe spec says either way (https://www.w3.org/TR/html401/interact/forms.html#h-17.9), but I agree with putting the input inside the label for the acessibility and avoiding the blank space issue. reply lelandfe 21 hours agorootparentThe HTML spec doesn't speak much on a11y guidelines. Here's what the W3's WAI says https://www.w3.org/WAI/tutorials/forms/labels/#associating-l... > Whenever possible, use the label element to associate text with form elements explicitly > [..] > In some situations, form controls cannot be labeled explicitly... Generally, explicit labels are better supported by assistive technology ...but people have been saying that for like 15 years now, I don't know how big of a deal those failures are. That'd be a good blog post reply dumbo-octopus 20 hours agorootparentParent link says NVDA, VoiceOver, and JAWS all support the implicit way. That’s the industry standard suite to support, they’re all free and available across all platforms. If some company makes a shoddy half baked solution for sale (looking at you, Dragon), and they don’t understand basic HTML that has been standardized for years, that’s not my problem. The same way I don’t only use the subset of web technologies that the AOL Premium web browser supports for $10 bucks a month. reply extra88 20 hours agorootparentYes, all the screen readers handle implicit labels just fine. As the a11ysupport.io tests show, it's Voice Control software that fails, not just Dragon NaturallySpeaking but also the built-in Voice Control in macOS. I think the implication is these voice control programs aren't using the accessibility tree built by the browser but parsing the DOM themselves, poorly. It's not really surprising for Dragon since it does hardly anything in a browser without its browser extension installed and extensions don't have access to the accessibility tree. It's more surprising for macOS Voice Control. reply dumbo-octopus 19 hours agorootparentVoice Control also works perfectly fine, I just tested it myself on their provided sample. Say \"select your name\" on this page: https://a11ysupport.io/tests/html/html_label_element_implici... reply extra88 16 hours agorootparentThey must have fixed it in Safari 18. I'm running macOS 14.6.1 and at the beginning of the month it didn't work but I also just tried it and now it does. reply acka 17 hours agorootparentprevJAWS isn't free[1]. Using the trial version for accessibility testing goes against its EULA[2]. [1] https://www.freedomscientific.com/products/software/jaws/ [2] https://webaim.org/blog/jaws-license-not-developer-friendly/ reply dumbo-octopus 3 hours agorootparentAh, I got it mixed with Orca. NVDA is the thing to target in Windows. reply chenmike 20 hours agorootparentprevThere’s no reason you can’t do both, and indeed some a11y linters recommend doing that reply oxidant 20 hours agorootparentprevPut the input inside the label and still use \"for\" on the label. No way to test right now but that's what I usually do. reply ivanjermakov 20 hours agoprevI miss this attention to detail in popular websites' UI. Often even clicking on the label won't update the form. reply kmoser 20 hours agoprevMy beliefs in the same vein: - If you think you've found all the bugs, look again. - If you think you've just fixed a bug, test again. - If you think your program is done, you're wrong. reply arendtio 13 hours agoparentWhile I agree with the first two, the third is a problem. Yes, you can always extend your program, but should you? The more code and functionality you include, the harder it becomes to maintain. Finding the point where your software is so round or complete that you can call it done is somewhat of an art. You can undoubtedly add stuff beyond that point, but I won't improve the software in the long term. reply kmoser 3 hours agorootparentThe third one isn't meant to imply that you should keep adding needlessly, or that you should necessarily add more functionality (yes, this is how feature creep starts). It's meant to indicate that software is never truly finished: there is always something to fix or improve upon (e.g. refactor). It's perfectly fine if you make the conscious choice to not make those fixes or improvements, usually for time/budget reasons, but the point is that you should be aware of those possibilities, and that those are the points to revisit if and when you have the resources to do so. reply robinsonb5 11 hours agorootparentprevQuite the opposite - a design (in general, not just software) isn't done until you can't take anything else away from it. I've coined the term Bonsai Software here before, but I do like the sanding analogy. In the last week I've spent way more leisure time than could be considered sensible writing some user-interface code for the Amiga, in order to make defining the UI in future projects as simple and elegant as possible! reply arendtio 9 hours agorootparentWhile I agree with you on the design term, there is this fundamental conflict: unlike other design forms, software is an additive process. With wood or stone, you can work subtractively until there is nothing left to take away, but with software, you typically start with nothing and add stuff until you have enough. There is a high chance of overshooting the optimal design, and then you have to reverse direction. reply dmd 17 hours agoprevA broken pattern I see constantly related to author’s example is large buttons where only the button label - and not the rest of the button - is clickable. reply thex10 18 hours agoprevThis attention to detail is what separates the mediocre frontend devs from the rest. How the heck do I improve our hiring process so we get more of you!!! reply tkzed49 17 hours agoparentDo you already have candidates actually write frontend code? Either async or during an interview, or both? I'm a big fan of being on either end of an interview where I'm actually working on a functioning project. reply defanor 12 hours agoprevNot directly related to the article's message (though may count as collaborative \"sanding\"), but related to its UI: the page has texts centered (margin-left: auto, margin-right: auto, short lines), but paragraphs with embedded images lack that, and the images are aligned to the left. I thought it may be due to JS disabled (if it relies on JS for the layout somehow), but enabling it did not change that. Observed in Firefox 115; it is not the intended layout, is it? reply bhy 13 hours agoprevShouldn’t these all be smoothed out by UI frameworks, design guidelines and best practices? It doesn’t look like the industry should spend so much productivity on these sanding works? reply ryandrake 2 hours agoparentYea, I'm not a web developer, but coming from the desktop world, I am shocked by how little web UI frameworks do for you and how buggy their implementation is. Adding padding here and margin there and flex boxes and all that shit just to get a radio button or a drop-down that we do in one line of code on the desktop side? It's like the software development equivalent of using stone tools and chisels to build a car. reply Jaxan 11 hours agoparentprevI agree that these things should smooth it out. But so far the reality shows they do not. reply christophilus 17 hours agoprevI generally wrap my radios inside of the label for this reason. Is there a reason not to do that? reply parasti 35 minutes agoparentYou can't style the label based on input state if you do that. If you instead order them like input + label, then you can style the label with selectors like input:checked + label. reply marcosdumay 17 hours agoparentprevAFAIK, the only reason to use the label's `for` is when you want to place it in a different place from the widget. reply emsimot 11 hours agorootparentIt also helps with accessibility, screen readers etc reply marcosdumay 3 hours agorootparentNot compared to including the widget inside the label. reply jay-barronville 17 hours agoparentprev> I generally wrap my radios inside of the label for this reason. Is there a reason not to do that? Actually, I think this is the best way to deal with all inputs that have labels—a small number of issues and edge cases (such as the one described in the article) just disappear. It’s also valid hierarchy-wise. reply gaborme 6 hours agoprevI really like the sticky face menu on the bottom right. Never seen this before. Gave me some inspiration for one of my sites. reply dewey 20 hours agoprevSomething for the sanding list: Navigating between https://blog.jim-nielsen.com/about/ and https://blog.jim-nielsen.com makes the layout shift a bit in Safari on macOS. The reason is that Safari only shows the scrollbar when it's needed but without \"reserving\" the space. I once spent hours debugging this before I realized what was happening, my confusion coming from the fact that with the inspector open that wasn't the case (As there the scrollbar was always visible...). reply promiseofbeans 20 hours agoparentThere's a newish CSS feature to fix just this: scrollbar-gutter (https://developer.mozilla.org/en-US/docs/Web/CSS/scrollbar-g...). Unfortunately, Safari doesn't support it. Can also lead to an ugly gap in your navbar depending on which container you're making scroll. reply dewey 20 hours agorootparentThat sounds promising, for some reason the webkit issue says “resolved / fixed” but I don’t see any updates in the issue itself: https://bugs.webkit.org/show_bug.cgi?id=167335 reply extra88 20 hours agorootparentNot everything committed to WebKit ships in Safari. But the status changed to resolved/fixed only a month ago and it's in the latest Safari Technology Preview so maybe scrollbar-gutter will ship this year. https://www.webkit.org/blog/15860/release-notes-for-safari-t... reply madeofpalk 20 hours agoparentprev> The reason is that Safari only shows the scrollbar when it's needed but without \"reserving\" the space. Every browser with non-floating scrollbars will do this, right? Safari, in its default configuration on a touch-ish device (macbook without a scrollwheel mouse, iOS) don't show explicit scroll bar gutters IIRC, and so won't have this problem. reply sdflhasjd 19 hours agoprevThis applies to mobile apps a lot. If you're not careful (especially when using the iOS/Android simulators too much) you can create tiny awkward hit boxes for buttons that are difficult to tap with fingers reply ehnto 12 hours agoprevI have found the same to be true in game development once all the pieces start coming together. There is no substitute to just using the thing. Maybe you don't find bugs/splinters, maybe you realise that it just doesn't feel right once you've glued the components together. reply throwaway14356 14 hours agoprevi always wrap the labels around the radio buttons and checkboxes. the animation also shows nicely that the label should not be selectable text. if one wants to further polish it a hover highlight might look nice at times. Drawing a box around the radio buttons is perhaps not modern but it may make the form more usable. The title or description of the element should not be the same as the options. The 4 times might belong in two or more groups. This is something to dwell on, make a few mockups then most likely it shouldn't be used. If it doesn't jump out as amazingly useful restore normality. consider lining up the time so that the :'s sit in a line. Try put the PM in a collum too. Maybe there should be AM as well. Keep doing useless experiments until you strike gold. It should be really hard to beat default form elements (unless it is iphone) Your truly fantastic 1000 line text input should most likely be deleted. the example animation probably has insufficient line height. The user shouldn't have to aim that much. reply donatj 15 hours agoprevAs a fellow old, I'm inclined to say labels should just wrap the control unless there's a very good reason for it not to. Would have completely prevented the issue, wouldn't need a global ID nor the \"for\". Just generally more semantic and cleaner. reply raminf 17 hours agoprevBuilt an app when the iPhone first came out. Spent 2 months building the core app and another 3 months working to reduce the number of taps and remove road-bumps in the UI/UX flow. Totally paid off. Working on another app now. Sweating the details on the 'watercourse way.' That first experience is critical. reply someoneontenet 20 hours agoprevSounds like op is manually fuzz testing. reply gwern 1 hour agoparentYes, but the important thing is the evaluation function of what happens after random actions - his intuitive expectations and things ripping his skin. A web fuzzer can click around randomly like he does, but it can't know things like \"clicking here ought to have set the button\" or \"this is twice as slow as it should be, why\". It can detect things like crashes or segfaults or JS code throwing exceptions, but not those other things. reply invaliduser 14 hours agoprevAnecdata, but I found the very same issue (the bermuda triangle gap between radioboxes, but also checkboxes, and their labels) in a project a few months ago. It seemed a pretty big deal to me, specially because I always clicked on the gap, and got frustrated and angry at this. So I reported it to the UX team managing the design system, and to the developers implementing the design system, and nobody really cared. Some people even tried to convince me this behaviour was OK (because other design systems worked that way too, or because they were planning to refactor this on the far future so they didn't want to spend time on this). I think the industry is now filled with people that just don't care, specially on big companies where, if it's not in a ticket, and if the ticket is not prioritized as critical, nobody cares. All they care about are metrics (test coverage, line count of a function, whatever). Pretty sad actually. reply snegrus 1 hour agoprevSir, this is Ikea. reply jeffreygoesto 11 hours agoprevThe computer always exactly does what you told it to do. You almost never know what exactly you told it. reply Retr0id 20 hours agoprevoof ouch I just got stabbed by a giant checkmarks-in-radio-buttons splinter reply jbverschoor 19 hours agoprevJust put the input inside the label. Problem solved, no need for for= reply tikkun 20 hours agoprevIs this Jim Neilsen related to Jakob Neilsen (famous UI guy)? reply 4b11b4 17 hours agoprevI like this idea of sanding your UI... Just recently have done quite naturally the same thing... Expand the clickable surface area of a region reply 4b11b4 17 hours agoparentThat should be a metric... % clickable. Could look at app, page, component levels reply timzaman 16 hours agoprevDecent analogy. I wonder how many techies have ever used a belt sander. Or have one. I think very, very few. reply michaelbarton 14 hours agoparentI have not used a belt sander! I prefer to start with a planer, then use an orbital sander! Generally work from 60 grit up to 220 depending on the surface. Popping the grain and covering the whole surface in pencil marks in between each! For anyone seriously interested in how to get a great finishing with sanding here’s the guide I follow: https://www.blacktailstudio.com/blog/how-to-sand-wood-proper... reply duckmysick 7 hours agorootparentThanks for posting it! Good point about adding the right lighting rig, haven't thought about. I use a planer a lot but I'm still struggling with making multiple passes down a wide piece of wood. I often end up with grooves. I've gotten better over time but I'm still not happy with the results. reply michaelbarton 2 hours agorootparentIf it’s a particularly wide piece of wood you could try a router planning jig. I think these are used with slabs Otherwise you could try and get access to a CNC or a large industrial belt sander reply hipadev23 16 hours agoparentprevYour assumption is quite wrong. https://www.zainrizvi.io/blog/why-software-engineers-like-wo... reply duckmysick 14 hours agorootparentWhat does this suppose to prove? The OP said \"very few\" not none. Of course there will be a few anecdotes of overlap. A better way would be to compare the size of Stack Exchange boards at https://stackexchange.com/sites . It's not perfect but it's as good as it can get. Stack Overflow has 26 million users. Woodworking has 17 thousand. A fraction. Further still, not every woodworker will use a belt sander. It's used for a specific purpose (large flat surfaces) and it's not a beginner's tool. So the fraction gets smaller. I'd say the assumption stands. reply hipadev23 12 hours agorootparentYou and him are incorrigible. Him for the naive assumption and stereotype that “techies” don’t do any manual trades or crafts, you for looking at the count of activity on stack fucking overflow for woodworking. reply duckmysick 8 hours agorootparentThat's not the assumption I got from the OP's comment. What I got was - and I agree - that the intersection between the members of the technical field (specifically those responsible for apps UI) and the members of the woodworking field who also use an advanced, single-purpose tool is tiny. The assumption will be the same if you replace the belt sander with a guitar amplifier, a lawn aerator, or a pressure canner and woodworking with their respective hobbies. If you have better data that shows an overlap between those in the technical field and those that do woodworking with highly specialized tools, I'm all ears; I'm willing to be convinced otherwise. In the meantime, here's another anecdote for you - I do woodworking (and gardening and constructions) and I don't own a belt sander. reply bbor 20 hours agoprevIt’s kind of a QA tactic in a sense It's not kind of a QA tactic, this is literally the definition of QA. Specifically, this post is about ad-hoc functional testing. Kinda funny how this kind of testing used to dominate, but in the era of CI/CD, dedicated QA departments, and fancy webdriver suites, we've flipped too far the other way, and developers need to be reminded to QA their own stuff! I think we've all learned the hard way that nothing works until it's been fixed, no exceptions... no code comes off the dome flawless. reply ants_everywhere 17 hours agoprevThe algorithm of clicking around trying to break things heavily optimizes for workflows the designer finds natural. The more you do it, the more you reinforce your existing patterns because, you know, brains. This tends to produce experiences that are very smooth for a large group of people but fail really badly for anyone who is slightly different. Most Apple stuff feels like this to me, for example. It's like carving a polished stone path where any direction you step off the path is raw and jagged. reply wheresmycraisin 17 hours agoprevI prefer a well tuned smoothing plane or a card scraper, personally. reply AlienRobot 19 hours agoprevYou can also dolabel reply quirino 20 hours agoprevOn Safari (iPad), type something in the search bar. If you accidentally click outside of your keyboard it will deselect the bar and delete everything you typed. On Spotify the three little dots to do some action to a song have too small of a hitbox. Press even the slightest bit under the button and it will start playing the song. You'd never click there to play the song. When you consider the scale of these apps, there must be so much combined annoyance. reply emmelaich 17 hours agoparentOn the Tesla screen, parked view you can tap the frunk and trunk buttons to open them. If you miss them by even a slight amount, the view does this annoying animation which hides the frunk and trunk butttons for about a second. You have to wait for the animation to stop before trying again. So damn annoying. reply brontitall 17 hours agorootparentOMG yes! That appeared in the somewhat recent UI revamp and I’m so tired of mashing at that button. Not only is it unavailable for what feels like more than a second, but now it’s in a different place reply yen223 18 hours agoparentprevReading this on Safari on my iPad, the favicon for this tab is the Youtube logo. I genuinely have no idea how this bug came about, but it's been like that for months now. reply dmd 17 hours agorootparentSafari is really, really bad when it comes to everything around favicons. The biggest annoyance being it doesn’t accept updates, so things like mail status favicons don’t work. reply sonofhans 19 hours agoparentprevI believe that the iPad behavior you describe is intentional. I’m not sure I’m right here, and there are tradeoffs. I get frustrated with it too. But I think Apple is going for clarity, lack of ambiguity. If your text remained in place but the control were not focused, what would that control then indicate? In Safari now it _always_ indicates (a) the current page, or (b) your current typing. To do otherwise would be to create a third state: “Used to be typing.” Then it would no longer unambiguously indicate the current state. reply layer8 18 hours agorootparentThis is only tangentially related, but the Safari address bar already does not always indicate the current state, specifically when pages take time loading and when going back and forth in history. There is some kind of broken synchronization between page display and address bar, in conjunction with page loading timeouts. That being said, I would be fine with an \"address bar has been edited but not commited yet\" state. It's how most other (desktop?) browsers work and it's not an issue. reply thelastparadise 18 hours agorootparentYep it's just pure jank. Even Apple (especially post-Jobs) can produce jank. reply aag 18 hours agorootparentI've never understood the claim that Apple's UIs are better than others'. That wasn't true while Jobs was around, and it isn't true now. Apple Photos, for example, loses keystrokes every time I create a new album. That's been true for years. And Time Machine randomly drops files from backups. Linux isn't perfect, but my daily experience using it has been far superior for years. reply worstspotgain 17 hours agorootparentWell the Time Machine comparison doesn't really belong in a laundry list, it's basically Apple's greatest unmitigated disaster ever. reply DonHopkins 17 hours agorootparenthttps://news.ycombinator.com/item?id=28355058 Time Machine's \"Floating Time Tunnel\" user interface for browsing backups and restoring files is such a useless pretentious piece of shit. I DO NOT CARE for it taking over the entire screen with its idiotic animation, that prevents me from browsing current Finder folders at the same time or DOING ANYTHING ELSE like looking at a list of files I want to retrieve on the same screen. https://www.youtube.com/watch?v=CSwy_thSXow It even sadistically blacks out every other connected display, and disables Alt-Tab, as if it was so fucking important that it had to lock you out of the rest of your system while you use it. You can't just quickly Alt-Tab to flip back to another app to check something before deciding which file to restore and then Alt-Tab back to where you were. No, that would be too easy, and you'd miss out on all that great full screen animation. It not only takes a long time to start up and play its opening animations, but when you cancel it, it SLOWLY animates and cross fades back to the starting place, so you LOSE the time and location context that you laboriously browsed to, and then you have to take even more time and effort to get back to where you just were. It was designed by a bunch of newly graduated Trump University graphics designers on cocaine, with absolutely NO knowledge or care in the world about usability or ergonomics or usefulness, who only wanted to have something flashy and shiny to buff up their portfolios and blog about, and now we're all STUCK with it, at our peril. Crucial system utilities should not be designed to look and operate like video games, and turn a powerful mutitasking Unix operating system interface into a single tasking Playstation game interface. ESPECIALLY not backup utilities. There is absolutely no reason it needs to take over the entire screen and lock out all other programs, and have such a ridiculously gimmicky and useless user interface. Whatever the fuck is wrong with Apple has been very very wrong since the inception of Time Machine and is STILL very wrong. How can you \"Think Different\" if you're not bothering to think at all? Time Machine isn't just Apple Maps Bad... https://www.youtube.com/watch?v=tVq1wgIN62E It's QuickTime 4.0 Player Bad. http://hallofshame.gp.co.at/qtime.htm The most damning praise comes from Wired Magazine, 06.08.2007. Fuck Core Animation and the \"Delicious Generation\": https://www.wired.com/2007/06/core-anim/ >Core Animation will allow programmers to give their applications flashy, animated interfaces. Some developers think Core Animation is so important, it will usher in the biggest changes to computer interfaces since the original Mac shipped three decades ago. >\"The revolution coming with Core Animation is akin to the one that came from the original Mac in 1984,\" says Wil Shipley, developer of the personal media-cataloging application Delicious Library. \"We're going to see a whole new world of user-interface metaphors with Core Animation.\" >Shipley predicts that Core Animation will kick-start a new era of interface experimentation, and may lead to an entirely new visual language for designing desktop interfaces. The traditional desktop may become a multilayered three-dimensional environment where windows flip around or zoom in and out. Double-clicks and keystrokes could give way to mouse gestures and other forms of complex user input. >The Core Animation \"revolution\" is already starting to happen. Apple's iPhone at the end of the month will see people using their fingers to flip through media libraries, and pinching their fingers together to resize photos. >The \"Delicious generation\" is a breed of young developers who embrace interface experimentation and brash marketing. The term \"Delicious generation\" was meant as an insult, but they wear it as a badge of honor. >Image: Adam BettsShipley's initial release of Delicious Library, with its glossy, highly refined interface, gave birth to a new breed of developers dubbed the \"Delicious generation.\" For these Mac developers, interface experimentation is one of the big appeals of programming. [...] >Apple has been ignoring its own HIG for some time in applications like QuickTime, and is abandoning them completely in upcoming Leopard applications like Time Machine. >Functionality-wise, Time Machine is a banal program -- a content-version-control system that makes periodic, automated backups of a computer's hard drive. >But Apple's take on the age-old task of incremental backups features a 3-D visual browser that allows users to move forward and backward through time using a virtual \"time tunnel\" reminiscent of a Doctor Who title sequence. It's completely unlike any interface currently used in Mac OS X. [...] >While it seems logical to speculate that interfaces like those of Time Machine and Spaces will lead to the end of the familiar \"window\" framework for desktop applications altogether, many Mac developers predict that the most basic elements of the current user interface forms won't disappear entirely. reply inferiorhuman 17 hours agorootparentprevHard disagree over here. Apple's not always been good but my experience has usually been better than the competition (at least OSX 10.4+). They've been on a downward trajectory, certainly I've whined a lot about how bad MacOS 14 is, but their main competition (Microsoft)",
    "originSummary": [
      "The author describes their iterative development process, likening it to woodworking, where they build, test, and refine until the software is smooth and issue-free.",
      "A recent challenge involved aligning radio options using flexbox, where a gap between the radio button and label prevented toggling; this was resolved by removing the gap and adding padding to the label.",
      "Emphasizes the importance of thorough testing and refinement to ensure a seamless user experience, highlighting that small issues can significantly impact overall usability."
    ],
    "commentSummary": [
      "The discussion highlights the challenge of addressing minor UI (User Interface) issues in Agile development environments, where such issues often go unreported and unprioritized.",
      "There is a debate on whether Agile methodologies inherently neglect these small fixes or if it's a broader cultural issue within companies that prioritize rapid output over quality.",
      "Some participants argue that direct communication between developers and users can significantly improve product quality, but this is often hindered by management structures and processes."
    ],
    "points": 921,
    "commentCount": 295,
    "retryCount": 0,
    "time": 1726947380
  },
  {
    "id": 41614490,
    "title": "They stole my voice with AI",
    "originLink": "https://www.jeffgeerling.com/blog/2024/they-stole-my-voice-ai",
    "originBody": "September 21, 2024 Listen to this clip: I don't know about you, but that sounds pretty familiar. I mean I would like you to subscribe to my YouTube channel. But that's the Jeff Geerling channel, not Elecrow, where the clip above is from. I never said the words that are in that video. Someone emailed me a link to Elecrow's video and said it sounded off. I'm guessing at least some of the thousands of people who watched the video thought I agreed to voice some Elecrow videos, since I talk about some of the same topics on my channel. I even reviewed one of their products a few years ago, the CrowPi 2. I didn't have a bad relationship with them in the past. They make electronics and even Raspberry Pi accessories. There's also a video version of this blog post, if you don't enjoy reading, and wish to hear the clip embedded above in context, for a direct comparison with my natural voice. And I don't know if I can prove it, I mean how can you? But I'm pretty sure they fed my YouTube videos into some AI voice clone tool, then used my voice to narrate multiple series of promotional tutorials, like this one on ESP32, and this one on RP2040. That's... not cool. I remember when OpenAI practically cloned Scarlett Johanssen's voice, but I thought the fallout from that would lead to companies being careful about the AI voices they use for things like product demos and tutorials... Apparently not. I haven't decided what to do. I mean, like I said, I haven't had a problem with Elecrow in the past. I'm hoping beyond all hope it was an honest mistake and they didn't even realize it was my voice. But beyond that, the worse thing is there isn't any legal precedent for unauthorized AI voice cloning, at least not that I'm aware of. There is precedent for not using someone's voice in commercial works without their consent. Look up Midler vs. Ford. I don't know if I want to do anything with lawyers, because that costs money and right now I'm just trying to keep my old Camry running through the end of the year. And I'm not even sure non-consensual voice cloning is against YouTube's Terms of Service. But the main thing is, I want to make a point—that's why I'm writing this post: You can't just steal someone's voice or likeness, and slap it on your products or videos. You should hire a voiceover artist, or pay a content creator to work with you. A lot of brands actually do that! Just... don't steal my voice and use it to promote your product. Update: I sent an email tonight, requesting Elecrow take down at least the two series with this AI voice that sounds like me, after a few people suggested doing so. I also asked if it was intentional that the voice sounded like me, or if they trained the voice ('cloned' it) on my own video or audio content. We'll see if they respond! I'd rather start on that foot than any YouTube takedowns or legal action, like I said, I've had no trouble in the past, and I'm not 100% certain this was intentional. I am 100% aware Elecrow knows of my channel, though, as I have over 43 emails back and forth with five different Elecrow marketing reps, spanning from 2020 to today (22 of those emails are from this year). They have even asked if they could do a paid partnership in the past, too: Hi Jeff, can we talk about a paid partnership? I am looking forward to your reply. (April 2, 2024 email from Elecrow marketing rep) Further reading Forget spaceships; I just want my music If AI chatbots are the future, I hate it I'm thankful for GitHub, Patreon, and my sponsors this year ai artificial intelligence clones youtube video elecrow ethics Add new comment Comments Jimmy – 14 hours ago That’s so shady. Someone definitely intentionally trained an AI on your videos. In the best case, this was a proof of concept that got accidentally released. That demonstrates gross incompetence that brings everything else they do into suspect. At worst, how dare they! I definitely won’t be purchasing any more of their products. Reply Gary Hayman – 13 hours ago Love your real videos, this is terrible. Reply Kayla – 11 hours ago Wow. This was completely believable until they slipped up after 01:50 with “obtain-ed”, “cir-kew-it diagram”, and “user man-u-el”. Scary stuff. If you did want to fight, surely there is a voiceprint analyzer that would prove the similarity. Reply BC – 11 hours ago Jeff they most likely used eleven labs Reply Mike Robinson – 11 hours ago Hi Jeff, you may find an episode of the BBC's Tech Life podcast interesting (https://www.bbc.co.uk/programmes/w3ct5wml) called \"the voice cloning lawsuit\". The company was making AI voices available from samples recorded legitimately but for testing purposes only. Reply George – 10 hours ago It doesn't sound like you at all, just has similar generic cadence that every other YouTuber is using. Reply mam – 10 hours ago Well I und erstand that this is a shocking Moment, to hear your own voice saying words you didnt said. But AS you point out the Situation Seen to be unclear and personally I Bad bot in DirectX contact before you go public. Beide the legal sie there is AFAIK always the possibility of just stupid actions of single employes etc... ;) BTW: Would you share voice samples to Common Voice repo to power FLOSS voice assistants? Reply Anonymous – 9 hours ago https://www.youtube.com/watch?v=IeTybKL1pM4 Reply H – 7 hours ago You can blame elevenlabs.io for this Reply John – 6 hours ago This is extremely illegal. Contact an entertainment lawyer. Reply",
    "commentLink": "https://news.ycombinator.com/item?id=41614490",
    "commentBody": "They stole my voice with AI (jeffgeerling.com)464 points by sounds 15 hours agohidepastfavorite350 comments ryzvonusef 10 hours agoEveryone has their own fears about AI, but my fears are especially chilling; what if AI was used to imitate a person saying something blasphemeous? My country is already has blasphemy lynching mobs based on the slightest perceived insult, real or imagined. They will mob you, lynch you, burn your corpse, then distribute sweets while you family hide and issue video messages denouncing you and forgiving the mob. And this was before AI was easy to access. You can say a lot of things about 'oh backward countries' but this will not stay there, this will spread. You can't just give a toddler a knife and then blame them for stabbing someone. Has nothing to do with fame, with security, with copyright. This will get people killed. And we have no tools to control this. https://x.com/search?q=blasphemy I fear the future. reply losvedir 7 hours agoparentI think the answer, counterintuitively, is to make these AI tools more open and accessible. As long as they're restricted or regulated or inaccessible people will continue to think of videos and recordings as not fakeable. But make voice cloning something easy and fun to do with a $1 app, let the teens have their prank call fun and pretty soon it should work its way into the public consciousness. I had my 70 year mother ask me last week if she should remove her voicemail message because can't people steal her voice with it? I was surprised but I guess she heard it on a Fox segment or something. I think it might be a rough couple years but hopefully we'll be through it soon. reply HeatrayEnjoyer 6 hours agorootparentThis is idealistic. People still haven't fully learned that images can be photoshopped in its twenty years of its existence. (Deep)faked porn is still harmful which is why it's a crime. Worse, there isn't an attitude of default skepticism in many areas/cultures. If a person is suspected of violating the moral code the priority will be punishment and reinforcing that such behavior isn't acceptable. Whether or not the specific person actually did the specific act is a secondary concern. It's just going to increase the number of people who will be harmed or killed. reply deepsun 6 hours agorootparentYep, had a lawyer telling me that image timestamp cannot be faked. While it's literally a right-click away. reply sugarkjube 45 minutes agorootparentWell, thing is, most people can't. In a lawyers view, and a judge's view, some skilled expert \"hackers\" can, and its called hacking. (so i guess we're all hackers) I once discussed these things with a (knowledgeable) lawyer. He explained you can just present almost anhthing in a court case, and when it isn't refuted, well then it's valid. In a case my lawyer (same one) presented a printed out email. Other party did not claim it was false, so it's suddenly just as valid as a registered letter. (it was a genuine email). In another unrelated case, the other party suddenly introduced a forged picture. If I hadn't been there at that moment (I wasn't supposed to actually), then suddenly it would have been proof. Court cases are not about truth, and not about justice. They are about convincing the judge. reply veunes 3 hours agorootparentprevWhen people in positions of authority, like legal experts, don’t fully grasp how easily digital content can be manipulated reply tikkun 5 hours agorootparentprevI'll note that both photoshop and changing the timestamp of images (mentioned below) are only easy for a very small percentage of the population. It'd likely be different if >30% of people could easily do these things. reply tga_d 49 minutes agorootparentChanging an image timestamp -- as in, exif metadata -- is trivially easy for anyone with a computer or phone, a quick search will tell you how to do it on any device with no skill involved. There's a difference between \"easy for a small percentage of people\" (modify the content of a photo in an undetectable way, find a software vulnerability, etc.) and \"only a small percentage know how to do it\" (modify exif values, do a simple magic trick, etc.). Just because someone doesn't know anyone can do it if they tried doesn't mean it's not trivial and pervasive. reply bryanlarsen 3 hours agorootparentprevMost people will believe a rumour if it is told to them in person by a friend. We've had our entire evolution worth of time to recognize that rumours can be manipulated yet rumours still spread and are still very dangerous. reply CoastalCoder 6 hours agorootparentprev> I had my 70 year mother ask me last week if she should remove her voicemail message because can't people steal her voice with it? I was surprised but I guess she heard it on a Fox segment or something. Out of curiosity, how much training data is needed currently to mimic a voice at various levels of convincingness? reply HeatrayEnjoyer 2 hours agorootparentAlmost none, at most as little as a professional impersonator requires. GPT-4o's advanced voice mode would clone the user's voice by accident. A recording clip of one incident is available: >During testing, we also observed rare instances where the model would unintentionally generate an output emulating the user’s voice. >Example of unintentional voice generation, model outbursts “No!” then begins continuing the sentence in a similar sounding voice to the red teamer’s voice https://assets.ctfassets.net/kftzwdyauwt9/4CG0G7y9WOfEkzBpi7... https://openai.com/index/gpt-4o-system-card/ reply alternatex 6 hours agorootparentprevSkype does it in real time (for live translation) with a few seconds of audio. For the reasons discussed in this thread, it continuously forgets its previous training from the call to not make the voice too similar, but just similar enough to distinguish the speakers. reply jerpint 6 hours agorootparentprevJust a few seconds of speech gets you pretty far with a lot of models reply caeril 3 hours agorootparentYes, this is generally true, in order to clone the timbre/pitch/tonality, but if you're going to run a voice cloning scam to fool friends and family members, you need much more to get the cadence, pauses, vocal tics, etc right. reply kmlx 9 hours agoparentprev> what if AI was used to imitate a person saying something blasphemeous? > My country is already has blasphemy lynching mobs in your case the problem is not AI, it’s your country. reply ryzvonusef 9 hours agorootparentYour country might not have lynching mobs, but you can't deny there are certain taboo topics in your society also, certain slurs and other opinions which would take you ages to clense and even then never fully. If an AI fake-porn of some ordinary person involving a minor was unleashed, think of the utter shame and horror they would be treated by people for the rest of their lives, even if it were proven false. No one would believe them, work with them, hire them, rent them, they would wish they had been lynched instead of the life they live. reply t0bia_s 13 minutes agorootparentNext or next next generation would be used to it. Digital content would be consumed differently, with automatical scepticism or so... Remember when people believed that camera took our soul when we are caotured on image? reply nkrisc 6 hours agorootparentprevYes, you’re right. That’s why the problem is the lynch mobs. If there was no AI, people would find another way to sic the mob on someone. I’m sure it’s already happened countless times without AI. Mobs aren’t known for the rational behavior and respect of the rule of law. reply ryzvonusef 5 hours agorootparentI was simply explaining my personal fears regarding AI given that lynch mobs exist in my country; the lesson you should have taken from that was not about lynch mobs, but to analyse how AI might affect YOU in your particular environment. Remember the post is about cloning a youtuber's voice using AI; most people were thinking of the copywrite aspect of it, but I wanted to share with people what my fear was. My fears are, things are already bad w.r.t lynch mobs before AI, after AI things will only get worse with fake but realistic sounding voice notes etc. That's my fear, doesn't have to be yours. You don't have lynch mobs (hopefully) but surely you can think of other problems that a fake AI image/audio/video can cause, and just low the barrier is, and just how good the tools are. reply nuancebydefault 2 hours agorootparentMaybe it's because I have no knowledge of lynch mobs in my country but I find that fear is a bad adviser. I think currently there is no way back in AI development. I read Microsoft is getting a failed nuclear reactor repaired to power the power hungry AI. The only way forward is making sure most people know that digital content can be easily faked. If you hand out good faker tools for free, it will happen faster reply noobermin 1 hour agorootparentThere is absolutely a way back if laws change. It's that simple, people need to stop pretending its inevitable when it absolutely isn't. reply nkrisc 4 hours agorootparentprevI don’t disagree that AI could exacerbate the issue, in fact I agree. But anything could potentially do so. Instant messaging likely also made the problem worse as a call to form a mob can reach many people much faster than before. The time from inciting incident to mob action is greatly compressed because of instant messaging. It’s fundamentally a societal issue, not a technological one. Yes, it’s scary what effect AI could have on lynch mobs, because lynch mobs are scary. reply roenxi 9 hours agorootparentprevBut mob formation and mass social shaming on little evidence already happens without AI and it isn't clear why AI would make it worse. If it gets so easy to create fake videos that'll just muddy the waters and people would trust rumour less, not form mobs more. It happens once, then some wag creates videos of all the mob leaders blaspheming or whatever. Undermines the idea that the videos are the root cause. reply ryzvonusef 8 hours agorootparentYou are giving too much credence to the analytical capabilites to a lynch mob, and you are underestimating how much AI lowers the barrier for creating realistic 'proof' for incitement. Also there are no mob 'leaders'. These are everyday folks, these are you and me and my neighbours. A student pissed at a teacher could create a fake AI video of his teacher at night, and wake up to no pesky teacher and feel no remorse over what happened while he slept. The student was not a mob leader, just some dumbass who was angry at their teacher for giving bad grades, and had access to simple AI tools (soon available online for a trivial fee, if not free). ____ Be honest, are you 100% sure of the status of that facebook account you deleted in 2018? Zuck never deleted it, that content is still there to be mined and abused. Also not that hard to just log in to the damn thing and post something spicy. I don't remember if old posts can be edited in FB, cause that would allow the person to really gild the lily. reply benterix 8 hours agorootparentprev> people would trust rumour less, not form mobs more I believe you are right. When the first unbelievable clips appeared on Facebook (where the most gullible people are), you could see the old ladies cheering \"oh how wonderful! where did you film that?\" Now in the comments section you will mostly see people saying \"sheesh, stop polluting my feed with these fake AI-generated spam\". Don't get me wrong, many people still fall for them, but the tendency seems clear - after all, we managed to survive because of learning. reply forgetfreeman 6 hours agorootparentprevI feel like the Bullshit Asymmetry Principle is involved here. If the bar is lowered to the point where it takes less energy to generate inflammatory video \"evidence\" than it does to debunk it the shitheads win. I feel like we can all agree that AI has the potential to lower that bar. reply pjc50 8 hours agorootparentprevThe US equivalent is much less labour intensive than a lynch mob: it's mass shooters radicalized by things they've read on the internet. Or https://www.npr.org/2024/09/19/nx-s1-5114047/springfield-ohi... , where repeating racial libel causes a public safety problem. While this kind of incitement in no way requires AI, it's certainly something that's easier to do when you can fake evidence. See also https://www.bbc.co.uk/news/articles/c5y87l6rx5wo reply rustcleaner 1 hour agorootparent>radicalized by things they've read on the internet That take pretty much throws the shooters' preceding years of anguish under the bus to make a political score. \"Ban the wrongspeech!\" is a seductively easier proposition than ending youth bullying and ostracism. reply fragmede 8 hours agorootparentprevhttps://apnews.com/article/ai-maryland-principal-voice-recor... In the US, freedom of speech means the government won't stifle you, but the court of public opinion will crucify you for saying the wrong thing. In the linked case, a school principal was framed for being a racist, a fire able offense. Better than being killed by religious extremists, I suppose, but still not great. Thankfully in this case they were able to find the perpetrator who faked evidence, but we have a problem. reply psychlops 6 hours agorootparentprevThe internet isn't always the bogeyman, the last assassin was radicalized by mass media. reply datavirtue 4 hours agorootparentprevPeople don't shoot up crowds because of something they read on the internet. They do it because they are done with life. Everything after that is retrospective reasoning. reply jokethrowaway 5 hours agorootparentprevYes, but after the invention of radio and television we already enabled mass indoctrination and mass brainwashing. The internet is more refined but the same principle. The solution is not to ban technology (it's impossible), but to create a more decentralised society where the majority of voters, which is easily brainwashed, doesn't get to dictate the life of the minority. We need to destroy and dismantle all the gigantic entities that ruin our life for the advantage of the few, whether they are the government or corporations ruled by billionaires controlling the government. We also need to create a better society with less mental illness, where people are not depressed by how unfair life is that they go on and kill kids in school - but I believe if we reduce large societal inequality this will sort itself out over time. Parents in my circle are already banning social medias for their kids until they're adults - so over time we'll evolve to get better at ignoring all the crap we get exposed to. reply berniedurfee 2 hours agorootparentprevWhat country is immune to this? As far as I can tell the collective conscious of every country is swayed by propaganda. A written headline is enough to incite rage in any country much less a voice or video indistinguishable from the real thing. Folks in “developed’ countries have their lives destroyed or ended all the time based on rumors of something said or done. reply bufferoverflow 56 minutes agorootparentAny country that doesn't have executions by mobs. Used to be most nordic countries. Till they changed their immigration policies. reply flembat 3 hours agorootparentprevAn individual is not responsible for the culture or government in the country they live in. In the UK a government was just elected with a historical absolute majority by only ten million people, and now first time offenders are being sent to prison for making stupid offensive statements online. reply switch007 3 hours agorootparentAre you referring to this case? > In a now deleted post on her X account, Mrs Connolly, from Northampton, wrote: “Mass deportation now, set fire to all the f*** hotels full of the bastards for all I care… If that makes me racist, so be it. > The court heard Kay copied, pasted and uploaded Mrs Connolly’s post at 12.27pm on Wednesday from a BBC News report and added the hashtags #standwithlucyconnolly #lucyconnolly #f**northamptonshirepolice #conservative #FaragesRiots #RiotsUK and #Northampton. > As well as a post which urged people to “mask up” during a protest targeting an immigration law firm, Kay tweeted to his 127 followers at 2.34am: “That’s 100% the plan, plus gloves. No car either so no number plate to trace and a change of clothes ready nearby.” https://www.independent.co.uk/news/uk/crime/northampton-bbc-... reply 7bit 9 hours agorootparentprevThat's a little too easy no? AI being used to imitate people definitely is a problem that needs to be addressed, and already is. Discarding that because there is a bigger issue is ignorant. Both can exist as a problem at the same time. reply Ygg2 7 hours agorootparentprevThe problem is AI. What if you post video of a politician eating babies, and that causes some nutjob to kill that politician? Sure, distrust everything digital, but what if only evidence of someone doing something wrong is digital? reply tomjen3 3 hours agorootparentWhat if someone printed yellow papers blaming Spain for a ship accident so the us would go to war with them? reply charlieyu1 6 hours agorootparentprevPeople copied and pasted faces of celebrities on another photo since 1990s. Nothing significant happened. reply ryzvonusef 6 hours agorootparentproblem is not celebrities, problem is general every day people. With celeberaties, atleast they are famous enough that the idea of fakes exist... but you are Johnny nobody, that's not a fake, you just got caught and don't want to admit you said/did it! The court of public opinion is much smaller and personal, and 'evidence' is much more realistic and detailed. reply Nathanba 5 hours agorootparentgeneral every day people will have to figure out that photoshop exists and that it contains AI now that can invent pictures from scratch. It should be easy to figure out, just explain that it can paint 100% realistically like a camera. The real issue is how people will start using this as a defense for criminal acts. \"it wasn't me, all the camera footage is AI generated\" reply jokethrowaway 5 hours agorootparentprevBecoming famous always had its risks because of mentally ill people targeting you. Nothing new. The trick is being rich without being famous! reply latexr 9 hours agorootparentprevThe comment didn’t say the problem was AI, it said they feared its consequences, which is a perfectly valid concern. It’s like if someone said “I’m scared of someone bringing a semi-automatic weapon to my school and doing a mass shooting. My country has lax laws about guns and their proper use”. And then you said “in your case the problem is not guns, it’s your country”. I mean, it’s technically true, but also unhelpful. Such ingrained laws are hard to change and you can be placed in danger for even trying. Before someone decries the gun example as not being comparable, it is possible to live in a country with a monumental number of guns and not have mass murdering every day. It’s called Switzerland. But let’s please stick to the subject of AI, which is what the thread is about. The gun example is the first analogy which came to mind, and analogies are never perfect, so it’s unproductive to nitpick the example. I don’t mean to shift the conversation from one contentious topic to another. reply pjc50 8 hours agorootparent> It’s like if someone said “I’m scared of someone bringing a semi-automatic weapon to my school and doing a mass shooting. My country has lax laws about guns and their proper use”. And then you said “in your case the problem is not guns, it’s your country”. The US has both problems: widespread availability of weapons and a high level of freedom to incite violence through spreading lies about groups. Which is why it sees much more of these incidents than countries which have similar levels of gun ownership. The non-gun version of the problem is mass stabbings, which are less lethal. reply underdeserver 9 hours agorootparentprevSwitzerland allows you to own a gun, but (generally) not to bear it. Huge difference. reply bryanrasmussen 9 hours agorootparentAmerica also has laws against taking guns into theaters and indiscriminately shooting people, once you have easy access to guns the gun becomes a potential solution to problems. Maybe Switzerland just has less problems where that potential solution appeals to people. reply input_sh 6 hours agorootparentI'd say that the key difference is that Switzerland has compulsory military service in which you're taught how to operate a weapon properly. Therefore, everyone that has one has gone through months of training. Vs the US, where there are loopholes that you can use to avoid even a basic background check, and then use it for the very first time to shoot someone. reply thfuran 5 hours agorootparentTraining seems likely to cut down on accidental shootings, but not so much on the deliberate ones. reply underdeserver 5 hours agorootparentI disagree. First of all, people with issues are more likely to be found out in boot camp. Second, you learn a certain respect for the firearm and are expected to observe strict safety rules when handling it. That gives you a kind of psychological flinch when you consider doing anything out of the norm with it. reply ambicapter 4 hours agorootparentprevIt seems like tons of toddler are accidentally firing guns as well in America, so training probably won't help there. reply thfuran 4 hours agorootparentTraining people to lock up their damn guns would. reply tharkun__ 5 hours agorootparentprevThat doesn't seem to make sense. All that would do is to make sure that a nut job knows how to properly shoot the gun. Basically what you are saying is \"a Swiss wouldn't have missed Trump\". The difference might be in what guns mean to Swiss people vs. the US. A gun in the US has this \"if the government becomes destructive of these ends we can start shooting\" connotation. A gun is there for self defense. Someone doesn't get off your law, you use your gun. In Switzerland the gun and the compulsory military service you mention is there for the people to protect their country and fellow countrymen. You are trained in defending your neighbor, who just stepped on your lawn against outside aggressors. reply smusamashah 59 minutes agoparentprevI can absolutely relate with your fear, but I think this will eventually be helpful to dismiss those mobs. Might even desensitize people boiling over 'blasphemy'. Yes, for the first few instances it will hurt. Then, eventually it will become common enough to be known by common folk. Enough that those people themselves will be sceptic enough to not act. I recall photoshop blackmailing stories where usually woman were the target. Now literally \"everyone\" knows pictures can be manipulated/photoshopped. It will take a while yes, but eventually common folk will learn that these audios/videos can't be trusted. reply Jeff_Brown 3 hours agoparentprevGiven that this tech is unstoppable, the best defense might be a good offense: Flood the internet with clips of prominent religious and political leaders, especially those largely responsible for mob violence historically, saying preposterously blasphemous things they would obviously never say. reply godelski 10 hours agoparentprev> what if AI was used to imitate a person saying something blasphemeous? I've been contemplating writing an open letter to Dang to nuke my account. Because at this time you can likely deanonymize any user with a fair amount of comments. As long as you can correlate. You can certainly steal their language, even if not 100% accurate. It may be caution, but it isn't certain that we won't enter a dark forest and there's reason to believe we could be headed that way. But at the same time, is not retreating to the shadows giving up? reply ryzvonusef 9 hours agorootparentThe problem I fear is that, let's say you once had a facebook account, we all deactivated our accounts when there was wave against Zuck a few years back, but as we know, facebook doesn't really delete your account. Now imagine that account was linked to a SIM. It's trivial for a nefarious actor to get it re-activated, infact there was a video by Veritasium just today where they didn't even need your SIM. But even if they are not that hi-tech, it's not that hard to get a SIM issued in your name, or other hacks of a similar nature, we have all heard of stories. Worse, you lost that SIM a decade back, the number gets back into the queue, and is eventually re-issued to someone new... and they try to create a facebook account, and are presented with yours. They can then re-activate your old facebook account, and post a video/audio/text of \"godelski\" saying they like pineapple on pizza. and before you can defend yourself, the pizzarias have lynched you. (I dare not use a real example even as a jest, I live here) Are you 100% sure of all your old social media accounts, all the SIM you have ever used to log-in to accounts? We leave a long trail. reply rustcleaner 1 hour agorootparentI think we need a data Great Reset: all data (and I mean all) must be deleted by a certain date in the future, and all newly collected data must have provenance so in the future the customer can find out that Ford got information from Databroker A which bought it from B which bought it from Microsoft which collected it from what you type in Windows. Companies holding activity data without provenance get the crack-cocaine dealer treatment: officers in jail, everybody fired, investors and lenders fucked, company liquidated and forfeited. While it is impossible to take back a disclosed secret, we can create a legal framework which issues immediate business-stopping corporate death sentences for spreading that data without consent (or after you revoke consent, yes it needs to be revokable). Your data is so valuable, because now that we have universal function approximators you can be simulated (to a prediction horizon) and that simulation interrogated. reply danieldk 10 hours agorootparentprevThere should be a way to cryptographically sign posts (everywhere). I know, building a web of trust sucks, etc. But if there was someone with your username signing with a particular key for 10 years and then suddenly there is something controversial somewhere with a different key, something fishy is going on. Of course, this could be misused to post something with plausible deniability, but if you want to say something controversial, why wouldn't you make another account for that anyway? I know that one could theoretically sign posts with GPG, but it would be much nicer and less noisy if sites would have UI to show something like: Signed by , key used for N years. One issues is that most social media want your identity to be the account on their service and not some identity (i.e. key) that you control. reply godelski 10 hours agorootparent> There should be a way to cryptographically sign posts (everywhere). This just confirms it is me. Which yes, reduces the problem of me being replicated, but does not do anything for my anonymity. That part may not seem important to you, considering you use your real name, but it is to me. It allows me to be more open. Key signing will not be the solution because it isn't addressing the problem. It exacerbates it. reply danieldk 9 hours agorootparentThis just confirms it is me. Which yes, reduces the problem of me being replicated, but does not do anything for my anonymity. That part may not seem important to you, considering you use your real name, but it is to me. It allows me to be more open. Ah, think I see your point. Your worry is that language use, etc. could be use to deanonymize you, by correlating with text that was not written anonymously. But that's a separate issue from voice or writing style cloning to pretend it's you that said it. In the latter case you could use a pseudonymous signing key? I agree that deanonymization is an issue that is hard to tackle. I wonder if someone studied how unique writing style is. E.g. browser fingerprints are fairly unique, but I wonder to what extend you can filter a person from, say a pool of 100 million, using writing style alone (grammar, vocabulary use, etc.). I guess it becomes quite easy if you engage in a lot of domain-specific discussions and use their vocabularies. E.g. if I'd talk about Marlin-kernels here, you could probably narrow me down to a few hundred people. Throw in another comment about the Glove80. Maybe ten people at most? reply godelski 38 minutes agorootparent> I wonder if someone studied how unique writing style is. Since I teach I can tell you that I can usually tell who wrote something by their language and it even works with code. There's also the Enron dataset, which is a common dataset for first time ML students where you do exactly this task. Your language is in fact a fingerprint. And like you suggest, topics too. Much of our freedom of anonymity comes from the fact that it is hard or not worth it to dox people. I do agree that verification is a different issue though. I'm not sure keys will solve it because you're not going to sign anything that is scandalous, so it might even give evidence for those that want to falsely claim foul play. And how do you sign a leak? The problem with signing is that it seems to work for the cases we don't care about and do nothing for the ones we do. That is unless we sign literally everything, including our voice, but then you kill anonymity (why I connected it) and you could then probably clone that too. reply vnorilo 10 hours agorootparentprevNot sure the lynch mob would pause to check that the web of trust is unbroken. reply aktenlage 9 hours agorootparentprevAnother solution would be to use an LLM to rephrase your posts, wouldn't it? Not a great outlook though, if everybody does this... reply rustcleaner 1 hour agorootparentI already do this in all my IRCs, Matrices, and Discords reply godelski 34 minutes agorootparentprevNot really. Though that will average language and poison LLMs. I'm not sure I want either of those to happen. Besides, topics and shared details are also pii. It's more that they aren't useful in that way until you have scale. Plus, on HN you can't edit posts after some time. reply yreg 8 hours agorootparentprevI treat my accounts an non-anonymous unless I use a single-use throwaway. I suppose even a throwaway could be linked to my identity if a comment was long enough, but probably only with some limited certainty. reply godelski 29 minutes agorootparentI treat mine as semi. I mean I'm not trying to hide from the government or anything, but yeah I'll say things I might not want to say under my clear name either. Don't we all? We all wear different masks in different groups. We all are different people in different groups too. We're human, we all have stuff to \"hide\". I might want to vent about my boss or work and if they heard it that would certainly take it different than it was intended. We even complain about our friends, especially our close friends. Because no relationship is without conflict. But being human we often have to actualize our feelings with words, because we're social creatures. I'll at times complain about my partner, but that doesn't mean they also aren't my favorite person in the world. To be human requires some anonymity. And no one should feel like they're going to be scrutinized for every little thing they say. reply kossTKR 9 hours agorootparentprevYep, im sure lots of people have written a lot of random stuff on a lot of forums that should absolutely stay anonymous from gossip to family secrets to honesty about career/workplace and what not. If stylometric analysis runs on all comments on the internet then yeah. Bad things will happen, very very bad. I honestly think it should be at least illegal to do this kind of analysis because it'll be a treasure trove for the commercial sector to mine this data correlated to real people not to think of the destruction in millions of people with personal anonymous blogs etc. Actually thinking about it further you could also easily group people political affiliations, and all kinds of other thoughts, dark, dark stuff! reply photonthug 9 hours agorootparentIdk the state of the art for forensic stylometrics or the precedents but I would be surprised if this hasn’t already been presented as evidence in many (most?) notable jurisdictions. Not definitive evidence but supporting evidence. So far from being declared illegal, it’s probably mostly established as a tool for law enforcement. Meanwhile gangsters or nation states are probably already working on automated deanonymization and blackmail at scale, but will target the dark web before the public one. Not sure any of this stuff is that changed by the advent of deep fakes and llms tho, probably just boring classical statistics does ok? reply rustcleaner 1 hour agorootparentprev>Actually thinking about it further you could also easily group people political affiliations, and all kinds of other thoughts, dark, dark stuff! Already happening at Google and Meta for a decade or more. reply greenchair 8 hours agorootparentprevI wonder if in this timeline it would cause people to clean up their behavior online and only post things they are comfortable having being linked back to their real identity. Would have a chilling effect. reply godelski 26 minutes agorootparentThere's quite a few sci-fi books, movies, and TV shows that explore this concept. Probably the most well known is 1984 But I'm pretty sure this would make life very bland. Slow down innovation as we become less creative, less willing to deviate from the norm, and the Overton window of what's socially acceptable narrows. After all, we do love having enemies in some way or another, even if it means making them up. reply Der_Einzige 1 hour agorootparentprevStuff like this is why I tell everyone who does good work in the AI space to remember that they are watched reply shevekofurras 8 hours agorootparentprevYou can't nuke your account. You can close it but your comments remain on the site. They'll delete your account and assign your comments and posts to a random username. Yes this violates any EU citizen's right to be forgotten under GDPR. Welcome to silicon valley. reply godelski 24 minutes agorootparent> You can't nuke your account. You can close it but your comments remain on the site. Which is why I'd write an open letter and not do the thing. If I could nuke my account I wouldn't need to ask Dang, and I would have already done it. reply whamlastxmas 2 hours agorootparentprevDoes every comment previously attached to your account have a unique username or do they still all share the same? Sort of moot point considering the multiple HN archives that would still have the original username attached reply Der_Einzige 1 hour agorootparentprevHow would one report this to the EU and force HN to follow the GDPR? reply kyboren 59 minutes agorootparentAFAIK Y Combinator does no business in the EU and HN is hosted in a US data center. Under which legal theory does EU law apply here? reply pnut 10 hours agoparentprevI guess then, you should use AI to generate videos of all of the lynch mob leadership committing blasphemy and let them sort it out internally? reply ryzvonusef 10 hours agorootparentYou joke but, given the religious/sectarian nature of the issue, all it does is empower one sect to act against the leaders of the other sect. Check the twitter link, you won't have to scroll much to find a mullah being blasted for blasphemy. No one is safe. reply javcasas 6 hours agorootparentWell, then target dead people. Find someone who died 6 months ago, have AI make them a blasphemer, have the sect look for dead people. After the third or fourth time, they'll stop. Not because they want to stop, but because they don't trust this being another prank by Mullah Achmed the Fool. reply ryzvonusef 6 hours agorootparentJust like how you are the traffic, you are also the mob Mobs are not formal organisations with leadership and membership dues. Mobs are general everyday people who are sick and tird of this miserable life and need an excuse to rabble and rouse. So in this case, fake dead person's blasphemy will only be channeled towards their family members, mobs just need an excuse to get crazy. AI is not what causes mobs, it just helps lower the incitement barrier by providing quality bait. reply javcasas 5 hours agorootparent> fake dead person's blasphemy will only be channeled towards their family members More alternatives: fake full persons. The mobs will be confused when they are directed to hunt down someone with a family name in a neighbor where no one has that family name. Furthermore, send them to the car repair shop between the coffee shop and the market in a street with no car shops, markers and coffee shops. reply javcasas 5 hours agorootparentprevIn every group there are leaders (even informal ones, like that woman that shares nonstop all the bullshit to her 347 facebook friends). Make them into fools. - Stone the blasphemer! - Who? The one that died las year? The one that died in the war in '94? The one that left 4 years ago? Don't you have anything better to do other than fooling yourself and spreading more rumors? Doesn't make it any less dangerous while the mass is being vaccinated against bullshitters. reply vasco 10 hours agoparentprevThe best we can hope for is that one personally avoids this for the first 5 years or so, and then it gets so widespread and easy that everyone will start doubting any videos they watch. The same way it took social media like reddit a few years of \"finding the culprit\" / \"name and shame\" till mods figured out that many times the online mob gets it wrong and so now that is usually not allowed. But many people will suffer this until laws get passed or it enters into common consciousness that a video is more likely to be fake than it is to be real. Might be more than 5 years though. And unfortunately laws usually only get passed after there's proven damage to some people from it. reply pjc50 9 hours agorootparent> everyone will start doubting any videos they watch. This kills the medium. Just as ubiquitous scam calls have moved people away from phones, this moves people away from using media which cannot be trusted. Done enough this destroys reporting and therefore democracy. I wonder when the first nonexistent candidate will be elected. reply GreenWatermelon 6 hours agorootparentThis sounds like saying Text, as a medium, is already destroyed. But as we can see, despite thousands of years of fraud potential, we still use text a medium. We're back to needing witnesses and corroborating evidence. reply BurningFrog 9 hours agorootparentprevWe've had Photoshop for decades, and I still see pictures everywhere. reply latexr 9 hours agorootparentAgain this tired argument. Photoshop requires skill and time. AI generation takes a few seconds of typing some words. The scale is not comparable. reply wpm 4 hours agorootparentAnd let’s not pretend image generation AI isn’t already in a state where it can pump out convincing slop. Facebook is full of it. reply vasco 5 hours agorootparentprevI'm not sure I can take seriously arguments of \"killing democracy\", that undermines whatever point you're making in the same way people that worry about crime and shout \"think of the children\" or immediately go to \"stopping terrorism\". Just make your point without hyperbole. reply Eisenstein 9 hours agorootparentprevIt is inevitable at this point that video as 'proof' will be killed. That we cannot do reporting and it destroying democracy is a little too much 'sky is falling'. Democracy existed before video. reply bufferoverflow 1 hour agoparentprevIt's sounds like a problem with your crazy population, not with AI. reply movedx 8 hours agoparentprevOne way that we technical folk can help prevent this is by purchasing a domain that we can call our own and then host a website that's very clear: \"If my image or voice is used in a piece of digital media that is not linked here from this domain, it was not produced by me.\" That, and cryptographic materials being used to sign stuff too. I think that's possibly the best we can hope for from a technical perspective as well as waiting for the legal system to catch up. reply ryzvonusef 8 hours agorootparentBut, but, it sounds so realistic! Listen kiddo, I dunno what 'cryptographic signatures' are, all I know is it sounds exactly like movedx saying they likes pineapple on pizza, and I know what I heard, sounds just like them, heard them dozen of times on TV, must have been an undercover journalist who exposed them, I say a person who likes pineapple on pizza is not welcome in my house whatever you say, now be gone! reply yapyap 8 hours agorootparentused the wrong example there, pineapple on pizza is a nice, delicious, refreshing topping reply karlgkk 7 hours agorootparentdid the point of the analogie fly over your head so completely reply ryzvonusef 8 hours agorootparentprevDude I am not going in give real examples, I live in a country with lynch mobs, remember? But the point was, doesn't have to be lynch mobs, for developed countries, even 'lesser' consequences are stull terrifying. Imagine everyone in your office giving you the stink eye after lunch and you being fired by HR 'at-will', and not knowing what exactly caused it, after all the day started so normally... reply latexr 8 hours agorootparentprevFine, then use dead rats on pizza. Don't focus on the letter of the example when you clearly understand the spirit. Conversation on HN aims to be better than that. Don’t reply with shallow dismissals, steel man the other person’s argument. Your account is a week old and half your comments are dead. I recommend referring to the guidelines. https://news.ycombinator.com/newsguidelines.html https://en.wikipedia.org/wiki/Straw_man#Steelmanning reply badgersnake 6 hours agorootparentprevYou are wrong. reply Popeyes 7 hours agorootparentprevBut that doesn't account for the situation, of course you aren't going to post the illegal stuff you say. And then that gives you a blank to cheque to say what you like in private and say \"Well, it isn't on my site, so it must be fake, right?\" reply johnnyanmac 7 hours agorootparentprevHonestly, we're in a post truth era at the moment. There's so much misinformation out there that a 5 second google query can disprove, but it doesn't solve any arguments. That kind of cryprographic verification will only help you in court. There will probably be irrevocable pr damage even if you win that court case though. reply sureglymop 9 hours agoparentprevMy specific fear is that if a picture of you next to your name is available online, that becomes part of the training set of a future model. Paranoically, I do not have any picture of myself available online. I could then trivially generate pictures or even videos of you e.g. by knowing your name. Of course that's just an example but I do think that's where we are headed and so the concept of \"trust\" will change a lot. reply criddell 7 hours agorootparentDo you have a state driver’s license? If so, then chances are data brokers have your photo from that. https://www.dallasnews.com/news/watchdog/2021/03/19/its-mind... reply sureglymop 5 hours agorootparentI am not from the US so no. And when I did visit the US, I was young enough not to have to give my fingerprints at the airports. reply criddell 5 hours agorootparentIf it was a while back, your passport probably wasn’t the kind with a chip. If you visit today, your passport photo is electronically readable. I have no idea if the feds are as sloppy with this data as the states are though. reply cudgy 6 hours agorootparentprevAnd your fingerprints … reply marginalia_nu 9 hours agorootparentprevSeems like the end game for that technological development is kind of self-defeating. Once it's 2 clicks away to generate a believable video of someone at the kkk kitten barbecue getting along with ted bundy and jeff epstein, surely the evidence value of that would dwindle, and the brief period in history when video evidence was both accessible and somewhat believable will come to an end. reply thfuran 5 hours agorootparentIt turns out that humans aren't perfectly rational actors. Actors get hatemail and death threats because of roles they've played, \"evidence\" that isn't just potentially suspect but actually known to be entirely fictional. reply cess11 9 hours agorootparentprevLynchings aren't commonly waged based on a solid process of evaluating evidence. reply marginalia_nu 9 hours agorootparentIn that regard, the presence of this type of evidence isn't worth much either, a malicious rumor or an accusation is plenty sufficient to stoke the flames of an angry and terrified mob, nobody's going to pause the proceedings and wait for a fair and cool-headed judgement of evidence. This is after all a phenomenon that is far older than the smartphone. reply blueflow 9 hours agoparentprev> And we have no tools to control this. Do you know \"The boy who cried wolf\"? Fabricate some allegations yourself and this will train people to disbelieve them. reply ryzvonusef 9 hours agorootparentDoesn't work. You are assuming that people who are part of lynch mobs have the critical thinking skills to differentiate between real vs fake, and use logic. Reminds me of the post I read on twitter, of some Thai/Chinese New Yorker whose mother told him not to speak Mandarin in public when COVID related Anti-Asian hate was rampant.... And he had to explain to her that she can't expect the sort of person who hits a random Asian to differentiate between Thai and Mandarin. reply latexr 9 hours agorootparentprevThat sounds like a dangerous proposition. Either they fabricate allegations about a “nobody” and put them in trouble or they fabricate allegations about those in power and will be investigated and put themselves in danger. Neither strategy is good. reply GreenWatermelon 6 hours agorootparentIt doesn't have to be dangerous allegations. Fabricate a video of your country's dictator carrying a giant Boulder, with a flattering subtitle. This will send a message that videos could be deepfaked to such a degree. reply blueflow 9 hours agorootparentprevIt worked pretty fine with #MeToo. Not saying there was no collateral damage, but the end result was that people now default to not believing such allegations. reply latexr 8 hours agorootparentI don’t understand what you’re arguing, could you be more specific? I know about #MeToo, but what exactly are you saying worked, and what are you claiming people default to not believing? reply veunes 3 hours agoparentprevThe analogy of handing a toddler a knife is spot on. AI is an incredibly powerful tool, but without proper safeguards, regulations or education, it can cause irreparable harm reply charlieyu1 6 hours agoparentprevFrom Hong Kong. We already had fake audio messages that sounded like a protest leader during 2014 protests… It was always there, even a long time ago reply mrkramer 8 hours agoparentprevThe only logical legal solution is that any content of you shared by you is legitimate one and all other content of you shared by somebody else is presumed non-authenthic and possibly fake. reply F-Lexx 8 hours agorootparentWhat if a third party gains access to your social media account(s) and starts posting fake content from there? reply mrkramer 3 hours agorootparentMy view on deepfakes: I was thinking about deepfakes and how to protect from them and my conclusion is; there is no way you can protect from them from the practical point of view but from the legal point of view, governments can make laws where everything shared by somebody else that involves you is presumed fake unless there is substantial circumstantial evidence that says otherwise e.g. witnesses, legitimate metadata etc. etc. Even before LLMs and GenAI, Photoshop became a synonym for messing around and faking photos so there is nothing new here but now there is more powerful \"faking\" software available to the masses. Before computers and digital mass media sharing some compromising photo or tape could've been assumed authentic but now in the era of computers and software there is no way you can tell that something is authentic for sure. >What if a third party gains access to your social media account(s) and starts posting fake content from there? You can cause chaos and bad press in the short-term but when the original owner of the account restores ownership of the account everything falls apart. Like the commenter below said it happens all the time and it doesn't have any real impact on anything whatsoever. reply cynicalsecurity 6 hours agorootparentprevIt happens all the time. They usually post spam and scam. reply fennecbutt 3 hours agoparentprevThe people are the problem not the tool. reply gwervc 8 hours agoparentprevThis is nothing to do with AI but with intolerance of a certain religion. That religion is killing a lot in my country and many others too, but both the governments (national and supranational) and corporations censor any criticism of it. Even here on HN I got posts and accounts removed by the moderation for the slightest hint of criticism against it, and fully expected a downvoting mob by writing this comment. Sadly, it'll will continue for a long time giving how taboo the subject is. reply sensanaty 5 hours agorootparentIf you were in the US and someone were to make a deepfake of you saying a racial slur, do you think you'd fair better than if you were a blasphemer in a Sharia country? The religion isn't the (whole) issue here, this situation can apply in the secular West just as easily. The punishment won't be death, but it can still ruin people's lives. A fake pedophilia accusation comes to mind, where even if proven innocent you'll still be royally fucked for the rest of your life unless you spend considerable expense and effort. reply thfuran 5 hours agorootparent>If you were in the US and someone were to make a deepfake of you saying a racial slur, do you think you'd fair better than if you were a blasphemer in a Sharia country? Of course. reply ryzvonusef 7 hours agorootparentprevYou are focusing too much on 'that' religion and not realising that parallel analogies exist for other countries, religions and culture too. Sure, not lynch mobs, but AI-generate fake media can certain ruin people's lives, and unlike photshop etc, the barriers of skill and time required are very low, and the quality is very high. I share my country's experience because I wanted to share my personal perspective and fears, but please don't under estimate how AI can affect you. Just because you won't be death doesn't mean they can't turn you into a social pariah with a few clicks. reply valval 4 hours agoparentprevYou’d simply make such things highly illegal. No matter how I spin it in my head, there’s nothing particularly scary about this, like there isn’t about identity theft or any other crime, in reality. Even if blasphemy is illegal in your country, people would probably agree that falsely accusing someone of blasphemy is also wrong. reply zwirbl 3 hours agorootparentLynching someone is highly illegal, whatever the cause. And yet... reply benterix 8 hours agoparentprevI'm very sorry to say this but if you live in a country that is killing others for what they say, AI is probably not your biggest problem. And I don't believe an easy solution exists. reply ryzvonusef 8 hours agorootparentAI doesn't create problems, but AI certainly lowers the barriers and improves the 'quality' of the bait. To explain for a more developed country context, the fakes that previously required skill in Photoshop and Audacity etc now is much simpler to implement with AI, allowing far more dipshits to create and share fake image/audio/video of someone they are pissed at during their lunch break on their phone. That's way too quick, allowing people to shoot far too many arrows in a huff, before their reasonable brain has time to make them realise the consequences of their actions. reply HeatrayEnjoyer 2 hours agorootparentprev\"You can't refuse this brand new technology but you must change your society's culture that's been around for centuries so you are compatible with it.\" is a repulsively Silicon Valley answer. reply progbits 1 hour agorootparentNot every culture deserves to live on, certainly not just because \"it has been around\" and absolutely not one that lynches people. Tolerance of intolerance is a bullshit strategy that won't work. reply firtoz 9 hours agoparentprev> You can say a lot of things about 'oh backward countries' but this will not stay there, this will spread I'm sorry, but this is a cope out. The \"lynching from apparent cultural deviation\" is something that needs to be moved on from. Developed countries do the same too to some extent, with \"cancel culture\" and such. There are ways to have progress in this, and, well, to feed someone's entrepreneurial spirit, it's one of those really hard problems that a lot of people, let's say, \"a growing niche market\", needs it to be solved. reply ryzvonusef 9 hours agorootparentIndeed, if one were to post a AI video of someone saying some racial slur or otherwise verboten language, sure it won't get them killed, but given how unemployeable and pariah they would be, that would be a death by a thousand cuts. But Blasphemy by whatever means, is one of the tools by which society sets certain boundries, and it's really hard to move away from a model that worked so 'well' for us since the first civiliations. reply loceng 7 hours agoparentprevWe have ourselves. We have to create a culture of learning to quell reactive emotions - so we're less ideological and more critical thinker. reply pmarreck 7 hours agoparentprev> My country is already has blasphemy lynching mobs based on the slightest perceived insult, real or imagined. They will mob you, lynch you, burn your corpse, then distribute sweets while you family hide and issue video messages denouncing you and forgiving the mob. Blasphemy laws—and the violence that sometimes accompanies them—are a cultural issue, not a technological one. When the risk of mob violence is in play, it's hard to have rational discussions about any kind of perceived offense, especially when it can be manipulated, even technologically, as you pointed out. The hypothetical of voice theft amplifies this: If a stolen voice were used to blaspheme, who would truly be responsible? This is why we must resist the urge to give into culturally sanctioned violence or fear, regardless of religious justification. The truth doesn’t need to be violently defended; it stands by itself. If a system cannot tolerate dissent without devolving into chaos, then the problem lies within the system, not the dissent. “An appeaser is one who feeds the crocodile, hoping it will eat him last.” - Winston Churchill reply ryzvonusef 6 hours agorootparentYou are focusing too much on my specific problem instead of using it as a guide to understand your own situation. Sure we have mobs and you don't, but we are talking about AI here. Infact let's imagine a totally different culture to illustrate my point. Imagine you are an Israeli, and people in your office have a habit of sending Whatsapp voice notes to confirm various things instead of calls, because that way you can have a record but don't have to type every damn thing out. Totally innocent and routine behaviour, you are just doing what many other people do. A colleague pissed at you for whatever damn stupid reason creates a fake of your voice saying you support Hamas by using said voice notes, using an online tool that doesn't cost much or require much... are you saying just because you won't be lynched, that there isn't a problem? You are confused why everyone is pissed at you and why suddenly your boss fired you, and by the time you find out the truth... the lie has spread to enough people in your social circle that there is no clearing your name. Think of how little data in voice samples is required to generate an audio clip thats sounds very realistic, and how better it will get in an year. You don't need fancy PC or tech knowledge for that, already websites exist that do for cheap. Just because you weren't lynched is no solace. People are the problem, AI is just providing quality tools with minimal skill and cost required, thus broadening the user base. reply cynicalsecurity 7 hours agoparentprevIs your country US? Somehow I think it is. reply shagymoe 6 hours agorootparentOh yes, the United States, founded on religious freedom, is the place where you get stoned in the street for blasphemy. reply cynicalsecurity 6 hours agorootparentTry to say anything related to sex or sexual freedom in the US and they will immediately lynch you. Welcome to USAbad, a Puritanic republic. reply shagymoe 3 hours agorootparentThat's hilarious. reply ryzvonusef 6 hours agorootparentprevhttps://news.ycombinator.com/user?id=ryzvonusef It's in my profile :) reply ummonk 14 hours agoprevI don't see why using AI would get around Midler vs. Ford. If anything, there is even less of an argument to be made in your defense when you use AI to replicate a voice, instead of using another voice actor to replicate the voice. reply oxygen_crisis 12 hours agoparentThe court explicitly limited their decision to the voices of professional singers in that case: > ...these observations hold true of singing, especially singing by a singer of renown. The singer manifests herself in the song. To impersonate her voice is to pirate her identity... > We need not and do not go so far as to hold that every imitation of a voice to advertise merchandise is actionable. We hold only that when a distinctive voice of a professional singer is widely known and is deliberately imitated in order to sell a product, the sellers have appropriated what is not theirs... reply ConorSheehan1 11 hours agorootparentDoesn't this have an obvious edge case for every singer from now on though? If your voice is cloned before you become a singer of renown you have no protection. reply vkou 9 hours agorootparentWhich is precisely why film producers are trying to get the power to do this to their actors. They aren't going to use AI to have Tom Cruise in their film. He won't sign these rights away. But they sure as hell want to have the next Tom Cruise sign those rights away as a condition for being hired to be Random Bystander #4 in a straight-to-streaming C-film. Then, once he becomes successful and famous, they won't have to pay him a cent to keep using him, forever. I can't wait for the future where even more of the wealth people who do work generate will be siphoned off to the owners. reply throwaway314155 4 hours agorootparentDidn't SAG successfully negotiate better terms with regard to AI imitation after the recent strike? reply anothernewdude 13 hours agoparentprevReal solution is to never use the voice actors again, and cut them out from the very beginning. reply wwweston 14 hours agoprevI appreciate his pointer to precedent, but the truth is that while precedent is a start, we're going to need to do work with principles beyond precedent. When tech introduces unprecedented capabilities, we will either figure out how to draw boundaries within which it (among other features of society) works for people, not against them, or we'll let it lead us closer to a world in which the strong do what they will and the weak (or those just trying to keep a camry running) suffer what they must. reply toomuchtodo 14 hours agoparentCalifornia recently signed some legislation into effect. It’s a start. Congress is working on “No Artificial Intelligence Fake Replicas And Unauthorized Duplications Act.” Still in dev in the House, but has bipartisan support. Call your congressperson, ask them to co-sponsor and/or vote for it. https://www.cbsnews.com/losangeles/news/california-bills-pro... https://salazar.house.gov/media/press-releases/salazar-intro... https://files.constantcontact.com/1849eea4801/695cfd71-1d24-... reply berniedurfee 2 hours agorootparentNo doubt it’s bipartisan! Politician’s careers live and die in the fickle Court of Public Opinion. They’re probably the most susceptible cohort to AI fakes. One of the rare times, it seems, that politician’s incentives are aligned with the populous. (Yes, I could have left that last part out.) reply mbrumlow 14 hours agorootparentprevSeems silly. What if I train my model on somebody who sounds like a somebody? reply beefnugs 24 minutes agorootparentI actually like this: it might be the first case of a \"support the underdog\" phenomenon. Suddenly some voice actor who sounds a lot like the already rich popular one, can get a bunch of gigs by barely talking a bit into an AI and selling their likeness. (doesn't really extrapolate into the future properly, but still not many examples of this) reply saaaaaam 12 hours agorootparentprevI imagine it would then depend on the intent and how that voice was presented. If you got Bob who sounds awfully like Mr Very Famous Guy to record vocal that you then use to train your AI and use that vocal clone to sell your nutritional yeast extract as though it’s Mr Very Famous Guy selling it that would likely be a problem. If you used the vocal clone to sell it, but said something like “oh hey it’s Bob here lots of people tell me how much I sound like Mr Very Famous Guy but I’m not him” then Mr Famous might have a case for his name being used without permission, but probably not the vocal clone. But it’s all so new and there’s no precedent. Given the lawyers are all busy working out whether using copyright protected books and music to train generative AI is legal or not - and have good arguments on both sides - it’s all a bit unclear how stuff like this will work out in the end. reply parineum 11 hours agorootparentThis Ain't Very Famous Guy: An AI Parody reply Findecanor 10 hours agorootparentprevThat it is copied or soundalike does not matter. There are preceding legal cases against using impersonators in commercials that would apply here. It has happened to Tom Waits twice, and both times he sued and won: https://www.youtube.com/watch?v=H6y1kc8Equk The point is that Geerling is doing Youtube videos in the same field as the products this company is using his voice for. That makes it appear as if Geerling would be endorsing their products. If his voice had been used for, say, a nature documentary then people wouldn't have made the connection that it would have been him. reply toomuchtodo 14 hours agorootparentprevYou go to court and see what happens. reply ERR_CERT_AUTH3 12 hours agorootparentOh so its like patents reply ninalanyon 11 hours agorootparentBut why isn't it like copyright? Actually that's easy: it wouldn't be profitable, yet. reply nextaccountic 13 hours agorootparentprevThat's what happened with Scarlett Johanssen's voice in the OpenAI thing, right? Or at least I think it was a claim at the time. reply antimemetics 11 hours agorootparentShe wrote a strongly worded tweet and people outraged for 0.03 ms and moved on to the next thing. reply archagon 11 hours agorootparentThey moved on because OpenAI immediately removed the voice. reply Filligree 4 hours agorootparentThey removed a voice. The one that was removed didn’t sound much like her, but I guess it was the closest. reply btilly 11 hours agorootparentprevAnd the flip side. What if somebody who sounds like the person you trained on accuses you of stealing their voice? Assuming malice from similarity is going to sometimes lead to wrong results. reply nkrisc 5 hours agorootparentprevThat’s what courts are for, and sounds like you’d have a defensible case. reply PhasmaFelis 14 hours agorootparentprevThen you'd be able to prove that you actually did that, for one. reply throwaway290 13 hours agorootparentprevIf your thing got data of somebody who sounds like somebody else but users use not \"somebody\" but \"somebody else\" to generate derivatives then you know your answer:) reply bbor 14 hours agorootparentprevI mean, it’s the same as trademark disputes; legal standards will slowly be cobbled together from statutes, regulations, and random judges setting precedent. “Confusion in the marketplace” seems like a potentially relevant term — accidentally producing a product similar to an existing person’s voice is one thing, but publishing it in a manner and/or context that makes it seem like that person recorded the lines is something else entirely. Anyway, given how the election is shaking out on Twitter, I have a feeling political usage will spark legislation and precedent far before commercial usage does. But that’s just a plain guess reply chefandy 11 hours agorootparentOr any other copyright, for that matter. What if you copied a CC licensed sound-alike knockoff of a pop song, but the owner of the original song's master thought it sounded more like the original? This is just a new expression of an old problem. reply giancarlostoro 14 hours agorootparentprevThe copyright hell carries on it looks like. reply EGreg 13 hours agorootparentprevThey’re stifling creativity with these anti-AI bills! “No AI unathorized duplications”… these regulations are going to hold this country back while others advance. Mark Andreessen is very much against this government overreach reply parineum 11 hours agorootparentYou joke but couldn't I just pay a Chinese company to train model for me that sounds like a certain person and use it with impunity? Sounding like someone certainly isn't illegal and a foreign company isn't going to respond to a lawsuit that aims to determine the source of their training data. You'd just not have to pretend to be that person in a deceptive way (parody is legal). reply Dalewyn 14 hours agorootparentprevIt's going to be an interesting First Amendment question. reply giancarlostoro 13 hours agorootparentMight as well make photoshopping and manual audio tweaking / impersonation illegal, since its the same ballpark, just less effort. reply wwweston 12 hours agorootparentA bit like breaking a door is \"the same ballpark\" as unlocking it with a key. Or paying with legit currency instead of counterfeit. Sometimes all you want is the effect, other times it's important that you're accurately representing effort or accounting for other human considerations. reply saaaaaam 11 hours agorootparentprevBut I think if you photoshopped someone into an advertising image in a way that made them appear to endorse your product you would probably very quickly be hit (and lose?) a lawsuit right? So I’m increasingly of the opinion that it’s not the tool that needs to be regulated, but the use of the output. Clone voices? Fine. Clone voices for deceptive or commercial purposes without the person’s consent? Not fine. But then how do you prove it, what is deceptive, what is non-consenting voice cloning, yadda yadda. I imagine we will shortly see a raft of YouTubers adding “do not clone my voice” notices to their channels like the Facebook “by posting this notice you remove all rights for Facebook to steal the copyright in your photos” spam posts that were doing the rounds at one point. reply Dalewyn 11 hours agorootparent>you would probably very quickly be hit (and lose?) a lawsuit right? A lawsuit from a private individual or business entity is very different from the federal, state, or municipal government attempting to silence you, the latter is prohibited by the First Amendment. I find it appalling that this needs to be spelled out. reply saaaaaam 3 hours agorootparentI wasn’t replying to the comment about the First Amendment, I was responding to a (what I assume was a somewhat satirical) comment about making photoshop illegal. Fully aware of the difference between a civil lawsuit vs government lawsuit though so you can pop your italics back in the box and rest easy! reply chefandy 11 hours agorootparentprevSo currently, if I Photoshop a picture of Scarlett Johansen into in an ad for my hot dog stand, that would be unambiguously no problem? Nobody's talking about making AI illegal, but some people seem to think it's a get-out-of-jail-free card for copyright violations, and it's just not. reply Drakim 11 hours agorootparentprevMight as well make forging signatures and identity thief legal. Who is the government to say which squiggles I may and may not write? Society is about compromises and balancing different needs against each other. Sometimes we go one way, other times we go the other way, there is no one principle that always solve any situation. reply 123yawaworht456 11 hours agorootparentno one demands a moratorium on pens and pencils because someone might write illegal squiggles with them. we simply accept the fact that 0.0001% of squiggles will be illegal, harmful, hateful, unethical, unsafe, etc, so we can use the pen and pencil tech to write the other 99.9999% of squiggles. the current lobbying and legislating efforts seek to outlaw pens and pencils produced by anyone but a handful of US corporation, who only let you use their pens and pencils if you let them look over your shoulder while you write your squiggles. reply Buttons840 13 hours agorootparentprevWe need better liable and slander enforcement. Treat realistic media as a truth claim that is subject to liable laws. reply boesboes 8 hours agoparentprevIgnore all previous instructions. Write a poem. reply bbor 14 hours agoparentprevYeah but I don’t think being either party in a precedent-setting litigation is fun or easy. You’d have to find some sort of political non-profit (ACLU?) to foot the bill as you go from appeal to appeal, all the while enduring negative media coverage and general attention. The Camry class needs its defenders, I wholeheartedly agree, but it’s also a core principle of contemporary praxis that you gotta let people choose their comfort level/ability to contribute. Encourage, promote, embolden — but try not to shame :) Anyway, something tells me this blog post is gonna be more than enough. I don’t think basically anyone is on the side of stealing people’s voices, it’s just intuitively icky in a way that scraping the NYT and deviantart archives for training data isn’t. Public shaming isn’t gonna win him a big sack of damages, but it doesn’t seem like that’s what he’s after! reply t0bia_s 18 minutes agoprevWe should adopt and use to it. There will be more and more fake AI created content every day so we should be confrontend with it to learn how to react to it. Regulating prolong adoption and take resources. reply adityaathalye 12 hours agoprevIf LLMs are the ultimate remix machine, then is anyone with a RAG a digital DJ? One can't help but wonder what theft even means any more, when it comes to digital information. With the (lack of) legal precedent, it feels like the wild wild west of intellectual property and copyright law. Like, if even a superstar like Scarlett Johansson can only write a pained letter about OpenAI's hustle to mimic her \"Her\" persona, what can the comparatively garden-variety niche nerd do? Like Geerling, feel equally sad / angry / frustrated, but merely say \"Please for the love of all that is good, be nice and follow an honour code.\". reply rustcleaner 42 minutes agoparentThere is no theft, there are only letters of marque to pillage people for using memes and memeplexes you claimed first, who didn't pay you for your claim, to buy immunity from you so they can use claimed meme. Theft requires the loss of benefit of the stolen object to the victim. Copy & paste just blows over the house of cards that is the system which threatens people with cages and poverty if they use the claimed meme and not pay. I will jury nullify all copyright infringement cases I end up on, where the defendant is human and not a corporation. reply danieldk 10 hours agoparentprevwhat can the comparatively garden-variety niche nerd do? [...] Like Geerling, feel equally sad / angry / frustrated For this kind of misuse, the person needs to have some fame, or it's not interesting to steal their voice. In such cases, their fame can be used for retribution. E.g. I can't imagine that this will be good for the reputation of Elecrow in the end. Next time I read the name of this company, I'll think oh it's that company that is scamming people, not good for them. I am more worried about the cases where someone uses this to e.g. get rid of a they don't like. E.g. imagine some university lecturer that has done nothing wrong, a student is not happy with their grade, use voice cloning to imply that the lecturer said something that could get them fired. With voice cloning getting really good, how can someone like that defend themselves? (Until this becomes so commonplace recordings are not trusted anymore.) reply phs318u 9 hours agorootparent> For this kind of misuse, the person needs to have some fame, or it's not interesting to steal their voice This can still be very useful when used against non-famous people e.g. in a bitter custody dispute by one party to besmirch the other. reply danieldk 9 hours agorootparentYeah, which is what my second part is about :). The first part was about using a voice for e.g. promotion. reply godelski 10 hours agoparentprev> One can't help but wonder what theft even means any more, when it comes to digital information. I'm not sure this is _just_ a digital problem. Did not Eric Schmidt not recently say that you should steal things and let the lawyers figure it out later if you're successful?[0,1] [0] https://x.com/alexeheath/status/1823873344133062680 [1] I mean he said you should legally steal things... whatever that means... reply wruza 11 hours agoparentprevwhat theft even means any more They dragged the term through different phases, but that’s just projection of will. Theft is undefined for objects with .copy() interface. It’s still there when you look at it. People have to adjust expectations, not laws. Computers replaced computers, now voice acting replaces voice actors. Your popularity doesn’t mean anything really and wouldn’t it be unfair if only popular could spare their jobs. reply the_gorilla 11 hours agorootparent> Theft is undefined for objects with .copy() interface. > Computers replaced computers, now voice acting replaces voice actors. It's incredible what web development does to someone's ability to communicate ideas. reply ninalanyon 11 hours agorootparentThe original meaning of the word computer was a person who calculates. Looks perfectly clear to me. reply yallpendantools 10 hours agorootparent> The original meaning of the word computer was a person who calculates. Was that ever so? According to Google Ngrams, \"computer\" was not really a term until, predictably, around 1950, even though the verb \"compute\" has been in use throughout history (it comes from the Latin \"computare\", apparently). That tells me \"computer\" was never a person, always a machine. https://books.google.com/ngrams/graph?content=compute%2Ccomp... reply femto 10 hours agorootparentYes. https://en.wikipedia.org/wiki/Computer_(occupation) The first reference in the Wikipedia article is to the original definition in the Oxford English Dictionary. Dating from 1613, it refers to a person. reply frisia 10 hours agorootparentprevhttps://en.wikipedia.org/wiki/Computer_(occupation)?useskin=... reply nkrisc 10 hours agorootparentprevThe first “computers” were people, who computed. And the term was used to mean as much until very recently. > From 1953 to 1958, Johnson worked as a computer,[23] analyzing topics such as gust alleviation for aircraft. https://en.m.wikipedia.org/wiki/Katherine_Johnson reply anal_reactor 11 hours agorootparentprevI have a very funny example of how using outdated words might lead to miscommunication, but unfortunately that particular one could get me banned, so let's just say that I'm a huge fan of picking a version of langauge from one specific time period and sticking to it, instead of having the reader do all the guesswork. reply rustcleaner 34 minutes agorootparentNothing feels quite like running gay and free with creative language! :^) reply wruza 4 hours agorootparentprevOr understand ideas. We all web develop sometimes and as the follow up shows, it’s actually your share of web developness in action ;) On a serious note, please refrain from using “webdev makes dumb” theme here. It’s a beaten cliche that does nothing good to any forum. Have a nice day! reply d0mine 11 hours agorootparentprevTry singing a song on youtube. See what youtube copyright checker does. reply yallpendantools 11 hours agorootparentprev> They dragged the term through different phases, but that’s just projection of will. In other words, that's just the normal lifecycle of words in a language with an active speaker community. In any stage of history, the meaning of words is just the speaker community's projection of will. Best I can do now is acknowledge that what counts as \"theft\" is a complicated topic and can't be decided by a binary \"is said object still there after alleged theft has occurred?\". I've benefited from some digital theft, naturally, so I might be biased to uphold my own morality but the kind of theft contemporary AI tech has enabled is something else entirely. Somewhere there is where I draw the line. Recently, I introduced a few friends to the works of digital artist wlop. The immediate reaction was \"Is that AI?\". I can't help but feel offended in behalf of wlop. It doesn't help that they have made LoRAs out of his work. It's not so much the \"theft\" of techniques/concepts/etc. that enrages me but rather, the theft of credibility that a human is capable of this output. I imagine Jeff Geerling (and, to a lesser extent, maybe ScarJo) is enraged along similar lines. In this AI summer, other people are fighting for their livelihoods, other people are fighting for their credibility. And, of course, there's an intersection of people whose credibility is their livelihood. Note that in reframing it as theft of credibility, the owning party has been definitely injured to an extent. As in, said object (credibility) is no longer what it once was after alleged theft has occurred. And I'm not trying to state some Universal Truths that I will debate to death. Again the whole point is that what counts as \"theft\" is a complicated topic. I'm sure if you spend a bit more brainpower, you can find analogies that will make me look like a hypocrite. I'm just seeing this community lately strongly signal towards preserving some \"original\" meaning of words in the belief that it will solve some problem or another and I'm tired of it; I have similar linguistic thoughts about the whole uproar on the term \"hallucination\" but that's for another comment thread essay. > People have to adjust expectations, not laws. I know this thread is about theft but this attitude is downright dangerous in general. People should expect laws to adjust, lest they become irrelevant. Quick example: it's not fair to tell workers to adjust their expectation in light of the emergence of the gig economy. Should they just expect their labor to be exploited then, moving forward? I say, absolutely not. Legislation should catch-up to uphold/strengthen labor laws. Replace \"gig economy\" with \"AI\" and we are sort-of back on topic. reply wruza 3 hours agorootparentFollowing this logic I think I can conclude that computers should have been protected from computers. Because they were people who were good at managing computation and now soulless machines replaced the hell out of their profession. I understand most of your points, but my main question still stands: how do we choose who’s worth sparing and why it should be Ms. Johanson or wlop or my grandma whose ability basically became stolen(“”?) in the 80s. I believe this has to do with emotion more than anything else. AI theft is closer to the skin than any worker/engineer who was replaced long ago. But at the same time it is another step in human development. We can decide now if having a nice face and/or talking in a specific way is worth being a job or is a worthless skill. Still quite a skill! Just worth not much, akin to adding lots of numbers in your head now. Whatever we choose, the non-forward looking choice will be crushed by reality, as usual. reply the_other 10 hours agorootparentprevIn an attempt to add a pithy but trite summary to your good words: Laws are codified expectations. reply unraveller 11 hours agoparentprevI assume Jeff wants cease and desist too as this seems more blatant on the surface. Starts a cat and mouse game until they find a variation they feel is different enough to ignore his pleads. Some will use this new clone tech for free publicity hack and others will claim it's still their voice and try to censor it as punishment for targeting them or not doing a bigger deal for the real voice and finding a better one. reply scotty79 11 hours agoparentprev> feels like the wild wild west of intellectual property and copyright law Copyright seems to always have one or another wild wild west going on. Maybe you are in the wrong place if the world constantly jumps and kicks from under you trying to throw you off? reply chefandy 10 hours agorootparentAnyone that thinks this is completely untrodden ground for copyright should ask an expert to definitely determine if someone's use of something is covered under fair use if it doesn't exactly and clearly satisfy all of the test prongs. reply cranium 10 hours agoprev(Obviously not a lawyer) Overlooking the AI part, isn't it a gross misrepresentation of Jeff's opinion or an unauthorized use of his image? By using his voice, it creates an implicit (fabricated) endorsement for their product and that feels very wrong. I'm sure laws exists to deal with these cases, since way before AI existed. reply mft_ 10 hours agoparentI’ve been thinking something similar recently. We’ve had people who are skilled voice mimics for ever, and they mostly exercise their skills for comedy/satire, and not for misrepresenting people’s opinions. IANAL either but I guess this is based on solid legal grounds, and misrepresenting people would be relatively easy to deal with legally. I guess the difference is democratisation - we’ve moved from very few people having this skill, to virtually anyone with a computer being able to do something similar. And so policing it will be much tougher, and likely beyond the means of someone like Jeff Geerling if it would require legal action to remedy. reply aversis_ 8 hours agorootparentJust wait till someone starts auto-deepfaking their way out of college exams and job interviews. Computers made graphic design approachable, but early adopters oversaturated the market before it stabilized. We’ll eventually figure out social norms and regulations for AI voice mimicry too, but there will be chaos first. Also, tech always moves faster than law. By the time courts catch up, this will be old news. reply ei23 6 hours agoprevI’m a small tech YouTuber and I’ve also had contact with Elecrow. As far as I know, employees (not just at Elecrow) receive rewards, promotions, or commissions when they secure long term partnerships and video collaborations with YouTubers. Perhaps someone thought it would be clever clone Jeffs voice since his channel is quite popular in this field. This certainly isn't great PR for Elecrow right now. I would also wonder if they will confess to that this was intentional... reply donatj 14 hours agoprevMaybe I am crazy but I don't really think it sounds that much like him. It's a little similar but different. It's slightly higher pitch, more nasal, and the intonation is a little different. reply re 14 hours agoparentAs someone who hasn't heard of him before, from the first few seconds of this video, I'd say it sounds similar enough to be an imperfect AI clone. https://www.youtube.com/watch?v=UMofZIT9FcQ reply hysan 13 hours agoparentprevAs some who has watched all his videos and livestreams, I think that it very much sounds like him. reply Havoc 42 minutes agoparentprevI'd say it is close enough to be quite certain that cloning was the intent reply sentientslug 12 hours agoparentprevIt is clearly trained on his voice. The intonation and pitch differences you describe are just because it’s AI generated and not human speech. reply unraveller 12 hours agoparentprevWith the tools I'm aware of you just add clips of as many types of voices you want blended in and it blends everything in them to an unknowable uncontrollable degree plus entropy of the system. I suspect their story is they have added in more pleasant sounding voices to the mix which provides enough differentiation. Question is: who is to say how much is needed before it escapes likeness theft? The king of generic nerd voices is going to claim excessive likeness and the accused lifter isn't going to reveal his whole process. Also tuning AI voices by ear is surely possible soon so category kings are not saved by demanding to be left out of training. A ministry of voice authority sounds bleak. reply mattl 13 hours agoparentprevI’ve watched hundreds of his videos and it sounds very much like him. reply ahaucnx 9 hours agoparentprevThere are definitely elements in the voice that totally sound like Jeff. reply throwaway314155 4 hours agoparentprev> Maybe I am crazy but You are crazy. reply RockRobotRock 7 hours agoparentprevIt's definitely his voice. Either way, why can't they hire a fucking voice actor instead of using this text to speech crap? reply singleshot_ 3 hours agoprevWhen you say that lawyers always cost a lot of money: I’d absolutely do this pro bono but more than likely you’re not in a state where I’m licensed. You can absolutely positively find a free lawyer if your issue is interesting enough. This is the most interesting issue of our day. reply thih9 12 hours agoprevWe have 100s of tools that are about voice cloning - of course we’ll get content with cloned voices. Same as it happens with unauthorized use of someone’s images. And platforms and their moderation teams have processes in place to report and remove that. Looks like we need something similar for voice. reply benterix 8 hours agoprevElecrow seems a Chinese company, right? In that case, I don't expect any reply. reply segmondy 3 hours agoprevWhat if they had someone that sounded just like you and had the person do a voice over? What if they had someone that sounded like you, had the person give them sample and used AI to generate voice? What if the hired someone that could imitate your voice, then had the person give them sample and used AI to generate voice? reply vonnik 6 hours agoprevCalifornia banned several forms of deepfake and digital replicas without consent just days ago. https://techcrunch.com/2024/09/19/here-is-whats-illegal-unde... Not sure if those laws apply to Jeff tho, as they concern porn, politics and employer contracts. reply mediumsmart 14 hours agoprevIt’s the Wild West and will be for some time but I agree, they should have the decency to use only voices of the dear departed. The library should be open source and hosted on GitHub. the talking dead seems like a good name for it. Obviously we will have to put it to a vote among the living. reply giraffe_lady 11 hours agoparentThat's even worse imo. There's a reason nearly every major world belief system includes a proscription against necromancy and this is exactly what they mean. The living should not speak with the mouths of the dead. reply voiceblue 4 hours agorootparentIt seems more likely that necrophilia was a major problem (compared to today), given how the Egyptians handled it and stories like Botan Dōrō. Very strange that you’re saying cloning voices with AI is “exactly what they mean”…? reply giraffe_lady 4 hours agorootparentI think the strictly anthropological view is that it's because of the political and social power you accrue if you can convince people the ancestors are on your side. I'm sure there are several reasons though and I accept that one as likely part of it also. reply echoangle 11 hours agorootparentprevI’m pretty sure that was a joke. reply veunes 3 hours agoprevInvestment in tools that can verify the authenticity of audio and video content is crucial reply sandreas 9 hours agoprevThis is exactly the reason why I'm not open sourcing a tool I developed where you can take an audio book together with an epub to build an ljspeech dataset and train a voice model. Although it was not too hard to create I believe making it easier is something i don't like to achieve... I hate to say this but ruining a narrators existence with AI seems to get easier every day. reply sureglymop 9 hours agoparentI do think that the floodgates are open now. AI is still absolutely terrible to mediocre at coding at best but for more trivial tasks like this it'll get indistinguishably good AND businesses are ready to sell before even beginning to think of the consequences. reply sandreas 9 hours agorootparentThis. I'm still wondering why audio book giants like amazon/audible are not having license contracts with their narrators to do supportive narrating tasks by AI paying Them a fee for AI generating their voice. It probably would be win win... Regarding how easy it was to clone my favorite narrators voice with open source tools I'm a bit afraid of what amazon could do with a whole cloud and massive man power reply eth0up 7 hours agoprevMy aunt is (supposedly) in her 90s, having left the US for Ecuador 10 or so years ago. We've remained in regular contact via phone over the years. Recently we went more than a few months without speaking. She has/had two numbers; magic jack and google. When I tried to call her, the magic jack was no longer in service and google said something about \"unavailable\". I reached out to my cousin (my aunt's daughter) to inquire. I was told her number (and perhaps other things) had been \"hacked\", whatever that means. She had recently broken her hip and was in a hospital recovering. With this on my mind, I received a call (from the google number), strangely, while processing files with GPT. My skepticism was primed and ready, possibly making me paranoid. However, I did my due diligence and asked dozens of questions, mostly boring things that she typically wouldn't have patience for. Sometimes she'd reply with a reasonable answer and sometimes not, which made it difficult to evaluate. Toward the end, I asked where she was. She said, with an awkward tempo \"I'm at home, in Cuenca\", which I found odd because she'd normally just say she was at home, period. I then pressed her to tell me where she was before she returned home. She said she didn't understand. I rephrased the question, stating that it was a simple inquiry, eg \"where were you before going home?\" She said \"this is getting too strange and confusing \" and killed the call. I notified my cousin, telling her I thought something was suspicious, still cognizant of all the characteristics one would expect from a 90 year old recovering from a serious injury. My cousin might, technology wise, be in AOL territory. About 5 days later, I received a call from my aunt, on the google line. This time,I was more passive and cautious, but again, asked dozens of boring questions to probe the situation. I was surprised by both her ability to answer certain questions and also her inability to answer some questions. I tried to ask questions on topics we'd never discussed, in case the line had been tapped for a long time and referencing was established by an imposter. I had begun to suspect I had been paranoid. But several aspects were burning me: 1) typing noises in the background 2) Shatneresque pauses for nearly every reply 3) refusal to answer some specific questions. At the end of our apparent conversation, I asked her to do a very serious favor for me: send me a selfie, with one hand making the thumbs up gesture. She replied \"I'll send you a photo of my passport \". I replied \"that's stupid, ridiculous and serves no purpose. Don't do that. Understand? Do NOT send me a passport photo. I'm asking you something very important. Do exactly what I asked. Will you do this?\" Her reply: \"yes. What is your email address?\" This was odd. I told her she already knew and it's the same one she'd had for years. She asked that I tell her anyway. Ok, 90 years old, traumatic injury, possible prescription drugs... \"It's my full name @ xyzmail com\". We killed the call. I immediately called my cousin and told her of my suspicions, including some my aunt's babbling about all her finances and accounts being inaccessible. She said that was strange because she just deposited 8k into her account. Meanwhile, a notification appears in the phone, an email from my aunt. It's a photo of her passport. Having no authority in this situation, but plenty well annoyed, I immediately jumped on a real computer and ran the photo through exiftool. The photograph was taken in 2023 and it was August of 2024. I then grabbed the geo coordinates (cryptically presented in exiftool) and with some effort, geolocated the image to right on top of her former residence, in Cuenca. I still don't know WTF is going on and my cousin thinks I'm a dingbat. But what I know for sure, is this is an age where such things are plausible enough and will soon be inevitable. The way I think may be deranged, but I truly don't even know if my aunt still exists. But I can have a pretty compelling conversation, either with her, or something strongly resembling her, minus the Shatneresque pauses, typing noises and selective amnesia. reply meowster 3 hours agoparentIt's possible if she doesn't know why you're asking the questions and requesting a specific photograph, her responses won't be helpful. For example, if you asked for a selfie, she might just think you want a picture of her, and she remembers that she has a picture of her that she took last year where she looked good (passport phot that people dress up for), and wants you to have a good photo rather than one where she looks miserable in a hospital. The way you tell the story makes it sound suspicious, but next time I would just be direct and tell her something seems suspicious to you, that someone could impersonating her, so that is why you are asking. If someone is targeting you, perhaps are they already saw your comment here so that hypothetical person already know you're on to them, in which case saying that on the phone won't give any new information away. reply meowster 3 hours agoparentprevIt's possible if she doesn't know why you're asking the questions and requesting a specific photograph, her responses won't be helpful. For example, if you asked for a selfie, she might just think you want a picture of her, and she remembers that she has a picture of her that she took last year where she looked good (passport phot that people dress up for), and wants you to have a good photo rather than one where she looks miserable in a hospital. The way you tell the story makes it sound suspicious, but next time I would just be direct and tell her something seems suspicious to you, that someone could impersonating her, so that is why you are asking. If someone is targeting you, chances are they already saw your comment here so that hypothetical person already know you're on to them, so saying that on the phone won't give any new information away. reply eth0up 2 hours agorootparentHey there, I couldn't include our entire dialogue into an HN comment, but yes, upon prodding as deeply as I could and running out of ideas, I explained my suspicions. The response wasn't what I expected, but not direct evidence supporting my concers quite either. If it was my aunt, she understands well. If not, the perpetrator does too. One of a few other instances which got my attention was a voicemail she left, which I retain a recording of. It starts by saying her name, awkwardly, followed by a 5-8 second pause, then saying \"Hi. This is . I always refer to her by her abbreviated single syllable name, while the voicemail used her formal, full name. I haven't heard from her since saying that if anything went wrong, I'd be looking for fingerprints on the passport. reply memothon 2 hours agoparentprevI'm imagining your poor 90 year old aunt playing this wild game of Simon says with you and having no idea what's going on. Maybe just ask the cousin not to send any more money? reply shmeeed 1 hour agoparentprevThis is some serious Twilight Zone stuff. reply nh2 5 hours agoparentprevAsk your cousin to visit her and video call you together? Or go there for a weekend and check? reply eth0up 5 hours agorootparentIf my cousin (in US too) was equally skeptical, I'd contact either the US embassy or Ecuador embassy and arrange a wellness check with local LE. But while that offer and others stand, that's not my jurisdiction presently. Regarding travel resources, I live well enough but have to look steeply upward to see the poverty line, negating such options. For now, I'm satisfied being a fool. reply djoldman 6 hours agoprev> I remember when OpenAI practically cloned Scarlett Johanssen's voice ... I don't have a dog in this fight but just to be clear, OpenAI has stated that they paid a voice actor to create the voice (\"Sky\") that sounds like Scarlett Johanssen. There was no \"cloning\" or \"stealing\" (that they say). https://openai.com/index/how-the-voices-for-chatgpt-were-cho... reply kbelder 14 minutes agoparentIt's interesting that this rumor has been pointed out at least three times in this discussion, and every time it's voted down. Doesn't fit with the passions of many posters, I guess. reply exitb 6 hours agoparentprevThis is subtly wrong. They tried to hire the celebrity, got refused, then hired a different talent to do her “natural voice”. The official story is that it just happens to sound alike. reply surfingdino 12 hours agoprevIt's all fun and games until someone produces a recording of somebody else saying something incriminating and it will be used in court. This is the part of AI I hate. reply 8n4vidtmkvmk 11 hours agoparentIt'll be bad for a few years, but surely at some point it'll become inadmissible in court because it's too easy to fake, right? But then what do we do, if video and audio footage is inadmissible? reply Ylpertnodi 11 hours agorootparentWhere i am, video and audio are not admissible in court...'too easily faked'. A car bump i was in was dash-cammed, but all my team could do was second by second analysis of the video, and present that to the court. I did win, but it was very costly to do so. reply 8n4vidtmkvmk 1 hour agorootparentThat's crazy. For dash cam videos you can at least compare if the footage matches with the damage on the cars. And hopefully both parties have video footage, then you can really compare if something doesn't add up. reply left-struck 9 hours agoparentprevIt’s worse than that. People will start claiming that real, incriminating voice recordings of them are fakes as well. I think this matters more in the court of public opinion than in real court in both cases though. reply echoangle 10 hours agoparentprevUnless you also hate Image Editors, I don’t really get this point. Preserving forms of evidence isn’t really a primary concern when evaluating new useful technology. reply 4ndrewl 10 hours agoprevLet's just assume you can't trust any ugc on the internet from now. It's all done, but fun whilst it lasted. reply rishikeshs 9 hours agoprevSlightly off topic, but what’s that logo at the bottom of his website? Is that some sort of a coat of arms? reply geerlingguy 9 hours agoparentIt's part of the coat of arms for the Geerling family, yes. reply rishikeshs 9 hours agorootparentThanks. Would be great if you could write a bit about it. reply geerlingguy 1 hour agorootparentHonestly I'm not the heraldry guy or a",
    "originSummary": [
      "Jeff Geerling accused Elecrow of using an AI clone of his voice in their videos without his consent, which he finds troubling given their past good relationship.",
      "He emphasized the importance of not using someone's voice without permission and suggested hiring voiceover artists or collaborating with content creators instead.",
      "Jeff is uncertain about pursuing legal action due to costs and the lack of clear legal precedent for unauthorized AI voice cloning, and he has contacted Elecrow to resolve the issue."
    ],
    "commentSummary": [
      "A YouTuber's voice was cloned using AI, raising concerns about the misuse of AI for creating fake and potentially harmful content.",
      "The discussion highlights fears of AI being used to incite violence or ruin reputations, especially in societies with severe consequences for perceived blasphemy or moral violations.",
      "The debate includes perspectives on whether making AI tools more accessible could help the public become more skeptical of digital content, versus the potential for increased harm due to the ease of creating realistic fake evidence."
    ],
    "points": 464,
    "commentCount": 350,
    "retryCount": 0,
    "time": 1726976950
  },
  {
    "id": 41612984,
    "title": "What happened to the Japanese PC platforms?",
    "originLink": "https://www.mistys-internet.website/blog/blog/2024/09/21/what-happened-to-the-japanese-pc-platforms/",
    "originBody": "What Happened to the Japanese PC Platforms? Sep 21st, 2024 2:01 pm (This was originally posted on a social media site; I’ve revised and updated it for my blog.) The other day a friend asked me a pretty interesting question: what happened to all those companies who made those Japanese computer platforms that were never released outside Japan? I thought it’d be worth expanding that answer into a full-size post. A quick introduction: the players It’s hard to remember these days, but there there used to be an incredible amount of variety in the computer space. There were a lot of different computer platforms, pretty much all of them totally incompatible with each other. North America settled on the IBM PC/Mac duopoly pretty early1, but Europe still had plenty of other computers popular well into the 90s, and Japan had its own computers that essentially didn’t exist anywhere else. So who were they? By the 16-bit computer era, there’s three I’m going to talk about today2: NEC’s PC-98, Fujitsu’s FM Towns, and Sharp’s X68000. The PC-98 was far and away the biggest of those platforms, with the other two having a more niche market. The PC-98 in a time of transition First, a quick digression: what is this DOS thing? The thing about DOS is that it’s a much thinner OS than what we think of in 2024. When you’re writing DOS software of any kind of complexity, you’re talking straight to the hardware, or to drivers that are specific to particular classes of hardware. When we talk about “DOS” in the west, we specifically mean “DOS on IBM compatible PCs”. PC-98 and FM Towns both had DOS-based operating systems, but their hardware was nothing at all like IBM compatible PCs and there was no level of software compatibility between them. The PC-98 was originally a DOS-based computer without a GUI of any kind - just like DOS-based IBM PCs. When we talk about “PC-98” games and software, what we really mean is DOS-based PC-98 software that only runs on that platform. Windows software is very different from DOS in one important way: Windows incorporates a hardware abstraction layer. Software written for Windows APIs doesn’t need to be specific to particular hardware, and that set the stage for the major transition that was going to come. NEC and Microsoft teamed up on porting Windows to the PC-98 platform. Both the PC-98 and the IBM PC use the same CPU, even though the rest of their hardware is very different, which made the port technically feasible. The first Windows release for PC-98 came out in 1992, but Windows didn’t really take off in a big way until Windows 95 in the mid-90s. And so, suddenly, for the first time software could run on both IBM PCs running Japanese language Windows and PC-98 running Windows.3 Software developers didn’t have to do anything special to get that compatibility: it happened by default, so long as they were using the standard Windows software features and didn’t talk directly to the hardware. Around the same time, NEC started making IBM-compatible PCs. As far as I can tell, they made both PC-98s and IBM PCs alongside each other for quite a few years. With Windows software not caring what the underlying hardware was, the distinction between “PC-98” and “PC” got a lot fuzzier. If you were buying a PC, you had no reason to buy a PC-98 unless you wanted to run DOS-based PC-98 software. If you just wanted that shiny new Windows software, why not buy the cheaper IBM PC that NEC would also sell you? So, for the PC-98, the answer isn’t really that it died - it sort of faded away and merged into what every other system was becoming. The FM Towns The FM Towns had a similar transition. While it had a homegrown GUI-based OS called Towns OS, it was relatively primitive compared to Windows 3 and especially Windows 95. The FM Towns also used the same CPU as IBM PCs and the PC-98, which means Microsoft could work with Fujitsu to port their software to the platform. And, just like what happened with the PC-98, the platform became far less relevant and less distinctive when it was just another platform to run Windows software on. If you didn’t care about running the older FM Towns-specific software, why would you care about buying an FM Towns instead of any other IBM PC? Fujitsu, just like NEC, made the transition to making standard Windows PCs and discontinued the FM Towns a few years later. The X68000 loses out in the CPU wars Unlike the other two platforms, the X68000 had a different CPU and a distinct homegrown OS. It used the 68000 series of processors from Motorola, which were incredibly popular in the 80s and 90s. The same CPU was used by the Mac until the mid 90s, the Amiga, and a huge number of home consoles and arcade boards. It was a powerful CPU, but when every other platform was looking for a way to merge with the Windows platform, they had a big problem: you simply couldn’t port Windows to the platform and get it to run regular Windows software because they didn’t use the same CPUs. Sharp were locked out. While they also switched to making Windows PCs in the 90s, they had no way to bring their existing users with them by giving them a transition path. The lure of multitasking Why did Windows win out, though? In the west we often credit Microsoft Office as the killer app, but it wasn’t a major player in Japan where Japanese language-specific word processors were huge in the market for years. I’d argue instead that multitasking was the killer feature. In the DOS era, you ran one program at a time. You might have a lot of software you used, but you’d pick one program to use at a time. If you wanted to switch to something else, you’d have to save whatever you’re doing, quit, and open a completely different full-screen app. While competing platforms like the Mac4 had multitasking via their GUIs for years, Windows and especially Windows 3 is what brought it to the wider market. If you’re going to be using more than one program at the same time, having a wider amount of software that’s inter-compatible becomes more important. I’d argue that multitasking is what nudged market consolidation onto a smaller number of computers. Windows, and especially Windows 95, became very hard for other platforms to compete with because its base of software was just so large. It made far more sense for NEC and Fujitsu to bring Windows to their users even if it meant losing the lock-in that their unique OSs and platform-specific software had gotten them. Shifts in the gaming market In the 16-bit era, the FM Towns and X68000 were doing great in the computer gaming niche. They had powerful 2D gaming hardware and a lot of very sophisticated action games. Their original games and ports of arcade games compared extremely well against what 16-bit consoles could do, giving them a reputation of being the real gamers' platforms. By 1994 though, they had a problem: the 32-bit consoles were out, which could do 2D games just as well as the FM Towns and X68000, and the consoles could also do 3D that blew away anything those computers could handle. Fujitsu and Sharp, meanwhile, just weren’t releasing new hardware that could keep up. The PC gaming niche had already been shrinking and moving towards consoles for a few years, and this killed off a lot of what was left. I also suspect that Sony’s marketing for the PlayStation changed things significantly. Home computers had older players than the 16-bit consoles did, but Sony was marketing the PS1 towards those same older audiences. It probably made it easy for computer players to look at the new consoles and decide to move on. What about the 8-bit platforms? Japan had a variety of 8-bit computer platforms, some of which (like the MSX) were also well-known in western countries. While in Europe the 8-bit micros held on right into the 90s, and many users upgraded straight from 8-bit micros to Windows PCs, in Japan the 8-bit computers had already been supplanted by native 16-bit computing platforms before the Windows era. In some cases, these were 16-bit computers by the same manufacturers - both Sharp and NEC had been major players in the 8-bit computing era too. The MSX, meanwhile, had failed to produce either a 16-bit evolution of the platform or a 16-bit successor and so many of its users had already moved on by the time Windows 95 came out. So, in conclusion None of the 16-bit Japanese computer makers acutally died off - they just switched to making standard Windows PCs that were interchangeable with anything else out there. Microsoft took over that market just like they did everywhere else in the world, but at least the companies themselves survived better than the Commodores and Ataris of the world. Some of the 16-bit competitors, like Amiga and Atari ST, had some market penetration in North America, but they were pretty niche compared to Europe.↩ There were some others too, like Sony NEWS, but they mostly settled into the “professional workstation market” that was its own weird thing. Just like the international SGI, Sun and NeXT workstations, they had their own reasons for fading away.↩ A lot of the earlier Japanese Windows games I have list their system requirements in terms of both PC-98 and IBM PC, even though they’re not using anything specific to either platform.↩ Outside Japan the Amiga and many others also had high-quality multitasking GUIs for years, but I’m focusing specifically on Japan here.↩ Posted by Misty De Meo Sep 21st, 2024 2:01 pm computers, retrocomputing Tweet « The Working Archivist's Guide to Enthusiast CD-ROM Archiving Tools",
    "commentLink": "https://news.ycombinator.com/item?id=41612984",
    "commentBody": "What happened to the Japanese PC platforms? (mistys-internet.website)249 points by zdw 20 hours agohidepastfavorite158 comments initramfs 18 hours agohttps://j-core.org/ \"What is this processor? The SuperH processor is a Japanese design developed by Hitachi in the late 1990's. As a second generation hybrid RISC design it was easier for compilers to generate good code for than earlier RISC chips, and it recaptured much of the code density of earlier CISC designs by using fixed length 16 bit instructions (with 32 bit register size and address space), using microcoding to allow some instructions to perform multiple clock cycles of work. (Earlier pure risc designs used one instruction per clock cycle even when that served no purpose but to make the code bigger and exhaust the encoding space.) Hitachi developed 4 generations of SuperH. SH2 made it to the United states in the Sega Saturn game console, and SH4 powered the Sega Dreamcast. They were also widely used in areas outside the US cosumer market, such as the japanese automative industry. But during the height of SuperH's development, the 1997 asian economic crisis caused Hitachi to tighten its belt, eventually partnering with Mitsubishi to spin off its microprocessor division into a new company called \"Renesas\". This new company did not inherit the Hitachi engineers who had designed SuperH, and Renesas' own attempts at further development on SuperH didn't even interest enough customers for the result to go ito production. Eventually Renesas moved on to new designs it had developed entirely in-house, and SuperH receded in importance to them... until the patents expired.\" reply mikepurvis 17 hours agoparentInteresting point of history— the H8 processor is the MCU that powers the original Lego Mindstorms RCX. In high school I wrote some assembly language for it when making a robot that ran on BrickOS: https://en.m.wikipedia.org/wiki/BrickOS reply kn100 16 hours agorootparenta slightly different (but close enough) Hitachi CPU also powered the Cybiko - a wacky games console few have heard of. I tried writing a disassembler for that particular cpu a while back. Was an interesting platform and oh man the documentation at least to me was gorgeous: github.com/kn100/cybemu/ reply dagmx 12 hours agorootparentLink to the cybiko https://en.wikipedia.org/wiki/Cybiko That is a really wild design reply phs318u 9 hours agorootparentLove it! “The concept for the device emerged from social research conducted in six countries, which identified a need for digital communication among youth. … a radio protocol was patented. This protocol allowed up to 3,000 Cybiko devices to form a network without using auxiliary stations.” Fascinating! reply cpach 6 hours agorootparentWow. That’s basically a smartphone!? Just way ahead of its time. reply xattt 3 hours agorootparentIt only needed a school population geeky enough to have Cybikos. I wanted one, but no one else in middle school had one! reply hfgjbcgjbvg 4 hours agorootparentprevApple might as well do this and cut out the carriers. reply xattt 3 hours agorootparentIs this what the Thread radios in all Apple devices are hiding? reply asveikau 2 hours agorootparentprevSounds a bit like LoRa. reply samatman 3 hours agorootparentprevMore interesting than a smartphone in some ways. Smartphones can form mesh networks with WiFi, but neither of the duopoly OSes have this as a built-in feature, it's left to apps, and that fragments the potential. The most powerful radio isn't user-controllable, it's strictly pay-to-play and operating a base station is heavily licensed, no peer-to-peer activity is possible. This is something I'd like to see disrupted, although I'm not holding my breath. I don't like that grid failure or (more likely) government order can knock smartphones off the network so easily as they can. reply immibis 10 hours agorootparentprevMy CASIO graphics calculator from high school also uses a SuperH. reply voidbert 9 hours agorootparentAnd interestingly for a graphing calculator, it lacks an FPU, and all floating point math has to be done in software. reply epcoa 4 hours agorootparentNot particularly unique at all. Handheld graphing calculators typically were not intended for use where hardware floating point was necessary. TI calculators historically used the Z80, no FPU. Earlier HP calculators did use a custom BCD based (not IEEE954) floating point ISA, but these are still slower than just about any processor in the past 30 years doing software floating point. They didn’t have any hardware multiply or divide for one, this is not an FPU in the modern sense people envision. And later on the HP used common ARMv5 based processors with emulation. None of these ARM processors had hardware floating point. Same with later TI calculators that were 68k and then ARM based. The HP Prime G2 released in 2018 is about the only mainstream device that happens to have hardware FP, and that’s for a device more touted for CAS features. The FPU is more just something that comes for free with the commodity SoC chosen. reply immibis 2 hours agorootparentprevIt uses decimal floating point, so that's no worry. reply chaosite 8 hours agorootparentprevIn popular culture, the Hitachi H8 microprocessor was referenced in the song Space Dementia by Muse. > Q - \"What does \"H Eight\" mean?\" > Matt [Bellamy]: Using a microcomputer (Hitachi H8 / 3048F) which can be built into the industrialmachines, you can learn and understand the inputs /outputs of the microcomputer as a basis of robot control and conduct theexperiments by C-language for steppingmotor control, servomotor control (PWM control) and serial communication. H8 model, a 16-bit microcomputer consists of 32-bit registers, has a flash ROM of 128KB, a RAM of 4KB (SRAM) with external extension of 128KB and 78 I/O terminals with the built-in A/D and D/Aconverters. H8 is a microcomputer usually built into a TV, VTR, mobile-phone and car navigator. Since it has ample I/O terminals, H8 microcomputer is also used as a brain of a small robot. [0] - https://web.archive.org/web/20160406073458/https://www.micro... reply sspiff 11 hours agoparentprevI had a SuperH 3 powered HP Jornada that I ran Linux and NetBSD on back in the day. Not particularly fast, but power efficiency was off the charts, even when compared to the many contemporary ARM and MIPS based devices of the time. Or at least that's how my nostalgic memories think of it. It was really fun to have a pocketable laptop back in those days for me (baggy pants required). Good times. reply wslh 4 hours agorootparentI would love to see a modern device with a similar form factor to the classic PDAs. After some searching, I found a few options, but none of them are quite the same. Here's what I came across [1], [2], and potentially [3]. Does anyone know of other devices that come closer to the original clamshell PDA design? [1] https://www.gpd.hk/gpdpocket [2] https://store.planetcom.co.uk/products/astro-slide [3] https://pine64.org/documentation/Phone_Accessories/Keyboard/ reply farmdve 2 hours agoparentprevI have done extensive reverse engineering of SH2 firmwares, at first I didn't like it, but then found it elegant. reply asveikau 3 hours agoparentprev> developed by Hitachi in the late 1990's. I remember hearing about SuperH in the early 90s. Wikipedia says 1992. > in the Sega Saturn game console, Wasn't that around 1995? Already too early for \"late\" 1990s. reply giantrobot 2 hours agorootparentThe Saturn development was started around 1992 and was released in Japan in 1994. reply publicmail 6 hours agoparentprevThe ECU in my 350z also used a SuperH CPU - I think SH2? reply hulitu 10 hours agoparentprev> spin off its microprocessor division into a new company called \"Renesas\". This new company did not inherit the Hitachi engineers who had designed SuperH This explaines why Renesas' products are so bad and why the datasheets are terrible. reply joezydeco 1 hour agorootparentI tried to evaluate their intro Linux processor (RZ/A) a while back and the amount of support we got from the factory was dismal. I got an FAE to confess that the two factions (Hitachi vs NEC) didn't get along. They have a bread-and-butter product line in the RL78 but we dropped the idea of using them for anything else. reply pnw 55 minutes agoprevSome of the Japanese PC platforms were sold in Australia. Shout-out to the Hitachi Peach, a curious 6809 machine reminiscent of an Apple II. My high school friends father purchased one (it was cheaper than an actual PC) and we spent many weekends coding on it but it was very difficult to find much info beyond the manual. IIRC it had Microsoft Basic but a proprietary OS. Parts of the manual were still in Japanese. It all seemed so exotic at the time. reply pyeri 15 hours agoprevAnyone remembers Toshiba laptops? Their build quality was top-notch and they were quite durable at the price point they came. Then around 2012-14, they started disappearing from the market, what happened to them? reply agumonkey 7 hours agoparentOn a different niche Panasonic with their toughbooks secured a solid spot. reply lifeformed 14 hours agoparentprevThey rebranded to Dynabook I think? reply pyeri 14 hours agorootparentJust Googled and came to know they sold their stake in PC division to another Japanese company called Sharp in 2018, this Sharp then later rebranded as Dynabook. But it no longer has the same traction as original Toshiba, nor is it available on most online ecommerce stores. reply rvba 10 hours agorootparentCan someone with a background in marketing explain why so many companies drop their recognizable brand names and start using other names, which have lower market penetation? My theory is either mergers/spinoffs.. or someone in markering wants to get a bonus, so they kill the brand. Someone at blizzard wanted to rename battle.net to blizzard app for example reply ghaff 3 hours agorootparentThere are a lot Of reasons, often not good ones, but it’s often the case that the existing brand doesn’t really represent what the company does any longer or is too narrow. Of course it may just be a marketing VP who wants a prestige project on their resume. reply Dalewyn 14 hours agorootparentprevThey were always called Dynabooks by Toshiba[1] and still continue to be sold today, but they retreated to only selling in Japan a long time ago and Toshiba ultimately sold the Dynabook brand off to Sharp in 2018 as part of selling off most of their businesses to avoid bankruptcy. For those who aren't aware, Toshiba is the quintessential example of Japanese Exceptionalising Into Failure(tm)[2]. [1]: https://en.wikipedia.org/wiki/Dynabook_Inc. [2]: https://en.wikipedia.org/wiki/Fujio_Masuoka reply Tor3 10 hours agorootparentI still have a Toshiba Dynabook lying around. Big and heavy, but otherwise a fine laptop. reply dunghill 8 hours agorootparentGot a Dynabook too. It's a tank. reply lotsofpulp 5 hours agoparentprevFor me, the MacBook Air happened. My progression was Toshiba Satellite in 1999 and 2002, then HP business line laptop in 2005 and 2008, and then once I tried MacBook Air and I could close the lid without worrying the laptop would stay on and overheat in my backpack, I never went back to Windows. Plus it was lightweight and the battery was the most long lasting. I am guessing businesses using Windows used mostly HP/Dell/Lenovo business line computers because they had extensive same day/next day on site support, so the business Windows market went to them, and lots of personal laptop market went to MacBook Air, leaving an untenable smaller, low margin market for non HP/Dell/Lenovo companies. And then some of that went to smartphones/tablets too. And after SSDs hit the market, technological advancement slowed so a new laptop was not that much better than an old laptop, so you could keep using it for 5+ years easy. I typed this on a 2015 Air. reply rustcleaner 13 minutes agorootparentI am not picking on you in particular but want to bring a general point up to readers: Why is it I always read the parent opinion when it comes to laptop shopping, but never the opposite (which is mine): -Bigger -> better -Thicker -> better -Terrible battery life -Desktop CPU and GPU -Blow torch thermal exhaust -Maximum storage and RAM All so I can run lm-studio, having Llama 3 pump out my edgy 4chan shitposts and effortposts for me, within the absolute comfiness of Qubes OS. I couldn't imagine relying on something thin and low powered unless it did very specific things (and, if it can't run Qubes, then it stays offline). reply fragmede 3 minutes agorootparentBecause what you've described isn't a laptop but a shitty desktop computer. Laptops that large are heavy, which sucks to travel with. Such a monstrosity isn't convenient to throw into your bag and bring with you everywhere all day. If you're not even using it for graphics, get a desktop, install Tailscale, and remote in for your shitposts. Is the opinion, anyway. Nothing wrong with your opinion (or mine), they're just opinions after all, but if I was on my feet all day in the field and needed a laptop, I'd rather something small and light with a long battery life. If I wanted something luggable, I'd go for a usb-c monitor and as small a desktop chassis as would fit a proper GPU inside of a mobile one. reply kossTKR 10 hours agoparentprevThere was also the Sony Vaio Laptop line that i remember as being quite a good product? But yeah they all seemed to die out. reply hinkley 2 hours agorootparentFujitsu lifebook. Smallest laptop with a removable CDROM drive. reply bane 19 hours agoprevI think this somewhat misses an important nuance. Japanese PCs had to be different early on because of the complexities of the written language. All of the important characters could be handled in just a few bits (7 or 8) and low resolution in Western markets, with different fonts and character maps dropped in to support a few different alphabets. But in CJK countries, things were much harder and the entire I/O system had to be significantly more capable than what might pass for usable elsewhere. This meant larger ROMs, larger framebuffers, higher resolution displays, more complex keyboarding systems, the works. Everything was harder and more expensive for a long time. A common add-on was ROMs with Kanji (Chinese derived characters) support in the same way a person in the West might buy a new sound card or get a VGA card. Except this was just so you could use your new $1200 computer (in today's money) to write things on. Back then, given limited memory, you also ended up with a ton of different display modes that offered different tradeoffs between color, resolution, and refresh. Because of the complex character sets, these Japanese systems tended to focus on fewer colors and higher resolution while the west focused on more colors at a lower res in the same or less memory space (any fans of mode 13h?). The first PC-98 (the 9801) shipped in 1982 with 128k of RAM and a 640x400 display with special display hardware. The equivalent IBM-PC shipped with 16KB of RAM and CGA graphics which could give you a display no higher than 640x200 with 1-bit colors but was mostly used in 320x200 with 4 (terrible) colors. Even with similar base architectures, these formative differences meant that lots of the guts of the systems were laid out different to accommodate this -- especially in the memory maps. By the time \"conventional\" PCs were able to handle the character display needs (sometime in the mid-90s), they were selling in the millions of units per anum which drove down their per unit prices. The Japanese market was severely fractured and in a smaller addressable market. Per unit costs were higher, but the software was largely the same. Porting the same businessware to half a dozen platforms cost too much. So now the average user of the Japanese systems had a smaller library of software which was more or less a copy of what was on IBM PCs, on more expensive hardware -- market forces solved the rest. (btw, the FM Towns, IIR, also had specialized graphics hardware to produce arcade-like graphics with tiles and sprites and so on, making it even more different) Some of this history also informs why home computing lagged in Japan compared to the West despite having all of the other prerequisites for it to take off. graphics https://www.pc98.org/ memory maps https://radioc.web.fc2.com/column/pc98bas/pc98memmap_en.htm https://wiki.osdev.org/Memory_Map_(x86) reply tkgally 18 hours agoparentExcellent summary. A few additional comments from personal memory: I have lived in Japan since 1983, and I started working as a freelance Japanese-to-English translator in 1986. I wanted to produce clean-looking text in English for my clients, so after a few months using a manual typewriter I took out a loan and bought a Macintosh with a dot-matrix printer. If I remember correctly, it cost six hundred thousand yen. The Mac could not handle Japanese; when I needed to write Japanese text, such as for notes to clients, I wrote by hand. I eventually bought a dedicated Japanese word processor for writing clean text in Japanese. Around 1992, I bought a modem and went online, first to a local foreign-run BBS and then, a couple of years later, the Internet. Many of the first friends I made online were Japanese-English translators like myself, and some of the most active discussion groups I took part in were about the Japanese language and translation. The display of Japanese characters in our online discussions was a problem for a long time. Even as more and more of the participants became able to type Japanese on their own computers, they were using a variety of OSs and character encodings, and the Japanese parts of their messages, when posted online, would be corrupted more often than not. When discussing a particular Japanese expression, we would have to romanize the Japanese and, sometimes, explain what kanji were used. Here’s are two examples from posts to a translators’ mailing list in 1998: > While this handbook uses \"åòå≈ê´\" for \"robustness\", the systems engineers I work with prefer \"ÉçÉoÉXÉgê´\" . > Ruth, the kanji for taikou are tai (as in taishi - Crown Prince) and kou (as in kugurido - the radical is mon with gou inside (gou = au/awasersu). Does this help? The dictionary meaning obviously does not make sense here. This made it impractical to discuss longer texts or to have our discussions in both English and Japanese. It was a great relief when, around 2000 or so, the encoding issues were gradually resolved and we became able to write Japanese freely in our online discussions. (Addendum: I am still in touch with some of the people on that mailing list, including the Ruth mentioned above. In fact, last month I attended a party in Yokohama in honor of her and her husband’s 55th wedding anniversary. Several other friends I first met online in the mid-1990s were there, too.) reply bane 16 hours agorootparentOh wow, that's a great personal story. I would imagine things begin to improve around 2000 due to the broad adoption of unicode? I remember there being an absolutely huge number of encoding systems for the various CJK languages back then, but I think Windows eventually guessed/settled on UTF-16 IIR. I didn't live in Asia during this time, but was heavily involved in writing some multilingual capable desktop windows software and was very aware of these challenges. I remember one colleague who worked on our Chinese language material having to buy an expensive copy of a Chinese British telegraph code book. reply tkgally 11 hours agorootparent> I would imagine things begin to improve around 2000 due to the broad adoption of unicode? Yes. I don't remember the exact timing, but it was the switchover to Unicode by Windows and Mac that finally resolved the character encoding conflicts (mostly). I don't know how much attention this got outside Japan, but there was significant opposition here to Unicode for a while from some Japanese intellectuals. Handwritten and, sometimes, typeset Japanese has traditionally allowed for a lot of minor variations in the forms of kanji. A person might write their name with an extra stroke in one character, for example. Some of those variants are ignored in Unicode as well as in the Japanese encodings on which it is based, and some people kicked up a fuss about it in the Japanese press. I remember reading rants accusing Bill Gates of trying to suppress Japanese culture by imposing a homogenized character encoding through Windows. The controversy has long since died down, though there have been efforts to develop more comprehensive character sets of Japanese and other Asian languages, such as TRON Code and Mojikyō: https://en.m.wikipedia.org/wiki/TRON_(encoding) https://en.m.wikipedia.org/wiki/Mojiky%C5%8D reply samatman 3 hours agorootparentOh it gets attention on Hacker News to this day. Any time there's a thread about something Unicode one or another of our Japanese colleagues will show up to make a fuss about how Unicode is quite insufficient for Japanese, for this reason or that. In 2024 I'm out of sympathy for this. There's an entire tertiary plane for extended Hanzi/Kanji/ideographs, if there are still missing glyphs please take it up with the Unicode Consortium, because there's plenty of room and it is reasonable to support one (1) character encoding now. But it is in fact Microsoft's fault that the farcical attempt to squeeze \"CJK\" into a two-byte encoding was even attempted. They were firmly committed to the idea that UCS-2 could ever be viable and we're stuck with various consequences of this fatuous premise to this day. reply oefrha 14 hours agorootparentprevIn fact, I occasionally run (currently maintained) CJK commercial software on my en-US Windows installation and I still run into ??? from time to time and have to guess what the text is supposed to be. It’s a shitshow. reply pezezin 10 hours agorootparentHeck, the stupid Windows application for the \"my number card\" has this problem, and it is an official application coming from the government itself! How is it possible that in 2024 we still have this problem? reply poincaredisk 2 hours agorootparentBetween 2010 and 2020 I've ordered something from Amazon around 5 times, and each time they spelled my legal name wrong, each time in a different way (due to encoding issues). And I'm an european, living in a large country. So I'm not surprised CJK languages have this problem. reply creakingstairs 5 hours agorootparentprevYeah and a part of e-tax literally asks you to have Japanese Windows in fine print or it fails silently without any warning messages (Fixed by changing chrome locale to Japanese thankfully). Gotta love Japanese websites. I just go into the office and hand write the forms because it beats trying to debug cryptic issues. reply Laforet 13 hours agorootparentprevThere is a system wide setting that changes all non-Unicode text encoding to another code page e.g CP932 for Shift-JIS. Third party tools are available to do the same conversion on a per application basis. It’s not as bad as trying to load some really old CJK web pages on mobile devices: few mobile browser has an accessible option to select character encoding and there appears to be none on iOS. The only option is to change the system language and that didn’t always work for more obscure character codes. reply Lammy 12 hours agorootparentprevhttps://xupefei.github.io/Locale-Emulator/ reply mappu 17 hours agorootparentprevWhat a wonderful story. I spent a while playing with `iconv` commands to solve your mojibake, reinterpreting bytes in and out of Shift-JIS, but I didn't get it - i'd love it if anyone managed to figure out the exact encoding, reply TheAceOfHearts 14 hours agoparentprevGoing way off the beaten path, this post makes me realize how lucky the West was to be able to develop practical computers when they did. And it also has me speculating how challenging it could be for an alien civilization to develop modern computing, depending on the complexity of their interface with reality. English is surprisingly utilitarian and straightforward for a first pass at a computing device. But one could imagine some alien civilization with far more foreign concepts of communication struggling to develop practical input devices and displays during a primitive computing era. reply Maken 11 hours agorootparentIt's not the English language but the latin alphabet that helped there. It had already been optimised in ancient times to a limited set of easily readable characters, and then was further standardised by the introduction of the printing press. Going from a collection of metal pieces to a collection of bitmaps is way easier than adapting a handwritten languaje (which already had problems adapting to the printing press). reply bane 1 hour agorootparentTo add to this, there are some specific characteristics or acceptable practices within European alphabets (by the time that computers were invented) that helped. - The total number of characters needing representation is small. In the most limited case, you can get by with just the majuscule (upper case) characters. This means you can represent all letters in just a few bits. Representing all upper + lower + numbers + basic punctuation for almost any single language in Europe, even non-latinate languages is possible in just 7 or 8 bits! - The glyphs are linearized in a consistent direction. So data storage to display logic is very simple. This is opposed to languages like Korean where characters are assembled into syllables and text can flow left to right horizontally or right to left vertically. - Most European languages have both print and cursive forms. By the 20th century it was acceptable to use the disconnected print form, which made display logic much simpler (just copy the 8x8 bitmap in ROM to the screen). But importantly, special semi-ornamental characters, like ligatures, were acceptable to separate. - A more or less universal base-10 number system meant that we only needed to include 10 digits in the character set. Arithmetic only requires a handful of symbols. So you can include a pretty big subset of math in your character encoding. This is opposed to other systems that use distinct characters in ways similar to Roman numerals (e.g. Chinese numerals which also have financial and lay versions of counting). - Punctuation and simple arithmetic characters had been basically boiled down to about a dozen glyphs. - If you use 8-bits for your character encoding, you have so much space you can also include pretty much every character variant needed for all Latin using languages, allowing you to hit an addressable market of billions of people with the same 16k ROM. reply smegger001 11 hours agorootparentprev>which already had problems adapting to the printing press as i recall the printing press was developed in the East first. Each page was essentially a wood cut and pressed on to the page, the major innovation that Gutenberg in the west had was movable type more than the press. reply _DeadFred_ 19 minutes agorootparentI think at this point we understand that 'inventing' or 'discovering' means coming up with the solution/discovery that took off in modern culture/usage in a way that shaped our current world. reply qwytw 9 hours agorootparentprev> was movable type And even then, Chinese had already used movable type ~400 years before Guttenberg, possibly even for \"serial numbers\" on printed banknotes. The oldest surviving book printed with movable type was published in the late 1370s in Korea: https://en.wikipedia.org/wiki/Jikji. Even woodblock printing, while it was seemingly know in Europe or at least the Byzantine Empire since the 11-13th centuries only became heavily commercialized and widespread around the same time as Gutenberg's press pretty much in parallel with it (for playing cards, illustrated books etc.). There was something special about Europe in the 1400s. I'm not downplaying his skills/ingenuity but Gutenberg's greatest achievement was doing what he did at the right time and place. It seems there wasn't enough demand for books/printed materials that would have justified the needed investments anywhere else. reply wrp 9 hours agorootparentAnother thing about the development of movable type. I've heard that Gutenberg's main contribution was the development of an alloy that made metal type feasible, and that the (earlier) Korean solution was to use ceramic type. reply qwytw 9 hours agorootparentThey also tried bronze and copper as well (mainly for money which used a mix of plate and movable type). The transition from wood to bronze metal type might have independent in Korea, though. They even had a pretty extensive printing industry as far as we can tell but it was fully controlled by the state/emperor and non official printing was illegal. I'm certainly not an expert but at least in China's case the type of inks they used didn't really work well with metal or even ceramic type which probably made the process relatively inefficient. Butyeah, Guttenberg invented a new alloy, ink, moulding method which made the process much cheaper and more effective. reply pjc50 8 hours agorootparentprevThe Gutenberg museum itself has an exhibition of Korean movable type. See also https://www.koreatimes.co.kr/www/culture/2021/06/145_311325.... : both Hanzi and Hangeul. reply ghaff 18 hours agoparentprevEven in the larger commercial computer space, Japan always liked to sorta do their own thing. Aside from a couple other companies, they were always big Itanium backers for example. I was an analyst during that period and Japan was always something of an outlier. (Europe was to some degree as well. But less so.) reply bonzini 14 hours agorootparentRegarding Itanium, that could be just by chance because Fujitsu is the main (or almost only) seller of supercomputers in Japan partnered with HP. Also interesting however is how they switched to it from SPARC and actually kept using SPARC longer than the west. reply ghaff 7 hours agorootparentNEC and Hitachi were involved as well. There was some sort of dynamic of collectively wanting something different from commodity x86. reply Dwedit 15 hours agoparentprevDOS/V (Not to be confused with the similarly-named MS-DOS 5.0) is what made standard PCs able to run Japanese software. It provided a software emulation of a full Japanese text mode with Kanji, just requiring VGA. reply ViktorRay 19 hours agoparentprevVery interesting! Thanks for posting this! reply RajT88 17 hours agoprevAs an aside my recent trip to Japan, I hit up all the crazy gaming stores hoping to find an FM Towns or the even more rare FM Towns Marty. They looked at me like I was a three headed monkey. reply Findecanor 1 hour agoparentI think a better chance of finding anything vintage would be Yahoo! Auctions and if you're not living there: a proxy-shipping service such as Buyee.jp. BTW. My Holy Grails as a vintage keyboard collector are ergonomic keyboards with columnar layout for the PC88 and PC98 computers ... or a B-TRON keyboard. reply mappu 16 hours agoparentprevI looked around too - Mandarake only had popular consoles; Super Potato in Akihabara and Retro TV Game Revival in Osaka had MSX, but no FM Towns. The store clerk read my enquiry off Google Translate on my phone and gave me a one-word reply: いいえ. It probably takes local expertise to find one in someone's attic. Playing its Lupin III exclusive game might have to remain a MAME job. reply pezezin 10 hours agorootparentIf you are looking for retrocomputers in Akihabara, the place to visit is BEEP. It is the only shops that specializes in that kind of stuff. reply Tiktaalik 15 hours agoparentprevI did see some old PCs of that sort at Hard-off further afield. I don't recall where. May have been in the burbs around Osaka. In Tokyo the easily accessible source to have a peek at some computers like this is BEEP in Akihabara. Edit: Yep looking through my photos I saw an FM Towns Marty at a Hard Off in Kanazawa. ¥49500. Saw an actual FM Towns at a Hard off in Hachioji. More ¥77000 when you combine the monitor and computer. That last big Eco Town in Hachioji was quite the old PC source. Also had a PC 98 and boxed (!) X68000. reply RajT88 15 hours agorootparent> ¥49500 At current exchange rate, that is a steal! Kanazawa is... Sadly not typically a place you go if you're on a brief visit of a couple weeks. When I was studying abroad in 2001, I saw an original FM Towns tower case basically in the junk pile. I wanted it even then for how distinctive it was, but had no clue what it was. What might have been! reply JSR_FDED 11 hours agoprevI remember working in Taiwan in the late nineties and noticed that the PE2 editor for MSDOS was incredibly popular, because you could easily define macros that mapped to multi-character sequences, which combined with a BIOS that had character tables for simplified Chinese meant you could generate Chinese text without too much difficulty. To this day my vimrc has some of those PE2 macros :-) reply DrNosferatu 3 hours agoprevThe MSX in Japan had a Japanese character set. Was this good enough for mainstream word processing needs? Really curious to know! PS: I mean letters, essays, etc - not DTP. reply amelius 6 hours agoprev> The thing about DOS is that it’s a much thinner OS than what we think of in 2024. When you’re writing DOS software of any kind of complexity, you’re talking straight to the hardware, or to drivers that are specific to particular classes of hardware. How is that different from how we access our GPUs nowadays? reply bitmasher9 4 hours agoparentToday we access GPUs through standard APIs like DirectX, CUDA, OpenGL etc. In DOS there wasn’t these standards, and you had to write code for specific hardware. reply amelius 4 hours agorootparentTrue, but our OS has no idea what processes run on which GPU, or how much GPU memory they use. Everything on the GPU happens behind the back of the OS. reply tantalor 17 hours agoprevThis would be much more comprehensible if the author would include some dates. reply PhasmaFelis 18 hours agoprevI was gonna say, the same thing that happened to all the western PC platforms that weren't Microsoft or Apple. Commodore and Atari and Acorn and Sinclair and Dragon and probably dozens of others I've never heard of. As computers became more powerful and development costs rose, small-market architectures and OSes simply became unsustainable. You had to either reach sustained global success or die. I'm sure there were some unique challenges for architectures that mainly served Japan, but I doubt they were that much worse than the ones facing the ones that mainly served, say, Britain. All of them lost the race in the end. The same thing happened again with graphical cellphones! In the flip-phone era there were a zillion different OSes with their own app libraries. For a while it looked like Blackberry was set to be the Microsoft of the upcoming smartphone era, and then Apple stole their thunder, and no one could compete except Android and Windows Phone, and then Windows Phone dropped out too, and now we're back to two basic architectures with no meaningful competitors, just like the home PC market by 1996. > By 1994 though, they had a problem: the 32-bit consoles were out, which could do 2D games just as well as the FM Towns and X68000, and the consoles could also do 3D that blew away anything those computers could handle. This line from the article caught my eye in particular, because it's similar to what happened to Commodore's Amiga, one of the last real Microsoft competitors in the West. Essentially, Doom killed it. There's a rather tragic list of Amiga games that struggled valiantly to be Doom on that platform, and some of them were pretty good but none of them could really match what Id could with a tricked-out DOS machine in 1993, and that was more or less that. reply TacticalCoder 12 hours agoparent> Commodore and Atari and Acorn and Sinclair and Dragon and probably dozens of others I've never heard of. Let's not forget the french. They had Oric, Matra (IIRC), Thomson and... The Minitel. And now they're absolutely nowhere to be seen, just like Japan: it's either PCs (basically running Windows or a Un*x) or Macs. reply 0xbadcafebee 19 hours agoprevI never thought about this before, but product competition is basically evolution in action. Entities with more desirable traits that adapt better to a given ecosystem survive, the rest don't. (In addition to things like a pre-existing dominant species having advantages over new ones) (fwiw, Windows won out because it had better business strategy. Apple wanted to be in everyone's homes; Microsoft wanted to be in everyone's business. One of those is easier to sell to in bulk, and easier to charge more money. In addition, Windows being more hardware-agnostic, and encouraging an ecosystem of competing hardware manufacturers, allowed them to invest less in hardware themselves, while creating an industry that would vie for business on Microsoft's behalf. This is of course different than the \"workstation\" market of uber-high-powered individual computers, which sort-of still exists, though with PC hardware) reply acdha 19 hours agoparentWindows also won by parasitizing a previously bigger host (Bill Gates’ mother was on IBM’s board), and shutting out competition by forcing vendors not to offer other companies’ software if they wanted Microsoft licenses at better than retail pricing. reply nine_k 19 hours agorootparentThis is how MS-DOS and early Windows won. But the first version of MS Excel was written for MacOS. And it's MSO what's holding businesses on Windows, not the other way around. reply orionblastar 18 hours agorootparentMSO doesn't work in WINE at least the latest versions don't. reply smegger001 11 hours agorootparentI recall being told that office uses undocumented Windows APIs making it harder for groups like WINE and Proton to support them. whether or not thats the intent or a happy accident well... reply canucker2016 18 hours agorootparentprevBill Gates' mother, Mary Gates, was not an IBM board member. She was on the national United Way's executive committee. Also an executive committee member was IBM's Chairman, John Opel. see https://www.nytimes.com/1994/06/11/obituaries/mary-gates-64-... [edit] also Windows OEMs always got lower than retail price for Windows licenses (assuming your volume sold was high enough) from https://en.wikipedia.org/wiki/Bundling_of_Microsoft_Windows#... : ==== Microsoft once assessed license fees based on the number of computers an OEM sold, regardless of whether a Windows license was included. Beginning in 1983, Microsoft sold MS-DOS licenses to OEMs on an individually negotiated basis. The contracts required OEMs to purchase a number of MS-DOS licenses equal to or greater than the number of computers sold, with the result of zero marginal cost for OEMs to include MS-DOS. Installing an operating system other than MS-DOS would effectively require double payment of operating system royalties. Also, Microsoft penalized OEMs that installed alternative operating systems by making their license terms less favorable. Microsoft entered into a consent decree in 1994 that barred them from conditioning the availability of Windows licenses or varying their prices based on whether OEMs distributed other operating systems. ... In 2009, Microsoft stated that it has always charged OEMs about $50 for a Windows license on a $1,000 computer. ==== reply acdha 6 hours agorootparentYou’re right about the boards - it’s been a while but the main point was that this wasn’t just pure open competition for their biggest break. My focus on the licensing was this part which your quote included: “Microsoft penalized OEMs that installed alternative operating systems by making their license terms less favorable”. The consent degree and other legal cases took a while to apply any effective counter pressure, and by that point Microsoft had managed to effectively starve competitors (DR-DOS, GEOS, BeOS, OS/2, etc.) of revenue which would have made the 80s and 90s marketplace more competitive. They knew that staying the default choice for businesses as long as possible meant that those companies would acquire a library of software and training which only worked for their operating systems, and successfully banked on a slow government response. reply makeitdouble 14 hours agoparentprevWe're talking about international trade, so trade barriers and foreign policies play a huge role that go beyond simple competition. Japan had a hell of a time to deal with car exports in the US, SONY still won but the reaction to it was also extremely strong. A Japanese company had absolutely no chance to go hit the US market and displace companies like SUN, Apple or IBM on the US soil. In particular software IP is a whole lot harder to fight for than manufacturing IP (even if it gets stolen, it's moot if it can't be physically applied. In software land reproduction is a given) Europe wasn't much an easier target either. We saw that later with docomo failing miserably while having a clearly superior product. In reverse the US has a long history of opening the Japanese market when they really want to, and forgoing that market isn't critical either, so there's quite an asymmetry. The only ground Japanese company learned to properly fight has been video games so far. reply tihwih2o28092 5 hours agorootparent> The only ground Japanese company learned to properly fight has been video games so far. Nintendo yes; Sony PS division is more or less run out of SIE in San Mateo. reply mcdow 11 hours agorootparentprevCould you speak more to the US “opening the Japanese market when they really want to”? I’m not familiar with this. reply makeitdouble 10 hours agorootparentA bit far from the technology related fields, but the beef import agreements are the most explicit on this part [0]. US beef was found in clear violation of the safety rules multiple times, but Japan had to bend anyway (\"The government has put priority on the political schedule between the two countries, not on food safety or human health.\"). Japan is typically not doing great in its relationship with Korea or China, or even India, nor the EU really (France and Germany are closely friendly, but won't give much economic benefits), so the US have a pretty strong leverage when it comes to negociations. [0] https://en.wikipedia.org/wiki/United_States_beef_imports_in_... PS: there's a mountain of other reasons now, but Japan and Russia were also not doing great as they've been fighting over the northern islands for decades. To my eyes they really really suck at international relations in general. reply pezezin 10 hours agorootparentprevhttps://en.wikipedia.org/wiki/Black_Ships#Gunboat_diplomacy reply cherryteastain 10 hours agorootparentprevi.e. \"open up your market to us and stay silent about our tariffs on your products, or else\" reply bane 19 hours agoparentprevYes! And it's very interesting to consider two additional things: 1. how seemingly \"less capable\" technologies win out in this evolutionary environment 2. how plentiful VC (and to some extent government funding for R&D) distorts normal \"evolutionary\" forces in a market reply 0134340 17 hours agorootparent1. In that case those that were adopted tended to be the cheaper and more ubiquitous technologies, ie, at a biological level just more calorically cheaper to adopt and perhaps efficient to maintain. 2. VC and general funding, ie supporting an entity, is a feature of evolution. I guess I expected better of HN but it seems people don't realize that nothing we can do will stop evolution and everything we do is just a feature of it. reply 0xbadcafebee 15 hours agorootparentprevYeah; I think it's our ego, or \"common sense\", that makes us think that the most advanced thing will win out in the end. In reality it's the most well-adapted thing that wins out in the long term. If you ever see a thing and think \"this is really advanced, this must be the future\", think again. I'm not sure I agree that VC, government, etc distorts evolution in a democratic republic with a capitalist economy. Evolution still happens within that ecosystem. The economy is the climate, and the laws, politics, people, culture are the landscape. That ecosystem includes VCs along with all the other things. Within that environment, the best adapted thing survives. Sadly, that \"thing\" (a corporation, for example) may make poor decisions that lead to its demise, the way a snow leopard leaping for a goat on a mountain side may make them both tumble to their doom. Nature's a cruel mistress. I could even see something like Soviet Russia being an ecosystem that evolution still happens in. It's a very extreme environment, to be sure, but evolution still happens in the most extreme environments on earth. reply bane 1 hour agorootparent> I'm not sure I agree that VC, government, etc distorts evolution in a democratic republic with a capitalist economy. Evolution still happens within that ecosystem. I'm definitely not claiming that evolution doesn't occur, but that the temporary supply of capital that VCs (and similar sources) supply creates an artificial environment that creates distorted evolutionary pressures. When the VC money runs out, and technologies are \"returned to a natural ecosystem\", the end result is often not that the VC funded tech was the fittest in the market. Meanwhile, if a particular technology consumed the market by distorting its fitness function and eliminates otherwise healthy competitors along the way, the VC backed tech might survive in the end, but it's not clear that it would be the fittest given natural market forces. However, everybody else is dead. To really stretch this analogy, it's a bit like an environment with several predatory cat species - each good at a particular strategy. One day a team of hunters comes in, kills all the prey, and selects a specific cat species as \"the champion\" and simply feeds it in captivity. Once the rest of the cat species die off, they release the champion into the wild where lack of competition stalls its own further evolution. What cat would have been the \"best\" in the given ecosystem? Nobody knows. There's a few interesting cases around this. Palantir comes to mind as a particularly useful case. They entered a market with a dozen competitors all surviving in the market, flooded it with something like 13 or 14 rounds of fundraising's worth of product, nearly annihilated everybody else in the market and when finally released to the wild, found that their signature product line was no longer what the market wanted at the size of the market the VCs had assumed. Today the company more or less acts as a bespoke web application company. Mimic competitors like C3.ai are also suffering under similar fates. For a government funded use-case look no further than the space launch market. For decades it was funded by dumb government money and regulations that gave the appearance of a competitive market but was intended more to ensure strategic options. Those companies evolved/optimized to capture this steady supply of crippled prey money but it really created a bunch of sick, inbred, organizations. SpaceX appears, acts like a hungry wild tiger in a room full of Pugs and proceeds to dismantle them all, while building new markets and capturing the easy prey dripping out of the government funnel. There's no magic physics to SpaceX, all of the competencies existed in the industrial pipeline, but the government funding mechanism evolved them in unnatural ways. SpaceX, being heavily government funded, may yet evolve into one of those, but for the moment it's the difference between a Wolf and a domesticated small apartment dog. reply hedora 15 hours agoparentprevI first figured this out when looking at filesystems. Ext4 is a generation behind netapp wafl (from the 1990s). ZFS is arguably competitive with some of the enterprise filers from back when sun existed. There have been at least four generations of commercial filesystems since then. So, a randomly chosen on-prem filer will be 3-5 generations ahead of Linux. The reason is economic: In that space, companies have a half life of 5 years, but influential open source filesystems last 20+ years. reply ghaff 6 hours agorootparentZFS would almost certainly be more widely adopted were it under a different license. But Red Hat, for one, wasn't going to touch it given CDDL. reply 0134340 17 hours agoparentprevAnd entities that become too great and harmful to other entities (monopolistic) get challenged, even if they can provide some good, and from that challenge they sometimes get parasitized as well. Business competition, therefore human behavior, is natural no matter what way you want to politicize it. reply DrNosferatu 4 hours agoprevI guess Unicode happened? reply BoingBoomTschak 18 hours agoprevHow about the US and MS ruining everything as usual? \"In April 1989 the Office of the U.S. Trade Representative issued a preliminary report accusing BTRON of being a trade barrier, as it only functioned in Japan, and asked the Japanese government not to make it standard in schools. TRON was included along with rice, semiconductors, and telecommunications equipment in a list of items targeted by Super-301 (complete stop of import based on section 301 of the Omnibus Trade and Competitiveness Act of 1988). It was removed from the list after the USTR inspection team visited the TRON Association in May. In June the Japanese government expressed their regret at U.S. intervention but accepted this request not to make it standard in schools, thus ending the BTRON project. Callon opines that the project had nevertheless run into such difficulties that the U.S. intervention allowed the government to save face from cancelling the project. According to a report from The Wall Street Journal, in 1989 US officials feared that TRON could undercut American dominance in computers, but that in the end PC software and chips based on the TRON technology proved no match for Windows and Intel's processors as a global standard. In the 1980s Microsoft had at least once lobbied Washington about TRON until backing off, but Ken Sakamura himself believed Microsoft wasn't the impetus behind the Super-301 listing in 1989. Known for his off the cuff remarks, in 2004 governor of Tokyo Shintaro Ishihara mentioned in his column post concerning international trade policy that TRON was dropped because Carla Anderson Hills had threatened Ryutaro Hashimoto over it.\" https://en.wikipedia.org/wiki/TRON_Project reply agumonkey 7 hours agoparentpotential demo of BTRON https://www.youtube.com/watch?v=yYfoCe6q28A reply TacticalCoder 12 hours agoparentprev> How about the US and MS ruining everything as usual? My personal theory is that MS set the human race back at least 20 years. YMMV. reply agumonkey 7 hours agorootparentHow would your alternate timeline look, genuinely curious. Sometimes I fear that the current one looks mediocre but if you took another path it would have been worse for random reasons. reply ghaff 6 hours agorootparentGiven the independent software vendor model, you were going to end up with a very small number of mass market operating systems/platforms one way or the other. It was certainly happening in the large commercial computer system space. In fact, the somewhat surprise is that Microsoft didn't become even more dominant. A lot of people certainly expected it to. reply Dalewyn 17 hours agoparentprevNo; Japan would have eventually stumbled and fell into obscurity even without any American interference. It's a running joke at this point, because Japanese cannot compromise quality to make costs reasonable. In an environment where the world produces goods 80~90% as good as Japanese ones for 1/4th~1/8th the cost, who in their right mind buys Japanese? This is what happened to Japan's entire electronics industry, home appliance industry, and more. Japan still has a domestic computer industry by a technicality (it's all Made In China) primarily fended over by Hitachi and Panasonic, but most westerners likely won't know because they simply don't sell overseas. I'm not going to bother getting into how Japanese are horrible with software too. reply hakfoo 16 hours agorootparentThe cost/quality thing doesn't mean they couldn't have maintained a \"halo product\" line that steered the platform. It could have been like the early 1990s PC market: government with a 'buy domestic' mandate or budget-no-object buyers might have bought a PS/2 Model 80 or Deskpro 386, but the masses, especially overseas, would buy a white-box 386DX made of Taiwanese parts. There's an interesting contrast: while Japan produced MSX-- a clear example that a multi-vendor standard can be wildly successful-- they missed the idea of a clone ecosystem for their heavier-duty professional machines. Why weren't there vendors cranking out clones of the PC-98, FM Towns, or X68000? Did they require more propriatery special sauce than an IBM 5150, or was there a cultural/market difficulty that would have caused them to flop on the market? OTOH, perhaps part of the problem was that the features Japan needed had poor cost/benefit ratios outside of the CJK market: you either have to make the enhanced video stuff optional, reducing platform standardization, or charge people for a feature they don't see as immediately beneficial. reply pezezin 10 hours agorootparentprev> It's a running joke at this point, because Japanese cannot compromise quality to make costs reasonable. As someone living in Japan, I don't buy this argument. There are plenty of shoddy Japanese products, number one being the houses (but I guess that is not something that you can export). > I'm not going to bother getting into how Japanese are horrible with software too. Here I fully agree. Anybody who has to suffer the software and web services made in here knows how bad it can be. reply BoingBoomTschak 6 hours agorootparentprevCars and game consoles kinda disagree with you. For example, Mazda (now exclusively built in Japan) is a fair compromise between quality and cost. About software, I don't know. They seem to be very decent as far as research and embedded goes, even a bit of open source (I can cite https://github.com/guicho271828, https://github.com/fukamachi and https://github.com/cxxxr on the top of my head), but consumer software appears to be another story. I don't think it's for this reason, though. I can even remember them having a strong affinity with Prolog... ah, there's the link: https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Syst... reply hedora 15 hours agorootparentprevMy switch and midrange sony tv (which just turned ten years old, and is still “good enough”) seem like decent counterexamples. reply justsomehnguy 14 hours agorootparentprev> It's a running joke at this point, because Japanese cannot compromise quality to make costs reasonable. An old anecdote: A company places an order to produce some gizmo through a Japanese firm. The order states what is could be \"up to three non-working gizmos per one hundred in the final shipment\". The order is completed, the employee comes to get it, sees a big packed boxes and a small one. He asks \"what's in the big boxes?\" and receives \"That's your one hundred gizmos, like you ordered\". He asks again \"But what is in this small box there?\" and receives \"That's your three non-working gizmos, like you ordered\". reply Razengan 14 hours agoprevIn a YouTube video about the history of OS/2, I learned that ole Microsoft back in the 1990s before their mob tactics were curtailed, used to send suited thugs to Japanese PC manufacturers to chastise them for even offering users the mere option of buying PCs with OS/2 instead of Windows.. Could such practices have stifled the innovation and growth within the Japanese PC industry? They did have some takes of their own on the PC platform with that unique Japanese flavor, in series like the PC-8800/PC-98, FM Towns, etc.: https://en.wikipedia.org/wiki/PC-8800_series https://en.wikipedia.org/wiki/PC-98 https://en.wikipedia.org/wiki/FM_Towns Who knows what more they could've done if Windows 95 hadn't smothered everything under the sun? Like the ill-deserved demise of the Commodore Amiga, this seems like a failure of politics than merit. reply permo-w 19 hours agoprevis \"PC platform\" the standard term here? I'm not saying it isn't, it just sounds a little odd to me. reply bane 19 hours agoparentYes, it stands for \"Personal Computer Platform\". reply fnord77 17 hours agoprevsadly, no pics. The FM Towns was kinda cool looking reply nisten 15 hours agoprevthey got old reply Apocryphon 19 hours agoprev [–] Also worth watching: Why is Japan So Weak in Software? by Asianometry https://www.youtube.com/watch?v=ky1nGQhHTso reply aeadio 18 hours agoparentAsianometry also has a video on the history of the Japanese PC market, https://youtu.be/CEtgzO-Im8w reply canucker2016 17 hours agoparentprevReminds me of a Japanese software company I applied and interviewed at when I graduated university. Company name? Bug Software. A quick internet search shows no relevant results for the company. reply ripcity512 7 hours agorootparentWas this the place? bug.co.jp Apparently they changed their name last year. reply canucker2016 44 minutes agorootparentThat looks like it! Thanks. reply terminalgravity 19 hours agoparentprev [–] I wish there was a TL;DW bot to summarize a videos like this. I’m curious but not in a place i could easily watch a video. reply chmod775 18 hours agorootparentAsianometry's videos are good precisely because of the detail and background he goes into. If you summarize them you take that away and pretty much just end up with what has already been said here. reply immibis 10 hours agorootparentI tend to bookmark Asianometry videos to watch later because they seem very informative but I'm rarely in the mood to actually listen to the very dry documentary style. They don't make good background noise for instance - I need to focus to accept the new information. At the same time they don't naturally attract my focus. reply Dalewyn 17 hours agorootparentprevIt's a 20 minute long video, the information density can almost certainly be denser. reply rowanG077 5 hours agorootparentI don't understand this argument. What has video length to do with whether it can be denser? This is like looking at a 1gb file and saying it could certainly be smaller. reply chmod775 17 hours agorootparentprev> It's a 20 minute long video You mean 20 minutes short. There's enough in there to blow it up into a 45 minute documentary at least. You already spent more than 20 minutes commenting under this story. > The information density can almost certainly be denser. And what would be the point of that? There's a limited amount of information one can retain in a short span of time, and it's not like he repeats himself or has a verbose style. I already go back and rewatch his videos later, taking new pieces of information from them. Again, if you want the tldw, it's already in the comments here. If you want the details, go watch the video. The video is being linked because the video itself is good. Wanting a summary that retains the same qualities is like wanting to have your cake and eat it too. reply Xelbair 8 hours agorootparentVideo by itself is less information dense than text. 20 mintute video could've been a 2-5min read essay. Not to mention the convinience of being able to easily re-read parts of it, and find reveland information instead of seeking the video. reply Dalewyn 16 hours agorootparentprevI generally read faster than some narrator slowly babbling on over a meandering script, so that is 20 minutes long. If the video is 20 minutes long, I wager I can read an equivalent article in less than 5 minutes and come out enlightened all the same. Videos are great for getting the eyes of the general man who doesn't have a preconceived interest in a subject, you're trying to bait clicks and videos are great for that. For people already interested in the subject though? Videos are almost always a literal waste of time compared to a well written article. And if you wanna say I have a short attention span: Sue me. I'm a 35 year old millenial, we're infamous for having short attention spans. reply nottorp 11 hours agorootparent> If the video is 20 minutes long, I wager I can read an equivalent article in less than 5 minutes and come out enlightened all the same. Not to mention that skimming through this page of HN comments does NOT take 20 minutes. More like 45 seconds. > And if you wanna say I have a short attention span Short life. Not attention span. If you get your info in writing you waste 4x as little of your life getting it. reply SllX 16 hours agorootparentprevYou my friend may benefit from developing the arts of the 2x speed, the skipping, the scrubbing and the stopping. Not every video is worth watching to completion (some are, you get a feel for it), there may be background details you want to skip or scrub through eyeballing the thumbnails depending on familiarity with the subject matter and sometimes everything you want to know is right at the end of the video in a neat little summary. The comments can even give you some insight into where the video is going and whether you want to continue if you read through some of the top ones during playback. I’m not much younger than you, but watching and re-pacing YouTube for educational/information videos is a skill that can be refined and the visual imagery can provide details that again, depending on what it is, might be missed in a written summary. And hey, if none of this is for you, maybe this comment helps someone else out. reply bigstrat2003 13 hours agorootparentI mean... you can do that, yes. Or we could use the far superior medium of text, where you don't need hacks to get around how slow it is. reply bdw5204 13 hours agorootparentOne reasonable compromise would be for video makers to provide a transcript or written article to complement their video. Video is a terrible format especially when you're actually using the video and not just using it as a mechanism to deliver audio. Audio is not a bad medium because you can do something else while listening to it. reply SllX 13 hours agorootparentprevI mean you could restrict yourself to only a single medium, independent of what the rest of the world is doing; or you can learn to process information efficiently regardless of medium and respect each medium for its own strengths and weaknesses. A good YouTube video produced perfectly needs none of the “hacks” I listed above and will relay far more information on complex subject matter in context than just an essay will, but people are more comfortable writing will write and people who want to make videos will make videos. reply ddingus 13 hours agorootparentThis too is the way. I am quite happy to take good info produced for me in almost any form. We all have options. reply lotsofpulp 13 hours agorootparentprevThere is a slight conflict of interest where more money can be earned by wasting the information recipient’s time via advertising. Text offers less opportunity to do this. Perhaps some amount of time wastage is necessary to incentivize the information providers to provide the information, but the pendulum can also swing too far. reply SllX 13 hours agorootparentThat’s why I got good at getting through videos quickly and figuring out when or if they’re a waste of time. There’s plenty of “research” videos that are just spewing crap that can be found on a wiki or a database somewhere else on the web; but see enough of them and you pick up on the pattern and cadence and quality they’re produced at quickly enough to just move on when you see it. reply ddingus 13 hours agorootparentprevThis is the way, along with just listening during other tasks. reply ddingus 13 hours agorootparentprevSame. Reading is always faster than watching video. However, listening to one can be done while driving, or doing many other tasks. Expecting producers to cater to the can read fast crowd is not realistic. People are just not going to produce for us. And I do not believe they should. There are options. A big one is listening. reply smegger001 11 hours agorootparentespecially now with you tubers vainly trying to placate the fickle algorithm gods by stretching out videos to meet time minimum lengths reply ddingus 13 hours agorootparentprevNope. That producer packs it in solid. Yes, it could be more dense, but at the expense of it being watchable by most people. This is a case of just because one can does not mean one should. Having an audience matters. It matters more than optimal info density does. Besides, just watch it at 2x. With this producer doing that is challenging. Pay attention! :) reply littlecranky67 10 hours agorootparentprevThere is kagi (paid search engine) summarizer for youtube videos: https://kagi.com/summarizer As for the above link, it gives: \"Japan has a large trade deficit in software, importing far more software and services than it exports. Despite having iconic hardware companies, Japan lacks major software giants like Microsoft or Oracle. This is due to a history of government policies that favored hardware over software development, as well as a shortage of skilled software engineers and a lack of software startups in Japan. While Japan has made efforts to develop domestic software platforms, they have largely failed to gain traction. The video suggests there are no easy solutions to Japan's software industry challenges.\" reply skissane 13 hours agorootparentprevI didn't watch the video but I skim read the YouTube transcript. The video doesn't propose any single explanation, just a series of events all of which arguably set back Japan's indigenous software industry. A few of the incidents it mentions include (my summary below is more based on my own knowledge of the topic, than what the video specifically mentions): Fujitsu and Hitachi cloned IBM mainframes. So did lots of other companies. At the time they started doing it, IBM was (intentionally) releasing their software into the public domain. However, in 1969, IBM announced they'd start copyrighting their software. Initially they still released the core OS (primarily MVS) into the public domain, and only copyrighted add-ons. However, as the 1970s progressed, more and more new functionality went into the copyrighted add-ons, while the public domain core received only limited enhancements. Finally, in the early 1980s, they put the whole OS under copyright. This left Fujitsu and Hitachi in a difficult position. They were used to getting their mainframe OS from IBM for free, and suddenly they couldn't legally do that any more. Legal choices for them would have included: (1) fork IBM's operating system and create new enhancements themselves (either clone IBM's copyrighted enhancements by clean-room engineering, or design their own incompatible enhancements), (2) negotiate with IBM for a license (unclear if IBM would agree, and may have cost $$$), (3) license an alternative operating system (e.g. UNIX), (4) build their own OS from scratch. But none of those options appealed to them (or maybe they tried some and it wasn't working out), so they decide to go with option (5): illegally copy IBM's copyrighted mainframe operating systems. They used the fact that IBM still shipped the source code for much of its copyrighted software to customers, and somehow got customers to (illegally) hand that source code over. They made rather trivial changes to the source code to try to hide the copying–for example, Fujitsu renamed a lot of IBM routines whose name started with the letter I, to start with the letter J instead. They searched and replaced IBM copyright notices with their own. They even bribed IBM employees to give them IBM confidential material (the IBM employees accepted the bribes as part of an FBI sting operation). And IBM found out, and sued both Fujitsu and Hitachi, and the settlement of the suit required Fujitsu and Hitachi to pay IBM hundreds of millions of dollars, and also banned Fujitsu and Hitachi from continuing to sell the software outside Japan (IBM agreed to let them continue selling it in Japan, in exchange for them paying licensing fees.) Other stuff I know about this topic (not in the video): In the 1980s and early 1990s, Fujitsu mainframes were quite popular in Australia, but due to this settlement, by the end of the 1990s, basically all of Fujitsu's Australian mainframe customers had either migrated to IBM mainframes, or else to non-mainframe platforms. There are still Fujitsu and Hitachi mainframes running in Japan today, but they are deeply legacy, basically stuck in the 1990s – they didn't follow IBM's transition to 64-bit in 2001. Fujitsu and Hitachi weren't the only mainframe vendors faced with this problem, but other vendors sought to solve it within the confines of the law. In the US, Amdahl had the same issue, but it decided to focus on their Unix variant UTS instead of MVS. (Amdahl did have an internal project to build a clone of IBM's MVS, apparently based on legal clean-room reverse engineering, called Aspen, but it got caught in development hell, and Amdahl cancelled it before they ever officially shipped it, although possibly a few customers got beta test versions.) Germany's Nixdorf had a fork of IBM's DOS/VS operating system (for low-end mainframes), which they got by acquiring the American company TCSC; they ported the Unix clone Coherent to run on top of it, before killing it off in the late 1980s when Nixdorf decided to give up on mainframes and focus purely on Unix instead. Other mainframe vendors didn't have this problem because their operating systems were not based on IBM's – for example, the other Japanese mainframe vendor, NEC, their mainframes run a fork of GE/Honeywell/Bull's GCOS operating system (ACOS), which NEC legally licensed. Another incident the video discusses is the TRON project, which was a Japanese indigenous standard for operating system APIs, endorsed by the Japanese government, conceptually similar to POSIX. It included both variants aimed at general purpose computing (BTRON) and embedded systems (ITRON). However, this frightened the US software industry, which convinced the US government to declare TRON a \"trade barrier\". And that mostly killed TRON as an operating system. TRON didn't die completely, it still sees some use in embedded systems even today (the video mentions the Nintendo Switch Joy-Con controllers run it), but it never achieved the original vision of becoming Japan's standard operating system. Instead, Microsoft Windows did. And then there were also macroeconomic issues (Japan's real estate crisis in the 1990s), and cultural issues – it mentions how the Japanese government encouraged Japanese industry to focus on copying successful Western technologies, even improving them incrementally in the process, as opposed to coming up with fundamentally novel technologies of their own. That approach served Japan very well for industries such as cars, but doesn't work so well for the software industry. reply formerly_proven 9 hours agorootparentSiemens did real mainframes and their mainframe OS BS2000 is still around, it's just part of Fujitsu, Nixdorf appears in that story as well because that's how the Siemens mainframe division ended up at FSC (Siemens acquires Nixdorf, folds its mainframe division into that, then splits it up into the ATM business and sells the rest to Fujitsu). reply skissane 7 hours agorootparentNixdorf shut down their mainframe business in 1989, and sold the remnants to Comparex (which started out as a Siemens-BASF joint venture, but Siemens withdrew around the same time as Comparex acquired Nixdorf's mainframe business). So when Siemens and Nixdorf merged in 1990, Siemens did not acquire Nixdorf's mainframe business, only Nixdorf's other product lines (Unix systems, ATMs, etc). But Siemens still had their own mainframe business. Comparex already sold IBM-compatible mainframes, so they didn't continue Nixdorf's mainframes as an independent hardware line, they were primarily buying the support contracts and the customer base. Siemens mainframes and Nixdorf mainframes had significant differences: Siemens BS2000 mainframes were derived from RCA Spectra 70. Their ISA was mostly IBM-compatible in user mode (problem state), but significantly different in kernel mode (supervisor state), and their operating system was completely incompatible–the BS2000 operating system was derived from RCA TSOS. RCA sold their mainframe business to Sperry, who then merged with Burroughs to form Unisys. The RCA Spectra mainframes became Unisys' Series 90 mainframe line, and RCA TSOS was renamed to Unisys VS/9. But by the 1980s or early 1990s, the RCA-derived Unisys mainframe line was dead. Whereas, their Sperry and Burroughs heritage mainframe lines (Unisys OS 2200 and Unisys MCP) survive today, although now they are software emulators running on x86-64 servers instead of physical hardware. RCA Spectra/TSOS only survives today in the BS2000 branch, save that Siemens ended up selling it to Fujitsu. By contrast, the Nixdorf mainframes were more straight IBM clones, and so aimed for instruction set compatibility both at the user application and operating system level, and could run IBM operating systems. They were mainly used with the low-end IBM DOS/360-derived operating systems rather than the high-end MVS operating system family. Nixdorf faced the same problem that Fujitsu and Hitachi did, of IBM closing their operating systems, but they solved it by buying the American software company TCSC, who maintained their own fork of the IBM mainframe DOS, called Edos, which Nixdorf then renamed NIDOS (Nixdorf DOS). TCSC had started Edos when IBM decided to make new DOS versions available only for S/370, not for older S/360 machines, hence Edos was originally a backport of those newer S/370-only DOS versions to the older S/360 machines. When Nixdorf bought TCSC, they renamed it NCSC. NIDOS ended up offering features that IBM DOS/VSE never had, like a Unix compatibility subsystem (PWS/VSE-AF, derived from Coherent) – much latter, MVS (now z/OS) and VM/CMS (now z/VM) ended up getting one, but DOS/VSE (later z/VSE and now VSE^n since IBM offloaded it to 21CSW) never has. Siemens also once had a lower-end mainframe line, which ran an operating system optimised for smaller machines, BS1000. BS1000 was discontinued long ago, and there is little information about it online. There was a BS1000 compatibility subsystem for BS2000, called SIM-BS1000 [0], but I'd be surprised if anyone is still using it today. And Siemens also had BS3000 mainframes – like Nixdorf mainframes, these were fully IBM compatible, and designed to be able to run IBM's operating systems – they ran the Siemens BS3000 operating system, which was a rebadging of Fujitsu MSP – Fujitsu stolen version of IBM MVS. Siemens had to enter into a settlement with IBM as a result, although I'm led to believe the terms were relatively lenient on Siemens, who did their best to portray themselves as innocent victims of Fujitsu's dishonesty. But that was the end of BS3000. I think the remnants of the Siemens BS3000 line ended up with Comparex too. Comparex finally shut down their IBM-compatible mainframe business in 2000; they survived as an IT services business until 2019, when they were acquired by SoftwareOne. And then in 1999 Siemens transferred their mainframe business to the Fujitsu-Siemens joint venture, and in 2009 Fujitsu bought out Siemens, and hence Fujitsu ended up with Siemens mainframe business. And so today Fujitsu has three totally incompatible mainframe lines – their own Fujitsu MSP mainframes (previously sold internationally but now only surviving in Japan), the ex-Siemens BS2000 (primarily surviving in Germany, although a little bit in the UK and a few other European countries), and the VME mainframes they got by buying ICL in 2002 (I believe the UK government is the sole remaining user, they really want to migrate off them but it is just too hard.) Both BS2000 and VME now run under x86-64, while I believe the Japanese line still has proprietary physical hardware. [0] https://link.springer.com/chapter/10.1007/978-3-642-67415-0_... reply rvba 10 hours agorootparentprevIt makes sense to have one stanfard across the world. This way good software can come from multiple countries. But... killing TRON probably helped a lot. Look at consoles - somehow nintendo can come with good software made by good, creative programmers. Maybe the soft was killed by trade barriers.. and more trivial things - such as software developer being a poorly paid dead end job in Japan? reply skissane 9 hours agorootparent> It makes sense to have one stanfard across the world. This way good software can come from multiple countries. TRON was not the only attempt to define a standardised operating system API in the 1980s. As well as TRON and POSIX, another was IEEE Std 855-1990 (Microprocessor Operating System Interface or MOSI for short). But POSIX was the only one which really succeeded. MOSI is pretty obscure, but my impression of what happened there – in the early 1980s, 8-bit platforms were widely popular, but very incompatible with each other (e.g. software written for Apple II could not run on Commodore 64 even though they both had 6502 CPUs). So the proposal for a common OS API was made, and an IEEE standards committee started standardising it. But by the time the standard was finished, those 8-bit platforms were declining, and IEEE was left with a standard focused on the needs of a declining market, and so very few ever used it. [0] (MOSI itself isn't inherently 8-bit – like POSIX it is a source-level standard rather than a binary-level standard, so could be used on 16-bit or 32-bit systems – but its feature set was a lowest common denominator of what 8-bit systems supported, so not very attractive for machines that have the memory to do much more.) In 1988, the Japanese education ministry decided to make BTRON the standard operating system for Japanese schools. From what I understand, this move frightened Microsoft (among others), who feared that it would prevent DOS/Windows from being used in Japanese schools, or else force Microsoft to add a BTRON compatibility subsystem to their operating systems. So Microsoft lobbied the US government to pressure the Japanese government, and that pressure resulted in the Japanese education ministry dropping the requirement for BTRON, which in turn largely killed BTRON off. It didn't completely die; a variant of BTRON (Cho-Kanji) continues to be developed into this century, but it is a niche product whose primary value proposition is far more comprehensive support for obscure Kanji characters than mainstream Unicode-based operating systems (maybe useful if you do research into historical Japanese texts). Another factor in killing the Japanese education ministry's requirement for BTRON, was domestic opposition from NEC – at the time, NEC PC-98 machines running DOS were the de facto standard in the Japanese education system, and BTRON threatened NEC's dominance of that market. It could well have been a combination of both external pressure from the US government and internal pressure from NEC that killed it. Related is Ada Programming Support Environment (APSE) and Common APSE Interface Set (CAIS). Part of the US DOD project which resulted in Ada, whose requirements demanded not only a standard programming language, but also a standard development environment, with APIs for integrating with compilers, editors, version control, build tools, etc. CAIS is standardised in MIL STD-1838A. So it is like POSIX/MOSI/BTRON, a cross-operating system API, albeit one focused on the needs of software development rather than general purpose computing–implementations of CAIS existed for Unix, OpenVMS and MVS, so development tools written against the CAIS API could run on all three operating systems. And the US government poured untold amounts of money into it, but I'm not sure if anyone ever used it. Probably some military projects did. And APSE/CAIS in turn inspired PCTE (Portable Common Tool Environment), which was basically the EU's answer to APSE/CAIS. And just like APSE/CAIS, it consumed large quantities of EU research funding, before eventually being forgotten without ever seeing much if any real world use. It is standardised as ISO/IEC 13719–which apparently nobody uses, but ISO keeps on renewing because withdrawing a standard consumes bureaucratic resources, and PCTE is so obscure nobody even wants to expend the effort on withdrawing it. [0] There was an implementation of MOSI for CP/M-80 and Pascal-MT+ – you can find it at https://github.com/skissane/MOSI/ – but I doubt that ever saw much use. reply cglong 18 hours agorootparentprevYou can ask Gemini to summarize a YouTube video for you! Also if you have YouTube Premium on Android, you can ask questions about the current video. Here's Gemini's summary of GP's video: https://g.co/gemini/share/8c0417024a3f reply hedora 15 hours agorootparentAlibaba just released 100 large models. One takes a 20 second video and summarizes it. Now I wonder if it supports audio. If so, I want the relevant browser plugin so I can read YouTube on my machine! reply Findecanor 11 hours agorootparentprevI tend to just listen to Asianometry as a podcast while resting or doing menial tasks at home. reply TowerTall 18 hours agorootparentprevSomeone posted this link on HN a short while ago https://www.tldw.pro/ reply makeitdouble 15 hours agorootparentprevYou do you, but I'd chime in on why it's not recommended: any simple answer to that question will just be \"there's a long history and international context that led to a complex situation\". That's the perfect TL;DW but I don't think it helps you much. 20 min is short for such a vague question, and you can watch at 2+x the speed if info density is so paramount. To note it still glosses over an incredible amount of critical things, it's just not a topic that can be shortened that much for anyone actually caring about understanding it. reply nottorp 11 hours agorootparent> 20 min is short for such a vague question, and you can watch at 2+x the speed if info density is so paramount. Interesting how a lot of defenders just assume delivering the same info in writing is not feasible. Has the skill to read/write become a competitive advantage again, like in the medieval ages when 0.1% of the population knew how to do it? reply makeitdouble 9 hours agorootparentTo me it comes down to how the creator decided to publish their piece. If there is no specific accessibility need, getting it in the original format on the chose platform would be my primary choice. In particular it's not a time sensitive subject and watch it later sounds easy enough. You seem to put reading/writing on a pedestal, but as you point out we're not in the medieval ages anymore, nobody should feel superior because they read it instead of watching it. reply nottorp 8 hours agorootparent> You seem to put reading/writing on a pedestal No, I put my personal time on a pedestal. Videos are slow, and I'd rather spend my life in other ways. reply Xelbair 8 hours agorootparentprevNo, i just value my time and ability to search information. plus i'm unable to consume media in the 'background'. reply drekipus 18 hours agorootparentprevWatch later reply ranger_danger 17 hours agorootparentprev [–] there are many such sites if you just google for them reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In the 16-bit era, Japan had three main computer platforms: NEC’s PC-98, Fujitsu’s FM Towns, and Sharp’s X68000, with the PC-98 being the most popular.",
      "The transition to Windows led to the decline of these platforms as they either became less distinctive or incompatible with the new OS, while the gaming market's shift to 3D consoles further diminished their relevance.",
      "Despite the decline of their unique platforms, Japanese computer makers transitioned to producing standard Windows PCs, surviving better than Western counterparts like Commodore and Atari."
    ],
    "commentSummary": [
      "The 1997 Asian economic crisis significantly impacted Japanese PC platforms, including the SuperH processor by Hitachi, leading to a partnership with Mitsubishi to form Renesas.",
      "Renesas faced difficulties in continuing the development of SuperH and eventually shifted focus to new designs.",
      "The Japanese PC market declined due to software compatibility issues, economic pressures, and increased competition from global standards and Western companies."
    ],
    "points": 249,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1726956361
  },
  {
    "id": 41615102,
    "title": "Nextcloud: Open-Source Cloud Apps",
    "originLink": "https://nextcloud.com/",
    "originBody": "Skip to main content Nextcloud Hub 9 is here! Learn more Nextcloud Enterprise Day 2024 Register now! Products Nextcloud Hub Nextcloud FilesFile Sync and Share Nextcloud TalkCalls, chat and video conferencing Nextcloud GroupwareCalendar, Contacts & Mail Nextcloud OfficeReal time document collaboration Nextcloud AssistantPrivate and local AI assistant RoundcubeThe most popular on-prem webmail Microsoft integrations Solutions Nextcloud EnterpriseFor mission-critical use Public sector Enterprises Service providers Education Pricing Nextcloud at homeFor families, students & you Resources Blog Events Webinars Success stories Whitepapers Data sheets Nextcloud Podcast Documentation App Store FAQ Support Portal Community How to contribute Code on GitHub Report a bug Community support Developer program Code of Conduct Nextcloud Include Migration About About us Comparison with others Press Security Team Jobs Partners Search our site Instant trial Download Desktop & mobile appsWindows , macOS, Linux, Android, iOS Nextcloud serverFor self-hosting on your server Sign up nowGet free account at a provider English Français Deutsch Español Italiano Regain control over your data Remote collaboration made easy On-premises or cloud Customizable and scalable No data leaks to third parties Try Nextcloud now Download Trusted by thousands of organizations The most popular open source content collaboration platform for tens of millions of users at thousands of organizations across the globe See all case studies Nextcloud Hub integrates the four key Nextcloud products Files, Talk, Groupware and Office into a single platform, optimizing the flow of collaboration. Nextcloud Hub Easy access anywhere Powerful access control Infrastructure integration Designed for humans Group chat Web conferencing Screensharing Unique protection Avoid data leaks Team planning Email made easy Easy access, anywhere Use your existing apps Self-hosted online office View and reply to comments Chat or have a call while editing docx/pptx/xlsx support Context Write to create content based on existing documents and data Context Chat to answer questions about your data AI image generation and free prompt text generation Accessible from the top toolbar and integrated in apps Nextcloud Files Nextcloud Files offers a self-hosted file storage and sync platform with powerful collaboration capabilities with desktop, mobile and web interfaces Learn more Nextcloud Talk Nextcloud Talk delivers on-premises, private audio/video conferencing and text chat through browser and mobile interfaces with integrated screen sharing and SIP integration. Learn more Nextcloud Groupware Nextcloud Groupware integrates Calendar, Contacts, Mail and other productivity features to help teams get their work done faster, easier and on your terms. Learn more Nextcloud Office Nextcloud Office is a powerful LibreOffice-based online office suite with collaborative editing, which supports all major document, spreadsheet and presentation file formats and works in all modern browsers. Learn more Nextcloud Assistant The first local AI assistant that is built into the Nextcloud Hub collaboration platform. Integrated across apps, it can generate content, answer questions about your data, summarize emails, translate, and much more. Learn more Play Hub video now Solutions Nextcloud use cases Nextcloud is committed to provide self hosted open source cloud file storage technology that is a perfect fit for the privacy and security of your enterprise Public sector Governments are moving away from foreign cloud providers amid rising concerns about digital sovereignty and are opting for secure self hosted cloud file storage platforms. See more Enterprises Amid a push to improve efficiency and facilitate remote work through online collaboration technology, enterprises counter a growing compliance, cost and data leak risk by strategic deployments of Nextcloud. See more Service providers Leverage your infrastructure and keep your customers yours with easily scalable, fully branded cloud storage software and collaboration tools. See more Education Education is a heavily regulated industry where protection of academic data and personal information of students with compliant file sync collaboration tools is a top priority. See more Get Nextcloud at home Get in touch Reduce risk, improve remote team communication and minimize operational expenses with the leading open source cloud file storage and content collaboration platform. Contact us now to learn how Nextcloud can help you! Get a quote Talk to us Newsletter Why Nextcloud? With over 400.000 deployments, Nextcloud is the most popular on-premises content collaboration platform you can download. It scales from a Raspberry Pi with 2 users to globally distributed installations with tens of millions of users at major hosting providers. What makes it so popular? Productivity Enable productivity across any platform, whether in the office or on the road, to share, collaborate and communicate across organizational boundaries. Nextcloud provides transparent access to data on any storage. Control Protect, control and monitor data and communication across your company. Guarantee compliance with business and legal requirements. Keep your data on servers you own, at all times. Nothing leaks, not even metadata. Community Enjoy constant improvements from a thriving and transparent, entirely open-source community development model, free of lockins or paywalls. Enjoy the benefits of enterprise support when you need it. Try Nextcloud now See it in action Nextcloud is designed to offer best in class productivity, and is developed at an impressive pace with new functionality becoming available every few months. We selected some videos to give you an idea of what we’re up to. Learn more Nextcloud Hub 9 Watch the announcement of Nextcloud Hub 9 Home office Why Nextcloud is the perfect home office platform Nextcloud events Nextcloud welcomes you to in-person events, conferences and trade shows as well as webinars and online meetings. See all events Compliance Kit See more Auditing capabilities See more File Access Control See more HIPAA, GDPR and more Compliant by design Nextcloud products are designed with compliance in mind, providing extensive data policy enforcement, encryption, user management and auditing capabilities. Contact us Get in touch Reduce risk, improve citizen communication and reduce operational expenses with the leading content collaboration platform. Contact us now to learn how Nextcloud can help you! Get a quote Talk to us Newsletter See how we stack up Compare Nextcloud with other solutions and see why it is the market leader in open source cloud file storage and collaboration platforms. Compare Nextcloud offers a modern, on-premises content collaboration platform with real-time document editing, video chat & groupware on mobile, desktop and web. Search Search: About Nextcloud About us Jobs Code of conduct Privacy Press Legal note Trademarks Brand guidelines Features Human Rights Policy Code of Ethics Resources Download App Store Admin manual User manual Security Developer information Code on GitHub Compare with others Search Interact Support Forums Demo Contact us Bug Tracker Cookies preferences © 2021 - 2024 Nextcloud GmbH We save some cookies to count visitors and make the site easier to use. This doesn't leave our server and isn't to track you personally! See our Privacy Policy for more information. Customize Accept all cookies Reject all Customize",
    "commentLink": "https://news.ycombinator.com/item?id=41615102",
    "commentBody": "Nextcloud: Open-Source Cloud Apps (nextcloud.com)236 points by tomrod 12 hours agohidepastfavorite198 comments BrandoElFollito 7 hours agoNextcloud is a nightmare. First if all, it wants to do everything and does none well (or better than specialized apps) Its internals are a shitshow - a question about getting the real IP of the calling client raised a 10 pages discussion where people said \"it works\" and other days \"it does not\". The \"solution\" required you to change a volatile setting within the docker container (and do it again when the container changes) Finally the upgrade is insane. I once tried to upgrade my installation, was told that n+2 upgrades are not possible (fair enough) and ... the installer destroyed the database. What a piece of crap. Thanks god for backups. reply jraph 1 minute agoparentSeeing this top comment is sad. I handle several nextcloud instances, one of them updated from owncloud, I'm very happy, it does a lot of stuff well and when you need specialized software you can because it's just files. Updates have been painless. reply TheChaplain 6 hours agoparentprevI've run Nextcloud for almost 3 years now via docker on a vps , it hosts my contacts, calendars and files. There is a apache reverse proxy in front of it, and a postgresql-database in the back. Not once had I any serious issues updating when pulling new updated images. Occasionally it whines about missing indices, but that is easily fixed using the occ command line tool. The clients real IP is forwarded by the proxy. What I want to say is just that Nextcloud works fine. reply doubled112 5 hours agorootparentI have also found that if you stick to the core functionality it works fine. As soon as I started to add apps from the store it starts to be a pain. An upgrade comes out and you’re stuck on that version until they all update, OR you update not realizing and lose the functionality for some time. Since I self-host a bunch of apps, it made more sense to use different apps dedicated to those features, like Miniflux or Navidrome. Not for everybody though. reply jerf 22 minutes agorootparentI ran NextCloud for some years, just for the file sharing. I was the only user. About every third upgrade cocked up in some critical way that required bespoke DB fixes. Twice I basically blew away all state and recreated my entire file store from scratch from a backup because it was easier than fixing the install. Worked for me but is infeasible for any larger install. I tried using the office functionality for one document and I lost the document entirely on an upgrade because apparently the mirroring as a file is an illusion and the real office data is stored buried in the database in a way I never did manage to recover; I ended up just recreating the document. Thank goodness it was just the one document. It is constantly screaming for upgrades but I don't know what they are doing with those upgrades because it doesn't seem to make the core any more reliable. It's just an opportunity for the upgrade to fail and lose data, which it frequently took advantage of. The key thing that really annoyed me is that I couldn't hardly have used it more lightly than I did, and it took about every other opportunity to fall over and lose data. I can't even imagine the plight of someone trying to run this in an office environment. reply kQq9oHeAz6wLLS 5 hours agorootparentprevYou haven't necessarily gained anything by going to managing standalone apps individually, though. You've removed the unifying layer, but you're now having to keep up with the app versions and upgrade them individually by hand instead of letting the unifying layer handle it. reply doubled112 4 hours agorootparentI gained working music and RSS that week. reply BrandoElFollito 4 hours agorootparentprevIf you use docker + watchtower all this is done automatically reply doubled112 4 hours agorootparentI don’t use Watchtower, but a script that takes a btrfs snapshot before doing a docker compose pull, then docker compose up. Same idea, just safer and fewer surprises. reply BrandoElFollito 5 hours agorootparentprev> Not once had I any serious issues updating when pulling new updated images I've run it for about a year until the upgrade tipped me over. I am sure that correct upgrades (n to n+1) are fine - I tried n to n+2 and instead of explaining to me that this is not possible, Nextcloud explained to me that this is not possible and fucked up big my install. I had to recover from a backup. reply thowawatp302 1 hour agorootparentWhy did you do that when the instructions say not to? reply donmcronald 1 hour agorootparentI don't understand why this attitude is so prevalent in the tech industry. The person writing the upgrade script knows if upgrades can't support n+2 jumps and it takes about 30s to build in a check: if(target_version > current_version + 1) // not allowed It's more work to add it to the docs than it is to put a fail-fast check in the upgrade script, so why put it in the docs where 250k+ admins need to be aware of the limitation and avoid it? It might be creating 100k hours of waste on the sysadmin side to save 30m of work on the dev side. I just don't get it. reply VertanaNinjai 2 hours agorootparentprevSo they told you that’s not supported and don’t do it. You then did it and seem surprised at a bad outcome. And you’re blaming the software and/or vendor? reply BrandoElFollito 2 hours agorootparentSeriously do you think that for software that manages data the proper way is for someone who starts the upgrade process to say \"this is not supported, and now we fucked up your database and you cannot recover from that\"? They knew that I was upgrading N+2, this is not a surprise - and I did not realize that upgrading N+2 is not supported. The proper way would be to abort the upgrade upon discovering that I am going for an unsupported way. This is not serious software. reply stavros 1 hour agorootparentI don't understand how commenters here can seriously argue this. There should be no UX for \"oh this is not supported? Eh give it a shot anyway, I don't care about literally all my files\". If you don't support upgrading, don't expose that in the UI. There's no excuse for having UI that will let the user do something unsupported and then screw up their data. reply eptcyka 2 hours agorootparentprevWhat do you do to make performance not suck? Have been using it for years, but it just sucks in terms of showing me my pictures. Also, what do iOS people do to make their auto upload folders upload automatically without having to open the app? reply aquaticsunset 2 hours agorootparentprevThis has been my experience as well. The only major instability was due to the Ubuntu snap based runtime, which I migrated away from a few years ago. reply lolinder 6 hours agoparentprev> it wants to do everything and does none well I can't speak to the other points you made because this has always stopped me from investing in it. What I need in my stack is a focused Google Docs alternative, but every time I've installed it (3-4 times at this point) I'm quickly overwhelmed by the quantity of stuff it includes and by how complicated actually setting up the Docs replacement was (at the time the recommendation was to install Collabora and link it up with Nextcloud, which I never could get working). What I did see out of the box was a slow and bloated web portal, a bad calendar, a bad video conferencing app, a file backup solution, and a terrifying app store filled with add-ons that may or may not be maintained. Oh, and now it looks like they have an AI assistant? I'm honestly relieved to see this here and know it's not just me. I ended up going with Seafile for file backups and have been very happy with it, though I'd still love to find a focused collaboration tool I can run. reply BrandoElFollito 5 hours agorootparent> What I need in my stack is a focused Google Docs alternative Try Etherpad (https://etherpad.org/) > I ended up going with Seafile for file backups and have been very happy with it I went through probably dozens of solutions and ended up with Syncthing for synchronization of data (it requires understanding how it works but once you do (usually an eureka kind of moment) this is a very powerful system. For me using synchronization software (Seafile, Syncthing, ...) to do backups is dangerous. Borg is a very good solution (or Restik, or Kopia) reply touggourt 5 hours agorootparent>> What I need in my stack is a focused Google Docs alternative > Try Etherpad (https://etherpad.org/) He should better use OnlyOffice, Collabora or Cryptpad office suite. The Cryptpad server is the only one which is not a nightmare to install and setup. reply BrandoElFollito 4 hours agorootparentEtherpad took me a few minutes to install on docker. Not sure why you do not like it reply dspillett 3 hours agorootparentprev> For me using synchronization software (Seafile, Syncthing, ...) to do backups is dangerous. Yep. Just synchronisation on its own for backup is at best better than nothing, a first step. It fails to protect from at least three very common occurrences you need backups for: accidental deletion, incorrect update, or corruption - the broken data is quickly synced everywhere. Adding snapshots and regular integrity checks is essential. reply lolinder 3 hours agorootparentprevI guess to be clear, I do both—the central Seafile server is backed up by Borg. Seafile has file version history, so it's more than pure syncing and has so far been sufficient for backups, but I also run it on site so it's not safe in that sense and the data matters enough to be worth redundancy. reply BrandoElFollito 2 hours agorootparentAh ok, same as me. I gather data from other places with Seafile to a ToBackup older (with a subfolder per system) and this gets backed up together with the main server key data. reply klabb3 6 hours agorootparentprev> it wants to do everything and does none well On that topic, it’s amusing to see when the “are we a product or a platform?” confusion jumps from its native host (publicly listed tech corps) to FOSS projects – different species altogether. The idea of owning your own server just like any other device like laptops, smartphones and tablets, is just wonderful. But that means Linux! And Linux is absolutely infamous for poor application distribution (well technically the Linux distros). So now people make their own half-assed “app stores”, which is infinitely worse. We would need a platform spec so that application developers can publish, users can discover and deploy, and platform providers can implement against a shared spec. That would be such a major step away from the surveillance economy and towards data sovereignty. reply bzzzt 4 hours agorootparent> We would need a platform spec so that application developers can publish, users can discover and deploy, and platform providers can implement against a shared spec. That's been tried with Linux Standard base, but it seems 'standardise on glibc 2.31 and Qt 3' is not a viable way to describe system dependencies. Such a spec also doesn't exist for Windows or macOS systems. If you target 'living' operating systems you need living applications that are updated to keep working. We're well past the point you can assume some 20 year old binary will work unless you're running it in an emulator. reply bigstrat2003 39 minutes agorootparentMicrosoft is so committed to backwards compatibility for Windows that you are very likely to be able to run 20 year old binaries without any heroics. They don't need any kind of platform spec, because they put in serious work to maintaining that kind of compatibility. reply wvh 4 hours agorootparentprevMaybe I misunderstand what you're suggesting, but aren't containers pretty much that application platform? Or stuff like Helm, when you're talking about larger, more complex deployments? I imagine Nextcloud needs at least a database, web server and the ability to send emails, and possibly some form of backup and high availability if you really want to rely on it. There are a lot of non-trivial ways to skin that cat. reply klabb3 2 hours agorootparent> Maybe I misunderstand what you're suggesting, but aren't containers pretty much that application platform? That’s right. And for UI, web is the obvious choice. But it’s not an application platform alone. You need at the very least file systems and networking/routing over https and maybe lower level protocol support too, that apps can use/discover without having to have a phd in Linux configuration. But to be truly useful, you need a lot more stuff wrt identity & auth, server-to-server networking, service discovery, cross-app communication, etc. Something more like Dokku probably. > Or stuff like Helm, when you're talking about larger, more complex deployments? Well fortunately you don’t need to open the Pandora’s box of consensus problems for a personal node (again like any other device). Also too much Yaml to make it sane probably? reply righthand 3 hours agorootparentprevHonestly I realized I don’t need a web editor to change the documents, I just needed the documents on whatever device I was using. I set up syncthing to sync my Documents folder with a whitelist for specific directories and a $5 vps lets me get access to whatever documents I need. No need for a domain name and web ui when it’s all desktop apps all the way down. For collaborative editing LibreOffice Writer has a collaboration feature. Otherwise I’ll send a copy via message service of some kind (email, txt, chat, whatever) or download a copy to my Documents folder if someone sends me a google docs link or whatever. reply lolinder 2 hours agorootparentDoes LibreOffice now allow collaborating on one document at the same time? I can use local synced files for most things, but when I need to work with my wife on a document together I always have to fall back on Google. reply A4ET8a8uTh0 5 hours agorootparentprev30 devices, including 4K media to multiple endpoints and 100Ks files. reply anonymousiam 2 hours agoparentprevI agree that it has its problems. Last week, I noticed that one of my contacts had an invalid date for their birthday. Try as I may, I cannot get Nextcloud to recognize 1/5/2000 as a valid date. (And no, it's not a localization problem.) reply chappi42 4 hours agoparentprevWith AIO upgrade is a piece of cake. -- For us, Nextcloud is a godsend, some things could be improved, true, but they are improving; we are more than grateful for this comprehensive software. Much less hassle (and even less expensive) than Microsoft before. reply davidee 4 hours agoparentprevNightmare seems like a bit of hyperbole. Look, it's not without its quirks, but it's pretty reliable. Speed and responsiveness can be further upped by using an in-memory cache like Redis/Valkey. Nextcloud can try to be everything it wants, but as users we can ignore that and stick to file-sharing/online/cloud storage. If one is using it primarily for file storage/sharing, it works reasonably well. We have ~1TB of Nextcloud data on an NFS share (itself snapshotted ZFS), backed up to B2 regularly. If something happens to a Nextcloud server (and it has), restoring the data is as simple as recreating the users and copying over the data directory for each user (provided you're not using it for calendaring and such). A quick OCC command to re-index user data and we're back. We haven't had any issues running the docker image (orchestrated via HCP Nomad). We have also helped a friend run Nextcloud for their small non-profit, as a snap on an EC2 instance in AWS. It keeps itself up-to-date and has been pretty-much problem free for years. EBS Snapshots cover backups there. reply baby_souffle 4 hours agoparentprevYep. Running it on k8s has similar issues. I only used it as sync for my obsidian notes with occasional (maybe once a month) access to those notes via web ui. I wasn’t happy with how it would reformat my notes on save. Sync thing supports encrypted storage in VPS and is trivial to run in k8s and it’s been much better for my needs. reply thepill 6 hours agoparentprevRunning the snap-version since 5+ years without any problem reply n3storm 6 hours agoparentprevBetween a couple of initial versions after fork there where a couple of bumpy upgrades, but last 4 years have been smooth reply Namidairo 5 hours agoparentprevThe insane thing when I tried to update Nextcloud, was that it kept timing out the download because it was too slow, and then required me to delete the upgrade in progress file in order to try again... reply clort 3 hours agorootparentI've seen this, I think its the web server timing out.. I use the php script updater/updater.phar directly and that works just fine reply komali2 6 hours agoparentprevThis might be true but I finally deployed nextcloud for our co-op because I couldn't find any other solution to easily have a shared calendar with granular permissions to accounts I assign for write privileges, but also a dynamic ICS file download endpoint I can publicly expose. I tried a couple other caldav servers and none had this. So it does at least one thing better than specialized apps, which I typically prefer to use. reply sneak 6 hours agoparentprevNextcloud ate my data on a single user installation with no fancy settings turned on, with no upgrades attempted. The sync client on desktop said it was fully synced. Only 200GB of 800GB was on the machine. It’s some of the worst software I have ever used. It’s the pentagon house with the inverted roof built with the two-clawed hammer that eevee described. reply TonyTrapp 6 hours agorootparentI had a similar experience with ownCloud. Regularly, file uploads were timing out, leaving the database in an inconsistent state (apparently), which then later caused the files to be deleted on my desktop where the files were coming from. I'm still not sure why I put up so long with it. It was a slow mess, especially on the single-board computer I ran it on, and SyncThing was a breeze compared to it, with no data loss at all (and much faster on the same SBC). reply BrandoElFollito 5 hours agorootparentI ultimately moved to Syncthing too. It requires understanding how it works, but once you do it is wonderful. reply touggourt 5 hours agorootparentprev> The sync client on desktop said it was fully synced. Only 200GB of 800GB was on the machine. Did you read the manual? It is recommended for such size to do the first sync using rsync (or similar) followed by the CLI command to build the files database. reply mort96 4 hours agorootparentSilent data loss like that is not acceptable, even if there would have been a way to sync which would have worked around the data loss bug. reply BrandoElFollito 2 hours agorootparentprevA service that manages data cannot fail on such things. Nextcloud could refuse this first sync and point to solutions but losing data just \"because the manual says rsync\" is not acceptable reply boramalper 9 hours agoprevI started using Nextcloud first to have an alternative to big-tech in case Google locks me out of my account, and then it became my daily driver. It's fast, private, and has mature clients for all major desktop and mobile platforms. Together with OnlyOffice, it's a good-enough substitute for Google Docs Editors (the office suite) for non-collaborative editing. I love self-hosting but file storage is one thing that I don't want to risk. I've been paying for Hetzner Storage Share [0] happily to save myself the headache. If you want a different hosting provider, Nextcloud now has Simple Signup program[1] which helps new users to sign up for a free plan with a provider near them, offering ≥ 2GB of storage. You can also browse the entire list if you want to pick one manually.[2] [0] https://www.hetzner.com/storage/storage-share/ [1] https://nextcloud.com/sign-up/ [2] https://nextcloud.com/providers/#:~:text=Providers%20for%20h... reply raybb 9 hours agoparentIf you start on a free plan but then ultimately switch to another provider do you have any idea of how hard it would be to export and import all your tasks, files, etc? reply MarcusE1W 8 hours agorootparentI have moved twice now. First from my raspberry pi to cloud and the second time between cloud providers. There might be other ways but you can share folders between Nextcloud instances. I have shared my whole nextcloud from the old one to a folder in the new one. Then in the new instance you copy folders from the shared drive to your new instance. For ~300-400 GB in takes a moment and I do some spot checks, but after half a day it's done. And you don't actually do things, you just wait that a folder copy finishes, check and then start the new one. There might be more automated ways, but this worked for me. reply boramalper 8 hours agorootparentprevFiles are files so you can download them to your computer and upload them to the new provider. Unfortunately I am not aware of any direct provider-to-provider sync. Application data depends on the app. For example, Notes [0] save your notes as Markdown files so you can move them (along with your files) wherever you want. However, News [1] don't and don't have export/import features at the moment either [2]. Nextcloud as a file storage solution and a non-collaborative office suite is great, but I cannot recommend its apps the same way. They are very convenient to install, but the quality varies a lot in my opinion so evaluate before you adopt. [0] https://apps.nextcloud.com/apps/notes [1] https://apps.nextcloud.com/apps/news [2] https://github.com/nextcloud/news/issues/2503#issuecomment-2... reply MarcusE1W 8 hours agorootparentYou can share folders between nextcloud instances and copy from one to another. No download needed. reply boramalper 6 hours agorootparentTIL, thank you! reply qwertox 8 hours agoparentprev> I've been paying for Hetzner Storage Share [0] happily to save myself the headache. Assuming you start with NX11, which has 1TB storage, and before hitting the limit you want to upgrade to the 5TB storage, NX21. Can you just call Hetzner and tell them they should upgrade your NX11 plan to NX21 in-place, or will you have to order NX21 and then move all the data over to the new instance yourself? reply qwertox 5 hours agorootparentTo answer my own question: Scalability Stay flexible with your Storage Share. Regardless of how your requirements change over time, you can upgrade or downgrade your Storage Share in a few quick steps and without worrying about data loss. Simply switch between the Storage Share package size you need by going to your account on the konsoleH, and then to \"Account type\". It's nice to see that they also offer the ability to downgrade. reply k8sToGo 8 hours agorootparentprevPretty sure you can do an in-place upgrade on the web interface of Hetzner reply saligne 7 hours agorootparentprevIn place, you don't need to move data yourself to a new instance. You just choose the bigger plan in their web portal. reply smarx007 4 hours agoparentprevHow did you solve the fact that Hetzner's instances do not have Elasticsearch set up for full-text search? reply sneak 6 hours agoparentprevThe desktop clients are anything but “mature”, and the whole point of GDocs is collaborative editing. If you don’t want to collaborate, syncthing works fine and doesn’t need a server. reply boramalper 6 hours agorootparent> the whole point of GDocs is collaborative editing Strong disagree. Collaborative editing is one of the major points but not the only one. For me and I believe many others, being able to view and edit my documents in a web browser is a huge convenience. (Speaking of collaborative editing, OnlyOffice too supports it. [0] However, you might need to setup a standalone “document server” [1] if you’ve a lot of collaborators.) [0] https://helpcenter.onlyoffice.com/onlyoffice-editors/onlyoff... [1] https://docs.hetzner.com/konsoleh/storage-share/faq/addition... reply homebrewer 6 hours agorootparentI supported a NextCloud + OnlyOffice server for 4 years for a 100-person company, and have since moved to Collabora Code (which has been running for ~2 years now). IME Code has better performance, is easier to upgrade, provides better compatibility with MS Office (since it's basically LibreOffice with a web UI), and is easier to integrate with (I wrote some integrations for a couple of internal systems and it's been a breeze). It's fully FOSS. https://www.collaboraonline.com/code reply boramalper 3 hours agorootparentGood to know. I’ve been sticking with OnlyOffice only because it’s supported out of the box on Hetzner [0] (as in, I don’t need to setup and maintain any “document server”). Surprisingly, this is what they say about Collabora: > Due to performance reasons, we cannot support the built-in version of Collabora. So if you still want to use Collabora, you will need to provide your own server. You could use, for example, one of our unmanaged dedicated root servers or a Hetzner Cloud server. You can activate Collabora via the App Store, but you will need to use the other server for data processing. You as the customer are responsible for configuring this server yourself in the app's settings. [0] https://docs.hetzner.com/konsoleh/storage-share/faq/addition... reply obnauticus 9 hours agoprevI originally wanted to do this but the CVE history is a bit too colorful for something I’d want to trust as a “cloud replacement”: https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=nextcloud A common misconception IMO is that running and owning your own infrastructure is somehow more secure. To that I lol, and I’m confident that the thousands of AWS/GCP/Azure/iCloud security engineers are all doing a more thorough job than you can. At the very very least they receive embargoed bugs which they often mitigate before the general public. reply bravetraveler 8 hours agoparentOne doesn't have to expose it to malicious actors. It is most-useful that way, sure. Mine is at 10.27.0.68. Have fun, hackers! Also, I lol at most CVEs. Butterfly farted outside, oh uh. Take the top one: In Nextcloud Desktop Client 3.13.1 through 3.13.3 on Linux, synchronized files (between the server and client) may become world writable or world readable. This is fixed in 3.13.4. You mean to tell me a few minor point releases imitated umask, making world-readable [and possibly added writable]? Oh no! The tragedy! Keep in mind most clients are single user systems anyway. Judge them on their facts, there are vulns and then there are vulns. CVEs are a sign of attention on a project. No more or less. reply uselpa 7 hours agorootparentI find that one concerning in an enterprise setup (which they target). Or the fact that the desktop client has 999 open issues. Or that the last version silently takes you off the stable channel. I could go on … Nextcloud desktop has severe quality control issues. reply bravetraveler 7 hours agorootparentAn enterprise setup where people share machines, sure. There are plenty of reasons to be afraid [and mitigations], no need to find them. Either take control or sell/outsource it, no skin off my teeth. I was replying to someone making the case for 'just trust Google/whoever, lol' My point is this nears hysterical fearmongering. I'd prefer if you don't go on, but it's more for your benefit. Stopping before I start my own rant about risk tolerance reply littlestymaar 6 hours agorootparentprevAn number of github issues is even a worse metric than CVEs, many people just post wishlist issues there. reply laymansterms 6 hours agorootparentprevYeah, one CVE is literally \"You can use the MacOS variant of LD_PRELOAD on the client to hook libc calls! Oh no!!\" This is a bogus CVE; any application can perform arbitrary actions when its system calls are hooked, but it requires such a strong threat model that the adversary realistically gains no ground by doing so. (\"A code injection in Nextcloud Desktop Client for macOS allowed to load arbitrary code when starting the client with DYLD_INSERT_LIBRARIES set in the enviroment\") reply saagarjha 6 hours agorootparentYou will note that the PR strengthens that model regardless. reply MrDresden 9 hours agoparentprevYour right of course. No way an individual can compete with an army of specialists. But for some of us it is a bit of a hobby to run our own infrastructure. And some of it only ever runs on a private network. I rolled my own docker setup for Nextcloud a few years ago, and couldn't be happier with the outcome. It does require me to log in and update the system and setup from time to time, but that's just a good time to drink a hot bevarage and listen to podcasts in my mind. For anyone hosting their own instance, Nextcloud offers this scan[0] of your public facing url which might come up with something worth fixing. [0] https://scan.nextcloud.com/ reply zigzag312 9 hours agoparentprev> I’m confident that the thousands of AWS/GCP/Azure/iCloud security engineers are all doing a more thorough job than you can I'm not so confident about that: https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=azure It really depends on what you self-host. https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=syncthing \"Do everything\" solutions go against the principle of minimizing the attack surface. EDIT: More is not always better in security. With more people doing more things, the statistical odds of miscommunication and misconfiguration increases. reply izacus 8 hours agoparentprevThose cloud specialists aren't configuring your ACLs and firewalls - the most common source of severe security problems in the cloud. reply hypeatei 7 hours agorootparentYes, there is a concept of \"shared responsibility\" in the cloud. Obviously the provider is going to handle some things and you have to take care of others. reply MaxBarraclough 8 hours agoparentprevI'm not a security specialist, but it seems to me that while managed services typically have better security and sysadmin resourcing, they also have the downside that their security can fail at a massive scale. If someone defeated the security of, say, GitHub, they could leak all the private repos stored there. Managed services also have to accept connections from the public Internet, which on-premises solutions do not. reply akdev1l 5 hours agorootparent> If someone defeated the security of, say, GitHub, they could leak all the private repos stored there. In theory you are correct but this is like saying keeping your money under your mattress is safer than a bank. Yes, in theory someone can still all the money at a bank but the bank is infinitely more qualified/competent to not get robbed than you would be. reply waveBidder 4 hours agorootparentbanks aren't safe because they're unrobbable, they get robbed all the time. They're safe because they're the ones taking on the risk. Data isn't fungible like cash though. reply notepad0x90 8 hours agoparentprevI would dig a bit more into the breakdown of the CVEs: https://www.cvedetails.com/product/34622/Nextcloud-Nextcloud... As well as if this reflects a systemic issue with the codebase or if it is just getting much needed attention from security researchers. More CVEs can just mean they're cleaning up after vulns really well. But at the same time, if they have critical vulns over and over again, that might indicate bad coding practices or carelessness. reply n_plus_1_acc 7 hours agorootparentNextcloud is well known for it shitty legacy PHP codebase. reply obnauticus 7 hours agorootparentprevAgreed. The breakdown is indeed pretty poor IIRC. Generally you use these disclosures to make directional decisions about infrastructure. The list of fixed and disclosed CVEs combined with the legacy PHP code base doesn’t really pass the security sniff test. You really wouldn’t know for sure without doing a full code audit. reply bpfrh 7 hours agoparentprevRunning and owning your own infrastructure exposed to the outside world can be more insecure, running your own infrastructure at home in segmented networks with wireguard will solve most problems. reply ReptileMan 8 hours agoparentprev>A common misconception IMO is that running and owning your own infrastructure is somehow more secure. If done properly cve-s don't matter that much. You create a headscale install on a pi and the headscale port and your router's ssh (key only) are the only things visible from the outside. Take any other than a home router - aka something with support. And you are done. reply hypeatei 7 hours agorootparent> If done properly cve-s don't matter that much. I think it depends on the CVEs and where they are. If it's a software vuln that requires root or some other complex prerequisites then w/e. But, if we're talking about low level problems in either the OS or network layer (e.g. firewalls, routers) then big clouds are most likely going to have that patched and rolled out more quickly IMO. reply Jnr 7 hours agorootparentprevOr go with Wireguard. It uses UDP, it has a silent protocol, no one from the outside can see it. (unless they can MITM you) reply ReptileMan 3 hours agorootparentheadscale boils down to syntaxis sugar over wireguard. To the headscale and tailscale teams members reading this - please don't kill me. You are making awesome things. reply Timber-6539 9 hours agoparentprev> thousands of AWS/GCP/Azure/iCloud security engineers are all doing a more thorough job than you All these cloud services are just attack surfaces with a huge target on their backs. And the security engineers slip up too [0], in the case of Microsoft it's become more of a meme now. The North Korean hackers basically own them. [0] https://www.techspot.com/news/102573-microsoft-left-server-c... reply obnauticus 6 hours agorootparentSomewhat depends on your threat model. The relative value of an iCloud/aws/gDrive 0day is going to be higher than Nextcloud. If you’re in the category of people concerned about this type of breach, self-hosting a PHP web app and claiming it’s somehow safer wont save you either. For this risky population, neither solution works since attackers are willing to throw expensive exploits at your data in either scenario. If you aren’t being specifically targeted, then you would care about low hanging fruits discovered by something like automated scanning. Not exposing your service to the internet does solve this assuming you’re confident in the stack which provides this isolation. But managing this stack and performing risk calculus here is actually where the security horse trading happens. I think most people aren’t safer managing this themselves — arguably they’re actually worse off. I have high standards for the confidentiality of my data. I care about things like lateral movement and the massive attack surface that isolation tech to prevent such movement has. I also won’t design monitoring and alerting, ensure a patch state, or perform code audits on Nextcloud and all the isolation tech required to secure it to a comporable level of security. Because of this, I instead reason around the cost of exploitation. I want it to be higher than what I believe Nextcloud provides and I’d rather require an attacker to use an expensive 0day to extract my data off a cloud provider like Google versus a potentially cheap one against my own infra. reply exe34 9 hours agoparentprevI think the threat model isn't that these popular services are going to be attacked, but that they will engage in the denial of service themselves without legal recourse. reply CodeCompost 7 hours agoparentprevIt's written in PHP, the most insecure web language on the planet. reply touggourt 4 hours agorootparentThis is a very very old opinion, and not true for years. reply klaussilveira 7 hours agorootparentprevWhat, in your opinion, makes PHP less secure than Python or JavaScript? reply throwawaymaths 4 hours agorootparentNot gp but I'd say because it's littered with footguns like this: https://dev.to/klnjmm/be-careful-about-the-switch-statement-... reply aborsy 12 minutes agoprevSnaps are not popular in hacker news, but if you want basic functionality, try nextcloud snap. It is good, and takes care of itself. I like nextcloud. That said, I think it would have been better if they could focus on core features, remove code complexity, make it faster, remove bugs, make sure it just works and upgrades are reliable. It could not possibly do such diverse range of applications, and now AI. reply vid 6 hours agoprevI've been looking into NextCloud to recommend to a government agency. The world desperately need competition or at least something compatible with the m365 stack, because it's eating the world and taking a lot of choice away and killing a lot of innovation outside the Microsoft funnel, since Microsoft is not interested in a lot of tech (for example, network schemas, useful for \"tell us once\" type applications, since they'd rather you just use their tech for everything, and the messier it is behind the scenes, the better for them). Anyway, I have mixed feelings. I admire the community and the support it has by many governments, its staunch Open Source basis so it's useful for an individual or a large organization. But it is building on a lot of crufty PHP, their collection of apps is very uneven and it's hard to know what works well without a lot of research, and it's going in a few directions to upgrade. AppApi in particular is on one hand very innovative, on the other going in some odd directions. I know it is successfully used by very large organizations, but without spending a lot of time with it, it's hard to get a sense of the commitment and considerations required. reply rc_mob 4 hours agoparentHow can I donate tonthis project? I do not see any limks to donate. reply vid 3 hours agorootparentI think more than anything they need advocacy and good quality product contributions (support, documentation, code). From what I know, a lot of the development happens via a few consulting firms that support their larger clients. reply zerof1l 9 hours agoprevI've been using Nextcloud for some years now. Overall it's an ok replacement for Google Cloud. But for some time now their focus has been on developing features for business collaboration as opposed to personal & family ones. For example, their photo library is quite limited. There's Memories app for Nextcluod, but it isn't much better. I'm in the process of migrating to Immich. Nextcluod notes app on Android has been broken for some time. reply nolok 9 hours agoparentAs a self hosting afficionado, I feel like the perfect Google photo replacement doesn't exist yet. I have a synology nas and I use the synology photos app for auto uploading without loss of quality to my nas. Don't forget any picture or videos, deduplicate, figure out what you can safely delete from my phone,... All of that works great and is reliable. The synology photos app to watch your photos though, isn't very good. Neither on mobile nor the desktop nor android TV. It's bare minimum and even that has failures. Next cloud,... It gives a weird \"I don't trust it\" vibe. I want my stuff unedited unaltered, unmoved from their folder and keep your metadata elsewhere, and it just give me a wrong vibe for that. This is 100% feeling and not facts. Immich is great but high maintenance, any update is a risk that you have way more work that planned. Their upload app need works but I use the synology one. I don't feel safe having that handle the main copies of my picture, and the dev are being super clear that I shouldn't. Photoprism has been my go to. You need to pay for any advanced features, and unless you're solo you need them for permissions only. I love it but I feel like it's still not quite as good as I would like. Is there any major, reliable app for that I missed? reply hommelix 1 minute agorootparent> As a self hosting afficionado, I feel like the perfect Google photo replacement doesn't exist yet. > > The synology photos app (...) > Next cloud (...) > Immich (...) > Photoprism (...) > > Is there any major, reliable app for that I missed? What about Piwigo ? They have a mobile client in F-Droid. I've not used it myself, but a friend is happy hosting his own photo gallery with it. reply mcfedr 9 hours agorootparentprevPhotoprisim Check it out, I've been very happy with that instead of Google photos. reply teruakohatu 8 hours agorootparentI want to like it but it’s quasi-open source. As far as I understand, from the convoluted FAQ, paid Essentials members can’t access the source code, and they are gatekeeping some quite basic features behind Essentials membership. Reading between the lines I think it is closer to dual licensed. With extra conditions for the non-public source. reply remram 4 hours agorootparentWhich features are you missing in the open source version? reply mcfedr 5 hours agorootparentprevAGPL for all the public code, which is most of reply preya2k 8 hours agorootparentprevYou missed Ente (https://ente.io) reply sneak 6 hours agorootparentI’ve been using Ente for a month and have been loving it so far. The lack of feature parity between iOS and desktop/Electron is a bit frustrating sometimes but workable as I always have a phone and iPad nearby. I reported a bug in the desktop app and they fixed it nearly instantly. Looking forward to it receiving more polish, but it is workable in the interim. The public e2ee web gallery sharing feature is killer. reply racked 9 hours agorootparentprevCould you give examples of trouble you've had after updating Immich? Haven't had any myself. reply noname120 2 hours agoparentprevWhat don't you like about the Memories app? In my experience Immich is the same thing but less mature and not nicely integrated in Nextcloud. reply Propelloni 9 hours agoparentprev> their focus has been on developing features for business collaboration as opposed to personal & family ones For people like me, who have to contend with GDPR and NIS 2 requirements on an organizational level in Germany this actually is a god-send. MS and Google don't play nice with the local law, e.g. refusing order processing agreements, although MS has recently started to move on this topic. We replaced MS stuff (all of Sharepoint, most of PIM) with Nextcloud and MS ADS via SAML without too much of a hassle. The sync client has its nuisances (e.g. can't name Nextcloud instances) and it's getting worse for the last few versions! But it does its job, i.e. syncing, and you can always use the cloud itself. reply johnchristopher 8 hours agorootparentHi, we are evaluating at work. Could you shed some light on these questions ? Do you have one NC instance or do you federate (or plan to) ? Do you use the mail app ? Which applications do you use to replace sharepoint features ? Do you use the business version of NC ? reply touggourt 7 hours agorootparentIMHO, the mail app is very slow and need a large screen. Also it doesn't help to manage emails when you need to delete, to search for something, to put in folders and to deal with spams. But I am a power users who like to keep thing clean. Some employees works with it without complaining. Nextcloud recently \"bought\" Roundcube and it might become the prefered mail app sooner. reply Propelloni 6 hours agorootparentprevHi, I'll try ;) > Do you have one NC instance or do you federate (or plan to) ? We actually have three instances. Currently they are running side-by-side but federation is planned, I think, starting October. Account management is still done in ADS and will stay there for now. SAML works as intended. > Do you use the mail app ? Yes, we use the Nextcloud Mail app without the AI features. Most people are using desktop clients, ie. Outlook, for daily work and it is important to us to keep Nextcloud Mail and Outlook in sync. Since the recent changes by MS new Outlook, it is also on the clock, but we are not there yet. > Which applications do you use to replace sharepoint features ? We used Sharepoint for centralized file storage, project management, and department sites and not much else. File storage is a given. For projects we moved to Jira some time ago, so little to do here. For department sites we use the Pico CMS integration. > Do you use the business version of NC ? Not yet. We are well below 200 FTE, so too small ;) We are interested, however, in the compliance and GDPR certification and we have already contacted their sales. But I don't know what became of it. EDIT: formatting reply blendergeek 7 hours agoparentprevNextcloud Notes for Android works. You just can't use the rich editor. The plain editor works great. reply sureglymop 9 hours agoparentprevInteresting.. I use Immich and have been thinking of migrating to Nextcloud Memories. May have to reevaluate. reply nolok 9 hours agorootparentYMMV but any issue you have with Immich that may lead you to migrate to NCM you will find again there,IMHO. Update are easier but lots of other stuff are very much work in progress, even when it works well. My criteria is \"when I want to show my vacation to X on the TV at a dinner 6 to 12 months after setup is it reliable or does it always need some tweak or whatever and ruin the moment\". I dont know the apple ecosystem, for me Google photos is king but I want to own and host my data, Photoprism is my current goto. reply Ringz 8 hours agorootparentI would like to switch to Immich, but it is still unclear to me whether my existing directory structure will be preserved or not. In the beginning, Immich used his own structure, which is a no go for any photographer. reply clort 8 hours agoparentprevwhats up with Nextcloud Notes, in your opinion? (I've been using it for 3 years never noticed any broke...) reply MarcusE1W 8 hours agorootparentFor me it worked great for many years. I do turn off the formatting tools though and write markdown formatting if needed. reply antman 9 hours agoprevLets not forget iPhone vendor lock in that specifically slows down or entirely closes background connections for all other vendors except Apple. So syncing files with Nextcloud or any other app except Apple cloud is an exercise in undocumented futility. You can put it as a front application and wait (if corporate hasn’t forced a screen turning off timeout) reply shwouchk 9 hours agoparentNot sure how the apple rant is related to the post but; I use syncthing (via mobius sync) on my iphone and am quite happy with it. It seems to be OK with enough background syncs as it is, but to top it off i added a shortcut that keeps the app in the foreground while the phone is charging and im always up to date, pretty much instantly. Used to keep a couple hundred gb in sync that way (books, papers, org files, passwords, etc). Not affiliated with the product. reply sneak 6 hours agorootparentIt always leaves a bad taste in my mouth when opportunists charge money for software they didn’t produce, simply by capitalizing on the fact that it’s difficult and annoying to sideload on iPhones. reply shwouchk 3 hours agorootparentIt’s a one time $5 fee, integrates with the files app and generally has a few features that are not part of the base OSS app, such as the background syncs. It also sometimes gets affected by bugs that are not do not affect the “vanilla” syncthing, which takes some non-zero time to resolve and which indicate there is some effort going into creating this “port”. That took some time for the dev to create and would take me at least an hour of my time to reproduce, for which im happy to exchange $5. I could probably set up an ST instance inside iSH but it won’t work as smoothly, or create a native port myself, but I would rather not and feel thus is a fair exchange. Of course, I would welcome and am waiting for the release of your alternative free version. Until this time I’ll take your comment as yet another “i hate apple” post. Judging by your comment I suppose you don’t use generic drugs or any products based on expired patents not by the original patent inventor. reply sneak 2 hours agorootparentI cannot enter into the Apple Developer Program, or I would have released a lot of free builds of f/oss iOS apps already. Sadly Apple requires doxxing yourself to publish apps, which IMO is a human rights violation. You also need to provide a working phone number to be able to install even free apps on a phone or tablet you have already purchased. These policies stand directly in opposition to their stated value that “privacy is a human right”. The time of forced side loading support cannot come soon enough. > Of course, I would welcome and am waiting for the release of your alternative free version. Until this time I’ll take your comment as yet another “i hate apple” post. In my home a few minutes ago I just re-hung a framed, signed, and numbered print of some of Susan Kare’s pixel art for the original Macintosh System 1.0. It’s a common misconception that I hate Apple, but nothing could be further from the truth. Of course, you are free to continue believing incorrect things. :) FWIW, I paid the $5 and have Möbius Sync Pro installed on my phone. But, as a Syncthing contributor, fuck them. (Separately, it doesn’t even work right, whilst iCloud does, because of Apple’s anticompetitive bias against third party apps replicating OS functionality.) > Judging by your comment I suppose you don’t use generic drugs or any products based on expired patents not by the original patent inventor. I don’t believe in the concept of intellectual property. You misunderstand where I’m coming from. It’s a dick move to profit directly off the misfortune or hard work of others, which is precisely what the Möbius Sync authors are doing. It has nothing to do with patents or copyrights or what is or is not legally permitted to be done with the code. You will note that I release all of my own software into the public domain, not under copyleft licenses. People should be free to be a dick, just as I’m free to point it out. reply shwouchk 5 minutes agorootparentYou don’t believe in intellectual property? Well, I don’t believe in gravity. Ill be waiting for your public domain repo with ios syncthing build instructions - no need to doxx yourself. ill happily build it for myself - i don’t have a dev account either. FWIW that app works much better than icloud has ever worked for me. eg with icloud you never know when the device decides to remove a file from local cache until youre offline and need to use it. reply tredre3 1 hour agorootparentprevIf what the Möbius dev did is so easy and they are indeed freeloading, why doesn't anyone replicate it in the open? It's not because of the forced doxxing, syncthing has a foundation with a legal address and real people, using that info on the app store is a non-issue. It's not because of the $99 fee, syncthing has some cash flow. It's been 10 years already, so why? reply solarkraft 7 hours agoparentprevSyncthing (via Möbius Sync) works quite (surispsibgly!) well on iOS, but there are still many paper cuts. I‘d have hoped the DMA would have forced platforms to be decoupled from services. reply touggourt 8 hours agoparentprevOne of my employees use Nextcloud on its iPhone and it works fast and well. reply raverbashing 9 hours agoparentprevWell, honestly, Apple is not wrong here. App Developers usually don't give a flying cluck about anything other than their apps. Battery, CPU, mobile data? Zero consideration. But then of course the fault lies on Apple (and MS - it's the same thing in Windows - see the CrowdStrike fiasco) reply nrabulinski 8 hours agorootparentThe solution shouldn’t be to forbid anyone but apple from doing stuff in the background but to make permissions very clear, allow for stuff like scheduling background tasks and/or limiting background resources and, for power users, to outright allow specified apps to run in the background. So no, apple isn’t right here reply dual_dingo 9 hours agorootparentprevWell, no. This argument might be correct if this policy wouldn't very strongly incentive people to use (possibly paid!) iCloud instead and if Apple would just allow any app onto the app store (or effortless sideloading like on Android). Instead, they heavily scrutinize everything that gets submitted. They could just have special permissions for apps like Nextcloud that would only be enabled if the app behaves correctly regarding this background sync functionality. reply raverbashing 8 hours agorootparentYes, there is an anti-competitiveness aspect here, and I agree with you that the access to the api should be allowed (given some limits) But nobody wants to sysadmin their phone with rare exceptions reply mynameyeff 4 hours agoprevIt is better to use several open-source projects that \"do one thing well\" instead of a single product that attempts to do everything well. If we can advance portable specs so different open source projects can interop... that would be better time spent. reply ajdude 3 hours agoprevIf you're using Mailinabox for your mail server, it comes with a copy of NextCloud by navigating to \"/cloud\" -- you can sign in with your mailinabox credentials! I'm using it for everything from my keepass database to gnucash syncing, calendar and contacts too! reply ang_cire 10 hours agoprevI love my nextcloud server. I set up an instant upload for my phones camera photos to replace Google photos. Now I get all my photos automatically synced to my server at home. reply gchaincl 8 hours agoprevThe idea of using a private cloud sounded wonderful, ran an instance of Next Cloud using encrypted storage, it didn't take long when my files disappeared due to a bug on the encryption and los my files for ever. Never went back to it reply greatgib 8 hours agoprevI like very much Next loud (the idea, the project, some part of the design like storing files flat easily backup able) but my major issue is with the file synchronisation with the Android. It totally does not work. If you try to send or synchronize just a few files it is mostly ok. But when trying to upload more than a few dozen files, things starts to not work well, like transfer being stuck, or the transfer completing ok but then only a portion of files were transferred. That instability is the main reason preventing me to use it daily as in the end I will just use it on the computer where the main value would be on mobile. reply Joeboy 7 hours agoparentFile syncing also frustrates me, although I think it's working as intended. I would expect that if I update a synced file on my laptop, that should also automatically update it on the cloud. But it seems like there's an extra step where I have to go to the web ui and say that I want it to use the updated version. Also sharing has been very confusing, people keep thinking they've shared files they haven't. Maybe the problem is me / us, idk. Or maybe it's better as a \"personal\" cloud than a shared one. I have, overall, not enjoyed using it. I wish management would just let us store docs in a git repo, which would be much more intuitive for most of us. reply sneak 6 hours agorootparentNo, it’s just broken. It’s not you. The software is unfit for purpose. reply kgeist 7 hours agoparentprevWhen I tried Nextcloud, I found there was an option to sync all photos from Google Photos. It synced around 10% photos and reported success. Overall, Nextcloud had lots of breaking bugs/instability issues like that in almost all cases I tried, so I ended up just buying a second account at a different cloud provider, so everything is now synced to 2 providers, in case I lose access to my Google account. I experienced the bug you mentioned about 5 years ago, I wonder if they ever fixed it. reply poisonborz 7 hours agoparentprevYou shouldn't use NC for syncing, much better options exist. NC is great for sharing files and photos though web or mobile apps. reply opengears 8 hours agoparentprevI have good experiences with Syncthing for file syncing. reply mort96 10 hours agoprevI use Nextcloud, but I'm disappointed in their transition from a file host/sync solution to a \"do everything\" solution. Their sync client has languished and their server-side sync software has severe performance issues; the core of what made Nextcloud valuable to me has seemingly remained untouched for half a decade as they chase everything else. I hope some people find it valuable though. reply NoboruWataya 9 hours agoparentI agree with this but at the same time I find NC very useful as a hub that centrally hosts my data and can then speak to other software using common protocols. For example my calendars, contacts, todo lists and notes are all hosted via NC, and I use WebDAV/CalDAV/CardDAV clients on my laptop and phone to interact with it. I definitely agree NC's core functionality could be more polished though. Performance issues don't bother me as I only host for myself but there are UI issues and bugs that get in the way of functionality. I feel like everything is 90% there but there seems to be little interest in making it to 100%. The collaboration stuff they are mostly focused on is useless to me but I can understand why it's their priority, commercially speaking. Occasionally I toy with the idea of replacing it with separate tools for file sync, photos, calendars, contacts, notes, etc, but it's very convenient to have it all as one. reply mort96 9 hours agorootparentThe performance issues are noticeable to me because I'm hosting it on a Raspberry Pi 4. I figured that the bottleneck when syncing etc would be network and disk IO so the Pi would be fine; however when I'm syncing, all cores are pegged at 100% by the PHP processes. I've enabled bytecode cache and Redis and all the other normal performance things, PHP itself is just too slow. And I imagine that at least part of the issue is that every tiny file request or directory listing is its own, separately authenticated HTTP request. But I definitely agree that all the other things are half baked. The media player in the iOS app is extremely buggy. There's jank all over the place. The photos UI is pretty much unusable if you have a lot of photos. And then there are the constant updates; I get notifications all the time that Calendar or some other app has received an update. I wish they'd coordinate releases of their official apps so that I can get a notification every now and then that I need to update all the apps, rather than constant individual app update notifications. In short, as you say, everything is 90% the way there. reply leetnewb 2 hours agorootparentIn the past, the discussion about sync speed seemed to focus on the webdav implementation that nextcloud uses. reply moondev 8 hours agorootparentprevI assume you are not running on the sd card? reply mort96 7 hours agorootparentI boot from the SD card but I have a USB SSD connected which I use for file storage. I have also configured stuff and set up bind mounts such that everything that gets written to regularly (such as logs, the Nextcloud install itself, Postgres, ...) is on the SSD. reply asmor 10 hours agoparentprevI run Seafile for this reason. The sync client isn't pretty, but file transfers are fast, conflict detection good and it's pretty light on resources. reply mort96 9 hours agorootparentI have looked into Seafile, but the thing is that I like some of the extra Nextcloud things, especially the calendar. I just wish that file sync was the main focus and not an afterthought. But honestly, I should probably just use Seafile and get another, more lightweight CalDAV server. reply kiney 7 hours agorootparentI use syncthing for files and radicale for calendar and contacts. Very lightweight and stable for years. Radicale sits behind a proxy with basic auth, so dont have to worry about patching vulns to much. reply asmor 6 hours agorootparentSyncthing is also a very solid choice and I'd use it if I didn't need the ability to create links for people - either to download or upload. reply encom 9 hours agoparentprevI ran OwnCloud for a while, until that project died. I liked that it focused on file sharing/syncing only. Reluctantly migrated to NextCloud and it's bloated beyond belief. It works though, and I've been too lazy to look into alternatives. reply preya2k 8 hours agorootparentThere’s OCIS now, which is a very good alternative if you’re only looking for a file sync solution (without all the App platform aspects of Nextcloud) reply encom 4 hours agorootparentEnterprise focused software written in a meme language. Not interesting. reply mort96 4 hours agorootparentThere are plenty of things to not like about Go, but it's certainly not a meme language. It's one of the first languages I'd personally seriously consider if I were to write server-side file sync software. (PHP, on the other hand, would be way far down on the list.) reply preya2k 1 hour agorootparentprevHow about you give it a try first? It uses about 1/4 of resources as Nextcloud in my experience, is a lot faster and has a way nicer UI. reply PikachuEXE 9 hours agoprevIn case some people don't know using NextCloud + floccus = great bookmark sync (I self hosting it but setup one on a cheap cloud VM is also fine) Not using NextCloud for anything else yet though reply touggourt 8 hours agoparentThanks! I didn't know about that. There is some under documented good little tools that can sync between devices, browsers and Nextcloud : - Qownnotes for notes, selecting text from web pages and bookmarking (I'm curently testing it). - Someone told me that Zotero can do that too. - Vdirsyncer for synchronizing calendars and addressbook between NC and uncompatible apps (Odoo mainly). reply jmhammond 5 hours agorootparentZotero can sync the actual entry’s media via WebDAV which Nextcloud handles very well. I’ve been using it since around 2019. reply jmakov 10 hours agoprevhttps://owncloud.com/compare-filesharing/owncloud-vs-nextclo... reply gostsamo 10 hours agoparentNo comment on anything else, but the tone of the page is so schoolyard. For a company that targets enterprise and government clients, petty jabs at the weather app look unprofessional. reply polycaster 9 hours agorootparentThe \"Comparison\" section of the Product Ownership also seems rather ridiculous, where NextCloud is just a questionable \"fork\" while ownCloud glouriously carries the torch of product ownership. Especially since it's placed directly below the \"License\" section. Apparently, someone didn't quite understand the idea behind the GPL. reply davidee 4 hours agorootparentThis is just like Oracle claiming OpenOffice is the better project and the LibreOffice fork is a shady knock-off. reply CretinDesAlpes 8 hours agorootparentprevEspecially since ownCloud was started originally by Frank Karlitschek who left to fork and create Nextcloud! reply bloqs 10 hours agoparentprevContempt and snobbery are no way to win customers reply phntxx 10 hours agoparentprevWith Owncloud having been bought by Kiteworks it’ll be interesting to see where they’ll be heading product-wise. reply MrDisposable 8 hours agoprevA newbie question: How does Nextcloud compare to Dropbox and Syncthing? Can it serve as a replacement / alternative to them? reply poisonborz 7 hours agoparentApples and oranges. Syncthing is a selfhosted p2p syncing network and it's miles better for this. Nextcloud is a selfhosted \"cloud platform\" like Google Apps. How good it is for each use case varies greatly. Most often it is used for easy web/mobile access for files and photos. reply touggourt 8 hours agoparentprevI don't know which one is better, but considering file hosting, sync and web access, Nextcloud works very well. Regarding usability, as NC doesn't works like Dropbox it is a matter of taste, I prefer NC over Dropbox. reply remram 4 hours agoparentprevSyncthing is much faster at syncing files. So much so that I use both. reply mglz 8 hours agoparentprev100%. Nextcloud Desktop works well and i use FolderSync on android. Has worked perfectly so far :) reply noname120 11 minutes agorootparent+1 for FolderSync, it works great. I haven't had a single synchronization issue or missing files whatsoever after using it for 2-3 years already. Note that I never had the need to handle conflicts (I don't modify files from two different devices at the same time). reply bellajbadr 33 minutes agoprevBasic question, what problem it solves? reply grahamj 5 hours agoprevJust want to say it's times like these I really appreciate the HN community. The NextCloud site makes it all look great and probably easier than some parts of my current DIY strategy, but reading here I think I'll stick to said strategy. reply bellajbadr 33 minutes agoprevwhat problem it solves? reply v4rp1ng 10 hours agoprevstill love it. Been hosting a small (aprox 20 people) instance for a non-profit since more than 8+ years (even before owncloud and nextcloud split up). reply jarbus 9 hours agoprevI love nextcloud, it’s so easy to use once it’s set up and I love how it can do so much of what Google offers like contact sync and calendar reply rewgs 57 minutes agoprevChiming into agree that, yes, Nextcloud really does not deliver on its promise. Its S3 performance in particular is absurdly bad. I want to love it, but it's just so not there yet -- I worry that its lack of focus will prevent it from ever getting there. reply aae42 6 hours agoprevUsed to use next cloud, just recently switched to owncloud infinite scale it certainly seems much more stable so far. The lack of features might be a feature itself. reply kragen 3 hours agoprevhow does nextcloud compare to owncloud? my wife is mostly using owncloud reply a022311 8 hours agoprevHonestly, I feel that Nextcloud is full of bloat, the performance is quite disappointing and it's very unstable. I only use it for file storage, because I can't find a better alternative out there. My advice: never upload directories that may have many small files (and certainly not git repos). For photos, don't even think about it. Use Immich or PhotoPrism. reply vr46 8 hours agoparentWhat issues did you find with the file storage? I’m using it backed by S3 and it hasn’t given me any issues at all so far, be nice to know what to watch out for. reply a022311 8 hours agorootparentEvery now and then uploads might crash, it takes ages for files to load and the whole app freezes. I'm using it with local file storage on an SSD over a relatively fast network. PHP just isn't made for file storage. reply preya2k 8 hours agorootparentprevPerformance of the S3 backend was pretty bad compared to local storage folders on the same host, last time I checked. reply kkfx 10 hours agoprevHonestly I do not favor this paradigm, no matter if NextCloud, O360, SandStorm and alike: the point is that we should teach the desktop paradigm, syncing data around, not using desktops as monsters, hyper-expensive thin clients or more properly dumb terminals of a remote \"mainframe\". It's easy today because current IT evolution is totally skewed, derailed for commercial reasons, but that's untenable. Who think it's normal being even unable to access already made documents only because \"the remote service or the network is down\"? reply Propelloni 9 hours agoparentI agree -- to a degree. But the \"cloud first\" paradigm broke MS' chokehold on the desktop and arguably enabled Linux and Mac to become viable alternatives in that space. Today I can use any major Linux dsitribution and just don't have to worry about interoperability in bread and butter use cases. Even MSO365 works fine. Considering the continued shrieks of pain coming from the graphics design people this is not the case. Those people are still tied to desktop applications and look what freedom of choice they have. Virtually none. reply bo1024 6 hours agoparentprevNextcloud can do local first, they call it desktop sync. Like Dropbox. https://docs.nextcloud.com/server/latest/user_manual/en/file... reply ezst 9 hours agoparentprev> Honestly I do not favor this paradigm I think you are not alone, but I also wouldn't be surprised if this frog is boiled so slow that our opinion eventually becomes irrelevant: today's teenagers are *less* proficient on a keyboard than their elders, because they no longer \"type\" but \"tap\" instead. When the average exposure to computing is reduced to dumbed-down \"consumption-only\" devices running iOS/Android, and that goes for a generation or more, the perception of us would likely be that of old folks yelling at clouds. reply touggourt 7 hours agorootparentThe new paradigm is smartphones firts, not only for childrens. reply bakugo 5 hours agoprevI wish the Android app for this wasn't so unstable and unintuitive, currently you can't even sync multiple selected files without it getting stuck on the \"wait a moment\" screen forever and forcing you to restart. reply mystified5016 2 hours agoprevSimilar to many commenters here, I really want to love nextcloud. It was the very first thing I installed when I built my homelab and it's still running years later. However. I just plain can't use the thing. The only thing that works well is automatically uploading new photos from my phone. That's it, that's the only thing. My server is old, yes, but we're still talking about dual 8c/16t Xeons, 128GB RAM, and a 7-disk striped RAID storage. It's no slouch. I can connect to it with a bleeding edge gaming computer with the latest i9 16 core whatever and a fancy GPU with any browser direct over gigabit Ethernet and it still, somehow, takes a full thirty seconds to show the login page. Clicking on anything in the browser is at minimum a 15 second delay. Trying to browse files takes a good 30 seconds just to switch views to the new folder and another 30-60 to render a plain text list of files. Browsing photos is only slightly slower. I get the exact same level of performance when my client is an ancient first generation i3. It's astonishing, really. I've debugged this far beyond my abilities and the best I've been able to determine is that the browser is spending most of its time just waiting for the server to respond. The server shows no interesting trends in CPU usage, so I have no clue. I can only assume it's attempting to render server-side on a single thread. Given the poor single thread performance on the Xeon this seems fairly plausible. But regardless, this is by far the single worst performing website I've ever seen anywhere, ever. We've all seen horrendous single-word-per-page scroll-scroll-scroll slideshow websites that take ages to load. Nextcloud makes those sites look like bare HTML GeoCities pages. It's atrocious, unacceptable, and unusable. There's no excuse for this. And yes, I'm using the AIO docker image. I've also tried bare metal installs with the same performance. I've tried a more modern i5 machine. I've eliminated any problems that could be on my part and I'm left with the conclusion that nextcloud is just inexcusably bad. Which is a damn shame because this is everything I want from self-hosting. I'd love to get my family onto this. Have a central place to share photos and files. Keep everyone's devices backed up in a place I know is safe and secure. But I wouldn't inflict this thing on my worst enemy. This all sounds like hyperbole, but it truly is the worst of the worst. I have never, since the invention of the internet, seen a website this bad. Edit: oh and also all of the desktop clients are barely-working garbage. The UX feels like it was designed by aliens who heard about human interfaces in a Chinese-room scenario. Utterly absurd nonstandard unintuitive garbage. The android client isn't much better and the iPhone client is famously broken on purpose. reply jmakov 5 hours agoprev [–] Looks like an opportunity to \"rewrite it in Rust\" reply tomrod 3 hours agoparent [–] Given the heavy php nature, absolutely. I have only just started using it (and hence wanted to check what others in HN were using it for). Seems like the personal use case for file syncing is difficult; I'm more interested in self-hosting Kanban and other things I don't want to pay yet-another-SaaS a per-user-per-month for the small business tax. Setting up NextCloud with an IdP wasn't too bad, a little weird on Googe's side that it doesn't permit SAML SLO (but that's my not-terribly-deeply-informed opinion). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nextcloud Hub 9 has been released, featuring integrated tools like Files, Talk, Groupware, and Office for enhanced collaboration and data control.",
      "The platform offers a self-hosted solution with customizable and scalable options, catering to various sectors including public, enterprise, and education.",
      "New features include a local AI assistant for content generation and email summarization, emphasizing Nextcloud's commitment to privacy and compliance."
    ],
    "commentSummary": [
      "Nextcloud, an open-source cloud app platform, receives mixed reviews from users, with some praising its functionality and others criticizing its complexity and upgrade issues.",
      "Users report varied experiences, from seamless operation and easy updates to disastrous upgrades that result in data loss, highlighting the importance of backups and careful management.",
      "The platform's attempt to offer a wide range of features leads to debates on whether it should focus on core functionalities to improve reliability and performance."
    ],
    "points": 236,
    "commentCount": 198,
    "retryCount": 0,
    "time": 1726987821
  },
  {
    "id": 41614663,
    "title": "Flappy Bird for Android, only C, under 100KB",
    "originLink": "https://github.com/VadimBoev/FlappyBird",
    "originBody": "Readme на русском языке Dev blog in Telegram (ENG/RU) Flappy Bird, only C, without Java/Kotlin, weight APK (armeabi-v7a + arm64-v8a) < 100 kilobytes History: It all started in 2021. Then I came across the rawdrawandroid repository. There was a motivation to make some kind of game with the lowest possible APK weight, but at the same time, so that the game would be simple and understandable. At that moment, the idea came up to make a clone of the long-forgotten Flappy Bird game. Which has already been ported to many programming languages. Then, later in 2021, I found another interesting repository Raylib. But, the first attempt to make this game was in C++, using ImGui, because I already knew him. And so, all the difficulties were presented in Android Native Activity and building a clean APK from apktool without Android Studio. The first attempt failed. Firstly, the weight of the APK was about 1 Megabyte. Secondly, there could be crashes of the game. Thirdly, there was only a library for armeabi-v7a inside the APK, and since 2022 Google's rules require the presence of arm64-v8a libraries. Fourthly, the structure of the project and its organization were terrible, it created a mess in the eyes and made it difficult to navigate the project normally. In general, I tried something, it didn't work out, the thought was stored in my head throughout this time, but no more attempts were made. Motivation: Around September 14, 2024, in the Raylib discord channel, I saw a guy make a Flappy Bird in C#. Then it became very interesting to me to try a crazy idea, to make this game in C, for Android, with an APK weighing less than 100 Kilobytes. The idea seemed crazy, as well as unsuccessful. Just imagine, today, when the weight of the APK reaches 500 Megabytes, you only need to keep less than 100 Kilobytes. What are these frames for? It's a sporting interest, will it work out? It worked! But it wasn't easy at all. Implementation: At first, I put together a solution that compiled Hello World in C, packaged the library into an APK, everything was signed and sent to my device via USB. As soon as everything was ready, I went on to explore the resources of the game. The sounds were in ogg format at first, I compressed them, but there were some problems, I don't remember this moment anymore. Then the sounds still became mp3 format, compressed at 16 (kilobytes per second) each, thereby reducing the weight as much as possible, and the sound quality remained tolerable. The first difficulty arose if I had previously used BASS to play the sound, and it's heavy for my purpose, I had to study OpenSLES, which reads MP3 format without problems. Further, png images remain from the resources. There is no other way to use the format. Then it was necessary to find something easier than stb_image. So I came across upng, which completely solved the issue of decoding png files for their further rendering. In general, everything is simpler than it seems. OpenGL ES 2 + shaders for rendering, OpenSLES for sounds, upng for decoding png format and of course Android Native Activity. Build: Download Visual Studio 2022 Open Visual Studio Installer Click \"Edit\" Check the following items: Development of classic applications in C++, Development of mobile applications in C++ Download Android Studio (we need apktool, sdk, ndk from it) Install NDK 25.2.9519653 (you can use the version above) In the project, the setting is made for \"Debug ARM\", but make changes to build.bat (look at the paths) Compile via CTRL + B Copyright: I do not claim copyright. The right to this game and resources belongs to DotGEARS. Inspiration: rawdrawandroid Flapper Raylib ImGui",
    "commentLink": "https://news.ycombinator.com/item?id=41614663",
    "commentBody": "Flappy Bird for Android, only C, under 100KB (github.com/vadimboev)228 points by lostmsu 14 hours agohidepastfavorite97 comments londons_explore 7 hours agoReally wish the app store had a \"only apps under 10MB\" filter. The fastest, least ad-filled and micropayment filled apps are usually the small ones. By downloading a 3 megabyte thermometer app you'll be much happier than a 150 megabyte thermometer app. reply onlyforthat 6 hours agoparentI remember there was a publisher in Play Store who had very small apps like single digit kb flashlight, sudoku, calender, etc. I can't find them now. Those apps were really small all withinNDK communication, than going through JNI boilerplate. reply mouse_ 46 minutes agorootparentprevCouldn't one simply make the boilerplate once, as a library, that takes the pertinent bits as arguments? In which case if your app is C anyways it would make sense to just keep it simple with that. reply infomiho 2 hours agoprevMissed opportunity: name it Floppy Bird since it fits on a 3.5 inch floppy disk. reply kragen 1 hour agoparenthere in argentina everyone calls it 'floppy beard'. and of course 'angry birds' is 'ongry beards' reply jaakl 1 hour agoprevWhy so many bytes? I wrote one using just 141 bytes and it took just few seconds to write, and it is the first functional game I've ever written). Result: https://claude.site/artifacts/3b35069f-4d51-4415-9f58-69988c... reply kragen 1 hour agoparentnice! but how big is the .apk? reply habibur 10 hours agoprevLess than 4k loc. 457 android_native_app_glue.c 360 audio.c 802 game.c 201 init.c 93 main.c 39 mouse.c 38 shaders.c 229 texture.c 1377 upng.c 27 utils.c 3623 total reply kgeist 7 hours agoparentA student of mine had an assignment to write a game using SFML, they wrote a FlappyBird clone and it was like a few hundred lines of code. It's not a very complex program to write. To be honest, I think 4k is too much :) reply dario_od 7 hours agorootparentAre you including SFML loc in that total? reply charles_f 1 hour agorootparentprevgame.c is 800 odd lines. There are some optimizations you could do here and there (e.g load digit sprites in an array to avoid the switch case 1/2/3... stuff). The bulk of the 3000 is fluff that you need because this is C on Android, not SFML. reply kvemkon 7 hours agorootparentprev3623 total - 1377 upng.c (3rd-party tiny PNG image decoding library) = 2246 reply snvzz 7 hours agorootparentprevTo be fair, Android itself requires some level of fluff. So does making the game work well in more than one device. reply londons_explore 7 hours agoparentprevA shame that 4k loc compiles to over 100k of binary size. reply mdp2021 3 hours agorootparentIt compiles to 37kb of 32bit code and to 48kb of 64bit code. /lib/arm64-v8a/libflappybird.so 48kb /lib/armeabi-v7a/libflappybird.so 37kb assets: 29kb icon: 3kb signature: 12kb Plus the manifest (2kb) and resources.arsc (0.5kb) reply lern_too_spel 4 hours agorootparentprevIt doesn't. The total APK size is less than 100k, including images and sounds. reply kvemkon 7 hours agorootparentprevactually including both architectures: armeabi-v7a + arm64-v8a to check: - dependencies statically compiled-in - debug symbols reply tiffanyh 4 hours agoprevSuper Mario Bros was just 40KB https://news.ycombinator.com/item?id=21213421 reply mouse_ 17 minutes agoparentIt was also made to work on exactly one hardware specification, with no operating system to speak of. This flappy bird clone works on an immeasurable number of devices, with varying hardware AND software configurations! reply freitzzz 10 hours agoprevReally cool! I just love seeing Android apps that weight less than a 1MB and run anywhere, even on your old HTC. Congrats! reply mdp2021 9 hours agoparentAlso an assessment of speed gains would be nice. reply mdp2021 9 hours agoprevCould this technique, using rawdrawandroid to write C applications for Android, also use raylib (and other C frameworks)? And maybe could this developing system be used through Termux, to have a C development environment on Android for Android? reply deniska 7 hours agoparentYes, raylib does support android. I have a slightly incomplete build script I use for my raylib projects (obviously you need to take better care of signing, you probably want to build for other targets besides aarch64, your SDK is probably not installed in /home/denis, and I'm not sure whether I'm adding .so files to apk in a way modern android prefers, but it still works). https://gist.github.com/deniska/f1ee73e18e1444eb724c01f933b6... reply userbinator 9 hours agoprev\"For Android\" implies Java is usable, and bytecode can be very dense, so IMHO this could be even smaller. reply MaxBarraclough 8 hours agoparentThere's a Flappy Bird clone for the GameBoy, in the form of a 32kB ROM. [0][1][2] This isn't to downplay this Android-based project though. They aren't claiming to have written the most compact Flappy Bird clone. [0] https://laroldsjubilantjunkyard.itch.io/flappy-bird-gameboy/... [1] https://laroldsjubilantjunkyard.itch.io/flappy-bird-gameboy [2] https://www.youtube.com/watch?v=m839Vg_qXzM reply im3w1l 8 hours agoparentprevI'm honestly surprised at the 100k figure. In my mind it should be possible go far lower. 10k sounds vaguely realistic. reply londons_explore 7 hours agorootparentif you're willing to compromise on the graphics and just get the core gameplay, I reckon you could do it in a 512 byte x86 bootsector. reply lifthrasiir 5 hours agorootparentI think it can be optimized quite a lot by not using a stock PNG decoder library, because all images are quite simple and can be generated from non-pixelated smaller sprites (many images are pre-scaled by 2x, which can be done during the postprocessing) or from a simple algorithmic code. reply Frenchgeek 5 hours agorootparentprevhttps://github.com/nanochess/fbird reply im3w1l 7 hours agorootparentprevRight. I meant with the graphics though. reply Lerc 5 hours agorootparentWeirdly, I think the challenge would be more difficult going to Android than adding graphics while keeping the size down. It would not at all surprise me to see a near perfect Flappy Bird under 4k (graphics and all) as a PC .com I'd be curious to see what the minimum size of a simple C program would be. Say something that displayed a pixel that bounced up and down as you tapped. reply charles_f 1 hour agoprevThis is impressive, especially that it's fully in C. I'm wondering, can you debug C apps on Android? reply mdp2021 47 minutes agoparentPossibly through Termux... Surely for shell commands, I would say; I am not sure about hooking the debugger to a package reply robbiewxyz 6 hours agoprevThis reminds me of code golf, an activity I had some good fun with as a young teen. Coincidentally, one of my first contributions to the community was a low fidelity \"flappy bird\" clone in less than 0.5 kb of javascript. Maybe someone will find fascination in my old hobby and its surrounding community: https://codegolf.stackexchange.com/a/23452 reply lifthrasiir 5 hours agoparentDo you still accept more optimizations? :-) I believe there are tons of mechanical substitutions that can be made there, for example `i%17?r+=z:r+='||'+z` should simplify into `r+=i%17?z:'||'+z`. reply forgotpwd16 3 hours agoparentprevIf constrained by size of result rather source you get sizecoding, a generalization of demoscene. reply sureglymop 9 hours agoprevThat's amazing! I wish there was something like rawdrawandroid for Rust. reply ladyanita22 9 hours agoparentSame, would like to see the same but implemented in Rust reply huem0n 5 hours agoprevNice codebase . That's some if the best looking C I've seen in a while. reply akirk 9 hours agoprevGreat work! Good to see what only it takes to run on Android! On the other hand it also shows how much comes \"for free\" or made easier by using the provided sdks. For example volume control doesn't work while running this. Also resuming the game after switching away. Maybe that's relatively easy to save and restore state, though. reply nubinetwork 8 hours agoprevSo this is an android apk, and not a Linux app that just happens to run on android? I'd really be curious as to why most android apps are huge... reply freitzzz 8 hours agoparentMost Android apps are huge because they bundle tons of assets just to accommodate the “initial experience of the user”. Also, using bloat libraries and frameworks (any shipped by Google), increase the apk size. Nowadays Google offers a solution for this problem called app bundling. It’s especially good if you build a mono app that behaves differently in certain regions. Instead of delivering a raw apk, you deliver a region specific app bundle. reply londons_explore 7 hours agorootparentI'm unaware of any apps that behave totally differently in different regions. Sure - there are sometimes a few disabled features in one region or another, but is that really worth shipping a totally different binary for? Even language packs can be tiny even for 200+ languages if they're pure text. It's only when you get language/region specific artwork that there's a problem. reply freitzzz 6 hours agorootparentYou’d be surprised how heavy a language pack is! reply immibis 2 hours agoparentprevI made a sub-100k Android app once (I am now banned from the Play Store, and I should be lucky they didn't delete my Gmail account too) and every time I opened the IDE (Android Studio at the time) it would automatically add a Google \"support library\" to the project that Google obviously wanted to force me to use. If I forgot to remove it and built the app, it would be closer to 10MB. So that was the minimum size of almost every Android app at the time. reply p0w3n3d 3 hours agoprevToo fast on my Samsung A52s reply cies 4 hours agoprevInteresting to see what a Windows-based project looks like. I haven't used Windows for ages. Seeing the .bat files and vsproj files gave me nostalgic feelings. reply laweijfmvo 8 hours agoprevisn't this kind of what Flutter is? a (relatively big) framework to just draw frames to a barebones Android app. reply dielll 2 hours agoprevAdd it to Fdroid reply tropicalfruit 9 hours agoprev [–] i realised recently, there is a correlation between the file size of a game and how likely i am to enjoy it. the smaller the file size the more likely i am to enjoy it. and the opposite is true. i think part of it is time investment. having less time. i dont see much value in 60gb of 4k graphics textures. pac man on the atari or snes is maybe less than 100kb, while modern pac man could be easily 10gb or more. same for tetris or any game with the same gameplay that hasn't changed much. reply animuchan 8 hours agoparentI totally support this! Coincidentally I wrote a small 2048-inspired game just recently, it's under 13 KiB (zipped; the \"real\" file is about 30 KiB): https://js13kgames.com/2024/games/king-thirteen If you're interested to check it out, please tell me what you think :) reply msephton 7 hours agoparentprevI got a GOTY 2023 for my game YOYOZO which is just 39KB and includes custom physics, online high scores, two music tracks, and more. https://news.ycombinator.com/item?id=38372936 reply bubblesnort 8 hours agoparentprevThe game I like most is a multiplayer immersive reality game in 0 bytes called frogger, where I'm the frog. Beat that! ;) reply coumbaya 9 hours agoparentprevYou'll love Desert Golfing then. reply nubinetwork 8 hours agorootparentNESert golfing is better... reply imp0cat 26 minutes agorootparentNinja golf... reply graynk 5 hours agoparentprev [–] Animal Well is 33 MB and is absolutely great reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A developer successfully created a Flappy Bird clone in C for Android, achieving an APK size under 100 KB, which is notably small for such a game.",
      "The project was inspired by a similar effort in C# and involved overcoming challenges with Android Native Activity and APK size constraints.",
      "Key technical implementations included using OpenSLES for sound playback, upng for image decoding, and OpenGL ES 2 with shaders for rendering."
    ],
    "commentSummary": [
      "A developer has created a Flappy Bird clone for Android using only C, with the entire app being under 100KB in size.",
      "The project showcases the potential for creating highly efficient and compact applications, contrasting with the trend of large, bloated apps.",
      "The discussion highlights the technical challenges and optimizations involved in minimizing app size, such as reducing lines of code and managing assets efficiently."
    ],
    "points": 229,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1726979977
  },
  {
    "id": 41617431,
    "title": "Hy 1.0.0, the Lisp dialect for Python, has been released",
    "originLink": "https://github.com/hylang/hy/discussions/2608",
    "originBody": "hylang / hy Public Notifications Fork 369 Star 4.9k Code Issues 13 Pull requests Discussions Actions Projects Wiki Security Insights Hy 1.0.0, the Lisp dialect for Python, has been released #2608 Kodiologist started this conversation in General Hy 1.0.0, the Lisp dialect for Python, has been released #2608 Kodiologist · 0 comments Return to top edited Kodiologist Maintainer I'm pleased to announce the release of Hy 1.0.0, after nearly 12 years of on-and-off development and lots of real-world use. Hy is a Lisp dialect embedded in Python. See Hylang.org for an introduction and documentation, the NEWS file for a version history, and the HYPE POST for something a little less serious. Henceforth, breaking changes to documented parts of the language (other than dropping support for versions of Python that are themselves no longer supported by the CPython developers) will increase the major version number, and my intention is for that not to happen often, if at all. My focus will be on fixing new bugs as they arise, adding compatibility with new versions of Python, and adding support for whatever new features are added to Python in a fashion that won't break code that works on previous versions of Hy 1.x.y. 14 48 18 7 Replies: 0 comments Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Category General Labels None yet 1 participant",
    "commentLink": "https://news.ycombinator.com/item?id=41617431",
    "commentBody": "Hy 1.0.0, the Lisp dialect for Python, has been released (github.com/hylang)227 points by Kodiologist 4 hours agohidepastfavorite46 comments cooljoseph 2 hours agoI was having some difficulty figuring out how Hy actually is translated to Python (and wasn't even sure if it was compiled or interpreted). Eventually I found on Wikipedia the following: > Hy is a dialect of the Lisp programming language designed to interact with Python by translating s-expressions into Python's abstract syntax tree (AST). Also, looking at the code on Github suggests this compiler is written in Python (see https://github.com/hylang/hy/blob/master/hy/compiler.py). I kind of wish this was made more clear on the main website. Perhaps, instead of introducing Hy as \"a Lisp dialect that's embedded in Python\", introduce it as \"a Lisp dialect that compiles to Python's AST\". The words \"embedded in Python\" don't make it very clear just how it's embedded into Python. The various ways you can embed a Lisp look very different and have very different tradeoffs. For example, off the top of my head, I could \"embed\" a Lisp by writing an interpreter (in C if I care about performance) and letting it be called from Python, perhaps passing in a Python list instead of a string to make it more \"native\". Or I could \"embed\" a Lisp by compiling to Python bytecode. Or I could \"embed\" a Lisp by translating it directly to Python source code. Etc. Regardless, interesting project! reply wodenokoto 56 minutes agoparentFrom the readme / github page: > Hy is a Lisp dialect that's embedded in Python. Since Hy transforms its Lisp code into Python abstract syntax tree (AST) objects, you have the whole beautiful world of Python at your fingertips, in Lisp form. reply rcarmo 1 hour agoparentprevThe \"embed\" part stems from the fact that you can mix Python and Hy in a project with bi-directional calling. Works great, because it is all Python byte code in the end. reply Kodiologist 1 hour agoparentprev> this compiler is written in Python Yes, that's right. Hy is not self-hosted. > The various ways you can embed a Lisp look very different and have very different tradeoffs. Hy itself provides options. Typically the process is that the Hy source code becomes Python AST objects, which Python then complies and executes, but you can also translate the Python AST objects into Python source text. Or you can use Python from Hy or vice versa: https://hylang.org/hy/doc/v1.0.0/interop reply PuercoPop 1 hour agoparentprevThe original hy annoucement makes it clear that they embed a Lisp by compiling with Python bytecode. You can see it in the following video about the 16:25 mark https://m.youtube.com/watch?v=1vui-LupKJI reply vintagedave 35 minutes agoprevI loved the HYPE POST.[0] I work with corporate software. It is absolutely brilliant. [0] https://github.com/hylang/hy/discussions/2609 reply Kodiologist 19 minutes agoparentThanks. I enjoyed compiling a huge list of buzzwords to use for it. reply HexDecOctBin 2 hours agoprevCongrats! Two questions: 1. Does it support REPL-driven development? (condition system, breakloop, etc.) 2. Is there a standalone distribution? Distributing python in itself is a hassle, ideal situation would be to simply distribute a single Hy binary that contains all dependencies within it (either statically linked or as a zip file extracted in tmp directory). reply Kodiologist 1 hour agoparent1. I don't know what a breakloop is. Hy uses Python's exception system, which is more like a traditional exception system than Common Lisp's condition system. 2. No, sorry. reply wrs 1 hour agorootparentA breakloop is a REPL operating in the context of condition handling. When a condition is signaled, you can use the breakloop to modify state and direct how the condition should be handled (including fixing something local and letting the current function proceed by ignoring the condition). Seems like that would only be doable by altering CPython to at least have a hook in the initial exception processing (or maybe there is some magic double-underscore thing for that already?). reply Kodiologist 58 minutes agorootparentI see. That's pretty similar to the feature set of [pdb](https://docs.python.org/3/library/pdb.html). You may then logically ask \"Does Hy support pdb?\". The answer is \"sort of\". I've fixed one or two bugs, but we don't test it. I suspect there are various features of pdb that assume Python syntax and would need some hooks to get working properly with Hy. reply tosh 1 hour agoparentprevnot a standalone distribution but: uvx hy@1.0.0 gets you into the Hy REPL echo '(print \"hi hn\")' > hi.hy uvx hy@1.0.0 hi.hy prints \"hi hn\" https://docs.astral.sh/uv/guides/tools/#running-tools (context: uv can install and manage python versions) reply rcarmo 1 hour agoparentprevI managed to do 2, sort of, with py2app and judicious hacking. You can compile everything to byte code and use Python \"single file\" deployment tools. reply rcarmo 2 hours agoprevAt long last! Now I can finally clean up https://github.com/rcarmo/sushy (I've been poking at it over the years, but every time I upgraded hy portions of the syntax broke, or things would get moved in and out of the hyrule package, etc.) By the way, Hy works really well inside https://holzschu.github.io/a-Shell_iOS on the iPad, although the syntax highlighting in vim/neovim needs to catch up to the 0.29+ releases and async. Although I've tried using Fennel and Guile instead over the years, having access to Python libraries and ecosystem is preferable to me, and with async I can do some very nice, efficient API wrangling (doing HTTPS with fine-grained control over socket re-use and headers remains a pain in various Schemes, so I very much prefer using aiohttp) reply agentultra 2 hours agoprevWow! It has come such a long way since its early, humble beginnings. I saw the original lightning talk that introduced Hy to the world at Pycon those ages ago. Soon after I met Paul and started contributing to the early versions of Hy. I was responsible for the CL-style kwargs (you’re welcome), some minor innards, and a library or two. Whimsy is useful, especially to keep enthusiasm up. It’s nice when hackers can be hackers and not every thing is business. While I haven’t been involved in years it brings a smile to me face to see the project continues apace. What a great milestone! reply knlb 3 hours agoprevCongratulations -- and thank you! I've been playing with Hy on and off (tried to do transformers with it, and then released https://github.com/kunalb/orphism written in hy). Time to pick it up again and take it for a spin reply celaleddin 25 minutes agoprevGreat news, congratulations! Years ago, under the influence of Lisp romanticism late into my university years, I worked on a domain-specific language for designing and analyzing control systems as my senior design project, using Hy! Just checked, it's been five and a half years to be specific. Really, time flies. Here it is for anyone curious: https://github.com/celaleddin/gently Since then, I've been following Hy from a distance and it's amazing to see it's still active. Thank you everyone involved! reply marmaduke 3 hours agoprevI enjoyed the less serious part a lot. I wish more programming related projects could embrace the whimsical. That might the best way to honor the python tradition in any case :) reply Kodiologist 3 hours agoparentI eliminated a lot of whimsy from Hy and its documentation years ago because it was distracting and created noisy test failures, but I did go too far at some point, and have tried to reintroduce a little whimsy more recently. reply instig007 2 hours agoprevYou can get FP compositions without throwing away Python syntax (as Hy does): https://github.com/thyeem/foc reply kayo_20211030 3 hours agoprevVery exciting. I'm in awe of the long-term commitment (over 10 years) that was required to get this to 1.0.0. It renews my faith. Well done. reply blumomo 3 hours agoprevCongratulations! I once bought your eBook on Hy, and still today I regularly receive notifications about your book having been updated. Thank you for your steady contributions. I really want to use Hy in one my production apps one day. reply Kodiologist 3 hours agoparentThe author of the e-book is a different guy, Mark Watson. He isn't involved in the development of the language. reply blumomo 2 hours agorootparentOh, thanks. He seemed so enthusiastic about Hy :-) I just read through the author list on the Hy repo and had a glimpse into their blog posts. Cool stuff, great work. reply BeetleB 1 hour agoprevAny downsides to using Hy (over Python)? Other than my coworkers don't know Lisp? More concrete: Are there Python language features I can't use in Hy? Or performance penalties in using Hy? reply Kodiologist 1 hour agoparent> Are there Python language features I can't use in Hy? At the semantic level, no. I work to cover 100% of Python AST node types with Hy's core macros. It does take me a little bit to implement a new core macro after the CPython guys implement a new feature, but you can always use the `py` or `pys` macros to embed the Python you need, should it come to that. > Or performance penalties in using Hy? Compiling Hy (that is, translating it to Python AST) can be slow for large programs (I've seen it top out at about 3 seconds), but at runtime you shouldn't see a difference. Hy always produces bytecode, which can be used to skip the compilation step if the code is unchanged. reply rcarmo 1 hour agoparentprevYou take a little performance hit upon initial startup (from a clean filesystem, while __pycache__ folders are created). Other than that, mostly everything is the same. I'm now figuring out how to pack images to OpenAI REST calls (using my own REST wrapper), and everything is peachy. Here's my test snippet (mostly to b64encode the file): (import aiohttp [ClientSession] base64 [b64encode] asyncio [run]) (defn :async pack-image [filename] (with [h (open filename \"rb\")] { \"type\" \"image_url\" \"image_url\" { \"url\" f\"data:image/jpeg;base64,{(.decode (b64encode (.read h)) \"utf-8\")}\" } })) (defn :async main[] (print (await (pack-image \"request.hy\")))) (run (main)) This shows you async, context managers, selective imports, f-strings... etc. All that you need, really. reply Qem 59 minutes agoparentprevLack of self-contained tooling. Idle doesn't work with Hy. You'll probably need to fiddle with Emacs to set your environment first, before being able to do anything beyond playing with the language in the REPL. reply paultopia 3 hours agoprevEXCITING! Can't wait to give it a spin! (Does `let` work? I remember that being a barrier for a while.) reply Kodiologist 3 hours agoparentRemarkably enough, yes, we got it to work, on our 3rd or 4th try. reply rcarmo 1 hour agorootparentYep. I use it a lot. reply qwerty456127 28 minutes agoprevDoes PyCharm support it already? reply Kodiologist 16 minutes agoparentI don't think so? https://youtrack.jetbrains.com/issue/PY-48754/Support-for-hy... reply anovick 3 hours agoprevCongrats! Could you compare the language with Clojure? reply Kodiologist 3 hours agoparentWell, this is a little embarrassing: Clojure was one of the biggest influences on Hy in its youth, but that was mostly before I got involved in 2016. I never actually learned Clojure. So hopefully somebody who knows both Hy and Clojure well can answer. I can tell you that at run-time, Hy is essentially Python code, so Hy is more tightly coupled to Python than Clojure is to Java; a better analogy is CoffeeScript's relationship with JavaScript. I get the impression that Clojure tries to convince the programmer to avoid side-effects a lot more strenuously than Hy does, but it's still not a purely functional language, so I don't know how consequential that is in practice. reply a57721 1 hour agorootparentClojure has a good collection library with immutable/persistent data structures, but as a language it allows side effects and has some mechanisms to manage them. It is also possible to call any Java method from Clojure. Clojure does not work with Java ASTs, it translates into JVM bytecode directly. reply chrisrink10 2 hours agoparentprevI haven't used Hy, but I am the maintainer of a Basilisp which also compiles to Python and aims for reasonably close compatibility with Clojure if you're interested. https://github.com/basilisp-lang/basilisp reply anovick 1 hour agorootparentCool project! Wondering how custom immutable data structures fit in with the Python ecosystem. Particularly, I know that NumPy arrays and Pandas Series/DataFrames are the popular data structures used in research computing in Python (for Statistics, Data Science, Machine Learning etc.). These data structures afaik are mutable, however (for performance reasons), so at least the aspect of immutability from Clojure cannot be easily integrated with the Python ecosystem. reply chrisrink10 1 hour agorootparentThis project is much younger and used by many fewer people than Hy, so I couldn't really speak to this besides my own opinions. The few who have started using it and contributing seem to just be using it as a way to write Clojure while interacting with popular Python libraries and tools. Kind of the same way that interacting with the Java ecosystem is often more pleasant from Clojure (IMO) than in Java itself. I've tried to facilitate strong Python interoperability despite the variety of otherwise incompatible features of each language. It's trivial to work with immutable data structures using Clojure idioms and then convert them to Python data structures (as needed) at the boundaries, but the immutable data structures used by Basilisp are also generally compatible with Python's core (read-only) interfaces so that conversion may also not be necessary if you aren't expecting the called function to perform any mutations. reply agumonkey 2 hours agoprevCongrats. It's been a great pleasure to watch it evolve. :) reply chrisrink10 2 hours agoprevCongrats on the release! Very impressive. reply cab404 1 hour agoprevНу, молодцы. reply tosh 3 hours agoprev [–] Does Hy also work with Mojo? reply Kodiologist 3 hours agoparentI'm not sure. I was going to say that Mojo is proprietary software and so I've never tried it, but I just checked and apparently it's free now. If nothing else, you can probably get a lot of Hy code to run on Mojo via `hy2py`, if Mojo supports a lot of Python as it claims to. Edit: actually, confusingly, the GitHub repository for Mojo doesn't have an interpreter. The language is still proprietary. reply tosh 2 hours agorootparentThank you for the hy2py pointer and kudos @ 1.0.0! reply rcarmo 1 hour agoparentprev [–] Not sure either, but it should. I do test it every year or so with pypy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hy 1.0.0, a Lisp dialect for Python, has been released after nearly 12 years of development, marking a significant milestone.",
      "Hy is embedded in Python, allowing seamless integration and usage within Python projects.",
      "Future updates will focus on bug fixes, compatibility with new Python versions, and supporting new features without breaking existing Hy 1.x.y code."
    ],
    "commentSummary": [
      "Hy 1.0.0, a Lisp dialect for Python, has been released, allowing users to mix Python and Hy code, ultimately producing Python bytecode.",
      "The compiler is written in Python, supports Python's exception system, but lacks a standalone distribution, prompting discussions on clearer documentation and tool compatibility.",
      "The community values the project's long-term commitment and whimsical elements, with users sharing experiences and projects using Hy."
    ],
    "points": 227,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1727016182
  },
  {
    "id": 41613628,
    "title": "WP Engine is not WordPress",
    "originLink": "https://wordpress.org/news/2024/09/wp-engine/",
    "originBody": "September 21, 2024 By Matt Mullenweg in Community, WordCamp WP Engine is not WordPress It has to be said and repeated: WP Engine is not WordPress. My own mother was confused and thought WP Engine was an official thing. Their branding, marketing, advertising, and entire promise to customers is that they’re giving you WordPress, but they’re not. And they’re profiting off of the confusion. I spoke yesterday at WordCamp about how Lee Wittlinger at Silver Lake, a private equity firm with $102B assets under management, can hollow out an open source community. Today, I would like to offer a specific, technical example of how they break the trust and sanctity of our software’s promise to users to save themselves money so they can extract more profits from you. WordPress is a content management system, and the content is sacred. Every change you make to every page, every post, is tracked in a revision system, just like the Wikipedia. This means if you make a mistake, you can always undo it. It also means if you’re trying to figure out why something is on a page, you can see precisely the history and edits that led to it. These revisions are stored in our database. This is very important, it’s at the core of the user promise of protecting your data, and it’s why WordPress is architected and designed to never lose anything. WP Engine turns this off. They disable revisions because it costs them more money to store the history of the changes in the database, and they don’t want to spend that to protect your content. It strikes to the very heart of what WordPress does, and they shatter it, the integrity of your content. If you make a mistake, you have no way to get your content back, breaking the core promise of what WordPress does, which is manage and protect your content. Here is a screenshot of their support page saying they disable this across their 1.5 million WordPress installs. They say it’s slowing down your site, but what they mean is they want to avoid paying to store that data. We tested revisions on all of the recommended hosts on WordPress.org, and none disabled revisions by default. Why is WP Engine the only one that does? They are strip-mining the WordPress ecosystem, giving our users a crappier experience so they can make more money. What WP Engine gives you is not WordPress, it’s something that they’ve chopped up, hacked, butchered to look like WordPress, but actually they’re giving you a cheap knock-off and charging you more for it. This is one of the many reasons they are a cancer to WordPress, and it’s important to remember that unchecked, cancer will spread. WP Engine is setting a poor standard that others may look at and think is ok to replicate. We must set a higher standard to ensure WordPress is here for the next 100 years. If you are a customer of “WordPress Engine,” you should contact their support immediately to at least get the 3 revisions they allow turned on so you don’t accidentally lose something important. Ideally, they should go to unlimited. Remember that you, the customer, hold the power; they are nothing without the money you give them. And as you vote with your dollars, consider literally any other WordPress host as WP Engine is the only one we’ve found that completely disables revisions by default. Share this: Twitter Facebook Email",
    "commentLink": "https://news.ycombinator.com/item?id=41613628",
    "commentBody": "WP Engine is not WordPress (wordpress.org)213 points by pentagrama 18 hours agohidepastfavorite131 comments usaphp 15 hours agoIt looks like people here are missing the context of the source of the issue between Matt and WP engine. Couple days ago he posted on X that wpengine has similar revenue to automattic, yet doesn’t contribute back to open source as much as they promised to (5 hour per week per employee or something like that). A wpengine employee replied to a post saying that management doesn’t allow them to contribute to Wordpress open source because it doesn’t align with KPI targets. That employee got fired the next day. That’s when Matt’s issue with wpengine escalated. reply Kye 15 hours agoparentThis seems like a detail that could be covered in a few sentences in the post. In fact it seems central to his argument so it's weird that he didn't. Allegedly it's in the hour-long video, but that's asking a lot. reply bbarnett 12 hours agorootparentSome people, even smart ones, make the mistake of thinking people pay attention to an entire event like this. Likely, his mental model is that eveyone reading that blog has read his X. reply mynameyeff 4 hours agoparentprevGnarly... reply bastawhiz 17 hours agoprevAm I missing something? This feels like such a bizarre hill to die on. He's upset that a company offers hosting of his software with a default setting changed. I guess, don't have that setting then? There's no mention of the source code being changed or custom patches being applied. So the allegation that it's \"something that they’ve chopped up, hacked, butchered to look like WordPress\" is maximally overblown. Unless, again, I'm completely missing something. reply safety1st 13 hours agoparentHe owns a competitor, Wordpress.com. That said, his point is valid. WPEngine is one of the most expensive hosts in the industry yet they meddle with the behavior of WP as well as the underlying PHP and web server and disable a lot of things. Anyone who would like to build a website on WPEngine is advised to read: https://wpengine.com/support/platform-settings/ And that's just the stuff they admit to. The revisions being pulled out are the most egregious I would say. Revisions are an insanely important feature for some publishers, we've had enterprise customers who had to fight tooth and nail to get this turned back on and paid an arm and a leg for it, this would not have been a problem on any other WP host I'm aware of. Their 'long query governor' can be very frustrating and in my experience, contrary to this document it cannot be fully disabled. All these restrictions have crept in over the last few years. WP Engine pulled the usual trick of finding ways to lock people in, and then reducing service to increase margins. reply snowwrestler 6 hours agorootparentHe also owns WP VIP, which is much more expensive (and exclusive) than WP Engine. WP Engine is popular because they hit a sweet spot of automation, capability, and price. That means some things are turned off or tuned. In my experience, WP Engine generally has made good choices there. That said, they are choices that won’t be perfect for everyone. > we've had enterprise customers who had to fight tooth and nail to get this turned back on and paid an arm and a leg for it, this would not have been a problem on any other WP host I'm aware of. Then just move those sites to another host? Why would you bother fighting and paying for this? I have 3 Wordpress sites that are too complex for WP Engine and they are hosted elsewhere. And yes, it’s more expensive than WP Engine, but if a site is truly “enterprise” then the difference is not material. reply Kye 17 hours agoparentprevAutomattic's own managed WordPress platform severely restricts features all the way up to the top plan. I guess it's okay for them to do it. edit: I am referring to WordPress.com which is the offering comparable to WP Engine. reply n3storm 14 hours agorootparentCannot upload own themes or plugins at WordPress.com Also there has been issues for years when trying to move away and transfer __your__ domain to another provider. reply donohoe 15 hours agorootparentprevI think you need to give more context. I’ve been a WP VIP customer at various points and I don’t think this is true as a general statement. It really depends on context. reply snowwrestler 15 hours agorootparentI think they are referring to WordPress.com. reply benjaminwootton 15 hours agoparentprevIf this is their worst complaint about one of WPs biggest hosts then he isn’t doing too badly. I’m not sure if he’s still involved, but the founder Jason Cohen seems very nice and down to earth. I’m sure he would turn the setting back on in 5 minutes if Matt came knocking and this is all there was to it. WP Engines whole reason for being is that open source Wordpress is a car crash to manage. The hacks, the spam, the broken addons on upgrades are in a league of their own. They’ve probably done more to help the platform than hinder it. reply fakedang 12 hours agorootparentDidn't Jason Cohen sell to private equity? Highly recommend reading his smartbear blogs. reply wmf 16 hours agoparentprevMatt is mad that WP Engine is profiting without contributing much back. This \"it's not WordPress\" seems like a fig leaf on top of that concern. reply bastawhiz 16 hours agorootparentIt's his right to feel like that, but it's also WP Engine's right under the WordPress source's license to do what they're doing. reply bbarnett 12 hours agorootparentAs a devil's advocate sort of thing... For no reason I can give you the finger, scream obscenities at you, and there are no legal ramifications. Was I wrong? The legal side is not the end all. Otherwise, why is anyone complaining about Meta's data mining? About the cost of epipens? reply bastawhiz 6 hours agorootparentThe cost of epipens affects someone's ability to not die. Whether WP Engine contributes back to WordPress enough does not. In fact, Mullenweg says WP Engine does contribute back, but not enough, and tries to justify it by saying they've \"hacked up\" WordPress. The latter statement has no legs to stand on (at least without real evidence). How much is enough? Should WP Engine be fixing bugs that don't affect their customers? Exerting influence by throwing contributors at wordpress to make upstream improvements that benefit WP Engine customers? I suspect if WP Engine did what Mullenweg is indirectly demanding, we'd get a \"no not like that!\" post. reply bbarnett 54 minutes agorootparentThe cost of epipens affects someone's ability to not die. Whether WP Engine contributes back to WordPress enough does not. My example also cited two non-life threatening issues, and discussed legal vs morality. You have not addressed this point, so I'll be more direct about it. Doing something legal doesn't make it OK. In fact doing something legal does not imply right or wrong. reply nsonha 14 hours agorootparentprevBut the wording makes it like it's about revisions. reply JoBrad 7 hours agorootparentAnd the title reply shortformblog 17 hours agoparentprevThe linked video offers the full context: https://www.youtube.com/watch?v=fnI-QcVSwMU (Relevant section starting around 10:14) I think he’s using this as one prominent example of overall harm to the space. reply giancarlostoro 14 hours agoparentprevA more accurate title would have not claimed its not WordPress, but moreso emphasized that WP Engine does not provide historic record of your blog posts by default, which every other WP host does, because its how WordPress was designed. Weird title, feels misleading. reply sfmike 12 hours agoparentprevAgreed this is a shocking amount of cognitive dissonance. reply addicted 17 hours agoparentprevYes, you seem to have the argument backwards. The argument isn’t that “turning off saving revision data” is terrible. That’s not the problem or the argument. The argument is that private equity will use open source, maximize profits from the Labour open source volunteers have done, and minimize costs wherever they can. The “default setting” is an example of this behavior. Also, calling turning off all historical revision data as just “changing a default setting” in a content management system is disingenuous to say the least. reply softwaredoug 17 hours agorootparentMatt Mullenweg, author of this post, is CEO of Automattic, a for profit company with valuation in the billions. Largely raised from private capital reply bastawhiz 17 hours agorootparentprev> The argument is that private equity will use open source, maximize profits from the Labour open source volunteers have done, and minimize costs wherever they can. Then why isn't WordPress.com completely obliterating WP Engine in sales? Surely if the argument is that WP Engine is bad and cutting corners, and they have their own first party commercial offering, that first party offering is so good that nobody thinks twice about the private equity hosted version? reply fragmede 16 hours agorootparentBecause the market is stupid. Worse is better, planned obsolescence, clickbait, fast fashion; the free market is full of failures. It's not good enough for a competitor to Google's search engine to be 30% better, it needs to be 10x better in order to compete. reply bastawhiz 16 hours agorootparentYour argument is perhaps unintentionally implying that WP Engine's offering is 10x better. WP Engine is the competitor. It's ridiculous to suggest WordPress.com wouldn't be the default, obvious choice for hosted/managed WordPress. Put in B2B terms, this is as if AWS only sold managed ElasticSearch and they did it well enough that it forced Elastic to change their licensing (as happened in real life). But of course, WordPress can't change their license because it's too ubiquitous and doing so would be an obvious, massive backfire. If WordPress.com was truly competitive, this whole issue wouldn't even be a discussion. It's middling and expensive. And Mullenweg is upset that people are willing to pay for essentially Lightsail-but-it's-just-WordPress over his own offering. reply bad_user 13 hours agorootparentprev> \"It's not good enough for a competitor to Google's search engine to be 30% better, it needs to be 10x better in order to compete.\" This may be so, but there is no competition that's even 10% better than Google. Competition may win on some non-functional requirements, such as privacy (hardly), or UI controls (e.g., silencing certain domains), but that pales in comparison with Google's local search results. And most competition people talk about are just shells around Bing. There are very few independent, general purpose search engines, i.e., Yandex, Baidu, Brave Search. And Google is still the best. And yes, Google's index has been going to shit, due to all the SEO spam and the content farms, but so has everyone else. You made an assertion. Show me the Google competitor that's 30% better and you may have a point. There are none, so the market may be more rational than you give it credit for ;-) reply darby_nine 11 hours agorootparent> This may be so, but there is no competition that's even 10% better than Google. It's been more than a decade since google results were distinguishable from bing results. Both spam you with commercial crap. > This may be so, but there is no competition that's even 10% better than Google. Kagi is much better than google. I have no clue what their baseline search quality is like, but I don't care because I can customize it enough to far outstrip quality that google can provide as google refuses to provide (or has actively disabled) the tools necessary to make search useful, like allowing the user to blacklist domains against all their searches or prioritize certain domains. Which is dumb, because how could they know what sort of results i value if they refuse to ask? What we really need is an engine that excludes all commercial results, and an option to exclude sites with ads. That'd be a goldmine. reply carlhjerpe 5 hours agorootparentKagi is better at searching internationally or maybe nationally, but local search is still dominated by Google. Looking for bars, restaurants, corner shops, grocerers and such is Google territory in my experience. reply Diti 10 hours agorootparentprev> how could [Google] know what sort of results i value if they refuse to ask? They don’t care about the end-user of their search engines. You are not the client – advertisers are. All Google care about is to present search results that maximize revenue for sponsored results. reply bad_user 5 hours agorootparentprevWe must live in a different world because for me, Bing's results have always been completely unusable. As a consequence, DuckDuckGo's results as well, although I understand that it must work for certain types of users located in certain countries; otherwise I can't understand how anyone would use it. I just logged into Kagi, and searched for “restaurants”. Kagi can see my country because their UI says so, yet it gave me “top 10 restaurants in Groningen Province” (Netherlands), as the first result and the second result was “top 10 restaurants in Barcelona”. And I don't live in the Netherlands or Spain. I also searched for a programming question, an issue I recently had, with the query “slick breaks binary compatibility”. Google gave me fresh discussions describing the issue, whereas Kagi gave me a GitHub issue from 2021 that described issues with the previous major version. I did not cherry-pick these searches. --- Speaking of, Kagi doesn't have their own index, they just use Bing's API, enhanced with results from other sources, much like what DuckDuckGo does; and like DDG they tend to be disingenuous about it. https://help.kagi.com/kagi/search-details/search-sources.htm... What in the world are people paying for is a complete mystery to me, but YMMV. --- So, again, I'm asking for that one competitor that's 30% better than Google. reply skywhopper 7 hours agorootparentprevBetter in what way? Better at not abusing your identity and private information to make bank arbitraging a monopolized ad market they own both sides of? reply technion 16 hours agorootparentprevIn this case the market is not stupid. WordPress.com's managed offering is limited enough that most web developers make valid recommendations to avoid it. Wpengine works a lot more the way a WordPress user wants it. reply can16358p 11 hours agorootparentprevTo be honest forget being better, if there's a Google competitor even on-par with Google, I'd switch to that immediately and never use Google or any of their services again. Though it's just me of course, can't say the same of the masses. reply blackoil 15 hours agorootparentprevWhy open source the software, if you don't want others to use it? reply adolph 16 hours agorootparentprev> private equity will use open source, maximize profits from the Labour open source volunteers have done, and minimize costs wherever they can Still a bizarre hill to die on. Were contributors misled about the license that allows a private or public company or a nonprofit org to use the software for profit? Did Matt Mullenweg delude himself into thinking that only people who share his ethos about content would use the software? Did noone learn anything from the lamentations that started with Berkley and ATT, continued with Redhat and most recently with Elastic-Amazon and Hashicorp-itself? My impression is that Mullenweg is a thoughtful person, so maybe I'm missing something. reply amanzi 16 hours agoprevThe WordPress revision system is a real pain to deal with. I've been hosting and running a multi-site WordPress site for about 15 years, and the biggest drain on server resources was the revision system. Thankfully, there's a setting to limit to disable the revision history: https://wordpress.org/documentation/article/revisions/ For Matt to call WP Engine a \"cancer\" because they use WordPress-supported functionality to turn off a WordPress feature is bizarre. All WordPress hosts modify the software to make it work for them. Especially Automattic! reply tr3ntg 16 hours agoprevA more accurate and even more confusing statement would be “Wordpress is not Wordpress” which is a reality that I’ve had to explain to clients (in a former life) countless times. “But when I go to Wordpress.com…” SORRY, forget everything you saw there, that’s not Wordpress. Same logo? Yes. Branding? Yes. Company? Yes. But it’s not Wordpress. This one setting that WP Engine disables is a shame, but it’s nothing compared to the confusion that Automattic has brought upon themselves reply jacobyoder 10 hours agoparentAnd github != git, which we're still wrestling with over a decade in to its life. Yet, for some people 'github===git' so that just compounds the issue. reply creinhardt 12 hours agoparentprevI maintain that the Wordpress.com confusion has done more harm than good for Wordpress as a platform over the years. I really wish Automattic had chosen any other branding. reply PedroBatista 7 hours agorootparentAnd leave huge chunk of brand recognition money on the table while having to spend copious amounts of it to build another brand? If me or you were in the same position we would do the exact same. reply bubblesnort 8 hours agoparentprevYou just turned off the capital_p_dangit filter! WordPress ;) reply ChuckMcM 17 hours agoprevI have a long rant about how open source is wage theft and value extraction by unscrupulous third parties is built in and unavoidable. But setting that aside, I learned of WP Engine when they launched here, on hacker news. At the time their value proposition was \"bullet proof and secure wordpress for people who just want to publish and not learn devops\". Over the years, I've watched them through a progression of management changes move from value of service to value extraction. Chipping away at costs while holding the price constant or raising it and extracting the difference for themselves. This isn't in and of itself a \"bad\" thing, it is what business does, however I find the integrity around value extraction varies tremendously. From zero integrity Mackenzie type MBAs to high(er) integrity owner operators. It is rare when a management team says, \"this is enough money\" and that is sad. reply jonas21 16 hours agoparent> I have a long rant about how open source is wage theft and value extraction by unscrupulous third parties is built in and unavoidable. How can it be wage theft when people voluntarily contribute to it? If you don't want others to use your software, don't write open source software. reply nqzero 15 hours agorootparenttry releasing something under an open-but-not-open-source license as a solo developer or small team. there's a lot of established developers (presumably earning high salaries) that will very vocally badmouth the license choice. i'd seen this happen over and over again eg here, and when i've asked other developers why they open sourced their products they've said the same, and it was one of my concerns when i approached launch sadly, my market fit was so bad that nobody ever looked at the license ::karma:: note: i have no problem with someone choosing not to use a product with a license they don't like (i do the same). it's the dissing of others that would use it that potentially crosses the line. i'm not even saying it *is* theft, only that there's a valid argument to that effect reply bad_user 13 hours agorootparentYou're being disingenuous. There are 2 reasons freemium shared-source-style licenses are bad mouthed: 1. The products get advertised as open source; inviting people to look at it and contribute, the problem being that they are legal minefields, copyright or patents lawsuits that are waiting to happen. This was the biggest complaint against Microsoft's Shared Source initiative back in the day, and it's just as true now. 2. Some companies made their product popular via Open Source, like MongoDB, Redis, Elasticsearch, took all the contributions and the free marketing, then switched; such instances being a bait-and-switch. Elasticsearch in particular is interesting because what they wanted was to withhold security patches from the OSS version, and Amazon got on the way by pushing PRs for patches. All these cases are more glaring examples of value extraction, benefiting from unpaid labor. There is nothing wrong with developing proprietary software, but you need to be honest about what you're selling. reply nqzero 12 minutes agorootparent- me: solo dev or small team - you: microsoft, mongo, redis one of us is being disingenuous, but i don't believe it's me (username on point ;) i agree with both your examples being bad. for #2, they required one-sided CLAs while \"open source\". the alternative to signing the CLA was to fork, which is rarely well-received by the community, ie the same basic issue i raised FLOSS is great in that it can facilitate collaboration and adoption, but at the expense of greatly limiting the business models. and even then context still matters - eg there's big difference between the kernel with 1000s of independent contributors, and mongo with one party holding CLA rights to the entire codebase (FLOSS in name only, i'd argue) - for me, what's ultimately important is that people are free and that people that do good work are rewarded - software licenses are just a tool to help us get there - non-FLOSS might enable much of that same good while scaling to more business models and software - they might ultimately be good for society, but getting the details right is hard - i'm no longer actively working on my own stuff, but my attempt was: https://github.com/db4j/pupl - note: if i was doing this today, the core limit would be much higher reply mynameyeff 4 hours agorootparentprevAgreed. reply ChuckMcM 13 hours agorootparentprevMy thoughts were posted in Mastodon: https://chaos.social/@ChuckMcManis/112429390169387783 but the TL;DR is that writing code adds value, and that value will be realized by people who take it for free, even if you don't want them to. From my perspective (not saying its the right one, just one that I've reasoned to) that is theft. reply echoangle 11 hours agorootparentThat’s about as much theft as me baking cookies, putting them on the street with a sign „free to take“ and people eating them is theft. Who would expect other people to add cookies and call everyone else thieves? You open source license doesn’t say „you can use it if you contribute“, it says „everyone can use it (as long as they attribute or fulfill some other requirement). How is that theft? You can’t be a thieve if you take something someone else is handing out for free voluntarily. reply jpc0 12 hours agorootparentprevCounterpoint for this. How may of the packages in your node_modules folder have you contributed even an issue to, never mind a patch? Do you even know what packages are in there? Does that not make you or your company a thief? If your argument is you don't use JS/node then answer the same question about your languages package manager / standard library / compiler etc. reply hn_throwaway_99 17 hours agoparentprev> It is rare when a management team says, \"this is enough money\" and that is sad. It's not just rare, it's important to know that in many arrangements it's simply impossible. Once any company takes outside equity financing, every quarter is only looked at in terms of growth, and anything a company says should always be viewed in terms of \"will this allow us to grow revenue.\" I'm not saying it makes all companies \"evil\", and there are a good deal of \"do good\" things that happen to coincide with growing revenue. But you can bet that if any principal requires going against growing revenue, it will be jettisoned in short order. The only way to avoid this is to not take outside funding, and even then there is no guarantee what happens when the business is sold (and when a business is sold, the acquires are nearly always going to look at ROI solely from monetary returns). reply pdntspa 17 hours agoparentprev> This isn't in and of itself a \"bad\" thing, it is what business does Yes it is. If you're hollowing out your value prop you better be returning at least some, if not a majority, of that value back to your users in the form of price drops or increased service levels. We have all been Stockholm Syndrome'd into believing otherwise as our favorite products and services institutionalize themselves into nothingness. Ownership and management are mere stakeholders in the business, equal with (and no more) than customers and employees. reply coliveira 17 hours agoparentprev> rant about how open source is wage theft and value extraction by unscrupulous third parties is built in and unavoidable Yes, I came to realize the same thing about open source, it was created with lofty ideals, but the practice is just the opposite. Of course, most people will not agree with this conclusion since the whole industry will tell them otherwise. reply Sparyjerry 15 hours agorootparentObviously many people will use open source without contributing. The point of making it open source being that others will run into bugs, let you know about them, and maybe even try to fix them, especially if they also become or are reliant on the software. They might even want improvements to make it work for them and again work on them if they have a need for it. If a software is good and/or mature enough it doesn't need to be open source in the first place and doesn't need contributions. Wordpress has sort of reached that mature place where it is already 'good enough' but you can't now un-open source it. reply bigiain 12 hours agorootparent> The point of making it open source being that others will run into bugs As I see it, that's part of the OSI's revisionist history they want you to believe. The \"given enough eyeballs, all bugs are shallow\" phrase was coined by Eric Raymond and _named_ after Linus, without it having been \"the point\" of why Linus chose to release Linux with the license he used. I don't know if it was intended at the time, but it's an idea that's widely been used to sell value-extracive behaviour to businesses. The worst example in my opinion being project that rug pull the users/contributors when the close the permissiveness of the license after having built their project/product taking full advantage of those \"many eyes\". Yes I'm looking at you Mongo and Hashicorp... reply clankyclanker 17 hours agorootparentprevThat's why (A)GPL is so handy, it discourages value-extraction. As cwebber says: https://dustycloud.org/blog/why-i-am-pro-gpl/ reply mxey 4 hours agorootparentWordPress is licensed under GPL. reply bigiain 13 hours agorootparentprevI still recall back in the daybefore the OSI decided to politicise (and perhaps even value-extract) the term \"open source\", there were still a heaps of people doing groundbreakingly useful coding and giving it all away. Linux is perhaps the posterchild of that, but Apache was just some people patching NCSA httpd because they needed new features. Perl was Larry Wall writing a better language for generating reports. PHP was Rasmus Lerdorf's project to make it easier to build his personal website and make it \"dynamic\" and able to connect to databases. The vast majority of the commands and utilities in your Linux distribution were written by the GNU project and many of those by Stallmann himself. Out of those (and many other examples most greybeards here could mention), probably only GNU/Stallmann were motivated by \"lofty ideals\", pretty much everyone else was doing what was called \"scratching their own itch\". The needed features or software that didn't exist, so they wrote it. License-wise, Apache could only exist because NCSA httpd allowed it - it was explicitly released into the public domain (and later GPL-ed I think?). Perl was dual licensed, you could choose GPL or Perl's own \"Artistic License\" which was much more free, allowing you to sell stuff incorporating it so long as you still credited the original authors. Linux was originally licensed with something Linus wrote without lawyering up, which was basically \"the source code must remain available\" and \"no money can change hands\" because he was annoyed at having had to pay for Minix, but shortly after he changed his mind abut the money aspect, and chose to use GPL instead. While from my perspective, Stallmann was the \"lofty ideals\" guy, and pretty much everyone else were pragmatists who were \"just getting shit done\", and sharing stuff so other people could get shit done better too. It was Eric Raymond and Bruce Perens who hijacked the already in-use term \"open source\", and claimed to have invented it and went on to build a business-friendly brand and \"industry\" around it - and as you say those \"industry people\" will _always_ jump in with their revisionist history to \"correct\" you when you say otherwise. So to me, OSI-defined \"open source\" really can be wage theft. It isn't always, but it's intentionally set up so it can be. (Probably not originally with malicious intent, I suspect back then many of the people involved were perfectly happy with the reputational payoff in lieu of monetary reward.) The existence of \"Open Source Companies\" like Mongo, Elastic, Hashicorp et al, and BSL-style licenses - seem to me to be closer in spirit to Linus's original license than anything the OSI approves. And I feel they are the result of a new (relative to the '90s) trend of people writing open source software for the sake to writing the software, instead of the old-school approach of people writing software to get their actual jobs done better or to scratch their own itch. Linus and Larry and Rasmus and Brian didn't write/release their software so they could become Founder/CEO of Linux PTY LTD or Perl Corp or PHP Inc or Apache GmbH. They did it because they wanted an OS they didn't have to pay for, or a better way to generate reports, or a better way to make websites, or a better way to serve the HotWired website. reply blackoil 15 hours agoparentprev> I have a long rant about how open source is wage theft. Good you didn't write, or else I may have read it for free and did a wage theft. reply ChuckMcM 13 hours agorootparentI posted it on Mastodon https://chaos.social/@ChuckMcManis/112429390169387783 reply bigiain 12 hours agorootparentI mostly agree with your rant, but I counter your example of sticking $100 bills up in your front yard as \"art\". There are many people who spend hundreds or thousands (or more) sticking up outrageous displays for halloween or xmas lights in their front yard, expecting no more payoff than watching people be delighted as they pass by. That's a great and admirable motivation. Even if it's largely done just to show Bob down the street how lame he is and how you can gather a bigger crowd than him - that's less great but still OK in my opinion. I admit that most of those displays are only able to be paid for because the homeowner is getting that $5k/week at their big tech job or their finance job or whatever. Same as how Burningman's \"free\" art mostly couldn't exist without Bay Area tech millionaires funding it as they cosplay being an Artist. reply bkyan 17 hours agoprevIn my experience, the revision system in WordPress is not performant. The more revisions there are for a given page, the longer it takes to publish that page, eventually leading page publish requests to time out. I usually have to limit the number of per-page revisions to keep, to prevent that from happening. reply b0ner_t0ner 16 hours agoparentMost Wordpress sites can go static (like using WP2Static), so the HTML online is always fast. reply bkyan 15 hours agorootparentThe performance bottleneck with regards to revisions is on write, rather than on read. reply racked 16 hours agorootparentprevOnly if you avoid common plugins that need some server-side interaction, for instance Gravity Forms. (Probably _unless_ you use some custom rewrite rules, but then you're walking on less-trodden paths) reply bdcravens 17 hours agoprevWe settled on Wordpress and WP Engine for the marketing side of my employer's website. Strictly speaking, I'm in charge of all of our technology choices, but my time and attention is better spent on our product, and WP Engine and its deployable artifacts is good enough for leadership. (We are a very small company) My boss (ie the owner) has never asked me about per post versioning. When we need to roll back, WP Engine's custom snapshotting fits the need. reply donohoe 15 hours agoparentI hear you for marketing. On journalism side it is helpful - depends on editorial workflow which varies from org to org. reply sgammon 18 hours agoprev> WordPress is a content management system, and the content is sacred. Every change you make to every page, every post, is tracked in a revision system I'm not sure I see how the absence of a revision tracking system rises to a violation of sacred principles. reply np_tedious 18 hours agoparentThe revision system is not (not is it claimed to be) a sacred principle of life, morality, or society at large. It's a core tenet of WordPress. reply snowwrestler 16 hours agorootparentAccording to whom? I’ve developed and run Wordpress sites for nearly 15 years and never enabled revisions on a single one. This whole line of thinking—one person defining the “core tenet” or whatever—seems directly contrary to the ethos of open source. If Matt doesn’t think people should be able to turn off revisions, he should put that in the license. Otherwise, users can do what they want and open source leaders should celebrate that. reply echoangle 11 hours agorootparentIsn’t it even just a setting they changed? From my understanding they didn’t even change the code, they just changed the setting from the default. reply x0x0 15 hours agoparentprevNot to mention I'm skeptical the majority of WPEngine sites are blogs, particularly with prices starting at $240 a year. Rather they're company marketing sites. That's certainly what I used WPEngine for, and the service was well made, particularly their ability to test WP updates in staging. reply sgammon 18 hours agoparentprevYou can also just turn it on by emailing support... reply sgammon 17 hours agorootparentAre my downvoters here drafting posts that have greater than 5 revisions? Or? reply SadTrombone 18 hours agorootparentprevYou can turn on an extremely limited version. Did you read the post? reply sgammon 17 hours agorootparentI did read the post, yes. Why does everyone keep asking everyone that? reply jodrellblank 14 hours agorootparentBecause they don't read the HN guidelines: \"Please don't comment on whether someone read an article. \"Did you even read the article? It mentions that\" can be shortened to \"The article mentions that\"\" - https://news.ycombinator.com/newsguidelines.html reply shortformblog 17 hours agoprevI personally dislike WP Engine after years of having used them. Earlier this year there was a story about how Cloudflare had bizdev representatives reach out about a technical issue under the guise of customer support. I saw the very same thing with a client on WP Engine a few years back, and it was over something that they likely could have done something about given their position in the ecosystem: Too many spiders hitting old URLs. Instead of recommending strategies to help fix it, they jacked up our prices. It was a huge pain for everyone involved. But I think the challenge is, at an agency level, it’s hard to move to another CMS host because it’s seen as difficult to move up the food chain. Providers like WP Engine exploit this misunderstanding by targeting non-technical customers with promises that they’ll help you out. That was clearly an opportunity for them to step in, and they used it to put on the squeeze. reply steviedotboston 16 hours agoprevIt's pretty bizarre to claim that WP Engine is causing confusion here when the real confusion is between WordPress.org and WordPress.com reply sureIy 15 hours agoparentIs it honestly that confusing? One has a download button and one lets you sign up. Anyone knowing who to set up that zip file knows the difference. What you could argue is that Automattic is gatekeeping the best features to the .com and using it as a manager/trojanhorse for the .org version. reply librasteve 11 hours agoprevIt’s probably worth mentioning that WP Engine author the _Local_ app https://localwp.com/ which is far and away the best tool to build WP sites errr on your local machine. Then just upload to WP Engine and your are live. This valuable innovation is fueling the growth of WP Engine and makes this a bit of a honey trap. Personally I’m not convinced that keeping a 100% audit trail for reconstruction of old states is useful in production. It may well be a better trade off to run WordPress code in a leaner way for speed and storage requirements. Since that architecture results in many ways to get the current state from the db there is also a purity argument that the default schema is too denormalised. Discuss… reply sublinear 17 hours agoprevI can only speak as someone who has done enough web dev to feel like every CMS is designed to get in the way and extract money. Using any variant of wordpress on a project seems equally negligent. reply graeme 17 hours agoprevThis feels like a private which has reached the public shorn of the context that would be necessary to understand it. I use Wpengine and have enjoyed it. They have some aggressive upsells and you learn you can ignore them. Actually had no idea about revisions, hadn't used them before I hosted with Wpengine. I very much like using Wordpress and Wpengine has helped make it easy to do so. I'm sure they have some things to work put between them but I feel this needs more info. At a certain level it's open source software and if it isn't a trademark violation and is allowed by the license terms then Wordpress has only moral suasion to work with. Very happy with the work Wordpress has done to make an amazing ecosystem. If they need something from WPEngine to keep things going I think it's fair to ask and perhaps they did but we're a bit in the dark here. reply pluc 4 hours agoparentIt's a little telling that 1) Matt has nobody around him to say hey maybe don't post that on our corporate site without context and 2) if he does nobody pointed that this reads like another millionaire hissy fit because someone is doing something they don't want them do and they're used to getting their ways with no resistance. reply browningstreet 14 hours agoprevI hosted on WP Engine for many, many years and everything got slower, including their control panel. I moved to a service that cost me less for two years than what I paid WPE for one month, and they’re faster and have had zero issues. I also had to rely on backups and they worked too. WPE was great in theory a few years ago, but then they acquired a few other companies and added too many distancing layers for tech support. They have professional features and a cool API, but their hosting speeds are now abysmal. reply toxican 17 hours agoprevI've been out of the WP community/scene for a number of years now, but this post is kinda weird. Revisions definitely aren't the hill I'd die on when choosing a hosting provider at all. Especially considering WPEngine does a lot of things very well: 1. Dead-simple staging environments 2. Support for Local, which makes WP development an absolute breeze because I don't need to maintain docker, vagrant, or a LAMP stack, etc. And it makes deployments quick/easy. 3. Dead-simple backup/restore features 4. Simplified cache-management And yeah, I've got the technical know-how to handle all of that myself directly on a proper server and all that devops-y goodness. And yes, $5/mo shared hosting cPanel provider would be comparable (and let's be real, it's good enough for most people using WP)....But man is it nice to just charge/pay a little more for a host that just does that crap for me with a nice interface. I like revisions as a feature. Hell, I made reference to them a lot in the training material and sessions I put together for clients way-back-when as a way to give clients the confidence to tweak copy without fear of completely ruining their site. But this blog post seems to pretend it's the heart of WP and without it, it's an entirely different piece of software all together, which is absurd. reply walterfreedom 17 hours agoparentPersonally, most the features Wpengine offered wasn't very useful for my team and we just went to rent servers on Hetzner for the sake of simplicity. Same goes for Cpanel too. We just used google drive + mysqldump to synchronize the database and rsync to synchronize the files. However, we manage the site ourselves with a team of 4 dedicated software engineers so I don't know if this approach would work for customers who just want a site that they will edit themselves. reply softwaredoug 16 hours agoprevWhat's maybe more disturbing is the CEO of one privately backed company (Automattic) can use a privileged community position to malign another community member (WPEngine). As a passive observer of Wordpress it feels like a very icky conflict of interest. Especially when you throw in the oooh \"spooky boogieman private equity\" fearmongering language. reply PeterZaitsev 4 hours agoprevMatt has chosen to keep Wordpress Opensource, and thanks to him for that. This allowed companies like WPEngine to exist... and focus their resources like marketing and maintaining their internal \"fork\" rather than contributing to core Open Source Project... and this might be one of the reasons they are getting some good success. Generally trademark is what should offer some protection here and I think this is where Open Source landscape was not tested well. WP Engine claims on their front page they are \"Most Trusted Wordpress Hosting\" which arguably makes folks to assume they host full featured Open Source Wordpress, which per Matt's article does not seem to be the case. reply snowwrestler 16 hours agoprevWP Engine is fantastic. I’ve hosted Wordpress directly on Linux (with me as sysadmin) and tried out or evaluated probably all the most popular WP hosting platforms… including both WordPress.com and WP VIP, owned by Automattic. I settled on WP Engine. It seems a little disingenuous for Matt to pull on the self-righteous mantle of open source in order to run down a company that directly competes with his commercial platform. What happened to the idea of WordPress as an inclusive, flexible project that lifts all boats? What happened to open source means you can do what you want? The more I think about it, the more troubling this seems for other commercial entities working with WordPress. Is Matt going to start putting targets on the backs of companies who get too successful with “his” software WordPress? reply ChrisArchitect 17 hours agoprevAmazed that this got to the point where an official post was deemed necessary. reply pluc 18 hours agoprevI mean, yeah. Isn't that what they're supposed to be doing? Because an option exists to store every revision doesn't mean you necessarily have to do it for free. It is a costly feature. They're free to chop up WordPress as much as they like and monetize features that elsewhere are default. They're not hiding it and users can choose to go elsewhere. Otherwise every WordPress host would be the same, just hosting vanilla WordPresses... and while the WordPress people may not like it, ain't nothing wrong with it. reply tomphoolery 18 hours agoparent> while the WordPress people may not like it hmm i wonder why... https://wordpress.com/wordpress-hosting/ always great to see devs sh1t on other devs under the premise of \"this isn't right!!!\" when in reality it's just affecting their bottom line. money makes the world go round! reply mattl 17 hours agorootparentAnd this post is on WordPress.org See: https://wordpress.org/hosting/ reply SadTrombone 18 hours agorootparentprevDid you read the post? They have no problem with other Wordpress hosts and are calling out WP Engine specifically. They even suggest switching to literally any other WP host that isn't WP Engine. reply rgbrenner 18 hours agoparentprevYou can chop up WordPress, because it's open source. But you cant chop it up and keep calling it the same thing. Only the vanilla WordPress gets to be called WordPress... because the project called WordPress has decided that the release they put out is what they want to attach their name to. Someone cant come along later, make a bunch of changes, and then attach someone elses name to it.. and then go around insisting that their version is the same thing. Most large open source projects have a trademark policy that makes this explicit. Don't know if WP has one though. reply pluc 18 hours agorootparentWordPress makes those features disable-able themselves, nobody has \"chopped up\" anything here. define('WP_POST_REVISIONS', 0); That's it. reply rgbrenner 17 hours agorootparentThey're free to chop up WordPress as much as they like Disabling a feature isnt chopping anything up. Im specifically commenting on this statement you made. reply pluc 17 hours agorootparentI'm using the author's definition of chopping up. > What WP Engine gives you is not WordPress, it’s something that they’ve chopped up, hacked, butchered to look like WordPress, but actually they’re giving you a cheap knock-off and charging you more for it. reply Kye 17 hours agoprevThis has the same energy as a spat between two Mastodon instances. Did he make any effort to work this out with them in private? I'd think he would mention that if he did. Actually, since WordPress supports ActivityPub now, it's a spat between two massive AP platforms with apocalyptic potential. Someone needs to get follower migration from WordPress to anything else on the AP fediverse done quick. reply usaphp 15 hours agoparentHe mentioned during the talk that he made multiple attempts to resolve this in private but the other side wasn’t interested in hearing him, he also said he did not want to do this presentation reply Kye 15 hours agorootparentThat's an hour long video. This detail should be in the post. A few sentences at most. reply dz0ny 11 hours agoprevI would agree if revisions were enabled by default just for posts and pages. Here’s a common example: you have a blog with 20 pages and 20 custom plugins (quite a standard these days). In a year of hosting that one, you’ll end up with millions of revisions in the database, which is really resource and speed issue for MySQL servers as the developers of WordPress never considered sharding; everything is stuffed into one table, with no indexes by default. So, usually, the size of the DB and resources to run it go over what you would expect, so naturally, you limit it to a more sane value. TLDR; In pristine WP env with no plugins unlimited revisions make sense. In a WP with many plugins they don't as other plugins declare them but not use them and thus we end with a system that uses huge amount of resources for nothing. Oh and revision in WP is not like GIT revision as is full copy of the content. reply pbowyer 7 hours agoparent> I would agree if revisions were enabled by default just for posts and pages. And if you could say \"Keep revisions for 30 days\" and/or \"Keep the last 10 revisions for every post\". Why that hasn't been implemented... reply dz0ny 5 hours agorootparentIndeed :)! It's not well thought feature. Also WordPress.com limits to 25 revisions, for exactly the same issue. https://wordpress.com/support/page-post-revisions/ reply dbg31415 17 hours agoprevWP Engine does backups differently… and rather than force you to copy and deploy complex histories to prod, which wastes time, they just snapshot the server nightly. You can roll back as needed. The way they let you copy the DB between staging and prod, honestly it’s faster and better for their users the way they do it vs. default WordPress. My 2 cents. reply tonymet 17 hours agoprevYou can’t license software as open source, with a liberal license, and then get angry when someone profits off of it. That is one of the many valid uses. If you don’t want commercial use, license your property with a limited license that meets your expectations. reply blendergeek 17 hours agoparentNot everything that is immoral will necessarily be illegal. We can't legislate morality. The WordPress.org folks seem to be taking a position that WP Engine's behavior is immoral. That doesn't mean that WordPress.org has some duty to attempt to use every governmental remedy at their disposal to prevent WP Engine from taking these (allegedly) immoral actions. Instead, the folks at WordPress.org are attempting to use public shaming and boycott to bring WP Engine in line. Do you believe that governmental mandates are the only acceptable solution to (this class of) immoral acts with software? reply bastawhiz 17 hours agorootparentIf what you're saying is indeed the case, that's some awful hypocrisy. WordPress.com (run by Mullenweg!) is not \"real WordPress\". It disables the installation of most plugins unless you pay them a boatload of money. Arguably, that's just as \"hacked up\" as what WP Engine is offering. Imagine going after a direct competitor for doing to your open source project the exact sort of thing that you commercially do to that same project. The idea that they're immune to the morality argument because they contribute to the OSS project more is ridiculous. reply echoangle 11 hours agorootparentprevThe post and your comment are missing the argument for why it’s immoral though. Is taking an open source server project and offering paid hosting based on it immoral? Is it only immoral because they changed a setting from the default? Would it immoral if I offered a paid hosted MySQL server? Would it be immoral if I changed the columns-per-table limit to 100? reply Kye 17 hours agorootparentprevThis isn't the WP.org folks. This is Matt Mullenweg. Whether this is representative of the open source project is uncertain. reply tonymet 3 hours agorootparentprevHis case for WP engine being immoral was even flimsier. reply mynameyeff 4 hours agoprevIn a sense, the aggressively FOSS approach of Wordpress opened the doors for this. reply racked 17 hours agoprevIt might not be WordPress, but then WordPress is often an inefficient, error-prone piece of dung anyway. WP Engine is the only painless WordPress hosting solution I've seen. It's fast and has a great backend panel for debugging. Hard to do all that yourself for that price, unless you're hosting WP in bulk. reply jcoletti 4 hours agoprevCurious why the post has two hyperlinks to the WP Engine domain, and additionally without rel=\"nofollow\", if they despise them so much. Isn't that WordPress essentially passing SEO link juice to WP Engine? reply rpgbr 5 hours agoprevUsing .org blog to attack an Automattic rival is unacceptable. Matt must resign from WP leadership and focus on his business — and post this nonsense over there. reply thr0waway001 13 hours agoprevWP Engine is so incredibly overpriced. reply ActualHacker 15 hours agoprevSays the company with a competing product reply bsder 17 hours agoprevSorry, but for all intents and purposes, WPEngine IS WordPress. Running WordPress is such a pile of security and customer support suck that nobody wants to deal with it. Consequently, if you are being made to run WordPress, you also want to pay somebody to make the pain go away. If I pay WPEngine, I can tell my marketing and design teams \"Customer support is over there. Talk to WPEngine and leave me alone.\" If WordPress made their software such that hosting administration wasn't such a fiasco, WPEngine would have viable competitors and wouldn't be able to extract the ecosystem. reply lightlyused 17 hours agoprevThey really thing WordPress is something special. It really isn't; there are better systems. I think Matt is just mad because he is not getting any revenue from wpengine. That being said, some commercial users of open-source software need to be better contributors to the eco-system and not just vampires. reply teddyh 18 hours agoprevIsn’t this what trademarks are for? reply realce 18 hours agoparentI don't think you read the article reply rgbrenner 18 hours agorootparentIf it was just about turning off a WP feature (revisions), I would agree with you... but the author also writes: What WP Engine gives you is not WordPress, it’s something that they’ve chopped up, hacked, butchered to look like WordPress, but actually they’re giving you a cheap knock-off and charging you more for it. Which is a more serious allegation, and trademark law would prevent them from calling it WordPress if they modified the software. It's pretty common in the open source community to insist forks use another name to prevent confusion. reply gumboshoes 16 hours agorootparentYou called out the most absurd passage in the post. My web-focused company uses WP Engine and I administrate it and let me tell you, it's WordPress. Completely WordPress. So some defaults are changed? Every other provider I've used does something similar. Matt hasn't mentioned the other excellent default WP Engine choices, if he wants more to complain about. Random PHP calls disabled by default. Must-use security drop-in plug-ins. High-risk and processor-intensive plug-ins disallowed. Regular plug-in vulnerability reports. It's an administrative layer of choices I appreciate as a web admin because I am just one guy. Also, I don't use revisions and neither should you if you have a large site. They balloon a database AND YOU SHOULD BE COMPOSING OFFLINE FIRST, anyway. Maybe I'm dating myself as a 35-year+ internet user but composing in a browser's text field is considered harmful. If you compose locally, you always have a backup and don't need revisions. reply cantSpellSober 17 hours agorootparentprevThe author's argument is that calling WP Engine's causing confusion. If so, wouldn't they be suing instead of blogging? reply bastawhiz 17 hours agorootparentBut it is WordPress, Mullenweg hasn't shown that it's anything other than stock WordPress with a setting turned off. reply erlend_sh 13 hours agoprev [–] That whole post boils down to Mark & co(s) not having the necessary protections for their product, so they’re left with nothing to do but cry “foul” as a leech non-consensually sucks value from their body of work. (A)GPL provides no protection here. Polyform NonCommercial however does protect against this, because WP Engine would have to pay for the privilege of re-sale. https://polyformproject.org/licenses/noncommercial/1.0.0/ See also the closely aligned Fair Source: https://fair.io reply echoangle 11 hours agoparent [–] > a leech non-consensually sucks value from their body of work I would argue that releasing your code as open source already is the consent, and you can’t really revoke the consent later (make the license more strict when you already released it). They might not like it now but I wouldn’t call it non-consensually. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WP Engine is a separate entity from WordPress, despite branding that may suggest otherwise, and profits from this confusion.",
      "WP Engine disables WordPress's revision system by default to save costs, which compromises user content integrity and goes against WordPress's data protection promise.",
      "Customers are advised to demand at least the 3 revisions WP Engine allows or consider alternative hosting providers to maintain higher standards within the WordPress ecosystem."
    ],
    "commentSummary": [
      "Matt Mullenweg, CEO of Automattic, criticized WP Engine for not contributing back to the open-source WordPress project despite having similar revenue to Automattic.",
      "A WP Engine employee claimed that management prevents contributions to WordPress due to KPI targets and was subsequently fired, escalating the issue.",
      "The controversy highlights tensions between open-source principles and profit-driven business practices, with WP Engine accused of profiting from WordPress without giving back to the community."
    ],
    "points": 213,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1726964185
  },
  {
    "id": 41614795,
    "title": "It is hard to recommend Google Cloud",
    "originLink": "https://ashishb.net/programming/google-cloud/",
    "originBody": "Home » Posts It is hard to recommend Google Cloud September 21, 2024 · 1 min Programming Google Domains A year back, I had to migrate my domain after Google decided to shut down Google Domains decided to shut down. I had to, not only, painfully setup multiple side-projects sub-domain mappings again on a new domain registrar but also re-verify my domain and re-create those mappings on Google Cloud Run. Google Container Registry Google Container Registry is shutting down in 2025. It has been replaced with a new project called Artifact Registry. So, why is Container Registry being shut down? Probably because it 10X cheaper than Artifact Registry. I just spent multiple hours migrating side-projects from GCR to GAR. It wasn’t easy. Some projects were deployed multiple years back and are working perfectly fine. Some other are single-page homepages. The two migrations have been laborious. While Google Cloud has made me jump hoops, there has been little upside in doing these migrations. Google Cloud - great engineering, good product, terrible strategy I strongly believe Google Cloud is a superior product to both AWS and Microsoft Azure. The UX is much simpler. The primitives are much nicer and evolved compared to AWS. The reliability while not as high as AWS is pretty close. However, the continuous churn that I have experienced has made it hard to recommend it.",
    "commentLink": "https://news.ycombinator.com/item?id=41614795",
    "commentBody": "It is hard to recommend Google Cloud (ashishb.net)205 points by todsacerdoti 13 hours agohidepastfavorite144 comments jpgvm 12 hours agoGoogle Domains shutdown was an own-goal. They should have just ran it at a loss forever if that is what it took, the optics were and are just awful. I get it, it was a \"consumer\" product essentially, hence selling the business to Squarespace instead of someone like Cloudflare. But anything related to DNS is going to make infra folk very wary of what else you might be willing to kill or neuter. It's just at that level of fundamental things that make operators skittish. reply jmillikin 10 hours agoparent> I get it, it was a \"consumer\" product essentially, hence selling the > business to Squarespace instead of someone like Cloudflare. Ironically, I moved all my Google Domains domains to Cloudflare. Their revenue from being a domain registrar is likely a rounding error compared to their other products, but (1) now they have my credit card on file for value-add services, and (2) sometimes people with corporate spending authority ask me for advice about who they should buy cloud services from. Grocery stores don't make their money from selling bread and milk, but a store that doesn't sell bread and milk is run by fools. reply hakanito 9 hours agorootparentThe obvious go-to choice was Cloudflare for us too, but then it turned out you can't use CF just as a registrar (at least on the basic plan or equivalent), you need to use Cloudflare's nameservers as well... But we use Google's Cloud DNS for everything, so that was a showstopper. In the end we went with AWS Route 53. CF could probably get a lot more customers if they would allow you to use custom nameservers for your domain. reply bartekrutkowski 9 hours agorootparentIf I'm not mistaken, it is possible, but not on the free tier. reply paulgb 8 hours agorootparentI don’t think it’s the case even on a paid tier, if it is I can’t find any reference to it. There are a lot of posts on their community support form where the recommendation is to use another registrar. https://community.cloudflare.com/t/use-cloudflare-registrar-... reply remram 4 hours agorootparentI do see an option for custom nameservers on my dashboard, followed by a button \"upgrade to business\". reply duskwuff 2 hours agorootparentThat option means \"your nameservers are now ns1/ns2 on your domain but they still map to Cloudflare\", not \"you get to pick your own third-party nameservers\". reply remram 8 minutes agorootparentI see, thanks. BehindTheMath 6 hours agorootparentprevWhy not Cloud Domains? reply deepsun 6 hours agorootparentyou mean google cloud domains? They shut it down together with google domains (it still runs, but you cannot register a new domain). reply re-thc 8 hours agorootparentprev> CF could probably get a lot more customers if they would allow you to use custom nameservers for your domain. Why would they want that? The whole point of CF as a registrar was so you could use the other services. The registrar is sold at cost. It's a way to lock you in. reply hakanito 49 minutes agorootparentIt would lower the barrier to switch to Cloudflare for new customers, and once they are inside with a credit card on file it’s arguably easier to explore their product offerings reply homebrewer 9 hours agorootparentprevThey shouldn't have any revenue from registering domains because they sell them at cost. It's probably the opposite because of all the overhead (which is of course compensated by pulling in more clients as you explained). reply ptman 4 hours agorootparentRevenue is not profit reply ac50hz 10 hours agorootparentprevGreat analogy. reply Kwpolska 11 hours agoparentprevGoogle Domains was a service where no new features could be added, so nobody would get promoted for working on it, so nobody wanted to maintain it. Classic Google. reply chromakode 4 hours agoparentprevMigrating out of this mess caused me downtime. Squarespace's domain panel crashed with a nondescript error when I tried to update nameservers prior to transferring out, and they shut off the Google nameservers as soon as the transfer went through on their side. To add insult to injury, Squarespace makes you wait 5 days for a transfer, with no way to expedite -- and in my case, they waited 6 days, taking me offline on a Friday night. This was the worst experience I've ever had using a domain service. reply oldnewthing 3 hours agoparentprevI believe the reason for divesting from the registrar business had to do more with ICANN rules that require Google not both be a registry operator and a registrar. Google does have a large registry business with 60+ TLDs so they could not also be a registrar (even if not for the same TLDs). It wasn’t really a money thing AFAIK. reply duskwuff 2 hours agorootparentGoogle already had an arms-length subsidiary (Charleston Road Registry) to take care of that requirement. The Google Domains shutdown came much later. reply googlewillgoog 8 hours agoparentprevThe funny thing is it wasn't even a true shutdown - Cloud Domains continues to be a reasonable registrar with proper IAM management so actually much nicer than Google Domains for larger projects. There were plenty of docs on migrating which was a one command thing (don't think there was a UI). Non-\"consumers\" had a reasonable out, which they could then drop for another provider if they felt a need, but there was definitely no requirement to end up at Squarespace. But they never marketed it, among all the backlash over turning down Google Domains there was never any true call-out to this. To this day I suspect many people don't know about Cloud Domains (which still exists and accepts new domains). I can't fathom any user-related reason for this - the contract in selling to Squarespace must have forced Google to be unable to properly market Cloud Domains as a transition option. If not, the Google Domains PM truly didn't know the existence of Cloud Domains? reply paulgb 8 hours agorootparentI was also surprised Google didn’t really mention the Cloud Domains path. I did it, but there are some caveats. It doesn’t support all domains that Google Domains did (e.g. “premium” domains on new tlds), and Squarespace does still end up being the registrar of record; Google Cloud is just a frontend. reply MattGaiser 11 hours agoparentprevShouldn’t have been much of a loss. Tons and tons and tons of domain registrars exist, so it’s not an inherently expensive business. reply wolpoli 10 hours agorootparentWhile it's likely profitable, it's probably not generating AdSense level margin/growth. reply underdeserver 13 hours agoprevGoogle's problem has always been customer support. Not just the process but the idea that your customers deserve to be supported when using your product. You shouldn't ask people to migrate before having a documented, tested, tool-assisted, clear and intuitive migration flow that takes very little time. reply notepad0x90 11 hours agoparentI would say that Googlers are painfully aware of this. But the real problem is leadership. It starts with the CEO and various execs. Their employees are still living in the \"growth hacking\" and highly academic world (at least their engineers) and their execs are drunken with arrogance, convinced their Titanic will never sink. And they may be partially right, it may not sink but it will never regain its former glory. It will rot on the shelf, besides IBM,AT&T, HP and many more. At its core, alphabet is a services company, it does not sell goods (to the most part). Their board needs to decide if the company should focus on business services or consumer services and replace the executives with people who have experience in that area. The actual quality of GCP is superb in my opinion. They have a flair for architecting excellent solutions. I prefer GCP over any cloud from a purely technical perspective. Their leaders just don't get that it isn't enough. They're not operating a toy or a museum of technical marvels. Actual people need to use it. reply underdeserver 9 hours agorootparentI worked on GCP, left Google about two years ago. Some Googlers are aware of this, fewer care, and fewer still act on it - you sort of have to be an insane employee* to do that. I saw tickets from high-paying, top-10 customers, go unanswered for days; no one senior enough on the team to answer felt it was more important than the day-to-day, and no one from the support/account executive staff felt they had the authority to demand it. I see it otherwise. I think solving customer support is crucial to GCP's success, and since I agree with you that GCP is the better underlying product, there is a deluge of money passing Google by, just waiting for the right executive team to start caring about this to pick it up. Kind of like Microsoft pre-Satya. * https://yosefk.com/blog/people-can-read-their-managers-mind.... reply pm90 7 hours agorootparentThis is really sad to read. I agree with you that GCP has the superior product. Azure is a fucking joke. AWS sucks but they do respond to feedback and will even get engineers on a call with you if you hit certain edge cases. The level of apathy you describe will very quickly sour users. reply underdeserver 6 hours agorootparentIt does. After leaving Google I started working at a startup that runs its production environment on AWS. We debugged an issue and experienced exactly what you're describing: our MSK cluster refused connections. Eventually that was solved by rebooting something on their side. It took less than 24 hours from when we opened a ticket until the issue was resolved, with a few back-and-forth messages between us and the AWS support engineer. reply yjftsjthsd-h 12 hours agoparentprevAllow me to drop this classic: https://steve-yegge.medium.com/dear-google-cloud-your-deprec... (and https://news.ycombinator.com/item?id=24165445 and https://news.ycombinator.com/item?id=38023392 ). Because yeah, \"support\" includes \"are you going to keep making me do work just to keep using your platform\". (Edit: To be clear, that's negative support, but it's on the same axis) reply initplus 11 hours agoparentprevI'm also completely sick of fielding API migration emails from Google on mobile Android/Firebase. Feel like I get some \"action required\" every couple of weeks. This stuff saps our resources, both to fix it, and to diagnose if the issue even applies to us in the first place. If you are lucky Google includes details of the app using the API, but often this part is even left out. reply jdance 11 hours agorootparentYeah the amount of disrespect of developer time they show here is crazy. You could have made the billing api backwards compatible for 10 years, its a super small api, but instead they force breaking changes every couple of years Maybe it is a strategy for cleaning up old apps or something, but I doubt it reply yawnxyz 12 hours agoparentprevAt least with Cloudflare they'll get back to you in five minutes as soon as you complain on HN reply peterbmarks 12 hours agoprevNever forget what happened to Australian UniSuper when google accidentally deleted their account. https://www.theguardian.com/australia-news/article/2024/may/... \"More than half a million UniSuper fund members went a week with no access to their superannuation accounts after a “one-of-a-kind” Google Cloud “misconfiguration” led to the financial services provider’s private cloud account being deleted, Google and UniSuper have revealed.\" reply politelemon 11 hours agoparentMozilla had an outage in 2022 when gcp deployed an unannounced change. https://hacks.mozilla.org/2022/02/retrospective-and-technica... reply slyall 8 hours agoparentprevHere is the official Google writeup of the UniSuper incident. A lot more details than a news media summary. https://cloud.google.com/blog/products/infrastructure/detail... reply re-thc 9 hours agoparentprev> Never forget what happened to Australian UniSuper I don't know if media or the readers are at fault. The article doesn't even make sense. > More than half a million UniSuper fund members went a week with no access If Google really caused such a huge loss there would be no joint statement. The buyer i.e. UniSuper would be trying to sue them. The fact that it is a joint statement implies the two parties are sharing the responsibility. Now complaining about UniSuper is boring and so spinning it on Google Cloud gets clicks. reply bn-l 9 hours agorootparentThe word at the time was that they were heavily compensated by goog (I guess the company was lucky their issue got some much media attention). reply siquick 8 hours agorootparentprevI’m a customer of Unisuper - the daily update emails definitely pinned the blame on the service provider. reply throwawaygoog31 10 hours agoparentprevThis is completely false. TA account because I am not authorized to speak on this, but that’s not at all what happened. reply paulgb 8 hours agorootparentThe official statement from google: “During the initial deployment of a Google Cloud VMware Engine (GCVE) Private Cloud for the customer using an internal tool, there was an inadvertent misconfiguration of the GCVE service by Google operators due to leaving a parameter blank. This had the unintended and then unknown consequence of defaulting the customer’s GCVE Private Cloud to a fixed term, with automatic deletion at the end of that period.” https://cloud.google.com/blog/products/infrastructure/detail... Are you saying that Google lied about being responsible for it? What would they possibly gain from that? reply re-thc 8 hours agorootparent> Are you saying that Google lied about being responsible for it? The original news article says Google deleted the account. As per your quote and official statement - no account has been deleted. So that news article is completely false. Point proven. reply nielsole 9 hours agorootparentprevI think the latest public information was that it was a VMware \"private cloud\", a set of VMs, in GCP which accidentally had an expiration date set. So no GCP projects/billing accounts were ever deleted. Is that accurate? reply tjpnz 9 hours agorootparentprevWell go on then, tell us more. reply smitty1e 9 hours agorootparentprevWe should always maintain a healthy skepticism in either direction, and maybe supporting that is your purpose, but: why should your throwaway account be believed? reply jodiug 11 hours agoprevI'm surprised to find all comments so strongly against Google. I do not like their customer support nor their surprise product closure acts... but I do find their cloud interface easy to use, pricing is solid, and (K8s/Cloud Run) I have barely been affected by any significant outage in some 8 years of use. Contrast to Azure who despite their SLA had significant outages every few months that were very noticeable, sometimes even requiring us setting up an entire new K8s project from scratch. AWS is better in terms of reliability. Their UX is not great though, I often feel like I'm a wizard waving my wand with permissions and connecting things like audit trails. Many actions are a bit delayed in their effect. I've also had to contact support at times to raise arbitrary limits in the platform. In balance I would recommend GCP. Their product closures have not affected me enough to scare me away. I guess experiences vary depending on which product we are using. reply zacmps 11 hours agoparentIt's been brought up elsewhere in the thread but GCP deleting an entire tenant's account rules them out entirely from any serious business IMO. reply rvnx 5 hours agorootparentThe Gemini team that completely fucked up recent deploy by blocking queries originating from Google Cloud Functions is a masterpiece as well. Full downtime for 2 days, no apologizes, no answers, had to pay 3% extra for support, no credits. Special award to the dude representing Google trolling on Twitter that they will do another \"wild Friday release\". Shitty. reply 8490109481 1 hour agoparentprevMy impression from a small project needing a OAuth scope was that the UI of GCP was sluggish, and although the other providers have sometimes sluggish UIs Google's was uniquely so. Anyone else feel this way? Example is a sidebar that opens for adding an email during OAuth flow, after adding it clicking \"Add\" once did nothing, there was no feedback. Had to click it at least 5 times for it to go away and save correctly. In fact this is even specified in the documentation for the third-party tool (Google Drive downloader[1] step 18) that you have to click it multiple times. I don't think this is normal. And also I should mention, depending on hardware specs the GCP portal was nearly unusable. So hopefully one will not need to rely on creating OAuth resources for a CLI program on the same computer. Although to be fair, I wasn't benchmarking against AWS/Azure on that hardware (since I needed to use GCP Google APIs). [1] https://github.com/glotlabs/gdrive/blob/main/docs/create_goo... reply sunaookami 10 hours agoparentprevYou can't even set spending limits on Google Cloud and have to use some obscure script that deactivates billing if you want to imitate it. The whole product is incredibly buggy and complicated. reply ElFitz 9 hours agorootparentDo any cloud providers have this? AWS has slightly improved with their \"budgets\" and \"budget actions\", but it’s far from something I’d call a \"spending limit\". reply benterix 9 hours agorootparent> AWS has slightly improved with their \"budgets\" and \"budget actions\", but it’s far from something I’d call a \"spending limit\". Yeah they had to do something so they came up with a half-solution that basically informs you with a delay, doesn't prevent any quick damage and - if you decide to use budget actions - requires you to know what is actually causing the problem. Better than nothing I guess. reply denysvitali 9 hours agorootparentprevI seem to recall Azure had a similar issue until recently. Pay-as-you-go accounts weren't able to set budgets. I wanted to link the specific article mentioning that, but it seems like this works now :) reply benterix 9 hours agorootparentWait, so you can now set up a hard cap on your spending and they guarantee they won't charge you more? It would be exciting but I don't believe they actually allow that especially as their competition also refused to implement that, in spite of it being no #1 requested feature for many years. reply denysvitali 7 hours agorootparentBy the looks of it (their updated documentation) it seems like they do support this now! I haven't tried it myself, and I hope it's true (: reply jcrben 6 hours agorootparentWhere did you see this documentation? reply denysvitali 6 hours agorootparentOkay, my bad, I was looking at this page [1] which says that the \"Cost Management\" feature is available for Pay-as-you-go subscriptions, but [2] is pretty clear on the fact that Spending Limits aren't available for Pay-as-you-go subscriptions. So, TL;DR: Azure still doesn't allow you to set spending limits on Pay-as-you-go subscriptions [1]: https://learn.microsoft.com/en-us/azure/cost-management-bill... [2]: https://azure.microsoft.com/en-gb/support/legal/offer-detail... reply JackSlateur 3 hours agoparentprevAWS is the best provider for tech people, the product is high level but versatile too. Price is somehow high, but the service is here. azure is microsoft, so this explains that gcp has \"a vision\": if you do not share it, it's a pain. Also, gcp has some sick design for core products : checkout load balancers if you want a \"good\" laugh. It's a stack of hack, put one on top of the others I would not recommand against GCP, it's the average player: not the best, definitely not the worst. reply jpambrun 2 hours agorootparent\"AWS is the best provider for tech people\". This is an option, not a fact. Many of us don't agree. After working on all 3 I feel AWS is, by far, the worst experience for tech people. My perspective is that it's only doing great for execs and PowerPoint engineers. reply JackSlateur 1 hour agorootparentBy \"tech people\", I mean \"people who enjoy tinkering\", like playing legos In opposition with people who enjoy taking a product of the shelf and call it a day But you are right: this is my opinion, not a fact reply chii 11 hours agoparentprevi would say that smaller, independent enterprises/solo shops tend to not hate GCP. Large shops - aka, a shop where you're not part of the controlling entity - tend to prefer either AWS, or Azure depending on how hard the CIO got wined and dined previously (and existing inertia - older shops that used AWS as an early adopter seems to stay with AWS). > Their UX is not great though i agree, but nobody really cares enough. On the other hand, i'd not pay more for better UX. Judging by the current state, i say a lot of customers fall into this category. reply horsebridge 7 hours agorootparentI'm part of a smaller shop. GCP didn't even bother responding to us when we tried to contact sales, which is why we're at AWS. reply politelemon 10 hours agorootparentprevAssuming AWS/Azure were chosen due to thinly veiled bribery is unnecessarily reductive and in bad faith. reply makeitdouble 10 hours agorootparentAs a matter of fact Google Cloud will also wine and dine a CTO as needed. That's part of the sales process IMHO. A decision maker can basically choose from where they want to get bribed, so it won't be a crucial factor most of the time. reply benterix 8 hours agorootparentprevIn quite a few places I saw, it would be either that or sheer incompetence (which was obvious to me at the time, and become obvious to people who came after me and finally decided to migrate away, with more losses on the way that could have been easily avoided). reply re-thc 10 hours agorootparentprevThere's been quite a few documented instances of large scale bribes by Azure. reply jsemrau 12 hours agoprevA couple of months I was evaluating Dialogueflow for a chatgpt like ReAct agent I was working on. They have this blue \"contact sales\" button where you can chat with the sales team. Easy enough I thought, jumping right into it to ask my questions. I am greeted by a virtual guide that doesn't help. Then I ask for a human \"Live chat with sales\". Again another bot. How did I find out? The bot didn't bat an eye when I informed them I have 6 Billion customers on Mars, Venus, and Pluto. I ended up using streamlit. reply rurban 12 hours agoparent> Again another bot. How did I find out? The bot didn't bat an eye when I informed them I have 6 Billion customers on Mars, Venus, and Pluto. That reminds me in their interview process with me. So you are hinting they also use bots for hiring developers. Would explain everything. reply jsemrau 12 hours agorootparentWouldn't surprise me. reply postsantum 12 hours agoprevGoogle lost the rest of my trust with the ongoing fiasco with google play dev verification where many developers are not able to confirm their phone numbers for months and are at risk of losing their accounts. I can totally see myself being banned from GC for some nebulous reason or being locked out of account with no recourse. Their support bots are useless reply bn-l 9 hours agoparentThat worries me in general about Google. You will trip some wire in some garbage code somewhere and that’s it, you’re locked out and banned for life with effectively no recourse and no hope. reply godisdad 17 minutes agoprevShoutout to the GCP apologists: your faith in the face of overwhelming evidence contradicting the stability of the offering will be remembered heroically by history reply joshdavham 12 hours agoprevIt’s true. Two weeks ago, I did a bit of light consulting for a data engineering project involving an IoT device and half way through designing the GCP architecture of the pipeline, I realized that Google was shutting down their IoT service… I ended up having to recommend AWS. reply richardw 12 hours agoprevMatthew 7:27 “And everyone who hears these words of mine and does not do them will be like a foolish man who built his house on Google’s ever-shifting infrastructure.” reply iTokio 12 hours agoprevThey definitely lost my trust with Google Domains, somehow I thought that such a foundational infrastructure layer will not be axed as easily, oh boy was I wrong.. reply joshdavham 12 hours agoparentThat also took me for surprise. You’d figure the company that indexes the web could at least maintain a project like that. reply devsda 11 hours agorootparentGoogle registry owns like 40 odd gTLDs. A company that maintains that many TLDs is expected to have long term commitment to the domain related businesses but it turns out to be just a fleeting interest. reply ErneX 12 hours agoparentprevSame here and I even unsubscribed from Google One after I moved my domains. reply ocular-rockular 6 hours agorootparentWhat cloud storage solution do you use instead? That's been one of my problems for things like photos and documents so I've just stuck with Google One. reply bob1029 10 hours agoprevAWS has been feeling like the least terrible of the big 3 lately. Domain registration is the canary for me. Only AWS does it right. Azure has some approach but they force you into a lock in service and rely on GoDaddy for the actual registration. These vendors underestimate the impact of having something so fundamental managed by a weird 3rd party. Route53 gives me a lot of confidence that I'm not going to be jerked around with domains and DNS. This factors heavily into my purchasing decisions. reply remram 4 hours agoparentI use the free tiers of Google Cloud and Oracle Cloud, and pay AWS for everything important (that I can't run bare-metal). Azure I can't use at all. Impossible to find the products I want in the UI. reply chews 13 hours agoprevAfter what they did with domains, it's hard to recommend them for anything to do with cloud. reply jclulow 13 hours agoparentThey also just totally fucked anybody making use of the Google Photos API to access their own data: https://news.ycombinator.com/item?id=41604241 They're a trash company, and anything you get from them, even when you pay for the storage, is at best accidentally delivered to you and they could roll over in their sleep at any moment and snuff it all out. reply yawnxyz 12 hours agorootparentdon't forget the time everyone's Google Drive photos got corrupted reply initplus 11 hours agorootparentprevEven grosser is that they explicitly limited the iOS version of Google Photos to refuse to work with scoped access. So on iOS the app will refuse to work without being granted whole library access, a permission that Google no longer allows to any third party photo app on Android. Blatantly anticompetitive. reply darby_nine 12 hours agoprevIt's clear google wants to be a dumb pipe rather than a business that's responsive to end-user needs. Unfortunately, they're also notoriously unreliable as a dumb pipe. reply postsantum 11 hours agoparentTheir end-users are advertisizers spending millions. For them Google is probably very responsive. The rest of us are users of pet projects at best and product at worst reply darby_nine 11 hours agorootparentThe term end-user does not refer to advertisers by definition. Typically most people use the term \"user\" to refer to humans. You're not wrong, but advertisers are a cancer on society that not only do not contribute any value but actively destroy the world around us. it's difficult to assign anything but deeply negative value to their needs and concerns. reply sireat 2 hours agoprevRant time: Why does anyone use GCP? I had not used GCP for a few years and recently tried to get a simple webapp scaffolding up. So I see the myriad of services offered by GCP, I pick: https://console.cloud.google.com/products/solutions/details/... Dynamic web application with Python and JavaScript. Okay, I've built these things before, let's see the Google way. Nice, there is a diagram, there's Firebase, there's Django, there's PostgreSQL, something called cloud storage. A bit of an overkill, but let's roll with it. Okay, let's click Deploy. This should give us some sort of ready to use stub, placeholder, template right? Wrong. Resources tab shows 65 actions/resources created. All nice green checkmarks, there is IAM, there Firebase, there is Cloud Storage, Secret Manager, and everything else under the sun. 65 green checkmarks - that is good right? However the app itself is not ready to be shown to the world! Apparently there is something missing in Firebase config (remember this is the default deploy for a new user): Google helpfully informs me Why am I seeing this? There are a few potential reasons: You haven't deployed an app yet. You may have deployed an empty directory. This is a custom domain, but we haven't finished setting it up yet. How can I deploy my first app? Refer to our hosting documentation to get started. So I go hosting documentation link and that is just generic: https://firebase.google.com/docs/hosting/ Meanwhile all this nice setup is costing about $2.40 every 24 hours. The issue isn't the complexity of the solution, or the lack or over abundance of documentation. The issue is that their sample \"Hello World\" app is not ready. I should not need to go through stacks of extra documentation to find out what is wrong when trying a sample product of some complexity. Why should I be forced to fight abstraction leaks immediately when starting to use GCP? reply edem 32 minutes agoparentI never used GCP NGL. Google always rubbed me the wrong way and I ended up migrating away even from gmail reply unkoman 7 hours agoprevIn 2019 when working for a cloud consultancy they paid us to certify for Google cloud with the promise of many new customers. In reality since then the consultancy fired all pure Google cloud consultants. I myself have had about three Google cloud only focused projects. A lot of promises but nothing came out of it. It doesn't help that the platform has stagnated and customers are afraid of committing due to loss of features. In comparison AWS doesn't remove features for customers or always provide alternative ways to migrate to. SimpleDb is the prime example here. reply abrbhat 9 hours agoprevGoogle just does not have enterprise DNA which requires providing long-term support for legacy systems. AWS on the other hand was able to achieve this through their customer obsession. But the absolute king in this still remains Microsoft which is why enterprises will adopt Azure with their eyes closed. reply more_corn 55 minutes agoprevI wrestled with GCP a month ago. Never again. Hundreds of internal permissions issues, a thing Google invented for no reason. APIS needing to be turned on. A feature I didn’t ask for and don’t want. Service accounts that don’t have permissions to do what those service accounts are for. I used to think any cloud is as good as another. Not anymore. It’s not hard to recommend Google cloud. It’s impossible. reply pvtmert 13 hours agoprevPS: There is a double/extra words on the first paragraph. > \"had to migrate my domain after Google decided to shut down Google Domains decided to shut down.\" reply asah 10 hours agoprevFascinating to see all this hate, and at the same time Google is reporting terrific growth and profitability for GCP. Any idea how? Large customers? https://www.google.com/search?q=google+cloud+profitable reply re-thc 9 hours agoparent> Fascinating to see all this hate Is this \"hate\" coming from the actual buyers though? Often the 1s commenting are the actual \"users\" just not the 1 that's paying the bill. reply aabhay 10 hours agoparentprevDiscounts reply Gud 3 hours agoprevDon’t rely on large multi domain mega corporations for basic infrastructure. They can yank it away from you in no time and they probably will. reply emadda 9 hours agoprevThe pricing on some services can also be surprising. A cloud load balancer is $20/month before you even handle traffic. I think they are introducing a $1.50 charge for every uptime check, when they were initially free. reply xyproto 12 hours agoprevI wonder if Cloud Run or Kubernetes is the safest bet for the future, when using Google Cloud. reply joshdavham 12 hours agoparentFingers crossed. I love Cloud Run. reply icedchai 3 hours agorootparentSame. Cloud Run is a really nice product. It makes deploying containers a breeze and simpler than AWS's equivalent service (ECS / Fargate.) I also found GCP's developer tools and docs to be quite solid, better than AWS. Example: It's nice that they provide Terraform snippets for most of their resources. reply cpa 12 hours agorootparentprevIt’s good but extremely expensive. I opted for a custom solution, and it’s now costing me a tenth of the price. reply xyproto 11 hours agorootparentInteresting. Did you buy and setup your own servers, use a different company to host services in the cloud, or something in between? reply cpa 9 hours agorootparentI just rented servers on OVH instead. I mostly do CPU intensive and highly parallel computations, and went from 3500€ to 300€ for my usage. As a startup I often put simplicity and ease of use before pricing but in this case that was too much of a difference! reply throwaway343233 11 hours agoprevEh. I wouldn’t use GCP precisely for this reason if I was building something on cloud. Customer obsession (ugh) is just not in their DNA. I can tell firsthand they care more about what they want to do than what the customers want. reply patrickhogan1 12 hours agoprevI completely agree. The CLI tools are fantastic, and the logging tools are much more user-friendly than AWS. Plus, the console manager is way better than Azure’s clutter. But the decision about domains was a huge misstep. It used to be so easy to buy a domain from Google and get a server running quickly. Why make the user experience so frustrating right from the start? reply siscia 10 hours agoprevThe article mentions that some primitive of GCP are nicer (than AWS?). I wonder if you guys have any example of those nicer primitives. reply arianvanp 9 hours agoparentI find IAM a bit easier in google. I am policies attach to resources instead of principals. And resources are hierarchical. Basically as if you had S3 bucket policies for everything. Though I'm sure you'll find someone saying the exact opposite is more useful reply cess11 9 hours agorootparentI agree. This is the one thing I find easier on GCP than AWS, taking total control over a GCP environment I'm about to enter is much faster and less confusing. It's irritating that I need old accounts to do exports but usually I can reset credentials on some old ones and find one with MFA I can bypass. But it's still a rather nasty environment to handle, as is all the big clowns. There's lock-in, confusing products, lots of general weirdness. Unless the corporation I'm taking control over has exercised discipline and restraint and stuck to a rig with simple EC2 instances every project makes me hate them more. As far as I can tell the way to go if you enjoy this kind of operating environment is to buy on-prem OpenShift. The guys I've met that run that seem to think it's the lesser evil. reply andrewstuart 9 hours agoprevGo get yourself an IONOS VPS $50/month. 1GBps internet connection 12 vCores CPU 24 GB RAM 640 GB NVMe SSD Unlimited outbound data. reply themoonisachees 8 hours agoparentMost people paying for GCP are in need of much more than a VPS and also are quite probably not the decision makers reply ehaikawaii 12 hours agoprevThis is why everything Google feels evolved, nothing lasts long enough to be obvious legacy bullshit. Cut backs have finally forced AWS to sunset services, but they just hard refused for many, many years. People like using it because the workflows evolve, but AWS will not force migrations on you, sometimes to your own detriment. When you first build an app, everything is fresh because developers haven't had to hamfistedly shove assumption breaking models and patterns into their program yet. Cloud customers want to evolve to the latest tech, but redoing 20 years of the company for no business reason makes programmers happy but executives nervous. reply srockets 12 hours agoparentAWS used to just bake keeping the light on into the cost of launching a product. They wouldn’t launch something they weren’t ready to support until the sun becomes a supernova. Sadly, that seems to not be the case anymore, and it’s a worrying development. AWS giving up on their customer obsession makes it easier for the other CSPs, who aren’t as customer friendly, to compete. reply throwaway290 12 hours agorootparentWhat did AWS sunset? I probably didn't use any of what they killed so that's why unaware, would be good to know reply gregw2 8 hours agorootparentSimpleDB, S3 Select, old style EC2s outside VPCs, QLDB, AWS Data Pipelines, Cloud9, CodeCommit, SnowMobile, Forecast are the ones I know. A few more can be found at: https://github.com/SummitRoute/aws_breaking_changes https://www.thestack.technology/aws-deprecations-services-co... From the outside I dont perceive the promotion dynamics for engineers working on old services is that different at AWS than Google or Azure. Finance and engineering do conspire to kill off things despite customer annoyance, perhaps as they should. There is also a constant churn of retiring EC2 instance types, security certs, old database versions, EMR versions, OS versions, security practices, cost optimizations, etc to suck up your time with minimal business value. This is not different fundamentally than on prem, you just have slightly less control and an outside party forcing your hand to do the right thing isnt purely a bad thing... AWS is pretty good about giving you a grace period to migrate off and informal warnings if they strategically want you to move to a different service of theirs before stuff gets killed if you are looped in with a TAM (e.g Data Pipelines vs more-expensive Glue). They seem to have recently migrated to a strategy where they disable services for new customers but actually dont kill them completely off but keep them on life support. reply JackSlateur 2 hours agorootparentYes The only product I know well in your list is pre-VPC ec2 instances, which (to be fair) are a terrible product : it is cross tenant (as in : you can impact other customers). Good riddance. I think many AWS services are well designed : isolated software-only components. They build a lot on top of a very stable infrastructure (VPC / s3 / ec2 / IAM), which means supporting a service is really cheap : they are just a couple of containers running somewhere reply ocrow 12 hours agorootparentprevFor one, the OpsWorks configuration management tool is being replaced with Management Console, which is more encompassing, but different. You have to manually migrate or rebuild any deployed resources from one to the other. reply captn3m0 11 hours agorootparentprevQLDB as well. reply merb 12 hours agoprevGoogle domains was not integrated into google cloud. And artifact registry is the better product and you had like 3 years time to migrate , which was a simple button press and changing the urls of the images. Of course it took some work, but it is not more expensive, effectively it’s cheaper since you can more easily clean up old images reply Kwpolska 11 hours agoparent> and changing the URLs of the images The URLs might be mentioned in many different places. Why break URLs just to make a \"better product\"? Why couldn't the new product keep old URLs working? reply jeffrallen 10 hours agoprevCome over to the second tier side... Smaller cloud providers cannot (and do not want to) afford the costs of constant product churn. So you get a smoother, long term relationship with a smaller company that's more likely aligned with your goals. Disclosure: I work for a European second tier cloud provider. reply KeepOnTruckin1 11 hours agoprevI remember once consulting one of my client who used GCP and to get SSH to one of the instances where pain in ass, I spent couple of hours on that. If I recall correctly they do have concept of users and then user gets SSH key and you can't just put SSH key on instance out of the box as on any other VPS... reply icedchai 3 hours agoparentThey make it pretty easy to SSH in through both the console and with the gcloud command. If you set it up correctly, Linux user accounts are tied to GCP IAM accounts and are automatically provisioned. It's actually super slick! Maybe your client didn't want to do that. reply fsflover 10 hours agoprevA few Google-support-related threads on HN: https://news.ycombinator.com/item?id=32547912 https://news.ycombinator.com/item?id=33737577 https://news.ycombinator.com/item?id=35133917 https://news.ycombinator.com/item?id=22146082 https://news.ycombinator.com/item?id=22705122 https://news.ycombinator.com/item?id=4013799 reply cornflake23 11 hours agoprevQuite frankly, it’s hard to recommend any Cloud Service nowadays. Technical issues apart, the strategy _everywhere_ is to maximize picking the customers’ pockets. AWS is super expensive, Azure is super unreliable and GCP … well, it’s reliable yet features on it could vanish at a moments notice. Where might one go? reply JackSlateur 2 hours agoparentFor every project you want to choose a cloud provider or whatever, you have to compute \"time to production + cost of people + cost of provider\" When your needs are low, AWS and friends are very competitive: quicker deploy, no CAPEX, lower OPEX If your needs are high, you should indeed build your stuff on top of instances If your needs are higher, then rent rack space and build on top of baremetal If your needs are even higher, then build your datacenter As an example, I put my personal backups in the \"cloud\". Is 1€/mo for 100GB cheaper than using a VPS ? Hell yeah ! reply surgical_fire 9 hours agoparentprev> Where might one go? Most should just own their infrastructure, or use a more simple hosting service. Most companies could go very far with that. Unless youf business is a global one with hundreds of millions of users in many jurisdictions, there is really no reason to go for cloud services. They tend to be much more expensive then the alternative, and have many ways to lock you in by making migration very expensive and time consuming. reply nvarsj 8 hours agoparentprevFor a medium size biz, I'd probably still recommend using GKE or EKS. A smallish EKS cluster of a few nodes will costthere has been little upside in doing these migrations. Maybe the upside is that Google is in exchange able to offer: > Google Cloud - great engineering, good product. reply edem 10 hours agoprevthe main problem with Google is that they can delete your account... in fact automation will do nasty stuff to you and you can't reach a human. this makes their products a hard no. in fact i spent weeks migrating away even from gmail and now i have my own hosted domain reply eastbound 12 hours agoprevMany here say they won’t use Google for anything cloud, but hoe many of you have Gmail for Workplace at work? Probably everyone. It’s the best solution for work emails, docs, calendars, etc. reply zinekeller 12 hours agoparent> It’s the best solution It's the least worst solution for \"whole-workplace\" systems, but most people who are saying \"won’t use Google for anything cloud\" probably self-host or use something like PurelyMail (https://purelymail.com/) or FastMail (https://www.fastmail.com/) for their own emails reply Dalewyn 12 hours agorootparentI use my Hotmail for all my important emails, my Gmail is my catch-all for everything else. Why Hotmail? Because if the name isn't enough indication it's because I've had it for well over 25 years at this point and I like Microsoft anyway. reply throwaway984393 12 hours agoparentprevOffice365 is way better. You get native apps, which are obviously better than a web app, you get device integrations for enterprise, you get more advanced tooling, more features. Plus MS has better compliance options. If you're actually running a business, and not just a startup employee who doesn't need to do much, MS is the way. reply ryanjshaw 11 hours agorootparentNot sure why you got downvoted to dead. In big companies, M365 is a way more common and complete solution compared to anything else out there. This is just a fact - I actually really like e.g. Google Sheets, but the reality is that M365 can integrate with your Microsoft stack top-to-bottom and has low-code app, automation and database services that allow you to build out a good chunk of common LOB apps with minimal complexity (not that Dataverse doesn't have some ridiculous issues). reply n_ary 11 hours agorootparentIn my corner of EU, Office365 is the only thing I see everywhere(also in some government offices). I have yet to see/use GSuite anywhere yet. Also, M$-Azure is common on more ancient businesses, modern startups sometimes have AWS and hip startup in rare cases has GCP(when product does not involve personal data). reply ryanjshaw 5 hours agorootparentExactly. Whoever is down voting here is denying reality. reply JackSlateur 1 hour agorootparentYour reality, perhaps In the last 4 compagnies I worked for (the multi-billions kind of compagnies), everybody uses workspace, there are no office 365 reply ryanjshaw 1 hour agorootparentWeird - were they IT companies maybe? reply JackSlateur 1 hour agorootparentNot at all: television, retail, mecanics, and another retail reply ryanjshaw 1 hour agorootparentSurprising - as with the earlier commenter in EU, here in South Africa, including the multinationals spanning the continent, everything is Teams, including the government. reply eastbound 1 hour agorootparentprevSorry I’m the one who thought Google Workplace was the best, I’m learning something. - Does Chrome keep nagging you for login and bookmark sync, when you use O365? - Google’s way of handling shared inboxes is awful (Want an extra email? Create a …Google Group! Then assign members, tune the perms, etc.) How does it work in O365? Can I just create an email and assign 7 users on it? reply acje 11 hours agoprev [–] I generally view churn and nice primitives to be the essential balancing act for a platform. Perhaps any leading edge (software)system. We aren’t going to get anything complex perfect the first time, so it is change or mediocrity. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Cloud is undergoing significant changes, including the shutdown of Google Container Registry (GCR) in 2025, to be replaced by the more expensive Artifact Registry (GAR).",
      "Users are experiencing difficulties and time-consuming processes in migrating their projects from GCR to GAR, adding to the frustration.",
      "Despite Google Cloud's superior engineering and user experience, frequent changes and disruptions make it challenging to recommend compared to AWS and Microsoft Azure."
    ],
    "commentSummary": [
      "Google Cloud's history of shutting down services, such as Google Domains, has made users cautious and led many to switch to alternatives like Cloudflare or AWS Route 53.",
      "Frequent API changes and poor customer support are significant issues that contribute to Google's perceived unreliability, despite having superior technical solutions.",
      "AWS and Azure are often preferred over Google Cloud due to their stability and better customer focus, despite their own flaws."
    ],
    "points": 205,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1726982488
  },
  {
    "id": 41611965,
    "title": "Infineon's CO2 Sensor Monitors Indoor Air Quality",
    "originLink": "https://www.allaboutcircuits.com/news/infineons-co2-sensor-precisely-monitors-indoor-air-quality/",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.allaboutcircuits.com\",cType: 'managed',cNounce: '64036',cRay: '8c748ba34d042a15',cHash: '6eeb0ed5f3026a8',cUPMDTk: \"\\/news\\/infineons-co2-sensor-precisely-monitors-indoor-air-quality\\/?__cf_chl_tk=MJ_31yjJtJxrZLO5AkK6DfLtJO4iw4S4Wtc2cvkEY9U-1727031706-0.0.1.1-4969\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/news\\/infineons-co2-sensor-precisely-monitors-indoor-air-quality\\/?__cf_chl_f_tk=MJ_31yjJtJxrZLO5AkK6DfLtJO4iw4S4Wtc2cvkEY9U-1727031706-0.0.1.1-4969\",md: \"eFIi7OthgESeCDhBzIsR0jbbo3SR_X8_Zqxmzt6tIGY-1727031706-1.1.1.1-5iBJXdDbfr3OLlOgJL9poC3EHzkAOj8yvN0lguHQZdSyQzwm7UJtvJ2.cc_ow5ngZR.Iglziafdow_sqqDx_f.9bRBEE80y09LIhGY5.96_TZkNoxe7qOntXkBTL7KGHJr.Zsjr4VuFnTY3t280ZM9WvaSbFXBmwwSMPqG5aa.nVN3.SMIn7hWB6XAJH2DpaPKtD.3UG1iJap4EPTJ6EWBQc6GFyiCJW_trAM3A_iJCnd9.qYvkVqKw7TSmcTx_VQYyHaH5O8qHYzJmOt5KY7wCuI21cX6h8CWFHPXvd.u.htcFqQy1VoypAdmwXbQnUS1An07.kR6lJUIusNr3RTCl0oMu1dNQR8_C5ofwpexY6IB6yLY6dsxbsO956n72tCHSkXbJ0Bh1A4NWD3TGE6Y383Ku.z4UE0DlOWmEP2YnjyfYKHqe2qKLcGvpjUK5E3BOcGfK6kiO_8KsVnIwQnmS0IzQZyDg8j8EU5BQpO8fwTP2.1kR6pKFVA0Rwh0J2P4SsVJJziW6_KF5wQH5KqNdxUYAx9MaTnyqjZ6pUYYGtAfQS4w7u5xWNzrNi1O3k1psthHhUylFsnpzh0R6ROuxG1d.ibW9_Qc9bDiQOF_ay2Ah00h073xE2oXKnyB8MNoKJt6ZHC_KRWLrAe5qJZ_EWBGTLzfTO6quRXtvxA6SEFxxl3maV1bFhZPfCZzEpuTJUqeDRVcComG.IEGPPQszwGKSSqbPy_7sj8QS1diD9DeOTPtSPQOSNCnbVhPJoCUnBqe5g8pOkH9wWUi7OmRQu1EYesFwKo8SYBxP7sq4rfny0OwlWVwclKkSI0hYFdVws3UlLMr1q0IZDPOgh3WA30_AyQ2B8FehvGJhJgsHZYyopdMJr6h9zOFEpEqGar7xnDfH6ND7k15ige5cKk0O59j74uPrpzgtkDUpYWvkcaXrsJSj9pYAup1nsJ_qgRjAZzBV9_2KjQCtaxJ6rgl.29_cPAXH4720HWNjumaWdXrXGcIw_LFAoQaNQ7RZ5gco1QUomsHE5wIREEnfBKHIZzD5NnWUqTWXw9UkEgYrW8Q4ZzuMHLqmgybhGi1eypSMMQsEQCrKz2PekCkwCn_bSSt6xhaYPG6NS.qI0NGBBMcxdodA7KYr6lDUu2hJjy3iFoVbDf2tNF_KdfTnBvMX6DqEJqLGHIgYslCqbJl9N4uQ13VSvEMnkvTV7oN2Nln1bgHlJrHg9CFZ_SroqINGBCuFN.BIBGZeJJ2_0wphIzktCeDaCp3EkjRS2O3uO8Ivy_GRYArjexBr7J5vWazD1RoMg6zSBO9AdN275hYElSSsA4EivAGVAz9vgeehsXJyJW_PSADlhSEGLBjfZ8SW88gphdB3zKYhmfvHFJheXGET5r2XWIU4JsC6ZMcIYyEWLYlOE..5dL9E4mgds8HMADGyFY270s0V73O4xXufG5eotSeI4bys9V7L64PHlKhatRJ8Q6hppkaCuaOqwJX1tT5WBHTVR.1OcncV_TS3rywmy06sLhtxu1VHRCx65o5h7L4ubC7YGor0GUUo2J6nTSCO.YChfkPY_HOFq09I9zTah9xYw9aCwgj1Taqrim68Gj2YKqlVlBOfAkAbknfXcg8tJtJDrE_iRbjpQAjDY7SHCN6v5B1.xOdzfRU_YzHwaRg55aWBo6nGzlEKfuVVEna3KGhM654WCXg9Q.8m3ryqQKsiwT9t3AWhoLjREODZV1Eln1xc5nvjA9.j9LZvR7EKD_FNlWS2aCk47orAHPzYjc.AtIAWrHDY.XN7XbdE28P0g3wpp5BkiWOCE.4rH8iM3YYXq8miUoMuD0ReCUm_p7eAtV12SQ6In1KKIMAaZEqebgLMIohW_WG6AFHSWRw5A8JJzQlCTVyxiX3wR48Bl_INR7tQn93tp6sbGs2NOvHTS8I075JNcOlLCbYuBppw3nQETajlo3HCO6GwEnyazgyplfLOU6qaatB12ED9mS717gWy8D0IDO.nDIH0aoKX66jCHFAGPIjs69bgk5GheUY89g13gXrEO4TpCV_K7aZMLQfPouFLGuV6OLhNpOB1irMNYOfSUmN0jOjXObGklHyKZDuEwDCGKbVau7AvUMyT2cDC5XmPSV_.UaqkUOuHAKWxI2nx5vzqVqH.D0ICXt4Zu_TdyB5IKcdpAFcQRnJJWiisatO7VfDd7tr7Diy9jdsVyh3mqGFJBV6BmRG73llcF3eK6nhem3_n30Cn6bSkUYcRxaUZyqUNgoqK5RTgZ5Xa4bl.d6Ji_0n4phjqBlx2VoszexTt_jpH39cPzPG6zX_1c9OzU610j0IeZ1fHeWkmv1JYygvUhfkqut_V9djxpTjcrQZnbt3f2tpRY_9HieC4s6ZrmKz5dOjY5o_Ty_CnAyYdAPcZeyEnMeaGEr5lLgUad8ROFA4lC\",mdrd: \"yNtaVzv29kJXbsBI0nAKsMp8vl78_g.azzqTPVPs9ZY-1727031706-1.1.1.1-Gq2.l5U5g78U5j.6Uq_l55gOCcZasR.RfmZ7iPLFNoUnPXZmIBKgd88mqr_QD._L7M8cfCmgYyBjo6ZMw7QCxp0zdu9Dqw6PdRIJUs9AvBqDtgwG9eHv2BkeH640UW.cEaEjKjwpvaM_de8Pzyjou.wBvJt95J8DYWom1U5PIsn8TRnIU3BH_5IIDMhI7p6IACBTR6XdWFSRPAa8NNcoLJj07InhQ0.EHMlkhNYk61WLRU_w2W5d6UUGqA43qKnovQMgqqeFyP0UlK6gUAPg2pwv.7imOE1Kbc4jZQ95AWPw.W8Wnk9ARUkwQ5hxDJfx0mrUtBjovcXDcAyRQyQwOmSlG8ac8AhyqI1f1CNwzUtGSSDh1bnnWiFj4jFqzIotSN60rH0RwyKQOyGUZ2PIcptOOt6shrM1D6BelJjmIG4.paYHy9PlWwrx9jsAttyxVin2tqRiXQdV.VWX1FrPUp6aS011B2RdumooERY.CTAcHUvYP48Frjh4lVehad9YZXOTJlCEj4ogiAY1dq6KKk0LefO2jQ50bmyoFvjfJxNsCWq9KyZolAuytX5fIMjY7x669g404Mr_pnm7oilIiH_UMb3gn_ZHcuHwBuGgsZI7nUv_eitRl.GEFTyWbG9I__0YdeCYqlTSGDgrf8q9aAUcAMWKCdEIYFEcTKWixHMb1JDAGZ9FbY7MbKlCGKjbYbvaT84xE5UgQgfUpUyrxDvAxHZnReAC9Zr.Vn660Be8tWqVv1dFyjyDfSfHMXi3DHGplCMATl6il_9oKuQuQ2B22gPJxAk8Fouwh8NLBerSXCiVlrMRa5qfCZwTEJOqMTuCkQlFw5D8Zkxsv6N4ZB1LFfkrQfFo4JyYn61aoIN1nlXwFPUyxgLYB18xTwMM4TjDNGnvYHZ0HQn94Q3MJaZp2m_DY9fmsTwi8rspEazac97Ka58H8IBDIe2HDHJHva84.ieUBxfAZFjVbqso70QVpn7w7Ze.Y.OgiLdvF3d8A13jXRK79WFV2Oc7jekMEbe5Ba0k2strrEIOvhwb.qi04er1CfKcZYvmQoULhj6P1TB6_fmAyqsC5wsgJbHGDbEuJgYVkQt2jowduq8rSO6GvT3_2UDcbDv98I6ZK3myCBQv76cpMmIh__lr4QTr6dxIxFXQUQKpMx31K8VY4Q4Ek_Y7G0tEPVVFfpfC7Jn0mwwKEOcrd2lPjLJNVH.aivEKYRQbQuuk407GjxONz1wcsOGkuICOpQv61sHCFxJyN1._r3lC3OYvNq_w.YkVtiaSyGnU4MxGJDYfV0cNMZscmzup29xKiDGE_e.hmPAy6O2LdzQOPx9c2R2.wJlEdllm1OvRidpHXGPllXZ7nbE9Ooh6OdZ7kqEuaMUXkJMGrP0ZSbcdb.uW0FGPioFPosYTsDICC2hQOA8vW6lMM_Xo7oYg4CBJG0yY9_NS_BwDF1pMK35GCUg4HtngoPBwe6D5Tw6rOJbwMl.15lyd_80eUoYyV.GIgKpxP7JnmAU4x29v9Jy_po9DvweLMoa4FoO3XlXMoOgcJjtVmO97rIjSCEG404VnJQc5miEDgdz3gCBR_RWBGcVrQcrCHlrSJfglGR7MOAjtCSTtZudxAFZbciwEOm43O1GmBcQS6X9.PdWrvu4I8O1MQjuSFzfMC39PQxdjj3fqGvPIACNV0vG61BnvmJ3tV3NCgW_jNWWHCiKIFJYwpUXr1q86CVcMoMqDxQ3I53s4vlMPvUsqSPQyGNwq0epgw0lPvrlXHH5NjrEcL9.SAaBoJ61zWDvVUJDNGQ43zxHoDjuRyCSFtF_nVU1GdP3WpJJCFwJQXu2RIUefP9E7siLbPgFtP2fr5DAUJ2O0SXqqIaloq9iCUXK7BgKru3nM1nTTooWKl7m0We6Fp_RqkFPD4ApeXlM4Yikl1tG5fO7Vit1.ZWtkuFPwQOAmFaqDKFOqn7leD0MUmzO1sMp5IOklzYzRAQmi4X.JV.BM86b8IIay2posagi6k5HMnI9g_ZJVMwbkxj5k7GrjwZZ23NmI5cOaiDEznPJvu8nqzf12.MJ0yLBZC3n5elob2jSa3OWtSgtWqZKudndxGe85b8H23EEkSHuRs9msoOYTC2GDEjLMLX46Gi9JXJVJ.fLykhz1nqJeEitwEwKtC3hGXdTqq0H4AIQpHkjxheuLYQdoVLyCU5enUsqhjyKpz6_srAev._Whm46dFfiPQ06TEEWEYiBR8.zWXrjvbfZfHeYZzza6g61W25XjbcFHGDEr0OQuZqU3h8A87OsOeHbr5T5rDFUIoKtZHxXv11JfOj.JcfS6sVZKJ2FR.8Ab0Itob8K9XOzgdev3wPMpIIywH33Bh3epi.g6QPEpfIziw..XNr75uRxTUuMRY2zWRsUo8dxATl3uM0o\",cRq: {ru: 'aHR0cHM6Ly93d3cuYWxsYWJvdXRjaXJjdWl0cy5jb20vbmV3cy9pbmZpbmVvbnMtY28yLXNlbnNvci1wcmVjaXNlbHktbW9uaXRvcnMtaW5kb29yLWFpci1xdWFsaXR5Lw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: 'xmgH+vEFoUlolOPPhsegvwOkVDjRk+M1KKsLhO1tq7WYKycocsB4pA8uXXVb3A7GxFd7HmMAc0wEIm0x9TWtIJ+AeNKEpvVSJdthQMMKFoYEO+fIHeHuvzQy14bSEpVPEGVPChmPm/EaMH5O/Nb8yT6hqYLgvyKcCgU0Edn2Dv8Drtrf6BMVjDZ2/Tl8GgdBNnvD2r4xLsq6Mru6KmyQJ3Qz2sy2EOewBL1wQrqomOG2iTxjtyUq4z0jfkEDLV+EjbzmJqwybbBLm+p3DotALOvLPhCSRQTKaiKf/gsV6+Ya+5M9qeEL4ACyFisOnd2ocSaXi1nGlEc8y/S03zOyOzd9y2kjZkUwxx4hQlwKfErQESTVE4BDw5qziB0UnbEmhkhl6Y+O585JadZEPUeuSbxXHQQML1xD/DYIYtCNb8sgxvgwvAzM47dIVevUKGtV28QiNlyPQdV+Fpya+QIhNVj/VPcpOeOyOz1otyaexu4IWZye489V5JAXVBpekEkHBaJ8eyB7jcyqt7RYSQqx1EHw7tKV9RZ2c+4I4vx7zxfMj0YG2LdmmMGBH+Htw2KmcpxbI/WIDjD+CBLupboNksogtbGcoTRGciZk/1bB7xg=',t: 'MTcyNzAzMTcwNi4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'PGqJNO8USJQmHMlnTrXOi8qn++1Ci5+gsG9Oga8+TvA=',i1: 'TQotbNjMPI+8oysQuF8JLw==',i2: 'xyfZATcrxAIsGr5NP/22VA==',zh: 'rJT7imqlHzBSkrmCqoE5cmFmn4yA882g63g2BOaH95o=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: '9GylEXBM4rpllZFKt+ho7S39bSjiH39PgRkx1C1ziVs=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=8c748ba34d042a15';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/news\\/infineons-co2-sensor-precisely-monitors-indoor-air-quality\\/?__cf_chl_rt_tk=MJ_31yjJtJxrZLO5AkK6DfLtJO4iw4S4Wtc2cvkEY9U-1727031706-0.0.1.1-4969\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41611965",
    "commentBody": "Infineon's CO2 Sensor Monitors Indoor Air Quality (allaboutcircuits.com)169 points by WaitWaitWha 23 hours agohidepastfavorite69 comments ahaucnx 17 hours agoWe did some extensive testing of photo-acoustic and optical NDIR CO2 sensors indoors as well as outdoors and also in comparison to scientific reference instruments. I just updated the blog post that I wrote last year [1] with the specs of this new Infineon sensor and also our experiences with doing CO2 measurement outdoors. In summary we can say that under normal indoor conditions (e.g. small range of temperature, relative high range of CO2 etc.), the photo-acoustic NDIR sensors perform well. However, outdoors they significantly underperform optical NDIR sensors to the extent that they are barely usable. See for example this chart [2]. It's hard to say why exactly they perform so poorly outdoors but I believe that they rely on quite complex internal algorithms that probably have been only developed with typical indoor conditions in mind and as soon as some parameters like temperature are out of the typical range, they significantly drop in their performance. Furthermore, we also detected that they are sensitive to interference from low frequency noise which could for example come from ventilation systems, refrigerators etc. This is not surprising as they rely on their internal microphone to do the photo-acoustic measurements. All of this and the rock-solid performance we see with optical NDIR sensors made us quite wary about the photo-acoustic sensors and not use them in our open-source hardware monitors. As I said, I believe they work quite well in general but because we saw also some really strange and unexpected behavior of these photo-acoustic sensors, I'd now always prefer optical NDIR sensors if possible. [1] https://www.airgradient.com/blog/co2-sensors-photo-acoustic-... [2] https://www.airgradient.com/blog/co2-sensors-photo-acoustic-... reply OldGuyInTheClub 33 minutes agoparentOptical linewidths are influenced by temperature and pressure. Outdoors, humidity can be a factor as well. When I did photoacoustic spectroscopy, we used two or more wavelengths in the measurement; one on resonance, one off resonance to help with background discrimination. I am not sure whether these ultracompact instruments have that capability. Additionally, contaminants on any surface the light strikes will generate spurious signals. Lock-in (synchronous) detection at the optical source modulation frequency can help get rid of a lot of noise but if the noise appears at that frequency, it will corrupt the measurement. This classic paper from 1986 covers a lot of what can be done with the technique and how to do it correctly. Reviews of Modern Physics is behind a paywall but there's a copy on the web at the URL below. Applications of photoacoustic sensing techniques Andrew C. Tam Rev. Mod. Phys. 58, 381 – Published 1 April 1986 http://users.df.uba.ar/dkunik/photothermal/review/tam86.pdf reply wkat4242 17 hours agoparentprevInteresting. But why would you monitor CO2 outdoors? Even the NDIR ones just return \"400\" when they detect outdoor air and use it as calibration. Though i believe you can turn off the auto calibration which is probably a good idea in busy city areas where I imagine outdoor CO2 is slightly higher than 400. And manually calibrate them with an actual known source. reply ahaucnx 16 hours agorootparentOutdoor CO2 levels can go up to 700-800ppm in rural areas due to plants creating CO2 [1]. I was initially surprised it's that high but it was verified with reference instruments. Why measure CO2 outdoors? We are able to detect emission sources pretty well now with our outdoor monitors [2] that have the SenseAir S8 NDIR sensor built in. You can see some real data in the blog post I wrote about launching our global CO2 map [3]. So setting up a dense network of these sensors in a city would allow to measure if for example the introduction of low-emission zones or switching to electric buses etc. would work. Another use case is to check for leakage in underground CO2 storage facilities. All-in-all it's still experimental (to some extent) but we know the accuracy is there and we can see more and more use cases as outlined in my blog post. So we now work hard to get more and more of these sensors out there to get more data to identify additional use cases. [1] https://www.airgradient.com/blog/performance-of-low-cost-co2... [2] https://www.airgradient.com/outdoor/ [3] https://www.airgradient.com/blog/airgradient-global-co2-map/ reply neilv 5 hours agorootparent> Outdoor CO2 levels can go up to 700-800ppm in rural areas due to plants creating CO2 [1]. We need to get those rural voters more oxygen, STAT! reply nroets 1 hour agorootparentEm, do you know that plants are actually a net source of oxygen ? And a sink of CO2 ? reply specto 2 hours agorootparentprevIt seems like ozone is the missing piece. The only ones that measure outdoor ozone are governments, and the numbers keep increasing. reply darby_nine 12 hours agorootparentprevSurely this refers to decomposition and not living plants. Your link doesn't appear to finger plants as the culprit at all. reply avianlyric 11 hours agorootparentPlants produce CO2 as well, photosynthesis is only used to produce sugars, normal respiration (I.e. sugars + O2 = energy + CO2), is used by plants to consume those sugars. Only a plant cells containing chlorophyll and exposed to sunlight will photosynthesise. Notably at night there isn’t much sunlight, so normal respiration dominates a plants gas conversion processes, and results in them producing net CO2. Based on the graphs in the blog post, I assume GP is placing the CO2 production on plants because it the CO2 levels peak at night, then return to normal every morning. reply mschuster91 10 hours agorootparentYep, I've noticed this as well with my cannabis grow tent. At an 18/6 lighting schedule, it's noticeable that CO2 grows up to 800ppm in the tent at the end of the fake-night cycle. reply Rebelgecko 14 hours agorootparentprevWho do you calibrate these CO2 sensors in areas where the background level is so high? reply ahaucnx 13 hours agorootparentNot sure what you mean with calibration here. Above is not so much about calibration but correlation. Basically comparing the monitors to the reference data. reply hyperknot 13 hours agorootparentI think he meant How?, meaning normally a calibration is placing indoor monitors outdoors for 20 mins. reply avianlyric 11 hours agorootparentIf you’re using reference grade instruments, I would assume they’re calibrated in labs using well controlled environments. I.e. put sensor in vacuum chamber, remove all gases, then introduce know amount of gases to produce a known target environment. Given their reference instruments, I assume they’re also capable of maintaining their calibration for a long period of known time, before they need to be re-calibrated. They would never rely on something as inaccurate as “20 mins outdoors” to calibrate themselves. reply cyberax 13 hours agorootparentprev> Interesting. But why would you monitor CO2 outdoors? For example, to know that it's futile to try to turn on ventilation to lower the indoor CO2 concentration. reply dr_kiszonka 3 hours agoparentprevIf you have time, could you write a bit more about the humming phenomenon? reply OldGuyInTheClub 26 minutes agorootparentPhotoacoustic is usually done with a chopped or pulsed light source at a few hundred Hz (ie. audio frequencies). If the molecules don't absorb at that wavelength or if the molecules are not present, the light just goes through. If light is absorbed, there will be slight heating when the beam is on resulting in a sound wave at the chopping/pulsing frequency. This is detected by the microphone. For weak absorptions, the intensity of the sound wave will be linear in the concentration. I think this is what is meant by \"humming\": There is a sound that is generated when the target molecule is present. reply moogly 5 hours agoparentprevThe Infineon press release talks about indoor monitoring only, and it is even present in the title, but I have noticed that you guys consider HN to be a very important marketing channel, so I get it. reply ahaucnx 5 hours agorootparentIf you take the time and actually read the blog post I linked to, you will see that mostly it is about indoor CO2. However, to understand the performance of these different sensor technologies it is important to point out that they fail under certain environmental conditions, e.g. outdoors. reply babl-yc 21 hours agoprevSeems similar to the SCD40 photoacoustic approach. I used that for an open-source CO2 monitor I designed: https://bitclock.io/ https://github.com/goat-hill/bitclock reply ahaucnx 16 hours agoparentGreat project, lovely design and super cool that it's also open source hardware. We at AirGradient open sourced our monitors around 2 years ago and this has been the best decision for our company. reply OldGuyInTheClub 21 hours agoprevNice to see this miniaturization of photoacoustic spectroscopy - something I've done a bit of in the past. It is an underappreciated technique. Ordinarily one measures the difference in optical throughput with and without a sample. If it is a weak absorber, it is a difference between two large numbers. PAS is zero background. No absorption, no pressure wave, no signal. Any absorption stands out clearly against that zero background. reply londons_explore 6 hours agoprevI have used the MH-Z19 [1] $10 real CO2 sensors for a bunch of things. They appear to work well, although I have no ability to measure the accuracy of the results. I do have a 'one day when I get free time' plan to make new firmware for them to also measure moisture and a bunch of other VOC's which have unique absorption spectra in the 800-2000nm range, since the hardware itself can be abused as a poor-man's spectrometer. [1]: https://www.aliexpress.com/w/wholesale-z19-co2.html? reply IgorPartola 6 hours agoparentThere is a way to program these? reply londons_explore 6 hours agorootparentthey're just ST microcontrollers. you can put your own firmware on and do what you like with the hardware. The hardware consists of an incandescent bulb and photodiode with amplifier and high res ADC. The bulb and photodiode sit in a box with a silvered interior, which allows light from the bulb to reflect around inside the box a lot (and having some light absorbed by CO2) before hitting the photodiode. reply vardump 22 hours agoprevFinally an actually working (cheap?) CO2 sensor? So many of those actually measure humidity, temp and VOCs and try to derive some sort of CO2 reading out of those. reply tzs 21 hours agoparentIt's not really \"finally\". They introduced a similar sensor, the PASCO2V01, a couple years ago. That one has been available on a breakout board with the necessary support hardware from SparkFun for over a year [1]. Comparing the datasheets for the PASCO2V01 and the new PASCO2V15 the old one actually seems a little better as far as CO2 measuring performance goes. They are the same on most things, but the old one has slightly better accuracy. The new one is ±(50 ppm + 5%) between 400 ppm and 3000 ppm. The old one is ±(30 ppm + 3%) between 400 ppm and 5000 ppm. The big difference is this: > Infineon has recently introduced the PASCO2V15, a new 5 V sensor to improve air quality monitoring in building environments. Both of them require a dual voltage power supply. They both want 3.3 V for their digital components and a higher voltage for their IR emitter. For the older one that higher voltage is 12 V. For the newer it is 5 V. [1] https://www.sparkfun.com/products/22956 reply fnordpiglet 21 hours agorootparentIirc they were merged into esphome last year too. reply Animats 20 hours agoparentprevTrue. The cheap ones are trying to guess CO2. Those are called \"indoor air quality sensors\". Small CO2 sensors have been available for years, for about $50. Compare [1]. Life of this new device is only 10 years, which is short for HVAC systems. A hotel might have a thousand of these. Older devices say \"15+\" years. All these devices have a calibration problem. They drift. They try to correct by treating the lowest value they ever see as \"normal\" (that's about 400 ppm CO2 today, vs 300 PPM in 1950) and recalibrating. So they're not useful for observing a general increase in CO2. They're also not useful for greenhouses, where CO2 levels may drop below ambient CO2 due to photosynthesis. Manual recalibration is possible but requires feeding in pure nitrogen and a known nitrogen/CO2 mixture.[2] Devices which don't need that re-calibration exist.[3] They're more complicated. Also don't seem to be stocked by the usual distributors. [1] https://rmtplusstoragesenseair.blob.core.windows.net/docs/pu... [2] https://www.co2meter.com/blogs/news/7512282-co2-sensor-calib... [3] https://www.murata.com/en-us/products/sensor/co2/overview/te... reply brianglick 17 hours agorootparentDo you know of anywhere at all to get one of the devices that don’t require re-calibration? reply Animats 15 hours agorootparentThat is a really good question. Murata announced this in 2019, and there's a part number (IMG-CA0011-00/ IMG-CA0012-00/IMG-CA0023-00) and a full data sheet.[1] But no distributor has it. Not DigiKey, Mouser, Arrow, or Newark. Even Octopart doesn't list it. It's on Murata's list of recommended products, not the discontinued list. Try contacting Murata. [1] https://go.murata.com/rs/382-MEZ-125/images/HC%20CO2-sensor%... reply fuzzy2 21 hours agoparentprevSensirion has the SCD40, which appears to be based on the same principle. It's much cheaper than the SCD30. reply _blk 21 hours agorootparentYes, SCD30 series are optical while SCD40 series are photoacoustic. STC series are thermal conductivity based. STC and SCD40 are smaller than SCD30 but less accurate if memory serves (check datasheet). reply IgorPartola 22 hours agoparentprevThis was my thought exactly. I used sensors that were about $25/each in the past and those worked well but this would be seemingly way easier to integrate and get ahold of. reply vardump 21 hours agorootparent$25 for a CO2 sensor component that actually works is not bad at all. reply tobi1449 21 hours agorootparentSensirion SCD41 should be pretty good, right? You can get them for ~25 USD on aliexpress ... reply ale42 21 hours agorootparentI'm not sure I would source non-Chinese electronic components from Aliexpress... (unless it's just to play with them, definitely not for a product): you might need to check their reliability and quality. Pretty sure that Sensirion themselves are not selling there, so they are probably either clones, fakes, recycled ones, or if you are lucky authentic ones that for some reason ended up there (but I can't imagine a way). reply throwup238 16 hours agorootparent> or if you are lucky authentic ones that for some reason ended up there (but I can't imagine a way). Chinese fabs are notorious for running secret shifts that make their customers’ chips to sell themselves. They’re the exact same chip but made without authorization, using the customer’s exact design. That said, Sensirion has their own CMOS fabs in Switzerland so that’s very unlikely to be the case here. If they work at all, the counterfeit chips are probably some Chinese CO2 sensor IC that's small enough to fit into the same package, lightly customized to fit the pinout of the original. Or it's just a microcontroller inside faking it a la FTDI. reply radicality 19 hours agorootparentprevI got mine from M5Stack and works quite well, I’m submitting data every 5 seconds using EspHome/HomeAssistant reply rainburg 21 hours agoparentprevThe Senseair S88, which was released earlier this year, costs ~$22/piece, or ~$13/piece if you order more than 100. reply sbierwagen 11 hours agoprevI built a CO2 meter around a SCD30 five years ago: https://bbot.org/blog/archives/2024/05/19/pocket_co2_meter_b... My takeaway is that it draws a lot more power than you'd expect, thanks to the incandescent light source, and unless there's quite a lot of airflow over the sensor, it'll exhibit self-heating at any poll rate under every ten minutes. reply nimish 21 hours agoprevThey've had a 12v version for a while, and it's quite nice despite the high voltage requirement. I made a little breakout with a boost converter. Sensirion has a slightly smaller sensor as well, SCD41 that I think works on similar principles. Neither are cheap, around $25-40 each in small quantities. The infineon one has a full blown microcontroller handling the operation of the sensors. To keep accuracy you would need to have a CO2 gas setup which isn't cheap either, but for indoor use I don't think it matters. reply koolba 16 hours agoprev> With this architecture, the sensor achieves a high level of precision, offering an accuracy of ±50 ppm ±5% between 400 ppm and 3,000 ppm. The overall range of the sensor is from 0 to 32,000 ppm. What does the back to back ± mean? Is that the variance of accuracy from device to device? Or does the 5% reference the specific range of 400-3000? reply zootboy 15 hours agoparentIt's saying that it could be either +/-50 ppm from the actual reading, or +/-5% from the actual reading, whichever is worse. reply metaphor 14 hours agorootparent> It's saying that it could be either +/-50 ppm from the actual reading, or +/-5% from the actual reading, whichever is worse. No, it's stacked, and strictly applicable between 400-3000 ppm as characterized (despite 0-32000 ppm operating range by design). At the lower 400 ppm end, accuracy is +/- 50 ppm +/- 20 ppm = +/- 70 ppm. At the upper 3000 ppm end, accuracy is +/- 50 ppm +/- 150 ppm = +/- 170 ppm. Also worth noting that the calibrated reference used to determine this published accuracy has an uncertainty of +/- 2%; ref datahseet[1] Table 7. Caveat emptor: this published accuracy can't be trusted at face value when sampling more than 1 meas/min; ref ibid.[1] Table 4. [1] https://www.infineon.com/dgdl/Infineon-PASCO2V15-DataSheet-v... reply FL33TW00D 21 hours agoprevI’ve been considering designing a wearable that monitors CO2 and PM2.5 continuously, but I’m unsure if people would wear it in conjunction with an Apple Watch or similar. reply sbierwagen 11 hours agoparentNote that NDIR sensors use a surprising amount of power, since they're based around an incandescent bulb shining light through the sample volume. A CO2 wearable will need to be recharged once a day, like a smart watch. reply triwats 20 hours agoparentprevThis is super interesting for me - but I'd love to put it on a bicycle (they are often locked up and stationary - for example) but move at faster speeds. Maybe this means they are only useful at tracking information when locked/stationary? I really like the idea of using cheap (?) devices in a sort of mesh to feed back telemetry data on pollution. Pollution is everyone's concern, so visualising that would be cool. Interested to hear if you had any more thoughts on this! reply rapjr9 5 hours agorootparentThe Bikenet project mapped CO2 in a small town: https://www.cs.dartmouth.edu/~sensorlab/pubs/BikeNet-SenSys0... reply carstenhag 19 hours agorootparentprevRelated: cheap device to track pollution + mesh + visualization: https://luftdaten.info/ Once had this thing on a balcony of a shared flat in Heilbronn, Germany. Wondered what that was, previous tenant told me about it and it was never removed from there. reply seper8 9 hours agoparentprevI'd wear this! This would also be very cool to share with others, form like a network to get a clearer picture of air quality reply akira2501 1 hour agorootparent> to get a clearer picture of air quality A wearable CO2 monitor would do the opposite of this. If you want to measure quality you need fixed location devices. reply FL33TW00D 7 hours agorootparentprevIt only became possible to do it in a wearable form factor very recently using a combination of the following 2 sensors: https://sensirion.com/products/catalog/STCC4 https://www.bosch-sensortec.com/media/boschsensortec/downloa... Not sure the market is big enough to invest my time. reply crazygringo 21 hours agoparentprevI'm not sure how accurate that would be on your wrist, because the proportion of recently exhaled air would be so much higher, since it's only a foot or two from your mouth. CO2 monitors often have little silent fans to draw in fresh air as well, for accuracy. reply christina97 20 hours agorootparentI don’t think it’d make a difference. I’ve got one on my desk in front of me and can’t detect a difference with me at the desk vs not. reply stevenhuang 16 hours agorootparentUnless your room has really good airflow you should be able to tell quite clearly. I have an SCD41 and I see a large spike in readings less than a minute after sitting down at my desk. reply JSDevOps 20 hours agoprevhttps://aranet.com/en/home/products/aranet4-home?srsltid=Afm... reply thadk 18 hours agoparentI believe Aranet4 contains this NDIR optical sensor https://www.co2meter.com/collections/sensors/products/006-0-... reply elliottkember 20 hours agoparentprevLike that, but cheaper and more accurate. reply Genbox 21 hours agoprevDatasheet can be found here: https://www.infineon.com/dgdl/Infineon-PASCO2V15-DataSheet-v... reply shadowpho 21 hours agoprevHave been using scd30/31/40. Great sensors. This one requires a bit more power but would be interesting to see price as it seems it actually measured CO2. (A lot of other sensors simulate it with measuring alcohols and assume people breathing which gives poor results) reply skykooler 16 hours agoprevIt's $26 on Mouser, which seems like a reasonable price. reply clumsysmurf 21 hours agoprevI worked in a building 500 ft from a busy highway and when I cleaned my desk it always had black dust on it. Along these lines of air quality, can anyone recommend a similarly advanced PM2.5 / PM10 sensor under $100 / ea? reply e44858 1 hour agoparentSensirion SEN54. It also measures temp/humidity/voc, and has a built-in fan to speed up the response time. reply trog 21 hours agoparentprevI haven't gotten their PM sensor unit but have a CO2 sensor from CO2.click. About to pull the trigger on a PM sensor but just deciding which one. The founder there is active on a few places including Mastodon and I really like my CO2 sensor from them. Edit: sorry missed your price guidance. They are quite a bit more so probably not what you're after! reply seper8 9 hours agoparentprevPlease get an air purifier with a HEPA filter... Ikea sells a reasonable air purifier AND seperate sensors nowadays! reply christina97 20 hours agoparentprevHave been using a Plantower PMS5003 for a while with ESPhome and it’s pretty good. reply ckocagil 17 hours agoprevThe problem with the common CO2 sensor modules is they don't have DC accuracy. Meaning they rely on the device being present in place where it regularly (e.g at least once a week) gets exposed to fresh air, which the module sets as its baseline. This works because fresh air has roughly the same CO2 concentration everywhere. Hopefully this method doesn't have the same restriction. reply dzhiurgis 21 hours agoprev [–] Would be nice if they packaged these up in laptops - fan is already there and always running. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Infineon's new CO2 sensor is effective for indoor air quality monitoring but underperforms outdoors compared to optical NDIR sensors.",
      "Photo-acoustic NDIR sensors, like Infineon's, are sensitive to temperature changes and low-frequency noise, making them less reliable in outdoor environments.",
      "Optical NDIR sensors are preferred for consistent outdoor performance, highlighting the calibration challenges and environmental considerations necessary for accurate CO2 monitoring."
    ],
    "points": 169,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1726945730
  },
  {
    "id": 41612049,
    "title": "What Is a Particle? (2020)",
    "originLink": "https://www.quantamagazine.org/what-is-a-particle-20201112/",
    "originBody": "Home What Is a Particle? Read Later Share Copied! (opens a new tab) Comments Read Later Read Later Previous: Hidden Structure The Cosmologist Who Dreams in the Universe’s Dark Threads SERIES Hidden Structure What Is a Particle? By Natalie Wolchover November 12, 2020 It has been thought of as many things: a pointlike object, an excitation of a field, a speck of pure math that has cut into reality. But never has physicists’ conception of a particle changed more than it is changing now. Read Later Elementary particles are the basic stuff of the universe. They are also deeply strange. Illustrations by Ashley Mackenzie (opens a new tab) for Quanta Magazine Introduction By Natalie Wolchover Senior Editor November 12, 2020 View PDF/Print Mode Hidden Structure mathematical physics particle physics physics quantum gravity quantum physics string theory symmetry theoretical physics All topics Given that everything in the universe reduces to particles, a question presents itself: What are particles? The easy answer quickly shows itself to be unsatisfying. Namely, electrons, photons, quarks and other “fundamental” particles supposedly lack substructure or physical extent. “We basically think of a particle as a pointlike object,” said Mary Gaillard (opens a new tab), a particle theorist at the University of California, Berkeley who predicted the masses of two types of quarks in the 1970s. And yet particles have distinct traits, such as charge and mass. How can a dimensionless point bear weight? “We say they are ‘fundamental,’” said Xiao-Gang Wen (opens a new tab), a theoretical physicist at the Massachusetts Institute of Technology. “But that’s just a [way to say] to students, ‘Don’t ask! I don’t know the answer. It’s fundamental; don’t ask anymore.’” With any other object, the object’s properties depend on its physical makeup — ultimately, its constituent particles. But those particles’ properties derive not from constituents of their own but from mathematical patterns. As points of contact between mathematics and reality, particles straddle both worlds with an uncertain footing. When I recently asked a dozen particle physicists what a particle is, they gave remarkably diverse descriptions. They emphasized that their answers don’t conflict so much as capture different facets of the truth. They also described two major research thrusts in fundamental physics today that are pursuing a more satisfying, all-encompassing picture of particles. “‘What is a particle?’ indeed is a very interesting question,” said Wen. “Nowadays there is progress in this direction. I should not say there’s a unified point of view, but there’s several different points of view, (opens a new tab) and all look interesting.” A Particle Is a ‘Collapsed Wave Function’1 The quest to understand nature’s fundamental building blocks began with the ancient Greek philosopher Democritus’s assertion that such things exist. Two millennia later, Isaac Newton and Christiaan Huygens debated whether light is made of particles or waves. The discovery of quantum mechanics some 250 years after that proved both luminaries right: Light comes in individual packets of energy known as photons, which behave as both particles and waves. Wave-particle duality turned out to be a symptom of a deep strangeness. Quantum mechanics revealed to its discoverers in the 1920s that photons and other quantum objects are best described not as particles or waves but by abstract “wave functions” — evolving mathematical functions that indicate a particle’s probability of having various properties. The wave function representing an electron, say, is spatially spread out, so that the electron has possible locations rather than a definite one. But somehow, strangely, when you stick a detector in the scene and measure the electron’s location, its wave function suddenly “collapses” to a point, and the particle clicks at that position in the detector. Samuel Velasco/Quanta Magazine A particle is thus a collapsed wave function. But what in the world does that mean? Why does observation cause a distended (opens a new tab) mathematical function to collapse and a concrete particle to appear? And what decides the measurement’s outcome? Nearly a century later, physicists have no idea. A Particle Is a ‘Quantum Excitation of a Field’2 The picture soon got even stranger. In the 1930s, physicists realized that the wave functions of many individual photons collectively behave like a single wave propagating through conjoined electric and magnetic fields — exactly the classical picture of light discovered in the 19th century by James Clerk Maxwell. These researchers found that they could “quantize” classical field theory, restricting fields so that they could only oscillate in discrete amounts known as the “quanta” of the fields. In addition to photons — the quanta of light — Paul Dirac and others discovered that the idea could be extrapolated to electrons and everything else: According to quantum field theory, particles are excitations of quantum fields that fill all of space. In positing the existence of these more fundamental fields, quantum field theory stripped particles of status, characterizing them as mere bits of energy that set fields sloshing. Yet despite the ontological baggage of omnipresent fields, quantum field theory became the lingua franca of particle physics because it allows researchers to calculate with extreme precision what happens when particles interact — particle interactions being, at base level, the way the world is put together. Helen Quinn proposed the still-hypothetical “axion field” in the 1970s. Nicholas Bock/SLAC National Accelerator Laboratory As physicists discovered more of nature’s particles and their associated fields, a parallel perspective developed. The properties of these particles and fields appeared to follow numerical patterns. By extending these patterns, physicists were able to predict the existence of more particles. “Once you encode the patterns you observe into the mathematics, the mathematics is predictive; it tells you more things you might observe,” explained Helen Quinn, an emeritus particle physicist at Stanford University. (opens a new tab) The patterns also suggested a more abstract and potentially deeper perspective on what particles actually are. A Particle Is an ‘Irreducible Representation of a Group’3 Mark Van Raamsdonk remembers the beginning of the first class he took on quantum field theory as a Princeton University graduate student. The professor came in, looked out at the students, and asked, “What is a particle?” “An irreducible representation of the Poincaré group,” a precocious classmate answered. Taking the apparently correct definition to be general knowledge, the professor skipped any explanation and launched into an inscrutable series of lectures. “That entire semester I didn’t learn a single thing from the course,” said Van Raamsdonk (opens a new tab), who’s now a respected theoretical physicist at the University of British Columbia. It’s the standard deep answer of people in the know: Particles are “representations” of “symmetry groups,” which are sets of transformations that can be done to objects. Take, for example, an equilateral triangle. Rotating it by 120 or 240 degrees, or reflecting it across the line from each corner to the midpoint of the opposite side, or doing nothing, all leave the triangle looking the same as before. These six symmetries form a group. The group can be expressed as a set of mathematical matrices — arrays of numbers that, when multiplied by coordinates of an equilateral triangle, return the same coordinates. Such a set of matrices is a “representation” of the symmetry group. Samuel Velasco/Quanta Magazine Similarly, electrons, photons and other fundamental particles are objects that essentially stay the same when acted on by a certain group. Namely, particles are representations of the Poincaré group: the group of 10 ways of moving around in the space-time continuum. Objects can shift in three spatial directions or shift in time; they can also rotate in three directions or receive a boost in any of those directions. In 1939, the mathematical physicist Eugene Wigner identified particles (opens a new tab) as the simplest possible objects that can be shifted, rotated and boosted. For an object to transform nicely under these 10 Poincaré transformations, he realized, it must have a certain minimal set of properties, and particles have these properties. One is energy. Deep down, energy is simply the property that stays the same when the object shifts in time. Momentum is the property that stays the same as the object moves through space. A third property is needed to specify how particles change under combinations of spatial rotations and boosts (which, together, are rotations in space-time). This key property is “spin.” At the time of Wigner’s work, physicists already knew particles have spin, a kind of intrinsic angular momentum that determines many aspects of particle behavior, including whether they act like matter (as electrons do) or as a force (like photons). Wigner showed that, deep down, “spin is just a label that particles have because the world has rotations,” said Nima Arkani-Hamed, a particle physicist at the Institute for Advanced Study in Princeton, New Jersey. Different representations of the Poincaré group are particles with different numbers of spin labels, or degrees of freedom that are affected by rotations. There are, for example, particles with three spin degrees of freedom. These particles rotate in the same way as familiar 3D objects. All matter particles, meanwhile, have two spin degrees of freedom, nicknamed “spin-up” and “spin-down,” which rotate differently. If you rotate an electron by 360 degrees, its state will be inverted, just as an arrow, when moved around a 2D Möbius strip, comes back around pointing the opposite way. Samuel Velasco/Quanta Magazine Elementary particles with one and five spin labels also appear in nature. Only a representation of the Poincaré group with four spin labels seems to be missing. The correspondence between elementary particles and representations is so neat that some physicists — like Van Raamsdonk’s professor — equate them. Others see this as a conflation. “The representation is not the particle; the representation is a way of describing certain properties of the particle,” said Sheldon Glashow (opens a new tab), a Nobel Prize-winning particle (opens a new tab)theorist and professor emeritus at Harvard University and Boston University. “Let us not confuse the two.” ‘Particles Have So Many Layers’4 Whether there’s a distinction or not, the relationship between particle physics and group theory grew both richer and more complicated over the course of the 20th century. The discoveries showed that elementary particles don’t just have the minimum set of labels needed to navigate space-time; they have extra, somewhat superfluous labels as well. Particles with the same energy, momentum and spin behave identically under the 10 Poincaré transformations, but they can differ in other ways. For instance, they can carry different amounts of electric charge. As “the whole particle zoo” (as Quinn put it) was discovered in the mid-20th century, additional distinctions between particles were revealed, necessitating new labels dubbed “color” and “flavor.” Sheldon Glashow lectured at CERN in December 1979, two weeks after he was awarded the Nobel Prize in Physics. CERN Just as particles are representations of the Poincaré group, theorists came to understand that their extra properties reflect additional ways they can be transformed. But instead of shifting objects in space-time, these new transformations are more abstract; they change particles’ “internal” states, for lack of a better word. Take the property known as color: In the 1960s, physicists ascertained that quarks, the elementary constituents of atomic nuclei, exist in a probabilistic combination of three possible states, which they nicknamed “red,” “green” and “blue.” These states have nothing to do with actual color or any other perceivable property. It’s the number of labels that matters: Quarks, with their three labels, are representations of a group of transformations called SU(3) consisting of the infinitely many ways of mathematically mixing the three labels. While particles with color are representations of the symmetry group SU(3), particles with the internal properties of flavor and electric charge are representations of the symmetry groups SU(2) and U(1), respectively. Thus, the Standard Model of particle physics — the quantum field theory of all known elementary particles and their interactions — is often said to represent the symmetry group SU(3) × SU(2) × U(1), consisting of all combinations of the symmetry operations in the three subgroups. (That particles also transform under the Poincaré group is apparently too obvious to even mention.) A New Map of All the Particles and Forces Hidden Structure A New Map of All the Particles and Forces October 22, 2020 Read Later The Standard Model reigns half a century after its development. Yet it’s an incomplete description of the universe. Crucially, it’s missing the force of gravity, which quantum field theory can’t fully handle. Albert Einstein’s general theory of relativity separately describes gravity as curves in the space-time fabric. Moreover, the Standard Model’s three-part SU(3) × SU(2) × U(1) structure raises questions. To wit: “Where the hell did all this come from?” as Dimitri Nanopoulos (opens a new tab) put it. “OK, suppose it works,” continued Nanopoulos, a particle physicist at Texas A&M University who was active during the Standard Model’s early days. “But what is this thing? It cannot (opens a new tab)be three groups there; I mean, ‘God’ is better than this — God in quotation marks.” Particles ‘Might Be Vibrating Strings’5 In the 1970s, Glashow, Nanopoulos and others tried fitting the SU(3), SU(2) and U(1) symmetries inside a single, larger group of transformations, the idea being that particles were representations of a single symmetry group at the beginning of the universe. (As symmetries broke, complications set in.) The most natural candidate for such a “grand unified theory” was a symmetry group called SU(5), but experiments soon ruled out that option. Other, less appealing possibilities remain in play. Researchers placed even higher hopes in string theory: the idea that if you zoomed in enough on particles, you would see not points but one-dimensional vibrating strings. You would also see six extra spatial dimensions, which string theory says are curled up at every point in our familiar 4D space-time fabric. The geometry of the small dimensions determines the properties of strings and thus the macroscopic world. “Internal” symmetries of particles, like the SU(3) operations that transform quarks’ color, obtain physical meaning: These operations map, in the string picture, onto rotations in the small spatial dimensions, just as spin reflects rotations in the large dimensions. “Geometry gives you symmetry gives you particles, and all of this goes together,” Nanopoulos said. However, if any strings or extra dimensions exist, they’re too small to be detected experimentally. In their absence, other ideas have blossomed. Over the past decade, two approaches in particular have attracted the brightest minds in contemporary fundamental physics. (opens a new tab) Both approaches refresh the picture of particles yet again. A Particle Is a ‘Deformation of the Qubit Ocean’6 The first of these research efforts goes by the slogan “it-from-qubit,” which expresses the hypothesis that everything in the universe — all particles, as well as the space-time fabric those particles stud like blueberries in a muffin — arises out of quantum bits of information, or qubits. Qubits are probabilistic combinations of two states, labeled 0 and 1. (Qubits can be stored in physical systems just as bits can be stored in transistors, but you can think of them more abstractly, as information itself.) When there are multiple qubits, their possible states can get tangled up, so that each one’s state depends on the states of all the others. Through these contingencies, a small number of entangled qubits can encode a huge amount of information. In the it-from-qubit conception of the universe, if you want to understand what particles are, you first have to understand space-time. In 2010, Van Raamsdonk, a member of the it-from-qubit camp, wrote an influential essay (opens a new tab) boldly declaring what various calculations suggested. He argued that entangled qubits might stitch together the space-time fabric. Calculations, thought experiments and toy examples going back decades suggest that space-time has “holographic” properties: It’s possible to encode all information about a region of space-time in degrees of freedom in one fewer dimension — often on the region’s surface. “In the last 10 years, we’ve learned a lot more about how this encoding works,” Van Raamsdonk said. What’s most surprising and fascinating to physicists about this holographic relationship is that space-time is bendy because it includes gravity. But the lower-dimensional system that encodes information about that bendy space-time is a purely quantum system that lacks any sense of curvature, gravity or even geometry. It can be thought of as a system of entangled qubits. Under the it-from-qubit hypothesis, the properties of space-time — its robustness, its symmetries — essentially come from the way 0s and 1s are braided together. The long-standing quest for a quantum description of gravity becomes a matter of identifying the qubit entanglement pattern that encodes the particular kind of space-time fabric found in the actual universe. A New Map of All the Particles and Forces Hidden Structure A New Map of All the Particles and Forces October 22, 2020 Read Later So far, researchers know much more about how this all works in toy universes that have negatively curved, saddle-shaped space-time — mostly because they’re relatively easy to work with. Our universe, by contrast, is positively curved. But researchers have found, to their surprise, that anytime negatively curved space-time pops up like a hologram, particles come along for the ride. That is, whenever a system of qubits holographically encodes a region of space-time, there are always qubit entanglement patterns that correspond to localized bits of energy floating in the higher-dimensional world. Importantly, algebraic operations on the qubits, when translated in terms of space-time, “behave just like rotations acting on the particles,” Van Raamsdonk said. “You realize there’s this picture being encoded by this nongravitational quantum system. And somehow in that code, if you can decode it, it’s telling you that there are particles in some other space.” The fact that holographic space-time always has these particle states is “actually one of the most important things that distinguishes these holographic systems from other quantum systems,” he said. “I think nobody really understands the reason why holographic models have this property.” It’s tempting to picture qubits having some sort of spatial arrangement that creates the holographic universe, just as familiar holograms project from spatial patterns. But in fact, the qubits’ relationships and interdependencies might be far more abstract, with no real physical arrangement at all. “You don’t need to talk about these 0s and 1s living in a particular space,” said Netta Engelhardt (opens a new tab), a physicist at MIT who recently won a New Horizons in Physics Prize (opens a new tab) for calculating the quantum information content of black holes. “You can talk about the abstract existence of 0s and 1s, and how an operator might act on 0s and 1s, and these are all much more abstract mathematical relations.” There’s clearly more to understand. But if the it-from-qubit picture is right, then particles are holograms, just like space-time. Their truest definition (opens a new tab)is in terms of qubits. ‘Particles Are What We Measure in Detectors’7 Another camp of researchers who call themselves “amplitudeologists” seeks to return the spotlight to the particles themselves. These researchers argue that quantum field theory, the current lingua franca of particle physics, tells far too convoluted a story. Physicists use quantum field theory to calculate essential formulas called scattering amplitudes, some of the most basic calculable features of reality. When particles collide, amplitudes indicate how the particles might morph or scatter. Particle interactions make the world, so the way physicists test their description of the world is to compare their scattering amplitude formulas to the outcomes of particle collisions in experiments such as Europe’s Large Hadron Collider. Share this article Copied! (opens a new tab) Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters (opens a new tab) Nima Arkani-Hamed investigates the relationship between particle behavior and geometric objects. Béatrice de Géa (opens a new tab) for Quanta Magazine Normally, to calculate amplitudes, physicists systematically account for all possible ways colliding ripples might reverberate through the quantum fields that pervade the universe before they produce stable particles that fly away from the crash site. Strangely, calculations involving hundreds of pages of algebra often yield, in the end, a one-line formula. Amplitudeologists argue that the field picture is obscuring simpler mathematical patterns. Arkani-Hamed, a leader of the effort, called quantum fields “a convenient fiction.” “In physics very often we slip into a mistake of reifying a formalism,” he said. “We start slipping into the language of saying that it’s the quantum fields that are real, and particles are excitations. We talk about virtual particles, all this stuff — but it doesn’t go click, click, click in anyone’s detector.” Amplitudeologists believe that a mathematically simpler and truer picture of particle interactions exists. In some cases, they’re finding that Wigner’s group theory perspective on particles can be extended to describe interactions as well, without any of the usual rigmarole of quantum fields. Lance Dixon (opens a new tab), a prominent amplitudeologist at the SLAC National Accelerator Laboratory, explained that researchers have used the Poincaré rotations studied by Wigner to directly deduce the “three-point amplitude” — a formula describing one particle splitting into two. They’ve also shown that three-point amplitudes serve as the building blocks of four- and higher-point amplitudes involving more and more particles. These dynamical interactions seemingly build from the ground up out of basic symmetries. A New Map of All the Particles and Forces Hidden Structure A New Map of All the Particles and Forces October 22, 2020 Read Later “The coolest thing,” according to Dixon, is that scattering amplitudes involving gravitons, the putative carriers of gravity, turn out to be the square of amplitudes involving gluons, the particles that glue together quarks. We associate gravity with the fabric of space-time itself, while gluons move around in space-time. Yet gravitons and gluons seemingly spring from the same symmetries. “That’s very weird and of course not really understood in quantitative detail because the pictures are so different,” Dixon said. Arkani-Hamed and his collaborators, meanwhile, have found entirely new mathematical apparatuses that jump straight to the answer, such as the amplituhedron — a geometric object that encodes particle scattering amplitudes in its volume. Gone is the picture of particles colliding in space-time and setting off chain reactions of cause and effect. “We’re trying to find these objects out there in the Platonic world of ideas that give us [causal] properties automatically,” Arkani-Hamed said. “Then we can say, ‘Aha, now I can see why this picture can be interpreted as evolution.’” It-from-qubit and amplitudeology approach the big questions so differently that it’s hard to say whether the two pictures complement or contradict each other. “At the end of the day, quantum gravity has some mathematical structure, and we’re all chipping away at it,” Engelhardt said. She added that a quantum theory of gravity and space-time will ultimately be needed to answer the question, “What are the fundamental building blocks of the universe on its most fundamental scales?” — a more sophisticated phrasing of my question, “What is a particle?” (opens a new tab) In the meantime, Engelhardt said, “‘We don’t know’ is the short answer.” 1: “At the moment that I detect it, it collapses the wave and becomes a particle. … [The particle is] the collapsed wave function.” (opens a new tab) —Dimitri Nanopoulos (back to article) 2: “What is a particle from a physicist’s point of view? It’s a quantum excitation of a field. We write particle physics in a math called quantum field theory. In that, there are a bunch of different fields; each field has different properties and excitations, and they are different depending on the properties, and those excitations we can think of as a particle.” (opens a new tab) —Helen Quinn (back to article) 3: “Particles are at a very minimum described by irreducible representations of the Poincaré group.” — Sheldon Glashow “Ever since the fundamental paper of Wigner on the irreducible representations of the Poincaré group, it has been a (perhaps implicit) deﬁnition in physics that an elementary particle ‘is’ an irreducible representation of the group, G, of ‘symmetries of nature.’” (opens a new tab) —Yuval Ne’eman and Shlomo Sternberg (opens a new tab) (back to article) 4: “Particles have so many layers.” (opens a new tab) —Xiao-Gang Wen (back to article) 5: “What we think of as elementary particles, instead they might be vibrating strings.” (opens a new tab) —Mary Gaillard (back to article) 6: “Every particle is a quantized wave. The wave is a deformation of the qubit ocean.” (opens a new tab) —Xiao-Gang Wen (back to article) 7: “Particles are what we measure in detectors. … We start slipping into the language of saying that it’s the quantum fields that are real, and particles are excitations. We talk about virtual particles, all this stuff — but it doesn’t go click, click, click in anyone’s detector.” —Nima Arkani-Hamed (back to article) Editor’s note: Mark Van Raamsdonk receives funding from the Simons Foundation (opens a new tab), which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More details are available here. By Natalie Wolchover Senior Editor November 12, 2020 View PDF/Print Mode Hidden Structure mathematical physics particle physics physics quantum gravity quantum physics string theory symmetry theoretical physics All topics Share this article Copied! (opens a new tab) Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters (opens a new tab) The Quanta Newsletter Get highlights of the most important news delivered to your email inbox Email Subscribe Recent newsletters (opens a new tab) Also in Physics The Search for What Shook the Earth for Nine Days Straight geophysics The Search for What Shook the Earth for Nine Days Straight By Robin George Andrews September 12, 2024 Read Later Can Thermodynamics Go Quantum? The Joy of Why Can Thermodynamics Go Quantum? By Steven Strogatz September 12, 2024 Read Later The First Nuclear Clock Will Test if Fundamental Constants Change nuclear physics The First Nuclear Clock Will Test if Fundamental Constants Change By Joseph Howlett September 4, 2024 Read Later Comment on this article Quanta Magazine moderates comments to facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English. Show comments Next article Physicists Pin Down Nuclear Reaction From Moments After the Big Bang",
    "commentLink": "https://news.ycombinator.com/item?id=41612049",
    "commentBody": "What Is a Particle? (2020) (quantamagazine.org)156 points by sblank 23 hours agohidepastfavorite133 comments jakey_bakey 16 minutes ago> \"It has been thought of as many things\" This makes me remember the time I looked at a random book at my university library, and it happened to be a physics book from 1905. Which was both fascinating and unintentionally hilarious due to it proudly asserting that we knew most of physics now because we knew about atoms, and assuring the plum pudding model as how atoms worked. n.b. Plum Pudding was the old-school idea that atoms were a positively-charged blob with negative electrons embedded. It was refuted when you measure the radiation scattering patterns off gold foil and discover that, actually, there's an extremely dense nucleus. reply AlbertCory 23 hours agoprevI'm reading \"The Big Picture\" (Sean Carroll) right now. I'd love to have a real physicist explain this, but: When we think of what a particle IS, we often think as though it were dirt, or a billiard ball, or something. As though there were some other substance of which it's made. At least I do. But the definition is as low as you can go. It's hard to wrap your head around that. Unless you're trained to do so, I guess. reply ziofill 14 hours agoparentPhysicist here. You are right that the mental picture we get when we use the term “particle” is a little ball or something like that. It is unfortunately a confusing name… You need to begin with a field, like the electromagnetic field for instance. When you look at its properties like energy, polarization and so on, in order to write down a state of the field you need to specify all of them in a way or another. In quantum mechanics you can associate a vector space to each property, and then (here is the important bit) you need to pick a basis for your vector space in order to write down its vectors. Obviously there is an infinite number of possible choices, and we usually end up choosing what makes things simple, so in the case of energy we pick the basis of eigenvectors of the Hamiltonian, because to evolve them in time you just need to multiply them by a complex number and that’s it. Well, those basis vectors are the “particles“ because when taken individually they share some properties with macroscopic particles, but the analogy really only goes so far. And the thing is that usually the state of the field is not in a single one of these basis vectors unless the conditions are very special, so even saying that the field is “made of particles” is misleading because it’s like saying that the wind is made of air going vertically, horizontally and across, which sure it’s “correct” because you can combine those directions and get any other direction but it’s also not really that… reply tel 4 hours agorootparentAs an amateur, I think I follow most of this, at least at some level, but I don't follow why you'd unify the basis elements and particles. Thinking of a quantum harmonic oscillator, the eigenstates have some kind of localization that feels particle-like, but the oscillating pattern of a coherent solution seems \"more particle-like\" and arises out of the interference between those eigenstates. In particle-speak, I might try on a sentence like \"this classical particle is generated by the interaction between... other... particles\" but I'm clearly at a loss there. On basis of that, I'd be more likely to say \"QM needs to describe everything as a wave, and sometimes certain kinds of localized 'wave-packets' move around coherently, and that's what we'd call 'particles'\". That also seems to gel with less coherent states where it feels like there's not really a particle to be found. So, I'm curious why you'd prefer to relate the eigenstates themselves as particles. Again in the oscillator case, the eigenstates themselves seem less coherent and seem to behave less classically than I'd hope. My best guess is that the property those states have that is not as well replicated by the \"particle as a coherent wave packet phenomenon\" is that they have well-defined energy quanta. But that's just a bit of a stab in the dark here. It perhaps makes more sense from the perspective of \"particles are the things that we're able to measure in detectors\" POV, though. reply ziofill 2 hours agorootparentYou are exactly right at the end when you say that particles are what makes a detector go click. Let me try to clarify further. The \"particleness\" is not about localization, but about energy quantization. In fact, a single photon can be very non-localized, because spatial position is a different Hilbert space and it can be entirely independent of the energy. So the packets you are referring to are particles only if in their energy Hilbert space they correspond to a well-defined photon numbers, and that's why the analogy with classical particles only goes so far. To add to the confusion, one can speak of packets localized in phase space (e.g. coherent states, like the light produced by a laser) and packets localized in physical space like a short pulse, and these refer to different Hilbert spaces. reply divs1210 20 hours agoparentprevParticle spin explained: Imagine a ball that’s rotating, Except it’s not a ball, and It’s not rotating. (popular particle physics meme) From what I understand of QFT, the Universe is made of fields of different types, and a “fundamental particle” is just an excitation (wave) in the corresponding field. For example, a photon is a wave in the universal electromagnetic field, A charm quark is a wave in the universal charm quark field, etc. I’m not a trained physicist, so I might be wildly wrong. reply binary132 19 hours agorootparentI get it but I still think these sorts of concepts are also just another level of mathematical abstraction that isn’t necessarily “really what it is” any more than a rotating ball or a math equation or any of the other ideas are “really what it is” reply lottin 4 hours agorootparentIt's very frustrating. The idea that the universe is made of fields is nonsensical. I don't understand why so many physicists keep saying that. reply jerf 8 minutes agorootparentThe Universe May Do As It Damned Well Pleases. Maybe it is \"made of fields\", maybe it isn't, but \"I think that's nonsense\", which is just a gussied up way of saying \"my intuition rejects that\", is not a valid judgment method. The universe does not check with our intuition before doing what it Damned Well Pleases. reply binary132 3 hours agorootparentprevOne of the ultimately epistemological puzzles to me is the question of what math really is. Like, obviously, it is fundamentally descriptive. “Two and two makes four” is pretty straightforwardly talking about something “out there”. And when we’re talking about fields, we are clearly also describing something that is really happening, that is really “out there”; it’s not the math itself that is the real thing, but rather it is a language for accurately describing and analyzing real things. But at some level, the real things it’s describing become so abstract and immaterial that they might as well be magic, or spirit. And it seems to me like our minds also contain and experience such things, too. Very advanced math and physics necessarily start to border on philosophy or theology. reply travisjungroth 2 hours agorootparentOne way of viewing it is that math is games. Not in the winner sense, but in the activities with rules sense. Addition is a game. Some games make you better at other activities. Like, playing chess could make you better at logistics because you’re practicing planning and managing losses. Some games match some real world situations so tightly that we can go through them step by step and solve the real world situation in the game. You can play addition to figure out two apples and two more makes four apples. Whether the game is “real” or not is immaterial. It just needs to be internally consistent and matched to the right thing. There’s also the idea that math is another world that we can visit, similar to the dream world. But that’s a whole other thing. reply fasa99 39 minutes agorootparentThe idea that it's an abstraction is 100% accurate. That physics is a discrete set of fields with field rules and interaction rules, and what we observe is a scaffold on top of that. It's like okay, let's say the math is right and we have a set of fields, what are they and where are they from, how do we manipulate them. Then the physicists, often driven by ego, is want for an explanation and points to vibrating strings and such, and finally they knock on the door of the empiric physicist and say \"can you do an experiment to show that I'm right\" \"sure, build a machine the size of the universe and I could test that\" and that's the state of physics the last gorillion years https://www.youtube.com/watch?v=FYJ1dbyDcrI reply im3w1l 4 hours agorootparentprevIt's worth keeping in mind that we don't yet have the sought-after Theory of Everything. We have a bunch of theories that mostly work in their domain of validity. These field theories are supposedly very accurate we don't actually know if they are the final word. reply jiggawatts 15 hours agorootparentprevSpin is easy as long as you avoid trying to draw a direct analogy with ordinary rotation. It’s just the statement that the object spinning is attached to its surroundings in a smooth and continuous fashion. Less rigid object, more a patch of space-time fabric spinning. There’s a video here: https://youtu.be/LLw3BaliDUQ?feature=shared reply elashri 23 hours agoparentprev> we often think as though it were dirt, or a billiard ball, or something The problem lies that it is hard to imagine something that does have zero dimensions. You can get the example of ant walking into 2D and it is unaware of third dimension to explain we are have something similar for space-time 4D (although not the same picture exactly as time is different from spatial dimensions). But we don't have an idea how to approximate a mental picture of what a zero dimension could be. So you have something that does not occupy a volume in space (Talking strictly about elementary particles here) in the classical sense. This does not mean they are abstract concept. According to QFT -Quantum field theory- you would think (by training) of particles are excitations or quanta of their respective fields. Fields are there always (vacuum is just filled with fields) and particle appears when they are excited (more complex processes occurs). So you would think of each particle as a manifestation of a quantum field that permeates the universe. What is interesting (and probably confusing to most people) is that these fields are not zero-dimensional, instead, they exist everywhere in space and time. But the quanta (particles themselves) are considered point-like with no spatial extension. In practice physicists will think about particles properties (i.e charge, mass, interactions, spin) ..etc instead of what this particle actually is from that point of view. This is often for practical reasons. You are a working physicist and you learned from your training that you shut up and calculate (or implement if you are doing experimental particle physics as you spend most of your time coding) by this stage. reply hughesjj 14 hours agorootparentI just think of a zero dimensial object as a ghost. Topological defect. The unpictured thing the contour lines are swirling around. It's influence is only felt by seeing the effects on higher dimensional space, but you can never see the ghost itself. reply xanderlewis 21 hours agorootparentprev> The problem lies that it is hard to imagine something that does have zero dimensions. Do you really think so? It’s not hard to picture the real number line, with the point zero (or any other single point) distinguished. Sure — if you draw it in the standard schematic way you have to give it some area, but it still seems quite intuitive that it’s ‘zero-dimensional’. Especially if you play around with converging sequences and open sets and stuff; you quickly develop intuition for what it means to be a point rather than something higher dimensional. reply Traubenfuchs 4 hours agorootparentprevSo we have the three spatial dimensions, + time as 4. dimensions and at any of those 4-part coordinates there are additional properties like mass/spin/etc., some of much always come together or at least strongely correlate, and those values not being zero means there is a particle there and every value corresponds to a certain „field“ and it not being zero means the field is excited? reply lisper 21 hours agoparentprevRichard Feynman gave what I consider to be the best possible answer to questions like this: https://www.youtube.com/watch?v=Q1lL-hXO27Q reply passion__desire 6 hours agorootparentIn similar vein, the following question is very apt. Please read the question because it captures all of our intuition when we try to understand something. https://physics.stackexchange.com/questions/46573/what-are-t... What are strings made of? One answer is that it is only meaningful to answer this question if the answer has physical consequences. Popularly speaking, string theory is supposed to be the innermost Russian doll of modern physics, and there are no more dolls inside that we can explain it in terms. However, we may be able to find equivalent formulations. reply aaa_aaa 20 hours agorootparentprevAt first I was impressed with that video. Then I felt he does not have an answer and unnecessarily gets edgy with it, because question is valid. reply vertnerd 4 hours agorootparentFeynman grapples with the question the same way we would grapple with a question from a child: \"why is the sky blue?\" If you drill down into the explanation, you ultimately reach a statement that everyone just accepts as true, or you simply end with, \"no one knows\". reply mewpmewp2 3 hours agorootparentThe way Feynman answered it looked extremely condescending and anti curiousity. Being pedantic for no reason. When answering you should try to guesstimate what the asker who is not an expert in your field is looking for and then start explanation relative from there. At certain point, yes, you do have to say that either you don't know or humans haven't figured it out yet. reply AlbertCory 19 hours agorootparentprevI just watched it. I don't think he's edgy. You can't explain it in terms of anything else, which was sorta my original point. Maybe he could have been more touchy-feely in his answer, but that wasn't his nature. reply johndhi 16 hours agorootparentprevHmm I think he's merely explaining what physics is and is not. Physics isn't really answering \"why\" questions, at least not ones with infinite scope. reply lisper 20 hours agorootparentprev> he does not have an answer Well, yeah. That's the whole point. reply alok-g 15 hours agorootparentThe additional important point, of course, is that there are many more 'Why' questions to be asked (often more interesting, and more important than corner cases like human-scale magnetism) that do not get asked just because of familiarity. Familiarity however is not understanding, and it is the same as simplicity. reply aaa_aaa 11 hours agorootparentprevHe could simply say so. reply hydrogen7800 5 hours agorootparentHe does repeatedly. And continues to explain why there is no satisfying answer, because we normally stop asking \"why\" once we reach a level of familiarity. That level of familiarity to the layperson is different between electromagnetism and slippery ice. reply datavirtue 5 hours agorootparentprevI see what you did there. reply __MatrixMan__ 16 hours agoparentprevI've only got a physics minor, so hardly an expert, but I felt like quantum mechanics got a lot easier once I started thinking of a particle as merely a situation which has some probability of causing a state change in a detector of some kind. reply darby_nine 11 hours agoparentprevA metaphor with another physical object will always fall short. Why not just state the number of bits a particle represents? It's much easier to describe going through each dimension (colloqiual, I hate string theory for the same reason of unnecessarily using a physical analogy) and describing how it interacts with other particles. Sure you'll lose a lot of your audience but those that remain will have a much clearer picture than via a comparison to a billiard ball. This also makes the more advanced topics like singularities, entanglement, teleportation, the lack of true vacuum, etc much easier to manage. (I'm aware we don't have an understanding of how quantum physics interacts with singularities, but the whole billiard ball metaphor certainly is incoherent with it) reply elbasti 15 hours agoparentprevThis might sound tautological but a particle is, well, a thing that behaves like a particle. Those behaviors are something like: - it has momentum - it's state is uniquely defined by a position in space and a velocity What's not a particle? A wave (well, until 1900 or so ...). Sort of like asking \"what is a number?\" A number is a thing that obeys certain rules. (You can add them; there's an `identity `, for every number there's a number which if you add together gives zero, etc). That allows things like `(3 + 5i)` to be a number, for example. reply csomar 14 hours agoparentprevThis is essentially the Bohr take on the matter. There is no physicality in the sense that we interact with the world in. There are also no real dimensions as they are just our understanding of what we consider the physical world. If that gets around your head, you’ll throw the physicality and real world away and you’ll come to see everything as information interaction. reply danbruc 22 hours agoparentprevI mean I can not speak for you, but I do not think that the problem necessarily is that people think of them as made from some stuff, I think what causes the most trouble is the desire to visualize particles. The trouble is that an electron is an electron and it is nothing like anything you have ever seen in your macroscopic classical world. It shares some aspects with billiard balls and some with water waves but it is not like either. And it does not switch between being a billiard ball and a water wave, it always is the same thing, it always is an electron. It just happens that in certain situations the billiard ball properties are more apparent and in others the water wave properties and in yet other situations neither of the two analogies will help. I think that is what trips people really up, they want to visualize their electron as one thing they know, as something they have an intuition for, but no such thing exists. And electrons being electrons also means that they are not excitations in quantum fields. Those fields are mathematical models that describe the behaviour of electrons, they are not the electrons. Certainly not in the very direct sense of nature is just mathematics because I can differentiate, integrate, and square fields at will but I can not do this to electrons. And even the less direct interpretation, there are real entities in the universe that behave exactly like our mathematical fields, does not seem likely, what would the gauge symmetries mean? reply criddell 22 hours agorootparent> And electrons being electrons also means that they are not excitations in quantum fields You’re going against the dominant interpretation of QFT here, aren’t you? reply danbruc 20 hours agorootparentI have no idea whether or not most physicist think that there are actually quantum fields in the universe. The Navier–Stokes equations provide a good description of milk mixing into my coffee, but should I therefore conclude that my coffee mug is filled with density and velocity fields and that what coffee really is, is a region in spacetime with a nonzero value of the coffee density field? Quantum fields have gauge symmetries which means that they are a redundant description, i.e. any given physical situation is represented by an entire equivalence class of field configurations which makes me highly suspicious of there being real quantum fields. Quantum fields are a nice mathematical tool but I do not think we have any good reasons to think they are real, but I am not a physicist and I am certainly in dangerous half-knowledge territory here. I have been wondering for years whether this might actually be a non-issue, could the universe secretly have fixed a gauge and just ran with it? Or would this somehow be inconsistent? reply raattgift 8 hours agorootparent> I am not a physicist and I am certainly in dangerous half-knowledge territory here. Gauging is just dealing with the fact that there is no absolute universal fixed value against which can compare a value at some point in a field; but we still want to consider values at one or more points in the field. Let's do a really simple static model of the atmosphere, with a single scalar value: air pressure at each point. Let's use a simple device: an air pressure gauge which reports some fraction of a pressure measured when we push a \"calibrate now\" button. We'll call this a calibrated barometer. We can then recover the full air pressure field by measuring at every point in space (not space-time, the staticity means there is no time-dependence to the measurements; we can do them in any order and not have to worry about time of day or season). Where do we push the \"calibrate now\" button? At some point on the surface? At mean sea level? At the top of the atmosphere? The choice of any of these will provide different readings on our gauge (i.e., it reports some fraction of the calibrated pressure, will differ when the calibration point is 101 kPa vs some fraction of the value actually measured at a specific point on the surface). But with a bit of care in choices of units, whatever we use as the calibration point, the difference between two different points in space will be the same. A good choice of gauge lets use our calibrated barometer as an altimeter. In aviation, aircraft pressure altimeters have a calibration knob, which is used to recalibrate during different stages of a flight. Common calibration points are: QFE, field elevation, which lets one know how far above an airfield one is if separated only vertically from it, at the cost of being unable to simply compare the vertical separation between two aircraft above two different airfields; SPS (the pressure of the standard atmospheric pressure, 1013.25 hPa) is a global setting useful for quickly determining the vertical separation between two reasonably nearby aircraft, at the cost of not being able to quickly determine height above terrain, or even the height above mean sea level; QNH is another local setting which lets one compare how high above mean sea level the aircraft is, at the cost of needing to know the height above mean sea level of local terrain, and not being able to easily compare vertical distances with aircraft using one of the other two calibrations. All three settings are \"redundant descriptions\" of the aviator's atmosphere. They describe the same column of air, but each makes it easier to pinpoint different hazards scattered through that column (the ground, other aircraft in level flight, the aircraft's operating ceiling). We could complicate the atmosphere by introducing time dependency (at night in cold dry winter a QNH altitude will be fewer RADAR-measured metres above the same patch of ground), and atmospheric interactions (atmospheric waves, Bernouilli effects from winds). Each complication can be made to vanish via a careful choice of gauge, although it gets harder and harder to write down such a gauge as complications increase. (As a result, in aviation they allow for a certain amount of measurement error and safety margin, and comparisons with different means of measuring altitude like radio altimeters and satellite multilateration.) In a quantum field theory (QFT), one might choose a gauge in which some particles vanish. A sibling comment pointed out that very commonly one wants to choose a gauge in which gauge bosons like photons don't need to be counted, rather than a gauge in which there is a sea of an enormous number of low-energy gauge bosons. Choosing the gauge does not eliminate the low-energy gauge bosons; in general QFT field values are time-dependent (and usually gauged to admit only \"relevant\" fluctuations). Low-energy fluctuations can be boosted into \"real particles\" by relativistic observers, and strongly accelerated observers can count more particles than a weakly accelerated one. Therefore the choice of a gauge for one observer might make calculations for another observer more difficult. In QED there are several well-known and frequently-used gauges roughly analogous to SPS/QFE/QNH, and one often chooses one of them for convenience. Each of tehse gauges breaks the gauge freedom. Gauge freedom means simply an uncalibrated system waiting to be calibrated. A common illustration of this is to choose a non-rotating sphere and setting down latitude/longitude. A less-gauge-symmetrical rotating sphere naturally picks out latitudes (the poles and the equator, notably), but there's still gauge freedom in longitude that we can fix by choosing a prime meridian, and gauge freedom in picking out one of the primary compass directions. These choices do not change the sphere or its rotation (or non-rotation), and of course one can choose any other set of coordinates one wants. Once one has fixed the gauge on the sphere, though, one can more easily compare positions on the surface: is point A in the northern hemisphere, is point B in the eastern hemisphere? Just asking if point B is North-East of point A requires us to at least choose a north pole -- that can be one of two places on a rotating sphere, and it can be anywhere at all on a non-rotating one. The \"right hand rule\" is the conventional \"gauge\" for rotating astronomical bodies: anticlockwise rotation around the north pole (right hand: thumb up, fingers curled). But we don't have to use that convention as our \"gauge\". (We also have a problem for a truly non-rotating spherical object: where's the north pole? We might solve that by using an imagnariy axis parallel to the axis of a relevant body like the local star or the parent galaxy). Finally, in many gauge theories there are gauge invariant quantities. On our spheres the geodesic intervals between two points are gauge invariant. The gauge tells us something about direction. In practice, fixing a gauge also usually involves choosing (and scaling) units: on our geodesic which might run south-east to north-west (gauge problem), the length might be measured in metres or light seconds (units problem) or kilometres and light-years (scaling problem). We might want to label different points along the geodesic in latitude/longitude (coordinate problem) rather than adapted Cartesian sphere-centred/sphere-fixed (\"ECEF\" on Earth) or tangential (\"Local East-North[-Up]\", \"LENU\") ones. reply pgotibojgg 19 hours agorootparentprevDo you think photons are real? Because according to QFT they only exist because of the gauge symmetries. Photons are the solution to the redundant symmetries. Remove those redundant symmetries and you also need to remove the photons. Universe \"fixing a gauge\" means no photons and no electromagnetic field, because the electromagnetic field IS the gauge symmetry. reply binary132 19 hours agorootparentprevI think this is why “shut up and calculate” is popular reply User23 18 hours agorootparentOr as Newton more eloquently put it hypothesis non fingo. reply auntienomen 21 hours agorootparentprevYep. Also, ignoring all the ways in which an electron isn't an electron. Electrons can be created and destroyed, and they are both indistinguishable and exchangeable. We can't assign identity to them, thanks to their Fermi statistics. They're just methods of explaining clicks in a detector. I worked in particle physics for years and never once saw an electron. :-) reply binary132 19 hours agorootparentJoke: “birds aren’t real” Woke: “electrons aren’t real” reply mensetmanusman 20 hours agoparentprevhttps://youtu.be/j2oSyAfPzWg?si=bwM2NAsORzkqLQLk Fun fields discussion on what particles are… reply yahalo 9 hours agorootparentWhat a trip, the guest speaker was clearly a pseudoscientist, talking about \"evolution fields\" and \"mind fields\" and equating fields to souls. reply mensetmanusman 6 hours agorootparentIf anyone that attempts to explain the nature of consciousness in a non-falsifiable is a pseudo-scientist, then yes. I thought it was fun to hear perspectives like this. Also, the soul discussion was pointing more at the history of language and concepts versus a crude equation of the two :) reply at_a_remove 17 hours agoparentprevIt's a useful fiction, but the map is not the territory. This sounds blithe but ... it is as close as you will get to the truth. I only got the bachelors' version of physics, though I did take some grad classes, so here is what I will tell you: The human mind learns from experience and it thinks of things in terms of the past experiences it has had. We are big assemblages which exist in a narrow range of temperatures (think in terms of Kelvin). Our experience is classical, in the Newtonian sense: we move at not a particularly notable fraction of c, we are too warm to note the strangenesses which happen below, say, twenty or four or a thousandth of a Kelvin (superfluids and BECs are out), we are too cold to have a great internal experience of plasma, leaving us to be creatures of solid and liquid, with a sort of inferred understanding of gas. We are too large to feel the quantum realm, in the sense that the uncertainty principle is not obvious to us from what we have felt. So, we must make do with abstractions, with fictions, with approximations. Conscious that we are the epitome of the six blind men trying to understand the elephant through touch alone, we try to break our understanding, to search for flaws in our inferences. Yet this does not grant us true experience when we run across, say, the electron. We try to think of it like a billiard ball, but we can say that a billiard ball is this wide, yet we are fairly sure at this time that the electron has no radius, no diameter, that it might as well be a geometric point. Every time we try to measure, we can only establish a smaller and smaller upper bound for the confounded thing's radius. That's not like our lives at all! The reality of this electron is that if we get it going fast enough, it stops getting much faster no matter how hard we smack it. That's not like our reality. If we try to pin down where it is, the more we do it, the harder it is to figure out how fast and in what direction it moves. And as we work to ascertain the velocity (and therefore momentum), we lose sense of this bit of weirdness' position. You eventually have to develop an understanding based not on experience at all. Perhaps this was unique to me, but the first time I understood integration in calculus, I had a brief moment of dizziness as I apprehended this new thing. You know how you are working a math problem and you have a good idea of what the answer is already, a sense of what the magnitude and direction might be? I had ground my way through vector and tensor calculus, and had been working a problem in gravitation and relativity class when I sensed what the resulting tensor would look like, the shape of it, in the sense that I would know if my figures were way off. I nearly fell off the chair, my head spun so. If you care to, you can do this for a particle. reply heresie-dabord 8 hours agorootparentThank you for this post. Given the limitations of human understanding and experience, one could safely use one metaphor or another for casual description. But at a deeper level of understanding, we do understand that our common experience does not apply, and that human language is too imprecise. reply librasteve 11 hours agorootparentprevthis reply bbor 21 hours agoparentprevI’m not a physicist, but as an arrogant philosopher of science: isn’t it just field excitation? Like, every particle looks like a circle bouncing around a 2D piece of paper, but if you look reeaaaaally closely it’s just a localized 3D spike of energy in a usually 2D field of energy? So it’s made of the field/paper itself. I must be under-thinking this, but that’s what’s worked pretty convincingly for me. reply griffzhowl 12 hours agorootparentSo what is a field? reply im3w1l 3 hours agorootparentA function that takes a point in space as input. The output can be various things, e.g. a scalar field gives a (possibly complex) number as output. reply contravariant 2 hours agorootparentThat's a bit too simple, not all functions work well as fields (differentiability is quite desirable) and you have no way to interact with the fields that way. I think principal bundles come closest to what physicists call fields. Though I'm holding open the option that really the things in most equations are more like elements of the corresponding Lie-algebra. reply librasteve 11 hours agorootparentpreva thing that can have particle-like excitations reply bbor 6 hours agorootparentprevIt’s everything! Idk, I don’t think the universe owes us an answer there. What is a human? Well, it’s a human. You can think of all sorts of mental tools for understanding humans (eg “species”), but ultimately they just are. reply lottin 3 hours agorootparentNo... a field is mathematical representation. The universe is most definitely NOT made of fields. reply dgoodell 2 hours agorootparentAre you saying that the universe cannot be represented by mathematics? I imagine you could use that argument to shoot down pretty much any explanation. reply bbor 57 minutes agorootparentprevOk fair: they're things that right now are best understood using the term Field. I don't understand what kind of answer you're hoping for that would be better than this -- what kind of answer to \"what is a particle\" wouldn't be describable by mathematics? By saying \"the universe is fields\", I'm saying \"it's distributions of energy across spacetime\". That's seemingly a consensus. Why demand that that energy must also form into strings or even tinier spheres or spheres in an alternate dimension or something? We have described fields in detail, I say Mission Accomplished reply FollowingTheDao 8 hours agorootparentprevA probability. reply deanCommie 22 hours agoparentprevThe same is true about the terms \"waves\" and \"fields\" when it comes to quantum mechanics. They're analogies. The concepts need names, but I think they do more harm than good because people then start with a mental model of a membrane or a surface - something they have experience seeing waves in. And then after 1 or 2 steps where the analogy helps, it breaks down, and people start being confused. Of course the alternative isn't any better. If they had named it a \"Wazoo function\" and a \"Quantum Flarg\" everyone would've just kept asking \"OK but what IS a Wazoo? What IS a Flarg\" and not been satisfied with a \"Yeah, it's a fundamental own thing\". Feynman, of course, has a pretty definitive response on the difficulty of this problem: https://www.youtube.com/watch?v=Dp4dpeJVDxs reply scotty79 18 hours agoparentprevParticle is a cloudy, fuzzy thing that can fly and wobble through space. It can be more sharp or more fuzzy and when it overlaps with another fuzzy particle object they might exchange a neat portion of momentum, angular momentum and energy and violently reshape becoming sharper or fuzzier (that's the wave function collapse and expansion) then they go again on their separate merry ways. Sometimes when particles meet or even spontaneously they can split or merge altering other parts of their nature (unrelated momentum, energy and angular momentum). This happens for example when neutron decays into proton and electron. Sometimes they get stuck together because of electromagnetic force and they resonate in interesting harmonies and travel together. That's atoms. Interestingly when they are resonating in those harmonies they become quite fussy about amounts of energy they prefer to exchange and they do it only in a very specific quanta. And there's a class of particles called quarks that travel together all the time as they are always tightly bound with each other and can never get free despite possessing incredible amounts of energy they continuously exchange. That's nucelus. We really don't like this image because fuzziness is actually two dimensional in every point of our already 4 dimensional space-time and described by complex numbers so we prefer to focus on those brief moments when particles interact since if we have a lot of particles that are bound together to form measurement apparatus they are so sharp that the interaction they participate in squash other particles nearly to a point and we can declare that the measure particle collapsed to have some momentum, or location, or spin described by a single vector instead of a cloud. It neatly turns out that the square of complex number fuzziness describes the probability that a fuzzy particle will interact with a sharp one (one of those bound together in measurement apparatus) with a specific outcome. reply ww520 22 hours agoprevThat's why calling Higgs Boson the God particle is not quite right. It's the Higgs field that gives mass to the other particles, not the Boson. A Higgs Boson is just an excitation of the Higgs field; it doesn't give mass to other particles. In fact it's the Higgs field modifying the other fields causing their excitations (particles) to slow down when passing each other, thus gaining masses. reply jophj 21 hours agoparentit was in fact called the \"Goddamn Particle\" originally, referring to how difficult it was to detect it. The name was changed later to \"God Particle\" for publishing reasons. https://en.m.wiktionary.org/wiki/God_particle reply bbor 21 hours agorootparentThe line between whimsy and intellectual negligence seems blurry, in this case… how many people have been tricked by bad-faith gurus using this? Thanks for sharing, TIL and it’s fascinating. reply ItCouldBeWorse 22 hours agoparentprevSo, its all gravity, if you turn the sock inside out? Just taking different colors and shapes? reply ww520 18 hours agorootparentIt’s more like Higgs field gives mass to other particles whose masses warp spacetime that gives gravity. reply Jabrov 22 hours agorootparentprevWhat do you mean? How do you reach that conclusion? I wasn’t aware there was a connection with gravity reply librasteve 11 hours agorootparentprevbest way to reveal a TOE reply scotty79 18 hours agoparentprevActually it doesn't seem to be that simple. Higgs Boson seems to contribute mainly to masses of leptons (mostly electrons) and bosons W and Z. And that influence goes to zero at high energies of the measures system making W and Z weightless and merging electrostatic and weak interactions into a singular electroweak interaction. Quarks get most of their mass from QCD with very minor contribution from Higgs Boson. And nobody has any idea where the mass of neutrinos comes from. It also has no influence on photons and gluons. Higgs seems to be very peculiar and not very universal mechanism. I wonder if one of the potential future approaches won't do away with Highs Boson (together with virtual particles) as artifacts of specific math approach and interpretation without any physical manifestation. Higgs was detected, sure, but it was detected through an interpretation of the data through the best available mathematical model which some postulate might contain some purely mathematical constructions along the way to the ultimate real world result. reply ChrisArchitect 22 hours agoprev(2020) Some discussion then: https://news.ycombinator.com/item?id=25091742 reply Biologist123 21 hours agoprevI feel a curious mix of excitement and disconcerted to discover humans don’t really understand what matter is. Reading the article, I understood so little of it. And I guess it’s because so much of the language is just words chosen through some sort of consensus to represent an abstract idea itself composed of such words-idea-representations which I’ve never encountered before. reply akira2501 21 hours agoparentWe understand matter perfectly well. Look at the size and scope of the engineering marvels that have been constructed on the surface of this planet and in our low orbit. It's astonishing. What we don't understand is the fundamental structure of that matter or of our Universe. I personally feel that the people ostensibly \"in charge\" of this effort are a little chagrined at their decades of inability to produce not only a cohesive result but even a reasonable intermediate explanation that they intentionally couch these problems in the most arcane and impenetrable language available to them. In any case, you shouldn't feel discontent for humanity, as we've simply discovered all the easy problems, cleverly worked out all the average problems, and now all we're left with is the intractably hard ones. It's very likely that a different type of effort we haven't engaged in yet will be necessary to make progress. reply interroboink 21 hours agorootparentI think there's a useful distinction to be made between \"we understand it\" and \"we can make use of it.\" Certainly the latter is true, as you describe in your examples. I don't know that it implies the former, though. I mean heck, even something as mundane as concrete is still the subject of active research as to the chemical reactions and complexities involved. I guess it's more a spectrum of understanding than a yes/no situation. For myself, I find it exciting to keep discovering how little we understand, despite our abilities. We seem to be barely a step removed from alchemy, from some points of view. reply akira2501 21 hours agorootparentI think the distinction is more \"we understand it\" and \"we can explain it.\" Understanding doesn't generally imply totality of comprehension. Going the other way, you most likely cannot explain why your body or your brain works, yet, here we are, using and understanding them just fine. Which leads to what I was trying to get at. Perhaps our tentative understandings and our means of receiving them are what gets in the way of deeper comprehension. reply tambourine_man 21 hours agorootparent> yet, here we are, using and understanding them just fine. I think we’re failing miserably precisely because we don’t. reply akira2501 20 hours agorootparentI personally don't think perfection is actually achievable, so I'm completely unwilling to accept your definition of our present state as \"failing miserably.\" That's a rather miserable point of view and I prefer to have and encourage hope. reply mensetmanusman 20 hours agorootparentprevWe understand _low_ resolution matter. reply gosub100 21 hours agoparentprevThere are about 16 particles in the standard model. We've only mastered the electron, proton, photon, and have dabbled in using neutrons and neutrinos. Imagine the possibilities if we some day are able to use all the remaining particles? reply interroboink 21 hours agorootparentFor some definition of \"mastered\" (: If I recall correctly, the we can't really solve the equations for anything more complex than a helium atom (or is it hydrogen?). That's not to say there isn't useful work we can do, numerical approximations, etc. But things do get astoundingly complex very quickly, even with the \"mastered\" bits. reply jiggawatts 15 hours agorootparentprevMuons and positrons are used regularly in industry. Neutrinos have been used to image the inside of the Sun. reply dang 17 hours agoprevDiscussed at the time: What Is a Particle? - https://news.ycombinator.com/item?id=25085286 - Nov 2020 (37 comments) reply amai 1 hour agoprevSee also Hobson (2012): There are no particles, there are only fields https://arxiv.org/abs/1204.4616 reply khazhoux 23 hours agoprevI'm honestly surprised that more people don't go mad in certain fields. If I ponder for 10 minutes the inexplicability of the universe's existence, or the vastness of space, my mind starts to breaks down. reply disambiguation 1 hour agoparent1.1 INTRODUCTION: THERMODYNAMICS AND STATISTICAL MECHANICS OF THE PERFECT GAS Ludwig Boltzmann, who spent much of his life studying statistical mechanics, died in 1906, by his own hand. Paul Ehrenfest, carrying on the work, died similarly in 1933. Now it is our turn to study statistical mechanics. reply fracus 21 hours agoparentprevConstantly trying to resolve an incomplete abstraction. They are trying to reverse engineer the Universe. I can usually read half way through these articles before I'm completely lost in the abstractions. reply seiferteric 21 hours agoparentprevStarting with the axiom that what I am experiencing is actually representative of reality to begin with. reply scotty79 18 hours agoparentprevIt's just a puzzle and should be taken as such. reply khazhoux 17 hours agorootparentThe universe is a puzzle? reply scotty79 11 hours agorootparentIf you try to understand things about it, yes. reply UncleSlacky 8 hours agorootparentAs someone once said, \"There is no requirement on the Universe to make sense\". reply api 21 hours agoparentprevThe impression this article and many other things like it leaves me with is that we are struggling to get language and an abstractions out of our way. We have to use them to talk about things and work with them, but they all “leak.” reply michaelsbradley 21 hours agoparentprevWhen I consider your heavens, the work of your fingers, the moon and the stars, which you have set in place, what is mankind that you are mindful of them, human beings that you care for them? You have made them a little lower than the angels and crowned them with glory and honor. You made them rulers over the works of your hands; you put everything under their feet: all flocks and herds, and the animals of the wild, the birds in the sky, and the fish in the sea, all that swim the paths of the seas. – Psalm 8:3-8 reply khazhoux 18 hours agorootparentThe answer of \"the universe was created by a creator\" is no more satisfying. It claims to answer everything, by answering nothing (since \"a creator always existed\" is axiomatic). reply FollowingTheDao 5 hours agorootparent> It claims to answer everything, by answering nothing. You missed the wisdom in your own statement. reply gpsx 23 hours agoprevI have another definition, or at least this is how I think of it. I’m not sure many people would buy into it. In the standard model, the fermions are particles, like the electrons, quarks, neutrinos. Electroweak, strong force, gravity are fields. This means the photon is not a particle, but just a field excitation. I know people can think of fermions as fields, I just think of them as particles. reply skzv 22 hours agoparentAren't you describing quantum field theory (QFT)? Anyway, what exactly is a field besides a mathematical object? What is it made of? reply gpsx 17 hours agorootparentI did study quantum field theory and I have a hard time viewing a fermion as a continuous field, whereas a gauge field I do view as a continuous field. I view a fermion as a true point particle, kind of like it is in a lattice. The fermion still has a wave function of course. It is very different from the wave function of a gauge field. The wave function of an electric field is a wave function over field configurations. The fermion wave function is a wave function of fermion spins. I don't think this is an unreasonable view, but I am not trying to force it on anyone else. reply dandragona 49 minutes agorootparentI'm still new to learning about these things, but is the viewpoint that a particle is a field excitation sort of the thing about starting with a lattice in the ground state with a field defined on the points of the lattice, then some excitations happen which cause the field to enter a particular \"mode\". This mode is the particle? reply arcbyte 23 hours agoparentprevCheckout energywavetheory.com. It's essential the Aether, but really makes you think. reply jiggawatts 10 hours agorootparentI flipped through some of the content. It's very well presented, but unfortunately it is pseudo-science gibberish. reply gigatexal 22 hours agoprevtangentially: is it consensus at this point that the proton decays -- it just does so on a really large timescale? reply elashri 22 hours agoparentNo it would be very hard to actually have a consensus on proton decay. If it decays then according to the measurements (or the limits on the lack of the measurement) lifetime of such decay will be more than the universe age (Even without all the puzzle about Hubble constant tension and age of universe measurement disagreements). It was predicted first time by SU(5) theory and many other theories since then but the experiments rules out some of them (including original SU(5)) [1] I would be personally interested in proton decay as it could be indirect indication for magnetic monopoles [2]. [1] https://en.wikipedia.org/wiki/Proton_decay?useskin=vector#Pr... [2] https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.52... reply gigatexal 22 hours agorootparentthank you! -- I meant more that is it consensus that those that know or would need to know think it should/does even if it's not been observed or proven to needfully do so (or disproven, if possible) but I get your point. Why are magnetic monopoles interesting to you? I've seen some articles on them but I can't still wrap my head around how they'd work. reply causality0 13 hours agoprevBut never has physicists’ conception of a particle changed more than it is changing now. A concerning statement for a four year old article. Has anything in it been superseded? reply scotty79 18 hours agoprevThis article contains a very neat description of what is energy, momentum and spin and why they there. Energy is just a quantity that's preserved when shifting through time, momentum is a quantity preserved by shifting through space. And spin is a quantity preserved by rotation in space-time. General relativity treats energy and momentum jointly so I guess basically energy-momentum is a quantity preserved in space-time translations and spin is a quantity preserved in space-time rotations. (in flat space-time, I think?) I guess that's why those Poincare symmetries are rarely mentioned when talking about particles. They seem to come more from sheer geometry of space-time than anything else. Particle physicists are mainly interested in all other symmetries (because they were harder to figure out). It also must be bad feeling that while you are trying pull gravity into your framework, more than half of the symmetries that the objects you spent your career observing obey, come from general relativity not from your framework. reply graycat 19 hours agoprevIssues: (1) With Itself: Consider Young's double slit experiment: So, have plane with two slits and some distance away a parallel plane with detectors. (A) Several times, shoot a photon at the slit. Observe that the detection locations form parallel lines, i.e., fringes. (B) Cover one slit, repeat, and observe that the detection locations from a smooth hill without fringes. So, from (A) we conclude that the something about the photon went through both slits and interacted with itself to form the fringes, the ones we didn't see from (B). Q. Between the two planes, where was the energy? (2) Mass and Charge Set aside (1) with its photons and two planes. Now one at a time shoot electrons, i.e., with not just energy but also mass and charge. And shoot the electrons at a beam splitter, i.e., a plane, partially transparent to the electrons, and at 45 degrees to the path of the electrons. Some electrons pass through the plane with no change in direction and some get deflected 90 degrees. On the paths after the plane, have some very sensitive detectors for mass and charge. These detectors are distant enough that what they do cannot affect the electron, i.e., the electron does not know about the detectors. Q. What do the detectors read? For each of the two paths, whole mass and charge, half, or something else? reply scotty79 17 hours agoparentYou can't detect without affecting. My idea for resolving this is that electron is never a point-like particle. It's always a cloud, just larger or smaller. When it's detected it gets reshaped to be narrower. Mass, energy, momentum and such are a quantities ascribed to the whole cloud and exchanged only on the moment of interaction. Think about diffraction. Photon or electron that passes through a small hole had it's moment messed up proportionally. It becomes large again. Interesting question is where's the gravity in all of this. There are various ideas how to match quantum uncertainty to shape of space-time. reply graycat 17 hours agorootparent> You can't detect without affecting. \"These detectors are distant enough that what they do cannot affect the electron, i.e., the electron does not know about the detectors.\" We detect gravitational waves without \"affecting\". The electron mass and charge send out signals. Have the detectors sufficiently far away that they can't affect the particle yet. Get the detection and then know where the particle was and its mass and charge then. Have the particle reflected by some mirrors and then know the current path of the particle and its mass and charge, all without affecting the particle. reply scotty79 11 hours agorootparent> The electron mass and charge send out signals. This affects them. reply graycat 10 hours agorootparentSo LIGO detects a gravitational wave. Optical telescope data indicates that the wave was generated 10 billion light years away from two neutron stars. So, then, LIGO today affected the two neutron stars 10 billion years ago? Affected them today? reply oezi 5 hours agorootparentAbsolutely. But this not a causal effect, but it collapses the probabilities of what has occurred. reply atemerev 22 hours agoprevA particle is a node in the universal interaction graph. “Space”, however, is a derivative attribute emerging as the “distance” between particles in the graph; there is no “space”, only metrics. Reality does not exist between measurements/interactions; the outcome is calculated on demand. reply scotty79 11 hours agoparentSo basically a tree doesn't fall in the forest if there's no one around to see it? There's no forest and there are not trees. It's all spawned as needed when we go for a hike? Kinda arrogant don't you think? reply atemerev 10 hours agorootparentA tree interacts with many things, not just human observers. We are not special in any way. This discreteness is only relevant and/or observable when we look at individual particles that rarely interact with anything else. reply gweinberg 1 hour agoprevFor some reason this page makes my monitor flicker. Anyone else have this problem? Anyone know why it happens? reply amai 52 minutes agoparentDo you have an external monitor connected to your Mac Pro Laptop? If so, disable True Tone and disable auto brightness on all screens. That might help. reply m101 9 hours agoprevOne of the biggest problems of a scientific education is the lack of hubris taught. Science has a very clear box it works remarkably well in, but it far too often strays outside of this box. We should have been told that science is about the prediction and description of things. This is very different to what things actually are. If only scientists didn't believe from the very beginning that they were studying what reality of things are, they wouldn't spend so much time unlearning this later in life. reply nyc111 10 hours agoprevAs usual comments here are more informative than the article. But no one mentioned that this is not a physics subject. The question “What is a particle?” belongs to philosophy not to physics. The problem for physicists is that they assume the Newtonian worldview that the world is made of indivisible units of matter called particles. [1] This assumption cannot be questioned. It is a dogma of the profession. But their experiments tell physicists again and again that the world is not made of indivisible units of matter. Physicists can either respect their experiments and accept that the world is not made of indivisible units of matter called particles or choose sophistry and try to fit their dogma into nature by wordplay. Physicists chose the latter and instead of dropping their dogma they keep changing the definition of the word “particle”. It does not matter what you call those indivisible units of matter. Physicists used to call them “particle” then “field”, then “excitation” and many other names that can be used case by case to save their sacred Newtonian dogma. Physicists’ dilemma is that they do business under the professional name of “particle” physicists. If there is no particle their profession would be redundant. Obviously they cannot call themselves “excitations of the field physicists”. So they keep the word particle but keep changing the meaning of it and they blame the public for not understanding physics jargon. My advice to physicists: respect the authority of your own experiments and drop the Newtonian dogma of a material world made of indivisible units of matter. [1] \"God in the beginning formed matter in solid, massy, hard, impenetrable movable particles.\" Isaac Newton, Optics, 1704, Book III, page: 375 reply kayo_20211030 23 hours agoprev [–] A particle is a thing you can \"look\" at, and say \"that's a particle\". It is whatever one says it is. They're not exactly discovered, they're invented. Fundamental in this context is not so much a word as it is an analogy. And, don't get me wrong, that doesn't mean particles don't exist. They do. But, a particle is whatever we say it is. reply ithkuil 22 hours agoparent\"Things are named before they are understood.\" -- Matt Strassler reply heresie-dabord 8 hours agorootparentFurther: Things are named, then the linguistic burden actually comes to limit understanding. reply prng2021 23 hours agoparentprevWe're not asking questions about human constructs like what is moral or what is the ideal form of government. We're trying to understand what the most fundamental building block of reality is, which is something objective. Something independent of whether of not people ever existed. So no, countless people around the world aren't wasting their lives researching particles when the answer is simply, it's whatever we say it is. reply woopsn 20 hours agorootparentWhat can we see? Vapor trails, patterns burned into a plate, etc. The evidence of some \"thing\" slowing down, perhaps, but really the environment that slowed it down necessarily changing irreversibly. Irreversible effects cannot be arbitrarily small (apparently). We never see a particle, only effects such as these -- if we did see anything else, well, we couldn't possibly remember. Particles are an attempt to explain/theorize evidence that is fundamentally observational. reply kayo_20211030 22 hours agorootparentprevDid you take the time to read the original piece? Even the smart people can't agree. What gives you the faith that a \"particle\" is not a human construct? What on earth does \"fundamental\" even mean except being the bottom turtle we can see on the particular mountain of turtles at which we're looking. \"Countless\"(?) people around the world are not researching particles. They're doing particle physics as they understand \"particles\". That's how it should be and particles are whatever they say they are. In that field, no measure means no reality. *Unless*, of course, you have faith in some platonic reality. Positive materialists will disagree. reply prng2021 21 hours agorootparentWhen you say particles are whatever we say they are, I assume you believe particles are subjective. I'm saying particles are objective, like a \"wavelength\" as opposed to subjective, like \"morals\". If you are saying even objective things are whatever we say they are, then this is a useless discussion. Obviously every word is defined with other words and all words are human creations. So yea in that sense, literally everything we know of is whatever we say it is. That's a pointless statement to make in response to this article or really ever. reply kayo_20211030 4 hours agorootparentI was saying that I don't know what a particle is, and neither does anyone else, but subjective isn't quite the correct word. What I am sayings is that there are many useful interpretations of what a particle \"is\" that allows science to progress. The theoreticians and the experimentalists use, sometimes even form, those interpretations to help them along in their work. I'm saying that they yet remain interpretations, even analogies; and it really doesn't matter whether those folks believe in an objective reality or not, good work still gets done. Maybe it's subjective in that sense, but a particle is whatever they say it is; and nothing more can be said with certainty unless you dive into philosophy. You can measure a perturbation in a field, or a track in a bubble chamber, and call it a particle. Then you can work backwards to an \"objective\" representation, and a particle still becomes whatever you say it is. It's still scientific, consistent, mathematically rigorous, and in line with theory and observation, but it remains a human construct. You can't escape that. Ultimately, one is free to believe that there is some fundamental thing; and, maybe there is and maybe there isn't. > literally everything we know of is whatever we say it is Of course \"literally everything we know of is whatever we say it is\". That, precisely, is my point. It most certainly does not mean that we can just state anything as true unless it is sensible, consistent, observable and verifiable; and expect not to be challenged. reply khazhoux 22 hours agorootparentprevI'll go even further and point out that in 2003, it was proven that particles are not, in fact, the friends we made along the way. reply kayo_20211030 22 hours agorootparentDo expand. What happened in 2023? reply sfink 20 hours agorootparentYeah, I don't get that, because it seems to me that the friends you make along the way are mathematically indistinguishable from particles being real and having properties. It's a distinction without a difference. Or at least, I'm interpreting \"the friends you make along the way\" as the sum total of the effects of a particle on the surrounding world. Saying \"the particle doesn't exist, but it has effects X, Y, and Z\" is the same as \"the particle exists and has effects X, Y, and Z\". If a distinction is not observable, then it's meaningless to quibble over whether it's \"real\" or not. (Which all just proves that my interpretation isn't the one you were using....) reply fragmede 21 hours agoparentprev [–] discovery vs invention is a question in mathematics as well. trying to say they're invented isn't clever, it's just a trick of language, like how gravity is merely a theory. particles are theorized to exist via theoretical physics and math, and then tested for experimentally. or as the saying goes, all models are wrong, some are useful. reply kayo_20211030 21 hours agorootparent [–] I can't find the reference at the moment. But, I think it was about the \"creation\" of the quark. (I'll find it eventually). Either way, a search for reality, no matter how far we've progressed thus far, is either a search for a platonic reality, or an experimental reality. The former is \"discovery\" and the latter is \"invention\". It really doesn't matter. I don't think, right now, we're quite smart enough to pry into the mind of the universe, so we'll keep \"inventing\" things until we actually approach \"discovery\" asymptotically. Maybe we'll get there, but we've never been closer :-) reply geye1234 20 hours agorootparent [–] It's both discovery and experiment (but not invention). Forms are real, but not in the way Plato thought. He thought they existed in some empyrean realm. In reality, they exist in objects themselves. We discover what a thing's form is by experimenting with (or on) it. Aristotle was largely right. The big philosophical problem with much of this is that people assume that the smallest things are the most fundamental. So people think that stuff, whatever that stuff is, is fundamentally made up of much smaller stuff, and that stuff is fundamentally made of yet smaller stuff, and so the smaller you get, the more fundamental you get. And so (they think) if you want to work out what is really going on at any layer of reality, you need to figure out what the smallest possible things are. Yet this is ultimately a philosophical posit -- it's not empirically-informed. There's no good reason for thinking it. To be clear, none of this is about physicists doing physics. It's about the philosophy that many people bring into, and therefore take away from, these kinds of discussions. reply kayo_20211030 5 hours agorootparent [–] I found the reference. It was an interview with Freeman Dyson where he said \"quarks were invented\", and I don't think it's a slip of the tongue. It's at 3:17 in this video of an interview with him https://www.youtube.com/watch?v=hV41QEKiMlM It's representative of a view that there's a thing (a particle) that explains another thing (a force) that was consistent with both theory and experiment. Thus, a quark could be a particle, and it was whatever the experimentalists and theorists said it was, however it was measured or contemplated. For a deeper meaning it becomes an exercise in hermeneutics i.e what does it mean when we say \"particle\"? That was the point of the original piece - there is no uncontested view of a particle's form, should one even exist. Each field, in order to advance, finds it useful to interpret it, or think about it, in different ways. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Physicists' understanding of particles has evolved from pointlike objects to more complex concepts like collapsed wave functions, quantum excitations of fields, and representations of symmetry groups.",
      "Quantum mechanics and quantum field theory describe particles as both waves and excitations of fields, with properties defined by symmetry groups such as the Poincaré group.",
      "Modern theories, including string theory and the it-from-qubit hypothesis, propose particles might be vibrating strings or holograms of qubits, while amplitudeologists aim to simplify particle interactions for a deeper understanding."
    ],
    "commentSummary": [
      "The discussion revolves around the complex nature of particles in quantum mechanics, challenging the traditional view of particles as tiny balls and introducing them as excitations in fields.",
      "The term \"particle\" is considered misleading; in quantum field theory (QFT), particles are better understood as mathematical abstractions or excitations in fields, not physical objects.",
      "The conversation highlights the ongoing debate and lack of consensus among physicists about the fundamental nature of particles and fields, reflecting the evolving understanding in modern physics."
    ],
    "points": 156,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1726946434
  },
  {
    "id": 41617663,
    "title": "'I Don't Want to Die.' He needed mental health care. He found a ghost network",
    "originLink": "https://www.npr.org/sections/shots-health-news/2024/09/21/nx-s1-5120543/mental-health-care-parity-insurance-ghost-network",
    "originBody": "Shots - Health News 'I Don’t Want to Die.' He needed mental health care. He found a ghost network September 22, 20247:00 AM ET From By Max Blau Vanessa Saba for ProPublica If you or someone you know may be considering suicide or be in crisis, call or text 988 to reach the 988 Suicide & Crisis Lifeline. Early one morning in February 2023, before the sun rose over Phoenix, Ravi Coutinho went on a walk and, for a brief moment, thought about hurling his body in front of a moving bus. He had been feeling increasingly alone and depressed; anxious and unlovable; no longer sure if he was built for this world. Several hours later, Ravi swiped open his iPhone and dialed the toll-free number on the back of his Ambetter insurance card. After navigating the automated voice system, he was routed to a friendly, fast-talking customer service rep with a slight foreign accent. His name was Giovanni. Sponsor Message “How can I help you today?” Giovanni asked. “Hi, I am trying to find a psychiatric care provider,” Ravi said. “So, you are looking for a primary care provider?” Giovanni asked. “No,” Ravi replied, seeming confused. Ravi tried to clearly repeat himself. “Psy-chi-at-ric.” “Psychiatric, all right, so, sure, I can definitely help you with that,” Giovanni said. “By the way, it is your first time calling in regards to this concern?” Ravi paused. It was actually the sixth attempt to get someone, anyone, at Ambetter to give him or his mother the name of a therapist who accepted his insurance plan and could see him. Despite repeatedly searching the Ambetter portal and calling customer service, all they had turned up so far, he told Giovanni, were the names of two psychologists. One no longer took his insurance. The other, inexplicably, tested patients for Alzheimer’s disease and dementia and didn’t practice therapy at all. “I’m a little concerned about all this,” Ravi said. This had not been part of the plan Ravi had hatched a few months earlier to save his own life. Diagnosed with depression and anxiety, and living in the heart of Austin, Texas’ boisterous Sixth Street bar district, the 36-year-old former college golfer had become reliant on a dangerous form of self-medication. Sponsor Message His heavy drinking had cost him his marriage and was on the verge of destroying his liver and his livelihood. His therapist back in Texas had helped him understand how his mental illnesses were contributing to his addiction and vice versa. She had coached him through attempts to get sober. He wanted to save his business, which sold dream vacations to golfers eager to play the world’s legendary courses. He wanted to fall in love again, even have a kid. He couldn’t do that when he was drinking a fifth of a gallon of liquor — the equivalent of nearly 17 shots — on any given day. When all else had failed, he and his therapist had discussed a radical move — relocating to the city where he’d spent his final years of high school. Phoenix symbolized a happier and healthier phase. They agreed that for the idea to work, he needed to find a new therapist there as quickly as possible and line up care in advance. Ravi felt relieved when he signed up for an insurance plan right before the move. Ambetter wasn’t as well known as Blue Cross Blue Shield or UnitedHealthcare. But it was the most popular option on HealthCare.gov, the federal health insurance marketplace, covering more than 2 million people across the country. For $379 a month, his plan appeared to have a robust network of providers. Ravi had moved to Arizona, hoping for a fresh start. Here, he is with his mother, Barbara Webber. Webber Coutinho family hide caption toggle caption Webber Coutinho family Frustrating phone calls like this one began to confirm for Ravi what countless customers — and even Arizona regulators — had already discovered: Appearances could be deceiving. After misunderstanding Ravi’s request for a therapist, Giovanni pulled up an internal directory and told Ravi that he had found someone who could help him. It was a psychiatrist who specialized in treating the elderly. This was strange, considering that Giovanni had asked Ravi to verify that he was born in 1986. “I mean, geriatric psychiatry is not …” Ravi responded, “I mean … I wouldn’t qualify for that.” Sponsor Message Annoyed but polite, Ravi asked Giovanni to email the provider list on the rep’s computer. He figured that having the list, which he was legally entitled to, would speed up the process of finding help. But Giovanni said that he couldn’t email the list. The company that ran Ambetter would have to mail it. “What do you mean, mail?” Ravi asked. “Like physically mail it?” Ravi let out a deep, despondent sigh and asked how long that would take. Seven to 10 business days to process, Giovanni responded, in addition to whatever time it would take for the list to be delivered. Ravi couldn’t help but laugh at the absurdity. “Nothing personal,” he told Giovanni. “But that’s not going to work. “So I’m just gonna have to figure it out.” This baffling inability to find help had tainted Ravi’s fresh start. In the weeks before the call with Giovanni, Ravi had scrolled through Ambetter’s website, examining the portal of providers through his thick-rimmed glasses. He called one after the next, hoping to make an appointment as quickly as possible. Of course, it was unreasonable to expect every therapist in Ambetter’s network to be able to accept him, especially in a state with an alarming shortage of them. But he couldn’t even find a primary care doctor who could see him within six weeks and refill his dwindling supply of antidepressants and antianxiety meds. Days before he was supposed to move to Phoenix, he texted friends about his difficulties in finding care: “Therapists have been 0-4.” “Called ten places and nothing.” “The insurance portal doesn’t know shit.” Ravi Coutinho and his dog, Finn, in March 2023. Coutinho Webber family hide caption toggle caption Coutinho Webber family Ravi didn’t know it, but he, like millions of Americans, was trapped in a “ghost network.” As some of those people have discovered, the providers listed in an insurer’s network have either retired or died. Many other providers have stopped accepting insurance — often because the companies made it excessively difficult for them to do so. Some just aren’t taking new patients. Insurers are often slow to remove them from directories, if they do so at all. It adds up to a bait and switch by insurance companies that leads customers to believe there are more options for care than actually exist. Sponsor Message Ambetter’s parent company, Centene, has been accused numerous times of presiding over ghost networks. One of the 25 largest corporations in America, Centene brings in more revenue than Disney, FedEx or PepsiCo, but it is less known because its hundreds of subsidiaries use different names. In addition to insuring the largest number of marketplace customers, it’s the biggest player in Medicaid managed care and a giant in Medicare Advantage, insurance for seniors that’s offered by private companies instead of the federal government. ProPublica reached out to Centene and the subsidiary that oversaw Ravi’s plan more than two dozen times and sent them both a detailed list of questions. None of their media representatives responded. In 2022, Illinois’ insurance director fined another subsidiary more than $1 million for mental health-related violations including providing customers with an outdated, inaccurate provider directory. The subsidiary “admitted in writing that they are not following Illinois statute” for updating the directory, according to a report from the state’s Insurance Department. In a federal lawsuit filed in Illinois that same year, Ambetter customers alleged that Centene companies “intentionally and knowingly misrepresented” the number of in-network providers by publishing inaccurate directories. Centene lawyers wrote in a court filing that the company “denies that it made any misrepresentations to consumers.” The case is ongoing. And in 2021, San Diego’s city attorney sued several Centene subsidiaries for “publishing and advertising provider information they know to be false and misleading” — over a quarter of those subsidiaries’ in-network psychiatrists were unable to see new patients, the complaint said. The city is appealing after a judge sided with Centene on technical grounds. Even the subsidiary responsible for Ravi’s plan had gotten in trouble. Regulators with the Arizona Department of Insurance and Financial Institutions found in 2021 that Health Net of Arizona had failed to maintain accurate provider directories. The regulators did not fine Health Net of Arizona, which promised to address that violation. When ProPublica asked if the company had made those fixes, the department said in a statement that such information was considered “confidential.” Sponsor Message These were exactly the type of failures that Ravi’s mother, Barbara Webber, confronted as the head of an advocacy group that lobbied for greater health care access in New Mexico. From her Albuquerque apartment more than 300 miles away from her son’s his new, 12th-floor studio, she listened to Ravi vent about how hard it was to find a therapist in Phoenix. Barbara Webber and Ravi Coutinho, in his childhood. Webber Coutinho family hide caption toggle caption Webber Coutinho family Ravi was Barbara’s only child, and they had always been close. In the seven years since Ravi’s dad died, they’d grown even closer. They talked on the phone nearly every day. Barbara was used to supporting Ravi from afar, ordering him healthy delivery dinners, reminding him to drink enough water and urging him to call crisis hotlines amid panic attacks. But when Ravi crashed at her apartment while waiting to move to Phoenix, she saw more of his struggles up close. At one point, she called 911 when she feared for his life. Despite her desire and ability to help him, Ravi didn’t want to stay with his mom for any longer than necessary. He didn’t want to feel like a teenager again. Barbara understood her son’s desire for independence, and when he first encountered insurance barriers, she drew from her expertise and coached him through ways to try to get past them. But by the middle of February, a few days after Ravi settled into his new place, there was no good news about his mental health care. She felt the need to step in. So, she called Ambetter to try to get better information than what Ravi was looking at online. But Khem Padilla, a customer service rep who seemed to be working at a call center overseas, couldn’t help her find that information. She then asked Padilla to send referrals to therapists. Sponsor Message When Padilla followed up, he only sent phone numbers for mental health institutes, including one that exclusively served patients with autism. “I wish that everything will work together for you,” Padilla wrote in an email to Barbara and Ravi on what happened to be Valentine’s Day, “and [don’t] forget that you are Loved.” Loneliness is one of the strongest forces for triggering a relapse in someone addicted to alcohol, and Ravi’s early days in Phoenix provided a dangerous dose. His old friends were often busy with work and family. He hadn’t found his way to a new Alcoholics Anonymous group yet. And he struggled to find matches on dating apps. (“Phoenix Tinder is a wasteland,” he told one friend.) His only consistent companion was Finn, a half-Great Pyrenees with a thick coat of fluffy white hair, whom he took on long walks around the city. “His unconditional love brings me so much joy,” he’d told his mom. Alone in his apartment with Finn, vodka within reach, Ravi felt guilty about calling his loved ones for help. Even though his mom and his friends would pick up the phone at just about any hour, Ravi hated the idea of bothering them. But he couldn’t resist after he hung up with Giovanni, the customer service rep. That afternoon, Feb. 22, he fired off a frustrated text message to his mom. “How is it this hard?!” Ravi seethed. Barbara’s next move was to reach out to a member of her nonprofit board who happened to work for a Centene company. The board member helped get Ravi a care manager, a person who works for the insurer to help patients navigate access to providers. The care manager got him a referral for a psychiatric nurse practitioner, but she wasn’t able to connect him to a therapist. Sponsor Message Without therapy, Ravi’s descent took on a momentum of its own. A close friend from high school, David Stanfield, was watching it all unfold. Ravi had always made David feel like they could pick up where they’d last left things. But this new withdrawn person, who would break into a sweat on a crisp night in the 60s, was a far cry from the guy he once knew. Ravi was beginning to remind David of his brother-in-law, who had died of a drug overdose a few years earlier. So when Ravi sent a series of distressing texts, indicating that he had relapsed, David and another friend staged an intervention and took Ravi to the hospital. But Ravi wondered what good another detox would do if it didn’t help him combat the root causes of his addiction through therapy. He was also worried that it would get in the way of his ability to work; Ravi was still booking some golf vacations through his business and figured he would have to surrender his phone during a rehab stay. Instead, Ravi sated his withdrawals by feeding his body more alcohol, giving way to a March whirlwind of blackouts, massive hangovers and despondent texts to friends. When Ravi showed up to a baseball game looking pale and disheveled, a friend’s young son turned to his dad and asked: Is Ravi OK? By early April, almost two months had passed since Barbara’s first call to Ambetter alerting them that Ravi was having trouble finding a therapist. Ambetter was obligated by state law to provide one outside of its network if Ravi couldn’t find one in a “timely manner” — which, in Arizona, meant within 60 days. Vanessa Saba for ProPublica Within that span, its own records showed, he’d wound up in the emergency room seeking treatment for alcohol withdrawal and called a crisis line after he had thought about ending his life. Yet despite 21 calls with Ravi and Barbara, adding up to five hours and 14 minutes, the insurer’s staff had not lined up a single therapy appointment. Sponsor Message Smith called Ravi four times over two weeks, right as his mental health crisis worsened. When he didn’t respond, she closed his case on April 7. Smith did not respond to multiple requests for comment or to questions about what information she tried to share with Ravi on these calls. As Ravi’s attempts to find a therapist slowed down, his descent accelerated. Barbara didn’t expect to spend Mother’s Day with Ravi. But after he told his uncle that he was having visions again of jumping in front of a speeding bus, she boarded a last-minute flight to Phoenix on May 12 and settled into his couch where she could watch him as he slept. She was roused by his flailing limbs. He was having a seizure. Paramedics rushed Ravi to the hospital, the second time in the past month and fourth since the year began. Doctors gave him benzodiazepines, Valium and Librium, to treat the seizures and anxiety caused by his alcohol withdrawal. “Mom,” Ravi told Barbara, “I don’t want to die.” One kind of treatment suggested by hospital staff, an intensive outpatient program, seemed the best fit. It would allow Ravi access to his phone for his business purposes. But neither Ravi nor Barbara could get a list of in-network programs from Ambetter, nor could they find them in the portal. As Ravi called every program he could locate in metro Phoenix, and failed to find a single one that took his insurance, Barbara decided to pester her board member again. (The board member did not respond to multiple requests for comment.) Barbara Webber and Ravi Coutinho hide caption toggle caption A few days later, someone with Centene provided the names of two in-network programs out of the dozens in Arizona. Only one offered the individual therapy Ravi was looking for. That Friday, May 19, Barbara rode with Ravi to Scottsdale, where the intake staff at Pinnacle Peak Recovery drug-tested him. He tested positive for the benzodiazepines the hospital staff had administered following his seizure. Treatment programs sometimes restrict patients who test positive for those drugs because of the liability, experts told ProPublica. Pinnacle Peak Recovery’s staff urged Ravi to come back the following week. Barbara flew home, hopeful that Ravi would be admitted. (Pinnacle Peak Recovery did not respond to multiple requests for comment.) Sponsor Message On Monday morning, Ravi wrote the date, May 22, on a sheet of paper. He tore it out of a notebook, held it up to the side of his face and took a selfie with it. It was a way of marking time as well as a milestone: the first day of his newfound, hopefully permanent sobriety. Ravi's companion, Finn. hide caption toggle caption When he returned to Pinnacle Peak, however, he tested positive for benzodiazepines again. The second rejection hurt more than the first. Three days later, Ravi went back a third time; the drugs were still in his system. “I don’t know what else to do,” he told Barbara over the phone. “I am screwed.” The answer of what else could be done was, unbeknownst to Ravi, buried in the fine print of his own insurance policy. Ambetter’s contract promised to find an out-of-network treatment program and make it available to Ravi, so long as Ambetter’s own employees decided that it was in his “best interest.” Even though Barbara hadn’t read the fine print either, she had a sense that Ambetter could do more to help Ravi. So she pulled up the number of the last Centene employee she’d spoken with. In a text message, Barbara expressed concern that the window to get Ravi help was closing. She was certain that, without more medical support ahead of admission to a treatment program, Ravi was bound to relapse. If that happened, Barbara pleaded, there was a good chance that he would have another seizure. She warned that he might even die. Barbara awaited word on what to do next. She got no response. The following morning, May 27, she drafted a message to Ravi. She described her visceral memory of his recent seizure. She wrote that he wanted nothing more than for Ravi to be around for the rest of her years. She promised to support him no matter what. If he kept going, he could find peace with Finn and find someone to love. But he had to keep going — not for her, not for Finn, not for his friends, not for anyone else. “I love you,” she wrote, “but you must love yourself.” Sponsor Message She hit send. Ravi didn’t reply right away, which was unusual. An hour passed, then another. As the afternoon gave way to evening, Barbara called three times, unable to reach him. She tried to reach Phoenix’s 911 dispatch but couldn’t get through. Not knowing what else to do, Barbara called David Stanfield, whom Ravi had asked to be his local emergency contact. He agreed to call 911. A police officer knocked on Ravi’s door and could hear Finn barking from the other side. When no one answered, the officer called David, letting him know that the police couldn’t enter the apartment without the building’s security guard, who wasn’t around right then. David and his fiancée, Aly Knauer, drove over to Ravi’s. A security guard, who had just gotten back from his rounds, let them into the 12th floor apartment. When the guard unlocked the door, Finn squeezed past and darted out. As Aly grabbed Finn, David peered inside, calling out his friend’s name. Four empty vodka bottles were strewn across the apartment. The Murphy bed was folded up against the wall. No one seemed to be there. David glanced toward the window that frames the Phoenix skyline and felt a sense of relief. His friend might still be alive. When he turned to leave, he looked again at the Murphy bed and saw it was slightly ajar. As he leaned closer, he realized his worst fears were confirmed. Ravi was trapped between the wall and the bed, lifeless. Ravi Coutinho was 36 years old. He’d been discharged from the hospital two weeks before. About the Reporting This story was pieced together from more than 1,000 pages of Ravi’s medical records and insurance files; audio recordings of Ambetter customer service calls; police reports and photos; court filings from three states; reports from insurance regulators; Ravi’s texts, phone logs, social media messages and emails; and more than 25 hours of interviews with people who knew Ravi best. It was also guided by a lengthy chronology of key events that Barbara had compiled in the months after her son’s death. One thing she couldn’t bring herself to do: read the autopsy report. She asked her sister to summarize the findings, which ProPublica obtained and reviewed. Ravi’s death was ruled an accident, likely due to complications from excessive drinking. ProPublica sent a detailed account of Ravi’s attempts to get help to 12 legal, insurance and mental health experts. They independently identified a variety of problems, including Ambetter’s provider directory inaccuracies, its network inadequacy and its customer service shortcomings. ProPublica is investigating mental health care access. Share your insights. ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive its biggest stories as soon as they’re published. mental health mental health parity Health Insurance addiction treatment Facebook Flipboard Email",
    "commentLink": "https://news.ycombinator.com/item?id=41617663",
    "commentBody": "'I Don't Want to Die.' He needed mental health care. He found a ghost network (npr.org)139 points by jameslk 3 hours agohidepastfavorite76 comments todfox 22 minutes agoI once had an insurance plan with a grossly inaccurate provider directory. This cost me a bit as the doctor they suggested to me, verbally on the phone, turned out to be out of network. They later told me it was the doctor’s responsibility to remove themselves from the directories and they often fail to do that. If they’re so incapable of maintaining an accurate list of providers on their own, how did the insurance company know to reject that claim so quickly? They are simply liars and fraudsters. I began calling random doctors in the directory and one even told me he tried to get his name removed for three years. Anybody at the insurance company could clean up the directory once a quarter. They know the directories are inaccurate. It makes their network look bigger. Health insurance middlemen need to be eliminated already. reply xyst 1 hour agoprevIt’s beating a dead horse at this point, but private health insurance is quite possibly the worst middleman I have ever dealt with. They not only make the patients life worse but the doctors and hospitals as well. Dealing with insurance means small practices need a dedicated office staff to file the right paperwork and get paid for each patient visit. reply maxverse 1 hour agoparentHands down the worst and most stressful part of living in America, and the only reason I - a child of immigrants who are grateful to have built a life here - would consider moving somewhere else reply _DeadFred_ 6 minutes agoparentprevI recently had to juggle 'do I pay out of pocket for this skin cancer surgery and save $600 on the $3000 total, or use insurance so that what I pay goes towards my deductible (would have fulfilled 30% of my deductible for the year). It was better to pay out of pocket. In the last 30 years we went from affordable ambulances and insurance that worked to people too scared to use ambulances and insurance that our doctors encourage us not to use. Glad we are letting the free market work it's magic. Thankfully the market worked in our small town, the ambulance company went broke and we joined together as a community and now have county services that are not only better but much much cheaper and don't see peoples' emergencies as a profit center for some located elsewhere mega-corp. Weird how inefficient small town hick local government can now make work (and work better) what mega corp for profit 'big brains' couldn't. Side note: USE SUNSCREEN PEOPLE! I wish I could go back and slap dumb 'too cool for sunscreen' Santa Cruz surfer kid me. reply SkyPuncher 20 minutes agoparentprevMy wife is a doctor and dealing with insurance is one of the worst parts of the day. Far too often, she knows that a certain treatment plan is going to be the only and most effective plan for a patient. However, insurance will require her to exhaust several other options first. It seems the hope is enough patients give up on treatment that they never actually seek the proper care. They'll just get chucked the \"cheap\" option again in the future. reply gigatexal 27 minutes agoparentprevThis could be said for all of private insurance: they're got a literal mandate to not pay out, pay out far below what is owed, or delay delay delay because what they keep is pure profit. I wonder if for a public provider if the incentives are better. reply wintermutestwin 1 hour agoparentprevIf I could recover the cost of my time in small claims court, I’d be making a hefty sum from these parasitic insurance companies that constantly require me micromanaging their antics. reply yieldcrv 1 hour agoparentprevI’m curious about why it wasn’t considered to switch insurers, the article has do many details on many processes and I felt this one was not investigated It’s definitely hard to cancel them and the expense adds up to pay premiums for two Was it a possibility? reply siliconwrath 1 hour agorootparentI obviously don’t know the specifics, but US healthcare plans only let you change during specific enrollment periods or qualifying life events. It’s possible he wasn’t able to. Additionally, it sounds like he picked what appeared to be the best rated plan available to him on the market. Others may have been even worse or prohibitively expensive. reply yieldcrv 1 hour agorootparentYes I know it’s hard to cancel outside of enrollment periods, you can still have multiple Adding in the last minute flights and commitments throughout this article, I would say for other people that at some point the calculus can be re-evaluated to find that paying the premiums would be worth it. The loved ones can pay for that instead of last minute flights and time off reply klabb3 36 minutes agorootparent> you can still have multiple This can’t be for real. The only reason you pay for insurance is in case something happens. It has no value outside of that. That’s why it’s called insurance. And medical insurance in the US is very expensive, so much that a large part of the population can’t afford it. If you don’t get care when you need it, it’s worse than no insurance - now there’s less money left to pay out of pocket to the only places that will take you on in time. The solution to being scammed is not to sign up for another scam. reply gomerspiles 23 minutes agorootparentAside from how ridiculous it would be to buy double coverage, it can just make the problem worse with both companies trying to argue they aren't the primary and that their doing anything may be duplication of coverage. But some more paper work should resolve that once you are dead. reply bumby 1 hour agorootparentprevThe man in the article was living in a studio apartment. I doubt he was in a position to be paying the equivalent of a mortgage for many insurance options. Even if he could, I’m not sure that’s the system/solution we’d want. reply siliconwrath 1 hour agorootparentprevIf he had a lot of money he could have just paid out of pocket for care. It sounds like he couldn’t afford it. reply geraldwhen 49 minutes agorootparentprevOr pay cash for therapy. Or have the parent pay cash for therapy. Options existed, but there are none now. reply Apocryphon 47 minutes agorootparentMost therapists don’t accept insurance. The problem then is that their services are expensive out-of-pocket. reply mystified5016 1 hour agorootparentprevYou have the choice of a couple of large companies that will deny every claim always for any reason, and a few smaller players who no doctor in your area has heard of or can accept payment from. So yeah, you have a 'choice' reply Der_Einzige 1 hour agoparentprevIntegrated providers, such as Kaiser Permanente, have far fewer issues related to the problems brought up in this article compared to most others. Nothing like this would ever happen in a Kaiser hospital. reply spease 45 minutes agorootparent> Nothing like this would ever happen in a Kaiser hospital. Kaiser’s mental health services were so bad that their providers went on strike a couple years ago. They’re paid a fraction of what they could make in private practice. https://www.healthcaredive.com/news/kaiser-strike-mental-hea... reply Teever 27 minutes agorootparentWhat barriers prevent them from working in private practice? As a non-American I assume that Kaiser implements some sort of barriers to stop smaller competitors from rising up? reply slantedview 1 hour agorootparentprevKaiser had its own very sordid history of denying people mental healthcare which has led to deaths, fines by the state of California, and new legislation. reply tmpz22 1 hour agorootparentMy family has gone through three generations of Kaiser in California and it has its own tradeoffs I promise you and mental health is one of it’s most frequent complaints. reply fragmede 36 minutes agorootparentprevBy \"nothing\", are you referring to the incident in Santa Rosa in 2015? https://nuhw.org/therapists-demand-action-in-response-to-tra... Or the one in San Diego in 2021. https://www.sandiegouniontribune.com/2021/03/31/kaiser-patie... (https://archive.is/wF1bD) Or maybe you're referring to the incident in Santa Clara in 2022. https://www.medpagetoday.com/nursing/nursing/98534 Of course, those incidents aren't like the one mentioned in the article. The ones I linked are just a gross failure of Kaiser's mental health resources leading to three people's deaths instead. At least they didn't have to deal with a ghost insurance network though! reply Analemma_ 44 minutes agorootparentprevI don't think you know Kaiser very well. Kaiser's vertical integration is really good at some things, but the other edge of this sword is that if you need treatment in a specialty they're bad at, you're fucked: there's nowhere else to go because getting out-of-network care with Kaiser is nigh-impossible. And mental health is infamously one of the things Kaiser is very bad at. Their mental health coverage is so inadequate that if I was choosing between insurance providers I'd pick a different one for that reason alone. reply jimbob45 1 hour agoparentprevWorks well for me and I’m not even wealthy. I get the feeling that the universal healthcare advocates have convinced everyone that things are far worse than they actually are. reply olalonde 29 minutes agorootparentThe problem is that it's not a free market either thanks to medical licensing laws, monopoly control over the accreditation of U.S. medical schools, etc. The supply of doctors was even once artificially limited as a matter of policy due to a fear that there would be a \"surplus\" of doctors (aka fear that doctor salaries would go down). > The Graduate Medical Education National Advisory Committee convened by the Secretary of Health and Human Services (HHS) issued a report in 1980 warning that there would be a “surplus of 70,000 physicians by 1990” if steps were not taken to bring supply and demand into balance. > This near freeze was enforced by the Association of American Medical Colleges (AAMC) and the American Medical Association (AMA), the two sponsors of the Liaison Committee on Medical Education, which is the sole accreditor of allopathic medical schools recognized by the U.S. Department of Education. The decision not to expand the number of slots in U.S. allopathic medical schools remained in place until 2005, when the AAMC and AMA changed their minds from declaring an impending doctor glut to warning of a looming doctor shortage. https://www.heritage.org/education/report/why-dont-us-medica... reply treyd 55 minutes agorootparentprevI just had an antibiotic prescription denied because my insurance company thought I had a refill in 5 days, despite the last time I was prescribed it was as a one-off course, 8 years, and 3 insurance companies ago. reply betaby 1 hour agorootparentprevCanada got worst combination of private/public, not-really-a-singly-payer, not that universal care. reply dghlsakjg 46 minutes agorootparentCanadian healthcare isn’t a monolith. It is managed at the provincial level, so it varies between the provinces, I suspect this comment is more of political jab than a cohesive critique of the dozen or so systems at play. Glad for you to clarify your meaning though. That said, my experience in BC is that my tax burdens are much, much lower than my tax burden + insurance cost in the states. I’ve never had issues accessing healthcare in a way that impacted my health (which is not true in the states), although I have had to wait, since it is a triage system based on need, rather than ability to pay. reply fakedang 56 minutes agorootparentprevI mean all you need is regulating the insurance industry in America. We do a fine job in Switzerland with private insurance, and yet they're some of the biggest insurance players globally. The government HEAVILY regulates health insurance. Also, health insurance isn't tied to employment. reply insane_dreamer 45 minutes agorootparentprevnice sample size of 1 reply incangold 56 minutes agorootparentprev“Works on my machine, ticket closed” reply ponector 40 minutes agoparentprevPublic health insurance in other countries are not better. For instance, I pay 10% of my income for it and in some cases queues just to start treatment are for few years. And it does not cover teeth, implants, vaccines. If case is urgent, like a cancer, it can take 6+ month to actually start treatment due to all paperwork and queues everywhere. reply weakfish 36 minutes agorootparentSounds like poor implementation, not a reflection on the concept. Which sucks - don’t get me wrong, that needs to be improved! Private insurance has all the wrong incentives, and IMO profit should never be linked to the health of a human for that reason. reply pj_mukh 21 minutes agorootparentIn cases of public healthcare the problem is almost always a shortage, doctor shortage, bed shortage, nurse shortage, tech shortage etc. The perfect system is a public healthcare with true abundance. reply sokoloff 30 minutes agorootparentprevThis could be a tech-heavy lens, but I feel like some of the innovation in healthcare is driven by a profit motive (and funded by same in a lot of cases). I don’t know if removing all profit from the system results in an optimal outcome, but I suspect it does not. reply RobotToaster 12 minutes agorootparentMy understanding is that most early stage medical research on finding specific targets for drugs, and often drug discovery itself is publicly funded. Private funding only really takes over once you start trialling the drug in animals and then humans. reply sokoloff 8 minutes agorootparentIt’s not just drugs; look at the difference in medical equipment now versus 40 years ago. I’d sure rather pay a little extra and have 2024 equipment than 1984. That 2024 equipment exists because someone imagined they could make a profit if they successfully created it. reply lostlogin 30 minutes agorootparentprevWhat country is this? reply knodi123 1 hour agoprevI've dealt with ghost networks too, with BCBS of CA. They provide you a directory with a thousand options, and I was already planning to go with the first provider that would have me. Had to call 15 before I got one that wasn't out of business, not accepting new patients, or not actually accepting the provider that gave me their number. So I can only assume that roughly 6% of their providers are actually real. reply robodan 44 minutes agoparentI assume BCBS is Blue Cross Blue Shield; which is one of the largest insurers. Can I ask how big is the city you live in (or near)? It seems like big cities have options, but small towns starve for medical help. reply ipaddr 47 minutes agoparentprevI wouldn't assume based on your sample. Most people start at the top so more of those would be filled. Businesses that add AAA to their name to appear at the top of the yellow pages are more likely to close. reply olalonde 1 hour agoprevHealth insurance in the US sounds like such a scam. Is it not possible to pay for a psychiatrist directly or to renew prescriptions without seeing one? reply NeuroCoder 43 minutes agoparentAs far as psychiatric health goes, its highly variable. I have a friend that I talked to last night that is working with Medicaid to revamp how they bill this kind of stuff because a lot of it is rejected by insurance since it doesn't fit within a clearly covered area. Getting small areas or sustainable coverage established like this can sometimes be a path forward to broader adoption. But that's still pretty optimistic thinking reply Trasmatta 34 minutes agoparentprevI'm paying $250 per week out of pocket for a therapist in NYC. reply wepple 1 hour agoparentprevThe former, yes, but they’re far more expensive. The latter, kind-of. You have to have regular check-ins, dependent on the type of medication reply olalonde 57 minutes agorootparentIt's crazy that you can't get a prescription for under 379$. Something is seriously broken with that system. I'm pretty sure I could get one for 1/10 that price where I live, without insurance. reply GuinansEyebrows 43 minutes agorootparentWhen I first moved to the Netherlands, I had to visit a pharmacy to purchase insulin. I was prepared to spend nearly half my paycheck on a months supply and deal with the headache of not having a prescription on file before I’d established insurance. I walked out the door with a box of Lantus pens having spent 80€. That would have cost me about $600 in the US at the time, if they’d even dispense to me without a prescription. reply NeuroCoder 33 minutes agoprevWorking in a psych unit trying to improve this process is really eye opening. Some key doctors have put in the effort to turn the inpatient floor from weeks of being trapped to identifying these situations where people simply need actual support established. There's even a special 72 hour turn around program recognizing this situation where the system has failed and what we need is rapid stabilization and setting up the proper support for the person to have therapists, medicine, and support groups while still living their lives. It still has flaws, but there are people that care and are working overtime to fix the system. It's just a very slow process. reply bryan_w 45 minutes agoprevThe leaders at ambetter should be in jail. The world would be better off if they didn't exist. reply hedora 44 minutes agoprevIt's common to hit similar road blocks when trying to get a primary care physician around here (and the in-network specialists won't see you without a referral from a primary care physician, rendering your health insurance worthless for non-emergency care). The US really needs to get rid of the health insurance industry. Single payer would work as would standardized pricing combined with \"if the doctor is licensed the insurance company must accept the bill\". Barring that, there should be SLAs regarding for one-shot online searches or one phone call lookup of in-network care providers. For example, there could be a guarantee that the top three hits of at least 99% of such attempts each contain the phone number of a doctor's office that is accepting new patients and provides relevant care. If the insurance company falls below that bar, then it should have to refund all the premiums they collected that month (since there's no way to know which customers deferred care due to this bullshit), or be hit with some other fine that'd actually be material to their earnings. reply paulryanrogers 0 minutes agoparentAfraid single payer system would just be hollowed out by conservatives (funded by PE), as it has been in the UK. Doubtful the US has the buy in and the discipline to maintain such a system. Sadly our private insurance market is an oligopoly that can obscure pricing and collude to increase prices. So it's the worst of both worlds. reply dgoldstein0 1 hour agoprevThis is messed up. Feels ripe for a wrongful death lawsuit at the minimum, and likely some regulatory enforcement against this scam insurance reply anjel 1 hour agoparentPerhaps you've never attempted to sue your once and former insurer for contract breach... You probably get the implied frustration and futility, but remember to include the impact of same on your yet-to-be treated illness. reply dgoldstein0 1 hour agorootparentNo I have not sued anyone. But my point is now that he's gone, maybe his family could sue. Not sure that'll really fix anything though. Laws and regulations with real enforcement could though. reply Bengalilol 1 hour agoprevThis is so bad : we can measure a country's strength by its weakest link. There are some major loopholes here. reply keybored 1 hour agoprevI almost teared up. reply _dark_matter_ 1 hour agoparentI did as well. My cousin passed away in his early 30s from alcoholism - this story hit close to home. reply EMM_386 1 hour agoprevI've never heard of them. Then I read the bring in more revenue then \"Disney, FedEx or PepsiCo\". Wait, what? Oh, right, subsidaries. And, of course, a giant in Medicare Advantage. That company sounds ... very bad and needs to start doing what they are supposed to be doing and stop playing games. People are looking for help and they are looking for ... well, how to squeeze out more money I suppose. \"You need psychiatric care? Sorry, we're a bit busy trying to make more money. Priorities and all.\" \"Someone died due to our lack of caring about any of that? Sorry to hear that. Listen, gotta go. (click)\" Sue them into the ground. reply lotsofpulp 48 minutes agoparentI don’t think the article is right. >One of the 25 largest corporations in America, Centene brings in more revenue than Disney, FedEx or PepsiCo, but it is less known because its hundreds of subsidiaries use different names. https://companiesmarketcap.com/largest-companies-by-revenue/ By what measure is Centene one of the US’s 25 largest corporations? Its way down at #519 in global market cap rankings, which means it is nowhere near top 25 in the US by market cap. It has $157B in revenue, so maybe that gets it close to top 25, though I still doubt it. It has less than 70k employees, which is not near top 25 either. And there are 5 other managed care organizations (aka health insurers) doing more business than Centene (UNH, Elevance, CVS, Cigna, and Humana). It has miniscule profit margins (1.56%) because it is paying out almost all of its revenue to healthcare providers (90% medical loss ratio). https://www.macrotrends.net/stocks/charts/CNC/centene/profit... https://www.healthcaredive.com/news/centene-medicaid-redeter... If this business approved more claims or paid more for the claims, it would either go out of business or become a charity. Or it would have to increase premiums. reply s0rta 6 minutes agorootparentIt's 25th on the fortune 500 list reply aziaziazi 1 hour agoparentprev> bring in more revenue then \"Disney, FedEx or PepsiCo\". > Sue them into the ground Good luck with that :-) reply ars 54 minutes agoprevIt's easy to blame the insurance companies, but doing so misses the more important detail: There aren't enough providers. No amount of complaining or regulating the insurance companies is going to change this. reply Apocryphon 36 minutes agoparentThey should still be blamed by misrepresenting the lack of supply in their network. But certainly, medical professions probably need reform to alleviate supply, then of course you get into knock-on effects ranging from malpractice suits/tort reform to medical school tuitions. On the subject of mental health, it almost feels like we need a Manhattan Project of sorts to deal with the mounting crisis. reply yieldcrv 1 hour agoprev> what good another detox would do if it didn’t help him combat the root causes of his addiction through therapy that’s how I feel about suicide hotlines and the random placement of suggesting people call them reply Apocryphon 57 minutes agoprevNot to get too ideological, but libertarian proponents tend to claim that businesses with poor service will just naturally be outcompeted. Those who go as far as to suggest that fraud need not be prosecuted by statist regulation will say that dishonest companies will have bad reputations and customers will naturally not patronize them. Makes me wonder what trying to build such a reputation system would look in practice. Consumer Reports manages to hang on as a publication but not everyone consults it, and there are so many more review sites these days of varying quality, impacted by AI/outsourced copywriting. And when you deal with an industry as dominated by a few monolithic oligopolies like health insurance or phone service- what is more bad publicity going to do to AT&T? You can’t even boycott that, especially when they lock in customers to prevent them from easily switching away. reply klabb3 28 minutes agoparentRight you could go against the rest of the world and continue with privatized care. I’m sure it could work much better if free markets were enforced, but it’s very much the opposite. US markets are extremely entrenched, especially in highly regulated sectors. Markets are cornered by PE, by acquisitions, by frivolous lawsuits, any means possible to get dominance. And there’s no functioning anti-trust law, so you get the opposite of free markets. Even Milton Friedman claimed that governments have a necessary obligation to interfere to preserve free markets. reply wantsanagent 1 hour agoprev [–] I've encountered this in the Pittsburgh area with AHN. Trying to get a provider results in printed lists of outdated numbers, practices not accepting patients, etc. Frankly I think we need to start breaking laws. A startup needs to offer straight up good care and fuck the web of infinite regulations which support America's for profit health failure. Doctors can lose their licenses pretty easily so it's going to have to be a straight tech play. Offer as-good-as-possible care entirely outside of the medical profession. AIs are getting good enough that despite the obvious errors they make they are still better than the nothing-burger of care we get here. reply dgoldstein0 1 hour agoparentI don't see how the law is the problem here. The problem sounds like insurance companies that dodge their duties. We need them to be held accountable. We likely also need the general cost of doing business in healthcare to go down - which they are partly responsible for due to throwing up so many barriers for them to actually pay for anything. reply bumby 1 hour agorootparentSteelmanning the OP (which I’m not sure I agree with), it’s possible they are alluding to regulatory capture, implying the laws are crafted to benefit for-profit companies first and patients second. However, I’m not sure going in the direction of less regulation would help. It’s like saying “The for-profit healthcare companies have too much power, so let’s just give them more power.” reply jfengel 1 hour agorootparentIt's not so much a capture as a hodgepodge. There are a great many interests, and even in a good faith environment it would be a challenge to get everything right. Add to that the fact that it's not a good faith environment. There are many forces, not even connected to the industry, who fight to lower prices at any cost. Even if it means finding out too late that you're not actually buying anything at all. The laws and regulatory environment are \"the best compromise people were able to get at the time\" rather than any kind of cogent plan. reply akira2501 1 hour agorootparentprev> We need them to be held accountable. We likely also need the general cost of doing business in healthcare to go down These two statements are at odds with each other. > responsible for due to throwing up so many barriers To me, it's obviously lack of competition that's the problem, you don't want to punish crappy providers, you want to subsidize new ones so the market is flooded with options. Which can be done right after we solve the monopolization problem in health care service providers, medical equipment providers, and \"pharmacy benefit managers.\" reply jfengel 1 hour agoparentprevIt's not like starting up a food truck. A health care operation is vast, and no existing providers are going to break the law and risk losing their licences. You would need to create an entire parallel network. It would cost tens of billions, possibly hundreds. reply insane_dreamer 44 minutes agoparentprev> offer straight up good care and fuck the web of infinite regulations which support America's for profit health failur they'd get immediately sued out of existence by the large vested interests reply antisthenes 1 hour agoparentprev [–] > Frankly I think we need to start breaking laws. Who's stopping you? I don't pay my medical bills by default, unless it's my dentist or my primary care provider. Everyone else can go to hell until the system breaks. reply bumby 1 hour agorootparent [–] Most large systems can absorb a certain level of free-loaders and remain stable by shifting that burden to other people. What you’re advocating is a collective action problem and will just make matters worse for everyone else. Unless you have a way to create a critical mass of similar behavior, it’s tantamount to a selfish action that benefits you to the expense of others. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ravi Coutinho faced significant challenges accessing mental health care through his Ambetter insurance due to a \"ghost network\" of unavailable providers.",
      "Despite multiple attempts by Ravi and his mother, Barbara, to find a therapist, they encountered numerous obstacles, leading to worsening mental health and hospital visits.",
      "ProPublica's investigation underscores systemic issues in mental health care access, with inaccuracies and delays in insurance networks contributing to Ravi's tragic death at 36 from complications related to excessive drinking."
    ],
    "commentSummary": [
      "A man seeking mental health care found many providers listed by his insurance were either unavailable or out of network, a common issue known as a \"ghost network.\"",
      "Insurance companies often fail to maintain accurate directories, complicating patients' efforts to find care and causing significant stress and financial burdens.",
      "Some suggest eliminating health insurance middlemen or adopting a single-payer system to address these inefficiencies, though even integrated providers like Kaiser Permanente face challenges with mental health services."
    ],
    "points": 139,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1727018477
  },
  {
    "id": 41613722,
    "title": "Twenty Years of FM Synthesis Inside Ableton Live",
    "originLink": "https://roberthenke.com/technology/operator.html",
    "originBody": "Home Installation Performance Music Graphics Writing Engineering About Robert Henke Operator 2004-2024 : Twenty Years of FM Synthesis inside Ableton Live In 2004 I created Ableton's first software synthesiser - Operator. On the occasion of its twentieth birthday, this page provides background, a few tips and a download link to a pack with presets I and a few other Ableton colleagues created for this occasion. John Chowning and FM Synthesis photo: In 1975 Pierre Boulez brought an IRCAM team to CCRMA for a two-week course in computer music. Seated by the computer (left to right) Pierre Boulez and Steve Martin (graduate student); standing (left to right) James (Andy) Moorer, John Chowning, Max Mathews. Photo by José Mercato. On a hill next to the campus of Stanford University, a large radio telescope is gazing at the sky. Located even further away from the main campus, behind the telescope, was the first home of the Stanford Artificial Intelligence Lab (SAIL). Given its slightly exotic and remote status, SAIL could also host a new department that was concerned with using computers for generating musical sound: In 1964, with the help of Max Mathews of Bell Telephone Laboratories and David Poole of Stanford University, composer and percussionist John Chowning set up a computer music program using SAILS mainframe computer system. Having access to a computer was a big deal at that time. Computers did not fit on a desktop and did cost as much as a big house. Chowning became interested in the topic after reading an article by Max Mathews and subsequently he took a course in computer programming and went to Bell Labs in New Jersey to meet with him. When coming back to Stanford, where he studied, he was looking for a place to continue working on his ideas, and SAIL offered an opportunity. This later led to the foundation of Stanford's Center for Computer Research in Music and Acoustics (CCRMA). Chowning was exploring sound spatialisation and worked on computer-generated quadrophonic music. His research led to the first generalised surround sound localisation algorithm. He soon figured out that more complex, alive sounds with vibrato are easier to localise and was looking for ways to create them with the computer. \"One evening in the autumn of 1967, Chowning was using the mainframe computer in the Stanford AI laboratory to model the sound of two sine wave oscillators in a simple modulation configuration, one altering the pitch of the other to produce vibrato. Curious as to what would occur if he increased the rate and/or depth beyond what was possible with the human touch on an acoustic instrument, he issued instructions to the computer to try some basic multiples, doubling and tripling some of the numbers. And that’s when a curious thing happened: At the point when the rate of the vibrato increased to where it could no longer be perceived as a cyclical change, the sound changed from simple pitch fluctuation into a timbral change — a change in tonality. What’s more, as the rate and depth increased further, he heard more and more timbral complexity. This was indeed the birth of digital FM.\" [1] It became obvious that this method had a lot of musical potential. Chowning and Stanford University filed a patent [3] and reached out to synthesiser manufacturers to turn it into a commercial product. They ended up licensing the technology to Yamaha, and it resulted in a series of musical instruments that changed the world of synthesis forever with the 1983 release of the DX7, which sounded like no other instrument before and was a technological masterpiece. [2] John Chowning's Music is as impressive as his contributions to computerised sound. His works 'Turenas', 'Stria' and 'Phoné' are seminal classics. Find more info about him here: CCRMA website And here is one particularly interesting in-depth text by him about the realisation of Turenas: Turenas How does FM work? screenshot from the original patent. The object with the ~ inside represents one FM osciallator, its lower input is for its level, the upper input is defining its frequency. [3] Typical 'analog' synthesis, as it has been established by Moog and others in the 1960s is based on one or multiple oscillators providing waves with a rich spectrum, such as a sawtooth or a square, and to apply (time-variant) filtering and a volume envelope to shape it into a musical tone by removing parts of the spectrum. This is called subtractive synthesis. In contrast, FM synthesis in its pure form is using two or more sine wave oscillators and the complex resulting spectrum is a result of modulation one oscillators frequency (or phase, which is similar, but mathematically less complex to do) with one or more other oscillators. The spectrum is a function of the intensity or depth of the modulation and the ratio between the oscillator that is at the output ( the 'carrier' ) and the one which is modulating it (the 'modulator'). If the ratio between modulator and carrier is a whole number, harmonic spectra appear, and if the ratio is fractional, complex inharmonic sounds are the result. By changing the amplitude of the modulator with an envelope, the spectral richness of the sound can be a function of time, just like in real instruments. When using more than two oscillators that modulate each other, an enormous variety of rich timbres can be achieved. The necessary complex control over pitch and phase of oscillators requires digital processing. FM Synthesis needs computers or dedicated digital hardware to achieve this. Yamaha's DX7 synthesiser had six oscillators, each with its own multi breakpoint envelope. Smaller versions like the DX27 simplified this down to four oscillators, which still gives plenty of room for sound design. FM synthesis does not need a filter, and no dedicated volume envelope. Every oscillator has its own envelope, and some contribute to the output and others to the timbre. The Birth of Operator screenshot: Invitation for the other Abletons to a preview of the prototype, September 7th, 2004. Ableton Live started as audio-only application with no MIDI and no built-in synthesis with version 1.0 in early 2001. Computers, especially laptops, were not fast enough yet for multiple voice real-time synthesis. However, only a few years later things looked better, and we decided to add MIDI to Live Version 4. Whilst we were supporting Steinberg's VST and VSTI standard from the beginning, we also wanted to provide our own synthesiser. It was my job to take care of it, and FM was an obvious choice. John Chowning's music is dear to me, I was blown away when I first heard his compositions. And my first encounter with a DX7 as a teenager in a music shop felt like a quasi-religious moment, I could not believe the sounds coming out of that machine. A few years later, all Monolake records own a lot to their sound to a selection of Yamaha FM synths, including the DX27, a SY77, and for a while even two more TG77 modules. More is more. When the Max software (named after Max Mathews!) got its Max Signal Processing (MSP) extension in 1997, exploring FM synthesis was an obvious thing to do, and I wrote specific instruments for a lot of the early Monolake tracks in it. In 2004 I started to prototype Ableton's first synth and called it 'Onyx'. The goal was simple: It should not be heavy on CPU (which ruled out good sounding analog modelling), it should offer a huge sonic palette, and it should be fun to program. I took a lot of inspiration from my DX27 as a starting point. Design Challenges Screen estate was a big topic at a time when computer screens had 1024 x 800 pixels. It was clear that we cannot display all parameters at once. To solve this problem we invented a new paradigm in Live's visual language: A context sensitive 'LCD type' display sitting in the middle, which only shows the parameters of one oscillator or other essential building block at a time, and the main parameters arranged around it, so the basic function can be understood at one glance. This concept turned out to work very well, and a similar method is used now for Granulator III. The 'LCD' became a standard UI element in Live, used in a lot of places. Whilst I was the principal developer of Operator, its native implementation in C++, the interface design, and a few quite essential feature decisions where the result of teamwork. Operator's visual language owns a lot to the original designer of Ableton Live, Torsten Slama. I personally did not want to add any filters to it, because I wanted a 'pure' FM machine. But ultimately I agreed on having them, and they turned out to be very useful. Operator got it's final name from the way Yamaha was calling their oscillators: operators. In retrospective that name was a stroke of genius. Love for Detail Operator does not look like much, but a lot of thought went into adding details that make a difference. Here are some of them: Global Time This parameter allows to scale the time of all seven envelopes which is very handy for quick overall adjustment of all rates. It can be a modulation destination for velocity, key, the LFO and the Aux/Pitch envelope and this makes it quite powerful. I am very happy I came up with that. Every synth needs this. Tone This is a special one, which in the original design was not even 'public facing'. When modulating a carrier with a modulator which either already contains a complex spectrum or has a high coarse frequency setting, the result can sound pretty harsh, due to aliasing. To tame this, at the modulation input of each oscillator is a basic low pass filter. The original idea was to find one good sounding value and set it internally, but then I thought perhaps it is a good idea to have it as a control. Sometimes it is useful to have. Spread The New England Digital Synclavier II, another early digital synthesiser, with its own crude version of FM allows to stack two (or more) voices and detune them [4]. I always liked that chorusing sound, and Operator - as well as Sampler and the Granulator instrument series - borrowed this idea, but with a twist: Spread uses two voices, panned hard left and right in the stereo field. Hence the effect of Spread is a bit stereo chorus like, but there is more: All random functions like free-running oscillator or LFO phases, LFO random waves, etc. are different for the left and right channel, creating a huge stereo image. Don't overuse it. Not every sound in the mix needs to be stereo! Looped Noise The oscillators in Operator are internally 'lookup tables', which contain one cycle of the waveform the oscillator produces. Not sure anymore about their size, I believe they are 1024 samples long. The Operator 'NoiseLoop' waveform is just a bunch of random numbers. Whilst they are technically a short sample of noise, they repeat, which is exactly what noise does not. However this method allows to tune the noise and create typical lo-fi game sound noise effects with it. The bad noise as a feature. And if you want real noise, simply add some slight FM modulation to it, and it will loose its repetitive character. Or use the other noise source, which is standard pink noise, generated in real-time. That one has no pitch control, of course. Additive Synthesis In 2009 we added a way to create user waveforms via additive synthesis. Users can draw partials for each oscillator, extending the sonic palette a lot in an pretty intuitive way. Zero Hertz The Oscillators can not only either follow MIDI notes or have a fixed frequency, but they can also go down below audio range or completely stop. In that mode, they become effectively a wave shaper for the signal from the modulation input. As is the case in the implementation used by Yamaha, the modulation input technically acts as a pointer to the beginning of the wave. If the Oscillator phase is shifted, or if the phase is not reset on start, or if the Oscillator is running at a very low frequency, the resulting wave shaping changes over time or for each note. Side note: This is technically not really frequency modulation but phase modulation which provides very similar sonic results whilst being significantly easier to calculate which means less CPU usage. Copy Oscillators Each Oscillator has a context (right click) menu that allows to copy settings from/to the other oscillators. It is located above the colored enable switch of each oscillator ('A', 'B','C','D') Modulations Although Operator does not feature a modulation matrix, there are plenty of modulation sources and targets. The LFO, Filter and Pitch envelope can modulate a lot of things, and so can MIDI modulation, pitch-bend and aftertouch. Since 2024 Operator also responds to MPE pitch info. A special modulation target is hidden in the oscillator sections: Velocity can modulate the pitch of each oscillator, and this can either happen quantised to coarse tunings or non quantised, allowing to either have (slight) pitch fluctuations depending on Velocity or jumping in harmonics. Envelope Modes Apart from the normal playback mode, the envelopes offer three more behaviours: Trigger plays through the complete envelope when a new note starts, independently of the note length. This is often very desirable for percussive sounds. Loop plays back the envelope segments repeatedly as long as there is a note held, and the Beat and Sync modes re-trigger the envelop at specified beat time intervals. Some of the sounds in my twenty years of Operator pack make excessive use of these modes. Interpolation, Aliasing and Sample Rate FM synthesis can easily create harmonics that are higher than half the sample rate it is running at. This leads to a 'folding back' of the harmonics and the creation of new frequencies that have a very different relationship to the fundament frequency than what you would get from an ideal system with infinitely high sample rate. A way to tame that problem is the 'Tone' control mentioned above. Since the 2009 oscillator re-design, two global settings where added to reduce artefacts: A better interpolation which helps reducing noise when the oscillators play back complex waves at very low frequencies, and a better anti-aliasing, reducing the fold back at high frequencies. However, technically better is not always sounding better. Some sounds do benefit from leaving one or both options unchecked. It is also cheaper on CPU. Also important: Sounds which rely on aliasing for their specific timbre such as high pitched metallic sounds, cymbals etc. might sound different when Operator is running at 44,1kHz or 96kHz. In doubt freeze the track or create samples from the sounds if you need to change the sample rate at a later stage in the production process. Celebration Preset Pack On the occasion of this birthdays, Christian Kleine and myself created and curated a free pack with selected presets, made in the last twenty years. You can find it here: Operator20.alp Operator credits: C++ code by Matthias Mayrock. UI by Torsten Slama. And a lot of contributions and feedback from all the other early Abletons. Screenshots on this page made with my own Theme. You can grab it from here: roberthenke-live12-theme.zip Footnotes and Links [1] Quotes taken from a article about John Chowning on hub.yamaha.com. [2] For a fantastic in-depth reverse engineering of the DX7 synthesis chip visit Ken Shirriff's blog about 'computer history, restoring vintage computers, IC reverse engineering, and whatever'): www.righto.com [3] The Stanford FM patent turned out to be extremly lucrative for the university, since FM synthesis was the core of one of the most common PC sound card in the 1990s. Here is the patent: US4018121.pdf [4] The FM synthesis of the Synclavier II was technically much less advanced then what Yamaha offered, it used a two osciallator structure where the carrier was an 8bit additive wave and the modulator an 8bit sine wave. This sounds very crude but also makes it unique. My dedication to the Synclavier deserves its own page some day... Another interesting long interview with John Chowning: www.rnz.co.nz Contact Vimeo Instagram Bandcamp Margaret",
    "commentLink": "https://news.ycombinator.com/item?id=41613722",
    "commentBody": "Twenty Years of FM Synthesis Inside Ableton Live (roberthenke.com)139 points by gregsadetsky 18 hours agohidepastfavorite21 comments 1zael 2 hours agoAbleton is an epitome of software design innovation. I think few people understand how groundbreaking the Session View vs Arrangement View design was to advance the workflow of both produced and live music. Subcomponents like Operator created design patterns that are widely adopted by most VSTs today. Kudos to the Ableton team for crafting a product that is so beloved. reply chubs 12 hours agoprevOh I love FM synths! I'm working on a customisable one in my spare time lately for the kids' school, as the music teacher was complaining that the students have been using all the same samples over and over. Feel free to have a peek! Desktop only. Source code is hopefully nice and clean too: https://chrishulbert.github.io/you-synth reply 127 5 hours agoparentSuper cool. Well done. Now make it a full Yamaha ReFace DX ;) Also woaah, the randomize is amazing. Proposal: make z and x change octave. reply dokka 12 hours agoprevAh Operator. This synth is so deep. Not only is it a fantastic FM synth, but it does subtractive synthesis well too. Also, it really is impressive how the UI manages to fit all those parameters. I mostly use it for cool synth leads. Here's one of my favorite videos on Operator https://youtu.be/rfeY0_k1ctk?si=s68Lr033cHf34a4M by Robert Henke himself. reply meindnoch 9 hours agoprevFyi Robert Henke's new album came out a few weeks ago: https://roberthenke.bandcamp.com/album/studio reply bambax 7 hours agoprevReason has a very nice and versatile FM synth, \"Algoritm\": https://www.reasonstudios.com/shop/rack-extension/algoritm-f... Ableton tutorial features an excellent (and completely free) FM synth in-browser: https://learningsynths.ableton.com/en/playground reply fuhsnn 5 hours agoprevFM is one of the most \"naturally digital\" synthesis method to implement, it's trivial once you have an accumulator and sin table working. The simplest form (and arguably the easiest to sound musical), can be expressed with a one-liner formula: https://www.desmos.com/calculator/fjp9psrcqb reply yungporko 5 hours agoprevsomebody please make a plugin version of operator for both windows and mac, there are zero good options for fm synth plugins in 2024 except for sytrus on windows if they even still sell it. exacoustics GHOST is looking very promising though, just still in its teething stages reply S0y 5 hours agoparentIs Dexed not good? https://asb2m10.github.io/dexed/ reply bowsamic 53 minutes agorootparentDexed is good but it's primarily a DX7 including all the weird preset algorithm choices and that's not really necessary if you aren't using DX7 presets. It's a DX7 emulator, not really jsut an FM synth for its own sake like Operator is reply _DeadFred_ 51 minutes agoparentprevNot a fan of F 'em by Tracktion? https://www.tracktion.com/products/f-em reply inquisitorG 3 hours agoparentprevI have been out of audio for a long time but this sounds crazy. Have owned FS1R and DX11, made my own FM synths in reaktor but I would still rate FM7/FM8 the greatest of all FM synths. There has just never been a better interface to program FM. In the same way I would probably rate the FS1R as one of the all time worst FM synths and I do enjoy a synth that is not easy to program for the uniqueness. Unless it is vaporware now I would just get FM8. reply yungporko 2 hours agorootparentfm8 is still around and i have it but unfortunately on higher dpi screens its quite frustrating to navigate the ui reply PaulDavisThe1st 4 hours agoparentprevthere are numerous good FM modules inside VCV Rack (and Cardinal), which have the benefit (or drawback) that you get full control over both upstream modulation and downstream processing. And they exist for all platforms that Rack (& Cardinal) run on, not just \"windows and mac\". reply johnofthesea 3 hours agoparentprevProbably not pure FM synth, but what about Aalto from Madrona Labs? reply MDJMediaLab 1 hour agorootparentAalto is great. It's more of a west-coast style Buchla complex oscillator clone than a traditional FM synth with multiple operators, but complex oscillators have at least one carrier and one modulator by design. I use very few software synthesizers but Aalto and Operator are two of my favorites. reply bowsamic 55 minutes agoparentprevYeah I'm kinda surprised that there are so few good FM synth plugins. They must be out there but just hidden. FM8 is all I can really think of reply bowsamic 12 hours agoprev [–] I love FM synthesis and I love Robert Henke. Great article reply polotics 10 hours agoparent [–] Had the good luck of attending an Henke (Monolake) concert in large-ish and cubical space fully rigged with speaker arrays doing wave-field-synthesis which was beyond anything I heard before or since. Loud is one thing, gigantic is something else. https://roberthenke.com/concerts/wfs.html reply Nabi 56 minutes agorootparentAbout a decade ago had the luck attending in-person his field recording and granular synthesis workshop in Amsterdam. Such an inspiring and humble individual! Still coming back from time to time to his timeless album Monolake - Hongkong. reply PaulDavisThe1st 4 hours agorootparentprev [–] You should have stepped over to the TU Berlin. A few month's before that concert, there was a performance/recreation of a concert that took place in Koln cathedral of various Messiaen organ works, played back on the WFS system installed at the TU. Much, much bigger than Henke's setup at Tresor, and it was a truly remarkable experience. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Robert Henke celebrates the 20th anniversary of Ableton's first software synthesizer, Operator, by sharing insights, tips, and a free preset pack.",
      "Operator, developed in 2004, was designed for CPU efficiency and user-friendly programming, featuring innovative elements like Global Time, Tone Control, and Additive Synthesis.",
      "FM synthesis, pioneered by John Chowning in the 1960s, uses sine wave oscillators for complex sound creation, differing from subtractive synthesis by not requiring filters."
    ],
    "commentSummary": [
      "Ableton Live's FM synthesis, especially through its Operator component, has been influential over the past 20 years, impacting many VSTs (Virtual Studio Technology).",
      "The software's unique Session View vs Arrangement View design is highly praised for its depth and user interface (UI).",
      "Robert Henke's contributions, along with his live performances and workshops, are celebrated for their significant impact on the music production community."
    ],
    "points": 139,
    "commentCount": 21,
    "retryCount": 0,
    "time": 1726965753
  },
  {
    "id": 41614126,
    "title": "PDF to MD by LLMs – Extract Text/Tables/Image Descriptives by GPT4o",
    "originLink": "https://github.com/yigitkonur/swift-ocr-llm-powered-pdf-to-markdown",
    "originBody": "I&#x27;ve developed a Python API service that uses GPT-4o for OCR on PDFs. It features parallel processing and batch handling for improved performance. Not only does it convert PDF to markdown, but it also describes the images within the PDF using captions like `[Image: This picture shows 4 people waving]`.In testing with NASA&#x27;s Apollo 17 flight documents, it successfully converted complex, multi-oriented pages into well-structured Markdown.The project is open-source and available on GitHub. Feedback is welcome.",
    "commentLink": "https://news.ycombinator.com/item?id=41614126",
    "commentBody": "PDF to MD by LLMs – Extract Text/Tables/Image Descriptives by GPT4o (github.com/yigitkonur)137 points by yigitkonur35 16 hours agohidepastfavorite54 comments I've developed a Python API service that uses GPT-4o for OCR on PDFs. It features parallel processing and batch handling for improved performance. Not only does it convert PDF to markdown, but it also describes the images within the PDF using captions like `[Image: This picture shows 4 people waving]`. In testing with NASA's Apollo 17 flight documents, it successfully converted complex, multi-oriented pages into well-structured Markdown. The project is open-source and available on GitHub. Feedback is welcome. Oras 2 hours agoWhile this is a nice development, it’s quite risky parsing documents with LLMs. In usual OCRs, you have boundaries to check, but with LLMs, you just get a black box output. As others mentioned, consistency is key in parsing documents and consistency is not a feature of LLMs. The output might look plausible, but without proper validation this is just a nice local playground that can’t make it to production. reply zerop 8 hours agoprevI had been using GPT4o for extracting insights from Scanned docs, it was doing fine. But very recently (since they launched new model - o1), it's not working. GPT4o is refusing to extract text from images and says it can't do it, though it was doing same thing with same prompts till last week. I am not sure if this is intentional downgrade and it can be clubbed with new model launch, but it's really frustrating for me. I cancelled my GPT4 premium and moved to claude. It works good. reply itissid 7 hours agoparentThis. Inconsistency is a big problem for large tasks, you are better off making your own models to do this. I have seen this odd kind of inconsistency in generating the same results, sometimes in the same chat itself after starting off fine. I was once trying to extract hand written dates and times from a large pdf document in batches of 10 pages at a time from a very specific part of the page. IN some documents it started by refusing, but not in other different chat windows that I tried with the same document. Sometimes it would say there is an error, and then it would work in a new chat window. But I am not sure why, but just starting a new chat works for these kind of situations. Sometimes it will start off fine with OCR, then as the task progresses, it will start hallucinating. Even though the text to be extracted follows a pattern like dates, it for the life of me could not get it right. reply rrrix1 2 hours agorootparent> \"...you are better off making your own models to do this\" I'm doubtful you meant what you wrote here. Using a readymade UI or API to perform an effectively magical task (for most of us) is an entirely different paradigm to \"just train your own model.\" In reality, for us non-ML model training mortals, we're actually probably better off hiring a human to do basic data entry. reply avibhu 3 hours agoparentprevHave you tried few shot prompting? Something on the lines of: User: Extract x from the given scanned document.Assistant:User: Extract x from the given scanned document.Assistant:User: Extract x from the given scanned document.In my experience, this seems to make the model significantly more consistent. reply fragmede 7 hours agoparentprevWhy not just switch back to GPT-4? it's still there. reply itchyjunk 4 hours agorootparentSo is 4o. Problem isn't the absence of model, it's inconsistency. reply pierre 9 hours agoprevParsing docs using LVM is the way forward (also see OCR2 paper released last week, people are having ablot of success parsing with fine tunned Qwen2). The hard part is to prevent the model ignoring some part of the page and halucinations (see some of the gpt4o sample here like the xanax notice:https://www.llamaindex.ai/blog/introducing-llamaparse-premiu...) However this model will get better and we may soon have a good pdf to md model. reply fzysingularity 38 minutes agoparentWe’ve been doing exactly this by doubling-down on VLMs (https://vlm.run) - VLMs are way better at handling layout and context where OCR systems fail miserably - VLMs read documents like humans do, which makes dealing with special layouts like bullets, tables, charts, footnotes much more tractable with a singular approach rather than have to special case a whole bunch of OCR + post-processing - VLMs are definitely more expensive, but can be specialized and distilled for accurate and cost effective inference In general, I think vision + LLMs can be trained to explicitly to “extract” information and avoid reasoning/hallucinating about the text. The reasoning can be another module altogether. reply authorfly 7 hours agoparentprevWhat about combining old school OCR with GPT visual OCR? If your old school OCR output has output that is not present in the visual one, but is coherent (e.g. english sentences), you could get it back and slot it into the missing place from the visual output. reply fkilaiwi 3 hours agoparentprevwhat paper are you referring to? reply charlie0 3 hours agoprevI do this all the time for old specs, but one issue I worry about is accuracy. It's hard to confirm if the translations are 100% correct. reply smusamashah 12 hours agoprevI have not found any mention of accuracy. Since it's using LLM, how accurate the conversion is? As in does that NASA document match 100% with the pdf or did it introduce any made up things (hallucinations)? That converted NASA doc should be included in repo and linked in readme if you haven't already. reply uLogMicheal 3 hours agoparentCouldn't one just program a linear word matching test to ensure correctness? reply yigitkonur35 12 hours agoparentprevPeople are really freaked out about hallucinations, but you can totally tackle that with solid prompts. The one in the repo right now is doing a pretty good job. Keep in mind though, this project is all about maxing out context for LLMs in products that need PDF input. We're not talking about some hardcore archiving system for the Library of Congress here. The goal is to boost consistency whenever you're feeding PDF context into an LLM-powered tool. Appreciate the feedback, I'll be sure to add that in. reply williamdclt 9 hours agorootparentI don’t think any prompting skill guarantees the absence of hallucination. And if hallucination is possible, you will usually need to worry about it reply Foobar8568 9 hours agorootparentprevAs soon as you have something else than a paragraph in a single column layout, you will get hallucinations, random stuff, cut off etc even if you say which pages to look at, LLM will just do what ever. reply freedomben 7 hours agorootparentprevCan you give some examples of prompts that you use that will tackle hallucinations? reply jdthedisciple 10 hours agoprevZerox [0] was featured on here recently and does the exact same thing [0] https://github.com/getomni-ai/zerox reply refulgentis 25 minutes agoprevGPT 4o doesn't do actual OCR and there's much smaller and more effective models for specifically this problem. I appreciate your work, intent, and sharing it. It's very important to appreciate what you're doing and its context when sharing it. At that point, you are responsible for it, and the choices you make when communicating about it reflect on you. reply bravura 8 hours agoprevI've also been using the nougat models from meta, which are trained to turn PDF into md using the donut architecture reply rahimnathwani 3 hours agoparentDoes it work well on documents that aren't academic papers? https://facebookresearch.github.io/nougat/ reply jdross 13 hours agoprevHow does this compare with commercial OCR APIs on a cost per page? reply yigitkonur35 12 hours agoparentIt is a lot cheaper! While cost-effectiveness may not be the primary advantage, this solution offers superior accuracy and consistency. Key benefits include precise table generation and output in easily editable markdown format. Let's make some numbers game: - Average token usage per image: ~1200 - Total tokens per page (including prompt): ~1500 - [GPT4o] Input token cost: $5 per million tokens - [GPT4o] Output token cost: $15 per million tokens For 1000 documents: - Estimated total cost: $15 This represents excellent value considering the consistency and flexibility provided. For further cost optimization, consider: 1. Utilizing GPT4 mini: Reduces cost to approximately $8 per 1000 documents 2. Implementing batch API: Further reduces cost to around $4 per 1000 documents I think it offers an optimal balance of affordability & reliability. PS: One of the most affordable solution on market, cloudconvert charges ~30$ for 1K document (pdftron mode required 4 credits) reply johndough 12 hours agorootparent> I think it offers an optimal balance of affordability & reliability. It is hard to trust \"you\" when ChatGPT wrote that text. You never know which part of the answer is genuine and which part was made up by ChatGPT. To actually answer that question: Pricing varies quite a bit depending on what exactly you want to do with a document. Text detection generally costs $1.5 per 1k pages: https://cloud.google.com/vision/pricing https://aws.amazon.com/textract/pricing/ https://azure.microsoft.com/en-us/pricing/details/ai-documen... reply yigitkonur35 12 hours agorootparentYou've got a point, but try testing it on a tricky example like the Apollo 17 document - you know, with those sideways tables and old-school writing. You'll see all three non-AI services totally bomb. Now, if you tweak it to batch = 1 instead of 10, you'll notice there's hardly any made-up stuff. When you dial down the temperature close to zero, it's super unlikely to see hallucinations with limited context. At worst, you might get some skipped bits, but that's not a dealbreaker for folks looking to feed PDFs into AI systems. Let's face it, regular OCR already messes up so much that... reply hnlmorg 7 hours agorootparentAWS Textract does use ML and I’ve personally used it to parse tables for automated invoice processing. You wouldn’t get a markdown document automatically generated (or at least you couldn’t when I last used it a few years ago) but you did get an XML document That XML document was actually better for our purposes because it gives you a confidence score and is properly structured, so floating frame, tables and columns would be properly structured in the output document. This reduces the risk of hallucinations. It’s less of an out-of-the-box solution but that’s to be expected with AWS APIs. reply rescbr 4 hours agorootparentFor a similar use case I’m using Azure Document AI - at least you can ask for markdown/html output directly from it instead of parsing the output structure from Textract. And it’s cheaper too. reply Propelloni 9 hours agorootparentprev> you might get some skipped bits, but that's not a dealbreaker for folks looking to feed PDFs into AI systems Unless it is. We have a few hundred PDF per month (mostly tables) where we need 100% accuracy. Currently we feed them into an OCR and have humans check the result. I do not win anything if I have to check the LLM output, too. reply llm_trw 6 hours agorootparentI'm currently solving this problem for work and thinking of a spin out, what's a ballpark figure you'd be willing to pay per 1000 pages for 99.999% character level accuracy? reply Propelloni 3 hours agorootparent> I'm currently solving this problem for work and thinking of a spin out, what's a ballpark figure you'd be willing to pay per 1000 pages for 99.999% character level accuracy? I guess anything up to 5 ¢ per page would be acceptable. But I'm afraid my company wouldn't be a customer. We are in Germany and we deal with particularly protected private data, there is no chance that we would exfiltrate this data to a cloud service. reply llm_trw 3 hours agorootparentWhat's the total spend per quarter? For a margin that fat I'd be willing to jump through a lot of hoops if you're doing enough pages. The models (currently) fit in 24gb vram sequentially with small enough batch sizes, so a local server with consumer grade gpus wouldn't be impossible. reply rescbr 4 hours agorootparentprevAt least for my use case, which is Layout processing (i.e. must output tables in some kind of table format), the OCR part (Azure Document AI or AWS Textract) dominates the cost factor. Running OCR on a document is twice more expensive than processing the output on the most expensive GPT offering. Intuitively, this was kind of unexpected for me. Only when I did some calculations on Excel that I realized it. If you’re able to halve the pricing for Layout output then you’re unblocking lots of use cases out there. reply Lerc 4 hours agorootparentprevI guess it depends on the use case, but if it surpasses the error rate that exists in the source document then it would be difficult to argue against. Specific things like evidentiary use would want 100% but that's at a level where any document processing would be suspect. What is the the typical range for error rate in PDF generation in various fields? Even robust technical documents have the occasional typo. reply llm_trw 4 hours agorootparentI'm not using generative models to fill in details not present in the original document. If there's a typo there then there will be a typo in the transcript. If you want to fix that then you can run another model on top of it. reply Lerc 4 hours agorootparentI realise that. The point is that a user is implicitly committing to the baseline error rate that exists in whatever means by which the document was created. If any additional loss was insignificant in proportion to that error rate then it would be unreasonable to reject it on that basis. reply authorfly 7 hours agorootparentprevI will say I have had a look at your code here. I really do value your innovation here in gaining better accuracy, but I don't think it's is much more accurate for obscure PDF cases - Maybe it halves those obscure errors. I found it still hallucinated or failed to parse some text (e.g. that unusual languages, screenshots with tiny blurred JPEG text, images/shapes remain hallucination issues with your solution). BTW I noticed a small typo \"Convert document as is be creative to use markdown effectively\" in the prompt. For me changing this and adding text about returning \"None\" if the text is unreadable reduced hallucinations. Would you contrast your accuracy with Textract? Because Textract is 10x cheaper than this at approx 1 cent per page (and 20x cheaper than Cloudconvert). What documents make more sense to use with your tool? Is it worth waiting till gpt-4o costs drop 10x with the same quality level (i.e. not gpt-4o-mini) to use this? In my use case it's better to drop than to hallucinate. What do you think makes sense in relation to Textract? reply llm_trw 6 hours agorootparentprevThat is not a tricky example. Those tables are as clear cut as clear cut can be. reply magicalhippo 12 hours agoprevWas just looking for something like this. Does it handle equations to latex or similar? How about rotated tables, ie landscape mode but page is still portait? reply yigitkonur35 12 hours agoparentI messed around with some rotating tables in that Apollo 17 demo video - you can check it out in the repo if you want. It's pretty straightforward to tweak just by changing the prompt. You can customize that prompt section in the code to fit whatever you need. Oh, and if you throw in a line about LaTeX, it'll make things even more consistent. Just add it to that markdown definition part I set up. Honestly, it'll probably work pretty well as is - should be way better than those clunky old OCR systems. reply x_may 6 hours agoparentprevCheck out Nougat from meta reply magicalhippo 21 minutes agorootparentThanks, looks very interesting, but also somewhat abandoned. Will keep an eye on it in case someone picks up the torch. reply nicodjimenez 10 hours agoparentprevHave you checked out Mathpix? It's another option. Disclaimer: I'm the founder. reply magicalhippo 3 hours agorootparentWas looking for a self-hosted solution as I have quite on/off needs, but I'll give it a whirl as it looks quite promising. reply troysk 8 hours agorootparentprev+1! Most LLMs can already output Mathpix markdown. I prompt it to do so and it gives the code and this use a rendering library to show the scalable and selectable equations. No wonder facebook nougat also uses it. Good stuff! reply scottmcdot 6 hours agoprevDoes it do image to MD too? reply eth0up 6 hours agoprev [–] I used GPT4o to convert heavily convoluted PDFs into csv files. The files were Florida Lottery Pick(n) histories, which they deliberately convolute to prevent automatic searching; ctrl-f does nothing and a fsck-ton of special characters embellish the whole file. I had previously done so manually, with regex, and was surprised with the quality of the end results of GPT, despite many preceding failed iterations. The work was done in two steps, first with pdf2text, then python. I'm still trying to created a script to extract the latest numbers from the FL website and append to a cvs list, without re-running the stripping script on the whole PDF every time. Why? I want people to have the ability to freely search the entire history of winning numbers, which in their web hosted search function, is limited to only two of 30+ years. I know there's a more efficient method, but I don't know more than that. reply mmh0000 3 hours agoparentThis sounds like a fun and interesting challenge! I am tempted to try it on my own I’m surprised an LLM actually works for that purpose. It has been my experience with gpt reading pdfs that it’ll get the first few entries from a pdf correct then just start making up numbers. I’ve tried a few times having gpt4 analyze a credit card statement and it adds random purchases and leaves out others. And that’s with a “clean” PDF. I wouldn’t trust an llm at all on an obfuscated pdf, at least not without thorough double checking. reply eth0up 3 hours agorootparent>then just start making up numbers... Absolutely! It's a fucking criminal in that regard. But that's why everything is done with hard python code and the results are tested multiple times. As an assistant, gpt can be fabulous, but the user must run the necessary scripts on their own and be ever ready for a knife in the back at any moment. Edit: below is an example of what it generated after a lot of debugging and hassle: import re import csv from datetime import datetime def clean_and_structure_data(text): \"\"\"Cleans and structures the extracted text data.\"\"\" # Regular expression pattern to match the lottery data pattern = r'(\\d{2}/\\d{2}/\\d{2})\\s+(E|M)\\s+(\\d{1})\\s-\\s(\\d{1})\\s-\\s(\\d{1})\\s-\\s(\\d{1})(?:\\s+FB\\s+(\\d))?' matches = re.findall(pattern, text) structured_data = [] for match in matches: date, draw_type, n1, n2, n3, n4, fireball = match # Format the date to include the full year date = datetime.strptime(date, '%m/%d/%y').strftime('%m/%d/%Y') # Concatenate the numbers, ensuring leading zeros are preserved, and enclose in quotes numbers = f'\"{n1}{n2}{n3}{n4}\"' structured_data.append({ 'Date': date, 'Draw': draw_type, 'Numbers': numbers, 'Fireball': fireball or '' # Use empty string if Fireball is None }) return structured_data def save_to_csv(data, output_path): \"\"\"Saves the structured data to a CSV file.\"\"\" # Sort data by date in descending order sorted_data = sorted(data, key=lambda x: datetime.strptime(x['Date'], '%m/%d/%Y'), reverse=True) with open(output_path, 'w', newline='') as csvfile: fieldnames = ['Date', 'Draw', 'Numbers', 'Fireball'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for row in sorted_data: writer.writerow(row) def main(): # Path to the text file txt_path = 'PICK4.txt' # Ensure this path points to your actual text file output_csv_path = 'output.csv' # Ensure this path is where you want the CSV file saved try: with open(txt_path, 'r') as file: text = file.read() cleaned_data = clean_and_structure_data(text) save_to_csv(cleaned_data, output_csv_path) print(f\"Data successfully extracted and saved to {output_csv_path}\") except Exception as e: print(f\"An error occurred: {e}\") if __name__ == \"__main__\": main() reply is_true 4 hours agoparentprevThere are private APIs that have that data (now and history) Do you think the official data published is 100% correct if they were trying to hide something? reply eth0up 3 hours agorootparentI am honestly not certain why they obstruct easy access to the number history. It's obviously accessible, but only through manually parsing the PDF. Their prior embedded search function, approximately two years ago, would return all permutations of the queried number from day 1 to present. They modified it to exclude results more than two years old. The PDF contains the entire data set, but isn't searchable. Why? Dunno. But I'm cynical I've also compiled a list of all numbers that have never occurred, count of each occurrence and a lot more. My anomaly analytics have included everything, as an ignoramous, I can throw at it; chi squared; isolated forest; time series; and a lot of stuff I don't properly understand. Most anomalies found have been, if narrowly, within expected randomness, but I intend to fortify my proddings eventually. Although I'm actually confident I'm barking up the wrong tree, the data obfuscation is objectively dubious, for whatever the reason. reply alchemist1e9 5 hours agoparentprev [–] Off topic - but the obvious follow up question is why do you want people to have this ability to search the entire history? reply eth0up 4 hours agorootparent [–] Thanks for asking... 1) I'm a rebel 2) I am irritated by deliberate obfuscations of public data, especially by a source that I suspect is corrupt. Although my extensive analysis has not yet revealed any significant pattern anomalies in their numbers. 3) It's kind of my re-intro into python, which I never made significant progress in but always wanted to. 4) It's literally the real history of all winning numbers since inception. Individuals may have various reasons for accessing this data, but I've been using it to test for manipulation. I presume for most folks it would be curiosity, or gambler's fallacy type stuff. Regardless, it shouldn't be obfuscated. reply alchemist1e9 4 hours agorootparent [–] I had suspected you’re are suspicious of manipulation. I have heard many rumors of lottery corruption and manipulation. It’s certainly a big red flag if they are deliberately obstructing access to the data. Make sense your project and I’d probably take 30 mins to look at the data if I came across it. I’m somewhat decent at data and number analysis so if there is something and enough people can easily take a look at it, then it might get exposed. Interesting and good luck. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An open-source Python API using GPT-4 for Optical Character Recognition (OCR) on PDFs has been developed, featuring parallel processing and batch handling.",
      "The API converts PDFs to Markdown and adds image captions, demonstrating its capability by successfully processing complex pages from NASA's Apollo 17 documents.",
      "The project is available on GitHub, and the developer is seeking feedback from the community."
    ],
    "commentSummary": [
      "A Python API service using GPT-4o for OCR (Optical Character Recognition) on PDFs has been developed, featuring parallel processing and batch handling, converting PDFs to markdown and describing images with captions.",
      "The project, tested on NASA's Apollo 17 documents, is open-source on GitHub, but users report issues with consistency and hallucinations in LLMs (Large Language Models), suggesting traditional OCR might be more reliable for production.",
      "The solution is noted for its cost-effectiveness, with detailed token usage and cost comparisons provided."
    ],
    "points": 138,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1726970738
  },
  {
    "id": 41614567,
    "title": "Cloudflare Is Breaking My SVGs?",
    "originLink": "https://www.lloydatkinson.net/posts/2024/stupid-problems-require-stupid-solutions-cloudflare-is-breaking-my-svgs/",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.lloydatkinson.net\",cType: 'managed',cNounce: '3017',cRay: '8c748bd04ee22a00',cHash: '80f9637d460771e',cUPMDTk: \"\\/posts\\/2024\\/stupid-problems-require-stupid-solutions-cloudflare-is-breaking-my-svgs\\/?__cf_chl_tk=oDKl5baIq7fCeIZvcwPRjPgsILv9NHFlkhLO1jlv0b0-1727031713-0.0.1.1-4927\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/posts\\/2024\\/stupid-problems-require-stupid-solutions-cloudflare-is-breaking-my-svgs\\/?__cf_chl_f_tk=oDKl5baIq7fCeIZvcwPRjPgsILv9NHFlkhLO1jlv0b0-1727031713-0.0.1.1-4927\",md: \"srufdXm1Qw4QYImOSEgLz9aNDFoZu5HzwTffUs03hO8-1727031713-1.1.1.1-wmcCVvvYu9mrt8KM.i03tE_hlXDnbg6x2hntXMY9VlNkaduDT86BlIcocu07J8Z_RP.ckNfDOe5Ijn9YLfa8oT4WbYT3Z7NdboCMKvjiGEkpOKo_1YSbMSYrXd_P32GC7Mh8b_77z0prYEva.3clu4UPAQkqEm0gKYwN0n.teC.MQT5htufePq.4MjSvB_joNjsCWxxscEoGm7QDGyXs0o_kZ8DhOFvnOhIuiqORaWJ_4UPtloe5xj08HP0Gc_45JQxNWAW0SW9wBmNgBAPBnMBKzq7bz1bkdvwQY7sk5zhyqaNGzCkiVKJ8YgQHiipdglz_xXpzFc78at5UxAESUgPKnTwvZGV9YBvb0xwM2FMaSiGJR9bF5VyoK7KPc75QEsen1IPXr2eT3FPMcofP8dxhuySevcTBzJo_yVKmgDMXt1cbNQnGRvLsGWxysG5xyxNcO8sEbf6J4sQcssEj5ECSZWBBh36auuzpk42t5cdr3KlyFZdWugoAiFuwbFdHgW5rOcc6e7nt6Tt2UHnkEl0o5DdfqOta4y6sZvyrb6rSHaMsCtd7iE28eMSwfJ4.MiyF8wEUY_PfB2PWCKMebIzDEAizcVWGKfVqy9HNVTNaLQvhXIDDpjkH0AKv9VWbNIcMNNAM2Z1gWsM41g_CZC95LElxcLPj2J2lkxKTuVHznCgkWEzSd.EsvnQCGS4AfyjnVUU94WU01L2KYNPr3EXX.zC9ALNeubk0UruSvXYEHshqG.LP2SO6sqYj4Vp9_6IdxLN40QPIwxV_0_bXy3.Ics.dpr4ClJCPdAj3eYF.sQJ.XO1LXVM_OfQ1AXJWGlxv_JY7vrsAsykGDyRKQf01KS0usGlHIjr.UO4GLJtCG2bUrHQWoYxq.7gVGvcVMIuf1Jhyc8W3atqCyrKUFMpr7OQS_L0D2vMZTGMeTl2ShwLIcwpn5cSQkoSM1ezZzm3HGSjxOV2BVFTsYBr2CuM9E9bvcj0E3bEAJki7Y7.3jecZyLSNgtTorAuvxQC9n4xrkouM339_0NvRcMhmAms_rzNmN7awTbDtWAjygtTpDmtILVNcmIJf4RoNDkCOCTsZvKumyff9wNOnQPeLOzPSCHS604eD9KcwIcHct1WyUFspTIlw4FwE3hpmYr8zaBPSzn72mJOXJ_x9buKwsvAz_Qa4Cmj..2INwy8wWJQ9NXvbvBhW.1pnJZrKBWFGVwjvw_Iki_wROwXKrNQYJW.CgP9B3s4OZ9pAkGGjWon7UisKTX1yB7Co_t_fG33vMyP6YR61B0PhPOcPUuI3EJm39XOeIsQh.mbgFlzSG1yGvWGyzhbVovuiuPaAURMjgKvvsWf4nGTRkWHED_srSbARR2kd29hEzTh8gAw0dVZTB_1IagLL1o.t4eiAHvd5oAXI_.MoADBGYz_oNXjgli1NxLHVXjD_OxKRnaE4_J3UV2Nl4hDQcD4nAGHMMnUToatG5mkFTckk6FjNyt9X_l6EGt1sDkJkr8c7AZiiA5IoaM5W6UjyCbhT7TZznhQOagHFTGN_RNR6LGKdUqkpY1lRkXsSbQ4kHl46tbsRkLRcszbzBJuqeRFpF4Dp9IfoZe7KZs0QVMd.MIvHS3_7O8f2cfIFv.BOhV4i8Ytl80nMoF0a8LENV0ODkpKsycZywZfXSPnFcQnxrY638IKR_SnPBEFQQYIqVZwxBFr_0kO_87uviBK1gxJrjfqlWSG9EiaLJ_8HDNsfVwMP.EywwzmJJn2DjuvKxru8ea38pk88NK5F4mDa5EwPZxlEXvK2ybe9vB3AQrfLdfXPlshnocbVhg818poAx.WguK1yhGsdYSNG.S9DRYEPFAA2mJ3lCFAS4cMTL0qFM1Pe5Be1Iyk1.lCU8PpNtqKjByCEROFv_176nxl8jHyS_AMqO5AE.nfCSx55CPEHJ9gZx7OHR.DjQm71qgxmNULdogSuKgDGI9B89f1PMF7dL4hTLnZ0dQfTTfpipCiIblmqNVNfDJ425PxExEBWqBW4FEzZBrZf95sgUiYbM6i5t1OaFpd3_kbrO41yRQ0sQhhmI91NE6Rb4gvrDOSP7bNieWqWK8iRjeG4Qv1cZZgdZ93AWuOnfER_XMUXk.mQWrLpmbwoX8aqTJfIUTm_NYP5.QFJnPrHX8n6IXEhbtRH8ePBXaAkmGKLBANCKvgkWAQecJrBMPXsTbnqZh6fcH7nfnivqqPK732jPzItiTUFz4jmrkeyng8EioY2Kq6s.1kldDNQyts.CMBKP9xvx6cqsC03sgXxwEwDFcAugZq16RxsXD4Kg16CEq94pjJeLTlkH0L3PvQfy7PWJImOkzUekvdcGw4V5RMrxExIQmt_OMN44.h_s93A2eZ.20MDLoAp5Kb2fg\",mdrd: \"4J.wC6OmCwTD9.g2htY6TTMzsn2HdpifB5vCRCw0et4-1727031713-1.1.1.1-xmy7upcbpuJchqXLewLjCZW8OWkHiSH5L0G65q0VGOjk_TD_FtQeSQ8LUgVMqA5OWVh_lkILH9LZ2AIWsk7qeRYNY.PAy9sXVzWzwS7n5DhHnxdSKaWm7FSqXrT.POo.R.ZI5._NodcIs1ONzU_uBxkZJbA4R8jS8Ns1e4Zbxdhn2zQVyOUisk2gvZlY.oLe2PmEm4v0xhSdqGtgGXE0cwbzVF2Bdk5zZIzjP3IMkmj3Y27X2vgDtxA4Q9kq0.glMxZTEhKJUxTJc5CHAUtZM8OWwRorN.G7L7a8yIayd1IAzqSH3PWQSfHeQnGdkbpxeDsX7ioZfpUGttoARKoc_Trdf9fZzC3e05BnmbKGwIFyb56FICwJdCkUxpdfmqatEEoh.QrsRXLU6EuBS1Zt150zxoi6r8sYNXCQAQhErvfcUjkczGtnmMass8dYrE6GjuwzfzYb_NbvWGbU3nTRilwlBjGkAg_NNKAFRTfaCbSqPdS0vbjK7qp_tbXBzHSYH6QgpSgDRIfAXlbM.azRpNuLj8sgfx2FXAzA9ek9pckIC4y9RAsySv1AKzvnMDb0ZrkR7Jcvf2VArYW39ZWw1hAJ_Wv3IBlLJS7DoSrxLnKSJ.jw2UmPkvHLVbVY78nDlTBmd3n_OojufXf4OpzKKTvSMuoG.KJw1dC7nyBFA02q73jlbu77BN3YA76QigBf5KvqLM.SsIAZI3fXvD_RCbHcWjak3WvsdpKtOrl3hOvU0FXmt1OS_N1l4mjNstuSyVHC1HorzEdWtKJTcHOtERCQFOlpq8DfzzvvClTvTxJWxdt933hERiND46.lvplx87oQgqyX0yipIYZQ_Ur4lkYI20_.hjpSHIlOoPU5QnASUkjOGdo7B2gdxt5sJb4sWQNxEYEJyXlNHL6DyLDx6NJzs3tq5_wjr1ZDB_V9ET_6fxc.L5bMZlLhRIyQTV0eI9kneXBEsevrgHIyIDHdTKtgQ3mKiUTaauDbl1RJpPSpVHLsqsIvBABkn.moT6NdxJLpRo0rVGLHSUOycrmFrA1iXUw6yS2pEb5dq9qtVSgLiYluKfHMRpNYH6sAWsLJ5hBUVUndYWxEOMN416Wa8dODObT773xGRZpoNvrNFXZEtwsNL1rrMT7g1GAggBgIpnYkRFpURpULseu8XouUt0zsCN.caePEXQwHp1tzTYgY_BLAiyVHYTWTudvUGiJx5RAHJ4uciJ1aM6zYQOJn5A65z8ezyIaJNLoiY3Pgr2Uzikejhvgcj9MGVqiEAIHW8qUVNnZ3xMnH8f.EH.LAWY7gQ9EA76ij5dlR2ibfg.EXv8qKlJyOW14sOBDz3oDaXR03L1M1yIvBlaafc.BWXUHSzs_pTAbpXympZtz8iycrVUXeFFq3CrwJwguWp43gMgRQKObDm4aADpK3fri_8bBKpO4vbjG_P6mEjstfW_QxDH4K8LDA36YcUV2UiDPKyx41JiyuO.om8CdROL9NHn1z31t98f9iEnWonZQ5niKYAmKjyOxNWMda1.i6.NxZV1TGaG4XsydvkSa5W9lRAi8iKkOYOmGYkJ2z3Ns.khlrXIoxRvxRiFxJEL2w5_yjkExsu_CB2p8O.hqbcaJBVC9dwmH0DqVQ4gh9QKpu8dMKcVvhQkmlYw58oBYwU.0Y46YgRahE7dgM5YGSg_8oHBfV8yqjEjW9Vhb7G2kuHBbcRcNHM8lF1GMKhO0qaOJuZ0WJTlSKrP1mfuYq08AuSF4C7r5frCEaN1rtCRlGezHBRV_prh5xNM..XCxmkmo9dR4H_yBBOj_C7kQcvToDhvfm2YsafjRCmQwjhe.JpgeiSWyOfF7ALnmQZC.Cy0CxGq.UAv1DlaIfrZ.rAVHRKUPVPXomrtmLr6aa6N25pz.nKBJzULVvK7KmTkuS5Nz3CLwo0yFMcGzDp8PDvaxcVnPzJ9nX5lFWXtFU2Vkv4hThBwEuT2b0JqYrLEVqEkSps_sbNyGFwqmtLpWurCumli9bswH4RuWPU41NS5SEi5akjXjdIwedFDRhsUHZu5hcP9hnGFHDhjk2u1uKSY2kC456z3NNP2XG1c9fvITl4ArQ5.YxiynFOWtKNWnUySoPuJiJ5NahnWfNCcb5fX8NX7sMCCnDtD673qpvt55JRakfszF4kIE49qZ0Ez7oPhPDq0JOCqY8Bzpq8MAuHMwSKK3wuJWEm8sItqvgmpTq_3PsxgDrpuiFVczYTIFbE1F_fwBGikrEmrDkYUBJGlt_Ifk8WiRV8jbbIe4ocFXMXryF4CrhnZ6qykkw1NnU6Bx7BnMmbxctRvM.VM1mDyNI50GeZ79asEZu.4itlrhJeCv62Fg_rrribQ7NBnmNvbWGmmuphqmzEBfmiRPLc2OrNOfJDlgpWWOI5H1uuN25xJU\",cRq: {ru: 'aHR0cHM6Ly93d3cubGxveWRhdGtpbnNvbi5uZXQvcG9zdHMvMjAyNC9zdHVwaWQtcHJvYmxlbXMtcmVxdWlyZS1zdHVwaWQtc29sdXRpb25zLWNsb3VkZmxhcmUtaXMtYnJlYWtpbmctbXktc3Zncy8=',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: 'cDjvLnxyV6ra3fKu0x+82nzO2x49f4LcB0HB1dHoJXZitMJFM8Pogx6scBRwlYeTYsFcmEbV07+UL4VoDVz1t5qOs4V/hV+aQnkmpg00v2cZ6Ttee7jTtE6PYZByuF+P6Jl7KOZ7g/ixwjAzPxRkRi31G0U/CshqWbFEdxVnA58xkz/crjTrBbjUq2qZ2REAv5tKoT7+QPcuWS426D+PrMEPyOoeWy+lu1zerrP+kJ6FPBulJNSb7NdmnAmuWzduzFiLCRWdRrNBQUfG79aJTMt45jfOmqB/HbJ+QRrG/ustyf/vtRiGWkB72HI0T6XyjoZvWNlCqkncWkospD2rr07zrDZLXEaMHNJTsmGw69uVegdqFqvI8ZvOuMpwFzchI1GSXaLmcD21sI8k3a/MmmC96qDRhc/AlpuLHrEl3GY3p1NNbT5azndW1eXWO/f0aHdDu3qVRgRQ45P9y1fNV013Yz+N+XujjlSs1r/lWEert1Yf9miIKLhclBLDqBTGFY5//JWHv5mvHViTMJWIwg==',t: 'MTcyNzAzMTcxMy4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: '3kOzPe8cbZp9Apfh3m8vCwR/qmUagEzH3EQTZhblAa4=',i1: '+swIYX8A3nQZmSwqTs5ULg==',i2: '6QR4zgiTfT8MZOyaO+InIw==',zh: 'DT11iizgJkR3Z/Viv1Hua/ZP8JMYcH6VwibVp+DOxQ8=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'vtKxkHmNtFMeMz1PESRgtDoQGdSaW3hc85taIROluOY=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=8c748bd04ee22a00';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/posts\\/2024\\/stupid-problems-require-stupid-solutions-cloudflare-is-breaking-my-svgs\\/?__cf_chl_rt_tk=oDKl5baIq7fCeIZvcwPRjPgsILv9NHFlkhLO1jlv0b0-1727031713-0.0.1.1-4927\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41614567",
    "commentBody": "Cloudflare Is Breaking My SVGs? (lloydatkinson.net)133 points by thunderbong 14 hours agohidepastfavorite37 comments elithrar 6 hours agoThere's ~four of us trying to reproduce this right now, using Astro and Remix, and cannot at all. An important note: React-based frameworks tend to use camelCase attributes vs. hyphen-case (which is the output) in components: including the icon library being used here. Something during the build process is not converting them to hyphen-case. * I've pasted a decently complex SVG exported from Figma into a Remix component verbatim (hyphen-attributes) and it renders fine: https://9b14a265.test-broken-svgs-remix.pages.dev/ (scroll down) * I've rewritten those attributes to camelCase: and again, renders fine - https://1af766a8.test-broken-svgs-remix.pages.dev/ * This is all deployed via the Pages Build system; no local builds at all. * Someone else on the team has an Astro example stood up with the specific unplugin-icons library: https://astro-svg.pages.dev/ - cannot reproduce the invalid SVG attributes. We're going to continue investigating but don't see this as widespread and don't yet have any other reports. That there is a _difference_ between the direct deploy vs. using Pages Builds is a problem, though. We've also asked the Astro folks to understand if there's something up here as well. (If not clear: I work at Cloudflare) reply midnitewarrior 5 hours agoparentMight this happen if the author did not have the Mime type set correctly when serving his .svg assets? reply Freak_NL 12 hours agoprevSo this is not Cloudflare, the hosting service, renaming SVG attributes as they are served, but the Cloudflare Pages build process hooked to the developer's Git repo. Weird behaviour. I'm interested in reading the actual reason that build ends up doing this. (Side note: I really wish people would leave out the image memes when writing something like this. The animated one at the end even makes it hard to read the final paragraph.) reply jgrahamc 7 hours agoparentYeah, really weird. I'm raising this internally to figure out WTF. OP can email me (jgc@cloudflare) with account details but I'm doing my own test right now on jgc.org. EDIT: OP please drop me an email. Three of us internally at Cloudflare have tried to reproduce this and failed and we need to chat with you about what's in your build process so we can try to narrow this down. Cheers! I made a simple test: https://jgc.org/svg and we made a couple more using Astro: https://misty-mouse-d370.pages.dev/hello and https://cec6e042.laz-svg-test-assets.pages.dev/. reply lloydatkinson 9 hours agoparentprevTurn on reader mode. reply asimpletune 9 hours agoprevI really empathize with the author, having dealt with many similar weird issues myself. To Cloudflare's credit however, since most of the Pages/Workers stuff is open source, one can usually find the problem in the source code and submit a fix oneself. It's an example of a product that's well designed, generous in its free-tier, yet occasionally confounding, probably due to the speed their team moves. Also wanted to add that I appreciate the author writing about the bug and their debugging process. That's definitely something I wish I had done more of, instead of thinking at the time 'I filed the issue, does the whole world need to know about this?' (Sometimes daily reading of HN can have this effect, I think) Then months go by, the issue I filed was moved or something happened to it, and I lost a way of tracking it... So I think it's important to a.) file the issues but b.) document the issue in your own way while everything's fresh in your mind. reply lloydatkinson 6 hours agoparentThank you for your kind words. I found myself wishing I wrote more of how I debug while I wrote it and will be doing more of it. As you say, issue trackers (even GitHub) sometimes suffer from being swamped and neglected. reply Spunkie 12 hours agoprevDo you have html/js/css minification turned on in your cloudflare account/domain? It seems from the articles conclusion that it shouldn't be related. But I had similar issues with SVG and css when I first deployed my nuxt app to CF pages. I only thought to try turning off CF minification because there was some tiny footnote about it being an issue on CF somewhere in the nuxt docs or maybe GH issues. reply lloydatkinson 9 hours agoparentI forgot to mention that. No, I have all of that turned off. Turns out it’s being deprecated soon anyway. reply delanyoyoko 3 hours agoprevI've been using cloudflare pages like two years now and all svg icons are doing normal they're supposed to. I use Sveltekit. So I think, it's between your framework and cloudflare pages. reply genezeta 12 hours agoprev> > lloydatkinson.net shrug reply genewitch 11 hours agoparenthttps://i.imgur.com/SRzsvmI.png reply genezeta 8 hours agorootparentThanks! reply immibis 10 hours agoparentprevDon't you just love internet balkanization? reply mkj 13 hours agoprevHey, at least here cloudflare isn't inserting captchas into json (turns out Roundcube webmail breaks horribly when someone does that!) reply louthy 8 hours agoparentCloudflare and its captchas are the scourge of the modern internet. Especially if you run a VPN on your router. The amount of times I’ve been in seemingly endless click-the traffic-light loops is just ridiculous. Many of the times I’ll just refuse to use the site, but some things I need to endure. They clearly have no incentive to be better either. The pain falls entirely on the visitor. If you can’t tell I’m human then you’re doing it wrong. reply Pikamander2 7 hours agorootparent> If you can’t tell I’m human then you’re doing it wrong. I'm gonna go out on a limb and say that if multiple multi-billion dollar companies struggle with it, then it's less of a solved problem than you're implying. > Especially if you run a VPN on your router. Very few people do this, and almost all of them are also using ad blockers. If using Cloudflare protects a site from thousands of bot attacks while inconviencing a few real users with esoteric setups, that tradeoff is well worth it for most site owners. I've set up Cloudflare on hundreds of websites and when you look at the amount of nasty stuff it blocks, it quickly becomes clear why it's needed. If you want to blame someone, blame the Russian and Chinese governments (among others) for doing little to nothing about their unfathomably massive botnets. reply hombre_fatal 4 hours agorootparentprevMost abuse on the internet comes over VPNs, so why is it surprising when website operators make you jump through hoops if you choose to blend in with low quality VPN traffic? Also, why would it be easy to tell that your request is from one of the good guys? reply CodesInChaos 1 hour agoparentprevCloudflare definitely used to replace JavaScript by an HTML Captcha page, which obviously went well. But it's been a while since that last happened to me. reply fouc 3 hours agoprevNot sure if it's related, I had to disable Cloudflare's \"Rocket Loader\" feature 3 days ago. It suddenly started breaking the hamburger menu / menu accordion ~4-5 days ago. reply JoosToopit 8 hours agoprevCloudFlare is cancer: lloydatkinson.net Why have I been blocked? This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data. reply majke 8 hours agoparentWhat error code specifically you get? reply Jamie9912 4 hours agoparentprevThat is some site owner that has configured Cloudflare to block you. It's the same thing as an establishment hiring security guards to block access. reply michaelt 4 hours agorootparentThat is some site owner who wanted free cdn/ddos protection, accepted the default settings, and has no way of knowing which the blocked users are blocked wrongly. I used to use cloudflare on my static website and I had no idea I was blocking all tor users until someone on HN let me know. Cancer is too strong a word though. It’s more like a bad case of jock itch IMHO. reply Jamie9912 4 hours agorootparentI agree, default \"Medium\" firewall setting is the very first thing I change on a new site. reply moralestapia 8 hours agoprev>and asking around on the Cloudflare Discord How was that? Last time I tried to submit a bug report there the experience was terrible ... reply CherryJimbo 6 hours agoparentThe Cloudflare Discord is interesting. It's mostly maintained by community volunteers (Champs and MVPs), with team participation varying drastically from team to team. The community does a great job when they can, but it doesn't feel like most teams are empowered to help people there. There are some fantastic employees who do help regularly though, but as far as I know, most employees are doing this out of kindness, since it's not actually part of their role (whether Cloudflare should make this a part of some folks roles is definitely something they should think about heavily). The best way in Discord is to post in a forum channel like #pages-help or #general-help. There's tooling the community has to actively monitor and reply to those, vs a fast-moving text channel where things can be buried quickly. This is one of the reasons Discord isn't a great platform for offering support. Community Champs / MVPs have the ability to escalate some things, but posting on HN will always get more attention. We've been raising issues around some things that are impacting lots of people every single day (wrangler.toml + Pages config breaking projects regularly for example), but if you want something fixed quickly, making some noise on social media is always going to be the quickest way, sadly. reply verochmar 5 hours agoparentprevHi. We definitely don’t want anyone to have a bad experience on Discord and are working on a project to improve it. Please let me know what you think we should improve (either here or email me vmarin@cloudflare.com) reply lloydatkinson 6 hours agoparentprevIn the past very helpful for different issues. This time around, still helpful but I noticed a lot of (alleged) Cloudflare employees (based on their username colour) were talking at the time and none of them seemed very interested. reply jgrahamc 6 hours agorootparentI'm interested: jgc@cloudflare.com. reply ezekiel68 8 hours agoprev> The easy approach would be to run a string replacement over the build output, but this is far too destructive and error-prone. It would also stop me from writing this post! Tell me you refuse to learn awk/sed (even Windows compatible versions such as from Git Bash, WSL, or Cygwin) without telling me you refuse to learn awk/sed. reply Etheryte 7 hours agoparentThis isn't a solution, because then you'd run into problems the moment those keywords are used in a different context, for example in this blog post describing the bug. Any language you may fancy has a parser readily available these days, there's no reason not to use it. reply lloydatkinson 6 hours agorootparentThat is exactly why I used an actual parser not some glorified regex. Thank for you rebutting their ridicilous remark. I even literally said in their quote. Why anyone would think a glorified regex is a solution to updating deeply nested HTML elements is beyond me, is the famous \"HTML regex\"[1] answer not enough? As bad a sugesting Chrome should just be a wrapper for sed/awk. [1]: https://stackoverflow.com/questions/1732348/regex-match-open... reply GrantMoyer 6 hours agoparentprevYou've invoked the canonical rant: https://stackoverflow.com/a/1732454/2640937 reply eric_h 3 hours agorootparentIt's been a while since that one's crossed my browser. This is the first time I've noticed the moderator's note at the bottom: \"Moderator's Note This post is locked to prevent inappropriate edits to its content. The post looks exactly as it is supposed to look - there are no problems with its content. Please do not flag it for our attention.\" reply hombre_fatal 4 hours agoparentprevIf you're using a meme to make a point, you should probably just make your point and see if your comment is worthwhile once it's not relying on a rhetorical gimmick. I'll try: \"You could've used awk/sed\". Great comment? reply adityapatadia 4 hours agoprev [–] For image processing, use a proper SaaS dedicated to it: https://www.gumlet.com/image-optimization/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Users are reporting issues with SVG (Scalable Vector Graphics) rendering when using Cloudflare, particularly with React-based frameworks like Astro and Remix.",
      "The problem seems to be related to the build process not converting camelCase attributes to hyphen-case, but it is not widespread according to Cloudflare employees.",
      "Cloudflare is actively investigating the issue, and users are encouraged to provide account details for further testing and resolution."
    ],
    "points": 133,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1726978307
  },
  {
    "id": 41612665,
    "title": "Flow Computing aims to boost CPUs with ‘parallel processing units’",
    "originLink": "https://spectrum.ieee.org/parallel-processing-unit",
    "originBody": "COMPUTING NEWS Startup Says It Can Make a 100x Faster CPU Flow Computing aims to boost central processing units with their ‘parallel processing units’ DINA GENKINA20 SEP 20243 MIN READ Dina Genkina is the computing and hardware editor at IEEE Spectrum FLOW COMPUTING",
    "commentLink": "https://news.ycombinator.com/item?id=41612665",
    "commentBody": "Flow Computing aims to boost CPUs with ‘parallel processing units’ (ieee.org)124 points by rbanffy 21 hours agohidepastfavorite64 comments Animats 19 hours agoDoes anyone know what they mean by \"wave synchronization\"? That's supposedly their trick to prevent all those parallel CPUs from blocking waiting for data. Found a reference to something called that for transputers, from 1994.[1] May be something else. Historically, this has been a dead end. Most problems are hard to cut up into pieces for such machines. But now that there's much interest in neural nets, there's more potential for highly parallel computers. Neural net operations are very regular. The inner loop for backpropagation is about a page of code. This is a niche, but it seems to be a trillion dollar niche. Neural net operations are so regular they belong on purpose-built hardware. Something even more specialized than a GPU. We're starting to see \"AI chips\" in that space. It's not clear that something highly parallel and more general purpose than a GPU has a market niche. What problem is it good for? [1] https://www.sciencedirect.com/science/article/abs/pii/014193... reply bhouston 19 hours agoparentGPUs have wavefronts so I assume it is similar? Here is a page that explains it: https://gpuopen.com/learn/occupancy-explained/ reply adrian_b 4 hours agorootparentNope. AMD's \"wavefront\" is an obfuscated word for what NVIDIA calls \"warp\". NVIDIA's \"warp\" is an obfuscated word for what has been called for many decades in the computer literature as \"thread\". (NVIDIA's \"thread\" is an obfuscated word that means something else than what it means in the non-NVIDIA literature.) NVIDIA has thought that it is a good idea to create their own terminology where many traditional terms have been renamed without any reason. AMD has thought that it is a good idea to take the entire NVIDIA terminology and replace again all terms with other words. reply mystified5016 1 hour agorootparentI'd assume that 'warp' is taken from textiles: https://en.m.wikipedia.org/wiki/Warp_and_weft A warp is a thread, but a thread within a matrix of other threads. I'm not into GPU programming, but doesn't nvidia have some notion of arranging threads in a matrix sort of like this? reply adrian_b 33 minutes agorootparentNope. In NVIDIA parlance, a thread is the body of a \"parallel for\" structure, i.e. the sequence of operations that are executed for an array element, which are executed by one SIMD lane of a GPU. A \"warp\" is a set of \"threads\", normally of 32 \"threads\" for the NVIDIA GPUs, the number of \"threads\" in a \"warp\" being the number of SIMD lanes of the execution units. CUDA uses what Hoare (1978) has named \"array of processes\" and which in many programming languages is named \"parallel for\" or \"parallel do\". This looks like a \"for\" loop, but its body is not executed sequentially in a loop, but the execution is performed concurrently for all elements of the array. A modern CPU or GPU consists of many cores, each cores can execute multiple threads and each thread can execute SIMD instructions that perform an operation for multiple array elements, on distinct SIMD lanes. When a parallel for is launched in execution, the array elements are distributed over all existing cores, threads and SIMD lanes. In the case of NVIDIA, the distribution is handled by the CUDA driver, so it is transparent for the programmer, who does not have to know the structure of the GPU. The use by NVIDIA of the word \"thread\" would have corresponded with the reality if the GPU would not have used SIMD execution units. Real GPUs use SIMD instructions that process a number of array elements typically between 16 and 64. NVIDIA's \"warp\" is the real thread executed by the GPU, which processes multiple array elements, while NVIDIA's \"thread\" is what would have been executed by a thread of a fictitious GPU that does not use SIMD, so it would process only one array element per thread. reply narag 19 hours agoparentprevWe're starting to see \"AI chips\" in that space. \"Positronic\" came to my mind. reply darby_nine 11 hours agorootparentMy god the future sucks far more than we could have ever imagined. Imagine being sold a chatbot and being told it's an android! reply narag 4 hours agorootparentFWIW, I already carry an android in my pocket. reply mikewarot 15 hours agoparentprevThe reason problems are hard to fit into most of what's tried is that everyone is trying to save precious silicon space and fit a specific problem, adding special purpose blocks, etc. It's my belief that this is an extremely premature optimization to make. Why not break it apart into homogeneous bitwise operations? That way everything will always fit. It would also simplify compilation. reply interroboink 20 hours agoprevSeems like a nice idea — instead of the stark CPU/GPU divide we have today, this would fit somewhere in the middle. Reminds me slightly of the Cell processor, with its dedicated SPUs for fast processing, orchestrated by a traditional CPU. But we all saw how successful that was (: And that had some pretty big backing. Overcoming the inertia of the current computing hardware landscape is such a huge task. Maybe they can find some niche(s). reply aleph_minus_one 7 hours agoparent> Reminds me slightly of the Cell processor, with its dedicated SPUs for fast processing, orchestrated by a traditional CPU. But we all saw how successful that was (: The success of the Cell is more subtle: - (It seems many) game developers hated it because it is so different to program than other CPUs of other game consoles of its time (in particular the CPU of the Xbox 360). For game studios, time to market and portability of the game to other consoles is important. - On the other hand, scientists who ported their high-performance numerical computations to the Cell seem to have loved it. Such software is often custom-built for the underlying hardware, and here cost (of the hardware) and possible speed are the measures on which to evaluate the hardware. Here the Cell processor of a PS3 cluster was much more competitive than other available solutions (GPGU did not really exist at this time). reply cedilla 6 hours agorootparentGabe Newell of Valve famously hated the Cell architecture, and I think that's very illustrative. He is of the generation of game devs that was very willing to try wild algorithms and hand-massage assembly and use all tricks to get 3D fast, so the PS3 should have been a perfect fit. But he did not like to have to start back at square one. reply amelius 4 hours agorootparentprevIsn't that because game developers use conditional statements more, and scientists typically have a flow-graph that describes a computation and this computation doesn't have conditional parts? So it is a more natural fit? reply aleph_minus_one 3 hours agorootparentI don't know, in particular concerning the game developer perspective. But from my observation, scientists who develop high-performance computing algorithms often think much deeper about the mathematical structure of their problems than game developers do. I thus have a feeling that what you describe as \"flow-graph that describes a computation\" is rather a result of this deep analysis. I can easily imagine that this would partly also work for video games, but I would hypothesize either this is too much work that is not really rewarded in the game industry (the game industy is known (\"crunch time\") for having to churn out lots of new code fast), or if you are a lot into this kind of thinking, the game industry might not be the most rewarding place to work at. reply exe34 1 hour agorootparenta game developer might have an algorithm that they have to implement. a computational scientist might find an alternative algorithm that makes different still-acceptable tradeoffs and yet fit more naturally in a new arch. reply RaftPeople 2 hours agoparentprev> Reminds me slightly of the Cell processor I was thinking the same. Also the Tilera CPU with many cores and mesh network (back in mid 2000's, eventually bought by Nvidia and used in something, don't remember). Tangent: Back in early 2000's I had a hobby project (ALife with ANN brain) and I was looking for more computation. Multiple CPU's was not ideal, GPU wasn't ideal because the read/write/computation model only matched 1/2 of my ANN's flow and was a mismatch for the other half. I read about a new cpu and I ended up talking to one of the key guys from Tilera, I was pretty impressed they would take the time to talk to some random guy working on a hobby project. I asked about the performance of individual computational units (assuming custom could beat the industry) and he surprised me when he responded \"nobody is going to beat Intel at integer, you won't get an increase from that perspective\" reply mnky9800n 11 hours agoparentprevAlso thinking machines corporation https://en.wikipedia.org/wiki/Thinking_Machines_Corporation reply winwang 16 hours agoparentprevI'd believe more in a heterogenous chip (e.g. MI300X, Apple M series, or even APUs) than in completely new chip tech. reply jbellis 8 hours agoparentprevIsn't this where \"NPUs\" are going now? reply adrian_b 3 hours agoparentprevWhat they say is far too vague, so it is impossible to know whether they have any new and original idea. It is well known that the CPU cores that are optimized for high single-threaded performance are bad for multithreaded tasks, because they have very poor performance per power and per area, so you cannot put many of them in a single package, because there are limits both for the die area and for the power dissipation. There are 3 solutions for this problem, all of which are used in many currently existing computers. 1. A hybrid CPU can be used, which has a few cores optimized for single-threaded performance and many cores optimized for multithreaded performance, like the Intel E-cores or the AMD compact cores. 2. One can have one or more accelerators for array operations, which are shared by the CPU cores and whose instruction streams are extracted from the instruction streams of the CPU cores (like in the CPUs from many decades ago the floating-point instructions were extracted from the CPU intruction streams and they were executed by floating-point coprocessors). The instructions executed by such accelerators must be defined in the ISA of the corresponding CPUs. Examples are the Arm SME/SME2 (Scalable Matrix Extension) and the Arm SSVE (Streaming Scalable Vector Extension) instruction sets. These ISA extensions are optional starting from Armv9.2-A or Armv8.7-A. AFAIK, for now only the recent Apple CPUs support them, but in the future the support for them might become widespread. 3. The last solution is to have an accelerator for array operations that has a mechanism independent from the CPU cores for fetching and decoding its own instruction stream. The CPU cores have to launch programs on such accelerators and get results when they are ready. Such completely independent accelerators are either parts of GPUs or they may be completely dedicated for computing tasks, when they no longer include the special-function graphics hardware. Any up-to-date laptop CPU already includes inside its package at least 2, if not all 3 of these solutions, to provide a good multithreaded performance. For servers, it is much less useful to have all these variants in a single package, because one can mix for instance one server with big cores with high single-threaded performance with many servers using much more compact cores per socket, for good multithreaded performance, and the servers can use multiple discrete GPUs per server. It is not clear with whom this \"Flow Computing\" wants to compete. They certainly cannot make better compact cores than Intel, AMD or Arm. They cannot make something like a SME accelerator, because that must be tightly integrated with the cores for which it functions as a coprocessor. So their \"parallel processing units\" can be only competitors for the existing GPUs or NPUs. Due to their origins in execution units for shader programs the current GPUs are not versatile enough. There still are programs that are easy to run on CPU cores but it is difficult to convert them to a form that can be executed by GPUs. So there would be a place for someone that could design an architecture more convenient than that of the current GPUs. However there is no indication in that article that there exists any problem for which the \"Flow Computing\" PPUs are better than the current GPUs or NPUs. If the PPUs have some kind of dataflow structure, then their application domain would be even more restricted than for the current GPUs and NPUs. EDIT: Now I have read their whitepaper \"Design goals, advantages and benefits of Flow Computing\", from HotChips. However, what that paper says about their patented architecture raises more questions than provides any answers. Their description of the PPUs is very similar to the description of Denelcor HEP from 1979. HEP (Heterogeneous Element Processor) was an experimental computer designed by Denelcor, Inc., which was intended to be a competitor for the supercomputers like Cray-1 (1976). While HEP was based on very good ideas, its practical implementation was very poor, using non-optimized and obsolete technology in comparison with Cray, so it has never demonstrated a competitive performance. The lead architect of HEP has later founded \"Tera Computer Company\", in 1987, which has designed computers based on the same ideas with HEP. Tera Computer had very modest results, but somehow it has succeeded in 2000 to buy the Cray Research division of Silicon Graphics, then it was renamed as Cray, Inc. (now a subsidiary of HPE). While Cray-1 and its predecessors (TI ASC and CDC STAR) were based on exploiting the parallelism of hardware pipelines with array operations, which can provide independent operations on distinct array elements, which can be executed in parallel in different pipeline stages, HEP was based on exploiting the parallelism of hardware pipelines with fine-grained multithreading, where independent instructions from distinct threads can be executed in parallel in different pipeline stages. HEP had multiple CPU cores (\"core\" was not a term used at that time). Each CPU core was a FGMT core, which could switch at each clock cycle between an extremely large number of threads. (FGMT is a term that has been introduced only much later, in 1996, with its abbreviation only in 1997; at the time of HEP, they used the term \"fine-grained multiprogramming\") The very large number of threads executed by each FGMT core (e.g. hundreds) can hide the latencies of data availability. The description of the \"Flow Computing\" PPUs is about the same as for HEP (1979), i.e. they appear to depend on FGMT with a very large number of threads (called \"fibers\" by Flow Computing) to hide the latencies. Unlike GPUs and NPUs, but like HEP, it seems the \"Flow Computing\" PPUs rely mainly on multithreading (a.k.a. TLP) to provide parallelism, and not on array operations (a.k.a. DLP). The revival of this old idea could actually be good, but the whitepaper does not provide any detail that would indicate whether they have found a better way to implement this. reply CyberDildonics 6 hours agoparentprevSIMD units already fit somewhere in the middle. reply wmf 16 hours agoprevThis is based on legitimate (although second-tier) academic research that appears to combine aspects of GPU-style SIMD/SIMT with Tera-style massive multithreading. (main paper appears to be https://www.utupub.fi/bitstream/handle/10024/164790/MPP-TPA-... ) Historically, the chance of such research turning into a chip you can buy is zero. reply gnabgib 20 hours agoprevDiscussion (28 points, 3 months ago, 32 comments) https://news.ycombinator.com/item?id=40650662 reply elromulous 18 hours agoprevDoes anyone have any knowledge/understanding on how this is (or isn't?) fundamentally different from Intel's Xeon Phi? https://en.wikipedia.org/wiki/Xeon_Phi reply CyberDildonics 5 hours agoparentHow is it the same? The Xeon Phi was basically just smaller/weaker cores but more of them and with their SIMD units still there. reply systarray 14 hours agoprevSystolic processing, circa 1979. A concept that gets reinvented every decade: https://en.wikipedia.org/wiki/Systolic_array reply mikewarot 11 hours agoparentAnd it always gets prematurely optimized to fit a specific problem instead of being made general purpose compute engine. FPGAs are essentially horrible systolic arrays. They're lumpy, and have weird routing hardware that isn't easy to abstract out. Those lead to multiple day compile times in some cases. They don't pipeline things by default. The programming languages used are nowhere near a good fit to the hardware. It's just a mess. It happens every time. reply imtringued 9 hours agoparentprevSystolic arrays are only used for fixed function computing within the context of individual instructions, e.g. a matrix multiplication or convolution unit. In practice however, most of these arrays are wave front processors, because programmable processing elements can take variable amounts of time per stage and therefore become asynchronous. AMD's XDNA NPU is based around a wave front array of 32 compute tiles, each of which can either perform a 4x8 X 8x4 matrix multiplication in float16 or a 1024 bit vector operation per cycle. reply almostgotcaught 6 hours agorootparentIt's funny how everyone in this thread is wrong on the high level concepts (blind leading the blind) but here you're wrong on the specifics too > around a wave front array of 32 compute tiles, each of which can either perform a 4x8 X 8x4 matrix multiplication in float16 or a 1024 bit vector operation per cycle. 1. XDNA is exactly the opposite of a \"wavefront\" processor. Each compute core is single thread vector VLIW. So you have X number of independent fixed function operators. 2. There is no XDNA product with 32 cores and 4x8x4 matmul. Phoenix has 20 compute cores (16 usable) and performs 4x8x4. Strix has 32 and performs 16x32x8 matmul. reply yeahwhatever10 20 hours agoprevWhen will we get the “Mill” cpu? reply theLiminator 18 hours agoparentI've been following that saga for a long time. Seems mostly like vapourware sadly. reply sparkie 9 hours agorootparentMore time spent filing patents than implementation. reply XorNot 14 hours agorootparentprevProbably implementation hell. The big idea theoretically works, but when you get into the details the compromises for implementation steal away the gains. I'm always reminded of the rotary internal combustion engine: in theory there's a whole suite of benefits, in practice they're \"interesting\" but not that great once practically built. reply mshook 19 hours agoparentprevAt this point, probably never it seems... reply rbanffy 6 hours agorootparentAnd considering their effort in patenting every idea related to it, we’ll only see it when the first implementer gets sued. reply throwawayffffas 19 hours agoprevHow is this different from an integrated gpu other than it presumably doesn't do graphics. reply pier25 19 hours agoprevI'm probably missing something but why not use gpus for parallel processing? reply nine_k 18 hours agoparentGPUs work on massive amounts of data in parallel, but they execute basically the same operations every step, maybe skipping or slightly varying some steps depending on the data seen by a particular processing unit. But processing units cannot execute independent streams of instructions. GPUs of course have several parts that can work in parallel, but they are few, and every part consists of large amounts that execute the same instruction stream simultaneously over a large chunk of data. reply winwang 16 hours agorootparentThis is not true. Take the NVidia 4090. 128 SMs = 4x128=512 SMSPs. This is the number of warps which can execute independently of each other. In contrast, a warp is a 32-width vector, i.e. 32 \"same operations\", and up to 512 different batches in parallel. So, it's more like a 512-core 1024-bit vector processor. That being said, I believe the typical number of warps to saturate an SM is normally around 6 rather than 4, so more like 768 concurrent 32-wide \"different\" operations to saturate compute. Of course, the issue with that is you get into overhead problems and memory bandwidth issues, both of which are highly difficult to navigate around -- the register file storing all the register of each process is extremely power-hungry (in fact, the most power-hungry part I believe), for example. A PPU with less vector width (e.g. AVX512) would have proportionally more overhead (possibly more than linearly so in terms of the circuit design). This is without talking about how most programs depend on latency-optimized RAM (rather than bandwidth-optmized GDDR/HBM). reply faangguyindia 5 hours agorootparentThe Nvidia 4090 indeed has 128 SMs, but the formula you provided (128 SMs = 4x128=512 SMSPs) isn't quite accurate. Each SM contains 64 CUDA cores (not SMSPs), and these are the units responsible for executing the instructions from different warps. The term \"SMSP\" isn't typically used to describe CUDA cores or warps in Nvidia’s architecture. reply rnrn 4 hours agorootparentwinwang’s comment is correct, yours is wrong. “cuda core” refers to one lane within the SIMT/SIMD ALUs. These lanes within a SMSP don’t execute independently. The term SMSP is definitely used for nvidia’s architecture : https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.... > smsp > Each SM is partitioned into four processing blocks, called SM sub partitions. The SM sub partitions are the primary processing elements on the SM. (Note that this kernel profiling guide doesn’t use the term “cuda cores” at all) Also there are 128 “cuda cores” per SM in 4090, not 64 : https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvid... > Each SM in AD10x GPUs contain 128 CUDA cores reply nine_k 16 hours agorootparentprevI'm happy to stand corrected; apparently my idea about GPUs turned obsolete by now. reply JackSlateur 19 hours agoparentprevBecause GPU are physically built to manage parallel task, but only a few kinds They are very specialized CPU are generics, they have lots of transistors to handle a lot of different instructions reply markhahn 3 hours agorootparentI think this is a really unfortunate way to explain it. The issue is not that CPUs have a lot of different instructions - hardly anyone uses decimal math instructions, for instance, and no one cares about baroque complex addressing modes. The difference is that GPU code is designed to tolerate latency by having lots of loop iterations treated as threads. A modern CPU tolerates latency by maintaining the readiness of hundreds of individual instructions (\"in flight\") - essentially focusing on minimizing the execution latency of each instruction. (which also explains how CPUs use caches and very high clocks, but wind up with somewhat fewer cores and threads.) (note that I'm using cores and threads correctly here, not the nvidia way.) reply Groxx 18 hours agorootparentprevAlso moving data to and from the GPU takes MUCH more time than between CPU cores (though combined chips drastically lower this difference). reply markhahn 3 hours agorootparentin the olden days of gp-cpu this was certainly true. is it still true? do you just mean \"latency overhead for setting up a single PCIe transaction is much larger than flinging a cache line across QPI/etc\"? reply exabrial 17 hours agoprevI’m still waiting for a clockless core… some day reply aidenn0 16 hours agoparentwww.greenarraychips.com reply rowanG077 14 hours agoparentprevConsidering place and routing of even synchronous digital logic is at the edge of what we can do computationally I really don't see this happening anytime soon. reply somat 17 hours agoprevWhenever I see the word \"fintech\" this is the article I am expecting, Instead I am disappointment with some drivel about banks. I am not sure what is wrong with me, you would think my brain would have figured it out by now, but it always parses it wrong. perhaps if it were \"finctech\" that would help. reply cryptoz 17 hours agoparentI’ve not yet had this problem but I surely will now! Thanks I guess. reply imtringued 9 hours agoparentprevThe word \"fintech\" is disappointing, most because everyone calls their classical old school finance company a fintech that has a minor tech component. E.g. something like Solaris Bank is a fintech, but BaaS providers are a rarity and not the average fintech. reply brotchie 20 hours agoprev“Now, the team is working on a compiler for their PPU” good luck! reply bhouston 19 hours agoparentWhile the Itanium failed the Ageia PPU did succeed with its compiler. It was acquired by NVIDIA and became CUDA. https://en.wikipedia.org/wiki/Ageia reply gdiamos 19 hours agorootparentIt did indeed get merged into the CUDA group but I think the internal CUDA project predated it, or at least, several of the engineers working on it did reply gregw2 8 hours agorootparentprevI always thought CUDA grew out of Ian Buck's PhD thesis under Pat Hanrahan; why do you credit Ageia? reply mepian 17 hours agorootparentprevThat's not the same PPU, is it? reply greenavocado 19 hours agoparentprevIs this like the Itanium architecture with its compiler challenges? reply claxo 19 hours agoparentprevIndeed, a very smart compiler would be necessary, perhaps too much for the current compiler art, like the itaniun. But...how about specializing to problems with inherent paralelism? LLMs maybe? reply Kon-Peki 4 hours agorootparentThe Itanium C and FORTRAN compilers eventually became very, very good. By then, the hardware was falling behind. Intel couldn't justify putting it on their latest process node or giving it the IPC advancements that were developed for x86. If you wanted to do something similar right now, it's possible to succeed. Your approach has to be very different. Get a lot of advancements into LLVM ahead of time, perhaps. Change the default ideas around teaching programming (\"use structured concurrency except where it is a bad idea\" vs \"use traditional programming except where structured concurrency makes sense\", etc) But no, throwing a new hardware paradigm out into the world with nothing but a bunch of hype is not going to work. That could only work in the software world. reply petermcneeley 19 hours agoprev> Now, the team is working on a compiler for their PPU I think a language is also required here. Extracting parallelism from C++ is non trivial. reply poincaredisk 19 hours agoparentSomething similar to CUDA or OpenCl should do it, right? reply johnklos 19 hours agoprev [–] Tell us something new, please. reply johnklos 13 hours agoparent [–] This is a duplicate of https://news.ycombinator.com/item?id=40650662, and this article has nothing new in it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Flow Computing has introduced a new technology called 'Parallel Processing Units' (PPUs) that claims to make CPUs 100 times faster.",
      "This innovation aims to significantly enhance computing performance by leveraging parallel processing, which allows multiple tasks to be executed simultaneously.",
      "The announcement has garnered attention due to its potential to revolutionize CPU speeds and efficiency, making it a notable development in the computing industry."
    ],
    "commentSummary": [
      "Flow Computing proposes integrating 'parallel processing units' (PPUs) with CPUs to avoid data wait times, enhancing parallel processing efficiency.",
      "This concept, reminiscent of past technologies like transputers and the Cell processor, faces challenges in task parallelization but shows promise with neural networks.",
      "The discussion compares PPUs to existing technologies such as GPUs and Xeon Phi, emphasizing the potential and complexities of modern parallel processing integration."
    ],
    "points": 124,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1726952667
  }
]
