[
  {
    "id": 41891694,
    "title": "Accountability sinks",
    "originLink": "https://aworkinglibrary.com/writing/accountability-sinks",
    "originBody": "2024-10-16 Accountability sinks A Reading Note In The Unaccountability Machine, Dan Davies argues that organizations form “accountability sinks,” structures that absorb or obscure the consequences of a decision such that no one can be held directly accountable for it. Here’s an example: a higher up at a hospitality company decides to reduce the size of its cleaning staff, because it improves the numbers on a balance sheet somewhere. Later, you are trying to check into a room, but it’s not ready and the clerk can’t tell you when it will be; they can offer a voucher, but what you need is a room. There’s no one to call to complain, no way to communicate back to that distant leader that they’ve scotched your plans. The accountability is swallowed up into a void, lost forever. Davies proposes that: For an accountability sink to function, it has to break a link; it has to prevent the feedback of the person affected by the decision from affecting the operation of the system. Davies, The Unaccountability Machine, page 17 Once you start looking for accountability sinks, you see them all over the place. When your health insurance declines a procedure; when the airline cancels your flight; when a government agency declares that you are ineligible for a benefit; when an investor tells all their companies to shovel so-called AI into their apps. Everywhere, broken links between the people who face the consequences of the decision and the people making the decisions. That’s assuming, of course, that a person did make a decision at all. Another mechanism of accountability sinks is the way in which decisions themselves cascade and lose any sense of their origins. Davies gives the example of the case of Dominion Systems vs Fox News, in which Fox News repeatedly spread false stories about the election. No one at Fox seems to have explicitly made a decision to lie about voting machines; rather, there was an implicit understanding that they had to do whatever it took to keep their audience numbers up. At some point, someone had declared (or else strongly implied) that audience metrics were the only thing that mattered, and every subsequent decision followed out from that. But who can be accountable to a decision that wasn’t actually made? It’s worth pausing for a moment to consider what we mean by “accountable.” Davies posits that: The fundamental law of accountability: the extent to which you are able to change a decision is precisely the extent to which you can be accountable for it, and vice versa. Davies, The Unaccountability Machine, page 17 Which is useful. I often refer back to Sidney Dekker’s definition of accountability, where an account is something that you tell. How did something happen, what were the conditions that led to it happening, what made the decision seem like a good one at the time? Who were all of the people involved in the decision or event? (It almost never comes down to only one person.) All of those questions and more are necessary for understanding how a decision happened, which is a prerequisite for learning how to make better decisions going forward. If you combine those two frameworks, you could conclude that to be accountable for something you must have the power to change it and understand what you are trying to accomplish when you do. You need both the power and the story of how that power gets used. The comparisons to AI are obvious, inasmuch as delegating decisions to an algorithm is a convenient way to construct a sink. But organizations of any scale—whether corporations or governments or those that occupy the nebulous space between—are already quite good at forming such sinks. The accountability-washing that an AI provides isn’t a new service so much as an escalated and expanded one. Which doesn’t make it any less frightening, of course; but it does perhaps provide a useful clue. Any effort that’s tried and failed to hold a corporation to account isn’t likely to have more success against an algorithm. We need a new bag of tricks.",
    "commentLink": "https://news.ycombinator.com/item?id=41891694",
    "commentBody": "Accountability sinks (aworkinglibrary.com)441 points by l0b0 19 hours agohidepastfavorite254 comments rougka 13 hours agoI remember experiencing this in one of the German airports/airlines and having that exact thought. It was this fully automated airport, where the checkin is self serviced and you only interact with computers. Eventually, when I inserted my boarding pass I had a printed piece of paper back that said that they had to change my seat from aisle to midseat I then tried to find someone to talk to the entire way, but computers can only interact in the way the UI was designed, and no programmer accounted or cared for my scenario The ground attendant couldn't have done anything of course because it wasn't part of the scope of her job, and this was the part of germany where nice was not one of their stereotypes. Eventually I got a survey a week later about a different leg of the flight, so could I really complain there? that one was fine? I had a paranoid wonder if that was intentional reply nh2 10 hours agoparentGermany is somewhat rubbish. I arrived at the train station in the night after 6 hours train journey. German Railways app shows there will be my final leg train in 45 minutes. I wait in the cold at night, sitting in the station building because it's warmer there. 5 minutes before departure I go on the platform. The local display shows no train, even though the all still shows it. I waited for nothing. Syncing the app with the train station? Somebody else's problem. In half an hour there should be a replacement bus for another cancelled train. There are no signs in the app or the station that indicate where that bus is to be found. You just need to know. Putting sings for replacement buses due to degraded service that's long planned and already happening for 2 months? Somebody else's problem. An old man asks if the bus will allow to catch the train connection at its destination. The bus driver bitches at him for asking that question -- not his job. Somebody else's problem. Training the bus driver that, being an official replacement of a train, he needs to know that, clearly also somebody else's problem than that of the German Railways. It's pitch black outside, the windows are opaque due to moisture, so I can't tell where we are even though I was born the area and lived here for 18 years. The bus driver makes no announcements about the stops, there is no display. Knowing when to request a stop to get off? Somebody else's problem. The bus is ice cold for an hour. When am old lady gets off and tells the bus driver that it was freezing all journey, he asks \"well what can you do\". Bewildered she answers \"turn on the heating\"? He didn't expect that. He seemed to think that everything except driving was somebody else's problem. This is just one night's bus journey story. I also got my SIM card deleted and a parcel was lost in the subsequent week. Documenting here the amounts of \"somebody else's problem\" I encountered in their customer support hotlines is somebody else's problem for me for now. reply heisenbit 9 hours agorootparentThere is some degree of accountability for DB: Other organizations like Swiss and Austrian railways stopped taking schedules of DB seriously and stopped waiting or booking through. reply atoav 10 hours agorootparentprevAnd the root of all that? Privatization. reply RandomLensman 9 hours agorootparentThe German mail rail and track operator (Deutsche Bahn) isn't private but 100% state owned (and control sits with the federal government). They wanted to privatize it a couple of decades ago but abandoned it. There is still some hybridization between supposedly it being a business and also a public service left in the law, though. reply Dilettante_ 8 hours agorootparentThe Deutsche Bahn AG is in fact a private Aktiengesellschaft(which is to say a stock company) with the german gov't owning 100% of the shares. I'd very much argue that it is run mostly like a private enterprise and only occasionally compelled by the government to act like a public service. reply RandomLensman 7 hours agorootparentYes, but doesn't change that it is 100% state owned and controlled with a mixed mandate (business/public service - see art 87e GG). No reason to absolve the owner and the associated politics from their responsibilities. The legal form doesn't determine whether something is state owned or private. reply carlosjobim 5 hours agorootparentprevIt's still the government running the company if they own all the shares. So what's your point? reply medstrom 6 hours agorootparentprevThe actual root is even widely acknowledged: DB has been underfunded for a long time. reply rad_gruchalski 3 hours agorootparentIt’s an AG, why should it be funded? reply chgs 44 minutes agorootparentMost of the world agrees that public transport has external benefits and therefore is deserving of public funding. reply RandomLensman 1 hour agorootparentprevWhy would it not given the legal framework put in place? reply atoav 8 hours agorootparentprevYou sure the ~600 companies that the Deutsche Bahn is made up of can be compared to one state-owned entity? reply RandomLensman 7 hours agorootparentAll rolls up into one 100% owner and having a lot of subsidiaries isn't unusual. reply medstrom 6 hours agorootparentI believe they make a good point: making a vertically integrated entity could matter more than just buying most of the shares. If we are discussing tendencies of \"privatized vs public\", it's hard to ignore that factor. Public entities that historically worked well weren't just masses of 600 subcontractors. reply RandomLensman 6 hours agorootparentBut again, that is up to the owners - and the owner is the state. Also, the state didn't buy most of the shares: it had full ownership before and kept it while the legal form changed. reply presentation 9 hours agorootparentprevGiven how good the rail systems are in several Asian cities despite/thanks to being private, you might want to reconsider that opinion. reply dambi0 4 hours agorootparentI presume running a national rail system is somewhat different from a railway for a single city. How good are the national rail systems in these countries? reply presentation 1 hour agorootparentJapan’s is the greatest in the world, for one. reply JumpCrisscross 30 minutes agorootparentprev> the root of all that? Privatization Honesty, it's German politics doing precisely this that's part of the problem: flippant diagnoses too broadly applied from afar. reply maccard 9 hours agorootparentprevNo, it's not. It's bureacracy, and it exists in every big organisation, private or public. I'd actually suggest that public sector bodies are often worse for this. reply Moru 9 hours agorootparentThey are only worse because they are bigger. If the private replacement organization gets as big, they get the same problems. reply JumpCrisscross 28 minutes agorootparent> only worse because they are bigger Japan has a massive, private and lovely rail system. reply Panzer04 8 hours agorootparentprevReally? The more focused a company is (the more reliant it is on its core service) the more accountable it can be. I'd argue many companies are if anything more accountable than the government. It doesn't have to be true, but I'd argue it often is. reply eloisius 12 hours agoparentprevI had a similar experience in Germany about a year ago. Train stations are mostly self-service now. The ticket kiosk ate my €50 and promptly rebooted. It didn’t print a receipt or anything. The only human I could find was a security guard. He told me to call the number on a sticker on the machine. The person who answered couldn’t speak English. My €50 is out there somewhere but it would cost me more than that to track it down. reply larodi 11 hours agorootparentThat’s a sad experience and I would definitely try to chase them robots. Sadly even though German public transport fascinates with its ease of use and quality, though when it comes to human service you can find yourself in peculiar position. And particularly if you are not German and happen to be in one of those international cities there where Germans are fed up with visitor. You waive goodbye to your 50€ and keep a story to tell, that’s all. Sadly I don’t expect this all to get any better with robots and LLms and thing. We will be crying to meet a human sooner than later, and my hope is this far cry will eventually get us to the dawn of new era when you actually have people in the loop, just for humanity’s sake. reply olex 10 hours agorootparent>German public transport fascinates with its ease of use and quality Ease of use maybe, although my parents and grandparents would like to argue differently. They are not as quick to work their smartphone, and the ticket machines are being removed everywhere to be replaced by apps that are much cheaper to run. This works fine for the younger generations, but older and less tech-savvy people are getting left behind. Quality though, no way. Every single time I tried to give ÖPNV a chance in the last 3-4 years I was either different degrees of late or didn't arrive at all without switching to some alternative method of transport on the way. Doesn't even matter if I tried local routes (Frankfurt and Darmstadt) or longer inter-city connections to Munich or Leipzig, it's all completely broken. People in my company routinely book connections several hours earlier than they need to be places to have a chance of arriving in time, and often are still late. Trains are overbooked, connections are late or often cancelled altogether, seat reservarions don't work more often than they do, WiFi on the trains never works... Many, many things have to change for me to reconsider my default of taking the car everywhere, and I don't think they will in any sort of a relevant timeline. reply bojan 10 hours agorootparentprevIt's not that they don't want to have people, it's that there are no people. Germany, as most of other countries in Europe, has an aging population and the workforce is hard to find. So all these \"easy\" things that can get automated, do get automated, oftentimes indeed at the price of quality of service in exceptional situations. reply someoneiam 9 hours agorootparentWell, they say that, but in my experience at least, that is just conjured up as a more palatable explanation after the fact. While I do think that a certain, even significant, amount of automation is good, there is also a large mass of unemployed that can undoubtedly be trained to fill these \"human interaction\" kind of roles (support). This workforce is still not hard to find at all. We just don't want to do that - there is not a single western country left that has low unemployment as its key prerogative. reply Moru 8 hours agorootparentYes, it's only a profit thing. If you cut out the humans you can make more money. If not for your boss then for the company that gets the contract to make the automation. reply RandomLensman 10 hours agorootparentprevThe automations predate any demographic issues - mostly a (sticky) cost thing. reply bryanrasmussen 11 hours agorootparentprev>my hope is this far cry will eventually get us to the dawn of new era after the Butlerian Jihad. reply formerly_proven 10 hours agorootparentprev> german public transport fascinates with its ease of use and quality You have something mixed up there. reply immibis 10 hours agorootparentIt's definitely easy to use. You show up at the station when the train is coming. You get on the train. Later, you get off the train. No security checks like an airport. No multi step check in. Just be there and get on. In many cases your ticket won't even be checked, and when it is it's while you're seated while the train is moving. Getting a ticket is no problem: the ticket machines are multilingual, and you type in the stops you want to go from/to and the date. You can also book one online and get a QR-like code you can print or display on your phone. Quality is mediocre. The trains are often delayed, which is a problem with the size of the network and cascading failures. Once they do get to A, they get from A to B just fine, the seats are okay, the luggage space is okay, etc. The DB Navigator app is useful for finding alternative routes but it won't tell you whether your ticket is valid for them. It will tell you if the delay is so long that you're allowed to use any route. reply JumpCrisscross 26 minutes agorootparent> You show up at the station when the train is coming. You get on the train. Later, you get off the train. The train is late. The lounges suck or are tied to a complex system of ticket tiers that seemingly don't correlate to price. You bought a specific seat but the train was changed so now no assigned seat and lol on a refund. And fuck you if you're crossing borders. Germans travel a good amount by car for good reason [1]. When I'm in Germany, I tend to drive between cities because the alternative is burning several hours in buffers and delays. [1] https://ec.europa.eu/eurostat/statistics-explained/index.php... reply jhrmnn 10 hours agorootparentprev> The person who answered couldn’t speak English. It sounds like this was the main point of failure. I’m not sure it can be considered an error in the system. I’d consider the risk inherent in traveling in a country without knowing its language. reply scrollaway 9 hours agorootparentGermany is the only country in which I’ve had 112 (emergency services) hang up on me because they couldn’t speak English. It’s worse than France in this regard. reply f1shy 7 hours agorootparentYes sir. A friend of mine, the girlfriend passed out, being pregnant. In the moment of total stress, we called 112, and said “passed away“ instead of „passed out“. The guy on the other side “well, if she is dead, why are you calling?!” Very rude. He went on to explain, it was an error, an instead of just dispatch an ambulance, had to hear a 10 minute lesson in english (from a german) after which the ambulance was dispatched. When the ambulance finally arrived, she was “ok” so they had to pay couple of thousand Euros for a “negligent dispatch”… The level of arrogance and lack of empathy and service is beyond limits. reply bmicraft 14 minutes agorootparent> When the ambulance finally arrived, she was “ok” so they had to pay couple of thousand Euros for a “negligent dispatch” That part seems really hard to believe for me. The only time you should get charged at all is for prank calling. In fact, if you call and tell them and decide you don't need EMS after all they will in fact come anyways because they need to check on every call. And you will not get charged for that. reply aziaziazi 9 hours agorootparentprevYes, but it’s still fine to have a customer service only answering in the official language. The chance are high that a random German speaks english so you’ll probably be good but if that’s not the case, blaming the company seems unfair to me. reply bryanrasmussen 6 hours agorootparent>The chance are high that a random German speaks english so you’ll probably be good What does high mean in this context? I experienced what I would call the inverse Danish maneuver, the German obviously understand English because they often answered our English questions correctly - In German. In Denmark if a Dane understands what you said in Danish but you have a definite accent they will often answer your question in English. Maybe Germanic cultures are geared towards the rude. reply bmicraft 10 minutes agorootparentThey might have been rude, but that's besides the point. Even if they could speak English you shouldn't expect them to be comfortable doing so. That actually seems pretty rude in itself to me. reply HighGoldstein 5 hours agorootparentprevThis sounds like a language education issue. It's easier to understand a language than to express yourself in it, so possibly Germans on average have good enough knowledge of English to understand you but not enough to adequately reply in English. Conversely, Denmark has some of the highest English literacy in Europe. reply bryanrasmussen 2 hours agorootparentI mean sure, that sounds plausible, until I point out in Germany that I don't speak German and they continue trying to explain to me in German the answer to my question. If I'm talking to an Italian and trying to explain to them in English and they don't understand then I try with a combination of my broken Italian and hand signals, not obdurate sticking to English because that's being a jerk. At the same time, yes Danes have a high English literacy, but switching to English when someone is talking to you in Danish is rude no matter how you slice it. reply kleiba 8 hours agorootparentprev> The chance are high that a random German speaks English Not sure how random my selection process was, but that certainly wasn't my experience when I lived in Germany a few years ago. Maybe in big cities, yes. But even in the burbs, chances are you have to look for the metaphorical needle in the haystack to find someone speaking English. Your best bet might just be teenagers and young adults. reply f1shy 7 hours agorootparentAbsolutely. There is a big myth that “germans (all?) speak good English“ and nothing can be further from the truth. There are good ones, sure, maybe even more percent than other places, but go out of Berlin, Hamburg, Munich… and good luck!!! reply f1shy 7 hours agorootparentprevPlease look my comment to the parent comment. If you do find a german that speaks understandable english (that you can differentiate “think” and “sink” or “g” and “she” or “zoo” and “sue” then may be the arrogant crap that got my friend. For that they receive years of “Ausbildung”… reply chgs 42 minutes agorootparentprevI tried speaking German to a random security guard in Arizona and he just walked off. reply jhrmnn 9 hours agorootparentprevSure, that’s bad, and a service dealing with train ticket machine failures not available in English isn’t as bad. reply yxhuvud 7 hours agorootparentTicket machines are still something foreigners can be expected to interact and need help with. reply aziaziazi 4 hours agorootparentPeople visiting a foreign country can still expect to have to interact with local not speaking their language. reply f1shy 7 hours agorootparentprevGo to any museum… just brutal. reply graemep 4 hours agorootparentprevI had something similar happen to be on the tube in london. My ticket got demagnetised (combined intercity rail with travel card are/were still magnetic stripe tickets) and there were not staff at the station so I could not get the barrier open to leave. reply lmm 10 hours agoparentprevAt this point everyone needs to get in the habit of using small claims court. You can often do it online in a few minutes these days. Make a good faith effort to get your problem addressed, and record the fact that you've done so to use in your hearing if it gets that far. Then just file the claim. Generally they fold immediately, and this way you incentivize actual customer service in the only language they understand. reply bryanrasmussen 5 hours agorootparent>At this point everyone needs to get in the habit of using small claims court. You can often do it online in a few minutes these days. what country is this \"small claims court\" in? And are you sure this country's small claims works the way your country does? reply RandomLensman 10 hours agorootparentprevWhat claim would there have to be to file on the scenario outlined? reply lmm 10 hours agorootparentAh, I meant that post to be a reply to the ticket machine eating the 50 euro note. reply RandomLensman 10 hours agorootparentThank you for clarifying! reply switch007 10 hours agorootparentprevI do agree but also feel if people did this en masse, that system would get a rate limiter. After 2 claims per year you would be barred for being \"vexatious\" reply maccard 9 hours agorootparentBeing realistic, if you have these sorts of issues more than twice a year there's probably something wrong and you should fix that. Everyone has a few of those stories, but the only people who consistently have them are likely looking for trouble and picking fights. reply immibis 10 hours agorootparentprevOnly if the claims are illegitimate. reply m463 12 hours agoparentprevMany businesses build walls around themselves like this. Hiding the customer service number. Making an FAQ that is missing the common but time-consuming questions. Chatbots instead of people. I remember when amazon sent me a package once, said it was delivered, but it was nowhere to be found. There was no way to get help. They did have an FAQ at the time that said to check in the bushes. What was annoying was the search auto-complete had many variations of \"package not found says delivered\" Now, it is a little more filled out but still. reply ben_w 11 hours agorootparentI've got an actual email address to a real business, but the humans* are struggling with the concept of \"$company created the account with the wrong billing address, ignoring my agent who could have received it when my agent did contact $company, it's provably $company's fault that the bills were not received, so $company must tell me who this debt collector is and refund me for the late payment penalties and admit their own fault to the CRA\". * not that I could tell if they were LLMs reply gregmac 12 hours agorootparentprevI just switched ISPs, and the new one has one of the most obnoxious phone processes I've ever interacted with. I go through the usual hoops: press 1 for English, \"we detected an account linked to the number you're calling from, is that that you're calling about?\" ... Press 1 for support, press 1 for Internet, \"no outages detected in your area. Most problems can be solved by rebooting your modem. Press 1 if you want to try rebooting.\" (Pause)... \"thank you for your call click\" First off, rebooting doesn't solve my problem. But I guess I have to try anyway? So I call back, this time I do pick to reboot, and get \"your modem will reboot in the next few minutes, and could take up to 10 minutes to come online. If things still aren't working, try our online support chat\" So, basically there doesn't seem to be any phone technical support (with a human), at all. Also, rebooting is offensive to me as a programmer. Kernel updates and memory leaks are the only reason you need to reboot. How absolutely shitty is modem firmware that the ISP actually spent the time to build this reboot system out??(Never mind that I personally don't feel like I've ever had a modem/isp actually problem solved by rebooting) Made me wonder if I should have switched. reply yetihehe 11 hours agorootparent> (Never mind that I personally don't feel like I've ever had a modem/isp actually problem solved by rebooting) I had problems solved several times by rebooting modem. One time it was \"reboot modem and access point in proper order\", me naively rebooting them both at the same time didn't help, only phone support solved this problem. > Also, rebooting is offensive to me as a programmer. Hmm, I might be desensitivised from too much programming in erlang. It's implied that your program will encounter bugs or strange data and parts WILL be restarted, better account for that and plan on what to do on restart of each small part at the start of writing your program. > So, basically there doesn't seem to be any phone technical support (with a human), at all. Because it's cheaper. Those who don't have support can offer lower prices. When people search for trinkets, they only have information about what is supported, there is no good information about quality of device and support, high price also not always means better support. SO they just go for lower price and hope not to suffer too much. reply eru 10 hours agorootparentYes, see also https://news.ycombinator.com/item?id=40212967 reply auggierose 11 hours agorootparentprevWhy did you switch? Can't have been their reputation for support. reply maccard 9 hours agorootparentprev> Also, rebooting is offensive to me as a programmer. Kernel updates and memory leaks are the only reason you need to reboot. This surprises me - as a programmer you should realise that reboots can often help. Cache invalidation is one of the notoriously hard CS problems and an awful lot of systems will start fresh on reboot. > (Never mind that I personally don't feel like I've ever had a modem/isp actually problem solved by rebooting) My current ISP is better, but my previous ISP cycled IP addresses at 2am (and lost connectivity for about 30 seconds at the same time) on a Friday night. I would semi-frequently be up playing games at that hour, and it was about 50/50 as to whether devices on my network would survive the blip. Rebooting the router had a 100% success rate. I currently (unfortunately) have a google wifi mesh system. It works great, except about once a month it reports that absolutely everything is fine, all tests pass from my mobile device, but my laptop has no internet connectivity. Rebooting fixes it just fine. > How absolutely shitty is modem firmware that the ISP actually spent the time to build this reboot system out? Firmware is still software, like it or lump it. Modem firmware has been shitty for a long time. A major ISP [0] in the UK had an issue with their firmware that caused massive latency spikes under load. Alsom Power loss happens sometimes. The modem/router has to be able to turn on in the first place, so a \"reboot\" is just going through that process again. It's attempting to return to a \"last known good\". [0] https://community.virginmedia.com/t5/Forum-Archive/Hub-3-Com... reply eru 10 hours agorootparentprev> Also, rebooting is offensive to me as a programmer. Kernel updates and memory leaks are the only reason you need to reboot. How absolutely shitty is modem firmware that the ISP actually spent the time to build this reboot system out??(Never mind that I personally don't feel like I've ever had a modem/isp actually problem solved by rebooting) Why is rebooting offensive to you? State is hard; resetting your system to know state can fix many issues. See also https://news.ycombinator.com/item?id=40212967 reply sethammons 8 hours agorootparentIf your microwave had an error, you would be put off if you had to power cycle the whole house. Installed a new receptacle, sure, but operating an appliance? No way. Now you would have to reset your clocks everywhere at a minimum. I have a linux computer running a public server that has not be restarted in three and a half years. This is what I expect. Every time I have to reboot my work laptop due to work pushing some updates or that I have to reboot my windows machine because it is running unreasonably slow, I am reminded that inconsiderate assholes have become more lazy and are ok with polluting the whole system, mismanaging state and resiliency, and when the equivalent of the microwave has an error, the only solution is rebooting my house. We can do better. reply eru 4 hours agorootparent> I have a linux computer running a public server that has not be restarted in three and a half years. This is what I expect. I restart my Linux desktop every few weeks, when the kernel updates. For a reliable server, you want to exercise the restart ritual somewhat regularly, because when anything goes wrong (eg with the hardware), you might have to restart anyway, so you want to be sure that this works. reply bornfreddy 4 hours agorootparentprevWindows is awful at this. Completely weird problems with many apps (especially VPN) which get resolved with a reboot. Seriously? It is the whole culture around this OS which finds this acceptable. reply chgs 36 minutes agorootparentOften this will be resolved by the network being quiet long enough for a reboot - with connection tables in intermediate firewalls timing out etc - rather than the actual reboot. reply ginko 11 hours agorootparentprev>If things still aren't working, try our online support chat >So, basically there doesn't seem to be any phone technical support (with a human), at all. I wish everything had support chat. IMO it's much less hassle than having to call. It's usually trivial to get through the first layer of automated support and get a human on the line. reply maccard 9 hours agorootparentI agree, but; Support chat is universally shitty. My mobile provider's website only works if you keep the browser window open, and times out if you go away for 2 minutes. The replies often take more than 2 minutes. I can only access \"certain\" information about my account if I'm on mobile data, except my carrier's website doesn't work if I am out of data. (granted, I am on a super budget mobile network, but still). In the last 18 months, the chat experience has been taken over by LLM's which are just acting as full text search for the doc pages that don't solve my problem. I still choose web chat over any other method of interaction though. reply RobotToaster 8 hours agorootparentprevI've started just sending physical, paper, letters if I need to communicate with a company. It seems to have a better success rate. reply jowea 8 hours agoparentprevDoes Germany have a consumer protection agency? I might have complained there after the flight. reply clktmr 10 hours agoparentprevI can provide another POV to that story. We checked in as a family of four, and we're assigned seats in four different rows, with a two and a four year old. Only when entering the plane we had the possibility address this to a human and we were assigned new seats. So this might be the reason you had to change seats. reply rougka 2 hours agorootparentthey claimed they had to change planes though i had selected that seat when booking the flight, and there were no humans available to address such issues reply lazide 12 hours agoparentprevIt’s a way to fully automate a Brazil scenario. [https://en.m.wikipedia.org/wiki/Brazil_(1985_film)] Since at least in that scenario, there were humans in the Bureaucracy that could (but didn’t particularly) feel bad. In this scenario, no humans need to be directly involved, which allows the scope and scale to be even more Dystopian. reply noisy_boy 11 hours agorootparentOf all the various useless laws that keep getting enacted, one that guarantees that every company needs to have a phone number, manned by actual human(s) with organic intelligence, advertised prominently on their products/advertisements, never gets passed. reply immibis 2 hours agorootparentGermany has this law for commercial websites. Mostly it makes people afraid to have websites. reply lazide 11 hours agorootparentprevBecause that would cost them money, instead of forcing people to find their own workarounds (or die). Many parts of gov’t aren’t far off, and those are the really scary ones. reply larsrc 10 hours agoprevI've long thought that that is one of the main functions of corporations. There's a reason they're called limited liability. The fact that you can conjure up new companies at a whim makes it easy to shuffle responsibility into an obscure corner. This is a strong reason that corporations should not be considered people. People are long-lived entities with accountability and you can't just create or destroy them at will. reply 7952 6 hours agoparentAt a more basic level money eliminates the need for social obligation. There is no expectation of reciprocity or mutual respect. You pay for a product, it is delivered and that is the end of it. Corporations do this within their own internal economy or with partner companies. A cost centre pays an amount of money and delegates responsibility. reply swayvil 5 hours agorootparentEnjoying the benefits of living in a society (a degree of trust, no deadly combat, services like police) without suffering its liabilities (mandatory politeness and respect). It's the profitable course. reply 627467 10 hours agoparentprevI agree with the feeling, but State orgs are effectively eternal (think the various level of government) and still great at diffusing accountability to various scapegoats reply lesuorac 9 hours agorootparentState orgs (and federal ones) often have length processes before they can do stuff though. As well as after they do something there is typically a recourse path provided by that org for you to protest their decisions and if that doesn't resolve favorable you can also sue them. Which differs from the article because the corporation doesn't provide any protest path nor did it have to publish any memo/etc describing how they're going to downsize cleaning for cost-savings. But you can still sue them (but good luck showing damages over an unclean room)! reply InsideOutSanta 9 hours agorootparent\"the corporation doesn't provide any protest path\" This. The problem with \"voting with your wallet\" is that you can't vote \"no\", you can only vote \"yes\" or abstain from voting altogether. reply f1shy 7 hours agorootparentThat can be said of many many democracies around the world. reply izacus 8 hours agoparentprevJust to be clear, LLC is supposed to be about limited financial liability, not criminal liability. But we seem to have forgotten that on the way. reply atoav 10 hours agoparentprevThe buck has to stop somewhere and a human has to be responsible for things. reply rwmj 10 hours agorootparentOh sweet summer child. Companies are frequently structured and created in multiple jurisdictions to obscure beneficial ownership, responsibility, profits and taxes. reply atoav 9 hours agorootparentWhat I said is how it should be, not how it is. reply RandomLensman 9 hours agoparentprevLimited liability corporations are a relatively new concept and there is certainly scope to change how/when/where they could be created and run, for example. reply ChrisMarshallNY 7 hours agorootparentI recently had to submit a copy of my drivers license to the feds, for my LLC. I have heard that they are working on the accountability shield for LLCs. reply tucnak 6 hours agoparentprev> People are long-lived entities with accountability and you can't just create or destroy them at will. This notion is currently being contested reply InsideOutSanta 9 hours agoparentprevYeah, this dysfunction is not a bug, it's the feature. In some ways, it's useful, because it allows positive risk-taking that could not be taken if anyone was actually held (or even just felt) accountable. But at this point, as a society, we've shifted too far towards enabling accountability-free behavior from corporations. I think a good example of the dichotomy here is Starlink. On one hand, it's an incredibly useful service that often has a positive impact. On the other hand, a private corporation is just polluting our low earth orbit with thousands of satellites. It's not clear to me where exactly the right balance for something like this should be, but I do think that as of today, we're too far on the lessez-faire side. reply dale_glass 9 hours agorootparent> I think a good example of the dichotomy here is Starlink. On one hand, it's an incredibly useful service that often has a positive impact. On the other hand, a private corporation is just polluting our low earth orbit with thousands of satellites. Seems like a terrible example to me. I'm no fan of Musk, but I don't see how that is \"polluting\". They provide an excellent service. They're a minor hindrance for astronomy, true, but I think it would be hard to make a good case for that a few people having a good view of the sky is more important than millions having good communications. Then there's that there's nothing really special about Starlink. It's merely one of the first users of cheap rocket launches. It could be somebody else, or 1000 different entities launching smaller numbers, in the end the effect on astronomy would be the same. reply InsideOutSanta 9 hours agorootparent\"Then there's that there's nothing really special about Starlink\" I didn't say there was, and this isn't about Musk. I'm just using Starlink as an example, my point is not about Starlink. \"I don't see how that is polluting\" Starlink satellites create light pollution and disrupt radio frequencies. Astronomers are already running into issues with research due to the light from Starlink satellites. There's also the issue of reentry. We now have a Starlink reentry almost every single day, which is at least damaging to the ozone layer, and very likely causing other issues. But like I said, this is not about Starlink. It's just an example to illustrate accountability sinks having both positive and negative effects. reply dale_glass 9 hours agorootparentI don't think it works at all, no. There's no accountability sink to speak of here. \"Accountability sink\" in the article's meaning means that accountability got obscured, something bad happened (eg, lies on TV, terrible customer service), yet nobody can be clearly blamed for giving the order. Here, it's Musk's invention, and he's clearly to blame for it. In fact Musk has a propensity to take more credit than he deserves, so it's almost the opposite from a sink really. reply InsideOutSanta 9 hours agorootparentMusk is not accountable for Starlink. Starlink is an LLC, a limited liability company. There is no single person who is accountable for Starlink's satellites. reply TJSomething 8 hours agorootparentThe chief difference here is that you can plausibly point at every investor in Starlink and say if they have the slightest idea of Starlink's business plan, they know that causes light pollution. There is exactly one degree of separation from putting satellites in the sky to causing light pollution. There's no plausible deniability there. This article is more about the phenomenon where decisions are removed by multiple degrees. The locus of decision making is either obscured or non-existent, creating plausible deniability. This is often done by rewarding activities that don't obviously create harm but nevertheless require causing harm to carry out. reply InsideOutSanta 7 hours agorootparentBut that is how it works, isn't it? They're saying, \"we want to make the Internet available to as many people as possible.\" They don't want the light pollution, and they don't create the light pollution. It's analogous to the Fox example in the article, where somebody at the top says, \"we want high viewership.\" They don't want their employees to lie to their audience, and they don't force them to lie to their audience. Does the Fox leadership at some point become aware that \"lying to the audience\" is a result of their performance goals, just like the decision makers at Starlink become aware that light pollution is a result of their goals? They very likely do. Does that make them feel accountable for the negative side effects? Probably not, because they didn't tell anyone to lie and pollute the skies, somebody else did that. reply marci 8 hours agorootparentprevAstronomy isn't the only issue with space pollution (e.g: https://en.wikipedia.org/wiki/Kessler_syndrome) reply ernst_klim 9 hours agorootparentprevSorry but I find your example totally wrong. Things like radio frequencies and space launches are hard regulated by govs, no corporation can launch satellites at will without permission from the government(s). reply InsideOutSanta 5 hours agorootparentDoesn't that apply to all companies? They have to follow the laws. Accountability sinks exist orthogonal to that. reply alilleybrinker 17 hours agoprevCathy O'Neil's \"Weapons of Math Destruction\" (2016, Penguin Random House) is a good companion to this concept, covering the \"accountability sink\" from the other side of those constructing or overseeing systems. Cathy argues that the use of algorithm in some contexts permits a new scale of harmful and unaccountable systems that ought to be reigned in. https://www.penguinrandomhouse.com/books/241363/weapons-of-m... reply bigiain 15 hours agoparentBrings to mind old wisdom: \"A computer can never be held accountable, therefore a computer must never make a Management Decision.\" IBM presentation, 1979 reply k1t 12 hours agorootparent\"A computer can never be held accountable, therefore all Management Decisions shall be made by a computer.\" - Management, 2 seconds later. reply lifeisstillgood 10 hours agorootparentTherefore all management decisions are made by the people writing the code Hence coders are the new managers, managers just funnel the money around, a job which can be automated reply intelVISA 4 hours agorootparentSoon(tm) reply heresie-dabord 7 hours agorootparentprev> presentation, 1979 = Presentation, 21st Century A computer is not alive. A computer system is a tool that can do harm. It can be disconnected or unplugged like any tool in a machine shop that begins to do harm or damage. But a tool is not responsible. Only people are responsible. Accountability is anchored in reality by personal cost. = Notes Management calculates the cost of not unplugging the computer that is doing harm. Management often calculates that it is possible to pay the monetary cost for the harm done. People in management will abdicate personal responsibility. People try to avoid paying personal cost. We often hold people accountable by forcing them to give back (e.g. community service, monetary fines, return of property), by sacrificing their reputation in one or more domains, by putting them in jail (they pay with their time), or in some societies, by putting them to death (\"pay\" with their lives). Accountability is anchored in reality by personal cost. reply lifeisstillgood 11 hours agorootparentprevAdmittedly the context matters “we are trying to sell to Management, therefore let’s butter them up and tell them they make great decisions and they won’t get automated away” while the next page of the presentation says “we will Automate away 50% of the people Working for you saving globs of money for your next bonus” IBM in 1979 was not doing anything different to 2024. They were just more relevant reply RobotToaster 4 hours agorootparentprevSee also: “To err is human, but to really foul things up requires a computer.” —Paul Ehrlich reply spencerchubb 15 hours agoparentprevIt's much easier to hold an algorithm accountable than an organization of humans. You can reprogram an algorithm. But good look influencing an organization to change reply conradolandia 14 hours agorootparentThat is not accountability. Can the algorithm be sent to jail if it commit crimes? reply Timwi 14 hours agorootparentYes. Not literally of course, but it can be deleted/decommissioned, which is even more effective than temporary imprisonment (it's equivalent to death penalty but without the moral component obviously). reply hammock 14 hours agorootparentWhy should it be obvious that the moral component is absent? Removing an algorithm is like reducing the set of choices available to society… roughly equivalent to a law or regulation, or worse, a destructive act of coercion. There are moral implications of laws even though laws are not human reply lucianbr 14 hours agorootparentprevIs the point revenge or fixing the problem? Fixing the algorithm to never do that again is easy. Or is the point to instill fear? reply melagonster 12 hours agorootparentIf algorithm can do some thing wrong, but nobody should be responsible for it, everyone will just hide their crimes under algorithm and replace it when someone find problem. reply lucianbr 11 hours agorootparentIf a mechanical device does something wrong, are we in the same conundrum? I don't see what the problem is. There's malice, there's negligence, and there's accident. We can figure out which it was, and act accordingly. Must we collapse these to a single situation with a single solution? reply TeMPOraL 13 hours agorootparentprevThe point is that \"accountability of an algorithm\" is a category error. reply lucianbr 11 hours agorootparentThat's reasonable. Let's just call it root cause analysis in this case. The original point seemed to me to be \"we can't use computers because they're not accountable\". I say, we can, because we can do fault analysis and fix what is wrong. I won't say \"we can hold them accountable\", to avoid the category error. reply sethammons 8 hours agorootparentI think folks may have different interpretations of accountability. If your algorithm kills someone, is the accountability an improvement to the algorithm? A fine and no change to the algorithm? Imprisonment for related humans? Dissolution of some legal entity? reply lazide 12 hours agorootparentprevThe point of accountability is to deter harmful activity by ensuring actions/decisions somewhere result in consequences for who are responsible for them. Those consequences can be good or bad, though it is often used to refer to bad. An algorithm has no concept of consequences (unless programmed to be aware of such), and the more plausibly whoever wrote it can deny knowledge of the resulting consequences, also the more whoever wrote it can avoid consequences/accountability themselves. After all, we can tell Soldiers or Clerks that ‘just following orders’ is no excuse. But computers don’t do anything but follow orders. Most people/organizations/etc have strong incentives to be able to avoid negative consequences, regardless of their actions or the results of their actions. Everyone around them has strong incentives to ensure negative consequences for actions with foreseeable negative outcomes are applied to them. Sometimes, organizations and people will find a way for the consequences of their actions to be borne by other people that have no actual control or ability to change actions being performed (scapegoat). Accountability ideally should not refer to that situation, but sometimes is abused to mean that. That tends to result in particularly nasty outcomes. reply lucianbr 11 hours agorootparent> The point of accountability is to deter harmful activity by ensuring actions/decisions somewhere result in consequences What I read is yes, the point is revenge. If I can offer you a different way of preventing harmful activity, apparently you're not interested. There has to be some unpleasant consequences inflicted, you insist on it. I think you should reconsider. reply KronisLV 10 hours agorootparentI think they’re just right in this case. Suppose I’m a bad actor that creates an unfair algorithm that overcharges the clients of my company. Eventually it’s discovered. The algorithm could be fixed, the servers decommissioned, whatever, but I’ve already won. If the people who requested the algorithm be made in that way, if the people who implemented it or ran it see no consequences, there’s absolutely nothing preventing me from doing the same thing another time, elsewhere. Punishment for fraud seems sane, regardless of whether it’s enabled by code or me cooking some books by hand. reply lazide 9 hours agorootparentOne could even argue (from a raw individual utility perspective - aka selfish) that if the person/people who did that suffered no negative consequences, they’d be fools to not do it again elsewhere. The evolutionary function certainly encourages it, correct? Ignoring that means that not applying consequences makes one actually culpable in the bad behavior occurring. Especially if nothing changed re: rules or enforcement, etc. reply lazide 11 hours agorootparentprevEvery rule/boundary/structure needs both a carrot, and a stick, to continue to exist long term. Ideally, the stick never gets used. We aren’t dealing with ideals, however, we have to deal with reality. On any sufficiently large scale, an inability/lack of will to use the stick, results in wide scale malfeasance. Because other constrains elsewhere result in wide scale push to break those rules/boundaries/structures for competitive reasons. No carrot, magnifies the need to use the stick, eh? And turns it into nothing but beatings. Which is not sustainable either. It has nothing to do with revenge. But if it makes you feel more comfortable, go ahead and call it that. It’s ensuring cause and effect get coupled usefully. And is necessary for proper conditioning, and learning. One cannot learn properly if there is no ‘failure’ consequence correct? All you need to do to verify this is, literally, look around at the structures you see everywhere, and what happens when they are or are not enforced. (Aka accountability vs a lack of it). reply closeparen 13 hours agorootparentprevInteresting that you mention jail… the rule of law is kind of the ultimate accountability sink. reply rini17 12 hours agorootparentprevYou now have to not only find someone responsible for the algorithm but also competent and with permission to do it. Isn't it clear that this is very hard? reply dragonwriter 14 hours agoparentprev\"Cathy argues that the use of algorithm in some contexts permits a new scale of harmful and unaccountable systems that ought to be reigned in.\" Algorithms are used by people. An algorithm only allows \"harmful and unaccountable systems\" if people, as the agents imposing accountability, choose to not hold the people acting by way of the algorithm accountable on the basis of the use of the algorithm, but...that really has nothing to do with the algorithm. If you swapped in a specially-designated ritual sceptre for the algorithm in that sentence (or, perhaps more familiarly, allowed \"status as a police officer\" to confer both formal immunity from most civil liability and practical immunity from criminal prosecution for most harms done in that role), it functions exactly the same way: what enables harmful and unaccountable systems is when humans choose not to hold other humans accountable for harms, on whatever basis. reply alilleybrinker 14 hours agorootparentYeah, I think you're conflating the arguments of \"Weapons of Math Destruction\" and \"The Unaccountability Machine\" here. \"The Unaccountability Machine,\" based on Mandy's summary in the OP, argues that organizations can become \"accountability sinks\" which make it impossible for anyone to be held accountable for problems those organizations cause. Put another way (from the perspective of their customers), they eliminate any recourse for problems arising from the organization which ought to in theory be able to address, but can't because of the form and function of the organization. \"Weapons of Math Destruction\" argues that the scale of algorithmic systems often means that when harms arise, those harms happen to a lot of people. Cathy argues this scale itself necessitates treating these algorithmic systems differently because of their disproportionate possibility for harm. Together, you can get big harmful algorithmic systems, able to operate at scale which would be impossible without technology, which exist in organizations that act as accountability sinks. So you get mass harm with no recourse to address it. This is what I meant by the two pieces being complementary to each other. reply spit2wind 12 hours agoprevThis is a terrible example because it's simply not true: > Davies gives the example of the case of Dominion Systems vs Fox News, in which Fox News repeatedly spread false stories about the election. No one at Fox seems to have explicitly made a decision to lie about voting machines; rather, there was an implicit understanding that they had to do whatever it took to keep their audience numbers up. Rupert Murdoch conceded under oath that \"Fox endorsed at times this false notion of a stolen election.\"[1] He knew the claims were false and decided not to direct the network to speak about it otherwise. Communications from within Fox, by hosts, show they knew what they were saying was false.[2] These two examples clearly fit the definition of lying [3]. The \"External Links\" section of Wikipedia gives references to the actual court documents that go into detail of who said what and knew what when [4]. There are many more instances which demonstrate that, indeed, people made explicit decisions to lie. [1] https://www.npr.org/2023/02/28/1159819849/fox-news-dominion-... [2] https://www.nbcnews.com/politics/elections/dominion-releases... [3] https://www.dictionary.com/browse/lie [4] https://en.m.wikipedia.org/wiki/Dominion_Voting_Systems_v._F... reply BartjeD 12 hours agoparentI think the point of the citation is that there wasn't an original decision to lie about it. It happened without coordination and later on wasn't stopped by the people in management, either. It was number-2 all the way up. reply bjornsing 11 hours agorootparentSo there were many decisions to lie about it, and the lying was condoned from the top. reply throwaway48476 11 hours agoparentprevVoting machines are hacked every year at DEFCONs voting village. They're wildly insecure and no one should trust them. Frankly, any claims of manipulation of voting machines are at worst plausible. reply jazzypants 10 hours agorootparentAnd, yet these companies won their lawsuits, and no wide scale voter fraud was ever found. So, your point is entirely irrelevant. reply throwaway48476 10 hours agorootparentYou're confusing terms here. Voter fraud is fraud by voter. This is not what we're talking about with the voting machine susceptibility. reply albedoa 4 hours agorootparentNo, you are confusing terms here. We are talking about Dominion Voting Systems v. Fox News Network, which alleged that Fox had broadcast false statements that Dominion's voting machines had been rigged specifically to steal the 2020 election. That is a wildly different than reporting on what is demonstrated at DEFCON's voting village. What are you trying to pull. reply throwaway48476 3 hours agorootparentHere is a definition of voter fraud. https://www.browardvotes.gov/voter-information/voter-fraud reply carlosjobim 5 hours agorootparentprevWinning or loosing a lawsuit doesn't say much about the truth of any matter. Especially when it's a civil suit between corporations, or a criminal suit in a jurisdiction that has plea deals. reply notTooFarGone 10 hours agorootparentprevAccounts are hacked every second. They are wildly insecure and noone should trust them. Frankly saying you are just a hacked account is at worst plausible. You logic is flawed at the core. With that train of thought you can infer everything. Why trust voting it can be manipulated. reply throwaway48476 10 hours agorootparentIs the objective truth of what I say contingent on whether it is a hacked account? I'd say no. All voting systems can be manipulated, there's no need to make it so easy though. reply 082349872349872 10 hours agorootparentIn the days before voting machines, the dead people simply voted in alphabetical order, eg (1962) https://news.ycombinator.com/item?id=41706655 [my recommendation: try to build a high trust society instead of patching epicycles onto the side of a low trust one] reply xg15 11 hours agoprevMy suspicion I'd that one of the major appeals of automation and especially \"app-ification\" for management and C-Suite types is specifically its ability to break accountability links. A lot of corporations now seem to have a structure where the org chart contains the following pattern: - a \"management layer\" (or several of them) which consists of product managers, software developers, ops people, etc. The main task of this group is to maintain and implement new features for the \"software layer\", i.e. the company's in-house IT infrastructure. Working here feels very much like working in a tech company. - a \"software layer\": This part is fully automated and consists of a massive software and hardware infrastructure that runs the day-to-day business of the company. The software layer has \"interfaces\" in the shape of specialized apps or devices that monitor and control the people in the \"worker's layer\". - a \"worker's layer\": This group is fully human again. It consists of low-paid, frequently changing staff who perform most of the actual physical work that the business requires (and that can't be automated away yet) - think Uber drivers, delivery drivers, Amazon warehouse workers, etc. They have no contact at all with the management layer and little contact, if any, with human higher-ups. They get almost all their instructions through the apps and other interfaces of the software layer. Companies frequently dispute that those people technically belong to the company at all. Whether or not those people are classified as employees, the important point (from the management's POV) is that the software layer serves as a sort of \"accountability firewall\" between the other two layers. Management only gives the high-level goal of how the software should perform, but the actual day-to-day interaction with the workers is exclusively done by the software itself. The result is that any complaints from the worker's layer cannot go up past the software - and any exploitative behavior towards the workers can be chalked up as an unfortunate software error. reply smugglerFlynn 9 hours agoparentIf you think back to less automated times, management was the programming —- you built instructions and procedures that allowed organisation to scale and improve your end product. The only thing that changed is that now instructions and procedures are oftentimes executed by software and hardware, not by actual human beings. Hence the use of software engineering wing, in addition to your usual, sorry for the lack of better word, “meat programmers” aka organisational execs. Interestingly, the end result customers get has not changed, despite many people coloring it that way. People still get same cup of coffee or a taxi ride, just quicker/cheaper/marginally better. But such incremental improvements were achievable in the business world before IT era using same exact means, through internal product management and imrovement of org procedures, applied to people and processes instead of pieces of software. reply xg15 8 hours agorootparentYes, in principle nothing has changed since at least Fordian times - back then we had factory workers on one side and owners, managers and engineers on the other side, with the intermediate role perhaps being the foreman or something similar. I still think there is some difference in kind, not just degree: A human operational exec at least has to engage with the workers personally, witness the conditions they are working in, is exposed to complaints, etc. Even the most uncaring foreman is therefore forced into a position where he is subjected to accountability. He also has personal contact with the upper layer and can pass on that accountability to his higher-ups. In contrast, a software layer is physically unable to hear complaints and to pass them back up the chain. Because it's not a human, it cannot take accountability itself - however, it can still give higher-ups plausible deniability about \"not having known\" about problems. (A knock-on effect is also that it will prevent workers from even attempting to communicate the problem, because no one wants to talk to a wall) Therefore it creates an accountability sink where there was none in the old structure. (None in theory at least, of course there were enough other ways to be shielded from accountability even before computers) reply smugglerFlynn 8 hours agorootparentMy experience: only the good operational execs engage personally, and only for specific reasons of gathering feedback and improving the overall system. You’d be surprised how often decisions are made without ever seeing people at work, or communicating with them in a meaningful way. There are managers who do engage first hand, but they are not real decision makers, and just relay the report on situation and context upstream / execute on decisions of others. Relay of accurate first hand information from workers to execs almost never happens. As one of the neighbor threads accurately highlighted: this is by design, both on customer side and on worker’s side. Customers get vouchers, workers get retainers, among both there is a calculated percentage of people facing what they see as “accountability sink”, what is in reality a machine intentionally designed that way. reply sameoldtune 4 hours agorootparentprevNothing has changed for rich people who didn’t see their employees as people anyway. When you are the one stuck with a computer as your boss then tell me nothing has changed. Good luck getting a reference for a better job! reply pzmarzly 8 hours agoparentprevIn my opinion it's even more complicated, as the \"management layer\" is also using these tactics against itself. \"You must use an iPhone\", \"You cannot expense this trip with company card\", \"Your permission request to do XYZ in our cloud was automatically declined\", \"This tool only works in Google Chrome\". Why? \"The rules say so\" / \"The system says so\". Who set \"the rules\"? Who set up \"the system\"? Nobody seems to know, and digging into it yourself is a herculean effort and usually a waste of time. reply tomaskafka 10 hours agoparentprevThat’s what @vgr observed some time ago - people split into “above AI” and “below AI”, and the AI slowly moves up in the stack. reply sega_sai 6 hours agoprevThat's a really thought provoking article. And my thinking is this highlights the importance of government consumer protection agency/laws as the protection against that. I.e. when you fly through Europe or use European airlines, there is this EU law that gives you compensation of ~ 600 EUR if your flight is delayed by more than 3 hours or cancelled or whatever. This is a good insurance that no matter what BS is thrown at you at the airport by the company, you will get your compensation. And the process of getting the money is reasonably straightforward. What that gives is a way of avoiding any kind of airline systems, and just leads to the compensation. Also I hope that serves as an actual motivation for the airline to perform reasonably well, because otherwise they'll pay too much in fines. I think we really need this kind of protection laws in order to avoid the situation of chatbot-wall shielding companies from customers. reply rahimnathwani 4 hours agoparentAnd the process of getting the money is reasonably straightforward. Not always. The airlines often lie. reply sega_sai 3 hours agorootparentAt least in my experience, I asked for compensation 3 times and got the money in every case (with different companies: American Airlines, United and Lufthansa). But I agree the system could be improved further. reply miki123211 9 hours agoprevI experience this pretty often with the newfangled, automated government e-filing systems. As a screen-reader-using person who cannot use pen and paper without assistance, I was once quite enamored by them, but I've changed my stance a bit. The thing about pen and paper is that it accepts anything you put in, and it's up to a human to validate whether what you put in makes any sense. Computers aren't like that, if they tell you that the numbers in your application have to match up, you need to lie to the government to make them match up, even if you're a weird edge case where the numbers should, in fact, be slightly off and \"inconsistent\" with each other. I called the local govt office responsible for this specific program, and they essentially told me to lie to the government in not so many words. Their system is centrally managed, they have no power of introducing updates to it, they wish they could fix it, but even they aren't empowered to do so. reply jsemrau 12 hours agoprev>The comparisons to AI are obvious, in as much as delegating decisions to an algorithm is a convenient way to construct a sink. There is a flag on my LinkedIn account that bars me from getting a \"follow-me\" link on my profile. No one of their support team knows why. No one knows since when. No one knows when it will change. We are already living in this world. reply MathMonkeyMan 2 hours agoparentsounds like a bug reply aeturnum 16 hours agoprevWhen I was a grad student in STS I was considering doing a project on how software can function as an \"agency adjuster\" where individuals come to bear the risks of something (generally an economic transaction) and the majority of the profits go to the owner of the software. In many ways Uber & related services are about allowing individuals to take on very low-probability high-acuity downside risk for a small fee. reply walleeee 16 hours agoparentI think this sort of analysis is valid and fruitful in a very general sense. Software as a recently adopted vehicle in a long tradition of liability displacement / diffusion of responsibility / agency modification reply aeturnum 2 hours agorootparentYah - it lies outside of the narrowly technical (though technical systems come up a lot) and part of what I would talk about would have been: how much is this a trick and how much of this is real? Like, is software doing slight of hand and really Uber (or whoever) should be taxed on an externality / risk? Or does this electronic machine of software genuinely create a new arrangement of responsibility? My unhelpful understanding is \"it depends\" and even in the Uber case it's a bit mixed, though on balance I think Uber is more of a scam than a truly new thing (even though there's some new there). reply gradschoolfail 16 hours agoparentprevHmm, the analysis with respect to FOSS could also interesting. might make less sense to consider profit/compensation. Might be more useful to think of responsibility flows.. (or sources as well as sinks) reply aeturnum 2 hours agorootparentYah - I think there's value there too. I am a huge fan of FOSS ofc - and also it's good to look at how FOSS allows companies to avoid hiring developers because they can use FOSS products. The first benefit I think of is trying to come up with FOSS approaches that would convince or coerce companies to contribute back to the projects they use at least a little. reply solatic 13 hours agoprevToo focused on the bottom level. If a given business process results in employee A doing their job correctly according to the process, passing work to employee B doing their job correctly according to the process, passing work to employee C doing their job correctly according to the process, and the end result is shit, then the person who is accountable for the end result being shit is the manager who is responsible for the process itself. As more and more employees are involved, and the processes get more and more hierarchical (rather than \"employee A\", you have \"middle-manager M\"), then the person with accountability is higher and higher up the hierarchy, who also has more and more power and responsibility to fix it. The idea of \"unaccountable\" failures only makes sense if both (a) the problem is so systemic that actually an executive is accountable, (b) the executive is so far removed in the hierarchy from the line employees doing the work that nobody knows each other or sometimes even sits on the same campus, (c) the levers available to the executive to fix the problem are insufficient for fixing the problem, e.g. the underlying root cause is a culture problem, but culture is determined by who you hire, fire, and promote, while hiring and firing are handled by \"outside\" HR who are unaccountable to the executive who is supposedly accountable. But really this is another way of saying that accountability is simply another level higher, i.e. it is the CEO who is accountable since both the executive and HR are accountable to the CEO. No, you have to have an astoundingly large organization (like government) to really have unaccountability sinks, where Congress pass laws with explicit intent for some desired outcome, but after passing through 14 committees and working groups the real-language policy has been distorted to produce the exact opposite effect, like a great big game of telephone, one defined by everyone trying to de-risk, because the only genuine shared culture across large organizations is de-risking, and it is simply not possible to actually put in place both policy and real-life changes to hiring, firing, and promotion practices in the public sector to start to take more risks, because at the end of the day, even the politicians in Congress are trying to de-risk, and civil servants burning taxpayer money on riskier schemes is not politically popular, though maybe it should be, considering the costs of de-risked culture. reply jldugger 12 hours agoparent> Too focused on the bottom level. If a given business process results in employee A doing their job correctly according to the process... then the person who is accountable for the end result being shit is the manager who is responsible for the process itself. The book's point is that while this _should_ be the case, all too often it's not. AFAIK, nobody has been charged with forging documents in the case of Wells Fargo cross selling. Not the counter clerks who directly responded to incentives and management pressure nor the executives who built that system. reply solatic 12 hours agorootparent> nor the executives who built that system This is exactly why being an executive of a large organization is so incredibly difficult to pull off well. Sure, you can let your assistant fill your calendar with a bunch of meetings you don't want to be in to spend 95% of the meeting listening, 4% being the arbiter who tells people what they already knew they needed to do but refused to do it until asked by someone in authority, and 1% saying you'll take it further up the ladder. You will also fail hard because you will be constantly blindsided by people either fucking up (at best) or gaming (at worst) the processes for which you are responsible. Small example litmus test: in organizations that use Jira, whether the executives are comfortable with JQL and building their own dashboards to tell them what they need to know, or whether they expect their direct reports to present their work. If it's the latter, how can an executive be surprised that their reports are always coming in with sunny faces and graphs going up and to the right? That too many companies are not willing to hold executives accountable for processes that they are, in theory, supposed to be accountable for is an entirely different problem. The law proscribes, the officer arrests, and the judge presides, but all rests upon the jury to convict. If a company's \"jury\" is not willing to \"convict\", because the crime is one of negligence and not treason, then the company has larger problems and I'd like to short their stock, please. reply lazide 12 hours agorootparentprevAlso, usually these situations involve intentionally (or not, depending on how charitable one is being) passing back and forth between different divisions/groups. So the only one with consistent power over all groups is an executive so high up the food chain (in some cases not even the CEO!) that they can plausibly claim ignorance. reply _kidlike 12 hours agoparentprevI think \"accountability\" here was the wrong word to begin with. I believe they are more talking about \"ability for feedback\" or even better \"just in time corrections\". Feedback exists, but from my experience nobody reads those form submissions - maybe an AI these days that will create a summary... The latter is purposefully removed from all processes :( reply throwaway2562 11 hours agoprevDouglas Adams was here in 1982 with the invention of the SEP field ‘An SEP is something we can't see, or don't see, or our brain doesn't let us see, because we think that it's somebody else's problem. That’s what SEP means. Somebody Else’s Problem. The brain just edits it out, it's like a blind spot. The narration then explains: The Somebody Else's Problem field... relies on people's natural predisposition not to see anything they don't want to, weren't expecting, or can't explain. If Effrafax had painted the mountain pink and erected a cheap and simple Somebody Else’s Problem field on it, then people would have walked past the mountain, round it, even over it, and simply never have noticed that the thing was there.’ https://en.wikipedia.org/wiki/Somebody_else's_problem reply TZubiri 17 hours agoprevI was thinking about something similar today. Sometimes accountability can be a blocker, for example for hiring. If you have 1 candidate, it's an easy call, if you have 3 candidates, you evaluate in less than a week. If you have 200 candidates, you need to hire somebody to sift through the resumes, have like 5 rounds on interview and everybody chiming in, whoever pulls the trigger or recommends someone is now on the hook for their performance. You can't evaluate all the information and make an informed decision, the optimal strategy is to flip a 100 sided die, but no one is going to be on the hook for that. reply cj 17 hours agoparent> If you have 200 candidates, you need to hire somebody to sift through the resumes, have like 5 rounds on interview and everybody chiming in, whoever pulls the trigger or recommends someone is now on the hook for their performance. That's not how accountability works, in the traditional sense. What you described is Person A (accountable for hiring) hiring person B (responsible for screening and evaluating candidates). Person A is still accountable for the results of Person B. If Person B hired a sh*t candidate, it still lands on Person A for not setting up an adequate hiring system. Being accountable for something doesn't forbid you from delegating to other people. It is very common for 1 person to be accountable for multiple people's work. reply lstodd 16 hours agorootparentheh never works that way. an experienced bureucrat like you describe always has a shit-deflecting canopy. so whatever decisions he personally took are never attributable to him personally. it just so happened. reply cj 16 hours agorootparentwhat you’re describing is not someone who is accountable for something. In the hiring example, perhaps the person A stops being accountable for hiring someone successful in the role, and rather they are accountable for successfully hiring persons B who is capable of hiring someone to fill the role. Essentially creating an accountability chain. If you want to describe a logical chain of accountability instead as a “accountability sink”, then I’d go along with that. It’s true that accountability chains can be difficult to keep track of and the longer they get, the blurrier they get. The comments here are grossly oversimplifying this concept. reply TZubiri 4 hours agorootparentSure you are a little bit responsible if your hiring manager hires a dud, but not as much. Similarly your hiring manager is not as responsible as the dud, accountability loses power in each chain. You can fire your hiring manager and pick another one if he fails too often for example reply bigiain 14 hours agorootparentprevThe terms \"shit umbrella\" and \"shit funnel\" have been around for a long time, at least in the context of management in software development. https://managementpatterns.blogspot.com/2013/01/pattern-shit... I learned early on when I moved from development to management that a big part of my job was being accountable for everything my team did (short of outright sabotage). You don't hold junior devs accountable for anything, you do your best to monitor their work anytime they're working on something mission critical and to mentor them through the mistakes they make. Senior devs take on some or a lot of that monitoring and mentoring role, especially as the team size grows, but as their manager I am still accountable for any errors they make too (especially including letting errors from junior devs slip through). Sometimes I think the most important part of my job is standing up before senior management and saying something like \"My team made this series of decisions which resulted in the bad outcome we are here to discuss. I apologise and accept full responsibility. The team has learned from this, and we can assure you we will never repeat this mistake.\" And then deflecting and outright refusing to throw any of my team under the bus by naming them - to the point of being accused of insubordination occasionally. (To be honest, I didn't internalise that quite early enough. There are probably a few apologies I should have made from back then...) reply from-nibly 16 hours agorootparentprevThat's what TFA is saying, and they call it an accountability sink. reply from-nibly 16 hours agoparentprevYou can still be on the hook for rolling a 100 sided die. And in some cases that's effectively all you can do. At the end of the day it's a trolley problem (the real one, not deciding between two bad things, but looking at how people typically define reponsibility) One way or another you gotta own the decisions you make and deal with it. Even if the decision is to let someone else make the decision. The issue is that, yes, absolving yourself of accountability sure does free you to scale in ways previously thought unimaginable, it doesn't mean you absolve yourself of responsibility. The cure is keeping accountability in favor of scaling which means a much smaller scale to everything we have been doing. Another way to think about it. If you said you would give me 1 million dollars but I had to fully own up to what 1000 random people do in the next 24 hours I'd say thats a pretty raw deal. Basically no chance that a million will cover the chaos that a few of those 1000 people could cause. What some people do is take the million and then figure out how to rid themselves of the reponsibility. reply bigiain 14 hours agorootparent> You can still be on the hook for rolling a 100 sided die. And in some cases that's effectively all you can do. Sure. And the article allows for that. You need to have \"an account\" that acknowledges that at the time you didn't and couldn't have enough information to completely de risk the decision, but that you'd discussed and agreed that the 1/100 (or 1/5 or 1/10,000) risk of the bad outcome was a known and acceptable risk. \"where an account is something that you tell. How did something happen, what were the conditions that led to it happening, what made the decision seem like a good one at the time? Who were all of the people involved in the decision or event?\" reply pvillano 5 hours agoprevThere was a leaked memo essentially instructing to form a committee when you make an illegal decision so that one person cannot be sent to jail. Does anyone remember this? I've had a hard time finding it reply naitgacem 13 hours agoprevIn my country they enacted this system for student management that is national. It handles signups, restauration and housing services, grades, everything. One example is that the grades are entered by professors and mistakes happen all the time, for everyone, due to the insane server load. There's no one to complain to, because the excuse is always \"it's the system, not us\" reply rurban 11 hours agoprevI rather think accountability improved a lot. Esp. with the decline of buerocratic walls. Accountibility always was down. Back in aristocracy you were never allowed to ask for support. Only in modern civilisation this improved. Middle management, the clueless in the Gervais principle, need their walls. Don't be fooled by the decline of customer support in big orgs, like Google, Apple, or Amazon. They believe that support cannot scale, or if it's really needed, it needs to be outsourced to India or East Asia. reply pessimizer 3 hours agoparent> They believe that support cannot scale, or if it's really needed, it needs to be outsourced to India or East Asia. I disagree. They believe that support shouldn't scale with the size of the business, and should provide economies. reply eru 14 hours agoprevTom Schelling's 'The Strategy of Conflict' touches on similar themes, but mostly in a more positive light. One of his examples is that you should make yourself unavailable for contact, when you suspect someone is trying to blackmail you. That's exactly the same severing of a link as described in the article. reply busyant 13 hours agoparent> you should make yourself unavailable for contact, when you suspect someone is trying to blackmail you. Maybe I'm missing something, but how often does blackmail happen that it rises to the level of needing strategic advice like \"make yourself unavailable\" ? Who is Tom Schelling's audience? reply TeMPOraL 12 hours agorootparent> Who is Tom Schelling's audience? Politicians setting policies for use of nuclear weapons during the cold war, IIRC. Among others, at least. I read parts of that book many years ago, I recall the major theme is that voluntarily sacrificing control over the situation can be a powerful way to force the other party to do what you want. Like if you and me are playing \"chicken\", speeding towards each other and wanting the other to turn away first, you ripping out your steering wheel and throwing it out for me to see is a guaranteed way to force me to turn first and lose. This kind of stuff. I guess it ties into the larger topic here in that you can avoid being held accountable if you remove the ability to make any choices yourself. reply Eisenstein 12 hours agorootparentThis is how we get a Dr. Strangelove situation. If both people take that advice then they both crash into each other, even if they realize at the last minute it was a terrible idea. reply eru 10 hours agorootparentThat's why you should read the whole book, instead of just the three line summary in a comment. reply eru 10 hours agorootparentprev> Who is Tom Schelling's audience? Parents, of course. You might think I'm joking, but dealing with toddlers throwing tantrums is a prime example in some of his books. reply bruce511 14 hours agoprevI feel like the article, or perhaps just the example, is missing the point. >> a higher up at a hospitality company decides to reduce the size of its cleaning staff, because it improves the numbers on a balance sheet somewhere. Later, you are trying to check into a room, but it’s not ready and the clerk can’t tell you when it will be; they can offer a voucher, but what you need is a room. This reads from the perspective of a person checking in. But it should read from the perspective of the person who made the decision. The decision was made like this; On most days we have too many cleaners. If we reduce the cleaners we reduce expenses by x. On some days some customers will need to wait to check-in. Let's move checkin time from 1pm to 2pm (now in some cases to 4pm) to compensate. n% of customers arrive after 4pm anyway. We start cleaning early, so chances are we can accommodate early checkin where necessary. Where there's no room available before 4pm, some % will complain. Most of those will be placated with a voucher [1] which cost us nothing. Some small fraction will declare \"they'll never use us again\". Some will (for reasons) but we'll lose a few. But the savings outweigh the lost business. Put some of the savings into marketing and sales will go up. Costs remain lower. More profit. There is perfect accountability of this plan - the board watches to see if profits go up. They don't care about an individual guest with individual problems. The goal of the business is not to \"make everyone happy\". It's to \"make enough people happy\" to keep profits. [1] the existance of the voucher proves this possibility was accounted for. So accountability in this case is working - except for the customer who didn't get what they want. The customer feels frustrated, so from their perspective there's a failure. But there are other perspectives in play. And they are working as designed. reply praptak 12 hours agoparentThe economic calculation is often an accountability sink too. We can say that the economy has spoken, profit was made, case closed. But we can also look for accountability in the political system. Maybe the hotel should be obliged by the law to pay real money instead of a voucher? reply dgreensp 13 hours agoparentprevCame here to say this. And even in the case where the company's decision is arguably just \"bad,\" it still might not be a problem from the company's point of view. Companies (including start-ups) create buggy products all the time and don't care, and aren't very responsive to requests for support, as long as money is coming in. I don't think they are using special accountability-flushing techniques. It takes real work, intention, experience, and power in a company to create feedback channels, and use them, and ensure that the customer has an experience of quality. It doesn't happen by magic or by default. reply stana 14 hours agoprevInteresting. Wonder sometimes how much of consulting business is motivated by accountability avoidance - \"accountability sinks\" for hire reply jiggawatts 12 hours agoparentConsultant here: A lot of it. Ideally, a consultant is hired for their specialist skills, rare experience, sage advice on niche topics, etc... In practice, about half the work I do is to act as a lightning rod so that the guy with the power to sign the cheque for my time doesn't get fired if things go sideways. Instead, they can just blame me, shrug their shoulders, and hire another consultant. I've had a customer where I got \"fired\" for an \"error\". My coworker replaced me. Then he was fired, and I replaced him. We alternated like this for years. Upper management just saw the \"bad\" consultants get fired for their incompetence, they never noticed that we were the same two guys over and over. reply thierrydamiba 14 hours agoparentprevI don’t know how much it works with consulting because someone has to approve paying them. If they do a bad job you can blame whoever brought them in. It does make it easier to do something you were going to do anyways though. reply cj 17 hours agoprevThis article seems to redefine the word \"accountability\". In the first sentence: > In The Unaccountability Machine, Dan Davies argues that organizations form “accountability sinks,” structures that absorb or obscure the consequences of a decision such that no one can be held directly accountable for it. Why not just call it \"no-consequence sinks\"? It's somewhat of an oxymoron to say \"accountability\" isn't working because there's no consequence. Without any consequence there is no accountability. So why call it accountability in the first place? This article is describing something along the lines of \"shared accountability\" which, in project management, is a well known phenomenon: if multiple people are accountable for something, then no one is accountable. If someone is accountable for something that they can't do fully themselves, they are still accountable for setting up systems (maybe even people to help) to scale their ability to remain accountable for the thing. reply travisjungroth 16 hours agoparentI think it’s that the accountability falls into the sink and doesn’t reach the decision maker. I still find accountability poorly defined, even after the effort. Clicking through to the definition helps. It’s all kinda mushy. Being accountable is hearing and knowing a story. I don’t see why that has to correlate with decision power. The point of the article could be made much more clearly by talking about systems that leave decision makers not aware of the consequences of their decisions. All the anecdotes in the article fit that pattern. I think people don’t use the language of decision-consequences because it doesn’t capture an emotional aspect they’d rather not say out loud. They want the decision maker to feel their pain, they want the decision maker to hurt. Decision makers can be aware of how many unready rooms are caused by less cleaning staff, how many flights they’re cancelling. I’d actually bet they are. But that’s not enough, the harmed person wants to tell their story. reply 082349872349872 12 hours agorootparentIn the article, there are human agents involved at all times; sometimes people create accountability sinks even without humans. You're a neolithic farmer, and plant your barley, but that year there's a drought; you suffer the consequences, but who (or what) do you hold accountable? reply godelski 15 hours agoparentprevSounds like you perfectly understood the article. I don't get what you're complaining about. You agree but don't like the language? reply cj 15 hours agorootparentBasically, yea. Maybe being pedantic. In certain fields, there is a serious and distinct difference between Accountability, Responsibility, Consulting, and Informing. Source: https://en.m.wikipedia.org/wiki/Responsibility_assignment_ma... There’s a whole philosophy behind it. My spidey senses tingle when those words get misconstrued. reply 23B1 16 hours agoparentprevAuthor is describing a specific phenomena different from shared accountability. reply cj 16 hours agorootparentI disagree with the authors definition of accountability: > The fundamental law of accountability: the extent to which you are able to change a decision is precisely the extent to which you can be accountable for it, and vice versa. No. You can absolutely be accountable for something that you can’t change a decision about. Simple example: You’re a branding agency and you decide to rename X to Y. (No pun intended). The rebrand to Y fails. You’re accountable for the failure, but likely don’t have the ability to change anything by the time you know the results of your decision. Edit: ok, fair I agree. Bad example. A simpler example would be the person in the article continuing to point the the boss above them until there’s no one left. The chain would break somewhere along the way, but the broken chain is communication rather than one of accountability. The information may not reach the person able to make a change. But that doesn’t make them not accountable. If that person is unable to make a change because they’re in vacation for a month without anyone filling in, that person is accountable for both the results AND future results that are caused by not having someone monitor/reroute their acckuntability. reply bigiain 15 hours agorootparentIt's not clear from the article (which I largely agree with), but that \"ability to change the decision\" can just as easily refer to change the decision before it is made, instead of any ability to change it afterwards. Amazon's concept of \"two way and one way doors\" is useful here. A two way door decision lets you go back if the decision turns out to be bad and can be made with significantly less scrutiny that a one way door decision which you cannot back out of after you've acted on it. reply Terr_ 16 hours agorootparentprevIt's much like the distinction between responsibility and blame. At least in English, it seems like a lot of different meanings often get blurred together. At its root, responsibility is about who responds, rather than who causes. reply eduction 16 hours agorootparentprevIn that example, accountability is not with the branding firm at any point. Someone at the client that hired the branding firm is accountable for approving the rebrand and someone at the client is accountable for leaving it in place. The branding firm certainly does not seem to have performed well, from the scenario you described. But accountability is not the same as performance or even culpability. reply paulddraper 15 hours agorootparentprevReplace the word \"change\" to \"make\" and it may be more intuitive reply 23B1 12 hours agorootparentprevwe can argue semantics all day but when I see 'accountability' to me it means 'this person's ass is on the line' shared accountability is spreading that risk around to a group (but I don't think it necessarily eliminates that accountability – you can fire an entire department if you need to) author's point, which I think is interesting, is that there's bermuda triangles where accountability cannot occur and that these can manifest naturally, outside of any traditional RACI reply timst4 5 hours agoprevThe future iterations of this are purely terrifying. This is so elegantly demonstrated by the Oscar Nominated short film “Please Hold” from 2022. Picked up by mistake by a roving police drone, a young man is incarcerated autonomously and has no way of release outside of money or time. https://m.imdb.com/title/tt11383280/?lang=en&ref_=ext_shr_ln... reply BartjeD 12 hours agoprevAccountability sinks sounds a lot like the Toyota factory story, where on the contrary, every employee in the factory could pull the 'stop' lever if they thought there was a quality problem. Which of course drastically increased quality and feedback because the process is interrupted and stops. But I don't think it is quite so black and white in the world. Because the legal system is also a way to give feedback to companies. And it can stop them in their tracks. reply libraryofbabel 16 hours agoprevpatio11 has an episode of his podcast Complex Systems where he interviews the author of this book. It’s well worth a listen / read, as is the book itself. https://www.complexsystemspodcast.com/episodes/dan-davies-or... reply gmuslera 17 hours agoprevTaleb's Skin in the Game seem to be related to this, but from a different optic. Goodhart's Law is also mentioned, but is not the core argument. In the end, is about agency, who have it, and system dynamics to get rid of responsibility. reply RachelF 16 hours agoparentTrue. The modern company is a very very limited liability company: - Cut corners so your jets crash and kill people (Boeing)? - Cheat on emissions testing so your product kill people (VW)? - Hush up drug trial results so you kills people (Pfizer)? - Sloppy security leads to hundreds of millions of people's personal data being leaked (too many to mention)? What happens to those in charge? Nothing. Perhaps they leave with a big golden handshake. If it's really bad, they get a don't do it again agreement with the Feds. No accountability means no feedback/skin in the game. So nothing gets better. reply blackeyeblitzar 15 hours agorootparentLiability and retroactive clawbacks need to be introduced, particularly for very large organizations that like the government, don’t face real competition. reply pzmarzly 8 hours agorootparentThe EU and the FTC are slowly reintroducing the concept of liability to big corps, especially the big tech. Is the world healing? reply hn_throwaway_99 14 hours agoprevI liked this article a lot - it made me think about the ways large companies operate from a different viewpoint. At the same time, though, I think it's a mistake to leave out the fact that, in many ways, modern society is just so fundamentally complex that we (as a society at large) deliberately forego demanding accountability because we believe the system is so complex that it's impossible to assign blame to a single person. For example, given this is HN and many of us are software developers, how many times have we collectively supported \"blameless cultures\" when it comes to identifying and fixing software defects. We do this because we believe that software is so complex, and \"to err is human\", that it would be a disservice to assign blame to an individual - we say instead that the process should assume mistakes are inevitable, and then improve the process to find those mistakes earlier in the software lifecycle. But while I believe a \"blameless culture\" is valuable, I think a lot of times you can identify who was at fault. I mean, somebody at CrowdStrike decided to push a data update, without verifying it first, that bluescreened a good portion of the world's Windows machines and caused billions in damages. I just think that if you believe \"accountability sinks\" are always a bad thing, don't forget the flip side: would things always be better if we could always assign \"root cause blame\" to a specific individual? reply shkkmo 13 hours agoparentI think you are conflating accountability and blame when I don't think those terms can be used interchangeably here. Accountability can be used as a way of assigning blame, but that isn't all that it is good for. Accountability, at least as presented here, is about feeeback between those affected by a decision and those making it. In a \"blameless culture\", people are still held to account for their decisions and actions but are not blamed for their results. I would argue that a blameless culture actually makes accountability sinks less likely to develop. In blameful cultures, avoiding accountability avoids blame, but that is not needed in a blameless culture. reply hn_throwaway_99 12 hours agorootparentThanks, I found your response really helpful, and it helped identify some of the mistakes in my thinking. \"In blameful cultures, avoiding accountability avoids blame, but that is not needed in a blameless culture.\" - that really made a lot of sense to me. reply shkkmo 12 hours agorootparentThat wasn't a thought I had articulated before I read your comment, so your comment was also very productive for me. reply closeparen 12 hours agorootparentprevA blameless postmortem culture says that when a human error is identified in the causal chain leading to an incident, there will be no consequences for the individual. In a sense it embraces blame but eschews accountability. reply shkkmo 12 hours agorootparent> In a sense it embraces blame but eschews accountability. The two concepts we are talking about are each talked about under each label so there is enough ambiguity in both words that this is true. However choosing to use 'blame' in the opposite sense from the one being used in that context adds nothing to the conversation. reply Ozzie_osman 10 hours agoprevSystems Theory would describe this as \"intrinsic responsibility\". From Donella Meadows: “Intrinsic responsibility” means that the system is designed to send feedback about the consequences of decision-making directly and quickly and compellingly to the decision-makers. reply smugglerFlynn 8 hours agoparentSuper interesting concept, because these complaints tend to end up at the bottom of top management backlog due to amount of time and attention required to analyse these. In real life it just does not work unless your org is in the low hundreds of customers. This could change if technology could solve aggregation and analysis problem, making ready-made decision propositions to management. High risk of this mechanism just becoming another accountability sink, though. Another solution is to build large organisations out of federated micro-orgs, where such intristic responsibility is feasible. reply unit149 10 hours agoprevIn the 400 blows, Truffaut plays a schoolteacher whose disciplinary methods in the classroom only accentuate the rebelliousness of a boy. At home, this trickles down and he decides to run and drink milk. At the end, we find him on a beach and the film ends. Enforcing copyright law through an honest projection of 35mm film footage is a philanthropic endeavour. Making sure that every member of the production team, even the gaffers and stage hands take part in the exclusivity of re-capitalisation efforts, like the Fox complaint, is purely, legalistic sleight of hand. reply ksec 15 hours agoprev>> In The Unaccountability Machine, Dan Davies argues that organizations form “accountability sinks,” structures that absorb or obscure the consequences of a decision such that no one can be held directly accountable for it. Government and Civil Servant are the biggest example. I guess its time to re-watch \"Yes Minister\". reply Ma8ee 15 hours agoparentThat is a problem of organisation size, not so much whether the organisation is private or public. reply ksec 8 hours agorootparentI dont believe the largest company on planet earth by market cap has the problem as bad as government, even in a smallish country. reply Ma8ee 6 hours agorootparentI guess you haven’t worked in many large companies. I’m currently working for a large American company, and the waste and inefficiencies there beat most Swedish government agencies I’ve ever have had to deal with. reply ksec 4 hours agorootparentI wouldn't be",
    "originSummary": [
      "Dan Davies introduces the concept of \"accountability sinks,\" where organizations obscure the consequences of decisions, making it difficult to hold anyone accountable.- These accountability sinks are prevalent in industries like hospitality, health insurance, airlines, and government agencies, where decision origins become unclear, breaking feedback loops.- The use of AI can worsen accountability sinks by further obscuring responsibility, highlighting the need for new strategies to ensure organizations remain accountable for their decisions."
    ],
    "commentSummary": [
      "Organizations, including governments, often create \"accountability sinks,\" which obscure responsibility for decisions, complicating accountability.- Automated systems minimize human interaction, leading to frustrating customer experiences and a lack of direct accountability.- The complexity of modern systems results in decisions being made collectively or through automation, leaving individuals without clear recourse for addressing issues."
    ],
    "points": 441,
    "commentCount": 254,
    "retryCount": 0,
    "time": 1729381183
  },
  {
    "id": 41890784,
    "title": "QUIC is not quick enough over fast internet",
    "originLink": "https://arxiv.org/abs/2310.09423",
    "originBody": "Computer Science > Networking and Internet Architecture arXiv:2310.09423 (cs) [Submitted on 13 Oct 2023 (v1), last revised 30 Sep 2024 (this version, v2)] Title:QUIC is not Quick Enough over Fast Internet Authors:Xumiao Zhang, Shuowei Jin, Yi He, Ahmad Hassan, Z. Morley Mao, Feng Qian, Zhi-Li Zhang View PDF HTML (experimental) Abstract:QUIC is expected to be a game-changer in improving web application performance. In this paper, we conduct a systematic examination of QUIC's performance over high-speed networks. We find that over fast Internet, the UDP+QUIC+HTTP/3 stack suffers a data rate reduction of up to 45.2% compared to the TCP+TLS+HTTP/2 counterpart. Moreover, the performance gap between QUIC and HTTP/2 grows as the underlying bandwidth increases. We observe this issue on lightweight data transfer clients and major web browsers (Chrome, Edge, Firefox, Opera), on different hosts (desktop, mobile), and over diverse networks (wired broadband, cellular). It affects not only file transfers, but also various applications such as video streaming (up to 9.8% video bitrate reduction) and web browsing. Through rigorous packet trace analysis and kernel- and user-space profiling, we identify the root cause to be high receiver-side processing overhead, in particular, excessive data packets and QUIC's user-space ACKs. We make concrete recommendations for mitigating the observed performance issues. Comments: 10 pages, 16 figures Subjects: Networking and Internet Architecture (cs.NI) Cite as: arXiv:2310.09423 [cs.NI](or arXiv:2310.09423v2 [cs.NI] for this version)https://doi.org/10.48550/arXiv.2310.09423 Focus to learn more arXiv-issued DOI via DataCite Journal reference: Proceedings of the ACM Web Conference 2024 (WWW '24), Pages 2713-2722 Related DOI: https://doi.org/10.1145/3589334.3645323 Focus to learn more DOI(s) linking to related resources Submission history From: Xumiao Zhang [view email] [v1] Fri, 13 Oct 2023 22:05:13 UTC (201 KB) [v2] Mon, 30 Sep 2024 22:56:00 UTC (238 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.NInewrecent2023-10 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41890784",
    "commentBody": "QUIC is not quick enough over fast internet (arxiv.org)285 points by carlos-menezes 21 hours agohidepastfavorite248 comments tjoff 12 hours agoIndustry will do absolutely anything, except making lightweight sites. We had instant internet in the late 90s, if you were lucky enough to have a fast connection. The pages were small and there were barely any javascript. You can still find such fast loading lightweight pages today and the experience is almost surreal. It feels like the page has completely loaded before you even released the mousebutton. If only the user experience were better it might have been tolerable but we didn't get that either. reply OtomotO 12 hours agoparentI am currently de-javascripting a React app of some project I am working on. It's a blast. It's faster and way more resilient. No more state desync between frontend and backend. I admit there is a minimum of javascript (currently a few hundred lines) for convenience. I'll add a bit more to add the illusion this is still a SPA. I'll kill about 40k lines of React that way and about 20k lines of Kotlin. I'll have to rewrite about 30k lines of backend code though. Still, I love it. reply pushupentry1219 10 hours agorootparentHonestly I used to be on the strict noscript JavaScript hate train. But if your site works fast. Loads fast. With _a little_ JS that actually improves the functionality+usability in? I think that's completely fine. Minimal JS for the win. reply OtomotO 10 hours agorootparentAbsolutely. I want the basic functionality to work without JS. But we have a working application and users are not hating it and used to it. We rely on modals heavily. And for that I added (custom) JS. It's way simpler than alternatives and some things we do are not even possible without JS/WASM (via JS apis to manipulate the DOM) today. I am pragmatic. But as you mentioned it, personally I also use NoScript a lot and if a site refuses to load without JS it's a hard sell to me if I don't know it already. reply selimnairb 7 hours agorootparentprevBuilding a new app at work using Web Components and WebSockets for dynamism. I’m using Bulma for CSS, which is still about 300KiB. However, the site loads instantly. I’m not using a Javascript framework or bundler or any of that (not even npm!), just vanilla Javascript. It’s a dream to program and I love not having the complexity of a framework taking up space in my brain. reply starspangled 8 hours agorootparentprevWhat do you use that good javascipt for? And what is the excessive stuff that causes slowness and bloat? I'm not a web programmer, just curious. reply _heimdall 4 hours agorootparentMy rule of thumb is to render HTML where the state actually lives. In a huge majority of cases I come across that is on the server. Some things really are client-side only though, think temporary state responding to user interactions. Either way I also try really hard to make sure the UI is at least functional without JS. There are times that isn't possible, but those are pretty rare in my experience. reply graemep 4 hours agorootparentprevTwo examples that come up a lot for me: 1. filtering a drop down list by typing rather than scrolling through lots of options to pick one 2. Rearranging items with drag and drop The excessive stuff is requiring a whole lot of scripts and resources to load before you display a simple page of information. reply LtWorf 1 hour agorootparentDoesn't the combo box input field already do this? reply NetOpWibby 12 hours agorootparentprevNature is healing. Love to see it. reply kodama-lens 10 hours agoparentprevWhen I was finishing university I bought into the framework-based web-development hype. I thought that \"enterprise\" web-development has to be done this way. So I got some experience by migrating my homepage to a static VUE.JS version. Binding view and state by passing the variables name as a sting felt off, extending the build env seemed unnecessary complex and everything was slow and has to be done a certain way. But since everyone is using this, this must be right I thought. I got over this view and just finished the new version of my page. Raw HTML with some static-site-generator templating. The HTML size went down 90%, the JS usage went down 97% and build time is now 2s instead of 20s. The user experience is better and i get 30% more hits since the new version. The web could be so nice of we used less of it. reply wlll 4 hours agoparentprevMy personal projects are all server rendered HTML. My blog (a statically rendered Hugo site) has no JS at all, my project (Rails and server rendered HTML) has minimal JS that adds some nice to have stuff but nothing else (it works with no JS). I know they're my sites, but the experience is just so much better than most of the rest of the web. We've lost so much. reply pjmlp 11 hours agoparentprevLightweight sites don't make for shinny CVs. Even on the backend, now the golden goose is to sell microservices, via headless SaaS products connected via APIs, that certainly is going to perform. https://macharchitecture.com/ However if those are the shovels people are going to buy, then those are the ones we have to stockpile, so is the IT world. reply Zanfa 10 hours agorootparentMy feeling is that the microservice fad has passed… for now. But I’m sure it’ll be resurrected in a few years with a different name. reply _heimdall 4 hours agorootparentI've come across quite a few job postings in the last could weeks looking for senior engineers with experience migrating monoliths to micro services. Not sure if the fad is still here or if those companies are just slow to get onboard. There are still good uses for micro services. Specific services can gain a lot from it, the list of those types of services/apps is pretty short in my experience though. reply pjmlp 10 hours agorootparentprevNah, it is only really taking off now in enterprise consulting, with products going SaaS and what used to extension points via libraries, is now only possible via Webhooks and API calls, that naturally have to be done somewhere, either microservices or serverless. reply Flex247A 7 hours agoparentprevExample of an almost instant webpage today: https://www.mcmaster.com/ reply loufe 7 hours agorootparentAnd users clearly appreciate it. I was going over some bolt types with a design guy at my workplace yesterday for a project and his first instinct is to pull up the McMaster-Carr site to see what was possible. I don't know if we even order from them, since we pass through purchasing folks, but the site is just brilliantly simple and elegant. reply 8n4vidtmkvmk 2 hours agorootparentprevSomeone did an analysis of that site on tiktok or YouTube. It's using some tricks to speed things up, like preloading the html for the next page on hover and then replacing the shell of the page on click. So pre-rendering and prefetching. Pretty simple to do and effective apparently. reply nbittich 2 hours agoparentprevTried that on my website (bittich.be), it's only 20ish kb gzipped. I could have done better if I didn't use tailwind css :( reply cletus 19 hours agoprevAt Google, I worked on a pure JS Speedtest. At the time, Ookla was still Flash-based so wouldn't work on Chromebooks. That was a problem for installers to verify an installation. I learned a lot about how TCP (I realize QUIC is UDP) responds to various factors. I look at this article and consider the result pretty much as expected. Why? Because it pushes the flow control out of the kernel (and possibly network adapters) into userspace. TCP has flow-control and sequencing. QUICK makes you manage that yourself (sort of). Now there can be good reasons to do that. TCP congestion control is famously out-of-date with modern connection speeds, leading to newer algorithms like BRR [1] but it comes at a cost. But here's my biggest takeaway from all that and it's something so rarely accounted for in network testing, testing Web applications and so on: latency. Anyone who lives in Asia or Australia should relate to this. 100ms RTT latency can be devastating. It can take something that is completely responsive to utterly unusable. It slows down the bandwidth a connection can support (because of the windows) and make it less responsive to errors and congestion control efforts (both up and down). I would strongly urge anyone testing a network or Web application to run tests where they randomly add 100ms to the latency [2]. My point in bringing this up is that the overhead of QUIC may not practically matter because your effective bandwidth over a single TCP connection (or QUICK stream) may be MUCH lower than your actual raw bandwidth. Put another way, 45% extra data may still be a win because managing your own congestion control might give you higher effective speed over between two parties. [1]: https://atoonk.medium.com/tcp-bbr-exploring-tcp-congestion-c... [2]: https://bencane.com/simulating-network-latency-for-testing-i... reply klabb3 17 hours agoparentI did a bunch of real world testing of my file transfer app[1]. Went in with the expectation that Quic would be amazing. Came out frustrated for many reasons and switched back to TCP. It’s obvious in hindsight, but with TCP you say “hey kernel send this giant buffer please” whereas UDP is packet switched! So even pushing zeroes has a massive CPU cost on most OSs and consumer hardware, from all the mode switches. Yes, there are ways around it but no they’re not easy nor ready in my experience. Plus it limits your choice of languages/libraries/platforms. (Fun bonus story: I noticed significant drops in throughput when using battery on a MacBook. Something to do with the efficiency cores I assume.) Secondly, quic does congestion control poorly (I was using quic-go so mileage may vary). No tuning really helped, and TCP streams would take more bandwidth if both were present. Third, the APIs are weird man. So, quic itself has multiple streams, which makes it non-drop in replacement with TCP. However, the idea is to have HTTP/3 be drop-in replaceable at a higher level (which I can’t speak to because I didn’t do). But worth keeping in mind if you’re working on the stream level. In conclusion I came out pretty much defeated but also with a newfound respect for all the optimizations and resilience of our old friend tcp. It’s really an amazing piece of tech. And it’s just there, for free, always provided by the OS. Even some of the main issues with tcp are not design faults but conservative/legacy defaults (buffer limits on Linux, Nagle, etc). I really just wish we could improve it instead of reinventing the wheel.. [1]: https://payload.app/ reply eptcyka 13 hours agorootparentOne does not need to send and should not send one packet per syscall. reply jacobgorm 8 hours agorootparentOn platforms like macOS that don’t have UDP packet pacing you more or less have to. reply tomohawk 7 hours agorootparentprevOn linux, there is sendmmsg, which can send up to 1024 packets each time, but that is a far cry from a single syscall to send 1GB file. With GSO, it is possible to send even more datagrams to call, but the absolute limit is 64KB * 1024 per syscall, and it is fiddly to pack datagrams so that this works correctly. You might think you can send datagrams of up to 64KB, but due to limitations in how IP fragment reassembly works, you really must do your best to not allow IP fragmentation to occur, so 1472 is the largest in most circumstances. reply Veserv 3 hours agorootparentWhy does 1 syscall per 1 GB versus 1 syscall per 1 MB have any meaningful performance cost? syscall overhead is only on the order of 100-1000 ns. Even at a blistering per core memory bandwidth of 100 GB/s, just the single copy fundamentally needed to serialize 1 MB into network packets costs 10,000 ns. The ~1,000 syscalls needed to transmit a 1 GB file would incur excess overhead of 1 ms versus 1 syscall per 1 GB. That is at most a 10% overhead if the only thing your system call needs to do is copy the data. As in it takes 10,000 ns total to transmit 1,000 packets meaning you get 10 ns per packet to do all of your protocol segmentation and processing. The benchmarks in the paper show that the total protocol execution time for a 1 GB file using TCP is 4 seconds. The syscall overhead for issuing 1,000 excess syscalls should thus be ~1/4000 or about 0.025% which is totally irrelevant. The difference between the 4 second TCP number and the 8 second QUIC number can not be meaningfully traced back to excess syscalls if they were actually issuing max size sendmmsg calls. Hell, even if they did one syscall per packet that would still only account for a mere 1 second of the 4 second difference. It would be a stupid implementation for sure to have such unforced overhead, but even that would not be the actual cause of the performance discrepancy between TCP and QUIC in the produced benchmarks. reply intelVISA 5 hours agorootparentprevAnyone pushing packets seriously doesn't even use syscalls... reply astrange 14 hours agorootparentprev> (Fun bonus story: I noticed significant drops in throughput when using battery on a MacBook. Something to do with the efficiency cores I assume.) That sounds like the thread priority/QoS was incorrect, but it could be WiFi or something. reply skissane 19 hours agoparentprev> Because it pushes the flow control out of the kernel (and possibly network adapters) into userspace That’s not an inherent property of the QUIC protocol, it is just an implementation decision - one that was very necessary for QUIC to get off the ground, but now it exists, maybe it should be revisited? There is no technical obstacle to implementing QUIC in the kernel, and if the performance benefits are significant, almost surely someone is going to do it sooner or later. reply conradev 13 hours agorootparentLooks like it’s being worked on: https://lwn.net/Articles/989623/ reply throawayonthe 1 hour agorootparentalso looks like current quic performance issues are a consideration, tested in section 4. : > The performance gap between QUIC and kTLS may be attributed to: - The absence of Generic Segmentation Offload (GSO) for QUIC. - An additional data copy on the transmission (TX) path. - Extra encryption required for header protection in QUIC. - A longer header length for the stream data in QUIC. reply lttlrck 18 hours agorootparentprevFor Linux that's true. But Microsoft never added SCTP to Windows; not being beholden to Microsoft and older OS must have been part of the calculus? reply skissane 18 hours agorootparent> But Microsoft never added SCTP to Windows Windows already has an in-kernel QUIC implementation (msquic.sys), used for SMB/CIFS and in-kernel HTTP. I don’t think it is accessible from user-space - I believe user-space code uses a separate copy of the same QUIC stack that runs in user-space (msquic.dll), but there is no reason in-principle why Microsoft couldn’t expose the kernel-mode implementation to user space reply astrange 14 hours agorootparentprevNo one ever uses SCTP. It's pretty unclear to me why any OSes do include it; free OSes seem to like junk drawers of network protocols even though they add to the security surface in kernel land. reply j1elo 4 hours agorootparentSCTP is exactly how you establish a data communication link with the very modern WebRTC protocol stack (and is rebranded to \"WebRTC Data Channels\"). Granted, it is SCTP-over-UDP. But still. So yes, SCTP is under the covers getting a lot more use than it seems, still today. However all WebRTC implementations usually bring their own userspace libraries to implement SCTP, so they don't depend on the one from the OS. reply supriyo-biswas 13 hours agorootparentprevThe telecom sector uses SCTP in lots of places. reply kelnos 14 hours agorootparentprevDoes anyone even build SCTP support directly into the kernel? Looks like Debian builds it as a module, which I'm sure I never have and never will load. Security risk seems pretty minimal there. (And if someone can somehow coerce me into loading it, I have bigger problems.) reply jeroenhd 12 hours agorootparentLinux and FreeBSD have had it for ages. Anything industrial too. Solaris, QNX, Cisco IOS. SCTP is essential for certain older telco protocols and in certain protocols developed for LTE it was added. End users probably don't use it much, but the harsware their connections are going through will speak SCTP at some level. reply rjsw 5 hours agorootparentprevI added it to NetBSD and build it into my kernels, it isn't enabled by default though. Am part way through adding NAT support for it to the firewall. reply lstodd 10 hours agorootparentprev4g/LTE runs on it. So you use it too, via your phone. reply astrange 9 hours agorootparentHuh, didn't know that. But iOS doesn't support it, so it's not needed on the AP side even for wifi calling. reply spookie 12 hours agorootparentprevAnd most of those protocols can be disabled under sysctl.conf. reply ants_everywhere 18 hours agorootparentprevIs this something you could use ebpf for? reply bdd8f1df777b 15 hours agoparentprevAs a Chinese whose latency to servers outside China often exceeds 300ms, I'm a staunch supporter of QUIC. The difference is night and day. reply pests 17 hours agoparentprevThe Network tab in the Chrome console allows you to degrade your connection. There are presets for Slow/Fast 4G, 3G, or you can make a custom present where you can specify download and upload speeds, latency in ms, a packet loss percent, a packet queue length and can enable packet reordering. reply lelandfe 17 hours agorootparentThere's also an old macOS preference pane called Network Link Conditioner that makes the connections more realistic: https://nshipster.com/network-link-conditioner/ IIRC, Chrome's network simulation just applies a delay after a connection is established reply mh- 13 hours agorootparentI don't remember the details offhand, but yes - unless Chrome's network simulation has been rewritten in the last few years, it doesn't do a good job of approximating real world network conditions. It's a lot better than nothing, and doing it realistically would be a lot more work than what they've done, so I say this with all due respect to those who worked on it. reply youngtaff 8 hours agorootparentprevChrome’s network emulation is a pretty poor simulation of the real world… it throttles on a per request basis so can’t simulate congestion due to multiple requests in flight at the same time Really need something like ipfw, dummynet, tc etc to do it at the packet level reply attentive 11 hours agoparentprev> I look at this article and consider the result pretty much as expected. Why? Because it pushes the flow control out of the kernel (and possibly network adapters) into userspace. TCP has flow-control and sequencing. QUICK makes you manage that yourself (sort of). This implies that user space is slow. Yet, some(most?) of the fastest high-performance TCP/IP stacks are made in user space. reply formerly_proven 10 hours agorootparentIf the entire stack is in usermode and it's directly talking to the NIC with no kernel involvement beyond setup at all. This isn't the case with QUIC, it uses the normal sockets API to send/recv UDP. reply WesolyKubeczek 10 hours agorootparentprevYou have to jump contexts for every datagram, and you cannot offload checksumming to the network hardware. reply Tade0 8 hours agoparentprevI've been tasked with improving a system where a lot of the events relied on timing to be just right, so now I routinely click around the app with a 900ms delay, as that's the most that I can get away with without having the hot-reloading system complain. Plenty of assumptions break down in such an environment and part of my work is to ensure that the user always knows that the app is really doing something and not just being unresponsive. reply pzmarzly 10 hours agoparentprev> I look at this article and consider the result pretty much as expected. Why? Because it pushes the flow control out of the kernel (and possibly network adapters) into userspace. TCP has flow-control and sequencing. QUICK makes you manage that yourself (sort of). I truly hope the QUIC in Linux Kernel project [0] succeeds. I'm not looking forward to linking big HTTP/3 libraries to all applications. [0] https://github.com/lxin/quic reply ec109685 19 hours agoparentprevFor reasonably long downloads (so it has a chance to calibrate), why don't congestion algorithms increase the number of inflight packets to a high enough number that bandwidth is fully utilized even over high latency connections? It seems like it should never be the case that two parallel downloads will preform better than a single one to the same host. reply dan-robertson 18 hours agorootparentThere are two places a packet can be ‘in-flight’. One is light travelling down cables (or the electrical equivalent) or in memory being processed by some hardware like a switch, and the other is sat in a buffer in some networking appliance because the downstream connection is busy (eg sending packets that are further up the queue, at a slower rate than they arrive). If you just increase bandwidth it is easy to get lots of in-flight packets in the second state which increases latency (admittedly that doesn’t matter so much for long downloads) and the chance of packet loss from overly full buffers. CUBIC tries to increase bandwidth until it hits packet loss, then cuts bandwidth (to drain buffers a bit) and ramps up and hangs around close to the rate that led to loss, before it tries sending at a higher rate and filling up buffers again. Cubic is very sensitive to packet loss, which makes things particularly difficult on very high bandwidth links with moderate latency as you need very low rates of (non-congestion-related) loss to get that bandwidth. BBR tries to do the thing you describe while also modelling buffers and trying to keep them empty. It goes through a cycle of sending at the estimated bandwidth, sending at a lower rate to see if buffers got full, and sending at a higher rate to see if that’s possible, and the second step can be somewhat harmful if you don’t need the advantages of BBR. I think the main thing that tends to prevent the thing you talk about is flow control rather than congestion control. In particular, the sender needs a sufficiently large send buffer to store all unacked data (which can be a lot due to various kinds of ack-delaying) in case it needs to resend packets, and if you need to resend some then your send buffer would need to be twice as large to keep going. On the receive size, you need big enough buffers to be able to fill up those buffers from the network while waiting for an earlier packet to be retransmitted. On a high-latency fast connection, those buffers need to be big to get full bandwidth, and that requires (a) growing a lot, which can take a lot of round-trips, and (b) being allowed by the operating system to grow big enough. reply toast0 15 hours agorootparentprevI've run a big webserver that served a decent size apk/other app downloads (and a bunch of small files and what nots). I had to set the maximum outgoing window to keep the overall memory within limits. IIRC, servers were 64GB of ram and sendbufs were capped at 2MB. I was also dealing with a kernel deficiency that would leave the sendbuf allocated if the client disappeared in LAST_ACK. (This stems from a deficiency in the state description from the 1981 rfc written before my birth) reply dan-robertson 7 hours agorootparentI wonder if there’s some way to reduce this server-side memory requirement. I thought that was part of the point of sendfile but I might be mistaken. Unfortunately sendfile isn’t so suitable nowadays because of tls. But maybe if you could do tls offload and do sendfile then an OS could be capable of needing less memory for sendbufs. reply gmueckl 18 hours agorootparentprevLarger windows can reduce the maximum number of simultaneous connections on the sender side. reply Veserv 18 hours agorootparentprevYou can in theory. You just need a accurate model of your available bandwidth and enough buffering/storage to avoid stalls while you wait for acknowledgement. It is, frankly, not even that hard to do it right. But in practice many implementations are terrible, so good luck. reply superjan 8 hours agoparentprevAs an alternative to simulating latency: How about using a VPN service to test your website via Australia? I suppose that when it easier to do, it is more likely that people will actually do this test. reply sokoloff 8 hours agorootparentThat’s going to give you double (plus a bit) latency as your users in Australia will experience. reply codetrotter 7 hours agorootparentRent a VPS or physical server in Australia. Then you will have approx the same latency accessing that dev server, that the Australians have reaching servers in your country. reply reshlo 17 hours agoparentprev> Anyone who lives in Asia or Australia should relate to this. 100ms RTT latency can be devastating. When I used to (try to) play online games in NZ a few years ago, RTT to US West servers sometimes exceeded 200ms. reply albertopv 11 hours agorootparentI would be surprised if online games use TCP. Anyway, physics is still there and light speed is fast, but that much. In 10ms it travels about 3000km, NZ to US west coast is about 11000km, so less than 60ms is impossible. Cables are probably much longer, c speed is lower in a medium, add network devices latency and 200ms from NZ to USA is not that bad. reply Hikikomori 8 hours agorootparentSpeed of light in fiber is about 200 000km/s. Most of the latency is because of distance, modern routers have a forwarding latency of tens of microseconds, some switches can start sending out a packet before fully receiving it. reply indrora 16 hours agorootparentprevWhen I was younger, I played a lot of cs1.6 and hldm. Living in rural New Mexico, my ping times were often 150-250ms. DSL kills. reply somat 12 hours agorootparentI used to play netquake(not quakeworld) at up to 800 ms lag, past that was too much for even young stupid me. For them that don't know the difference. netquake was the original strict client server version of quake, you hit the forward key it sends that to the server and the server then sends back where you moved. quakeworld was the client side prediction enhancement that came later, you hit forward, the client moves you forwards and sends it to the server at the same time. and if there are differences it gets reconciled later. For the most part client side prediction feels better to play. however when there are network problems, large amounts of lag, a lot of artifacts start to show up, rubberbanding, jumping around, hits that don't. Pure client server feels worse, every thing gets sluggish, and mushy but movement is a little more predictable and logical and can sort of be anticipated. I have not played quake in 20 years but one thing I remember is at past 800ms of lag the lava felt magnetic, it would just suck you in, every time. reply api 18 hours agoparentprevA major problem with TCP is that the limitations of the kernel network stack and sometimes port allocation place absurd artificial limits on the number of active connections. A modern big server should be able to have tens of millions of open TCP connections at least, but to do that well you have to do hacks like running a bunch of pointless VMs. reply toast0 16 hours agorootparent> A modern big server should be able to have tens of millions of open TCP connections at least, but to do that well you have to do hacks like running a bunch of pointless VMs. Inbound connections? You don't need to do anything other than make sure your fd limit is high and maybe not be ipv4 only and have too many users behind the same cgnat. Outbound connections is harder, but hopefully you don't need millions of connections to the same destination, or if you do, hopefully they support ipv6. When I ran millions of connections through HAproxy (bare tcp proxy, just some peaking to determine the upstream), I had to do a bunch of work to make it scale, but not because of port limits. reply jrpelkonen 20 hours agoprevCurl creator/maintainer Daniel Stenberg blogged about HTTP/3 in curl a few months ago: https://daniel.haxx.se/blog/2024/06/10/http-3-in-curl-mid-20... One of the things he highlighted was the higher CPU utilization of HTTP/3, to the point where CPU can limit throughput. I wonder how much of this is due to the immaturity of the implementations, and how much this is inherit due to way QUIC was designed? reply dan-robertson 18 hours agoparentTwo recommendations are for improving receiver-side implementations – optimising them and making them multithreaded. Those suggest some immaturity of the implementations. A third recommendation is UDP GRO, which means modifying kernels and ideally NIC hardware to group received UDP packets together in a way that reduces per-packet work (you do lots of per-group work instead of per-packet work). This already exists in TCP and there are similar things on the send side (eg TSO, GSO in Linux), and feels a bit like immaturity but maybe harder to remedy considering the potential lack of hardware capabilities. The abstract talks about the cost of how acks work in QUIC but I didn’t look into that claim. Another feature you see for modern tcp-based servers is offloading tls to the hardware. I think this matters more for servers that may have many concurrent tcp streams to send. On Linux you can get this either with userspace networking or by doing ‘kernel tls’ which will offload to hardware if possible. That feature also exists for some funny stuff in Linux about breaking down a tcp stream into ‘messages’ which can be sent to different threads, though I don’t know if it allows eagerly passing some later messages when earlier packets were lost. reply cj 19 hours agoparentprevI’ve always been under the impression that QUIC was designed for connections that aren’t guaranteed to be stable or fast. Like mobile networks. I never got the impression that it was intended to make all connections faster. If viewed from that perspective, the tradeoffs make sense. Although I’m no expert and encourage someone with more knowledge to correct me. reply dan-robertson 18 hours agorootparentI think that’s a pretty good impression. Lots of features for those cases: - better behaviour under packet loss (you don’t need to read byte n before you can see byte n+1 like in tcp) - better behaviour under client ip changes (which happen when switching between cellular data and wifi) - moving various tricks for getting good latency and throughput in the real world into user space (things like pacing, bbr) and not leaving enough unencrypted information in packets for middleware boxes to get too funky reply fulafel 13 hours agorootparentprevThat's how the internet works, there's no guaranteed delivery and TCP bandwidth estimation is based on when packets start to be dropped when you send too many. reply therealmarv 18 hours agorootparentprevIt makes everything faster, it's an evolvement of HTTP/2 in many ways. I recommend watching https://www.youtube.com/watch?v=cdb7M37o9sU reply therealmarv 18 hours agoparentprev\"immaturity of the implementations\" is a funny wording here. QUIC was created because there is absolutely NO WAY that all internet hardware (including all middleware etc) out there will support a new TCP or TLS standard. So QUIC is an elegant solution to get a new transport standard on top of legacy internet hardware (on top of UDP). In an ideal World we would create a new TCP and TLS standard and replace and/or update all internet routers and hardware everywhere World Wide so that it is implemented with less CPU utilization ;) reply api 18 hours agorootparentA major mistake in IP’s design was to allow middle boxes. The protocol should have had some kind of minimal header auth feature to intentionally break them. It wouldn’t have to be strong crypto, just enough to make middle boxes impractical. It would have forced IPv6 migration immediately (no NAT) and forced endpoints to be secured with local firewalls and better software instead of middle boxes. The Internet would be so much simpler, faster, and more capable. Peer to peer would be trivial. Everything would just work. Protocol innovation would be possible. Of course tech is full of better roads not taken. We are prisoners of network effects and accidents of history freezing ugly hacks into place. reply kbolino 5 hours agorootparentThe only mechanism I can think of that could have been used for that purpose, and was publicly known about (to at least some extent) in the late 1970s, would be RSA. That is strong crypto, or at least we know it is when used properly today, but it's unlikely the authors of IP would have known about it. Even if they did, the logistical challenges of key distribution would have sunk its use, and they would almost certainly have fallen into one of the traps in implementing it that took years to discover, and the key sizes that would have been practical for use ca 1980 would be easy to break by the end of the 1990s. Simply put, this isn't a road not taken, it's a road that didn't exist. reply tsimionescu 10 hours agorootparentprevI completely disagree with this take. First of all, NAT is what saved the Internet from being forked. IPv6 transition was a pipe dream at the time it was first proposed, and the vast growth in consumers for ISPs that had just paid for expensive IPv4 boxes would never have resulted in them paying for far more expensive (at the time) IPv6 boxes, it would have resulted in much less growth, or other custom solutions, or even separate IPv4 networks in certain parts of the world. Or, if not, it would have resulted in tunneling all traffic over a protocol more amenable to middle boxes, such as HTTP, which would have been even worse than the NAT happening today. Then, even though it was unintentional, NAT and CGNAT are what ended up protecting consumers from IP-level tracking. If we had transitioned from IPv4 directly to IPv6, without the decades of NAT, all tracking technology wouldn't have bothered with cookies and so on, we would have had the trivial IP tracking allowed by the one-IP-per-device vision. And with the entrenched tracking adware industry controlling a big part of the Internet and relying on tracking IPs, the privacy extensions to IPv6 (which, remember, came MUCH later in IPv6's life than the original vision for the transition) would never have happened. I won't bother going into the other kinds of important use cases that other middle boxes support, that a hostile IPv4 would have prevented, causing even bigger problems. NAT is actually an excellent example of why IPs design decisions that allow middle boxes are a godsend, not a tragic mistake. Now hopefully we can phase out NAT in the coming years, as it's served its purpose and can honorably retire. reply api 3 hours agorootparentThe cost of NAT is much higher than you think. If computers could just trivially connect to each other then software might have evolved collaboration and communication features that rely on direct data sharing. The privacy and autonomy benefits of that are enormous, not to mention the reduced need for giant data centers. It’s possible that the cloud would not have been nearly as big as it has been. The privacy benefits of NAT are minor to nonexistent. In most of the developed world most land connections get one effectively static V4 IP which is enough for tracking. Most tracking relies primarily on fingerprints, cookies, apps, federated login, embeds, and other methods anyway. IP is secondary, especially with the little spies in our pockets that are most people’s phones. reply johncolanduoni 17 hours agorootparentprevMaking IPv4 headers resistant to tampering wouldn't have helped with IPv6 rollout, as routers (both customer and ISP) would still need to be updated to be able to understand how to route packets with the new headers. reply ajb 16 hours agorootparentThe GP's point is that if middle boxes couldn't rewrite the header, NAt would be impossible. And if NAT were impossible, ipV4 would have died several years ago because NAT allowed more computers than addresses. reply tsimionescu 10 hours agorootparentVery unlikely. Most likely NAT would have happened to other layers of the stack (HTTP, for example), causing even more problems. Or, the growth of the Internet would have stalled dramatically, as ISPs would have either increased prices dramatically to account for investments in new and expensive IPv6 hardware, or simply stopped acceptong new subscribers. reply ajb 7 hours agorootparentYour first scenario is plausible, the second I'm not sure about. Due to the growth rate central routers had a very fast replacement cycle anyway, and edge devices mostly operated at layer 2, so didn't much care about IP. (Maybe the was done device in the middle that would have had a shorter lifespan?). I worked at a major router semiconductor vendor, and I can tell you that all the products supported IPv6 at a hardware level for many, many years before significant deployment and did not use it as a price differentiator. (Sure, they were probably buggy for longer than necessary, but that would have been shaken out earlier if the use was earlier). So I don't think the cost of routers was the issue. The problem with ipv6 in my understanding was that the transitional functions (nat-pt etc) were half baked and a new set had to be developed. It is possible that disruption would have occurred if that had to be done against an earlier address exhaustion date. reply ocdtrekkie 15 hours agorootparentprevThis ignores... a lot of reality. Like the fact that when IP was designed, the idea of every individual network device having to run its own firewall was impractical performance-wise, and decades later... still not really ideal. There's definitely some benefits to glean from a zero trust model, but putting a moat around your network still helps a lot and NAT is probably the best accidental security feature to ever exist. Half the cybersecurity problems we have are because the cloud model has normalized routing sensitive behavior out to the open Internet instead of private networks. My middleboxes will happily be configured to continue to block any traffic that refuses to obey them. (QUIC and ECH inclusive.) reply codexon 13 hours agorootparentEven now, you can saturate a modern cpu core with only 1 million packets per second. reply AndyMcConachie 9 hours agorootparentprevA major mistake of the IETF was to not standardize IPv4 NAT. Had it been standardized early on there would be fewer problems with it. reply bell-cot 7 hours agorootparentprev> It would have forced IPv6 migration immediately (no NAT) and forced endpoints to be secured... There's a difference between \"better roads not taken\", and \"taking this road would require that most of our existing cars and roads be replaced, simultaneously\". reply dcow 14 hours agorootparentprevNow that’s a horse of a different color! I’m already opining this alt reality. Middle-boxes and everyone touching them ruined the internet. reply paulddraper 19 hours agoparentprevThose performance results surprised me too. His testing has CPU-bound quiche at 900MB/s. I wonder if the CPU was throttled. Because if HTTP 3 impl took 4x CPU that could be interesting but not necessarily a big problem if the absolute value was very low to begin with. reply lysace 21 hours agoprev> We find that over fast Internet, the UDP+QUIC+HTTP/3 stack suffers a data rate reduction of up to 45.2% compared to the TCP+TLS+HTTP/2 counterpart. Haven't read the whole paper yet, but below 600 Mbit/s is implied as being \"Slow Internet\" in the intro. reply cj 19 hours agoparentIn other words: Enable http/3 + quic between client browseredge and restrict edgeorigin connections to http/2 or http/1 Cloudflare (as an example) only supports QUIC between clientedge and doesn’t support it for connections to origin. Makes sense if the edgeorigin connection is reusable, stable, and “fast”. https://developers.cloudflare.com/speed/optimization/protoco... reply dilyevsky 14 hours agorootparentCloudflare tunnels work over quic so this is not entirely correct reply dathinab 20 hours agoparentprevThey also mainly identified a throughput reduction due to latency issues caused by ineffective/too many syscalls in how browsers implement it. But such a latency issue isn't majorly increasing battery usage (compared to a CPU usage issue which would make CPUs boost). Nor is it an issue for server-to-server communication. It basically \"only\" slows down high bandwidth transmissions on end user devices with (for 2024 standards) very high speed connection (if you take effective speeds from device to server, not speeds you where advertised to have bough and at best can get when the server owner has a direct pairing agreement with you network provider and a server in your region.....). Doesn't mean the paper is worthless, browser should improve their impl. and it highlights it. But the title of the paper is basically 100% click bait. reply ec109685 19 hours agorootparentHow is it clickbait? The title implies that QUIC isn't as fast as other protocols over fast internet connections. reply dathinab 18 hours agorootparentBecause it's QUIC _implementations of browser_ not being as fast as the non quick impl of browsers on connections most people would not just call fast but very fast (in context of browser usage) while still being definitely 100% fast enough for all browser use case done today (sure it theoretically might reduce video bit rate, that is, if it isn't already capped to a anyway smaller rate, which AFIK it basically always is). So \"Not Quick Enough\" is plain out wrong, it is fast enough. The definition of \"Fast Internet\" misleading. And even \"QUIC\" is misleading as it normally refers to the protocol while the benchmarked protocol is HTTP/3 over QUIC and the issue seem to be mainly in the implementations. reply Dylan16807 21 hours agoparentprevJust as important is > we identify the root cause to be high receiver-side processing overhead, in particular, excessive data packets and QUIC's user-space ACKs It doesn't sound like there's a fundamental issue with the protocol. reply Aurornis 20 hours agoparentprevInternet access is only going to become faster. Switching to a slower transport just as Gigabit internet is proliferating would be a mistake, obviously. reply ratorx 20 hours agorootparentIt depends on whether it’s meaningfully slower. QUIC is pretty optimized for standard web traffic, and more specifically for high-latency networks. Most websites also don’t send enough data for throughput to be a significant issue. I’m not sure whether it’s possible, but could you theoretically offload large file downloads to HTTP/2 to get best of both worlds? reply pocketarc 19 hours agorootparent> could you theoretically offload large file downloads to HTTP/2 Yes, you can! You’d have your websites on servers that support HTTP/3 and your large files on HTTP/2 servers, similar to how people put certain files on CDNs. It might well be a great solution! reply kijin 15 hours agorootparentprevHigh-latency networks are going away, too, with Cloudflare eating the web alive and all the other major clouds adding PoPs like crazy. reply tomxor 20 hours agorootparentprevIn terms of maximum available throughput it will obviously become greater. What's less clear is if the median and worst throughput available throughout a nation or the world will continue to become substantially greater. It's simply not economical enough to lay fibre and put 5G masts everywhere (5G LTE bands covers less area due to being higher frequency, and so are also limited to being deployed in areas with a higher enough density to be economically justifiable). reply nine_k 19 hours agorootparentFiber is the most economical solution, it's compact, cheap, not susceptible to electromagnetic interference from thunderstorms, not interesting for metal thieves, etc. Most importantly, it can be heavily over-provisioned for peanuts, so your cable is future-proof, and you will never have dig the same trenches again. Copper only makes sense if you already have it. reply tomxor 14 hours agorootparentThen why isn't it everywhere, it's been practical for over 40 years now. reply nine_k 11 hours agorootparentIt is everywhere in new development. I remember Google buying tons of \"dark fiber\" capacity from telcos like 15 years ago; that fiber was likely laid for future needs 20-25 years ago. New apartment buildings in NYC just get fiber, with everything, including traditional \"cable TV\" with BNC connectors, powered by it. But telcos have colossal copper networks, and they want to milk the last dollars from it before it has to be replaced, with digging and all. Hence price segmenting, with slower \"copper\" plans and premium \"fiber\" plans, obviously no matter if the building has fiber already. Also, passive fiber interconnects have much higher losses than copper with RJ45s. This means you want to have no more than 2-3 connectors between pieces of active equipment, including from ISP to a building. This requires more careful planning, and this is why wiring past the apartment (or even office floor or a single-family house) level is usually copper Ethernet. reply jiggawatts 20 hours agorootparentprevHere in Australia there’s talk of upgrading the National Broadband Network to 2.5 Gbps to match modern consumer Ethernet and WiFi speeds. I grew up with 2400 baud modems as the super fast upgrade, so talk of multiple gigabits for consumers is blowing my mind a bit. reply Kodiack 20 hours agorootparentMeanwhile here in New Zealand we can get 10 Gbps FTTH already. Sorry about your NBN! reply wkat4242 19 hours agorootparentHere in Spain too. I don't see a need for it yet though. I'm a really heavy user (it specialist with more than a hundred devices in my networks) and I really don't need it. reply jiggawatts 15 hours agorootparentThese things are nice-to-have until they become sufficiently widespread that typical consumer applications start to require the bandwidth. That comes much later. E.g.: 8K 60 fps video streaming benefits from data rates up to about 1 Gbps in a noticeable way, but that's at least a decade away form mainstream availability. reply notpushkin 12 hours agorootparentThe other side of this particular coin is, when such bandwidth is widely available, suddenly a lot of apps that have worked just fine are now eating it up. I'm not looking forward to 9 gigabyte Webpack 2036 bundles everywhere :V reply wkat4242 3 hours agorootparentYeah for me it's mostly ollama models lol. It is nice to see it go fast. But even on my 1gbit it feels fast enough. reply wkat4242 3 hours agorootparentprevYeah the problem here is also that I don't have the router setup to actually distribute that kind of bandwidth. 2.5Gbit max.. And internal network is 1 Gbit too. So it'll take ) and cost) more than just changing my subscription. Also my TV is still 1080p lol reply TechDebtDevin 20 hours agorootparentprevIs Australia's ISP infrastructure nationalized? reply jiggawatts 19 hours agorootparentIt's a long story featuring nasty partisan politics, corrupt incumbents, Rupert Murdoch, and agile upstarts doing stealth rollouts at the crack of dawn. Basically, the old copper lines were replaced by the NBN, which is a government-owned corporation that sells wholesale networking to telcos. Essentially, the government has a monopoly, providing the last-mile fibre links. They use nested VLANs to provide layer-2 access to the consumer telcos. Where it got complicated was that the right-wing government was in the pocket of Rupert Murdoch, who threatened them with negative press before an upcoming election. They bent over and grabbed their ankles like the good little Christian school boys they are, and torpedoed the NBN network technology to protect the incumbent Fox cable network. Instead of fibre going to all premises, the NBN ended up with a mix of technologies, most of which don't scale to gigabit. It also took longer and cost more, despite the government responsible saying they were making these cuts to \"save taxpayer money\". Also for political reasons, they were rolling it out starting at the sparse rural areas and leaving the high-density CBD regions till last. This made it look bad, because if they spent $40K digging up the long rural dirt roads to every individual farmhouse, it obviously won't have much of a return on the taxpayer's investment... like it would have if deployed to areas with technology companies and their staff. Some existing smaller telcos noticed that there was a loophole in the regulation that allowed them to connect the more lucrative tech-savvy customers to their own private fibre if it's within 2km of an existing line. Companies like TPG had the entire CBD and inner suburban regions of every major city already 100% covered by this radius, so they proceeded to leapfrog the NBN and roll out their own 100 Mbps fibre-to-the-building service half a decade ahead. I saw their unmarked white vans stealthily rolling out extra fibre at like 3am to extend their coverage area before anyone in the government noticed. The funny part was that FttB uses VDSL2 boxes in the basement for the last 100m going up to apartments, but you can only have one per building because they use active cross-talk cancellation. So by the time the NBN eventually got around to wiring the CBD regions, they got to the apartments to discover that \"oops, too late\", private telcos had gotten there first! There were lawsuits... which the government lost. After all, they wrote the legislation, they were just mad that they hadn't actually understood it. Meanwhile, some other incumbent fibre providers that should have disappeared persisted like a stubborn cockroach infestation. I've just moved to an apartment serviced by OptiComm, which has 1.1 out of 5 stars on Google... which should tell you something. They even have a grey fibre box that looks identical to the NBNCo box except it's labelled LBNCo with the same font so that during a whirlwind apartment inspection you might not notice that you're not going to be on the same high-speed Internet as the rest of the country. reply dbaggerman 18 hours agorootparentTo clarify, NBN is a monopoly on the last mile infrastructure which is resold to private ISPs that sell internet services. The history there is that Australia used to have a government run monopoly on telephone infrastructure and services (Telecom Australia), which was later privatised (and rebranded to Telstra). The privatisation left Telstra with a monopoly on the infrastructure, but also a requirement that they resell the last mile at a reasonable rate to allow for some competition. So Australia already had an existing industry of ISPs that were already buying last mile access from someone else. The NBN was just a continuation of the existing status quo in that regard. > They even have a grey fibre box that looks identical to the NBNCo box except it's labelled LBNCo with the same font Early in my career I worked for one of those smaller telcos trying to race to get services into buildings before the NBN. I left around the time they were talking about introducing an LBNCo brand (only one of the reasons I left). At the time, they weren't part of Opticomm, but did partner with them in a few locations. If the brand is still around, I guess they must have been acquired at some point. reply jiggawatts 15 hours agorootparentI heard from several sources that what they do is give the apartment builder a paper bag of cash in exchange for the right to use their wires instead of the NBN. Then they gouge the users with higher monthly fees. reply dbaggerman 14 hours agorootparentWhen I was there NBNCo hadn't really moved into the inner city yet. We did have some kind of financial agreement with the building developer/management to install our VDSL DSLAMs in their comms room. It wouldn't surprise me if those payments got shadier and more aggressive as the NBN coverage increased. reply TechDebtDevin 8 hours agorootparentprevThanks for the response! Very interesting. Unfortunately the USA is a tumor on this planet. Born and Raised, this place is fucked and slowly fucking the whole world. reply oasisaimlessly 4 hours agorootparentThis is about Australia, not the USA. reply nh2 19 hours agoparentprevIn Switzerland you get 25 Gbit/s for $60/month. In 30 years it will be even faster. It would be silly to have to use older protocols to get line speed. reply 77pt77 19 hours agorootparentNow do the same in Germany... reply wkat4242 19 hours agoparentprevFor local purposes that's certainly true. It seems that quic trades a faster connection establishment for lower throughput. I personally prefer tcp anyway. reply nine_k 19 hours agoparentprevGigabit connections are widely available in urban areas. The problem is not theoretical, but definitely is pretty recent / nascent. reply Dylan16807 19 hours agorootparentA gigabit connection is just one prerequisite. The server also has to be sending very big bursts of foreground/immediate data or you're very unlikely to notice anything. reply Fire-Dragon-DoL 21 hours agoparentprevThat is interesting though. 1gbit is becoming more common reply schmidtleonard 20 hours agorootparentIt's wild that 1gbit LAN has been \"standard\" for so long that the internet caught up. Meanwhile, low-end computers ship with a dozen 10+Gbit class transceivers on USB, HDMI, Displayport, pretty much any external port except for ethernet, and twice that many on the PCIe backbone. But 10Gbit ethernet is still priced like it's made from unicorn blood. reply Aurornis 19 hours agorootparent> Meanwhile, low-end computers ship with a dozen 10+Gbit class transceivers on USB, HDMI, Displayport, pretty much any external port except for ethernet, and twice that many on the PCIe backbone. But 10Gbit ethernet is still priced like it's made from unicorn blood. You really can’t think of any major difference between 10G Ethernet and all of those other standards that might be responsible for the price difference? Look at the supported lengths and cables. 10G Ethernet over copper can go an order of magnitude farther over relatively generic cables. Your USB-C or HDMI connections cannot go nearly as far and require significantly more tightly controlled cables and shielding. That’s the difference. It’s not easy to accomplish what they did with 10G Ethernet over copper. They used a long list of tricks to squeeze every possible dB of SNR out of those cables. You pay for it with extremely complex transceivers that require significant die area and a laundry list of complex algorithms. reply schmidtleonard 19 hours agorootparentThere was a time when FFE, DFE, CTLE, and FEC could reasonably be considered an extremely complex bag of tricks by the standards of the competition. That time passed many years ago. They've been table stakes for a while in every other serial standard. Wifi is beating ethernet at the low end, ffs, and you can't tell me that air is a kinder channel. A low-end PC will ship with a dozen transceivers implementing all of these tricks sitting idle, while it'll be lucky to have a single 2.5Gbe port and you'll have to pay extra for the privilege. No matter, eventually USB4NET will work out of the box. The USB-IF is a clown show and they have tripped over their shoelaces every step of the way, but consumer Ethernet hasn't moved in 20 years so this horse race still has a clear favorite, lol. reply reshlo 17 hours agorootparentprevYou explained why 10G Ethernet cables are expensive, but why should it be so expensive to put a 10G-capable port on the computer compared to the other ports? reply kccqzy 16 hours agorootparentDid you completely misunderstand OP? The 10G Ethernet cables are not expensive. In a pinch, even your Cat 5e cable is capable of 10G Ethernet albeit at a shorter distance than Cat 6 cable. Even then, it can be at least a dozen times longer than a similar USB or HDMI or DisplayPort cable. reply reshlo 13 hours agorootparentI did misunderstand it, because looking at it again now, they spent the entire post talking about how difficult it is to make the cables, except for the very last sentence where they mention die area one time, and it’s still not clear that they’re talking about die area for something that’s inside the computer rather than a chip that goes in the cable. > Look at the supported lengths and cables. … relatively generic cables. Your USB-C or HDMI connections cannot go nearly as far and require significantly more tightly controlled cables and shielding. … They used a long list of tricks to squeeze every possible dB of SNR out of those cables. reply chgs 10 hours agorootparentTheir point was those systems like hdmi, bits of usb-c etc put the complexity is very expensive very short cables. Meanwhile a 10g port on my home router will run over copper for far longer. Not that I’m a fan given the power use, fibre is much easier to deal with and will run for miles. reply jsheard 20 hours agorootparentprevThose very fast consumer interconnects are distinguished from ethernet by very limited cable lengths though, none of them are going to push 10gbps over tens of meters nevermind a hundred. DisplayPort is up to 80gbps now but in that mode it can barely even cross 1.5m of heavily shielded copper before the signal dies. In a perfect world we would start using fiber in consumer products that need to move that much bandwidth, but I think the standards bodies don't trust consumers with bend radiuses and dust management so instead we keep inventing new ways to torture copper wires. reply crote 19 hours agorootparent> In a perfect world we would start using fiber in consumer products that need to move that much bandwidth We are already doing this. USB-C is explicitly designed to allow for cables with active electronics, including conversion to & from fiber. You could just buy an optical USB-C cable off Amazon, if you wanted to. reply Dylan16807 17 hours agorootparentWhen you make the cable do the conversion, you go from two expensive transceivers to six expensive transceivers. And if the cable breaks you need to throw out four of them. It's a poor replacement for direct fiber use. reply schmidtleonard 19 hours agorootparentprevSure you need fiber for long runs at ultra bandwidth, but short runs are common and fiber is not a good reason for DAC to be expensive. Not within an order of magnitude of where it is. reply Dylan16807 17 hours agorootparentThese days, passive cables that support ultra bandwidth are down to like .5 meters. For anything that wants 10Gbps lanes or less, copper is fine. For ultra bandwidth, going fiber-only is a tempting idea. reply michaelt 20 hours agorootparentprevAgree that a widespread faster ethernet is long overdue. But bear in mind, standards like USB4 only support very short cables. It's impressive that USB4 can offer 40 Gbps - but it can only do so on 1m cables. On the other hand, 10 gigabit ethernet claims to go 100m on CAT6A. reply crote 19 hours agorootparentUSB4 does support longer distances, but those cables need active electronics to guarantee signal integrity. That's how you end up with Apple's $160 3-meter cable. reply chgs 10 hours agorootparentA 3m 100g dac is 1/3 the price reply nijave 20 hours agorootparentprev2.5Gbps is becoming pretty common and fairly affordable, though My understanding is right around 10Gbps you start to hit limitations with the shielding/type of cable and power needed to transmit/send over Ethernet. When I was looking to upgrade at home, I had to get expensive PoE+ injectors and splitters to power the switch in the closet (where there's no outlet) and 10Gbps SFP+ transceivers are like $10 for fiber or $40 for Ethernet. The Ethernet transceivers hit like 40-50C reply crote 19 hours agorootparentThe main issue is switches, really. 5Gbps USB NICs are available for $30 on Amazon, or $20 on AliExpress. 10Gbps NICS are $60, so not exactly crazy expensive either. But switches haven't really kept up. A simple unmanaged 5-port or 8-port 2.5GigE isn't too bad, but anything beyond that gets tricky. 5GigE switches don't seem to exist, and you're already paying $500 for a budget-brand 10GigE switch with basic VLAN support. You want PoE? Forget it. The irony is that at 10Gbps fiber suddenly becomes quite attractive. A brand-new SFP+ NIC can be found for $30, with DACs only $5 (per side) and transceivers $30 or so. You can get an actually-decent switch from Mikrotik for less than $300. Heck, you can even get brand-new dualport SFP28 NICs for $100, or as little as $25 on Ebay! Switch-wise you can get 16 ports of 25Gbps out of a $800 Mikrotik switch: not exactly cheap, but definitely within range for a very enthusiastic homelabber. The only issue is that wiring your home for fiber is stupidly expensive, and you can't exactly use it to power access points either. reply spockz 15 hours agorootparentApparently there is the https://store.ui.com/us/en/products/us-xg-6poe from Ubiquity. It only has 4 10GbE ports but they all have PoE. reply maccard 18 hours agorootparentprev> The only issue is that wiring your home for fiber is stupidly expensive What do you mean by that? My home isnt wired for ethernet. I can buy 30m of CAT6 cable for £7, or 30m of fibre for £17. For a home use, that's a decent amount of cable, and even spending £100 on cabling will likely run cables to even the biggest of houses. reply hakfoo 15 hours agorootparentIsn't the expensive part more the assembly aspect? For Cat 6 the plugs and keystone jacks add up to a few dollars per port, and the crimper is like $20. I understand building your own fibre cables-- if you don't want to thread them through walls without the heads pre-attached, for example-- involves more sophisticated glass-fusion tools that are fairly expensive. A rental service might help there, or a call-in service-- the 6 hours of drilling holes and pulling fibre can be done by yourself, and once it's all cut to rough length, bring out a guy who can fuse on 10 plugs in an hour for $150. reply maccard 9 hours agorootparentThanks - I genuinely didn't know. I assumed that you could \"just\" crimp it like CAT6, but a quick google leads me to spending quite a few hundred pounds on something like this[0]. That said; > A rental service might help there, or a call-in service-- the 6 hours of drilling holes and pulling fibre can be done by yourself, and once it's all cut to rough length, bring out a guy who can fuse on 10 plugs in an hour for $150. If you were paying someone to do it (rather than DIY) I'd wager the cost would be similar, as you're paying them for 6 hours of labour either way. [0] https://www.cablemonkey.co.uk/fibre-optic-tool-kits-accessor... reply Dylan16807 14 hours agorootparentprevIf you particularly want to use a raw spool, then yes that's an annoying cost. If you buy premade cables for an extra $5 each then it's fine. reply hakfoo 13 hours agorootparentA practical drawback to premade cables is the need for a larger hole to accommodate the pre-attached connector. There's also a larger gap that needs to be plugged around the cable to prevent leaks into he wall. My ordinary home-centre electric drill and an affordable ~7mm masonry bit lets me drill a hole in stucco large enough to accept bare cables with a very narrow gap to worry about. reply inferiorhuman 12 hours agorootparentprevWhere are you finding them for that cheap? OP is talking about 20GBP for a run of fiber. If I look at, for instance, Ubiquiti their direct attach cables start at $13 for 0.5 meter cables. reply Dylan16807 12 hours agorootparentI was looking at patch cables. Ubiquiti's start at $4.80 reply chgs 10 hours agorootparentprevMy single mode keystones pass through were about the same price as cat5, and pre-made cables were no harder to run than un terminated cat5. reply cyberax 20 hours agorootparentprev40-50C? What is the brand? Mine were over 90C, resulting in thermal shutdowns. I had to add an improvised heat exchanger to lower it down to ~70C: https://pics.archie.alex.net/share/U0G1yiWzShqOGXulwe1AetDjR... reply akira2501 20 hours agorootparentprevIronically.. 2.5 Gbps is created by taking a 10GBASE-T module and effectively underclocking it. I wonder if \"automatic speed selection\" is around the corner with modules that automatically connect at 100Mbps to 10Gbps based on available cable quality. reply cyberax 19 hours agorootparentMy 10G modules automatically drop down to 2.5G or 1G if the cable is not good enough. There's also 5G, but I have never seen it work better than 2.5G. reply chgs 10 hours agorootparentI don’t think my 10g coppers will drop to 10m. 100m sure, but 10m rings a bell. reply akira2501 18 hours agorootparentprevOh man. I've been off the IT floor for too long. Time to change my rhetoric, ya'll have been around the corner for a while. Aging has it's upsides and downsides I guess. reply Dylan16807 17 hours agorootparentprev> My understanding is right around 10Gbps you start to hit limitations with the shielding/type of cable and power needed to transmit/send over Ethernet. If you decide you only need 50 meters, that reduces both power and cable requirements by a lot. Did we decide to ignore the easy solution in favor of stagnation? reply Fire-Dragon-DoL 19 hours agorootparentprevIt passed it! Here there are offers up to 3gbit residential (Vancouver). I had 1.5 bit for a while. Downgraded to 1gbit because while I love fast internet, right now nobody in the home uses it enough to affect 1gbit speed reply Dalewyn 17 hours agorootparentprevThere is an argument to be made that gigabit ethernet is \"good enough\" for Joe Average. Gigabit ethernet is ~100MB/s transfer speed over copper wire or ~30MB/s over wireless accounting for overhead and degradation. That is more than fast enough for most people. 10gbit is seemingly made from unicorn blood and 2.5gbit is seeing limited adoption because there simply isn't demand for them outside of enterprise who have lots of unicorn blood in their banks. reply paulddraper 18 hours agoparentprev> below 600 Mbit/s is implied as being \"Slow Internet\" in the intro Or rather, not \"Fast Internet\" reply lysace 17 hours agorootparentYeah. reply Tempest1981 21 hours agoprevFrom September: QUIC is not quick enough over fast internet (acm.org) https://news.ycombinator.com/item?id=41484991 (327 comments) reply lysace 20 hours agoparentMy personal takeaway from that: Perhaps we shouldn't let Google design and more or less unilaterally dictate and enforce internet protocol usage via Chromium. Brave/Vivaldi/Opera/etc: You should make a conscious choice. reply ratorx 20 hours agorootparentHaving read through that thread, most of the (top) comments are somewhat related to the lacking performance of the UDP/QUIC stack and thoughts on the meaningfulness of the speeds in the test. There is a single comment suggesting HTTP/2 was rushed (because server push was later deprecated). QUIC is also acknowledged as being quite different from the Google version, and incorporating input from many different people. Could you expand more on why this seems like evidence that Google unilaterally dictating bad standards? None of the changes in protocol seem objectively wrong (except possibly Server Push). Disclaimer: Work at Google on networking, but unrelated to QUIC and other protocol level stuff. reply lysace 20 hours agorootparent> Could you expand more on why this seems like evidence that Google unilaterally dictating bad standards? I guess I'm just generally disgusted in the way Google is poisoning the web in the worst way possible: By pushing ever more complex standards. Imagine the complexity of the web stack in 2050 if we continue to let Google run things. It's Microsoft's old embrace-extend-and-extinguish scheme taken to the next level. In short: it's not you, it's your manager's manager's manager's manager's strategy that is messed up. reply ratorx 19 hours agorootparentThis is making a pretty big assumption that the web is perfectly fine the way it is and never needs to change. In reality, there are perfectly valid reasons that motivate QUIC and HTTP/2 and I don’t think there is a reasonable argument that they are objectively bad. Now, for your personal use case, it might not be worth it, but that’s a different argument. The standards are built for the majority. All systems have tradeoffs. Increased complexity is undesirable, but whether it is bad or not depends on the benefits. Just blanket making a statement that increasing complexity is bad, and the runaway effects of that in 2050 would be worse does not seem particularly useful. reply lysace 19 hours agorootparentNothing is perfect. But gigantic big bang changes (like from HTTP 1.1 to 2.0) enforced by a browser mono culture and a dominant company with several thousands of individually well-meaning Chromium software engineers like yourself - yeah, pretty sure that's bad. reply jsnell 19 hours agorootparentExcept that HTTP/1.1 to HTTP/2 was not a big bang change on the ecosystem level. No server or browser was forced to implement HTTP/2 to remain interoperable[0]. I bet you can't point any of this \"enforcement\" you claim happened. If other browser implemented HTTP/2, it was because they thought that the benefits of H2 outweighed any downsides. [0] There are non-browser protocols that are based on H2 only, but since your complaint was explicitly about browsers, I know that's not what you had in mind. reply lysace 19 hours agorootparentYou are missing the entire point: Complexity. It's not your fault, in case you were working on this. It was likely the result a strategy thing being decided at Google/Alphabet exec level. Several thousand very competent C++ software engineers don't come cheap. reply jsnell 18 hours agorootparentI mean, the reason I was discussing those specific aspects is that you're the one brought them up. You made the claim about how HTTP/2 was a \"big bang\" change. You're the one who made the claim that HTTP/2 was enforced on the ecosystem by Google. And it seems that you can't support either of those claims in any way. In fact, you're just pretending that you never made those comments at all, and have once again pivoted to a new grievance. But the new grievance is equally nonsensical. HTTP/2 is not particularly complex, and nobody on either the server or browser side was forced to implement it. Only those who thought the minimal complexity was worth it needed to do it. Everyone else remained fully interoperable. I'm not entirely sure where you're coming from here, to be honest. Like, is your belief that there are no possible tradeoffs here? Nothing can ever justify even such minor amounts of complexity, no matter how large the benefits are? Or do you accept that there are tradeoffs, and are \"just\" disagree with every developer who made a different call on this when choosing whether to support HTTP/2 in their (non-Google) browser or server? reply lysace 18 hours agorootparentEdit: this whole comment is incorrect. I was really thinking about HTTP 3.0, not 2.0. HTTP/2 is not \"particularly complex?\" Come on! Do remember where we started. > I'm not entirely sure where you're coming from here, to be honest. Like, is your belief that there are no possible tradeoffs here? Nothing can ever justify even such minor amounts of complexity, no matter how large the benefits are? Or do you accept that there are tradeoffs, and are \"just\" disagree with every developer who made a different call on this when choosing whether to support HTTP/2 in their (non-Google) browser or server? \"Such minor amounts of complexity\". Ahem. I believe there are tradeoffs. I don't believe that HTTP/2 met that tradeoff between complexity vs benefit. I do believe it benefitted Google. reply jsnell 17 hours agorootparent\"We\" started from you making outlandish claims about HTTP/2 and immediately pivoting to a new complaint when rebutted rather than admit you were wrong. Yes, HTTP/2 is not really complex as far as these things go. You just keep making that assertion as if it was self-evident, but it isn't. Like, can you maybe just name the parts you think are unnecessary complex? And then we can discuss just how complex they really are, and what the benefits are. (Like, sure, having header compression is more complicated than not having it. But it's also an amazingly beneficial tradeoff, so it can't be what you had in mind.) > I believe there are tradeoffs. I don't believe that HTTP/2 met that tradeoff between complexity vs benefit. So why did Firefox implement it? Safari? Basically all the production level web servers? Google didn't force them to do it. The developers of all of that software had agency, evaluated the tradeoffs, and decided it was worth implementing. What makes you a better judge of the tradoffs than all of these non-Google entities? reply lysace 17 hours agorootparentYeah, sorry, I mixed up 2.0 (the one that still uses TCP) with 3.0. Sorry for wasting your time. reply bawolff 19 hours agorootparentprev> It's Microsoft's old embrace-extend-and-extinguish scheme taken to the next level. It literally is not. reply lysace 19 hours agorootparentBecause? Edit: I'm not the first person to make this comparison. Witness the Chrome section in this article: https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguis... reply bawolff 19 hours agorootparentWell it may be possible to make the comparison in other things google does (they have done a lot of things) it makes no sense for quic/http3. What are they extending in this analogy? Http3 is not an extension of http. What are they extinguishing? There is no plan to get rid of http1/2, since you still need it in lots of networks that dont allow udp. Additionally, its an open standard, with an rfc, and multiple competing implementations (including firefox and i believe experimental in safari). The entire point of embrace, extend, extinguish is that the extension is not well specified making it dufficult for competitors to implement. That is simply not what is happening here. reply lysace 19 hours agorootparentWhat I meant with Microsoft's Embrace, extend, and extinguish (EEE) scheme taken to the next level is what Google has done to the web via Chromium: They have several thousand C++ browser engineers (and as many web standards people as they could get their hands on, early on). Combined with a dominant browser market share, this has let them dominate browser standards, and even internet protocols. They have abused this dominant position to eliminate all competitors except Apple and (so far) Mozilla. It's quite clever. reply bawolff 11 hours agorootparent> They have abused this dominant position to eliminate all competitors except Apple and (so far) Mozilla. But that's like all of them. Except edge but that was mostly dead before chrome came on the scene. It seems like you are using embrace, extend, extinguish to just mean, \"be succesful\", but that's not what the term means. Being a market leader is not the same thing as embrace, extend, extinguish. Neither is putting competition out of business. reply Dylan16807 17 hours agorootparentprev> What I meant with Microsoft's Embrace, extend, and extinguish (EEE) scheme taken to the next level is what Google has done to the web via Chromium I think this argument is reasonable, but QUIC isn't part of the problem. reply jauntywundrkind 18 hours agorootparentprevMicrosoft just did shit, whatever they wanted. Google has worked with all the w3c committees and other browsers with tireless commitment to participation, with endless review. It's such a tired sad trope of people disaffected with the web because they can't implement it by themselves easily. I'm so exhausted by this anti-progress terrorism; the world's shared hypermedia should be rich and capable. We also see lots of strong progress these days from newcomers like Ladybird, and Servo seems gearing up to be more browser like. reply lysace 18 hours agorootparentYes, Google found the loophole: brute-force standards complexity by hiring thousands of very competent engineers eager to leave their mark on the web and eager to get promoted. The only thing they needed was lots of money, and they had just that. I think my message here is only hard to understand if your salary (or personal worth etc) depends on not understanding it. It's really not that complex. reply bawolff 11 hours agorootparent> I think my message here is only hard to understand if your salary (or personal worth etc) depends on not understanding it. It's really not that complex. Just because someone disagrees with you, doesn't mean they don't understand you. However, if you think google is making standards unneccessarily complex, you should read some of the standards from the 2000s (e.g. SAML). reply ratorx 19 hours agorootparentprevContributing to an open standard seems to be the opposite of the classic example. Assume that change X for the web is positive overall. Currently Google’s strategy is to implement in Chrome and collect data on usefulness, then propose a standard and have other people contribute to it. That approach seems pretty optimal. How else would you do it? reply lysace 19 hours agorootparentnext [3 more] [flagged] ratorx 19 hours agorootparentHow does this have any relevance to my comment? reply lysace 19 hours agorootparentHow does your comment have any relevance to what we are discussing throughout this thread? reply yunohn 9 hours agorootparentprevThis is one of those HN buzzword medley comments that has only rant, no substance. - MS embrace extend extinguish - Google is making the world complex - Nth level manager is messed up None of the above was connected to deliver a clear point, just thrusted into the comment to sound profound. reply GuB-42 20 hours agorootparentprevMaybe, but QUIC is not bad as a protocol. The problem here is that OSes are not as well optimized for QUIC as they are for TCP. Just give it time, the paper even has suggestions. QUIC has some debatable properties, like mandatory encryption, or the use of UDP instead of being a protocol under IP like TCP, but there are good reasons for it, related to ossification. Yes, Google pushed for it, but I think it deserves its approval as a standard. It is not perfect but it is practical, they don't want another IPv6 situation. reply vlovich123 20 hours agorootparentprevSo because the Linux kernel isn’t as optimized for QUIC as it has been for TCP we shouldn’t design new protocols? Or it should be restricted to academics that had tried and failed for decades and would have had all the same problems even if they succeeded? And all of this only in a data center environment really and less about the general internet Quic was designed for? This is an interesting hot take. reply lysace 20 hours agorootparentI'm struggling to parse my comment in the way you seem to think it did. In what way did or would my comment restrict your ability to design new protocols? Please explain. reply vlovich123 17 hours agorootparentBecause you imply in that comment that it should be someone other than Google developing new protocols while in another you say that the protocols are already too complex implying stasis is the preferred state. You’re also factually incorrect in a number of ways such as claiming that HTTP/2 was a Google project (it’s not and some of the poorly thought out ideas like push didn’t come from Google). The fact of the matter is that other attempts at “next gen” protocols had taken place. Google is the only one that won out. Part of it is because they were one of the few properties that controlled enough web traffic to try something. Another is that they explicitly learned from mistakes that the academics had been doing and taken market effects into account (ie not requiring SW updates of middleware boxes). I’d say all things considered Internet connectivity is better that QUIC got standardized. Papers like this simply point to current inefficiencies of today’s implementation - those can be fixed. These aren’t intractable design flaws of the protocol itself. But you seem to really hate Google as a starting point so that seems to color your opinion of anything they produce rather than engaging with the technical material in good faith. reply lysace 17 hours agorootparentI don't hate Google. I admire it what for what it is; an extremely efficient and inherently scalable corporate structure designed to exploit the Internet and the web in the most brutal and profitable way imaginable. It's just that their interests in certain aspects don't align with ours. reply chgs 10 hours agoparentprevQUIC is all about an advertising company guarenteeing delivery of adverts to the consumer. As long as the adverts arrive quickly the rest is immaterial. reply kachapopopow 20 hours agoprevThis sounds really really wrong. I've achieved 900mbps speeds on quic+http3 and just quic... Seems like a bad TLS implementation? Early implementation that's not efficient? The CPU usage seemed pretty avg at around 5% on gen 2 epyc cores. reply kachapopopow 20 hours agoparentThis is actually very well known: current QUIC implementation in browsers is *not stable* and is built of either rustls or in another similar hacky way. reply vasilvv 12 hours agorootparentI'm not sure where rustls comes from -- Chrome uses BoringSSL, and last time I checked, Mozilla implementation used NSS. reply AlienRobot 15 hours agorootparentprevWhy am I beta testing unstable software? reply stouset 35 minutes agorootparentYou’re the one choosing to use it. reply FridgeSeal 13 hours agorootparentprevBecause Google puts whatever they want in their browser for you to beta test and you’ll be pleased about it, peasant /s. reply lbriner 4 hours agoprevFunny though, we all implicitly buy into \"QUIC is the new http/2\" or whatever because fast = good without really understanding the details. It's like buying the new 5G cell phone because it is X times faster than 4G even though 1) My 4G phone never actually ran at the full 4G speed and 2) The problem with any connection is almost never due to the line speed of my internet connection but a misbehaving DNS server/target website/connection Mux at my broadband provider. \"But it's 5G\" Same thing cracks me up when people advertise \"fibre broadband\" for internet by showing people watching the TV like the wind is blowing in their hair, because that's how it works (not!). I used to stream on my 8Mb connection so 300Mb might be good for some things but I doubt I would notice much difference. reply spott 21 hours agoprevHere “fast internet” is 500Mbps, and the reason is that quic seems to be cpu bound above that. I didn’t look closely enough to see what their test system was to see if this is basic consumer systems or is still a problem for high performance desktops. reply jpambrun 5 hours agoprevThis paper seems to be neglecting to the effect of latency and packet loss. From my understanding, the biggest issue with TCP is the window sizing that gets cut every time a packet gets lost or arrives out of order, thus killing throughput. The latency makes that more likely to happen and makes the effect last longer. This paper needs multiple latency simulations, some packet loss and latency jitter to have any value. reply dgacmu 5 hours agoparentThis is a bit of a misunderstanding. A single out of order packet will not cause a reduction; tcp uses three duplicate acks as a loss signal. So the packet must have been reordered to arrive after 3 later packets. Latency does not increase the chances of out of order packet arrival. Out of order packet arrival is usually caused by multipath or the equivalent inside a router if packets are handled by different stream processors (or the equivalent). Most routers and networks are designed to keep packets within a flow together to avoid exactly this problem. However, it is fair to say that traversing more links and routers probably increases the chance of out of order packet delivery, so there's a correlation in some way with latency, but it's not really about the latency itself - you can get the same thing in a data center network. reply p1necone 20 hours agoprevI thought QUIC was optimized for latency - loading lots of little things at once on webpages and video games (which send lots of tiny little packets - low overall throughput but highly latency senstive) and such. I'm not surprised that it falls short when overall throughput is the only thing being measured. I wonder if this can be optimized at the protocol level by detecting usage patterns that look like large file transfers or very high bandwidth video streaming and swapping over to something less cpu intensive. Or is this just a case of less hardware/OS level optimization of QUIC vs TCP because it's new? reply zamalek 20 hours agoparentIt seems that syscalls might be the culprit (ACKs occur completely inside the kernel for TCP, where anything UDP acks from userspace). I wonder if BGP could be extended for protocol development. reply LittleOtter 5 hours agoprevThis paper has been shown one month ago:https://news.ycombinator.com/item?id=41484991. Now it's back to headlines of HN.Seems like people all interested in this topic. reply exabrial 21 hours agoprevI wish QUIC had a non-TLS mode... if I'm developing locally I really just want to see whats going over the wire sometimes and this adds a lot of un-needed friction. reply guidedlight 20 hours agoparentQUIC reuses parts of the TLS specification (e.g. handshake, transport state, etc). So it can’t function without it. reply krater23 21 hours agoparentprevYou can add the private key of your server in wireshark and it will automatically decrypt the packets. reply jborean93 19 hours agorootparentThis only works tor RSA keys and I believe ciphers that do not have forward secrecy. Quic is TLS 1.3 and all the ciphers in that protocol do forward secrecy so cannot be decrypted in this way. You’ll have to use a tool that provides the TLS session info through the SSLKEYLOGFILE format. reply giuscri 1 hour agorootparentLike which one? reply Thaxll 18 hours agoprevQUIC is pretty much what serious online games have been doing in the last 20 years. reply teleforce 17 hours agoprevPrevious post on HN (326 comments - 40 days ago): QUIC is not quick enough over fast internet: https://news.ycombinator.com/item?id=41484991 reply ec109685 18 hours agoprevMeanwhile fast.com (and presumably netflix cdn) is using http 1.1 still. reply dan-robertson 18 hours agoparentWhy do you need multiplexing when you are only downloading one (video) stream? Are there any features of http/2 that would benefit the Netflix use case? reply jeltz 18 hours agorootparentQUIC handles packet loss better. But I do not think there is any benefit from HTTP2. reply dan-robertson 7 hours agorootparentYeah I was thinking the same thing – in some video contexts with some video codecs you may care more about latency and may be able to get a video codec that can cope with packet loss instead of requiring retransmission – except it seemed it wouldn’t apply too much to Netflix where the latency requirement is lower and so retransmission ought to be fine. Maybe one advantage of HTTP/3 would be handling ip changes but I’m not sure this matters much because you can already resume downloads fine in HTTP/1.1 if the server supports range requests (which it very likely does for video) reply skybrian 20 hours agoprevLooking at Figure 5, Chrome tops out at ~500 Mbps due to CPU usage. I don't think many people care about these speeds? Perhaps not using all available bandwidth for a few speedy clients is an okay compromise for most websites? This inadvertent throttling might improve others' experiences. But then again, being CPU-throttled isn't great for battery life, so perhaps there's a better way. reply jeroenhd 11 hours agoparentThese caps are a massive pain when downloading large games or OS upgrades for me as the end user. 500mbps is still fast but for a new protocol looking to replace older protocols, it's a big downside. I don't really benefit much from http/3 or QUIC (I don't live in a remote area or host a cloud server) so I've already considered disabling either. A bandwidth cap this low makes a bigger impact than the tiny latency improvements. reply 10000truths 20 hours agoprevTL;DR: Nothing that's inherent to QUIC itself, it's just that current QUIC implementations are CPU-bound because hardware GRO support has not yet matured in commodity NICs. But throughput was never the compelling aspect of QUIC in the first place. It was always the reduced latency. A 1-RTT handshake including key/cert exchange is nothing to scoff at, and the 2-RTT request/response cycle that HTTP/3-over-QUIC offers means that I can load a blog page from a rinky-dink server on the other side of the world inThere's also the fact that TCP has an unfixable security flaw - any random middleware can inject data (without needing to block packets) and break the connection I am unsure how this is a security flaw of TCP? Any middleman could block UDP packets too and get the same effect, or modify UDP packets in an attempt to cause the receiving application to crash. reply o11c 18 hours agorootparentIn order to attack UDP, you have to block all routes through which traffic might flow. This is hard; remember, the internet tries to be resilient. In order to attack TCP, all you have to do is spy on a single packet (very easy) to learn the sequence number, then you can inject a wrench into the cogs and the endpoints will reject all legitimate traffic from each other. reply jeroenhd 11 hours agorootparentThat's only true if you use the kernel TCP stack. You can replicate the slow QUIC stack and do everything in user mode to get control back over what packets you accept (i.e. reject any that don't fit your TLS stream). reply suprjami 19 hours agorootparentprevWhat does that have to do with anything here? This post is about QUIC performance, not TCP packet injection. reply o11c 19 hours agorootparent\"Accept worse performance in order to fix security problems\" is a standard tradeoff. reply suprjami 17 hours agorootparentQUIC was invented to provide better performance for multiplexed HTTP/3 streams and the bufferbloat people love that it avoids middlebox protocol interference. QUIC has never been about \"worse performance\" to avoid TCP packet injection. Anybody who cares about TCP packet injection is using crypto (IPSec/Wireguard). If performant crypto is needed there are appliances which do it at wirespeed. reply andsoitis 19 hours agoprevDesigning for resource-constrained systems typically comes with making tradeoffs. Once the resource constraint is eliminared, you're no longer getting the benefit of that tradeoff but are paying the costs. reply jvanderbot 20 hours agoprevWell latency/bandwidth tradeoffs make sense. After bufferbloat mitigations my throughout halved on my router. But for gaming while everyone is streaming, it makes sense to settle with half a gigabit. reply kibwen 19 hours agoprevHow does it compare to HTTP/1 on similar benchmarks? reply AlienRobot 15 hours agoprevAnecdote: I was having trouble accessing wordpress.org. When I started using Wordpress, I could access the documentation just fine, but then suddenly I couldn't access the website anymore. I dual boot Linux, so it wasn't Windows fault. I could ping them just fine. I tried three different browsers with the same issue. It's just that when I accessed the website, it would get stuck and not load at all, and sometimes pages would just stop loading mid-way. Today I found the solution. Disable \"Experimental QUIC Protocol\" in Chrome settings. This makes me kind of worried because I've had issues accessing wordpress.org for months. There was no indication that this was caused by QUIC. I just managed to realize it because there was QUIC-related error in devtools that appeared only sometimes. I wonder what other websites are rendered inaccessible by this protocol and users have no idea what is causing it. reply superkuh 20 hours agoprevSince QUIC was designed for Fast Internet as used by the megacorporations like Google and Microsoft how it performs at these scales does matter even if it doesn't for a human person's end. Without it's designed for use case all it does is slightly help mobile platforms that don't want to hold open a TCP connection (for energy use reasons) and bring in fragile \"CA TLS\"-only in an environment where cert lifetimes are trending down to single months (Apple etc latest proposal). reply dathinab 20 hours agoparentnot really it's (mainly) designed by companies like Google to connect to all their end users Such a internet connection becoming so low latency that the latency of receiver side processing becomes dominant is in practice not the most relevant. Sure theoretically you can hit it with e.g. 5G but in practice even with 5G many real world situations won't. Most importantly a slow down of such isn't necessary bad for Google and co. as it only add limited amounts on strain on their services, infrastructure, internet and is still fast enough for most users to not care for most Google and co. use cases. Similar being slow due to receiver delays isn't necessary bad enough to cause user noticeable battery issues, on of the main reasons seem to many userkernel boundary crossings which are slow due to cache missues/ejections etc. but also don't boost your CPU clock (which is one of the main ways to drain your battery, besides the screen) Also like the article mentions the main issue is sub optimal network stack usage in browsers (including Chrome) not necessary a fundamental issue in the protocol. Which brings us to inter service communication for Google and co. which doesn't use any of the tested network stacks but very highly optimized stacks. I mean it really would be surprising if such network stacks where slow as there had been exhaustive perf. testing during the design of QUIC. reply austin-cheney 20 hours agoprev [–] EDITED. I preference WebSockets over anything analogous to HTTP. Commented edited because I mentioned performance conditions. Software developers tend to make unfounded assumptions/rebuttals of performance conditions they have not tested. reply sleepydog 19 hours agoparentQUIC is a reliable transport. It's not \"fire and forget\", there is a mechanism for recovering lost messages similar, but slightly superior to TCP. QUIC has the significant advantage of 0- and 1-rtt connection establishments which can hide latency better than TCP's 3-way handshake. Current implementations have some disadvantages to TCP, but they are not inherent to the protocol, they just highlight the decades of work done to make TCP scale with network hardware. Your points seem better directed at HTTP/3 than QUIC. reply akira2501 20 hours agoparentprevI'd use them more, but WebSockets are just unfortunately a little too hard to implement efficiently in a serverless environment, I wish there was a protocol that spoke to that environment's tradeoffs more effectively. The current crop aside from WebSockets all seem to be born from taking a butcher knife to HTTP and hacking out everything that gets in the way of time to first byte. I don't think that's likely to produce anything worthwhile. reply austin-cheney 20 hours agorootparentThat is a fair point. I wrote my own implementation of WebSockets in JavaScript and learned much in doing so, but it took tremendous trial and effort to get right. Nonetheless, the result was well worth the effort. I have a means to communicate to the browser and between servers that is real time with freedom to extend and modify it at my choosing. It is unbelievably more responsive than reliance upon HTTP in any of its forms. Imagine being able to execute hundreds of end-to-end test automation scenarios in the browser in 10 seconds. I can do that, but I couldn't with HTTP. reply bawolff 19 hours agoparentprevThis is an insane take. Just to pick at one point of this craziness, you think that communicating over web sockets does not involve round trips???? reply austin-cheney 19 hours agorootparentThat is correct. reply Aurornis 20 hours agoparentprev> QUIC is faster than prior versions of HTTP, but its still HTTP. It will never be fast enough because its still HTTP: > * String headers > * round trips > * many sockets, there is additional overhead to socket creation, especially over TLS QUIC is a transport. HTTP can run on top of QUIC, but the way you’re equating QUIC and HTTP doesn’t make sense. String headers and socket opening have nothing to do with the performance issues being discussed. String headers aren’t even a performance issue at all. The amount of processing done for when the most excessive use of string headers is completely trivial relative to all of the other processing that goes into sending 1,000,000,000 bits per second (Gigabit) over the internet, which is the order of magnitude target being discussed. I don’t think you understand what QUIC is or even the prior art in HTTP/2 that precedes these discussions of QUIC and HTTP/3. reply austin-cheney 19 hours agorootparent> String headers aren’t even a performance issue at all. That is universally incorrect. String instructions require parsing as strings are for humans and binary is for machines. There is performance overhead to string parsing always, and it is relatively trivial to perf. I have performance tested this in my own WebSocket and test automation applications. That performance difference scales in logarithmic fashion provided the quantity of messages to send/receive. I encourage you to run your own tests. reply jiggawatts 19 hours agorootparentBoth HTTP/2 and HTTP/3 use binary protocol encoding and compressed (binary) headers. You're arguing a straw man that has little to do with reality. reply quotemstr 20 hours agoparentprev> * String headers > * round trips > * many sockets, there is additional overhead to socket creation, especially over TLS > * UDP. Yes, in theory UDP is faster than TCP but only when you completely abandon integrity. Have you ever read up on the technical details of QUIC? Every single of one of your bullets reflects a misunderstanding of QUIC's design. reply Aurornis 20 hours agorootparentHonestly the entire comment is a head scratcher, from comparing QUIC to HTTP (different layers of the stack) or suggesting that string headers are a performance bottleneck. Websockets are useful in some cases where you need to upgrade an HTTP connection to something more. Some people learn about websockets and then try to apply them to everything, everywhere. This seems to be one of those cases. reply FridgeSeal 19 hours agoparentprev [–] QUIC isn’t HTTP, QUIC is a protocol that operates at a similar level to UDP and TCP. HTTP/3 is HTTP over QUIC. HTTP protocols v2 and onwards use binary headers. QUIC, by design, does 0-RTT handshakes. > Yes, in theory UDP is faster than TCP but only when you completely abandon integrity The point of QUIC, is that it enables application/userspace level reconstruction with UDP levels of performance. There’s no integrity being abandoned here: packets are free to arrive out of order, across independent sub-streams, and the protocol machinery puts them back together. QUIC also supports full bidirectional streams, so HTTP/3 also benefits from this directly. QUIC/HTTP3 also supports multiple streams per client with backpressure per substream. Web-sockets are a pretty limited special case, built on-top of HTTP and TCP. You literally form the http connection and then upgrade it to web-sockets, it’s still TCP underneath. Tl;Dr: your gripes are legitimate, but they refer to HTTP/1.1 at m",
    "originSummary": [
      "The study \"QUIC is not Quick Enough over Fast Internet\" reveals that QUIC's data rate can be up to 45.2% lower than the traditional TCP+TLS+HTTP/2 on high-speed networks.- The performance gap between QUIC and TCP+TLS+HTTP/2 increases with higher bandwidth, impacting file transfers, video streaming, and web browsing.- The paper identifies high receiver-side processing overhead as the root cause, due to excessive data packets and user-space acknowledgments (ACKs) in QUIC, and provides recommendations for improvement."
    ],
    "commentSummary": [
      "QUIC, a protocol aimed at enhancing internet speed, is encountering challenges with fast connections, where some implementations show lower data rates than traditional TCP (Transmission Control Protocol).",
      "The performance issues are attributed to current QUIC implementations being CPU-bound, especially in browsers, rather than flaws in the protocol itself.",
      "Despite offering advantages like reduced latency and improved packet loss management, QUIC's performance on high-speed connections is limited by existing hardware and software optimizations, illustrating the complexities in evolving internet protocols."
    ],
    "points": 285,
    "commentCount": 248,
    "retryCount": 0,
    "time": 1729371892
  },
  {
    "id": 41895718,
    "title": "Syncthing Android App Discontinued",
    "originLink": "https://forum.syncthing.net/t/discontinuing-syncthing-android/23002",
    "originBody": "imsodinSimonSyncthing Maintainer 9h Unfortunately I don’t have good news on the state of the android app: I am retiring it. The last release on Github and F-Droid will happen with the December 2024 Syncthing version. Reason is a combination of Google making Play publishing something between hard and impossible and no active maintenance. The app saw no significant development for a long time and without Play releases I do no longer see enough benefit and/or have enough motivation to keep up the ongoing maintenance an app requires even without doing much, if any, changes. Thanks a lot to everyone who ever contributed to this app! 12 2.9k views 1 link 6 users",
    "commentLink": "https://news.ycombinator.com/item?id=41895718",
    "commentBody": "Syncthing Android App Discontinued (syncthing.net)202 points by opengears 4 hours agohidepastfavorite106 comments jasonjayr 3 hours agoFrom the (current) final comment at https://github.com/syncthing/syncthing-android/issues/2064 > Nothing came of the discussions with google. Demands by Google for changes to get the permission granted were vague, which makes it both arduous to figure out how to address them and very unclear if whatever I do will actually lead to success. Then more unrelated work to stay on play came up (dev. verification, target API level), which among other influences finally made me realize I don't have the motivation or time anymore to play this game. reply izacus 2 hours agoparentI don't think Google was ever buy a \"I don't want to use file APIs because writing the code would be hard.\" excuse for a security issue. I don't know what kind of exact \"discussions\" were possible here for \"give me access to all user data, photos and everything because I don't think I want to use SAF APIs\". It's like that dude in your company that will have a meltdown in PRs over his better way instead of fixing the comments and having code submitted. Apple won't let you write into random directories past their APIs either, just because it would be too hard to use ObjC/Swift. reply swatcoder 56 minutes agorootparentThere's going to be loud, destructive friction when a 10-15 year old platform reduces the functionality available to its apps. Security models do need to evolve, but Android was introduced as a platform suitable for deep personal customization with few mandatory boundaries. This was a competitive distinction against Apple's closed \"safety first\" platform design in iOS and led to an ecosystem of applications that took advantage of all these extra possibilities. As Google tightens its grip over the platform and pursues more aggressive limitations for security reasons (and whatever other ones), it's inevitable that many publishers and users are going to be deeply frustrated when the features that made their device theirs are no longer available. (And incidentally, the restrictions on the Apple side have nothing to do with the application development language. I don't know where you would get that idea from or how to even make sense of it. It's just the nature of Apple's original design philosophy for iOS, where apps were deeply isolated and had no more capabilities than were necessary. Largely, Apple has been gradually adding capabilities to heavily-restricted apps over the lifetime of iOS, while Google has been moving in the opposite direction because they each started from opposite ends of the design space.) reply homebrewer 2 hours agorootparentprevPut a lot of scary warnings around it then. It's for the user to decide if they want to take the risk or not. Google took something that solved real problems better than any alternative could, did so for many years, and destroyed it for no real reason other than to further tighten control they have of the supposedly \"open\" platform. reply izacus 2 hours agorootparentThey did, the upstading app developers like this one just forced people to give them full access to all data in the app (or the app wouldn't run) and ended up not implementing scoped storage - something HNers outright demanded several times and exposed as a good upside of iOS. So stick had to come out. The full filesystem access is now reserved for apps that manage full filesystem (e.g. file explorers) and that's it. Scoped storage APIs were introduced in 2013, 11 years ago and Play started enforcing them in 2020, so the experiment with scary warnings was running for 7 years and developers refused to give up on that sweet full private file access. Granted, SAF is quite a shitty API. reply smashed 1 hour agorootparentIt's my phone. It's my data. It's my choice to install the app. It's my choice to grant the permissions to all files. Because guess what, I'm using the app to sync all my files. I really can't agree with Google in this particular case. reply sdoering 1 hour agorootparentI couldn't agree more. Given how much frigging hoops I had to jump through to get my Obsidian over syncthing setup to sync with my company iPhone - I nearly gave up. I grew up when computers didn't babysit me and tried to act like the good old GDR, knowing every thing better than their citicens. Nowadays, I feel more and more hindered by computers, not enabled. Computers used to be a production device (I could create things with them). Phones are not a computer - phones are a just \"consume like we want you to\" device. The problem is, I want my phone to be a creation device. A device that allows me to create content, text, to do lists, shopping lists, ideas and store them. And(!) sync them using the tools I decide to use. And not force me to use tools I friggin hate, because they just don't get the job done. reply skydhash 7 minutes agorootparentI gave up. My phone now is just a communication and utility device, and thus I don’t feel the urge to upgrade until it can’t do those tasks. I went back to computers (and Linux) to be able to just use them as a computer. Philpax 1 hour agorootparentprevHow did you get iPhone Syncthing + Obsidian working? I was under the impression that it was basically impossible to share Möbius Sync's directory with Obsidian. reply aborsy 14 minutes agorootparentThere is a new app Synctrain which does this. reply maxboone 1 hour agorootparentprevThat's why you can install through F-Droid right? reply izacus 1 hour agorootparentprevAnd yet you'll blame Android when some app steals a lot of data just like it always happens on this site. reply swatcoder 29 minutes agorootparentHave you considered that it's a plural \"you\" that you're choosing to pit yourself against, with different people each weighing different complaints? Almost by definition, the people who argue strongly for free use of their hardware and software are almost never the same people who argue strongly for safety and security restrictions. You seem to be frustrated by a contradiction or inconsistency that doesn't exist. It's true that Google can't win the hearts of both sides, but they surely know that -- you don't need to get so personally frustrated on their behalf. It's just a company with a product in a market, and the market is never going to be uniform. reply beeflet 1 hour agorootparentprevIt's an open-source program, it shouldn't be held to the same standards as a closed-source program that just shuttles all of your data to someone else's computer. The risk here isn't misuse of the data, it's that some exploit is found in the code, and the additional protection of limiting its filesystem access is marginal (but nice to have). reply monocasa 2 hours agorootparentprevI mean, syncthing is exactly the kind of app I would expect to have full filesystem access. reply growse 2 hours agorootparentWhy? I want it to have access to the folders I want it to sync, not everything on my device. Seems like a perfect fit for SAF. reply lukeschlather 1 hour agorootparentIn a perfect world, what Syncthing does would be handled at the OS level and all OSes would seamlessly interoperate. In the real world the OS vendors are hostile to interoperability so the only way to get that is with a third-party tool that has OS-level access. reply izacus 1 hour agorootparentThere are Android APIs to let syncthing integrate as cloud provider itself and the author didn't use those either. reply sdoering 1 hour agorootparentprevNo. Not in my world. Because I actually want to be able to backup my phone - not only demarcated parts of it. I want to be able to get all data from my phone - regardless of what it is and what app put it there. If I decide to only sync specific folders. So be it. But I want to be able to sync \"/\" reply ants_everywhere 53 minutes agorootparentprev> Google took something that solved real problems better than any alternative could, did so for many years, and destroyed it for no real reason other than to further tighten control they have of the supposedly \"open\" platform. Technically, the guy who inherited Syncthing Android maintenance destroyed it because he didn't want to use the file permission APIs. Which, of course, is a reasonable decision for a maintainer to make when they're working with limited resources. But I have to say in this case I find some of the maintainer's behavior to be a bit surprising for a project as mature as Syncthing. reply kstrauser 24 minutes agorootparentOr maybe this, plus Panic removing GDrive support from Transmit, plus iA dropping Android support from Writer because of it, point to a common perception that Google's API for doing things \"the right way\" suck to the point of unusability. If iA and Panic and Syncthing -- all who've supported Android for many years -- can't manage to make it work, then I suspect it's broken. Google use to allow just any app to access the whole drive. That's probably too permissive. Now they've obviously swung too far the other direction, where even well intended, experienced devs are unable to work within Google's new constraints. reply mcherm 35 minutes agorootparentprev> for no real reason other than to further tighten control they have of the supposedly \"open\" platform I think you're overlooking the obvious motivation of \"maintain a lock on a substantial profit stream\". reply toast0 1 hour agorootparentprevApple's position is different, because Apple never let you have this kind of access. Android/Google Play review keep restricting APIs and replacing them with less capable APIs, or keeping the APIs and reducing functionality. It works again, but I had a USB endoscope that stopped working because Google pulled APIs and took a while to replace them. I can't use location sharing in my choice of apps anymore because something on my phone blocks either app runtime, app internet access, app location, or gps decoding and I don't use it often enough to be motivated to delve through logcat to figure it out, if they even still let me use logcat?. I'm sure it helps my battery life, but it reduces the functionality of the phone. reply EasyMark 35 minutes agorootparentprevI think burn out on free projects is a real thing. Heck I get burn out on old projects and I’m being -paid- to maintain them. Good will and accolades only go so far and passion runs out eventually, especially if you’re only one person and there’s no one there to give you respite for a good recharge and you’re facing a hostile entity like Google. reply beeboobaa3 48 minutes agorootparentprevit's how the cross platform software works and has always worked. demanding a total rewrite just to publish on a single channel is insane, especially since this used to be the ONLY way to do things. google could always contribute to the open source app to implement the features they wish to see, but instead of using their billions for good they'd rather use it for evil. reply jsiepkes 3 hours agoprevThere is an Android Syncthing fork [1] which is active and 1.3K stars (for whatever that's worth). [1] https://github.com/Catfriend1/syncthing-android reply ajb 1 hour agoparentGood to know, although the Readme says: \"Planning to close my Google Play Developer Account. Please say hi if you are interested in obtaining the latest gplay release files from me to help in publishing this app.\" reply Macha 13 minutes agorootparentIt's on f-droid rather than google play. I think for the people interested in using Syncthing rather than Dropbox or Google's syncing option, that's not _that_ much of a problem. reply felurx 2 hours agoparentprevI've been using that one for a long time now. I recall that when I got started with Syncthing (some months to a year or two ago?), it seemed to have been the folk wisdom to use Syncthing-Fork, but I don't recall what the reason was. reply krick 1 hour agoparentprevWeirdly enough, I cannot find either on Play right now..even though I have it (the original, I believe) installed through Play. reply jasonvorhe 1 hour agorootparent> Head to the \"releases\" section or F-Droid for builds. It's in the description on GitHub. Get F-Droid. reply _fat_santa 3 hours agoprevLooking at the underlying thread[1], the author mentions that it's very hard to publish on Google Play > Reason is a combination of Google making Play publishing something between hard and impossible Can someone expand on what's going on here? [1]: https://forum.syncthing.net/t/discontinuing-syncthing-androi... reply izacus 2 hours agoparentIn this case the author doesn't want to use Storage Access Framework APIs to access the file system which were mandated a few years ago as the way to access data outside the app sandbox. They're Java-only APIs and since Syncthings core isn't written in Java, the author would have to write JNI glue to call Android when writing/reading files (and honestly this would be quite tedious, because the way SAF works is to assign Uris to each file and you need to keep querying it to get folder structure since you can't just concat paths like with files). The author didn't want to do that and tried to persuade Google to continue letting them access all files, all photos and all documents directly and Google said no. That was the \"difficulty\" - turns out it's really hard to publish on Play if you refuse to follow the guidelines. Just like on AppStore. reply fph 37 minutes agorootparentTo be fair, you're making the largest mobile operating system in the world and can't be bothered to make API bindings for more than one language? Or at least make the process easy so that someone else creates them? I am not an Android developer, but that seems also part of the problem. reply treyd 2 hours agorootparentprevTo be fair, it's really messy to do Go on Android calling back into Java because of how its runtime works. I'm not surprised they don't want to do it if it's a hobby project and it'd require making substantial changes to Syncthing's core logic. reply izacus 2 hours agorootparentIt is - the way I always structured our architecture in this case was to write as much as possible of file handling in Java side and keep JNI surface minimal (it's also better for performance). But that's really hard to do if you didn't begin with cross platform architecture that doesn't take into account having to modularize the filesystem layer for Android/iOS :/ reply beeboobaa3 43 minutes agorootparentSince you have such strong opinions on the matter, and experience, why don't you contribute to the SyncThing android app and implement this? Alternatively you could grab your time machine, travel back several years and let them know to anticipate this arbitrary change google would pull in the future. reply refulgentis 36 minutes agorootparent> (aggro question) I think it's because he doesn't have a time machine and doesn't have time to donate to rewrite someone else's project that the owner expressly doesn't want rewritten. n.b. it wasn't arbitrary reply beeboobaa3 32 minutes agorootparent> (snarky response) This is what we call \"Put up or shut up\". It's easy to bash someone for not wanting to spend many hours of their time to work they have no interest in, just because some third party is now demanding it. The change is absolutely arbitrary, also. There used to be no way to grant apps access to specific folders. This is when the app was written. This still works. Google's own apps work that way. But now Google has also implemented additional ways to access the filesystem, and they are demanding people who don't even work for them to rewrite their projects. It would be understandable if they demanded new apps to adhere to these new policies. But blocking older apps, that were written when there literally wasn't an alternative available, to do a full rewrite or be banned from updating? Absurd. reply pjmlp 2 hours agorootparentprevMany folks keep thinking that just because Android uses the Linux kernel, it is supposed to behave like GNU/Linux. Likewise they will be rather surprised to insist in targeting iOS/iPadOS/watchOS as if they are UNIX clones. reply bmicraft 59 minutes agorootparentAndroid being linux has exactly nothing to do with the fact that apps can't access resources outside their sandbox if they aren't mapped in somehow. Just like with lxc or docker containers on linux, jails on *bsd, or any other sandboxes. reply pjmlp 47 minutes agorootparentThe only Linux thing on Android is the kernel, use anything else at your peril regarding portability between updates and OEM custom forks. https://developer.android.com/ndk/guides/stable_apis reply bmicraft 30 minutes agorootparentThe only linux thing is always the kernel. Userspace isn't linux by definition on any* system. That might seem like a pedantic point to make, but I would like to point out that gnu utils aren't required to make a system \"Linux\", and whether you're using a drop-in replacement like uutils or go a more different route doesn't make it any less \"Linux\". Android Apps are and mostly always have been restricted to the Java virtual machine (or its modern equivalent) exactly because that makes them portable between sometimes quite different base systems from different vendors. If you insist that makes it less of a Linux system I'd like to know what you think of flatpak apps on top of immutable distros. I hope you agree that, conceptually, they're quite close actually. reply beeboobaa3 43 minutes agorootparentprevThe point is you can't use the regular filesystem syscalls on android, it has to go through a weird java layer reply codethief 53 minutes agorootparentprevWait, I seem to remember this discussion from years ago and thought it was resolved. Back then, Google wanted to drop the \"access all files\" permission from Android's permission system entirely but IIRC Syncthing & file manager devs then convinced them to keep it. But now Google comes back at them with a Google Play policy that prevents them from using that permission in practice? reply IshKebab 5 minutes agorootparentYou have to ask permission via a form from Google and your app must closely fit a white list of use cases (file manager, etc.) Obviously there's a high chance they just say \"nah\". Definitely sucks. reply binkHN 2 hours agoparentprevWhile I don't know about this developer's specific issues, I can comment on my own issues with Google Play as an Android developer. Google Play continues to become more and more stringent with app permissions and the approval of these permissions is very vague. With my own app, from one minor release the next, one day I'll receive approval for my app's permissions and the next week I will not even though only minor changes to the app have been made. When I reach out to Google Play support, the answers are always extremely vague, canned and repetitive and I never know if an update to my app will get approval or not. It's a horrible way to develop anything. reply lawgimenez 2 hours agorootparentThe most annoying requirement is their Play Store delete account url. We have an API where we can delete the user’s account. But no, Google wanted a stupid url. reply spankalee 1 hour agorootparentHow do users use the API? reply macintux 2 hours agorootparentprevIs that so that users who no longer have/want to use the app can still delete their account? reply bmicraft 56 minutes agorootparentprevIs it really that hard to set up a small proxy tool that calls your fancy api when it receives those requests? As an outsider, it does seem quite reasonable to me - Google couldn't possibly support all APIs there may possibly exist for every app there is. reply beeboobaa3 38 minutes agorootparenthow are you going to authenticate the user? now you need to solve that if you didn't have a web login before. --- Guess @dang decided to rate limit my account again so I can't post replies :-) > Some token that every account gets generated? It's really not that much to ask honestly. How is the user going to know this token when they visit the website on their laptop? Keep in mind that the Google requirement is that you link to this delete page from the play store, where the user is not authenticated with your app. You can't just generate an URL containing this token. reply bmicraft 26 minutes agorootparentSome token that every account gets generated? It's really not that much to ask honestly. reply 1over137 2 hours agorootparentprevSounds just like Apple’s stores! reply zo1 2 hours agoparentprevI've done a few apps as part of my day job. And my best explanation and/or analogy is government regulations. The store requirements, apis and rules are documented up to the upteenth degree in 49 pages spread across many areas, and unless you're \"in the know\", you have no way to implement it to a reliable degree. And then all this ends up doing is punishing small / low-budget / low-time developers, leading to consolidation around the big players. The big players can push their weight around to some degree, they get an element of built-in trust, and they have the sheer budget/time to implement all the ridiculous and sometimes onerous requirements. All in all, they're cementing their market position and trying to make \"sticky\" and invested players that will prop-up the play store for the coming decades. reply sdoering 1 hour agorootparentThis is the best analogy I have yet read. It perfectly sums up my experience. reply pomian 1 hour agoprevI would think that a user competent to use and want sync thing, is perfectly capable of depending on f droid as a source for the apk. Can that not be enough of a distribution channel. reply Symbiote 55 minutes agoprevDoes anyone know how long I could expect the current Syncthing Android app to continue working? reply suprjami 7 minutes agoparentPlease read the article before commenting. It is clearly stated. reply thenoblesunfish 37 minutes agoprevThat's a bummer! Can anyone suggest an alternative way I can get files from my macOS laptop to my Android phone? I \"just\" want to put my folder of music files and m3us there so I can play them (with PowerAmp). reply momo777 7 minutes agoparentLocalsend reply krick 1 hour agoprevThat's just great. It's literally the only use case I had for it. There are tons of better ways to share files on PC. reply bmicraft 53 minutes agoparentSharing files? Probably. Reliably syncing files without interaction, and especially without requiring a central server? i don't think so. The truth is, the android app has existed for over 10 years now and it has never been good. It kinda mostly worked for some use cases, but always-on continuous sync on battery wasn't really one of them. And that's been true even before Google justifiably started restricting filesystem access in KitKat(!) 11 years ago. If you are looking for an Android app that does syncing in the background well, I can highly recommend FolderSync. reply akvadrako 3 minutes agorootparentFolderSync with which protocol? The main reason I use Syncthing is not to use Dropbox for sensitive stuff, which I also use since it's on-demand mobile sync is way better. reply themoonisachees 1 hour agoparentprevDo give KDE connect a try. It's great reply krick 1 hour agorootparentI did, before syncthing. It... isn't great. reply blooalien 1 hour agorootparentKDE Connect is great, but not for this use-case. SyncThing was absolutely the choice for automated syncing of files between Android and PC. KDE Connect just doesn't do that (AFAIK). The two tools serve pretty different purposes. reply krick 1 hour agorootparentKDE Connect wasn't really reliable and easy to use while I did. And Syncthing pretty much eliminated the need to use KDE Connect for me. The only extra-feature of KDE Connect was sharing the buffer, and I just paste to Obsidian now instead (any text editor would do, but I don't know anything better, even though I hate it for being closed-source). Maybe slightly less ergonomic than KDE Connect in this regard, but it's negligible. And I don't need to send files, if I can just share files. In fact, I think I never even connected the phone via USB since I started using Syncthing. Copying/moving to/from shared folder is amazingly both more ergonomic and much faster (I never learned, why moving files to Android device via USB is so insanely slow). So I really don't know an alternative. It solved pretty much all my phone/PC sync needs. It also enables backuping the important stuff (I don't use Google for that, or course). Dropbox doesn't cut it either. I really don't imagine how would I use my phone w/o Syncthing, it's pretty much essential. reply raffraffraff 59 minutes agorootparentI run KDE on my \"TV\" and the media controls and clipboard sharing are worth it. I occasionally use my phone as a trackpad too. reply rcarmo 37 minutes agoprevI hope it can live on inside F-Droid somehow. reply analog31 2 hours agoprevWell, I'll be putting the APK in a safe place, along with my Turbo Pascal floppies. ;-) Syncthing for Android has been vital to managing my sheet music collection. reply meowster 1 hour agoprevDoes anyone have any other opensource recommendations? reply donatzsky 42 minutes agoparentThere's a fork called, rather creatively, Syncthing-Fork. It's available on F-Droid. reply warble 25 minutes agorootparentThis one is way more battery conscious as well. Works great, I prefer it. reply timetraveller26 1 hour agoparentprevrsync or rclone on termux reply jlokier 1 minute agorootparentI tried Termux and found the application files I wanted to rsync aren't visible in Termux, except maybe by rooting the phone. But I need to use banking and auth apps daily on it, so I assume that isn't an option. reply beeflet 1 hour agorootparentprevthat would require you to set up a domain name and use DDNS and trust your registrar, or to just always have your computers at a static IP with no NAT blocking you whatsoever. reply wazzaps 42 minutes agorootparentOr use Tailscale to solve both issues at once reply sabbaticaldev 3 hours agoprevGoogle is in self-destruct model reply krick 1 hour agoparentIt is self-destruct only if you have competition. They don't, so it's just destruct. reply hu3 2 hours agoprevWelp, there goes this recommendation out the window. Back to suggesting Dropbox. reply xnx 3 hours agoprevThis is disappointing. SyncThing is one of my reasons for picking Android over iPhone. I hope the author reconsidered. Would any GoFundMe-style donation goal make the hassle worthwhile? reply andreldm 2 hours agoparentWhen I migrated from Android to iPhone, syncthing was my main pain point. There is Möbius Sync, even though not open source, at least it’s an one-time small payment, which is fair considering the dev license cost. Unfortunately it’s not as reliable as the Android official app or the fork, I guess Apple is very strict with background processing, but hey it’s better than nothing. reply atmosx 1 hour agoparentprevI'm using Syncthing to sync documents between devices and I own a mac fleet and an iPhone. The Möbius Sync[1] client (no affiliation) works great for me. [1] https://mobiussync.com/ reply warble 22 minutes agorootparentIt works, and well, considering the constraints in iOS, but doesn't compare well with Androids version. Regular sync thing on Mac is awesome. reply exabyte 2 hours agoparentprevOne of the top comments has a link to a fork in case you haven't seen it reply 1over137 2 hours agoprevIs Google treating Dropbox the same way? Sounds like it could be anticompetitive behaviour. reply izacus 2 hours agoparentNo, because Dropbox actually follows the API guidelines. reply rkagerer 1 hour agorootparentHow about DropSync (which I find much more useful than the official app)? reply izacus 1 hour agorootparentI'm afraid I don't know, never used that app. reply clot27 1 hour agoprevShit, this was the app I used to sync my obsidian notes through multiple devices. reply onehair 39 minutes agoparentKeepass and logseq for me reply denismi 1 hour agoprevWithout Android what's even the point? If all I have is laptop and desktop, I probably just run scheduled rsyncs or a systemd timer something. reply EasyMark 23 minutes agoparentYou can run that on android? Sorry I’ve been out of the android world for a while :) reply havaloc 1 hour agoprevJoining the IA Writer club on Android: https://news.ycombinator.com/item?id=41658023 reply j1elo 3 hours agoprevWhat the heck, I literally come from upvoting another submission's comment about combining LocalSend with Syncthing [1], because the idea seemed great... That's gone very fast from \"oh yeah!\" to \"oh no!\" [1]: https://news.ycombinator.com/item?id=41891983 reply ensignavenger 26 minutes agoparentThere is a fork mentioned above. reply clot27 59 minutes agoprevShit, this is the app I use to sync my obsidian notes through multiple devices. R.I.P. reply dang 56 minutes agoprevUrl changed from https://old.reddit.com/r/Syncthing/comments/1g7zpvm/syncthin..., which points to this. Submitters: \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" - https://news.ycombinator.com/newsguidelines.html reply lopkeny12ko 3 hours agoprev [–] ...Does the author not understand that the Google Play Store is not the only distribution mechanism for apps? Why not just continue to distribute APKs for users to install manually? reply gurchik 3 hours agoparentThe app is on F-Droid, which is mentioned in the article: https://f-droid.org/en/packages/com.nutomic.syncthingandroid... I’m not sure why it can’t continue to be. Does anyone know why? reply suprjami 2 minutes agorootparentIt's stated in the post. They do not think F-Droid provides enough distribution because it's not as popular as Google Play. reply somat 1 hour agorootparentprevI suspect it could be, however it sounds like the author has lost all motivation to continue work on syncthing android and has formally announced that no further development will be done. And as much as I like syncthing-android I appreciate this sort of straight forward communications more and salute the author for clearly stating intentions. Now seeing as it is open source (hooray) The way I think that will go is as follows, we will continue using the existing apk(I get mine off f-droid) for a few months to years, in the meantime seeing as the app is so useful to so many a few forks will arise, one will end up being the best, and we will eventually start using that one. reply RussianCow 1 hour agorootparentprevPresumably because most people don't know about F-Droid, and there's a question of whether it's worth continuing to develop the app for a tiny subset of the original audience. reply tomn 3 hours agoparentprevthey do, but google play is the main distribution channel for android apps, and without that many people will not use it (and many will complain). from the actual announcement: > without Play releases I do no longer see enough benefit and/or have enough motivation to keep up the ongoing maintenance https://forum.syncthing.net/t/discontinuing-syncthing-androi... reply EasyMark 22 minutes agoparentprev [–] They do set how permissions on upgrades of Android work so even if it’s on f-droid, when they enforce the SAF protocols fsync will stop working if you keep your Android up to date reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Syncthing Android app will be retired, with its final release scheduled for December 2024 on Github and F-Droid.",
      "The decision to retire the app stems from challenges with Google Play publishing and a lack of active maintenance.",
      "The app has not experienced significant development recently, leading to insufficient motivation to continue its upkeep."
    ],
    "commentSummary": [
      "The Syncthing Android app has been discontinued from the Google Play Store due to unclear demands and additional work imposed by Google's requirements.",
      "The developer's loss of motivation highlights concerns about Google's increasing control over Android, affecting app functionality and user choice.",
      "Users can still access a fork of the app on F-Droid, an alternative app distribution platform."
    ],
    "points": 202,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1729435865
  },
  {
    "id": 41895764,
    "title": "Internet Archive breached again through stolen access tokens",
    "originLink": "https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/",
    "originBody": "Internet Archive breached again through stolen access tokens { \"@context\": \"https://schema.org\", \"@type\": \"NewsArticle\", \"url\": \"https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/\", \"headline\": \"Internet Archive breached again through stolen access tokens\", \"name\": \"Internet Archive breached again through stolen access tokens\", \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"id\": \"https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/\" }, \"description\": \"The Internet Archive was breached again, this time on their Zendesk email support platform after repeated warnings that threat actors stole exposed GitLab authentication tokens.\", \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://www.bleepstatic.com/content/hl-images/2024/10/09/internet-archive.jpg\", \"width\": 1600, \"height\": 900 }, \"author\": { \"@type\": \"Person\", \"name\": \"Lawrence Abrams\", \"url\": \"https://www.bleepingcomputer.com/author/lawrence-abrams/\" }, \"keywords\": [\"Access Token\",\"Authentication Tokens\",\"Data Breach\",\"GitLab\",\"Internet Archive\",\"Zendesk\",\"Security\",\"InfoSec, Computer Security\"], \"datePublished\": \"2024-10-20T10:46:56-04:00\", \"dateModified\": \"2024-10-20T13:13:22-04:00\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"BleepingComputer\", \"url\": \"https://www.bleepingcomputer.com/\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.bleepstatic.com/logos/bleepingcomputer-logo.png\", \"width\": 700, \"height\": 700 } } }!function(n){if(!window.cnx){window.cnx={},window.cnx.cmd=[];var t=n.createElement('iframe');t.src='javascript:false'; t.display='none',t.onload=function(){var n=t.contentWindow.document,c=n.createElement('script');c.src='//cd.connatix.com/connatix.player.js?cid=f9509d53-804e-427d-a0bc-1204c0a3bcb1',c.setAttribute('async','1'),c.setAttribute('type','text/javascript'),n.body.appendChild(c)},n.head.appendChild(t)}}(document); (new Image()).src = 'https://capi.connatix.com/tr/si?token=ce4d4c45-53cb-40cc-88d1-30d789f5b276&cid=f9509d53-804e-427d-a0bc-1204c0a3bcb1'; cnx.cmd.push(function() { cnx({ playerId: \"ce4d4c45-53cb-40cc-88d1-30d789f5b276\" }).render(\"e75afa31879d4019aa2749d6e1699a5d\"); });var freestar = freestar || {}; freestar.queue = freestar.queue || []; freestar.config = freestar.config || {}; // Tag IDs set here, must match Tags served in the Body for proper setup freestar.config.enabled_slots = [];freestar.queue.push(function() { googletag.pubads().setTargeting('section', ['news','security']);}); freestar.initCallback = function () { (freestar.config.enabled_slots.length === 0) ? freestar.initCallbackCalled = false : freestar.newAdSlots(freestar.config.enabled_slots) } ;(function(o) { var w=window.top,a='apdAdmin',ft=w.document.getElementsByTagName('head')[0], l=w.location.href,d=w.document;w.apd_options=o; if(l.indexOf('disable_fi')!=-1) { console.error(\"disable_fi has been detected in URL. FI functionality is disabled for this page view.\"); return; } var fiab=d.createElement('script'); fiab.type = 'text/javascript'; fiab.src=o.scheme+'ecdn.analysis.fi/static/js/fab.js';fiab.id='fi+o.websiteId; ft.appendChild(fiab, ft);if(l.indexOf(a)!=-1) w.localStorage[a]=1; var aM = w.localStorage[a]==1, fi=d.createElement('script'); fi.type='text/javascript'; fi.async=true; if(aM) fi['data-cfasync']='false'; fi.src=o.scheme+(aM?'cdn':'ecdn') + '.firstimpression.io/' + (aM ? 'fi.js?id='+o.websiteId : 'fi_client.js'); ft.appendChild(fi); })({ 'websiteId': 5971, 'scheme': '//' });window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-GD465VRQLD'); NewsFeatured LatestESET partner breached to send data wipers to Israeli orgsCisco takes DevHub portal offline after hacker publishes stolen dataIntel, AMD CPUs on Linux impacted by newly disclosed Spectre bypassMicrosoft warns it lost some customer's security logs for a monthSevere flaws in E2EE cloud storage platforms used by millionsInternet Archive breached again through stolen access tokensDitch the subscriptions—this Koofr cloud storage deal is only $120 for lifeMicrosoft creates fake Azure tenants to pull phishers into honeypots TutorialsLatest PopularHow to access the Dark Web using the Tor BrowserHow to enable Kernel-mode Hardware-enforced Stack Protection in Windows 11How to use the Windows Registry EditorHow to backup and restore the Windows RegistryHow to start Windows in Safe ModeHow to remove a Trojan, Virus, Worm, or other MalwareHow to show hidden files in Windows 7How to see hidden files in Windows Virus Removal GuidesLatest Most Viewed RansomwareRemove the Theonlinesearch.com Search RedirectRemove the Smartwebfinder.com Search RedirectHow to remove the PBlock+ adware browser extensionRemove the Toksearches.xyz Search RedirectRemove Security Tool and SecurityTool (Uninstall Guide)How to Remove WinFixer / Virtumonde / Msevents / Trojan.vundoHow to remove Antivirus 2009 (Uninstall Instructions)How to remove Google Redirects or the TDSS, TDL3, or Alureon rootkit using TDSSKillerLocky Ransomware Information, Help Guide, and FAQCryptoLocker Ransomware Information Guide and FAQCryptorBit and HowDecrypt Information Guide and FAQCryptoDefense and How_Decrypt Ransomware Information Guide and FAQ DownloadsLatest Most DownloadedQualys BrowserCheckSTOPDecrypterAuroraDecrypterFilesLockerDecrypterAdwCleanerComboFixRKillJunkware Removal Tool DealsCategorieseLearningIT Certification CoursesGear + GadgetsSecurity VPNsPopularBest VPNsHow to change IP addressAccess the dark web safelyBest VPN for YouTube Forums MoreStartup Database Uninstall Database Glossary Chat on Discord Send us a Tip! Welcome Guide HomeNewsSecurityInternet Archive breached again through stolen access tokensInternet Archive breached again through stolen access tokens By Lawrence Abrams October 20, 2024 10:46 AM 1 The Internet Archive was breached again, this time on their Zendesk email support platform after repeated warnings that threat actors stole exposed GitLab authentication tokens. Since last night, BleepingComputer has received numerous messages from people who received replies to their old Internet Archive removal requests, warning that the organization has been breached as they did not correctly rotate their stolen authentication tokens. \"It's dispiriting to see that even after being made aware of the breach weeks ago, IA has still not done the due diligence of rotating many of the API keys that were exposed in their gitlab secrets,\" reads an email from the threat actor. \"As demonstrated by this message, this includes a Zendesk token with perms to access 800K+ support tickets sent to info@archive.org since 2018.\" \"Whether you were trying to ask a general question, or requesting the removal of your site from the Wayback Machine your data is now in the hands of some random guy. If not me, it'd be someone else.\"Internet Archive Zendesk emails sent by the threat actorSource: BleepingComputer The email headers in these emails also pass all DKIM, DMARC, and SPF authentication checks, proving they were sent by an authorized Zendesk server at 192.161.151.10.Internet Archive Zendesk email headersSource: BleepingComputer After publishing this story, BleepingComputer was told by a recipient of these emails that they had to upload personal identification when requesting a removal of a page from the Wayback Machine. The threat actor may now also have access to these attachments depending on the API access they had to Zendesk and if they used it to download support tickets. These emails come after BleepingComputer repeatedly tried to warn the Internet Archive that their source code was stolen through a GitLab authentication token that was exposed online for almost two years. Exposed GitLab authentication tokens On October 9th, BleepingComputer reported that Internet Archive was hit by two different attacks at once last week—a data breach where the site's user data for 33 million users was stolen and a DDoS attack by a pro-Palestinian group named SN_BlackMeta. While both attacks occurred over the same period, they were conducted by different threat actors. However, many outlets incorrectly reported that SN_BlackMeta was behind the breach rather than just the DDoS attacks.JavaScript alert on Internet Archive warning about the breachSource: BleepingComputer This misreporting frustrated the threat actor behind the actual data breach, who contacted BleepingComputer through an intermediary to claim credit for the attack and explain how they breached the Internet Archive. The threat actor told BleepingComputer that the initial breach of Internet Archive started with them finding an exposed GitLab configuration file on one of the organization's development servers, services-hls.dev.archive.org. BleepingComputer was able to confirm that this token has been exposed since at least December 2022, with it rotating multiple times since then.Exposed Internet Archive GitLab authentication tokenSource: BleepingComputer The threat actor says this GitLab configuration file contained an authentication token allowing them to download the Internet Archive source code. The hacker say that this source code contained additional credentials and authentication tokens, including the credentials to Internet Archive's database management system. This allowed the threat actor to download the organization's user database, further source code, and modify the site. The threat actor claimed to have stolen 7TB of data from the Internet Archive but would not share any samples as proof. However, now we know that the stolen data also included the API access tokens for Internet Archive's Zendesk support system. BleepingComputer attempted contact the Internet Archive numerous times, as recently as on Friday, offering to share what we knew about how the breach occurred and why it was done, but we never received a response. Breached for cyber street cred After the Internet Archive was breached, conspiracy theories abounded about why they were attacked. Some said Israel did it, the United States government, or corporations in their ongoing battle with the Internet Archive over copyright infringement. However, the Internet Archive was not breached for political or monetary reasons but simply because the threat actor could. There is a large community of people who traffic in stolen data, whether they do it for money by extorting the victim, selling it to other threat actors, or simply because they are collectors of data breaches. This data is often released for free to gain cyber street cred, increasing their reputation among other threat actors in this community as they all compete for who has the most significant and most publicized attacks. In the case of the Internet Archive, there was no money to be made by trying to extort the organization. However, as a well-known and extremely popular website, it definitely boosted a person's reputation amongst this community. While no one has publicly claimed this breach, BleepingComputer was told it was done while the threat actor was in a group chat with others, with many receiving some of the stolen data. This database is now likely being traded amongst other people in the data breach community, and we will likely see it leaked for free in the future on hacking forums like Breached. Update 10/20/24: Added information about how some people had to upload personal IDs when requesting removal from Internet Archive.Related Articles: Internet Archive hacked, data breach impacts 31 million usersCisco takes DevHub portal offline after hacker publishes stolen dataTech giant Nidec confirms data breach following ransomware attackBianLian ransomware claims attack on Boston Children's Health PhysiciansHackers blackmail Globe Life after stealing customer data freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_728x90_320x50_InContent_1\", slotId: \"bleepingcomputer_728x90_320x50_InContent_1\" });Access Token Authentication Tokens Data Breach GitLab Internet Archive ZendeskLawrence AbramsLawrence Abrams is the owner and Editor in Chief of BleepingComputer.com. Lawrence's area of expertise includes Windows, malware removal, and computer forensics. Lawrence Abrams is a co-author of the Winternals Defragmentation, Recovery, and Administration Field Guide and the technical editor for Rootkits for Dummies. Previous ArticleNext ArticleComments notsahil- 1 hour ago Hello, just curious which software is the screenshot from that captioned: \"Exposed Internet Archive GitLab authentication token\" Thank You!Post a Comment Community RulesYou need to login in order to post a comment Not a member yet? Register NowYou may also like:(adsbygoogle = window.adsbygoogle || []).push({});freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_1\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_1\" });Popular Stories Microsoft warns it lost some customer's security logs for a monthMicrosoft creates fake Azure tenants to pull phishers into honeypotsESET partner breached to send data wipers to Israeli orgsfreestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_2\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_2\" });Sponsor PostsProtecting against password attacksHybrid Analysis Bolstered by Criminal IP’s Comprehensive Domain IntelligenceHow open source SIEM and XDR tackle evolving threatsAutomate all things security in the Blink of AIHow to leverage $200 million FCC program boosting K-12 cybersecurity freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_300x250_300x600_160x600_Right_3\", slotId: \"bleepingcomputer_300x250_300x600_160x600_Right_3\" }); freestar.config.enabled_slots.push({ placementName: \"bleepingcomputer_728x90_970x90_970x250_320x50_BTF\", slotId: \"bleepingcomputer_728x90_970x90_970x250_320x50_BTF\" });Follow us:Main SectionsNews VPN Buyer Guides SysAdmin Software Guides Downloads Virus Removal Guides Tutorials Startup Database Uninstall Database GlossaryCommunityForums Forum Rules ChatUseful ResourcesWelcome Guide SitemapCompanyAbout BleepingComputer Contact Us Send us a Tip! Advertising Write for BleepingComputer Social & Feeds Changelog Terms of Use -Privacy Policy - Ethics Statement - Affiliate Disclosure Copyright @ 2003 - 2024Bleeping Computer® LLC- All Rights Reserved Login UsernamePasswordRemember MeSign in anonymously Sign in with TwitterNot a member yet? Register Now$(document).ready(function(e) { $('.articleBody img').not('a>img').not('.contrib_but>img').click(function(e) { e.preventDefault(); $.fancybox({'href' : $(this).attr('src')}); }); }); $(document).ready(function(){ var content = $('.cz-main-left-section'); var sidebar = $('.bc_right_sidebar'); var count = 0; var myTimer; function setEqualContainer() { var getContentHeight = content.outerHeight(); var getSidebarHeight = sidebar.outerHeight(); if ( getContentHeight > getSidebarHeight ) { sidebar.css('min-height', getContentHeight); } if ( getSidebarHeight > getContentHeight ) { content.css('min-height', getSidebarHeight); } } // creating the timer which will run every 500 milliseconds // and will stop after the container will be loaded // ...or after 15 seconds to not eat a lot of memory myTimer = setInterval( function() { count++; if ( $('.testContainer').length == 0 ) { setEqualContainer(); } else { setEqualContainer(); clearInterval(myTimer); } if ( count == 15) { clearInterval(myTimer); } }, 500); $('#pinned').fixTo('.bc_right_sidebar', { bottom: 25, }); $('#more_dd').click(function (e) { e.preventDefault() }); $('.bc_goto_top a').click(function(){ $(\"html, body\").animate({ scrollTop: 0 }, 600); return false; }); jQuery('.bc_login_btn').on('click', function() { jQuery('.bc_popup').fadeIn(\"slow\"); }); jQuery('.bc_popup_close').on('click', function() { jQuery('.bc_popup').fadeOut(\"slow\"); }); });// validate comment box not empty function validate_comment_box_not_empty() {$('#frm_comment_box').submit(function(e) { if($('#comment_html_box').val().length==0) {alert(\"Please enter a comment before pressing submit\");return false; } else {return true; }}); } function cz_strip_tags(input, allowed) { allowed = (((allowed || '') + '') .toLowerCase() .match(//g) || []) .join(''); // making sure the allowed arg is a string containing only tags in lowercase () var tags = /]*>/gi, commentsAndPhpTags = /|/gi; return input.replace(commentsAndPhpTags, '') .replace(tags, function($0, $1) { return allowed.indexOf('') > -1 ? $0 : ''; }); } function cz_br2nl(str) {var regex = //gi; //var pure_str = str.replace(regex,\"\"); var pure_str = str.replace(regex,\"\"); return cz_strip_tags(pure_str,''); } $(document).ready(function(e) { // validate comment box not empty validate_comment_box_not_empty(); // report comment $('#comment-report-other-reason-wrap').css('display','none'); $('.cz-popup-close').click(function(e) { e.preventDefault(); $('.cz-popup').fadeOut(\"slow\"); }); $('.cz-comment-report-btn').click(function(e) { e.preventDefault(); $('.cz-popup').css('height',$( document ).height()+'px'); //var comment_box_report_top = $(this).offset().top; var comment_box_report_top = $(document).scrollTop(); $('.cz-popup-wrapp').css('top',(comment_box_report_top+100)+'px'); $('#comment-id-report').val($(this).attr('data-id')); $('.cz-popup').fadeIn(\"slow\"); }); $(\"input[type='radio'][name='comment-report-reason']\").click(function(e) { if($(this).val()=='Other') { $('#comment-report-other-reason-wrap').css('display','block'); } else { $('#comment-report-other-reason-wrap').css('display','none'); } }); $('.comment-report-submit-btn').click(function(e) { e.preventDefault(); var comment_report_reason = \"\"; var comment_report_reason = $(\"input[type='radio'][name='comment-report-reason']:checked\").val(); if (comment_report_reason=='Other') { comment_report_reason = $('#comment-report-other-reason').val(); } if(comment_report_reason=='') { alert('Please specify reason'); } else { $('.cz-popup-report-submiting').css('display','inline-block'); $.ajax({type: \"POST\", url: 'https://www.bleepingcomputer.com/report-comment/', data: { comment_id: $('#comment-id-report').val(), reason: comment_report_reason }, success: function(data) { $('.cz-popup-report-submiting').css('display','none'); $('.cz-popup').fadeOut(\"slow\"); }}); } }); // report comment $('.cz_comment_reply_btn').click(function(e) { e.preventDefault(); $('#parent_comment_id').val($(this).attr('data-id')); $('#comment_html_box').attr('placeholder','Replying to '+$(this).attr('data-name')); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); $('#comment_html_box').focus(); }); $('.cz_comment_quote_btn').click(function(e) { e.preventDefault(); var quote_comment_html =''; if($(this).attr('data-id')!=undefined && $(this).attr('data-id')!='') { $('#parent_comment_id').val($(this).attr('data-id')); quote_comment_html = $('#comment_html_'+$(this).attr('data-id')).html(); } quote_comment_html = cz_br2nl(quote_comment_html); $('#comment_html_box').val('\"'+quote_comment_html+'\"'); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); $('#comment_html_box').focus(); }); }); function editForm(cid) { $.ajax({ type: \"GET\", url: window.location.href+\"?sa=1\", data: { f: \"e\", cid: cid }, success: function(data) { $('.cz-post-comment-wrapp').html(data);validate_comment_box_not_empty(); } }); var comment_box_top = $('.cz-post-comment-wrapp').offset().top; $(\"html, body\").animate({ scrollTop: comment_box_top-100 }, 600); } $(document).on('click', '.cz-subscribe-button' , function(e) { e.preventDefault(); $.ajax({type: \"POST\", url: window.location.href, data: { a: 'sub' }, success: function(data) { if(data == '1')$( \"li.cz-subscribe-button\" ).replaceWith( ''); } }); }); $(document).on('click', '.cz-unsubscribe-button' , function(e) { e.preventDefault(); $.ajax({ type: \"POST\", url: window.location.href, data: { a: 'unsub' }, success: function(data) { if(data == '1')$( \"li.cz-unsubscribe-button\" ).replaceWith( ''); } }); });$('.cz-print-icon, .cz-lg-print-icon').click(function(e) { e.preventDefault(); var divToPrint = document.getElementById('.article_section'); var mywindow = window.open('','','left=0,top=0,width=950,height=600,toolbar=0,scrollbars=0,status=0,addressbar=0'); var is_chrome = Boolean(mywindow.chrome); mywindow.document.write($( \".article_section\" ).html()); mywindow.document.close(); // necessary for IE >= 10 and necessary before onload for chrome if (is_chrome) { mywindow.onload = function() { // wait until all resources loaded mywindow.focus(); // necessary for IE >= 10 mywindow.print(); // change window to mywindow mywindow.close();// change window to mywindow }; } else { mywindow.document.close(); // necessary for IE >= 10 mywindow.focus(); // necessary for IE >= 10 mywindow.print(); mywindow.close(); } return true; });var loginhash = '880ea6a14ea49e853634fbdc5015a024'; var main_nav_hide_flag = true; var scrollTop =0; var main_nav_hide_timer = ''; function call_main_nav_hide() { if(main_nav_hide_flag && scrollTop >=100) { $('header').addClass(\"nav-up\"); } } var cz_header_pos = $('header').offset().top; $(window).scroll(function() {$('header').each(function(){var cz_top_of_window = $(window).scrollTop()-100; if (cz_top_of_window > cz_header_pos) { $('.bc_goto_top').fadeIn(\"slow\"); } else {$('.bc_goto_top').fadeOut(\"slow\");}}); }); var prevScrollTop = 0; $(window).scroll(function(event){ scrollTop = $(this).scrollTop(); if ( scrollTop$('body').height() - $(window).height() ) { scrollTop = $('body').height() - $(window).height(); } if (scrollTop >= prevScrollTop && scrollTop) { $('header').addClass(\"nav-up\"); } else {if (scrollTop >=100){ $('header').removeClass(\"nav-up\"); main_nav_hide_timer = setTimeout(\"call_main_nav_hide()\",5000);}else{ $('header').removeClass(\"nav-up\"); clearInterval(main_nav_hide_timer);} } prevScrollTop = scrollTop; }); $(document).ready(function(){var bLazy = new Blazy(); $(\".bc_dropdown a\").mouseenter(function(e) { $(this).parent('.bc_dropdown').delay(250).queue(function(){ $(this).addClass('show_menu').dequeue(); bLazy.revalidate(); }); main_nav_hide_flag = false; }); $(\".bc_dropdown\").mouseleave(function(e) { $(\".bc_dropdown\").clearQueue().stop().removeClass('show_menu'); main_nav_hide_flag = true; if (scrollTop >=100) { main_nav_hide_timer = setTimeout(\"call_main_nav_hide()\",5000); } }); $('.bc_dropdown a').each(function(){ if($(this).is(\":hover\")) { $(this).mouseenter(); } }); $('#bc_drop_tab a').hover(function (e) { e.preventDefault() $(this).tab('show') bLazy.revalidate(); });$('#more_dd').click(function (e) { e.preventDefault()});$('.bc_goto_top a').click(function(){$(\"html, body\").animate({ scrollTop: 0 }, 600);return false;});jQuery('.bc_login_btn').on('click', function() { jQuery('.bc_popup').fadeIn(\"slow\"); $('#ips_username').focus(); });jQuery('.bc_popup_close').on('click', function() { jQuery('.bc_popup').fadeOut(\"slow\"); }); }); $(document).mouseup(function (e) { var container = $(\".bc_login_form\"); if (!container.is(e.target) // if the target of the click isn't the container... && container.has(e.target).length === 0 && $('.bc_popup').css('display') =='block') // ... nor a descendant of the container { jQuery('.bc_popup').fadeOut(\"slow\"); } }); if($(window).width()ReporterHelp us understand the problem. What is going on with this comment? Spam Abusive or Harmful Inappropriate content Strong language OtherRead our posting guidelinese to learn what content is prohibited.Submitting... SUBMITvar loadDeferredStyles = function() { var addStylesNode = document.getElementById(\"deferred-styles\"); var replacement = document.createElement(\"div\"); replacement.innerHTML = addStylesNode.textContent; document.body.appendChild(replacement) addStylesNode.parentElement.removeChild(addStylesNode); }; var raf = requestAnimationFrame || mozRequestAnimationFrame || webkitRequestAnimationFrame || msRequestAnimationFrame; if (raf) raf(function() { window.setTimeout(loadDeferredStyles, 0); }); else window.addEventListener('load', loadDeferredStyles);",
    "commentLink": "https://news.ycombinator.com/item?id=41895764",
    "commentBody": "Internet Archive breached again through stolen access tokens (bleepingcomputer.com)199 points by vladyslavfox 4 hours agohidepastfavorite76 comments trompetenaccoun 3 hours agoWe need archives built on decentralized storage. Don't get me wrong, I really like and support the work Internet Archive is doing, but preserving history is too important to entrust it solely to singular entities, which means singular points of failure. reply MattPalmer1086 2 hours agoparentLots of Copies Keeps Stuff Safe https://www.lockss.org/ This is a brilliant system relying on a randomised consensus protocol. I wanted to do my info sec dissertation on it, but its security model is extremely well thought out. There wasn't anything I felt I could add to it. reply ChadNauseam 13 minutes agorootparentI wish IPFS wasn't so wasteful with respect to storage. I tried pinning a 200mb PDF on IPFS and doing so ended up taking almost a gigabyte of disk space altogether. It's also relatively slow. However its implementation of global deduplication is super cool – it means that I can host 5 pages and you can host 50, and any overlap between them means we can both help one another keep them available even if we don't know about one another beforehand. For a large-scale archival project, it might not be ideal. Maybe something based on erasure coding would be better. Do you know how LOCKSS compares? reply TZubiri 44 minutes agorootparentprevHigh Costs Makes Lots of Copies Unfeasible reply MattPalmer1086 35 minutes agorootparentThat was actually one of the key constraints in the LOCKSS system, since it was designed to be run by libraries that don't have big budgets. The design is really very good. reply jdiff 2 hours agoparentprevThis seems to get brought at least once in the comments for every one of these articles that pops up. The IA has tried distributing their stores, but nowhere near enough people actually put their storage where their mouths are. reply creer 29 minutes agorootparentAnd it's guaranteed not to happen if the efforts don't continue. reply immibis 2 hours agorootparentprevKeep in mind the IA archives a lot of garbage. If it could be more focused it would be more likely to work. reply db48x 2 hours agorootparentThe attempts have actually been focused on specific types of content, such as historical videos. reply WarOnPrivacy 59 minutes agorootparentprev> nowhere near enough people actually put their storage where their mouths are. Typically because most people who have the upload, don't know that they can. And if they come to the notion on their own, they won't know how. If they put the notion to a search engine, the keywords they come up with probably don't return the needed ELI5 page. As in: How do I [?] for the Internet Archive?, most folks won't know what [?] needs to be. reply TZubiri 43 minutes agorootparentThis is literally torrents. Just give up reply __MatrixMan__ 28 minutes agoparentprevTo make the web distributed-archive-friendly I think we need to start referencing things by hash and not by a path which some server has implied it will serve consistently but which actually shows you different data at different times for a million different reasons. If different data always gets a different reference, it's easy to know if you have enough backups of it. If the same name gets you a pile of snapshots taken under different conditions, it's hard to be sure which of those are the thing that we'd want to back up for that particular name. reply oytis 3 hours agoparentprevWe'll need to find even more people willing to expose themselves to legal threats and cyberattacks then. reply trompetenaccoun 1 hour agorootparentThe legal side is a big issue, true. The simplest and best workaround that I'm aware of is how the Arweave network handles it. They leave it up to the individual what parts of the data they want to host, but they're financially incentivized to take on rare data that others aren't hosting, because the rarer it is the more they get rewarded. Since it's decentralized and globally distributed, if something is risky to host in one jurisdiction, people in another can take that job and vice versa. The data also can not be altered after it's uploaded, and that's verifiable through hashes and sampling. Main downside in its current form is that decentralized storage isn't as fast as having central servers. And the experience can vary of course, depending on the host you connect to. As for technical attacks, I'm not an expert but I'd assume it's more difficult for bad actors to bring down decentralized networks. Has the BitTorrent network ever gone offline because it was hacked for example? That seems like it would be extremely hard to do, not even the movie industry managed to take them down. reply sksxihve 2 hours agoparentprevThere's no real financial incentive for people to archive the data as a singular entity so even less for a distributed collection. Also it's probably easier to fund a single entity sufficiently so they can have security/code audits than a bunch of entities all trying to work together. reply riiii 2 hours agorootparentSome people are motivated by more than just financial incentive. reply sksxihve 1 hour agorootparentThat's true, but something like archiving the internet is very costly, IA has an annual budget in the tens of millions. reply trompetenaccoun 1 hour agorootparentYes, it's a good point. Though they could take that money and reward people for hosting the data as well, couldn't they? They don't have to be in charge of hosting. reply sksxihve 36 minutes agorootparentYes, they could, that's not much different than a single company distributing the archive to multiple storage centers though. My original comment was about it being more cost effective for a single company to do that than coordinating with a bunch of disjoint entities. reply notmysql_ 1 hour agoprevI sent them a resume almost a year ago, and got nothing back in response until yesterday. Looks like they are going through their backlog right now to find more hands. reply TZubiri 41 minutes agoparentInteresting, for a security position? reply sirolimus 1 hour agoprevIt’s incredibly sad to see threat actors attack something as altruistic as an internet library. Truly demoralizing to see such degeneracy. reply myself248 3 hours agoprevI'd like to imagine a world where every lawyer, when their case is helped by a Wayback Machine snapshot of something, flips a few bucks to IA. They could afford a world-class admin team in no time flat. reply thaumasiotes 3 hours agoparentThat's a terrible solution. The Wayback Machine takes down their snapshots at the request of whoever controls the domain. That's not archival. If the state of a webpage in the past matters to you, you need a record that won't cease to exist when your opposition asks it to. This is the concept behind perma.cc. reply db48x 2 hours agorootparentNo, they don’t delete the archived content. When the domain’s robots.txt file bans spidering, then the Wayback Machine _hides_ the content archived at that domain. It is still stored and maintained, but it isn’t distributed via the website. The content will be unhidden if the robots.txt file stops banning spiders, or if an appropriate request is made. reply speerer 1 hour agorootparentIn some cases they do appear to delete, on request. edit: \"Other types of removal requests may also be sent to info@archive.org. Please provide as clear an explanation as possible as to what you are requesting be removed for us to better understand your reason for making the request.\", https://help.archive.org/help/how-do-i-request-to-remove-som... reply db48x 20 minutes agorootparentNope. Nothing is deleted, just hidden. reply rascul 13 minutes agorootparentHow do you know? reply db48x 8 minutes agorootparentI worked there for a short while. Raed667 1 hour agorootparentprevThey do delete entire domains from the archive upon request & proof of ownership. reply db48x 20 minutes agorootparentAgain, no they don’t. They just hide them. reply myself248 2 hours agorootparentprevOoo, excellent. Yes, hiding items is imperfect, but I understood that it was legally required or something. (IANAL and IDFK, TBH) I wonder how perma.cc gets around that. reply berdario 2 hours agorootparentI'm afraid that it just hasn't been tested in court yet. I haven't read this paper yet, but... https://www.tesble.com/10.1080/0270319x.2021.1886785 from the abstract: > The article concludes that Perma.cc's archival use is neither firmly grounded in existing fair use nor library exemptions; that Perma.cc, its \"registrar\" library, institutional affiliates, and its contributors have some (at least theoretical) exposure to risk It seems that the article is about copyright, but of course there are several other reasons that might justify takedown of content stored on perma.cc: - Right to be forgotten... perma.cc might be able to ignore it, but could this lead to perma.cc being blocked by european ISPs - ITAR stuff - content published by entities recognized by $GOVERNMENT as terrorist organizations - revenge porn - CSAM reply immibis 2 hours agorootparentprevMost likely by breaking the law. reply speerer 1 hour agorootparentprevThat's correct, but only for present evidence - what about the past evidence, that you didn't know you needed until it was too late? IA is broad enough to cover the past five times out of ten. reply _fat_santa 3 hours agoprevI don't know what their funding model looks like but if they have some cash I'd say hiring a security team would be on top of the list of things to invest in. reply brendoelfrendo 3 hours agoparentI believe that, at this point in time at least, IA's funding model consists of sweating profusely while awaiting a colossal legal judgement. reply 999900000999 18 minutes agoprevDo any organizations have a mirror of this? Even if it's not publicly available... reply gweinberg 1 hour agoprevDoes anyone know who is targeting the Internet Archive, and why? I get the impression the attacks are too sophisticated for it to just be vandal punks. reply udev4096 3 hours agoprevIs it the same email spoofing attack vector of zendesk which was disclosed last week? reply steffanA 3 hours agoparentArticle says API token was stolen in original breach. reply wkat4242 3 hours agoprevOuch. Once can happen, twice in a row... reply fallingknife 3 hours agoparentOnce makes the second time more likely. Shows you are a soft target. reply anthk 21 minutes agoprevThe Internet Archive had legal gems such as the Jamendo Album Collection, a huge CC haven. Yes, most of it under NC licenses, but for non-commercial streaming radio with podcasts, these have been invaluable. Do you know Nanowar? They began there. Also, as commercal music has been deliberately dumbed down for the masses (in paper, not by cheap talking), discovering Jamendo and Magnatune in late 00's has been like crossing a parallel universe. reply TheFreim 3 hours agoprev> \"It's dispiriting to see that even after being made aware of the breach weeks ago, IA has still not done the due diligence of rotating many of the API keys that were exposed in their gitlab secrets,\" reads an email from the threat actor. This is quite embarrassing. One of the first things you do when breached at this level is to rotate your keys. I seriously hope that they make some systemic changes, it seems that there were a variety of different bad security practices. reply ghostly_s 1 hour agoparentIA is in bad need of a leadership change. The content of the archive is immensely valuable (largely thanks to volunteers) but the decisions and priorities of the org have been far off base for years. reply fngjdflmdflg 1 hour agorootparentDo you have any examples? reply wkat4242 7 minutes agorootparentPutting the organisation at risk by playing chicken with large publishing corporations. Trying to stretch fair use a little too far so they had to go to court. reply soygem 55 minutes agorootparentprevDeleting archives of kiwifarms. reply fngjdflmdflg 43 minutes agorootparentI don't believe IA itself takes down pages that kiwifarms archives/links to. Rather they get a request to take it down and comply with it (correct me if I'm wrong here). I think IA is actually in a tough spot on this issue because they might be able to be sued eg. for defamation if they don't take down pages with personal info after a request to do so is made. Lastly, I doubt any new leadership would be less harsh on kiwifarms. reply wkat4242 2 minutes agorootparentprevThat's something I completely support. There's a limit and that site crosses it. superkuh 43 minutes agorootparentprevIt's the least worst option. Remember when that happened with Mozilla? Now they're an ad company. Take the bad (some bad mis-steps re:multiple lending during the pandemic, not rotating keys immediately after a hack) with the good (staying true to the human centric mission and not the money flows). reply echelon 1 hour agorootparentprevI support archival of films, books, and music, but those items need to be write-only until copyright expires. The purpose of the Internet Archive is to achieve a wide-reaching, comprehensive archival, not provide easy and free read access to commercial works. Website caches can be handled differently, but bulk collection of commercial works can't have this same public access treatment. It's crazy to think this wouldn't be a huge liability. Battling for copyright changes is valiant, but orthogonal. And the IA by trying to do both puts its main charter--archival--at risk. The IA should let some other entity fight for copyright changes. I say this as an IA proponent and donor. reply withinboredom 1 hour agorootparentI'd agree with you if you live in a country where you can walk into your local library and read these for \"free.\" For people who live where there may not even be a library, your argument makes no sense except to make the publishers richer. They typically price some of these books at \"library prices\" so normal people won't be able to afford them, but libraries will. reply giantrobot 1 hour agorootparentprev> I support archival of films, books, and music, but those items need to be write-only until copyright expires. Which means no one alive today would ever be able to see them out of copyright. It also requires an unfounded belief that major copyright owning companies won't extend copyright lengths beyond current lengths which are effectively \"forever\". reply galleywest200 3 hours agoparentprev>\"It's dispiriting to see that even after being made aware of the breach weeks ago...\" These people are not dispirited whatsoever, if anything they are half-cocked that these script kiddies found an easy target. reply EasyMark 16 minutes agorootparentI highly doubt they are script kiddies. More than likely they are state actors or mercenaries of state actors attempting to bring down the free transmittal of information between regular folks. IA evidently has not so good security and wikipedia must be doing pretty well I guess? I can’t recall the last time one of these attacks worked on Wiki. reply chrisrhoden 1 hour agorootparentprevThe words came from a message written by the people you are calling script kiddies, rather than being editorializing by bleepingcomputer, as you seem to believe. reply compootr 1 hour agorootparentscript kiddie or blackhat hacker is irrelevant. IA has shit security practices, and that's a fact regardless of who figures that out reply badlibrarian 3 hours agoprevRestating my love for Internet Archive and my plea to put a grownup in charge of the thing. Washington Post: The organization has “industry standard” security systems, Kahle said, but he added that, until this year, the group had largely stayed out of the crosshairs of cybercriminals. Kahle said he’d opted not to prioritize additional investments in cybersecurity out of the Internet Archive’s limited budget of around $20 million to $30 million a year. https://archive.ph/XzmN2 reply semicolon_storm 3 hours agoparentIn security, industry standard seems to be about the same as military grade: the cheapest possible option that still checks all the boxes for SOC. reply EasyMark 12 minutes agorootparentMilitary grade has different meanings. I’ve worked in the electronics industry a long time and will say with confidence that the pcbs and chips we sent to the military were our best. Higher temperature ranges, much more thorough environmental testing, many more thermal and humidity cycles, lots more vibration testing. However we also sell them for 5-10x our regular prices but in much lower quantities. It’s a failed meme in many instances as the internet uses it though. reply incahoots 2 hours agorootparentprevBasically, whatever the liability insurance wants for you to be in compliance, than that’s the standard. reply Spivak 2 hours agorootparentprevHot take, this is the way it should be. If you want better security then you update the requirements to get your certification. Security by its very nature has a problem of knowing when to stop. There's always better security for an ever increasing amount of money and companies don't sign off on budgets of infinity dollars and projects of indefinite length. If you want security at all you have bound the cost and have well-defined stopping points. And since 5 security experts in a room will have 10 different opinions on what those stopping points should be— what constitutes \"good-enough\" they only become meaningful when there's industry wide agreement on them. reply abadpoli 1 hour agorootparentThere never will be an adequate industry-wide certification. There is no universal “good enough” or “when to stop” for security. What constitutes “good enough” is entirely dependent on what you are protecting and who you are protecting it from, which changes from system to system and changes from day to day. The budget that it takes to protect against a script kiddy is a tiny fraction of the budget it takes to protect from a professional hacker group, which is a fraction of what it takes to protect from nation state-funded trolls. You can correctly decide that your security is “good enough” one day, but all it takes is a single random news story or internet comment to put a target on your back from someone more powerful, and suddenly that “good enough” isn’t good enough anymore. The Internet Archive might have been making the correct decision all this time to invest in things that further its mission rather than burning extra money on security, and it seems their security for a long time was “good enough”… until it wasn’t. reply db48x 1 hour agorootparentprevYep. And worse, now matter how much you pay for security it is still possible for someone to make a mistake and publish a credential somewhere public. reply gjsman-1000 1 hour agorootparentprevThis ^ We can’t all have the latest EPYC processors with the latest bug fixes using Secure Enclaves and homomorphic encryption for processing user data while using remote attestation of code running within multiple layers of virtualization. With, of course, that code also being written in Rust, running on a certified microkernel, and only updatable when at least 4 of 6 programmers, 1 from each continent, unite their signing keys stored on HSMs to sign the next release. All of that code is open source, by the way, and has a ratio of 10 auditors per programmer with 100% code coverage and 0 external dependencies. Then watch as a kid fakes a subpoena using a hacked police account and your lawyers, who receive dozens every day, fall for it. reply gjsman-1000 32 minutes agorootparentHilariously, I’ve been downvoted to -2 by butthurt security experts without a counter-argument. reply alexey-salmin 1 hour agoprevA genuine question to commenters asking to \"put a grownup in charge of the thing\" and saying that \"Kahle shouldn't be running things\": he built the thing, why exactly he can't run it the way he sees fit? reply pvg 1 minute agoparentA good place to direct that question might be in a reply to the person who made that comment. reply et-al 48 minutes agoparentprevHe is. But at the cost of the greater good. Most of us care mainly about the Wayback Machine and archiving webpages; not borrowing books still under copyright and fighting publishers. reply TZubiri 41 minutes agorootparentSpeak for yourself, the internet archive successfully increased its scope and made creative contributions to case law (although it lost at the appeals court) reply pessimizer 3 hours agoprev [–] The Internet Archive has a management problem. They seem to be more comfortable disrupting libraries than managing an online, publicly accessible database of disputed, disorganized material. Despite all of the positive self-talk, I don't know if they realize how important they are, or how easy it would be for them to find good help and advice if their management were transparent and everything was debated in public. That may have protected it to some extent; as a counterexample, Wikipedia has been extremely fragile due to its transparency and accessibility to everyone. With IA being driven by its creator's ideology, maybe that ideology should be formalized and set in stone as bylaws, and the torch passed to people openly debating how IA should be run, its operations, and what it should be taking on. I don't mean they should be run by the random set of Confucian-style libertarian aphorisms that is running the credibility of Wikipedia into the ground, but Debian is a good model to follow. Or maybe do better than both? reply mrweasel 1 hour agoparent> Debian is a good model to follow. While I have no idea how Debian is actually funded I'd agree. One issue might be that The Internet Archive actually need to have people on staff, not sure if Debian has that requirement. You're not going to get people to man scanner or VHS players 8 hours a day without pay, at least not at this scale. The Internet Archive needs a better funding strategy that asking for money on their own site. People aren't visiting them frequently enough for that to work. They need a fundraising team, and a good one. Finding managers are probably even worse. They can't get a normal CEO type person, because they aren't a company and the type of people who apply to or are attracted to running non-profit, server the community, don't be evil organisation are frequently bat-shit crazy. reply badlibrarian 2 hours agoparentprevDon't forget the time Brewster tried to run a bank -- Internet Archive Federal Credit Union. Or that the physical archives are stored on an active fault line and unlikely to receive prompt support during an emergency. Or that, when someone told him that archives are often stored in salt mines he replied, \"cool, where can I buy one?\" reply avazhi 2 hours agoparentprevhttps://www.wired.com/story/internet-archive-memory-wayback-... I appreciate their ethos and I've used the site many times (and donated!), but clearly it's at the point where Kahle et al just aren't equipped either personally (as a matter of technical expertise) or collectively (they are just a handful of people) to be dealing with what are probably in many cases nation-state attacks. Kahle's attitude towards (and misunderstanding of) copyright law is IMO proof that he shouldn't be running things, because his legal gambles (gambles that a first year law student could have predicted would fail spectacularly) have put IA at long term risk (see: Napster). And this information coming out over the past few weeks about their technical incompetence is arguably worse, because the tech side of things are what he and his team are actually supposed to be good at. It's true that Google and Microsoft and others should be propping up the IA financially but that isn't going to solve the IA's lack of technical expertise or its delusional hippie ethos. reply kmeisthax 1 hour agoparentprev [–] > Confucian-style libertarian aphorisms that is running the credibility of Wikipedia Can you elaborate? I'm aware of Wikipedia having very particular rules and lots of very territorial editors, but I'm not sure how this runs their credibility into the ground aside from pissing off the far right when they come in with an agenda to push. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Internet Archive experienced a security breach due to stolen access tokens on their Zendesk email support platform, affecting over 800,000 support tickets.",
      "The breach occurred because the Internet Archive did not rotate exposed GitLab authentication tokens, despite prior warnings.",
      "The attack was motivated by the desire for cyber street credibility, with stolen data, including personal IDs, likely being traded in data breach communities."
    ],
    "commentSummary": [
      "The Internet Archive experienced another security breach due to stolen access tokens, prompting discussions on the necessity of decentralized storage to avoid single points of failure.",
      "Suggestions for alternatives include systems like LOCKSS (Lots of Copies Keep Stuff Safe), which use a consensus protocol, while IPFS (InterPlanetary File System) faces criticism for inefficiency.",
      "The breach underscores the Archive's security vulnerabilities, with calls for improved management, funding strategies, and debates over its copyright and security practices."
    ],
    "points": 199,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1729436406
  },
  {
    "id": 41889876,
    "title": "Ribbonfarm Is Retiring",
    "originLink": "https://www.ribbonfarm.com/2024/10/10/ribbonfarm-is-retiring/",
    "originBody": "Home Ribbonfarm Studio Substack For New Readers Now Reading About Books ribbonfarm constructions in magical thinking Ribbonfarm is Retiring October 10, 2024 By Venkatesh Rao After several years of keeping it going in semi-retired, keep-the-lights-on (KTLO) mode, I’ve decided to officially fully retire this blog. The ribbonfarm.com domain and all links will remain active, but there will be no new content after November 13th, 2024, which happens to be my 50th birthday. There will be one final roundup post before then, and perhaps a shortish epitaph post. And the main page will switch to a static landing page. But after that date, this will effectively be a museum site. I’m not personally retiring of course (I neither want to, nor can I afford to), but this WordPress blog is. Sometime in the next few months, I’ll figure out how to move it to a lower-cost archival hosting model, probably as a static non-WordPress site, simplify the design as befits a retiree, and put up some sort of museum-like landing page with self-guided tour maps, a little museum shop selling books, directions to the service entrance for AI scraper-bots, and so on. If you get your updates via the Mailchimp newsletter, that will be shutting down sometime in the next few weeks. So if you’d like to continue hearing from me, sign up for my substack (fair warning: It’s not a blog, and both the contents and style are distinctly different from what you’ve been used to here). But in the meantime, in what is going to be the last significant post on here, let’s look back on what has been a 17-year journey. It’s been 17 eventful years, and this blog has been my home online through some very interesting times, both for me personally and for the world, but it feels like the time is finally just right for it to retreat gracefully to the shadows. Not too early, not too late. There is no specific reason to retire the blog right now. While the recent minor civil war in the WordPress ecosystem (if you don’t know about it, you probably don’t need to) felt like the portent I was unconsciously waiting for, this decision has been brewing for a couple of years now. *** I do think that the end really is here for the blogosphere though. This time it really is different. I’ve weathered many ups and downs in the blogosphere over my 17 years in it, but now it feels like the end of the blogging era. And what has emerged to take its place is not the blogosphere (and really shouldn’t try to be), even though parts of it have tried to claim the word. I don’t think there is any single heir to the blog, or to the public social media landscape it dominated, anymore than there was a single heir to the Roman empire when it collapsed. And this is as things should be. Emerging media should emerge into their own identities, not attempt to perpetuate the legacies of sundowning media, or fight over baggage. And of course, many architectural elements of the blog will live on in newer media, just as many patterns we live with today originated in the Roman empire. Chronological feeds, and RSS-like protocols are part of our collective technological vocabulary. So at least in a technological sense, nothing is dying per se. But in a cultural sense, we are definitely witnessing the end of an era. But there’s no denying that the legacy we’re talking about is a powerful one that will cast a long shadow. The cultural reign of the blog, roughly 2000-20 or so, coincided with the second full chapter of the internet. Far more than aggregators, photo-sharing services, or social feeds, the blog was “Web 2.0.” Blogs didn’t produce the most Web 2.0 bytes, but they produced the most significant ones. Perhaps it’s my blogocentric conceit, but it feels like the active lifespan of ribbonfarm in particular, 2007-24, coincides rather neatly with a very well-defined chapter in the grand narrative of civilization itself, with the Global Financial Crisis (GFC) and the 2024 US Presidential election serving as neat bookends. Whatever the outcome of the election, when this blog officially retires on November 13, we’ll be in some sort of new era. An era blogs helped usher in, but won’t be a part of. Conceit or not, it’s hard for me to see the story of ribbonfarm as merely my story, or even merely the marginal story of the few dozens of contributors, or of the little subculture of a few hundreds that coalesced around it for the better part of a decade. The story of ribbonfarm has been, in a small one-of-a-million threads way, the story of civilization itself, through the 2007-24 period. That thought feels nice, in the way I imagine carving my initials onto an ancient monument might feel nice. VENKAT WAS HERE. Actually, I kinda did that, with a friend’s help. I’d argue that the blogosphere, and the public social media landscape, represented the spirit of this civilizational chapter in a much truer way than old media. Through this period, old media strongholds seemed increasingly like grand and sad old ruins of the past, falling into ever-greater dereliction. While they still exerted a certain baleful influence over the cultural landscape, there was something dead about the gaze of old media on the new world. But public social media… it was alive. Its gaze on the new world was not just lively, it was constitutive. It helped make the world it observed. It was where the real action was. Even the dying old media types knew it, as they increasingly turned to new media not just for informational sustenance, self-disruption ideas, and talent, but for what we might call a sort of life-energy. Journalists didn’t just trawl the old Twitter for stories to print on dead trees. They tapped into it like it was a life-giving elixir they needed to stay spiritually alive. The came for the leads and personalities, stayed for the elan vital. They’ll deny it furiously if you ask of course. They’re a stuffy lot. Nice, but stuffy. And snobby. They’ll pretend it was access to the corridors of power that was keeping them alive all this time. It wasn’t. It was us. For a couple of decades, public social media, with blogs serving as a sort of network of fortresses scattered across that feudal landscape, was where the world happened. A public, grand sort of world. It wreaked havoc too of course, but a grand sort of havoc. The maturing cozyweb phase we’re in right now, while it has its own intimate charms, to be found in the warren of discords, slacks, and group DMs we all inhabit these days, lacks the raw grandeur of the public social media era. Here on ribbonfarm, the era unfolded as a three-act story. The core 2012-2019 period, when Refactor Camp was an active event, is a good proxy for the “subcultural” phase of ribbonfarm. During those years, I had the social energy both to curate a lot of writing by others, and events of various sorts. I also tweeted up a storm. But perhaps more importantly, the zeitgeist itself had the right kind of energy to respond in a lively way to such curation. The periods before and after, 2007-12, and 2019-24 were more personal eras, defined by my own interests. If you’ve been a long-time reader, you know that this three-act story corresponds roughly to my rather grandiose scheme of “Ages” for periodizing the ribbonfarm archives. The three ages of ribbonfarm (the Rust Age (2007-12), the Snowflake Age (2013-18), and the Charnel Age (2019-24)) constitute a story that’s perhaps 10% about me personally, perhaps 20% about the various intersecting milieus that gave rise to the short-lived subculture of the second act, and 70% about the world at large. Throughout the life of this blog, an old world was dying, and a new world was struggling to be born. And this blog channeled the energy of that ongoing transformation for lulz and profit. Gramsci would have approved. I have joked to friends, building on a very clever Drew Austin (an early ribbonfarm contributor) observation, that the entire three-age ribbonfarm story was a product of low interest rates. This blog was sponsored by ZIRP. The future historians who dive into these archives for archaeological research will likely be economic rather than cultural historians, trying to reconstruct the play-by-play impact of ZIRP. Many of the big hits of this blog, such as The Premium Mediocre Life of Maya Millennial, and The Locust Economy (a forgotten hit from 2013) had ZIRPy subtexts. This is not an entirely flippant observation. The blog in its heyday was a product of open-source software, cheap hosting, and perhaps most importantly, essentially free global distribution. First through RSS, then through Twitter. This free distribution was in turn an artifact of cheap venture capital. It began to collapse when interest rates began to rise, forcing distribution deals with devils upon all of us, which only the most desperate and ambitious were willing to take. The rest of us began retreating to the cozyweb rather than pay up to the protection rackeeters who began taking over public distribution channels starting around 2017, culminating in Musk’s acquisition of Twitter in 2022. *** The young cozyweb era currently taking shape and replacing public social media is equally a product of higher interest rates and paid distribution, but is at least not a distribution-channel protection racket. I coined the term cozyweb in 2019 (appropriately enough in my substack, rather than here), which is around when it was born. It was perhaps my last major meme. Equally appropriately, that essay has ended up part of a rather cozy little book from a cozy little publishing startup. The cozyweb anchors an era that’s threatening to be as long-lived as the public social media era, so it might last till 2035 or so. In fact, I don’t think we’ll see really low/zero/negative interest rates again in my lifetime. Which means we may not see anything like the blogosphere or the public social media era either. It’s worth noting that the cozyweb is already 5 years old. Maybe I should start a new periodization. I need a label for my own first age in the cozyweb, 2019-24. I ported over my old Breaking Smart list into what is now my main substack in 2019, started and finished the short-run Art of Gig newsletter as another substack, helped found the cozy Yak Collective Discord community, got myself a 5-year Roam subscription, and (much to the dismay of some of you), went crypto-bro (crypto is a very cozyweb type part of the internet). There have been structural signs of all this of course, and many of my tactical moves over the past few years, which I thought of in the moment as random acts of creative experimentation, were in hindsight preparations for an exit from both the blogosphere and the public social media landscape. A few examples. For example, in 2019, I deliberately shifted gears to writing what I’ve called blogchains, over stand-alone posts. These eschew theatricality and engagement farming, and adopt an intimate rhetorical style better suited to the cozyweb. They do not work well with blogs either structurally or thematically. They are really closer cousins to a much older medium: The monograph. As another example, in 2020, I consciously stopped trying to write “viral hit” type posts, and even bent over backwards to try and write anti-viral posts. The Internet of Beefs, which I wrote in January 2020, was not just my last viral post, it was my last viral-intent post. I also kinda consciously intended it to serve as a sort of epitaph for the public social media era, which by that time I had come to see as being in irreversible decline. Events since then have only strongly confirmed those early premonitions. Virality as such, is mostly dead as a cultural phenomenon in long-form writing. When it occurs these days, it feels somehow deadening and cheap. The cozyweb zeitgeist favors a deeper, quieter sort of writerly ambition, with classier, more high-minded aspirations. Where the viral blog post of the last decade was something of a loud public-park pop-music performance, good 2024 longform feels more like chamber music performed for exclusive invite-only audiences. One under-appreciated reason for Substack’s success at taking over some of the blogosphere’s role is that it’s conducive to this chamber-music style of writing. One of my current operating theories is that this style is in fact the culturally load-bearing part of Substack, even though most of the attention is on high-profile refugees from old media. The proprietors of the platform don’t quite appreciate the chamber music scenes they harbor, which is one of the reasons I’m not convinced the platform will last. I don’t read any of the old-media refugee substacks, but I do read a lot of chamber-music style substacks. And one of the reasons I don’t quite feel at home on Substack is that while I like reading the chamber-music style stuff, I don’t really do it well myself. I’m too much of a careless shitposter, and the chamber music style calls for a certain earnestness of approach. It calls for a certain amount of self-conscious gravitas (which can easily turn into humorless self-importance, the main failure mode on Substack; one that gives it decided LinkedIn overtones). This sense of anxiously performed gravitas is perhaps the governing vibe of Substack. They’re sincerely trying to make meaning over there. We here in the blogosphere were just having fun watching the world burn for a couple of fiery decades. A kind of psychotic, nihilistic humor was the governing vibe of the blogosphere at its best. So far I haven’t seen anything like it on Substack. I miss it of course, but one must move on with the times. On Substack, the mere prospect of making money reliably (something the blogosphere was spectacularly shitty at, and remains so in its dotage) seems to get people to adopt a more respectable demeanor, and second-guess the shitposting instincts that would have served them well in the blogosphere at its peak. The blogosphere didn’t so much move to Substack as get gentrified by it, much as they’d like you to believe it did. And many of us transplanted bloggers got a shave and haircut, put on a suit, and went to work there, shoulder-to-shoulder with the old media types we once maintained ritual rivalries with, but are now increasingly indistinguishable from. The rest of them, that is. Not me of course. I’m only wearing the Substack Suit ironically. Why can’t anyone see I’m only wearing the Substack Suit ironically! But kidding aside, this is fine (and not in a burning-house dog gif sense). Mediums should grow into their own true natures. And if the message of the Substack medium (and subscription newsletters generally… there’s a handful of other contenders) is one of earnestness and gravitas, so be it. Let it get good at it, just as the blogosphere got good at shitposting. Really good. Perhaps the most revealing example. Sometime around late 2021, thanks to my interests shifting steadily towards what I might call studious interests in things like storytelling, fermi estimation, and tinkering in my pandemic-born lab, my posting here began taking on the tenor of a semi-private notebook. Ribbonfarm began to resemble my private Roam notebooks more than its own past as a slovenly, loud, meme-making, virality-chasing, content junkyard. And it’s no accident that the notebook/digital garden type of medium began to emerge around 2019. A vast amount of good writing these days is happening entirely in private. A tiny fraction is shared (Sarah Constantin, a long-time friend of this blog, has a public Roam for example), and a slightly larger fraction (including a lot of my own Roam content) is likely intended for eventual public sharing in some form. But much of the notebooking universe will remain private forever. It is interesting that a kind of semi-public notebooking is how my blogging story is wrapping up. A kind of semi-public notebooking is a good description of the original vibe of blogs, circa 2002-09. But we’ve moved on to a deeper kind of notebooking now (deeper in the sense of much more richly and densely hyperlinked internally). A much deeper kind than blogs can sustain. Deep notebooking and content gardening too, aren’t blogging — and shouldn’t try to be. That would be selling that new medium short. Fourth and final example: I quit actively posting on Twitter when Musk took over. Not directly related to the ribbonfarm story, but a key adjacent subplot. In the moment, it felt like a tactical reaction to the (political and cultural) writing that was clearly on the wall, and I’m glad I quit when I did. But in hindsight, the Muskening was not an independent story. It was just another chapter in the larger story of the rise and fall of the blogosphere and public social media. Anyhow, where to next? *** Much of the appropriate subset of my nonfiction writing, of course, has already migrated to my substack, which is already five years old (time flies, huh?), but still doesn’t quite feel like home, and I suspect never will. At least not in the way this blog has felt. Mostly what I write there has to do with technology trends and serialized book-length projects. There’s not much left here that’s suitable for substacking. It’s mostly stuff that does not look good when it puts on a suit, so I probably won’t try to put it there. These remaining active threads on this blog — narrative theory, fiction experiments, mediocrity musings, outtakes from my lab tinkering — don’t fit Substack and have never quite fit the blog medium either. They need transplanting to media where they can flourish. Lab tinkering really belongs in video, and in cozy conversations with small groups of co-conspirators, such as I’ve been able to find via the Yak Collective (also already 4 years old). Fiction really does not like the reverse-chronological tendency of the blog, and wants to create its own escaped reality, complete with its own temporality, in pre-temporal media spaces. I’m not yet sure where to take my fiction experiments. I suspect I’ll be parking ongoing efforts either in Roam or in novella-length ebook drafts. A few non-fiction threads — narrative theory, mediocrity — will likely eventually turn into monograph-style ebooks/books. They’re kinda done, really. They just need a round of editing and packaging. That leaves a few loose ends which don’t really belong anywhere in the future at all. They feel like increasingly fussy footnotes, not just to my own archives here, but to the blogging era itself. The impulse to post here now feels like an archival impulse. An impulse to tend to the needs of a gradually settling and cooling past, rather than the needs of a restless and warming future. Garbage collector janitorial work in a memory palace where the temperature is dropping slowly to cold-storage levels. I’ll be redirecting the energy fueling the memory-curation impulse towards gradually putting together a decent collection of book-length volumes from the archives. And perhaps a fine-tuned LLM that can serve as a ghostly era-bound after-image of myself too. And maybe a glossary and a set of footnotes too, why not. With my twitter archives (also sitting around) thrown in for good measure. They belong spiritually with this blog. I look forward to chatting with the Ghost of Ribbonfarm+vgr-on-twitter in the metaverse, whenever that option is available and cheap enough to exercise. For now, I’m starting out on this memory curation project by working on a book of key posts spanning the entire life of the blog. A selection I hope will tell at least my subplot of the larger story this blog belongs to. Taxidermy-ing the twitter archives (particularly the threads, not so much the conversations) into a suitably mummified form is a tougher challenge I haven’t yet figured out. But the memory curation is, to be frank, a backburner priority. Retiring this blog is a poignant moment for me, but I’m fundamentally a future-oriented guy. These half-assed retirement plans still leave an important function of this blog unaccounted for: As the default outlet for all my random impulses for nearly two decades. What I’ve called my salt-seeking tendencies. Without this blog serving as a sort of /etc folder, I will be sort of cognitively homeless after November 13, since such impulses are probably my most primal ones. It might sound silly, but “where will I put my random 2x2s now” is a real and serious question for me. I have no idea where I’m going to put my random hare-brained theories, 2x2s, crappy maps, bad cartoons, and so on. Substack and post-Twitter protocol media don’t feel right. Neither does a private notebook. There’s a decidedly social element to that kind of /etc salt-seeking thinking. It thrives best when embedded in global and public distributional media as shitposts, ready to trigger Cunningham’s Law dynamics (“the easiest way to get the right answer is to post a wrong answer”). It struggles along in freshwater publishing media like books and newsletters. And it wilts entirely in private media like notebooks. A bathtub is no place for ideas that yearn for the ocean. There’s no fun or dopamine in making a 2×2 if you can’t immediately share it, as publicly as possible. If you have to put it in a suit to send out via substack, or limit it to niche cozy media, or pay Elon to put it out in the post-apocalyptic sewage stream that is “public” social media, it doesn’t feel quite right. I’m sure having no outlet for this core part of my thinking will drive me nuts, but that’s a good problem to have. You have to harbor a certain inner restlessness if you want to be a good homeless nomad. Maybe I’ll start putting certain thoughts down on paper, and tossing them into the ocean in plastic bottles. Luckily I live by the Pacific Ocean, the biggest tank of saltwater around. And I’m told they just cleaned up the garbage gyre, so we can start making a new one, and killing a whole new generation of turtles. Speaking of nomadism, a curious inversion is underway in my life. I’ve been digitally at home here on ribbonfarm for 17 years, but a nomad in my physical life, having lived in 23 apartments in 10 cities over the last 27 years. If my cunning plans work out, that will flip. I’ll be digitally homeless once more (as I was 2000-2007), but hopefully manage to buy a house mansion within the next year if this damn housing market thaws soon. Ideally in the Seattle area. We’ll see. As I said, I’ll post a final roundup sometime in the next month, and perhaps some sort of final epitaph post on the 13th itself (rather appropriately, I’ll be in Thailand that week, a country that will likely go down in history as a sort of blogger Mecca during the heyday of public social media, when blogosaurs ruled the Earth). In the meantime, thanks for coming along with me on this long journey, and hope to see you at my rental apartment on Substack. I’m somewhat active in the Notes section there, and also on Farcaster. I’m on Bluesky too, but not very active there (something isn’t quite working for me there). Share Share this Share Share on Facebook 28 Comments Get Ribbonfarm in your inbox Get new post updates by email New post updates are sent out once a week About Venkatesh Rao Venkat is the founder and editor-in-chief of ribbonfarm. Follow him on Twitter Comments Aneesh Sathe says October 10, 2024 at 7:23 pm Long time reader first time commenter. I’ve been reading Ribbonfarm since the very early days. It’s been an unexpected lighthouse in the stormiest of times. In others it was simply the most fun stuff to read. Thank you very very much for the many good years Venkat! Reply Tim says October 10, 2024 at 8:38 pm Good luck to you, and thanks for all the thought-provoking posts over the years. Reply Naveen says October 10, 2024 at 10:11 pm Legend. Ultrapremium mediocre+1 content. Thank you for your humor and wit most of all. It has been a pure delight reading Ribbonfarm. Reply Kd says October 11, 2024 at 12:16 am The next chapter should obviously be a chatbot trained on ribbonfarm so far Reply adi says October 11, 2024 at 5:04 am Legendary run vgr. So long and thanks for all the fish! Reply HMSaïd says October 11, 2024 at 6:51 am Thank you, Venkat, for this gem. Reply Mark says October 11, 2024 at 9:16 am While Ribbonfarm may have been a beneficiary of zero interest rate policy, most blogs suffered from zero interest. Well done, and cheers! Reply Max says October 11, 2024 at 10:28 am Interesting that you’re taking the exact opposite approach to Mandy Brown’s excellent essay Coming Home (https://aworkinglibrary.com/writing/coming-home) where she posits that having your own “container” for your thoughts and content gives you more freedom than trying to cram your various thoughts/2x2s/essays etc across a never-ending undulation of commercial services. As the meme says “why not both? ¯\\_(ツ)_/¯” – write here, syndicate elsewhere. Reply JJN says October 11, 2024 at 1:16 pm Well, that’s both a surprise and yet not completely unexpected. As much as the reader in me will miss having new blogs to read, I fully understand and appreciate your decision. I think it’s far smarter to let go of something at just the right time than hold on for too long. Thank you very much for all of your writing and best of luck in your future endeavors. Reply Mr RIP says October 11, 2024 at 2:20 pm You’ve been one of the greatest source of inspiration for my writing and my creative endeavors in general. I didn’t want to read this post :( Thank you for everything you’ve done (hopefully, only “so far”), from your amazing quor answers to the Gervais Principle, from Breaking Smart (loved it so much!) to “A Text Renaissance”. Thanks again Venkatesh, I wish you good luck with your next adventure. Reply Roger James says October 11, 2024 at 3:17 pm Thanks I cam to your blog rather late in your 17 year journey (I only had the time/opportunity to read eclectically on retirement. But the more to substack whilst rational for you ($) is irrational for me – there is no syndication and the cost (vs benefit) is too much My favourite work was Truth in Inconvenience and the role of Mecanno – our society is gradually eroding the art of muddling and fixing things in favour of an exact fit plug compatibility (Lego). The instruction driven, custom part Lego experience – such as The Millenium Falcon – is the antithesis of learning by bodging and the genesis of creativity. Oh well! Reply Richard Meadows says October 11, 2024 at 6:35 pm Congrats Venkat! I am perhaps an unusual reader in that I’ve never actually followed the blog, but have bought and enjoyed every single of your ‘ribbonfarm roughs’ series. So I selfishly request that you do collate and republish some of the best stuff from the later epochs. All the best Reply Ernest Prabhakar says October 12, 2024 at 1:24 am Thanks for the mummeries! FYI, I’ve satisfied my itch for semi-public postings of inane ideas by emailing them to mostly-dead Google Groups. Lame, but it kind of works. Reply Visakan Veerasamy says October 12, 2024 at 1:14 am I actually think the 2x2s would go great as a shitposty Instagram meme account, have you ever encountered some of those? Will send you egs on there when I find them So long and thanks for all the fish, I owe Ribbonfarm more than I can pay 🫡 Reply Eli Schiff says October 12, 2024 at 5:37 am In a way your decision is right, partly due to historical contingency as you argue—partly due to your strategic audience fragmentation. Nonetheless it’s regrettable it came to this. May you get a proper send-off for your contributions. Reply Wiseman Zondi says October 12, 2024 at 6:31 am As someone who’s inspired me as a writer and as a thinker, this saddens me. But better to go out on a high than to peter out. Congratulations Ventakesh. You’ve built something great, and I speak for a lot of folks when I say we’re glad this platform existed. Reply sepiatone says October 12, 2024 at 9:16 am thank you for the music Reply Kevin W says October 12, 2024 at 3:11 pm Sad to see it go, but feels reasonable. Thank you for the content Reply Eagle says October 12, 2024 at 6:27 pm Damn, I guess I’m late. I’ll just wait for the tour guide. Reply Annie Mueller says October 12, 2024 at 6:50 pm Thanks for all you’ve shared. Perhaps micro.blog might work for your 2x2s. It’s a good hybrid. Reply Tyler says October 14, 2024 at 6:24 am Venkat, Thank you for all of your work here on Ribbonfarm – this was simply the most interesting corner of the internet. My best to your future, Tyler Reply Tom Crick says October 15, 2024 at 12:13 am Good luck with the retirement and I hope that you have no trouble with any backup house-buying plans if the finances for your first choice are discovered to be unviable. Since Ribbonfarm has been a strong influence on many IT thought leaders have you given any thought towards how the blog might persist beyond your and the Internet Archive’s combined lifetimes or if you think it even ought to? Reply Arnobio Morelix says October 15, 2024 at 2:35 pm Ribbonfarm has been amazing. I have been following for years and am so grateful for what you share. (hehe. came out as a LinkedIn comment) Lots of good memories! ‘premium mediocre reward program’ (lol), Toby as JC (!), and I frequently reflected and applied much of the stuff here. I used your ‘build your own rules for life’ to get out of a funk last year, on a difficult transition as a startup founder / moving countries / life-after basic mansion asap and it was really helpful. Today I told my wife (a real anchor for me: barely online, ‘normal’ job) “that blogger I really like is retiring. I am not sure how to feel about it.” I am not sure how to feel about it. Best of luck! Reply Joel Dueck says October 15, 2024 at 5:36 pm Likewise long time reader. Thanks for writing here, for the thought-provoking interactions on Twitter over the years (I also left) and for building out a stable of other interesting writers. I never saw anything like it that didn’t also have some kind of print publication aspirations. I’m tempted to scrape my favorite posts into a book for my own use. I’m trying to follow along on Substack so I don’t lose track of your next thing but something isn’t working for me there either. Reply Karthik says October 15, 2024 at 9:56 pm Hi Venkat, I have given Ribbonfarm a lot of my “attention”, particularly in the 2010-13 period. And TBH, received a lot more in return. I learned a lot here, and many mental models of mine took their first step from something I read here. Sincere thank you. Reply Catherine says October 16, 2024 at 9:44 pm Wow. I just stumbled upon this, specifically: https://www.ribbonfarm.com/2009/10/07/the-gervais-principle-or-the-office-according-to-the-office/ which somebody or something linked me to, and which I heartily agree with and love (the piece, not what it means for the world haha) and wish was talked more of in general discourse… I fell a bit in love with this blog already. I can’t believe this blog is retiring and I have just discovered it. Well, I’m sure there will be many more like me to come. Reply Victoria Allison says October 17, 2024 at 4:03 pm Thanks for your fabulous insights! I’ve only read ‘The Gervais principle’, but have just purchased all of your books from Amazon. And I regularly recommend ‘The Gervais Principle’ to my colleagues when they are puzzled by their workplace experience :) Reply mm says October 20, 2024 at 7:38 am longtime reader first time commenter, RIP ribbonfarm so long and thanks for all the posts Reply Leave a Comment Name * Email * Website This site uses Akismet to reduce spam. Learn how your comment data is processed. Blogchains Lunchtime Leadership Ribbonfarm Lab Psychohistory Zemser Into the Pluriverse Lexicon Mystifications Narrativium Fiction Clockmaking Subscribe Get new post updates by email Crash Early, Crash Often Be Slightly Evil Gervais Principle Tempo Meta Log in Entries feed Comments feed WordPress.org Return to top of page Copyright © 2024 · Prose on Genesis Framework · WordPress · Log in",
    "commentLink": "https://news.ycombinator.com/item?id=41889876",
    "commentBody": "Ribbonfarm Is Retiring (ribbonfarm.com)169 points by Arubis 23 hours agohidepastfavorite118 comments blfr 21 hours agoIt seems to me that the blogosphere was not a ZIRP but rather a young Internet phenomenon. Which could exists, like usenet before it, when mere access to it was a filtering mechanism. Once you have seven billion people with virtually no access control, you can't have a public blogosphere, and groups retreat to the cozyweb. Either way, I enjoyed it while it lasted. Thanks for the Office series! https://www.ribbonfarm.com/2009/10/07/the-gervais-principle-... reply spaceman_2020 13 hours agoparentThe public blogosphere died because of Google, simple as. Once search stopped prioritizing blogs over bloated, SEO-optimized slop, traffic to blogs died and discoverability was limited to you spamming your posts everywhere. reply lenderton 9 hours agorootparentGoogle bought Blogger in 2003; that was the main blogging service for a long time in the US, at least. Google not being able to optimize search results for their own hosted content implies...a lack of overall content available. It used to be a lot better in the earlier days, for ROMs and especially album downloads blogs would pop up all the time in the results. It's just that there are a lot more people online now, and more accompanying spam, and also the fact that Google delists any sort of \"download\" type site by default - which used to be a substantial subcategory of blogs. People forget how \"tightened up\" the web is nowadays, or many just aren't old enough to remember. It wasn't 10 years ago that Fox News would archive full, uncensored ISIS videos on their website - which is kind of insane, when you think about it. Average age of first phone ownership is really young these days, which is arguably the #1 factor in everything online becoming a closed system for sheep. On the other hand, no parents want their 11-year-old wandering into videos of hostages being burned alive. Or getting solicited for photos etc. So the internet is kind of dying for the sake of real life. You can always search old blogspots. https://www.searchblogspot.com reply throwaway14356 13 hours agorootparentprevYou forgot technorati! They lost the plausible deniability there. reply PaulHoule 5 hours agorootparentBack when the rat was in effect, the rat would always index the splogs I made but I could never get it to index my non-spam blogs. reply bartread 21 hours agoparentprev> Once you have seven billion people with virtually no access control, you can't have a public blogosphere, and groups retreat to the cozyweb. Why can’t you? There’s a logical leap in this statement I don’t follow. reply rogers12 21 hours agorootparentThose seven billion people aren't very good for the most part, and include a critical mass of spectacularly awful people. It turns out that public access forums calibrated for the small and self-selected community of mostly high quality internet pioneers aren't prepared to deal with 1000000x expansion of reachable audience. The Eternal September effect has been getting stronger ever since it's first been observed. reply enugu 13 hours agorootparentIt's not just that the social media is filled with low on substance posts with excess anger and snark, but this incentivizes everyone to be more forceful - as otherwise the louder voices can dominate the discussion. So, it's not just a quality of people issue but also an emergent dynamic which encourages tribalism instead of substantive posts. The same people can make reasonable posts in other contexts This need not be a unsolvable problem, and that one has to retreat places like HN relying on a single moderator(good, but doesn't scale). One can also rely on timelines/feeds being based on Distributed moderation - A user selects moderators or custom-algorithms who they find valuable. The moderation can be along different dimensions like accuracy, interest, or aligned with some political view. There could a moderator whose style is to purely check the soundness of the reasoning without taking any position on the issue itself. This can lead to improved standards of discussion. A key issue is how to reduce the energy required to moderate - typically a moderator evaluates the quality and rely on networks of other moderators each handling smaller domains. Current discourse encourages users to sort into strongly polarized groups, whereas more nuanced feeds in social media can lead to coalitions which don't neatly align with the standard fault lines. Platforms like Polis actively encourage common points of agreement across different groups. reply rtpg 19 hours agorootparentprevThere's a gap between public fora and the blogosphere though. Generally speaking there are plenty of blogs that get linked in places like here. Blogs just don't have comment sections hosted on their own as much anymore. Having discussions happen in separate places is also interesting, because the HN convo and some subreddit convo will be different, for example. There's a lot more mainstraeam stuff but I think niche communities still exist. Glibly, we're not a part of most of them on account of having gotten older. Or we are a part of some, but there's plenty we're not seeing. reply throwaway14356 13 hours agorootparentprevI had a hilarious time deleting comments from people who don't believe the topic is real. Specifically hard to believe stuff made the experience funny. People got really mad in private messages. As if it is a god given right to complain about others talking about something so elaborately that the conversation dies. All scaling issues solved. If you want to talk about the garden gnome liberation front you must believe they need to be rescued. reply whatshisface 21 hours agorootparentprev>It turns out that public access forums calibrated for the small and self-selected community of mostly high quality internet pioneers aren't prepared to deal with 1000000x expansion of reachable audience. \"Checklist for new theories purporting to prove that the social web is presently unworkable:\" ... 26. The predicted conflicts still wouldn't be as bad as Usenet flamewars. 27. Your theory proves that Hackernews does not exist.It would turn into a smoking hole in the ground if it somehow caught worldwide attention. This seems untrue? Of course I like HN, but from the perspective of a typical person, HN is an ugly, hard-to-use website with \"news\" that caters to a small fraction of the population and is likely quite uninteresting to the rest. I think this is why it manages to stay roughly the way that it is - that and extremely thorough and strict moderation to keep it that way. reply GavinGruesome 8 hours agorootparentprevFluids can't be fragile. reply ddulaney 20 hours agorootparentprevAs the guidelines [0] state: > Please don't post comments saying that HN is turning into Reddit. It's a semi-noob illusion, as old as the hills. See the link for some examples, but I can also recommend looking at some old front pages from over the years and poking through the discussions. Unscientifically, it seems that quality is pretty similar to me. [0]: https://news.ycombinator.com/newsguidelines.html reply Dylan16807 18 hours agorootparentContext. That's a rule for jumping into a conversation and making petty putdowns. It doesn't mean \"if someone says HN has never been better, you're not allowed to disagree\". reply philwelch 19 hours agorootparentprevMy HN account is older than either of yours, so I don’t think I can be dismissed as a “semi-noob”. rogers12 is mostly correct, sad to say. dang has done a good job slowing the decline (and I actually noticed an uptick in quality when he first took over) but HN is past its peak. reply kelnos 13 hours agorootparentMy account is a few weeks older than yours, and my opinion is pretty much the exact opposite of yours. I still get a ton of value out of HN, even after over 15 years. I visit multiple times a day, and genuinely enjoy reading articles and comments, and joining the comment threads myself. It's not perfect; there's certainly annoying crap, bad-faith posters, trolls, spam, LLM-generated junk, etc. But (with the exception of the LLM-generated junk) none of that is new since I first started hanging out here. Overall the quality of discussion (like this one!) is still quite high, and there isn't another news/interest/discussion site on the internet where I spend anywhere near as much time, even after 15+ years. (I'm not going to argue about whether or not it has \"peaked\", since that's not a particularly useful measure. If quality is a scale from 0-100, and we already hit 100 but are now hovering around 80, stably, then who cares if the peak is in the past? The quality is still fine.) reply tptacek 18 hours agorootparentprevWhen was its peak, what would you say characterized that peak, and what are some clear indicators of the decline? reply FreakLegion 13 hours agorootparentIt feels like some threshold was crossed in early 2023. That's when I noticed it, at least, in the long, crazed threads on the SVB bank run and murder of Bob Lee. There've been a lot more of those low-quality discussions since, but the good parts of HN are still here, too. reply philwelch 5 hours agorootparentprevThis is a good question that probably deserves more thought and effort than I can apply to it. I would say that when HN was at its peak, the overall vibe of the commentary reflected the perspective of people who people who built things, while the overall vibe today reflects the perspective of people who like to go online and bitch about things. There’s a famous email exchange—I’m sure you’re familiar with it—where someone writes to Steve Jobs complaining about a bunch of things Apple was doing, and they go back and forth a few times, and Steve Jobs finally gets annoyed and writes back, \"By the way, what have you done that's so great? Do you create anything, or just criticize others’ work and belittle their motivations?\" HN these days is absolutely full of people who don’t seem to do anything but criticize others’ work and belittle their motivations. I’m not saying that HN never had unfair criticisms in the good old days—the “middlebrow dismissal” has been a trope here for a very long time—but we didn’t use to have entire threads filled with nothing but middlebrow dismissals. And we also had tons of people criticizing Apple, but then again most of them were complaining that Apple was actively interfering with their attempts to create things (e.g. the uproar over arbitrary and unfair App Store moderation policies). The clearest indicators of decline to me have been the signs of evaporative cooling. Maybe I’m falling into a different common fallacy by saying this, but I do think HN was a lot better when the old regulars—yourself included—were more active. I don’t exactly blame you guys, but it is an indicator. reply MichaelZuo 19 hours agorootparentprevThat doesn’t seem to be the claim, just that the average quality is trending downwards just like reddit. It’ll probably never converge because reddit is getting worse at an even faster rate. reply Uehreka 19 hours agorootparentprevQuoting the HN guidelines at people is a semi-noob practice, as old as the hills. reply mrob 12 hours agorootparentprevI think you're underestimating how effectively the old-fashioned text-based design repels users who would lower the quality of the site. (Although as Usenet proves, high-quality moderation is also necessary.) reply Nihilartikel 20 hours agorootparentprevWeeellll. Not every forum has a dang. Just saying. reply immibis 20 hours agorootparentAlmost every one does. reply kelnos 13 hours agorootparentMost are nowhere near as thoughtful and effective as he is, though. reply dangerwill 19 hours agorootparentprevGiven that hn is the forum of yc, I think we should not feel comfortable with it's trajectory even if dang does a great job moderating. Garry Tan is in the ceo chair here and he is currently advocating for a purge of the homeless, democrats, and \"anti-tech\" people from San Francisco. A Republican who is too ashamed to admit being a Republican (preferring Grey vs Blue or the network state concept) , who drunkenly tweets death threats at his political opponents is not trustworthy. reply Arubis 19 hours agorootparentHN’s association with YC has felt looser every year for over a decade at this point. If not for the Jobs link, the subtle username colors, and the domain, it’d almost be forgettable. reply kelnos 13 hours agorootparentprevI'm not sure how much things have changed, but when HN was semi-spun-off into its own autonomous unit inside YC, dang was given the option to change reporting structure so he'd report directly to the board (and not CEO) if he ever thought that was a better arrangement. If he hasn't done so, then I trust that he hasn't felt he's needed to, and that YC's leadership hasn't meddled in HN's operations. And if he has pulled that trigger, I expect things are still fine, else he'd leave and go elsewhere. reply add-sub-mul-div 21 hours agorootparentprevYeah. That's why Twitter is useful as a kind of flypaper or quarantine. Let the passive stay and let the deliberate find new spaces that can be good the way Twitter once was. If Twitter was to go away, places like Bluesky would unavoidably get worse. reply lenderton 20 hours agorootparentMy experience with Bluesky has been similar to my experience with other \"disruptive\" platforms like Cara (the anti-AI art portfolio app/site). When a \"new\" (usually overall non-corporate) internet space opens up that, in theory, caters to a broader audience, the most immediate colonizers are the type of people that have some sort of \"underground\" bent to them - subcultural things like furries, erotic artists, etc. Opening up Cara produces an avalanche of large-breasted foxpeople, and the last time I opened Bluesky I was met with a photo of what appeared to be a boy in his underwear. Mastodon has its dubious reputation also for child pornography. I'm just saying, the mainstream internet is moderated for a reason. Being mainstream, there's money behind it, and with money comes power - this results in moderation that is usually politically motivated, and so in recent years there has been an exodus of the masses to low-moderation platforms like Tiktok, or things like Kick for younger users. When a platform or site is staffed small, such that it cannot afford to moderate, it will be suffocated by the \"undesirable\" groups I mentioned, earlier, as though they were some sort of choking algae. There are so many of these people \"empowered\" these days that, from what I have seen, it is really hard to start new social media sites without corporate resources. Twitter is already plagued with OnlyFans bots due to being smaller now, and streaming platforms are forced to aggressively build themselves to be resilient against similar sexual content creators, who are the first people that show up. Most times these creators will be working for an organization. In the end...could Twitter have existed in a non-sh*tty form in the first place? It was rapidly approaching bankruptcy when Musk was (in the end) forced to purchase it (lol). If not him, someone else would have acquired it, probably a corporation, and monetized the content to keep it afloat. I think in the end, the landscape is going to look more like Tiktok (computerized moderation) for anything beyond Meta. Smaller social media platforms will be seedy and not widely populated. Forums will continue to be used by countries with their own internet ecosystems, like Korea or Nigeria or Finland, but not really exist in global lingua franca English beyond a handful of major ones like SomethingAwful. reply julianeon 19 hours agorootparentBluesky and Mastodon, for the average user, are G rated compared to the avalanche of smut on Twitter/X. reply tbrownaw 18 hours agorootparent> avalanche of smut on Twitter/X I haven't seen this. Maybe you only run into that sort of thing if you go looking for that sort of thing? reply julianeon 16 hours agorootparentEven if you accept this at face value (I don’t) note the problem: on Bluesky and Mastodon, you have to look for racy content and then follow it on purpose. It must be a deliberate, intentional choice. For the average user, my experience has been that Bluesky and Mastodon are, if anything, too tame and boring. Whereas Twitter/X is pushing for whatever brings engagement, damn the consequences. reply aaronbrethorst 13 hours agorootparentprevIt’s incredibly hard to avoid graphic images of dead children if you even lightly engage with political content on Xitter reply add-sub-mul-div 17 hours agorootparentprevThe top replies to any big viral tweet are Onlyfans models and other spammers. reply tbrownaw 15 hours agorootparentI usually have to expand the \"more replies\" and often also the \"probably spam\" sections to see those. reply kelnos 13 hours agorootparentprev> It was rapidly approaching bankruptcy I don't think that's a foregone conclusion. > when Musk was (in the end) forced to purchase it And because it's private now, we have no idea what its financial situation is. My expectation is that they're much worse off financially since Musk's acquisition, even after shedding most of the staff. A lot of people I know predicted Twitter would be completely shut down within a year of Musk's acquisition. I wasn't quite so quick to agree, but I think we're still going to get there eventually unless Musk drastically changes course. reply AlexandrB 10 hours agorootparent> A lot of people I know predicted Twitter would be completely shut down within a year of Musk's acquisition. As one of those people, I'm definitely eating crow. Three things happened that bode well for Twitter's future: 1. Musk has attracted a loyal core of true believers that think he has saved free speech with the Twitter purchase. 2. The Overton window of online discussion has started moving right. In particular, companies are becoming less interested in toeing a left ideological line with their ad spending. 3. A bunch of people who hate X and hate Musk and his politics stayed on Twitter! To me this is most surprising of all, but perhaps shouldn't be because many of these same people posted to Twitter in the past while simultaneously calling it things like \"the hellsite\"[1]. I'm no longer sure we'll ever \"get there\" other than if a new paradigm marginalizes all of social media the way social media marginalized blogging. [1] https://samkriss.substack.com/p/welcome-to-hell reply LightBug1 4 hours agorootparentProud to be one of those that ditched \"the hellsite\", and it remains ditched. One of the best moves I've ever made. reply kQq9oHeAz6wLLS 4 hours agorootparentprevRegarding your 3rd point, this is the same behavior we see when people say they're leaving the country if X candidate wins an election. They never leave, because change is hard and they're addicted to the attention they get when they complain. reply add-sub-mul-div 17 hours agorootparentprev> It was rapidly approaching bankruptcy No. It was profitable in 2019. Under the old ownership it could have easily become profitable again by correcting the overhiring and not pissing off advertisers. reply MichaelZuo 19 hours agorootparentprevBingo, the problem is that with a world population of 8 billion, there are easily 8 million people who genuinely do want to see vast amounts of furry porn the moment they open up an app. Filtering out even a tenth of them, say 800 000, just takes too much effort for a startup, so there’s no viable pathway without being incredibly popular and scaling incredibly quickly to just drown out all the unpalatable users. i.e. Tiktok reply lstodd 17 hours agorootparentprevBeing in internets from before there were internets... I just cannot believe what I read nowadays. This tldr I reply to is especially pathetic. If your beliefs are so fragile, if someone's post can crush them then you must, MUST question yourself, not some irrelevant JSON sitting who knows where. Otherwise you do not deserve any respect or attention. You do not even have a right to be listened to. This thing is called self-respect. If you do not have it then you are nothing. It follows .. well 4chan follows. For some time I and many others thought that this mocking taken to extreme would tell people basic truths. Alas. Still we had some fun reply bhouston 19 hours agorootparentprevI think that AI generated personas who push an ideological direction on anonymous forums are more of a threat that just stupid people. Eg: https://theintercept.com/2024/10/17/pentagon-ai-deepfake-int... reply njtransit 19 hours agorootparentprevThere is a NYT op ed titled “The Tyranny of Convenience” that covers the phenomena well. reply shagie 18 hours agorootparenthttps://www.nytimes.com/2018/02/16/opinion/sunday/tyranny-co... reply 45y564hn54 20 hours agorootparentprevnext [3 more] [flagged] xterminator 17 hours agorootparentAre bloggers required to meet a quota? reply Arubis 19 hours agorootparentprevThere may be examples of this, but picking on Venkat Rao for not being sufficiently prolific is a laughable argument. reply philshem 3 hours agoparentprev> ZIRP stands for “Zero Interest Rate Phenomenon,” … https://www.ycombinator.com/library/LC-what-is-zirp-and-how-... reply seltzered_ 15 hours agoparentprevFor those missing context: > \"This blog was sponsored by ZIRP. The future historians who dive into these archives for archaeological research will likely be economic rather than cultural historians, trying to reconstruct the play-by-play impact of ZIRP. Many of the big hits of this blog, such as The Premium Mediocre Life of Maya Millennial, and The Locust Economy (a forgotten hit from 2013) had ZIRPy subtexts.\" I think the author might he referring to their own blog (ribbonfarm) as a ZIRP phenomenon, not the whole blogosphere. reply lacy_tinpot 18 hours agoparentprevDon't think it's really systematic. I think this is just a generational cycle. Plenty of content coming from newer people on other platforms through other mediums. reply p3rls 17 hours agoparentprevEh, even in his niche we have people like Gwern pioneering in the aesthetic web movement. I'm not sure I buy the web is dying so much as the cultural conversation revolves around the larger platforms ($$) along with in general web discoverability getting worse. reply tuatoru 20 hours agoparentprevSubstack is doing OK, I think. It's the intellectual child of the blogosphere. reply philwelch 19 hours agorootparentSubstack—and the surviving blogs that aren’t on Substack—are still going strong, and I don’t think they’re going to go away. Maybe Substack as a platform might decline the way Medium has. But the “Eternal September” types are usually either functionally illiterate or don’t like long form reading, and social media is increasingly optimized for those people. If you actually take the effort to write down your thoughts in text, you end up filtering for people intelligent and diligent enough to choose the written word even when video and photo content is readily available. reply RiverCrochet 5 hours agorootparent> If you actually take the effort to write down your thoughts in text, you end up filtering for people intelligent and diligent enough to choose the written word Interesting point. But, ChatGPT/AI summarization and Tiktok-style speech-to-text that the \"Eternal September\" perferrers are being trained to eat up. What is your thoughts on that altering the dynamic? reply immibis 20 hours agorootparentprevSubstack (together with Medium) appears to be the blogosphere. As usual, venture capitalists managed to take over an open protocol and turn it into a singular product. reply mattgreenrocks 19 hours agorootparentSurprised you lumped Medium in with Substack. I always associate Medium with C-tier tech tutorials at best these days. reply immibis 10 hours agorootparentThey are both neoblog platforms. reply lenderton 19 hours agorootparentprevI've read some A-tier libertarian noir novellas on there. reply kelnos 13 hours agorootparent> A-tier libertarian It's been a while since I've seen an oxymoron as perfect as this one. reply unsungNovelty 43 minutes agoprevVery curious. I started my personal website with the rise of Hugo (which I like to call as \"the new age of SSGs\"). 5 years ago. Time does fly. As I look more and more into the old days of anything, It's like, things survive long enough to get popular... Then it goes back to where and how it was. It doesn't necessarily die. I am curious if we had the expectation that if hosting and other tech became accessible, more people must've been blogging or maintaining personal websites. That is not how it works right? Pepole maintaining websites on their on were a niche in that time of the internet. And it is still a niche in this time of the internet. Fountain pens for example was the only way to write at one point. And once ball point pens came along which is more managed, common folks went behind it. Fountain pens didn't die. It has became more luxurious or a premium item once again. It's still thriving and there is a whole new world of fountain pens if you start digging about it. Heck, new brands have emerged. I can see the same parallels for bikes (MTB or Road Bikes etc) as well. More people will discover blogging and the blogosphere. And some of them will go away. Some of them stays. Some lives and some dies. Just like humans, websites will live and die. It is obviously sad when I see someone stop blogging or stop maintaining their personal website. Since I still consider myself young to the blogosphere, It's like reading up on celebrities you don't know about after their death. But it's way of life. Life of a blog. reply belzebalex 10 hours agoprevThis is quite sad, I've been very inspired by the writings of Artem Litvinovich [1] (although he stopped publishing 8 years ago). I inspired a lot of my research on what he did. https://www.ribbonfarm.com/author/artem/ reply cheschire 20 hours agoprevHaving access to wikipedia on a phone everywhere you go is what killed the bar conversation. No longer did you have to compare notes and argue over beers to remember trivia. And in that same way, no longer do people have to ramble on into the aether in blog form to work through some shit. Now they can do that with ChatGPT and actually get responses to their thoughts in real time. And most of the time it's agreeable in tone. Tech continues to change the world. Maybe that isn't what is contributing to this particular blog dying, but I bet it's contributing to the larger community of blogs dying, which has probably created some inertia. reply lenderton 19 hours agoparentFrom what I can tell, highschooler and younger, there's almost a complete abandonment of mainstream social media in favor of self-curated chats like Discord, and it revolves heavily around gaming. A sort of hearkening to the AIM days, which is naturally what you'd expect from individuals who socialize in friendgroups that are developed beyond \"work drinking buddies\", lol. But in general, without being too doom-and-gloom about it, and perhaps this is because of the election going on, it does feel like there is a greater trend going on of internet users stepping away from social media. There's no easy way to divert this weariness back to specialized forums a la the 00's or 90's, though, which is probably where everything should be for the internet to remain useful. This is exacerbated by the fact that 85% or so of internet traffic is phones, resulting in discussions being comprised of back-and-forth thumbtap-quality posts that nobody (including the senders) really seems to care about. It's also exacerbated by the fact that search engines cannot seem to index traditional message boards or wordpresses etc. properly; there are too many of them nowadays to navigate (most being identical templates like vbulletin). reply mannymanman 18 hours agorootparentDo you have any sources to read/learn more about this phenomenon? Would be great to understand why reply lenderton 13 hours agorootparenthttps://www.pewresearch.org/internet/fact-sheet/teens-and-so... https://influencermarketinghub.com/discord-stats/ Average age is 16 on Discord, average time spent per day is less than 10 minutes, so it's being used as a messaging service (but connected to a greater gaming-type ecosystem). 90% of servers are less than 15 members. 30% of teens use it, which is significantly higher than the rest of the population. I don't really have anything concrete to point to for my general feeling about American society slowly moving into a post-social media phase. Tiktok falling into relative unuse with most Americans except Hispanics is probably a main point of data. There hasn't been anything emerging to replace it besides (according to studies) the more cordial YouTube, which you cannot really say is a social media site. It is the most widely used of all of them, though, something like 94% penetration. reply satisfice 17 hours agoparentprevOn the other hand, the smartphone has enhanced the culture of watching TV and movies at home. It is acceptable etiquette in my house for any viewer to pause the show and read out the results of a web search about the writer, director, plot, history, concepts, etc. related to the flick at hand. reply kelnos 13 hours agorootparentOof, you consider that an enhancement? If anyone paused something we were watching to read something off a web page, I'd lose my patience real fast. My partner is often on her phone intermittently while we're watching something together, and even that bothers me. It seems quite sad to me that people can't put their phones down for even a half hour to watch an episode of a sitcom. reply auggierose 10 hours agorootparentLol. The argument used to be that it is quite sad that people waste half an hour of their lives watching a sitcom. reply gcanyon 6 hours agorootparentprevAre you sure that's acceptable? I do that all the time, but I think the right word for how my wife feels about it is \"tolerable,\" not \"acceptable\" :-) reply dcx 20 hours agoprevIf the claim that the blogosphere is dying is true, does that imply the public intellectual commons is dying too? I suspect that while the cosyweb is more pleasant for most, this retreat might hinder vital testing and cross-pollination of ideas, and make it much harder for people to polarize into being intellectually active. For example, I've never been an active participant on ribbonfarm, but Rao's writing has made me a little smarter and inclined towards certain vectors of thought. And you can see ripples of his work in later writing by others. What a shame it would be for this culture to be lost; while there's a lot of dross in the blogosphere, I don't know if the brightest jewels will still be possible in a future system of local, private, transient clusters of thought. reply fallingknife 17 hours agoparentI would say it's dead. Killed by a change in cultural attitude to one that sees an opposing idea as a declaration of war. Retreat into private walled gardens seems like the only option. reply ksec 8 hours agoprev>Sometime in the next few months, I’ll figure out how to move it to a lower-cost archival hosting model, probably as a static non-WordPress site, That is the part I am most interested with. I wish there is a one off payment services, for let say fixed price for 20 years where you blog and domains remains on the internet. reply tolerance 17 hours agoprevWhat I see most from people who appear very attached to the Web of 15-20 years ago is a constant iteration of values, branding (cozy, small, open), discussions over protocols and platforms, and not much writing or self expression. People don’t seem interested in “blogging” as much as they seem interested in building communities, or rather, full-fledged sovereign states. reply jes5199 3 hours agoprevI have to admit, the idea of a “viral hit blog post” sounds absolutely quaint now but honestly was that ever a good thing? reply krick 10 hours agoprevI always thought Venkatesh is often guilty of shaping the reality to fit the narrative of the current post, which often is quite random and possibly even contradicts all his other posts. This is also the case here. Unfortunately, I do share the suspicion that a lot of good things I am very much used to were sponsored by ZIRP and soon it will crash really, really hard. This thought makes me anxious, I really don't like it, but I do agree that the culture of open source, and free web, and all these things was a fleeting phenomenon. Some of it is already dead, some will die soon. But I don't believe it applies to blogging. I mean, fine, guy wants to retire. And it \"coincides\" with his 50th birthday (that's a magical coincidence for sure!) Just admit it's just about you, you are tired of doing what you did for 17 years. It's ok. But you weren't the only blogger, and everyone else isn't retiring together with you. Heck, I bet there is some new blogger writing his first post right now. I don't know if I'll like his posts as much as I liked some of yours, but I'm pretty sure life goes on without you, whether you like it or not (yeah, I know, I also don't really like the idea that life will go on without me!) Blogging is just one of the formats of how people express themselves, and while today there are many alternative formats that weren't really an option, say, 20 years ago, you cannot replace all of blogposts with a YouTube video or a TikTok. You can replace some, sure. Which isn't a bad thing either, it just means there now is a media that suits that type of content better. Also, that warning about your substack not being a blog... But it is. A different kind of blog, maybe, but let's not pretend words mean anything more than they do. Even a telegram channel with lengthy posts is pretty much a blog. And, BTW, I don't know if Scott Alexander is supposed to be an \"old media refugee\" who's posts Venkatesh doesn't read, but his content is pretty much the same as on SCX. FWIW. reply ChrisMarshallNY 19 hours agoprevI'd never heard of this blog before, but it looks like a really interesting, eclectic one, and the Web may be a bit darker, for its absence. That said... > anymore than there was a single heir to the Roman empire I'm not exactly sure I'd classify this blog at the same level of influence as the Original Italian Mafioso. reply kwerk 18 hours agoparentHe’s talking about blogs in general reply noam_compsci 11 hours agoprevLow wit: writers must have got busy Mid wit: this is a poignant soliloquy in the state of the open web High wit: writers must have got busy reply Kye 1 hour agoparentIt's like that ancient meme where a programmer starts at Hello World, ascends through corporate complexity, then comes back to Hello World. reply epolanski 17 hours agoprevI really don't get how and why are blogs dying. reply xterminator 17 hours agoparentBecause people don't go surfing on the web anymore, they stay in the same facebook-twitter-instagram-netflix-amazon-reddit loop forever. reply satisfice 17 hours agorootparentThat's not death. That's non-virality. For extroverts and fame-seekers, I guess that feels like death. But it's not. reply tsunamifury 16 hours agorootparentFalse, while you try to make this a moral point, the reality is where the audiences go the content goes. Sorry that’s just life. Source: worked on this research at Google for decades. We accelerated the issue unfortunately. reply kelnos 13 hours agorootparentThere's still plenty of audience for blog posts. The blogosphere (god I hate that term) hasn't grown at the pace of social media, certainly, but that doesn't matter. And in some ways I'd probably consider that a good thing for the medium. reply epolanski 6 hours agorootparentprevThat again only implies less popularity, if anything, not death. reply tsunamifury 1 hour agorootparentI’m sorry to belabor this but you probably don’t know how growth and death cycles work. Every day people churn in and out. As more churn out for audiences less new ones come in. Therefore it dies. reply epolanski 16 minutes agorootparentYou should be sorry for yourself with your sad patronizing attitude. reply tsunamifury 5 minutes agorootparentWhat? I’m sorry are you not on a public forum where people share their expertise? I think maybe you could just reread this as you being informed by someone who worked on this for many many years at internet scale. It’s ok to have less knowledge or be misinformed about something. We all are. It’s not ok to respond to that with insecurity and insults. satisfice 1 hour agorootparentprevI can put my content where I like. People who want my content follow the link I give them. That’s just life, too. reply kelnos 13 hours agoparentprevI agree, and I don't think they're actually dying. I think certainly they're a much smaller percentage of consumed content today than thy were 15 or 20 years ago, but I don't believe that's because readership has dropped. It's just that the pie is significantly bigger now, and other media has gobbled up most of it. I dropped off social media 5-6 years ago and don't miss it; I still read blogs and get much more value out of them. reply pryelluw 15 hours agoparentprevThe medium is shifting just like it shifted 25-ish years ago. People would write and publish on paper or fax. Then came email and websites. Eventually the blog came up because the self publishing process at the time was cumbersome and people didn’t want to be writing html. reply tsunamifury 16 hours agoparentprevGoogle made the web searchable not browsable. Social and media networks inside their walled gardens took over browsability. reply tsunamifury 17 hours agoprevAdam Curtis’s All Watched over by Machines of Loving Grace and Rivbonfarms The Gervis Priciple were foundations of my professional career growth. reply nopassrecover 16 hours agoparentThis might seem a little odd, but I’m trying to “find my tribe” of interesting thinkers to bounce around ideas with and your comments on this thread suggest you’d have some great/thoughtful/interesting takes. If you’re open it would be great to connect (email in my HN bio). reply tsunamifury 1 hour agorootparentThis is a hard journey and really one you need to go on finding people you work with. It will drive you to work with more interesting people and do harder things. What is it that you think you are missing? reply io84 9 hours agoparentprevI find the Gervais Principle very illuminating but would love to hear how you turned it into career growth. Did it motivate you to indulge in Machiavellian scheming and join the ranks of the Sociopaths? Or let you make peace with a fate among the Clueless? reply tsunamifury 1 hour agorootparentIt mostly taught me to discern when to speak game talk, baby talk or PowerTalk. This is the primary way to be promoted and seen as “part of the next level” in larger organizations. reply lukasb 19 hours agoprevI'm reading more blogs than ever. ¯\\_(ツ)_/¯ reply lenderton 19 hours agoparentBut the problem is that they have to be hosted on the same platform, which will be set up like a social media site with curated content, otherwise you'll have to spend a lot of time finding them. Maybe you get self-hosted things via github or whatnot, but that's about as non-consumerist as it'll get. And the younger generation is not doing this beyond work-related pages, so eventually the internet-as-literature phase will end. In the past you could type into Google \"new mothers discussion board\" and immediately find organic, non-corporate socialization geared towards Americans. That ease of use is sort of erroneously gone, and probably not returning. Might I ask...which blogs are you reading? reply kelnos 13 hours agorootparent> But the problem is that they have to be hosted on the same platform No they don't. I avoid Medium and Substack like the plague. I don't use those sites for discovery at all, and whenever I come across a link to a blog on either of them, I usually regret clicking it. I end up on plenty of blogs found through various sources. A few I've been reading diligently or on-and-off for many years, others I read one-off articles here and there that I've found through sites like HN. The blogosphere's death is highly exaggerated, regardless of what TFA says. > Maybe you get self-hosted things via github or whatnot, but that's about as non-consumerist as it'll get I don't see that as a bad thing. > And the younger generation is not doing this beyond work-related pages, so eventually the internet-as-literature phase will end I don't think it will. I think there will always be enough people writing interesting long-form articles to satisfy my curiosity. Mind you, I don't exclusively read blogs, but I haven't touched social media in 5 or 6 years and that hasn't caused me to run out of interesting things to read. reply lenderton 12 hours agorootparentThe \"Blogosphere\" represented something like Livejournal. You'd write your daily thoughts, not so much imagepost at all, and there would be your friends doing the same thing whom you'd check on. Some countries still have internet ecosystems like this; NAVER is the main one I can think of, in Korea; Livejournal itself was sold to a Russian buyer nearly 20 years ago and lives on in that country mostly intact and widely used. But longform internet posts are gone nowadays in America, largely - due to English. It's too widely spoken for things to be found easily. Think about finding information about heavy metal concerts in Finland: that's probably doable from Google purely based on the fact you're not imputing English. Last point: https://www.statista.com/statistics/990899/livejournal-users... reply collinvandyck76 5 hours agorootparentprevSelf hosting a newsreader has been remarkably satisfying. Over the past few years I've been adding blogs to as I encounter them here and in a few other places, and it's one of my favorite places to spend a few hours browsing through things other folks spent the time to write. I kinda feel like everyone should have something like this, and I deeply regret Google Reader's retirement for that reason. reply lukasb 13 hours agorootparentprevCrooked Timber, Daring Fireball, kottke.org, Econbrowser, DSHR's Blog, Jayarava's Raves, Pushing Ahead of the Dame, A Collection of Unmitigated Pedantry, Michael Tsai, Scripting News, Simon Willison's Weblog ... about 150 feeds in total I think (some of them inactive now but overall more than I can read.) I think most feed readers support OPML now, more people should put together blog starter packs (like Bluesky's starter packs, which are super useful.) reply polytely 4 hours agoparentprevCan you share some blogs you are following? I would love some more stuff to read. reply nopassrecover 16 hours agoprevI consider Venkatesh to be amongst the brightest and insightful thinkers of our times (The Gervais Principle being a particularly brilliant eye opener that I share with everyone who could be interested and ready to benefit from it). I hadn’t quite appreciated how long he’s been blogging for, nor that we was nearing 50 (the latter point gives me some inspiration in my mid-30s that there’s still time to contribute). In terms of the article, and this doesn’t injure his main point, but I feel he has been overgenerous about how long the blogosphere has lasted (though there are of course still exceptions eg for me recently Ludic’s blog https://ludic.mataroa.blog/, some old gems like Rands in Repose ticking away, and more focused series like Patrick McKenzie’s Bits about Money). To my mind it was well and truly done (in the way that it once existed) before Covid, with the absence of a resurgence during a period with so many locked down and online proving its end. By that point there had already been the rise of walled gardens, the fall of Google Search, the rise of social media and influencers (and since the subsequent fall post-Covid), clickbait, smartphones as the primary browsing device, constantly online culture, attention exhaustion, (low quality) content saturation, etc. If the blog had survived all this LLMs adding fuel to the attention harvesting noise would almost certainly have sealed the deal. On the whole “online” feels like it’s falling apart. The leading apps and sites are buggy and broken with little innovation in years despite obvious low-lying fruit yet still dominantly crowding out (or buying out and shutting down) challengers, websites are a mess of instinctual clicking past popups like your relative’s malware-infected WinXP desktop you had to fix up at Christmas, almost everyone seems to be an influencer cynically pumping out low quality noise that the algorithms seem almost determined to elevate over unloved crafted quality content (and we still don’t seem to be reaping benefits of AI to sort through this), US politics contextlessly infects nearly every platform and channel globally (thankfully usually not here except when relevant), the subscription sites have bait and switched after hooking us all in and continue to turn the screws, the “gig economy” and “disruptors” have done the same continuing to damage broader society with their externalities while skirting the purpose of established laws and norms and now raising prices to higher than what they displaced, our democracy is challenged by the inability to know truth accelerated by the overwhelm of targeted noise (and old media has just as much guilt in monetising then selling out the fourth estate while governments are now racing towards totalitarianism in an attempt to put the genie back in the bottle), attention span and ability to think as humans is being fundamentally reshaped in ways that most consider damaging, digital has accelerated (or at least coincided with) the society-consuming and society-destroying elements of capitalism over the last 50 years as it “cleverly” works out how to turn anything and everything (quality, brand value, trust, institutions, connections, spare time, relationships, attention etc.) into $ and destroy them in the process, and it all seems to have turned into walking through a crowded alleyway of people shouting and fighting and exploiting and vying for your attention and and and and. The fall of Rome analogy feels figuratively rather apt, because it certainly has a sense of that, and maybe this cozyweb is a nice hideaway answer from all this (I haven’t quite found my online cozy place yet). For those who haven’t seen it and find any of this resonates (especially as a Millenial), Bo Burnham’s Inside brilliantly talks to this all, and he’s the first I’ve come across to really do so. Is anyone else talking about this in a way that’s helpful, or at least helpfully relays the problem that’s emerged upon us? reply satisfice 17 hours agoprevIt's so strange when people say that such-and-such-internet-thing is dead. For the most part, I see no truth in that. There's SOME truth in SOME of it. Private mail servers are getting harder to use, because other servers don't trust them. But blogs will work. The technology is as good as ever, and people can go read stuff on blogs. Maybe masses of people are choosing to do that less. That's not what being dead is. That's being less popular. I mostly post on LinkedIn instead of my blog, because I like the shorter writing form. reply gcanyon 6 hours agoprevWell that was an interesting chain of events: I read the first several paragraphs. Since they seem wordy and light on actual meaning, and the post is long, as I often do, I grabbed all the text and headed to ChatGPT/Claude -- in this instance ChatGPT. \"Summarize this in no more than twenty bullet points.\" ChatGPT spits out exactly twenty bullet points, including pointless gems like: - He believes public social media, particularly blogs, played a more vibrant role than traditional media during their heyday - Rao humorously attributes the blog’s success to the low-interest-rate era, with its cheap hosting and free distribution. So I switch to Claude and prompt: \"Summarize this into no more than 20 bullet points. Skip unimportant housekeeping bits.\" Claude also feels the need to produce exactly 20 bullet points, including exactly the housekeeping bits I pointed it away from like: - Some content threads (lab tinkering, fiction) will find homes in other media or formats. - Rao plans to compile book-length volumes from the blog archives. Now I'm going on too long, so I'll just say it took several more prompts, with examples, to get them to coalesce to: ChatGPT: 1. Venkatesh Rao is retiring his blog *Ribbonfarm* on November 13, 2024, after 17 years. 2. He views this as the end of the blogosphere era, which played a major role in shaping the internet during the Web 2.0 phase (2000-2020). 3. The decline of blogs is linked to rising interest rates and the shift toward paid distribution models, which replaced the free, open web environment that allowed blogs to thrive. 4. Rao coined the term \"cozyweb\" to describe the more intimate, private online spaces (like Discord and Substack) that are replacing the public nature of blogs and social media. 5. *Ribbonfarm*'s closure symbolizes the broader cultural shift from the chaotic, public blogosphere to this new era of niche, cozy communities. Claude: • After 17 years, Venkatesh Rao is retiring his blog, Ribbonfarm, on November 13, 2024 (his 50th birthday). • Ribbonfarm's lifespan (2007-2024) coincides with the rise and fall of the blogosphere, Web 2.0, and public social media - a significant chapter in internet and civilizational history partly enabled by low interest rates and cheap venture capital. • Rao sees the \"cozyweb\" (a term he coined in 2019) as the successor to public social media, favoring more intimate, subscription-based platforms like Substack. • In recent years, Rao shifted to writing \"blogchains\" and avoiding viral content, reflecting broader changes in online writing styles. • Post-retirement, Rao plans to compile book-length volumes from the blog archives but is uncertain where to share his \"random impulses\" that found a home on Ribbonfarm. reply GavinGruesome 3 hours agoprev [–] \"our incredible journey\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ribbonfarm, a blog by Venkatesh Rao, will retire on November 13, 2024, concluding its 17-year run, though the site will remain accessible without new content.",
      "Rao reflects on blogging's evolution, noting shifts to platforms like Substack and the emergence of the \"cozyweb\" era, indicating changes in digital content consumption.",
      "He plans to transition to other media, focusing on technology trends and serialized projects, while expressing gratitude to readers and considering future endeavors."
    ],
    "commentSummary": [
      "Ribbonfarm, a blog by Venkatesh Rao, is closing after 17 years, marking the perceived end of the blogosphere era that flourished during Web 2.0.",
      "The decline of blogs is linked to rising interest rates and a move towards paid distribution models, reflecting a broader cultural shift.",
      "Rao introduced the term \"cozyweb\" to describe the intimate online spaces emerging as alternatives to public blogs and social media, and plans to compile book-length volumes from the blog's archives."
    ],
    "points": 169,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1729364794
  },
  {
    "id": 41891953,
    "title": "Origin of 'Daemon' in Computing",
    "originLink": "https://www.takeourword.com/TOW146/page4.html",
    "originBody": "Issue 146, page 4 Search Home FAQ Links Site map Book StoreBackIssuesNew Ask Us Theory About Sez You... We were copied on this interesting e-mail discussion. From Richard Steinberg/Mr. Smarty Pants (The Austin Chronicle): Professor Corbato I write a trivia column for a newspaper called The Austin Chronicle. Someone has asked me the origin of the word daemon as it applies to computing. Best I can tell based on my research, the word was first used by people on your team at Project MAC using the IBM 7094 in 1963. The first daemon (an abbreviation for Disk And Executive MONitor) was a program that automatically made tape backups of the file system. Does this sound about right? Any corrections or additions? Thank you for your time! From Fernando J. Corbato: Your explanation of the origin of the word daemon is correct in that my group began using the term around that time frame. However the acronym explanation is a new one on me. Our use of the word daemon was inspired by the Maxwell's daemon of physics and thermodynamics. (My background is Physics.) Maxwell's daemon was an imaginary agent which helped sort molecules of different speeds and worked tirelessly in the background. We fancifully began to use the word daemon to describe background processes which worked tirelessly to perform system chores. I found a very good explanation of all this online at: http://www.takeourword.com/TOW129/page2.html (Search on \"Maxwell\" to locate the pertinent paragraph.) To save you the trouble, I will cut-and-paste it right here. It comes from a web-column entitled \"Take Our Word For It\" run by Melanie and Mike Crowley, etymology enthusiasts! ------------------------------------ From Jan Danilo: I am interested in the origin of the word daemon. I work in information technology and I have always heard of system processes referred to as daemons. I assumed that it is an older spelling of demon. Can you shed some light on this point? Why certainly. Someone give us some of those phosphorescent genes that have recently been spliced to mice DNA and we'll shed light like mad. Demon and daemon were once used interchangeably. The former came to English from medieval Latin, while the latter was from classical Latin. The earliest use appears to have been in the phrase daemon of Socrates, which was his \"attendant, ministering, or indwelling spirit; genius\". That was in the late 14th century. It was a short time later that the term demon came to refer to \"an evil spirit\" by influence of its usage in various versions of the Bible. The Greek form was used to translate Hebrew words for \"lords, idols\" and \"hairy ones (satyrs)\". Wyclif translated it from Greek to English fiend or devil. This is how the evil connotation arose. By the late 16th century, the general supernatural meaning was being distinguished with the spelling daemon, while the evil meaning remained with demon. Today daemon can mean \"a supernatural being of a nature intermediate between that of gods and men\" or \"a guiding spirit\". [Warning: This paragraph is about science so, if this topic causes you undue alarm, please close your eyes until you've finished reading it.] The 19th century scientist James Maxwell once daydreamed (the polite term is \"thought experiment\") about a problem in physics. He imagined a closed container which was divided in half. In the middle of the divider was a tiny gate, just large enough to admit one molecule of gas. This gate, in Maxwell's imagination, was operated by a tiny daemon. This daemon observed the speed (i.e. temperature) of the molecules heading for the gate and, depending on the speed, let them through. If he let only slow molecules pass from side A to side B and only fast molecules pass from side B to side A, then A would get hot while B cooled. Maxwell's daemon was only imaginary, of course, but as it seemed to evade the laws of thermodynamics it caused quite a stir. Eventually, though, the theory of quantum mechanics showed why it wouldn't work. [OK, you may open your eyes, now.] As you probably know, the \"system processes\" called daemons monitor other tasks and perform predetermined actions depending on their behavior. This is so reminiscent of Maxwell's daemon watching his molecules that we can only assume that whoever dubbed these \"system processes\" had Maxwell's daemon in mind. Unfortunately, we have found no hard evidence to support this. [Now, of course, we have!] We also assume that this is the meaning behind the daemon.co.uk, host to many United Kingdom web sites. -------------------------------- Professor Jerome H. Saltzer, who also worked on Project MAC, confirms the Maxwell's demon explanation. He is currently working on pinpointing the origin of the erroneous acronym etymology for daemon in this sense. [We have edited Issue 129 to reflect this confirmation of our original assumption. Isn't it wonderful to be able to trace a word to its source so cleanly?] From Brad Daniels: While au jus does mean literally \"with juice\", it is short for sauce au jus [de beouf], meaning \"sauce made with juice [of beef]\", so, saying \"with au jus\", while admittedly awkward, is not as wrong as it seems at first glance. OK, so maybe I could be an anti-curmudgeon after all. Is there some requirement out there that I be consistent? But do you say \"with a la mode\"? From Fred Wells: RE: \"and etc.\" and Ken Williams' comment about \"with au jus\", close but no cigar. Avec is French for \"with\"; au is French for \"in\". But, silly me, I just learned that from a high school friend. Sorry, Fred, but in this case au is usually translated as \"with\". Look at cafe au lait. You don't say \"coffee in milk\", do you? Tell your high school friend to study a bit harder, and don't believe everything you hear (the cardinal rule of critical thinking). From Richard Hershberger: In the Sez You page of issue 145, Ken Williams comments on the waitress who asked if he wanted his roast beef sandwich \"with au jus\", to which you responded with \"Aaargh!\" While the waitress clearly was not fluent in French, her English was impeccable. English syntax requires a preposition in that construction. \"Au jus\" lacks an English preposition, so she provided the one. The fact that there is a French preposition in the phrase is irrelevant, since the construction as a whole has been adopted into English and reanalyzed to fit English syntax. This is a normal process which has occurred innumerable times in the past. It only seems incongruous because it is recent and because some of us have enough knowledge of French to recognize the original syntactical structure. Again, Barb and Malcolm ask, do you say \"apple pie with a la mode\"? They don't agree that the presence of a French preposition is irrelevant. To be consistent with constructions like \"apple pie a la mode\", one should not add an English preposition. The phrase au jus should be treated as an adjective if it isn't going to be parsed as a prepositional phrase. From Jane Harrington: Slightly off topic regarding the discussion \"and etc\" and Ken Williams's comment about \"with au juice\" I would like to contribute these \"Canadianisms\" for your enjoyment. In Canada all labels must be in both official languages. To save space, these labels often use one common word between a French and an English descriptive word. As a result I have heard people refer to \"The Jeux Canada Games\" (Jeux Canada being the French name for The Canada Games) Another more common one is \"old fort cheese\" which I admit to using. Old fort cheese has almost become legitimate now. It has been used on the CBC national radio programme \"This Morning\" at least twice, and if the CBC sanctions it, it must be correct. Thanks for many amusing discussions. Thanks, Jane! All of these are examples of macaronic phrases. (Clicking on macaronic will take you to the glossary section of Take Our Word For It.) From Jane Irish Nelson: In [last] week's Words to the Wise, you wrote that the Welsh word for rabbit is cwningen. I was struck by the apparent resemblance to the Spanish word for rabbit: conejo. Do you know it the two are related? Thank you! I love words and look forward to visiting your site each week. Thanks for the kind words! Read on. From Jeff Lee: In Issue 135 of TOWFI, you write: Of course there are rabbits in Wales! The Welsh word is cwningen (feminine gender, plural is cwningod) but we don't expect many English rabbits would stop at the border just because they can't speak Welsh. This reminds me of an old joke (from Wits Fittes & Fancies, by Anthony Copley, 1595) which runs: A manie Schollers went to steale Conies, and by the way they warn'd a nouice among them to make no noise for feare of skarring the Conies away : At last he espying some, said aloud in Latine: \"Ecce cuniculi multi.\" And with that the Conies ranne into their berries : Wherewith his fellowes offended, and chyding him therefore, hee sayd: \"Who (the Deu'll) would haue thought that Conies vnderstood Latine\". Out of curiosity, my dictionary indicates that cony derives ultimately from cuniculus. Is the Welsh cwningen related, or is it just a coincidence that they sound so similar? Yes, all of these rabbit words are related. The English and Welsh forms come from the Latin, and it is thought that Latin borrowed it from an ancient Iberian language. Good joke, by the way! From Brad Daniels: Your letters on \"ATM machine\", \"PIN number\", etc. reminding me of some more common acronym abuse: The other day, I received an invitation exhorting me to \"please RSVP\". \"Please respond please?\" Now, I know Répondez S'il Vous Plait isn't English, but surely people know RSVP means \"please respond\". And what about \"RAM memory\"? Surely, it's obvious that RAM is an acronym (unless there's some new ovine technology out there), and even if you don't know the \"Random Access\" part, the \"M\" pretty obviously stands for \"Memory\". Hmm... Maybe I wouldn't make a good anti-curmudgeon after all. See! From Lt. Maj. Michael Talbert: I think you treated the one who offered this as the origin of the term golf very kindly... \"In Scotland, a new game was invented. It was entitled Gentlemen Only Ladies Forbidden.... and thus the word GOLF entered into the English language.\" ...but have you done so at your own expense? I would have been impressed to hear you point out that none of the words \"gentlemen\", \"only\", \"ladies\", and \"forbidden\" existed with the familiar meanings at the time the game golf is thought to have been invented. We didn't think such explanations necessary. We already gave the etymology of golf and gave a link back to that discussion. Going on about how the words that make up the acronym are anachronistic as far as the word's timeline go would be like beating a dead horse. Also, we believe that the reader who wrote with that etymology was aware that it was ridiculous and sent it because he knew we'd get a kick out of it. However, if the modern name golf was only given to the game (however long the game itself existed) in recent enough linguistic times to accommodate the acronym, then your reader may be on to something... Got a final word on the subject? Love your stuff! Our final word: golf is not an acronym. Read our discussion of its etymology. A very important rule of etymology which we cannot repeat often enough: few English words derive from acronyms (sonar, radar, scuba are some examples); very few derive from acronyms before the 20th century (don't even try to suggest posh!. We don't count okay as an acronym: if it were one, it would be pronounced \"ock\"). Read our past discussion of acronyms. From Simon Rumble: My main experience with the term Piri Piri has been through the Nando's chain of Portuguese (the chain is actually South African) chicken shops. This link also supports the Portuguese origin: http://www.kingpiripiri.com/. However as avid colonizers, it's likely that the origin of the term is not Portuguese but one of their colonial victims... er, hosts. From Alan Wachtel: Rich Bowen wrote in Issue 145: I grew up in Kenya, where there is a large Indian community, and a lot of hot food, which we call pilli pilli or piri piri depending on ethnic origin. I had long wondered about the origin of this term, and I can see that it is a mutation of pippali. The most widely spoken language in Kenya, other than English, is Swahili. It's been a long, long time since I learned Swahili in the Peace Corps, but I still remember that the word for \"pepper\" is pilipili. It's easy to see how that could become piripiri. However, the term does not seem to have originated in Kenya's Indian community. The grammar of Swahili is Bantu. Most of its vocabulary is also Bantu, but it borrows many words from Arabic, Persian, Portuguese, German, and English, reflecting the region's trading and colonial history. According to the Oxford \"Standard Swahili-English Dictionary,\" pilipili is derived from a Persian word that I transliterate (with some difficulty, because I don't know the alphabet, and initial, medial, and final forms of letters are different) as plpl (vowels are not shown). Persian is closely related to Sanskrit, and the similarity to Sanskrit pippali is clear. Plpl immediately reminded me of Hebrew pilpul, a form of Talmudic disputation that involves close examination of minute distinctions. Sure enough, pilpul is the Aramaic word, and cognate to the Hebrew word, for \"pepper,\" from, I'm guessing, either the finely divided nature of the ground spice or its fiery taste. From Catherne [sic] Hackett: [Etc.] is of latin [sic] derivation meaning \"and the rest.\" I don't know why I returned to your site. It's still hopelessly WRONG. Er... sure. PREVIOUSNEXT Comments, additions? Send to Melanie & Mike: melmike@takeourword.com DO NOT SEND QUERIES TO THAT ADDRESS. Instead, ASK US. Copyright © 1995-2002 TIERE Last Updated 01/23/02 09:39 PM",
    "commentLink": "https://news.ycombinator.com/item?id=41891953",
    "commentBody": "Origin of 'Daemon' in Computing (takeourword.com)166 points by wizerno 18 hours agohidepastfavorite64 comments jasoneckert 6 hours agoThe *nix world is full of dark-but-fun terminology. Daemons run the system. New files get 666 (before the umask takes away unnecessary permissions). Parents kill their children before killing themselves. And sometimes you have to kill zombies. reply vhodges 5 hours agoparentFrom: https://devrant.com/rants/1101391/my-daily-unix-command-list... unzip; strip; touch; finger; mount; fsck; more; yes; unmount; sleep. reply Uehreka 1 hour agorootparent> unzip; strip; touch; finger; mount; fsck; more; yes; unmount; sleep. T E C H N O L O G I C … T E C H N O L O G I C reply oefrha 1 hour agorootparentprevExcept you shouldn’t fsck while mounted. reply JKCalhoun 5 hours agorootparentprevyum reply johnisgood 46 minutes agorootparentSuch an apt reply. reply _fat_santa 3 hours agoparentprevAlmost feels like a right of passage when you inevitably google something like \"kill self\" (in reference to killing the current process) and get a popup telling you about suicide resources. reply ganjatech 2 hours agorootparentOr indeed a rite of passage reply gberger 2 hours agoparentprevSometimes you have to kill orphans too! reply dcminter 6 hours agoprev> We also assume that this is the meaning behind the daemon.co.uk, host to many United Kingdom web sites Not sure if it was the origin of the company name, but the domain was demon.co.uk not daemon. E.g. I had pretence.demon.co.uk with them for a few years. https://en.m.wikipedia.org/wiki/Demon_Internet reply pixelesque 6 hours agoparentYeah, never heard of a 'daemon.co.uk' in the 90s, but likewise had a Demon account... reply trelane 2 hours agoprev\"Warning: This paragraph is about science so, if this topic causes you undue alarm, please close your eyes until you've finished reading it.\" Amazing. reply johnisgood 45 minutes agoparentOh my, reminds me of people fighting or having a heated argument on the Internet. Why does it have to be said that you can just close the window, the application, or your eyes? :/ reply gcanyon 7 hours agoprevHa -- I read the title and said to myself, \"gotta be Maxwell, right?\" The jolt of pleasure I get from being right about things like this is unreasonable. reply JKCalhoun 5 hours agoparentYeah, author tries to throw us a curve ball at first with that koo-koo backronym. reply mrngm 8 hours agoprevEarlier threads: - [2023] https://news.ycombinator.com/item?id=35283067 (24 comments) - [2022] https://news.ycombinator.com/item?id=31069163 (127 comments) - [2018] https://news.ycombinator.com/item?id=16299583 (46 comments) - [2011] https://news.ycombinator.com/item?id=2691752 (45 comments) reply etcd 7 hours agoprevAlways thought it was because it denotes a process that stays alive and so is like a little living demon in your computer. reply JD557 10 hours agoprevUnrelated to the word \"daemon\", but related to the article, I was a bit surprised by this assertion: > Eventually, though, the theory of quantum mechanics showed why it wouldn't work. I was familiar with the information theory arguments (the same presented in Wikipedia[1]). Is that why they mean here by \"quantum mechanics\" or is there another counterargument to Maxwell's daemon? 1: https://en.m.wikipedia.org/wiki/Maxwell's_demon#Criticism_an... reply Vecr 50 minutes agoparentIt probably (if the calculations are right) is unable to actually do much of anything useful (because it's too complex to avoid being extremely correlated with the rest of the universe (\"embedded\")), and even if it could it wouldn't be better than a standard ASI in most real-world situations. That's assuming you aren't trying to claw back more energy than you lose, I'm pretty sure that's not possible to reliably do without crazy hypothetical physics. reply n4r9 9 hours agoparentprevI'm guessing that the daemon's ability to allow only fast molecules through the gate depends on knowing their position and velocity simultaneously? reply eru 4 hours agorootparentBut the daemon doesn't need to know them all that precisely. reply n4r9 57 minutes agorootparentI think you'd have to be pretty precise to know if it's heading towards a hole that's only just large enough for it. reply pantulis 7 hours agoprevOf course it's not the origin of the usage, but I always found Lovecraft's quote from Lactantius to be pretty adequate: \"Demons have the ability to cause people to see things that do not exist as if they did exist. -- Lactantius\" reply compressedgas 1 hour agoparenthttp://blogicaster.blogspot.com/2011/10/lovecrafts-lactantiu... reply trash_cat 6 hours agoprev\"Disk And Executive MONitor\" does sound kinda cool, though. reply rubyfan 6 hours agoparentI’d guess someone backed into that acronym when asked “why do you call these things daemons?” reply modernerd 7 hours agoprevWonderfully written. “Warning: This paragraph is about science so, if this topic causes you undue alarm, please close your eyes until you've finished reading it.” reply twobitshifter 6 hours agoprevDo you pronounce it as demon or like Matt Damon? reply jamesog 4 hours agoparentIt should more properly be written as dæmon. The æ (\"ash\") character is usually pronounced more like \"ee\", as in encyclopædia. I've never heard anyone say \"encycloPAYdia\" :-) reply bbor 1 hour agorootparentFascinating! This is why I stick with nice, clean structural linguistics, this applied stuff gets sticky. I just confirmed on Youtube that the (some?) British people do indeed pronounce \"Aesthetic\" as \"ah-stet-ic\" not \"ee-stet-ic\", and upon diving a bit, it seems that the rule is \"don't ask for a rule, you fool! It's 'e' now except for when it isn't.\" Thanks for the interesting tidbit! The letter æ was used in Old English to represent the vowel that's pronounced in Modern English ash, fan, happy, and last: /æ/. Mostly we now spell that vowel with the letter a, because of the Great Vowel Shift. When æ appears in writing Modern English, it's meant to be a typographic variant of ae, and is pronounced the same as that sequence of vowel letters would be. So Encyclopaedia or Encyclopædia, no difference. https://english.stackexchange.com/questions/70927/how-is-%C3... Highly recommend the protracted arguments in the comments, that's a wonderfully pedantic StackExchange. Big shoutout to someone in 2012 defining \"NLP\" as an unusual word -- how the world has changed! It's only a matter of time before they open an AP/IB course in NLP... reply saltcured 1 hour agorootparentHmm, I'm a Californian and I pronounce daemon as demon, understanding the first vowel as the same vowel as for Aesop. Indistinguishable from the vowel in \"beam\" and \"niece\". But I pronounce the first vowel in aesthetic differently. For me, it's somewhat in between the vowels in \"bed\" and \"bad\" but closer to the former. reply jamesog 1 hour agorootparentprev\"Aesthetic\" gets even stickier! In the UK I tend to more commonly hear it pronounced as \"es-thetic\". The Great Vowel Shift indeed makes written English much more confusing than it perhaps should be. English is already a messy hodge-podge of a language, then our writing system started to get standardised (or standardized, if you're American!) right as pronunciation started to change, leading to the written version of words suddenly no longer being anything like the pronunciation. reply lagniappe 2 hours agorootparentprevbecause it's spelled encyclopedia reply jamesog 2 hours agorootparentUS English spells it as encyclopedia, British English spells it as encyclopaedia. reply everfrustrated 4 hours agoparentprevI pedantically use day-mon (working/servant spirit) to distinguish from dee-mon (evil spirit) but I suspect I'm very much in the minority. https://en.wikipedia.org/wiki/Daimon vs https://en.wikipedia.org/wiki/Demon I've never found any significance to associate the unix term with Demons and consider it a mis-association. reply iwaztomack 55 minutes agorootparentDon't let MAGA hear you... they'll start banning and burning linux boxes. reply dcow 5 hours agoparentprevI believe it officially is pronounced the same as demon. But colloquially I hear a lot of (and sometimes find myself using) “damon”. reply whartung 4 hours agorootparentI’m in the Matt Damon camp. Always pronounced it that way, never really gave it much thought. Just seems “right” to me. reply bbor 1 hour agorootparentHard agree. It's an archaic word, it seems a shame not to revel in that fact whenever you use it! I've been using \"automata\" a lot recently, and that's another one that is just more fun the unusual way. Also it helps that it's clear either way -- no one will be confused upon hearing \"daymon\" even if they're not used to it, unlike \"etsee\", \"user-slash-libe\", \"user-slash-bine\". Or, god forbid, \"oo-zir slash\"... reply JadeNB 3 hours agorootparentprevAnd now we can perhaps hash out whether the `lib` in `/usr/lib` is pronounced with a long or short 'i'. I hope I'm not the only one who pronounced it the first way with no real thought, and never noticed until I heard someone else say it with a long 'i' that that was obviously the logical pronunciation. reply brianmurphy 1 hour agorootparentI have always pronounced lib like the word liberal. I was mind-blown the first time I heard someone pronounce etc as \"et-see\". et-see rolls off the tongue so much better than ee-tee-see that it makes perfect sense now. reply dpassens 2 hours agorootparentprevSurely the logical pronunciation is the way you'd pronounce it in library, so a long 'ai' rather than any kind of 'i'? Though I personally always use the short 'i'. I was going to justify that by saying it's the same as /usr/bin, but that's also short for binaries, so should also be an 'ai'. reply macintux 2 hours agorootparentI've been pronouncing it with the short 'i' for 30 years, but mainly, possibly only, in my head. In 1998 I started a new job, and my boss pronounced \"URL\" as \"earl\". That threw me for a loop, had to fight my way through our first conversation before I figured out what he was saying. reply bitwize 1 hour agorootparentI pronounce API as \"appy\", which sometimes draws quizzical looks (people think I'm using next-level cutesy slang for \"application\"). But I never could do the \"earl\" thing. Or \"sequel\". reply JadeNB 1 hour agorootparentprev> Surely the logical pronunciation is the way you'd pronounce it in library, so a long 'ai' rather than any kind of 'i'? Yep, that's what I meant to say with: > … never noticed until I heard someone else say it with a long 'i' that that was obviously the logical pronunciation. But maybe the sentence structure was too tortured for it to be clear what I was saying. > Though I personally always use the short 'i'. I was going to justify that by saying it's the same as /usr/bin, but that's also short for binaries, so should also be an 'ai'. Oh, shoot, even after I noticed the logical pronunciation of \"lib\" (long 'i') it never occurred to me that the same applied to \"bin\". I guess I just can't say any paths out loud any more. reply dpassens 23 minutes agorootparent> that's what I meant to say Ah, that makes sense. I thought you meant long 'i' as in extending the duration of the 'i' sound, like in 'beep' vs 'bip'. reply saltcured 1 hour agorootparentprevDo you pronounce the vowel in /var as in \"bar\" or as in \"bare\"? Also, for those who try to pronounce everything rather than spell them out, where does it end? I now have a newly discovered, morbid interest in how such folks say path elements like \"selinux\", \"httpd\", and \"pgsql\"... reply fwip 1 hour agorootparentprevPerhaps the difference for you is that \"bin\" is already an English word with an official pronunciation. Personally, I also use short-i for \"lib,\" because I tend to pronounce shortenings of text as if they were words themselves. reply burcs 6 hours agoprevSaw this on a conspiracy theory subreddit recently, thought it was absolutely hilarious: \"Boy I love trapping demons in microscopic silicon megastructures to do my bidding, I sure hope nothing goes wrong\" reply sgarland 5 hours agoparentI told my eight year old the “we taught sand how to think” joke, and he loved it, and also indulged me in a brief explanation. It really is absolutely wild that it all works when you go to the absolute fundamentals and start working forward. reply somat 32 minutes agorootparentThe funny thing is that before that we taught light bulbs how to think(thermionic logic), and before that we taught magnets how to think(relay logic) and before that we taught levers how to think(gear logic) reply stuckinhell 4 hours agoparentprevDidn't some Wharton professor post something similar too recently? reply bitwize 2 hours agoprevI remember learning about Maxwell's Daemon through the Apple II game Dr. Maxwell's Molecule Magic, in which you take the role of the daemon. You must toggle the barrier on and off in order to trap enough gas molecules at high enough pressure to launch a rocket ship. Once you think you have enough, you can then launch the rocket to see how well you did. If you were successful, the rocket would blast off the screen and an image would show of an astronaut on the moon saying \"Hi, Mom!\" (Speech was provided via PWM through the Apple II speaker.) Eleven-year-old me was easy to entertain. Especially if rockets, robots, or science was involved. reply keepamovin 10 hours agoprevI always wondered about this. Now I know. Thank you :) reply reaperducer 7 hours agoprevI knew the origin of daemon from high school Greek philosophy courses. At the time, I thought \"when an I ever going to use this stuff in real life?\" Then I got into computers. reply AStonesThrow 3 hours agoparentI know, right? I used to read about the role of eunuchs in ancient royal courts, and I thought to myself, who does that anymore? reply FergusArgyll 6 hours agoprevHuh, I assumed: it does work whilst remaining unseen reply pyinstallwoes 9 hours agoprevfrom PIE dai-mon- \"divider, provider\" (of fortunes or destinies), from root da- \"to divide.\" to divide power, compute. reply pzmarzly 10 hours agoprev [–] Interestingly, Safari Reader View doesn't work on this page. Firefox's does. Alternatively, here's a readable mirror: https://ei.cs.vt.edu/%7Ehistory/Daemon.html And another: https://old.reddit.com/r/linux/comments/7w7914/the_origin_of... reply Dalewyn 10 hours agoparentThis was the internet we loved, I miss it. reply latexr 9 hours agoparentprev> Safari Reader View doesn't work on this page. What do you mean? I just tried it and it was fine. reply pzmarzly 9 hours agorootparentFor me (macOS 15.0.1), it only extracts the biggest quote (Your explanation of the origin of the word daemon ... United Kingdom web sites), the rest of the text is lost. reply dcow 5 hours agorootparentWorks on iOS. Page is so horrendous that I immediately reached for it. reply Teknomancer 2 hours agoparentprev [–] “Text of the site below because looking at that site hurt. A lot.” For fucking real! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The term \"daemon\" in computing, used by Professor Corbato's team at Project MAC in 1963, was inspired by Maxwell's daemon in physics, not an acronym.",
      "The discussion also explores language nuances and etymology, including terms like \"au jus,\" \"golf,\" and \"piri piri.\"",
      "Reader contributions and clarifications on language and etymology are included in the discussion."
    ],
    "commentSummary": [
      "In computing, a \"daemon\" refers to background processes that manage system tasks, originating from the *nix (Unix-like) operating systems.",
      "The term has roots in Greek mythology, where \"daimon\" means a guiding spirit, and in tech culture, it is often associated with quirky and dark humor.",
      "Pronunciation of \"daemon\" varies, with some saying \"day-mon\" and others \"demon,\" reflecting its mythological and cultural influences."
    ],
    "points": 167,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1729384521
  },
  {
    "id": 41893994,
    "title": "Bitwarden is no longer free software",
    "originLink": "https://github.com/bitwarden/clients/issues/11611",
    "originBody": "bitwarden / clients Public Notifications Fork 1.2k Star 9.2k Code Issues 608 Pull requests 322 Discussions Actions Security Insights New issue Jump to bottom Desktop version 2024.10.0 is no longer free software #11611 Open brjsp opened this issue · 30 comments Open Desktop version 2024.10.0 is no longer free software #11611 brjsp opened this issue · 30 comments Labels bug desktop Comments brjsp commented • edited Pull request #10974 introduces the @bitwarden/sdk-internal dependency which is needed to build the desktop client. The dependency contains a licence statement which contains the following clause: You may not use this SDK to develop applications for use with software other than Bitwarden (including non-compatible implementations of Bitwarden) or to develop another SDK. This violates freedom 0. It is not possible to build desktop-v2024.10.0 (or, likely, current master) without removing this dependency. 75 14 brjsp added bug desktop labels bitwarden-bot commented Thank you for reporting this issue! We've added this to our internal tracking system. ID: PM-13815 Author brjsp commented Namely trying to build with bitwarden_license directory removed (like we have been always doing) and sanitized node_modules results in the following: [Prel] assets by status 30.6 KiB [cached] 1 asset [Prel] orphan modules 28.2 KiB [orphan] 25 modules [Prel] ./src/preload.ts + 25 modules 28.4 KiB [not cacheable] [built] [code generated] [Prel] [Prel] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/abstractions/sdk/sdk.service.ts [Prel] 3:32-57 [Prel] [tsl] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/abstractions/sdk/sdk.service.ts(3,33) [Prel] TS2307: Cannot find module '@bitwarden/sdk-internal' or its corresponding type declarations. [Prel] [Prel] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/abstractions/sdk/sdk-client-factory.ts [Prel] 1:37-62 [Prel] [tsl] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/abstractions/sdk/sdk-client-factory.ts(1,38) [Prel] TS2307: Cannot find module '@bitwarden/sdk-internal' or its corresponding type declarations. [Prel] [Prel] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/default-sdk.service.ts [Prel] 3:54-79 [Prel] [tsl] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/default-sdk.service.ts(3,55) [Prel] TS2307: Cannot find module '@bitwarden/sdk-internal' or its corresponding type declarations. [Prel] [Prel] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/default-sdk-client-factory.ts [Prel] 1:21-46 [Prel] [tsl] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/default-sdk-client-factory.ts(1,22) [Prel] TS2307: Cannot find module '@bitwarden/sdk-internal' or its corresponding type declarations. [Prel] [Prel] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/default-sdk-client-factory.ts [Prel] 2:24-81 [Prel] [tsl] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/default-sdk-client-factory.ts(2,25) [Prel] TS2307: Cannot find module '@bitwarden/sdk-internal/bitwarden_wasm_internal_bg.wasm' or its corresponding type declarations. [Prel] [Prel] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/noop-sdk-client-factory.ts [Prel] 1:37-62 [Prel] [tsl] ERROR in /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/libs/common/src/platform/services/sdk/noop-sdk-client-factory.ts(1,38) [Prel] TS2307: Cannot find module '@bitwarden/sdk-internal' or its corresponding type declarations. [Prel] [Prel] 6 errors have detailed information that is not shown. [Prel] Use 'stats.errorDetails: true' resp. '--stats-error-details' to show it. [Prel] [Prel] webpack 5.94.0 compiled with 6 errors in 14233 ms [Prel] npm error Lifecycle script `build:preload` failed with error: [Prel] npm error code 1 [Prel] npm error path /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/apps/desktop [Prel] npm error workspace @bitwarden/desktop@2024.10.0 [Prel] npm error location /home/abuild/rpmbuild/BUILD/bitwarden-2024.10.0/apps/desktop [Prel] npm error command failed [Prel] npm error command sh -c cross-env NODE_ENV=production webpack --config webpack.preload.js [Prel] npm run build:preload exited with code 1 ulm mentioned this issue app-admin/bitwarden-desktop-bin: add Bitwarden license gentoo/gentoo#39027 Open xndc commented Also see bitwarden/sdk#898. It looks like this is part of a deliberate campaign by Bitwarden, Inc. to fully transition Bitwarden to proprietary software, despite consistently advertising it as open source, without informing customers about this change. For whatever the opinion of one user is worth, I've switched away from Bitwarden due to this. 41 aphedges commented I noticed and reported a similar problem with the NPM releases of the CLI client (#10648) two months ago, and I have yet to receive a response. Bitwarden definitely seems to be moving away from being open-source software without making any sort of announcement about it. 20 rafntor commented what alternatives do you guys recommend? 12 iHarryPotter178 commented Bitwarden was good to me.. Now it's time to switch to alternatives... 10 adrian-afl commented Best alternative is to fork the version before this change! 10 LalOpen commented Ohhh noooo... That's a shame. You're right: i'll go to a fork or to any alternative! impredicative commented • edited i'll go to a fork or to any alternative! I would be careful going to \"any alternative\". It's your passwords you're talking about. Also, a fork of the client still leaves the open issue of relying on the server service or software. 3 Gallocon commented This is... concerning to say the least. I'm a long term paid Bitwarden user, and it's making me reconsider that decision. 18 cat-pat commented https://github.com/dani-garcia/vaultwarden 15 ludouzi commented I'll be looking for an alternative after hearing this. Quietly moving away from open source raises serious concerns. zarlo commented 7.3 The Company may at any time, terminate the License Agreement with you if: ........ d) the Company decides to no longer provide the SDK or certain parts of the SDK to users in the country in which you are resident or from which you use the service, or the provision of the SDK or certain SDK services to you by the Company is, in the Company’'s sole discretion, no longer commercially viable or technically practicable. well so it can be striped from us at any time? impredicative commented • edited https://github.com/dani-garcia/vaultwarden That's interesting for a server, and while the server has a web client, are there comparable open source clients too for desktop and/or mobile? 1 stukinnear commented what alternatives do you guys recommend? If it's for the home Vaultwarden. NikunjKhangwal commented No no no, not Bitwarden please. A service i dearly loved and was satisfied with :( JeanneD4RK commented what alternatives do you guys recommend? If it's for the home Vaultwarden. Why home use only ? Ollie1101 commented enshitification is inevitable with these god forsaken companies 8 GauthierPLM commented Note that the SDK is used (and enabled as a feature flag) not only in the release of desktop app, but also in the browser, CLI and web clients. This mean that all versions of BitWarden 2024.10.0 are using the SDK. 9 Foosec commented How many times do we have to teach companies that try to rug pull this lesson, you want to end up like redis? This is how you end up like redis. 9 Yaikava commented Yikes, that sucks 5 impredicative commented • edited enshitification is inevitable with these god forsaken companies It's practically a given with almost any VC (venture capital) or PE (private equity) backed company with worth between 10 million and 1 trillion USD. When outside of this range, they can do what they want. People keep getting surprised every time this happens, but it's so common as to be inevitable indeed. ssddanbrown commented Bitwarden has before released projects advertised as \"open source\" while not under a non-open restrictive license details, discussion. This may now indicate a pattern or direction. 1 ninjadev64 commented This sucks. I am going to develop an alternative desktop app which wraps Vaultwarden's web interface using Tauri, if anyone is interested. 3 Paddy-NI commented So I guess all my customers and myself of course will be moving to an alternative. ercoppa commented • edited Removed the annual subscription (never used the extra features, I had it only to support the project) and moving away very soon to a truly free software solution. Very disappointed since I have pushed a lot of people toward Bitwarden. LalOpen commented Very disappointed since I have pushed a lot of people toward Bitwarden. Same to me. And I quite regret it now... russeg commented Spirit of open source died long time ago. Open source is now a business model. Member kspearrin commented • edited Hi @brjsp, Thanks for sharing your concerns here. We have been progressing use of our SDK in more use cases for our clients. However, our goal is to make sure that the SDK is used in a way that maintains GPL compatibility. the SDK and the client are two separate programs code for each program is in separate repositories the fact that the two programs communicate using standard protocols does not mean they are one program for purposes of GPLv3 Being able to build the app as you are trying to do here is an issue we plan to resolve and is merely a bug. LalOpen commented Spirit of open source died long time ago. Open source is now a business model. According to me, the spirit of open source still lives in free software philosophy. bitwarden locked and limited conversation to collaborators Sign up for free to subscribe to this conversation on GitHub. Already have an account? Sign in. Assignees No one assigned Labels bug desktop Projects None yet Milestone No milestone Development No branches or pull requests 26 participants and others",
    "commentLink": "https://news.ycombinator.com/item?id=41893994",
    "commentBody": "Bitwarden is no longer free software (github.com/bitwarden)153 points by ferbivore 10 hours agohidepastfavorite58 comments Technetium 4 hours agoWhen I paid, it was with the expectation of supporting something open source. That's why I came to Bitwarden from Lastpass. This is really twisting the dagger in my back. They're probably preparing for an acquisition, since they have hit the same financial inflection point that CEO Michael Crandell was at previously when selling RightScale: https://bitwarden.com/blog/accelerating-value-for-bitwarden-... reply wooque 7 hours agoprevCTO response: Thanks for sharing your concerns here. We have been progressing use of our SDK in more use cases for our clients. However, our goal is to make sure that the SDK is used in a way that maintains GPL compatibility. the SDK and the client are two separate programs code for each program is in separate repositories the fact that the two programs communicate using standard protocols does not mean they are one program for purposes of GPLv3 Being able to build the app as you are trying to do here is an issue we plan to resolve and is merely a bug. reply mattdm 4 hours agoparentIt's not necessarily about being \"one program\". It's this part: \"The “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.\" I get that it's really hard to make money as an open source company. (That's why I am one of your paying customers.) The exclusion you are putting on your SDK seems very similar to that of the \"bitkeeper\" version control software used for the Linux kernel for a short time. Look how that turned out. reply atanasi 1 hour agorootparentFSF has published a commentary: https://www.gnu.org/licenses/gpl-faq.html#MereAggregation GPL licenses have allowed so-called \"mere aggregation\", where separate programs are distributed together. Such programs don't have to be all covered by GPL. On the other hand, if parts are intimately tied to each other such that they are effectively a single program, GPL applies to the whole. The FSF commentary explains that the judgment depends both on the mechanisms and the semantics of the co-operation. Technical implementation details don't make programs separate if they are intimately designed to work together: \"But if the semantics of the communication are intimate enough, exchanging complex internal data structures, that too could be a basis to consider the two parts as combined into a larger program.\" reply wanderfowl 1 hour agoparentprevI find this response (and the class of responses like it) really frustrating, because it uses a (likely feigned) misunderstanding of the scope of the question to attempt to sidestep the real question. My question for the CTO would be, roughly: You've now answered \"Do your lawyers think you can get away with this?\". But the questions you're not answering directly, which I think underlie the 'concerns' you're appreciating our sharing, are things like... - Does the Bitwarden team see no ethical problems with making proprietary a project which many supported and contributed to explicitly because it was open source? - Given that password management is explicitly a high-trust enterprise, how does your organization intend to navigate the rupture of trust, and subsequent forks and waves of departure, caused by an open-to-proprietary rugpull? - Is there something that the community could do together which would help your company navigate through the dire situation you must be in to be considering something like this, without resorting to proprietarization? I know it's his job as CTO right now to be feigning concern, particularly in forums where you can't close the conversation, but the current approach is basically confirming the worst fears (\"They believe they can legally do it, and see no problem with their actions\"), and that seems like exactly the wrong vibe for a company whose bottom line depends on users trusting the code and the people updating it. reply ferbivore 7 hours agoparentprevIn other words: bitwarden/clients is GPLv3; any Bitwarden client as a functioning whole is proprietary; the CTO does not see a problem with this; issue locked. reply AnonC 5 hours agoprevI’m not too surprised with this move. Bitwarden has been moving strongly to enterprise sales over the last several years, and while doing so, neglected the consumer side. It’s only recently that somehow they switched to a native iOS app from the previous MAUI base that stood out like a sore thumb in more ways than one. The current iOS client still has some catching up to do. After getting VC funding, the focus also seemed to be more revenue and profit driven. There’s nothing wrong with this choice, but it does push companies in a direction that’s very different from how they started and grew. Proton Pass looked a little interesting, but its pricing model is always a bit on the higher side (like with other Proton services). I’ve been trying Proton Pass free for a while, and will see if it’s worth switching to as a replacement for Bitwarden (because the latter wasn’t improving in a way that I was waiting for, over many years). The built-in password manager on iOS is also good enough, but it’s only for passwords and doesn’t support storing, retrieving and filling other types of data. reply krick 44 minutes agoprevI was long thinking to move to bitwarden/vaultwarden from KeePass to solve sync problems, but never did. Is it now useless w/o that client? Is some sort of WebUI not enough? Does it apply to Android as well? reply SushiHippie 3 hours agoprev> There are no plans to adjust the SDK license at this time. We will continue to publish to our own F-Droid repo at https://mobileapp.bitwarden.com/fdroid/repo/ https://github.com/bitwarden/sdk/issues/898#issuecomment-222... This was a reply in July, where someone raised this issue already on the SDK repository, which has this license since over a year ago. reply johng 59 minutes agoprevI use BitWarden on iOS, PC and Mac. Now I have to find an alternative. What a sad day. I'm a premium member @$10/year as well. There goes lost business because of shady practices. Is there a way to export and move to an alternative? reply pikdum 9 hours agoprevI never really looked into it, but I thought Bitwarden was completely free software based on their marketing. Looks like it's had some weird proprietary bits introduced around 2020, though? reply minebreaker 7 hours agoprevI loved it, I paid for it, I even recommended it to my friends and they loved it too... Now I'm gonna move to KeePassXC. What is the recommended way to sync the db with Android? I only have a Raspberry PI server running cloudflared. reply blooalien 7 hours agoparentI just use SyncThing to distribute my KeePass vault to exactly and only those devices where I need it. Works great for me. YMMV depending upon your needs. reply terminalbraid 2 hours agorootparentHeads up if you're using this on Android https://forum.syncthing.net/t/discontinuing-syncthing-androi... reply brunoqc 2 hours agorootparentMaybe just use syncthing-fork reply TheChaplain 7 hours agorootparentprevAnother SyncThing user here, it works well with KeePass. reply 6ak74rfy 2 hours agoparentprevI really want to use KeePass and its ecosystem but the password sharing story isn't great there. I and wife have a lot of shared passwords and Bitwarden works really good for that. So, I don't know what's a good viable alternative for us. reply mdaniel 4 hours agoparentprevIf you want to stay with the convenience of cloud without going back to sync conflicts due to moving a file around, Proton Pass is GPLv3 although I don't personally know what the self hosting story is like if they were to rug pull https://github.com/ProtonMail/WebClients/tree/proton-pass%40... https://github.com/protonpass/android-pass/blob/1.26.1/LICEN... https://github.com/protonpass/ios-pass/blob/1.12.3/LICENSE reply bmicraft 7 hours agoparentprevKeepass2Android can sync over ssh (sftp), Nextcloud and many other others. I'm sure you'll find something that works for you. reply strijelac 4 hours agoparentprevKeePassXC with Nextcloud client for Android is a perfect match for me. Using it for years with zero problems. reply selfhoster11 7 hours agoprevI was hoping to move to Bitwarden eventually, but unless I can find a healthy open ecosystem, that’s not happening now. I will keep monitoring to see how this develops. reply andag 8 hours agoprevGoing proprietary is one thing, but doing it in several steps in different repos and saying nothing about it I really don't like... What are open alternatives at this point? reply ferbivore 8 hours agoparentThe KeePass ecosystem has gotten a little bit better. It's still not exactly seamless. File sync across all of Windows, Linux, macOS, iOS and Android seems to involve either proprietary user-hostile sync tools or dealing with lots of jank. As far as I can tell, the only competitor with a similar feature set that even claims to be open-source is Proton Pass. But I can't find any information on whether the server side can be self-hosted. reply selfhoster11 7 hours agorootparentKeePass as an ecosystem (and possibly other file-based ecosystems) is something I’ve used for around a decade, and while it’s not perfect, I am 100% sure it will be there for me in another decade. I want to own my passwords, and KeePass feels like a safe pair of hands that won’t turn hostile when I’m not looking. IMO, the secret to keeping the passwords synced with KeePass, is to make sure your client has a direct feature to sync the passwords database to a remote server - SFTP, DAV, SMB, etc. Then all you need to do is to set up a single remote file share to serve that file. Or sync manually, assuming your passwords change slowly - KeePass 2 can sync changes automatically between KDBX files. reply jasonjayr 5 hours agorootparentprevWindows, Linux, Android are pretty easily + reliably covered by SyncThing. What's the best story for iOS or MacOS? reply terminalbraid 2 hours agorootparentAndroid is not anymore https://forum.syncthing.net/t/discontinuing-syncthing-androi... reply luoc 3 hours agorootparentprevMeanwhile: https://news.ycombinator.com/item?id=41895718 While there seems to be an alternative, the root cause of Google's hostility to foss apps does not make me optimistic about this being a stable solution in the long run :( reply apitman 4 hours agoparentprevButtercup[0] looks promising. [0]: https://buttercup.pw/ reply jaggs 6 hours agoprevThey've now locked the Github issue to further discussion. Apparently one possible alternative is Vaultwarden? https://github.com/dani-garcia/vaultwarden Such a shame that this keeps happening with open source projects once the money people step in. reply echoangle 6 hours agoparentVaultwarden is a replacement for the server, the issue here is the license of the desktop client as I understand it. reply jaggs 6 hours agorootparentYes indeed. I was thinking this might spur more Vaultwarden client dev? reply jpeeler 2 hours agoparentprevSpeaking of Mr García, he recently started working at Bitwarden. Would be interesting to hear his thoughts. reply apitman 4 hours agoparentprevDoes Vaultwarden support passkeys? reply trissi1996 2 hours agorootparentYes, they work flawlessly for me. reply azophy_2 4 hours agorootparentprevhavent used it yet, but Github's readme contain \"feature\" section that lists \"YubiKey and Duo support\" reply jaggs 3 hours agorootparentprevI believe so -https://github.com/dani-garcia/vaultwarden/discussions/3355 reply beretguy 9 hours agoprevThe actually git issue title in linked web page: > Desktop version 2024.10.0 is no longer free software I'll have to watch the situation closely and migrate away when need be. reply dicytea 5 hours agoparentFrom a comment in the linked issue: > Note that the SDK is used (and enabled as a feature flag) not only in the release of desktop app, but also in the browser, CLI and web clients. reply ferbivore 9 hours agoparentprevThe mobile clients also have this proprietary dependency, from what I was able to tell. reply brimstedt 7 hours agoprevHaven't used bitwarden so not sure about it's festureset, but I'm looking into passbolt which seems nice and with self hosting options. reply elliotwu 8 hours agoprevEnshittification is a natural process, like aging and corrosion. I adopt a 3-2-1 backup strategy for my Bitwarden password-protected exports, which can be decrypted without needing Bitwarden. In addition, I use a separate non-Bitwarden solution for my MFA secrets. This minimizes damage and facilitates migration in the event Bitwarden degrades, or becomes outright malicious like Raivo. The same would apply to the password manager I'd switch to after Bitwarden in the near future, and any other password manager thereafter. reply 6ak74rfy 2 hours agoparentThis enshittification is surprising for Bitwarden, given how much it emphasized its open source strategy and that practically made a bunch of us recommending it to our friends and family. But maybe not too much because, as you say, its a natural process for organizations. This is primarily the reason I am careful going deep into the Tailscale ecosystem (which, similar to earlier Bitwarden, is touting a \"hey, we are the good guys\" horn for now). My network is a critical piece of my infra and I don't want to put too much trust in one company. reply de6u99er 3 hours agoprevThis is dumb becaue a lot of companies gladly pay for a supported versions of open source products. At many vompanies the use of open source us actually prohibited because of the lack of warranties. But it's a combination of greed and managment iconpetence which leads many succesful open source products from quick wins (after the initial shock) into gradually losing customers to alternative technologies reply colesantiago 9 hours agoprevYou should not trust any software that needs to raise money from VCs that markets itself as 'free software' https://techcrunch.com/2022/09/06/open-source-password-manag... reply Gys 7 hours agoparenthttps://www.vcnewsdaily.com/Bitwarden/venture-funding.php reply pixxel 6 hours agoprev>Spirit of open source died long time ago. Open source is now a business model. >According to me, the spirit of open source still lives in free software philosophy. The spirit of. reply wg0 9 hours agoprevEarlier, open source was about creativity, innovation, curiosity and sharing that with a wider ecosystem as a reactionary movement to closed walls that hinder curious minds to look under the hood how things exactly work. Lately, it seems like open source is mostly a marketing gimmick of gathering free traction early on without spending tons of money on advertising and then later gradually pulling the rug. Examples galore. At this point, I'd assume any open source project in past five to eight years taking this trajectory at any point. reply OutOfHere 9 hours agoparentOpen source software by organizations is at risk of becoming not open. Open source software by individuals is still fine, however. reply wg0 8 hours agorootparentTrue, VCs and private equity wherever they touch, things will change. These companies would love to build their empires on open source libraries, tools, frameworks and languages off the efforts of such individuals but would not give back enough and in most cases nothing at all. This imbalance needs to be recognised. reply bigfatkitten 9 hours agorootparentprevUntil those individuals form a company around their project. reply OutOfHere 8 hours agorootparentMore specifically, it is large VC funding or private equity ownership that does it in. As I see it, not having a lean operation, growing the company and expenses too fast without a commensurate growth in revenue and in profit, is the root of the evil. In contrast, a lean self-bootstrapped firm ought to be much less at risk of becoming not open. reply evanelias 4 hours agorootparent> In contrast, a lean self-bootstrapped firm ought to be much less at risk of becoming not open. Sadly, in my direct experience, VC-backed competitors will just use the bootstrapped firm's open source work as free R&D. Or even use their bootstrapped open source code in a way which directly competes with the bootstrapped business. And they'll hire marketers and directly target the bootstrapped firm's customer base. When the bootstrapper complains, the VC-backed companies all just proclaim \"You shouldn't have chosen an open source license if you didn't want this to happen!\" ... which is correct legally (the licenses don't prohibit this behavior), but blatantly ignores the complete destruction of the social contract which makes independent / bootstrapped open source possible. reply trissi1996 2 hours agorootparentAGPL prevents this easily, the reason this happens so often seems to be that way too many devs default to MIT/Apache and other way too permissive licenses and are then surprised when that permissiveness is used against them. reply de6u99er 3 hours agoparentprevMore like having the open source community report bugs, feature requests (ideas) and code until the product is good enough to transition into full commercial. reply robshep 6 hours agoprevSo much whining here. You have absolute freedom in truly open source software at the point of any particular release. So, you have the freedom to fork or self-build/host at discrete time points. Assuming software made by a company to remain and persist truly open source (compatible)is idiotic. Praise the freedoms you have had for this time. The constant criticisms will likely mean that new companies or new products will never opt for open source in the future . And that is a poorer outcome for the world. reply mhx1138 6 hours agoparentCompanies should not opt for and advertise with open source, if they don’t stand behind open source principles. Classic bait and switch. That’s what upsets users. They chose Bitwarden over e.g. LastPass, because they believed in FOSS. Companies exploit that and it’s sad. reply pixxel 5 hours agoparentprevYou blew the dust off this barely used account to post your position. Want to add a disclaimer? reply chickahoona 4 hours agoprev [–] I hope not to offend anyone, but if you want a real open source password manager take a look at Psono. I am Sascha, the main developer behind it and we have no VC money nor any plan to do a stunt like this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bitwarden's desktop version 2024.10.0 is no longer considered free software due to a new dependency, @bitwarden/sdk-internal, which limits its use to Bitwarden applications only.",
      "This change has raised concerns among users as it contradicts open-source principles, prompting some to consider alternatives or fork the previous version.",
      "Bitwarden has recognized the issue and intends to address it, emphasizing that the SDK and client are distinct programs, reflecting a broader trend from open-source to proprietary software models."
    ],
    "commentSummary": [
      "Bitwarden is experiencing criticism for shifting away from being fully open-source, causing concern among users who valued its open-source commitment.",
      "The move is perceived as a possible precursor to acquisition, drawing parallels to previous actions by CEO Michael Crandell, despite assurances from the CTO about maintaining GPL (General Public License) compatibility.",
      "Users are exploring alternatives such as KeePassXC and Proton Pass, reflecting the broader challenges and criticisms companies face when transitioning from open-source to proprietary models."
    ],
    "points": 153,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1729414289
  },
  {
    "id": 41891799,
    "title": "Regarding our Cease and Desist letter to Automattic",
    "originLink": "https://wpfusion.com/business/regarding-our-cease-and-desist-letter-to-automattic/",
    "originBody": "Frank Morrison October 19, 2024 at 2:50 pm > I hear that he’s a great person to work for This does not match my experience as a former Automattician. Kudos to you for acting swiftly and protecting your wonderful plugin! I hope others will follow your example. Reply",
    "commentLink": "https://news.ycombinator.com/item?id=41891799",
    "commentBody": "Regarding our Cease and Desist letter to Automattic (wpfusion.com)147 points by kemayo 19 hours agohidepastfavorite47 comments glenstein 17 hours agoIt's puzzling that in Automattic's reply to the cease and desist, they argued the listing of WPFusion was fair use. However, they copied it to their commercial Wordpress site and offered it as part of their \"business plan\" and also their paid premium plan. Their letter makes it sound like they are \"identifying\" it, like a bird watcher pointing to a bird or something. But they copied, re-hosted it and are, or were, offering it as part of the value proposition for which people should pay. I'll admit I'm fuzzy on fair use as anything more than a general concept but it's hard for me to square that one. reply IncRnd 16 hours agoparentIt's actually very straightforward. The reply was by the General Counsel of Automattic and was intended to reduce Autmattic's liability. To that end, the first thing that was stated is that they had done nothing illegal. reply bigiain 15 hours agorootparentThe Narcissist’s Prayer (by Dayna Craig) That didn’t happen. And if it did, it wasn’t that bad. And if it was, that’s not a big deal. And if it is, that’s not my fault. And if it was, I didn’t mean it. And if I did, you deserved it. I for one and very much starting to feel like I'm in an abusive relationship with WordPress... reply bzmrgonz 15 hours agorootparentIn automattic's defense, there are not many tools to fight open-source corporate freeloaders. We've seen these free-loaders before, AWS's freeloading on elasticsearch and others. Usually the free-loader eventually realizes that it needs in-house expertise to customize and build bespoke solutions to their customer base, in which case, why not contribute back right, but we've never seen a freeloader like WPEngine, where they change the code to disable what would otherwise be gotten for free, in addition to diverting revenue stream to their own coffers. reply chii 12 hours agorootparent> there are not many tools to fight open-source corporate freeloaders. and by design. These are freeloaders because open-source software's intention is to allow for people to freeload off it! Why should \"corporate\" freeloaders be considered any different than just a regular freeloader? Just because they have some money? If said software do not wish to have any freeloaders, don't use licenses that allow it. Use something like dual licenced AGPL or use a non-opensource license. As an example, Epic's Unreal Engine is not open source, but source available. It's licensed so that hobbiests can use it free of charge, but high revenue generating projects must pay. It's a good license, as it makes their intentions completely clear and on the outset. > AWS's freeloading on elasticsearch and others. if elastic doesn't want it, they could've followed the same licensing model as Unreal Engine. But of course, elastic wanted the exposure and network effect of a truly open source license, but wanted to collect when someone manages to make money off it. reply bawolff 11 hours agorootparent> and by design. These are freeloaders because open-source software's intention is to allow for people to freeload off it! Preach! I'm tired of people complaining about other people using open source software in the exact way it was intended as \"freeloaders\". If you want people to pay you for your software, then just make it proprietary. There is nothing wrong with proprietary software if that's what you want to do. But if you make it open source, you can't complain when people use it in an open source fashion. reply dylan604 10 hours agorootparentWhat do you call software that makes the code available to view, but has a license that forbids actually doing anything with the code other than run it? reply chii 9 hours agorootparentSo a proprietary license, and the software is called proprietary software. reply nahnahnahnah 2 hours agorootparentprevCockroachDB created what they call Business Source Licensing intended to leave the code open but prevent what AWS did to ElasticSearch. reply kemayo 2 hours agorootparentprevYou’ll often see “source available” used for that. reply marcus0x62 6 hours agorootparentprevYou’ve never seen a shared hosting provider that makes config changes to limit the use of resource-heavy features? You… Must not have seen many hosting providers. reply serial_dev 11 hours agorootparentprevIt’s open source, there are no freeloaders, not AWS, not WPEngine, they just take what everyone else takes according to the license. Either you have a legal case, or you don’t, everything else is just yapping. If you want to protect your business against “freeloaders”, choose a licensee appropriately. And yes, changing the code they run is fair game, especially if nobody cares about that feature. Let’s not pretend that whatever they disabled is such a core feature of WordPress, but somehow basically none of their users cared or care about it even to this day. Showing up a decade later, saying “nice business you got going on here, it would be a shame if something happened to it, better give me 8% of it, to make sure it is protected” is something I’d expect from a mafia movie, not blog hosting. reply bigiain 14 hours agorootparentprev\" ... where they change the code to disable ... \" This is blatantly untrue. WPEngine have repeatedly said they launch WordPress code that is bit for bit identical to the zip file published on wordpress.org. That's a claim that could _very_ easily be disproven if it were not true. I don't think WPEngine are entirely blameless with regard to the WooCommerce Stripe plugin stuff. But I have two thoughts about that, firstly I think it's sleazy of Matt/Automattic to have slipped in referral monetisation codes into a payment plugin in the first place, and secondly the GPL explicitly allows WPEngine to change the code however they choose anyway. And the \"nuclear war\" Matt's declared and is fighting is all around WordPress trademarks, not WooCommerce or the Stripe plugin code anyway. Both sides come out looking sleazy over Stripe commisions here though. The ElasticSearch/Redis/MongoDB vs AWS things are quite different in my opinion. They weren't purely relying on trademarks to \"Al Capone\" Amazon into handing over money they're not entitled to. Instead they all relicensed their code using something the OSF no longer considers \"Open Source\" using instead things like \"Business Source Licenses\", which they could do because they owned all the copyright to all the code, either having written it themselves or having contributors assign copyright to them. Matt/WordPress/Automattic _cannot_ do this, because they do not own copyright in all the code. WordPress itself was a fork of previously existing B2 code who's authors still own copyright but granted anybody (including WordPress) a license to use it under GPL2+ terms. I am almost certain (but do not know for sure) that WordPress does not have a copyright assignment agreement from everybody that's written new code that's made it's way into WP core (and that wouldn't have helped anyway since it was all built on \"viral\" GPL code so any changes also needed to be GPL2+). I'm 100% certain WordPress have no copyright ownership over almost all of the 60 thousand odd plugins on wordpress.org. reply AlotOfReading 15 hours agorootparentprevAm I misunderstanding your point? Changing default configs from upstream is an utterly normal practice in webhosting. That's why config options exist and having someone else make reasonable choices is one reason why you might choose a host. reply cscurmudgeon 13 hours agorootparentprevA though: any open-source project which worries about \"corporate freeloaders\" despite its license is not truly open-source. reply zugi 17 hours agoparentprev> However, they copied it to their commercial Wordpress site and offered it as part of their \"business plan\" and also their paid premium plan. That part is absolutely fine and is covered in the original article under \"Wait, aren’t WordPress plugins open source and free to modify\": ( https://wpfusion.com/business/regarding-our-cease-and-desist... ) You can copy, fork, and even \"sell\" free software if you like, as long as you comply with its license terms. This is a trademark dispute, not a copyright dispute. The trademark dispute here is the use of the phrase \"Advanced Custom Fields\" in Automatic's marketing and on their website, as illustrated in the picture just above that section of the article. Automatic's core response is probably correct in a technical detail: e.g. in marketing for \"Open Office Sheets\", you are allowed to say \"Open Office Sheets is better than Microsoft Office Excel because...\" Even though \"Microsoft Office Excel\" is trademarked by Microsoft, you're using the name to draw clear distinctions, not to imply endorsement, nor to confuse people into thinking you're selling their product. It would be an interesting legal case because even though Automatic may have been using the phrase in an allowable way, the result did create confusion in customers. Customers aren't reading the full sentences of the article, they're using Google to find something, clicking \"buy\", and then later contacting WP Engine for support for something they bought from Automatic. That's proof of the confusion that Automatic created. reply glenstein 17 hours agorootparent>The trademark dispute here is the use of the phrase \"Advanced Custom Fields\" in Automatic's marketing and on their website, as illustrated in the picture just above that section of the article. Yes, I read that section. But two things. I feel like that's restating the same point I was already making as though it were an elaboration, which is frustrating. Yes, they are using the disputed terms, not just \"Advanced Custom Fields\" but also \"WP Engine\". And secondly, it may be true in some notional sense that doing such a thing as using the term just in a comparative reference to something else (e.g. OpenOffice vs Microsoft Office) is fair use. But that's not what Automattic is doing, which is precisely why it's puzzling that invoking that sense of \"fair use\" is their response to a charge that is about their use in a much more specifically commercial context. reply bigiain 15 hours agorootparent> The trademark dispute ... Is something Matt started, when he suddenly went after WPEngine for trademark use after decades of their use - during which time Matt praised and even invested in their business. And he has publicly admitted trademark use not the real issue he has, but that he's using it as leverage in his attempt to extort WPEngine out of a huge amount of money that he's not legally entitled to - telling The Verge what he's doing is like Al Capone getting arrested/charged for taxes. The last sentence in the OP is 100% true: \"But as long as Matt’s motivations with WordPress are tied to his profits at Automattic, he can’t be trusted.\" (I'd go further and say there's pretty much nothing Matt can do now to regain the trust in him that's been lost.) “In my role as owning WordPress.org, I don’t want to promote a company, which is A: legally threatening me and B: using the WordPress trademark. That’s part of why we cut off access from the servers.” From: https://www.theverge.com/2024/10/4/24262232/matt-mullenweg-w... “The analogy I made is they got Al Capone for taxes,” Mullenweg says. “So, if a company was making half a billion dollars from WordPress and contributing back about $100,000 a year, yes, I would be trying to get them to contribute more.” reply duskwuff 16 hours agorootparentprev> Automatic's core response is probably correct in a technical detail... The legal term for this is nominative use [1] - sometimes \"nominative fair use\", although it's unrelated to the doctrine of fair use in copyright law. [1]: https://en.wikipedia.org/wiki/Nominative_use reply yarg 15 hours agorootparentprev\"Advanced Custom Fields\" is a trademarkable term? Beyond the point that custom fields are pretty much always advanced, there has to be a point at which a term is so fundamentally non-specific that it cannot be subject to trademark. I just think it's bloody madness sometimes. reply onli 10 hours agorootparentIn the last discussions about that it was stated the trademark wasn't granted. Exactly, too generic. ACF is the active trademark. reply zugi 14 hours agorootparentprevSingle English words cannot be trademarked. However, if you string two of them together, and demonstrate that you are actively using the phrase in commerce, you can get the phrase trademarked for use in a particular domain, e.g. computer software. Three words is even better. Note that a tractor manufacturer could still trademark \"Active Custom Fields\" for agricultural equipment, because it would not be confusingly similar to the \"Active Custom Fields\" software. Also trademarks have to be renewed every 5-10 years and you must show that you are actively using it. reply boolemancer 14 hours agorootparent> Single English words cannot be trademarked. Um... Apple? Shell? Alphabet? Chevron? Target? Caterpillar? Oracle? Orange? reply docdeek 11 hours agorootparentI think the key is that these are trademarked for a particular domain. If I tried to sell software as Alphabet I'd be shut down, but my kid's teacher doesn't need a license to teach the alphabet. reply anon7000 17 hours agoparentprevI mean. The situation is actually very simple. WordPress.com has a “business/pro” plan that lets you install plugins. Lower level plans do not allow that. (WordPress.com was initially just a big multisite with everyone’s blogs running off the same WP instance. That would only work securely if you disable custom code.) So this paid business plan is just normal WordPress hosting where you get plugins and advanced features like SSH access. WordPress.com also has a re-skinned admin experience that is more modern looking than wp-admin. Within the past couple years, WordPress.com extended that modern skin to the plugins page. It’s (as far as I know) data from the core plugin repo, just with a more modern look/experience. In fact, the WordPress.com code for this is totally open source. I’m not 100% sure how that’s not fair use — any WP host could do that to the plugin page of WP that they install. Other hosts just tend to be more hands off. reply chucky123 17 hours agorootparentThis also makes matt's initial claim about wpengine being a bastardized fork hilarious since wordpress.com is even more hacked up. Also, you cannot install most plugins on wordpress.com unless you're on the business plan. reply legitster 18 hours agoprevI'm interpreting this as a somewhat tongue-in-cheek response to what Matt is doing. Matt has been trying to walk back all of his claims about open source and private equity and pretend it's all a trademark dispute. Well, he doesn't have a leg to stand on for that argument either. And the whole trademark thing is... frankly just incredibly lame in the first place. \"Going nuclear\" and trying to launch a public crusade is one thing if there was an actual moral high ground at play. But even if you are in the right... average people do not care about your trademark claims. Do it with lawyers behind closed doors. Attacking your userbase and buying out employees over... a trademark? Shutting your mouth was free, as is admitting you were wrong. reply alfiedotwtf 15 hours agoparentThis. I could understand a trademark issue if he had done this the second WP Engine opened business, but it is WAY too late to claim they are now defending their trademark. His legal council must be externally contracted yes men getting paid by the hour because there is no way a lawyer with equity would have said this was a good idea. I also suspect he did all this without talking to legal first :slapforhead: reply pavlov 10 hours agoprevIs Wordpress dying? This kind of infighting is usually a sign of a shrinking market and stagnating product. In haven’t used Wordpress in probably fifteen years. Back then its main quality was ease of setup on prevailing hosting environments of the day. Everything else was a horrible mess that was patched over with plugins that did far too much with too little support from the system. It felt like the MS-DOS of web publishing. Did they ever graduate to a “Windows NT” stage, or is today’s Wordpress still the same PHP hairball? reply ceejayoz 3 hours agoparent> is today’s Wordpress still the same PHP hairball? PHP has moved on, but WordPress hasn't. Lots of long-EOLed PHP 5 shared hosting installs sitting out there, and they take great strides to keep things compatible as a result. No composer, global classes all over, etc. reply kemayo 18 hours agoprevTl;dr Wordpress.com has a sketchily-authorized copy of the Wordpress.org plugin directory that’s used as a funnel into signing up for a Wordpress.com hosting account. This was causing the author problems because it has better google rankings than the .org, and they were getting support requests from people who thought buying a plan on Wordpress.com meant they got the premium version of their plugin. So they C&D’d Automattic to remove them from the copied directory. This is only related to the recent WP Engine drama insofar as the author says it made them lose trust in Matt. reply lapcat 17 hours agoparent> This is only related to the recent WP Engine drama insofar as the author says it made them lose trust in Matt. I think that's understating the situation a bit, because the WP Engine drama was the provocation for the current action. Very Good Plugins learned of the WP Fusion copy in September 2023 but did nothing about it until October 2024. \"I didn’t want to start a legal dispute with Automattic and possibly damage our reputation. And, generally, I trusted Matt. I assumed he had a competent legal team that had already reviewed the legality of copying our plugins, and that he was acting in the best interest of the community.\" reply Aurornis 18 hours agoparentprevI’m amazed that Matt got away with claiming the Wordpress.org/.com duality for so long. Recent events have revealed that the dividing line between the two is virtually non-existent and Mullenweg used whichever organization was most convenient for pushing his agenda at the moment. M Mullenweg’s recent attempt to dunk on DHH for not capturing enough value from the Rails community is the most succinct mask-off moment of the recent events, in my opinion. He thought he was making a great point by criticizing DHH for not being extractive enough from the rails community. Instead, he only revealed his true thought process about how being at the center of an open source community should be a license to extract from said community. Really puts recent events into perspective. reply bigiain 15 hours agorootparentAt least for me, the \".com .org duality\" looked OK, because the existence of The WordPress Foundation as a non profit which owned trademarks, and the as it turns out incorrect assumption that they owned and ran the .org platform. The revelation that Matt considers he personally owns wordpress.org outright and can do whatever he chooses there even if it's diametrically opposed to the Foundations stated goals, dramatically changes the risk evaluation in relying on anything to do with Wordpress. reply ookblah 12 hours agorootparentprevnot that it should matter in \"principle\", but the stupid thing to me is that he's not some small time developer trying to make ends meet. he's extracted quite a bit from from it... in the hundreds of millions, but it's still not enough. i could even understand if he suddenly went full value extraction mode. there are ways to do it ruthlessly and coldly, but his actions have this angle of being petty as well. his blatant disregard for WP users while trying to gaslight them into thinking he's their champion, general \"assholery\" (for lack of a better word) to those who have been in the ecosystem for a long time who dissent. i guess the mask is truly off. reply glenstein 18 hours agorootparentprev>Mullenweg’s recent attempt to dunk on DHH for not capturing enough value from the Rails community is the most succinct mask-off moment of the recent events, in my opinion. He thought he was making a great point by criticizing DHH for not being extractive enough from the rails community. Is it possible to do a short version or medium version summary of that, and/or a link where I can find out more? reply unsnap_biceps 17 hours agorootparentHN discussion can be found at https://news.ycombinator.com/item?id=41839864 and contains archived versions of the post that has now been replaced. reply pevey 17 hours agorootparentprevhttps://archive.ph/4yLNR reply x0x0 15 hours agorootparentprevThe very short summary is Matt basically told DHH he sucked for not making billions off Rails and patronizingly suggested he get a business coach. DHH replied that made him happy / proud, giving a gift like that to the world. reply colinprince 17 hours agoprevhttps://archive.is/SvuYi just in case reply sroussey 18 hours agoprevWill I get into any trouble for starting a hosting site called WordPress Engine? What if I licensed WordPress? reply ceejayoz 18 hours agoparentYes; WordPress is trademarked. WP is not. WordPress's trademark policy, until late last month, stated \"The abbreviation 'WP' is not covered by the WordPress trademarks and you are free to use it in any way you see fit.\" \"WP Fusion\" is also, apparently, a trademark. reply djbusby 17 hours agoparentprevWordFusionEnginePressCom.org reply AlienRobot 15 hours agoprev [–] tl;dr version: \"Hey, that trademark is mine, take it down.\" \"Okay.\" I don't get why there are still so many articles being written about Wordpress drama. There are no gotchas. Nobody can force Wordpress to provide anybody updates for free. You have all the right to enforce your trademarks if you think Wordpress wronged you. Somehow, despite all of these articles, I still haven't seen a single one from WP Engine itself. I'm fairly sure they just deployed their own update repository in the deadline they were given and that was it. I have no idea where it will go from here, but I doubt it matters much. reply docdeek 11 hours agoparent [–] I imagine a reason that WPEngine has been relatively quiet is that they have competent legal counsel who have advised them to remain silent. While WPE has let their lawyers handle things, Matt has refused to shut up and, consequently, has fueled the drama and arguably put himself in a worse position legally. reply AlienRobot 3 hours agorootparent [–] I wonder if they will ever say anything, though. I'm honestly kind of tired of seeing posts about WP drama on several different platforms, and this has been going on for weeks. I had to unsubscribe from #wordpress on Mastodon because almost every post was about this... reply ceejayoz 3 hours agorootparent [–] > I wonder if they will ever say anything, though. \"Never interrupt your opponent while he is in the middle of making a mistake.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Automattic responded to a cease and desist letter about WPFusion's listing by claiming fair use, even though it was part of their paid plans, sparking debate over open-source software and trademark issues.",
      "Critics argue that Automattic's actions blur the distinction between WordPress.org (open-source) and WordPress.com (commercial), raising concerns about the intentions of Matt Mullenweg, a key figure in the WordPress community.",
      "The situation underscores ongoing tensions in the open-source community regarding corporate use and trademark enforcement."
    ],
    "points": 147,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1729382415
  },
  {
    "id": 41889990,
    "title": "Autism's Four Core Subtypes",
    "originLink": "https://www.thetransmitter.org/spectrum/untangling-biological-threads-from-autisms-phenotypic-patchwork-reveals-four-core-subtypes/",
    "originBody": "Despite the huge variation in how autistic people experience the condition, they can be divided into just four subgroups, according to a preprint. The people in these groups—who share similar traits and life outcomes—carry gene variants that implicate distinct biological pathways, the researchers found. Each group is associated with specific genetic variants that influence gene expression at different stages of development, the investigation revealed. The work includes genetic and phenotypic data from more than 5,000 autistic children. Identifying autism subtypes has been on the minds of autism researchers for many years, says Thomas Frazier, professor of psychology at John Carroll University, who was not involved in the work. “But what’s really cool is that they link that to the underlying biology. To my knowledge, this is the first study to do this in a comprehensive, replicable way.” Autism traits vary dramatically from person to person, and that diversity is reflected in the hundreds of genetic variants linked to the condition. But in most cases, genetic differences haven’t been reliably mapped to autism traits, and two people with the same variant can show different phenotypes. That lack of progress could stem from focusing on a single trait at the expense of the interactions among multiple genetic variants, says lead investigator Olga Troyanskaya, professor of computer science and integrative genomics at Princeton University and deputy director for genomics at the Flatiron Institute. “It’s not as simple as a single variant associated with a single phenotype. It’s a highly complex process,” she says. (The Flatiron Institute, and the datasets Troyanskaya’s team used in the study, are funded by the Simons Foundation, The Transmitter’s parent organization.) ” With more data and potentially even better phenotyping, there may be more than these four categories. — — Anoushka Joglekar Instead, Troyanskaya and her colleagues investigated the variants associated with a person’s collection of characteristics. They applied a statistical model to data on the traits and behaviors of 5,392 autistic people from the SPARK research cohort. By adjusting the number of groups, the team found the most significant similarities among participants when the model sorted the cohort into four autism subtypes. The largest group—consisting of 1,976 people—shows mild challenges in core autism traits, whereas the smallest—554 people—has severe difficulties across those same traits. The other two subtypes are somewhere in between: One group specifically experiences social challenges and disruptive behavior, and the other shows developmental delay and difficulties in select traits. T he algorithm organized data from an independent cohort, the Simons Simplex Collection (SSC), in a similar way, the team found. Considering the differences in the data—clinicians gather information for the SSC, whereas SPARK consists of parent-filled questionnaires—the consistency “is a very good sign that we’re finding true, clinically relevant biology here,” Troyanskaya says. People belonging to the same subtype often share the same co-occurring diagnoses, further analysis found. Those with social difficulties and disruptive behavior, for instance, are more likely than the other groups to have a diagnosis of attention-deficit/hyperactivity disorder or anxiety, and those in the strongly affected group are more likely to show cognitive impairment. People of the same subtype often harbor the same autism-linked common variants, according to a comparison of polygenic scores for 2,293 of the autistic children and 3,179 unaffected siblings from the SSC. And a search for rare mutations uncovered more spontaneous—or de novo—variants among the strongly affected group, whereas those with developmental delay harbor more inherited variants. Variants associated with each subtype influence different biological pathways, hinting that distinct mechanisms underlie autism’s phenotypic differences. For example, the variants detected among the mildly autistic group alter histone methylation, whereas those identified in the developmental delay subtype affect neuronal action potentials. T he variants contribute to changes in gene expression at different points in development, the study also found. The subtype characterized by developmental delay—which consisted of children diagnosed with autism at an early age—possesses variants that affect gene expression during fetal and neonatal development. And those with social problems and disruptive behavior—who are often diagnosed later in childhood—harbor variants that influence gene expression after birth. The findings were posted on medRxiv in August. The results suggest that a subtype’s genetic profile could—at least in part—determine clinical milestones, says study investigator Aviya Litman, a Ph.D. candidate in Troyanskaya’s lab. Given more data, the team’s approach could inform parents and guardians of potential milestone delays, Litman and her colleagues say. In fact, the researchers plan to train their model with more data as they become available, including noncoding and whole-genome sequences, says Natalie Sauerwald, associate research scientist of computational genomics at the Flatiron Institute and an investigator on the study. More detailed genetic data could provide more insight into the mechanistic differences between autism subtypes, Sauerwald says. And larger datasets may reveal more nuance between autism subtypes. “With more data and potentially even better phenotyping, there may be more than these four categories,” says Anoushka Joglekar, a computational biologist at the biotech company Immunai, who was not involved in the work. By analyzing whole genomes, the team may uncover important epigenetic mechanisms, she adds.",
    "commentLink": "https://news.ycombinator.com/item?id=41889990",
    "commentBody": "Autism's Four Core Subtypes (thetransmitter.org)134 points by domofutu 23 hours agohidepastfavorite101 comments geor9e 20 hours agoReminds me a bit of the MBTI personality test, where they make up 4 types of question to arbitrarily split the population, so 2x2x2x2 = 16 subtypes. It's true by it's own definition. Which is fine, but are these particular arbitrary boundaries useful? Perhaps. Could the splitting lines have been just as useful in different arbitrary place. Perhaps. A lot of people who take the MBTI find they're on the boundaries flip-flopping into a few different pigeonholes depending on different times they take the test. So it's important to let people know they can be in multiple buckets (are a bit in all of them), and take a little advice from each community. For this one they split autism into 3 groups (core,social,developmental) then split core into (mild,severe), to make 4 total. reply Y_Y 19 hours agoparent> Troyanskaya and her colleagues investigated the variants associated with a person’s collection of characteristics. They applied a statistical model to data on the traits and behaviors of 5,392 autistic people from the SPARK research cohort. By adjusting the number of groups, the team found the most significant similarities among participants when the model sorted the cohort into four autism subtypes. https://www.medrxiv.org/content/10.1101/2024.08.15.24312078v... Not my field, but the statiscal analysis seems legit. reply pfisherman 18 hours agorootparentHave not read the article in depth yet, but I am very familiar with the Troyanskaya lab. They are good and their articles are always worth a read. They are using a geodesic finite mixture model, which I am not familiar with, but seems to be an extension of mixture models to non Euclidean metric? Pleiotropy can be tricky though, as traits / diagnoses / observations can have causal relationships amongst themselves. For example, a variant that impacts obesity may be statistically associated with heart attacks; but the relationship between the variant and heart attacks is not directly causal in the sense of something like a variant that alters function of an ion channel expressed in cardiac smooth muscle. reply zahma 9 hours agoparentprevIt’s probably more helpful to think of them as spectra or overlapping circles of behavior. When we talk about psychological disorders, they are clustered and only useful insofar as they allow professionals to talk to one another and mean the same things. Undoubtedly each human displays a unique case and manifestation of whatever behavior we choose to specify, and that individuality can pull from different “trait” or “behavior” groups on a scale. It’s only when we start to lump together many cases that we begin to discern between types and patterns, but it’s silly to make a harsh distinction between clusters and then pretend like they are emphatically different from the next. Grouping can have its uses, but we shouldn’t forget each person presents uniquely. reply marxisttemp 3 hours agorootparentWell said. Psychological diagnoses are genres for the brain. reply kelseyfrog 19 hours agoparentprevReminding is different than being the same. For example, Big5/OCEAN/FiveFactor model uses factor analysis which has statistical reasoning behind the number of factors. In fact, the semantic meaning of the factors comes up after the analysis, not before. It's the same as clustering or GMM, the semantic meaning of the clusters is applied after the partitioning. The concept of partitioning people is the same, however, the order by which meaning is applied is opposite which makes them completely different in practice. reply bippihippi1 18 hours agorootparentdescriptive vs prescriptive reply kelseyfrog 17 hours agorootparentData driven vs some writers who read Jung. reply pfannkuchen 18 hours agoparentprevIf the data is a roughly uniform distribution across the dimension space, then yes absolutely. If, however, there is significant clustering within the dimension space, and those clusters are taken as groups, then it seems valid. I would tend to be generous to the researchers and assume a mostly discrete clustering until I see otherwise. To apply grouping like this without discrete clusters would be unprofessionally naive on their part. reply kobenni 45 minutes agorootparentThe distribution of the data is also the main problem with MBTI, AFAIK. Intra-/extraversion for example have a unimodal distribution with most people being somewhere in the middle. And exactly that middle is where MBTI makes the cut on their classification. reply kelseyfrog 37 minutes agorootparentThis is a great point. We have a generally tendency to typify in binary[black and white] terms. It overrides our ability to express nuance and un this case pushed out a larger category (if we even wanted to still categorize) of ambiverts. MBTI speaks more on our drive for categorization than it does on the people being categorized. reply h0l0cube 20 hours agoparentprevThey also posit correlations with epigenetic differences. If there’s distinct biological mechanisms at play, this gives some credence to splitting autism into separate conditions reply colechristensen 18 hours agoparentprevThere’s a better way to frame this issue: being able to consistently label a characteristic is only useful if it can help in identifying the underlying mechanism or is helpful in choosing an effective treatment. A whole lot of mental health knowledge fits into the category of repeatably identifiable categories of dubious usefulness. Think phrenology: it is indeed possible to categorize the shape of people’s head in a statistically valid way… it just has nothing to do with their mental health. reply d0mine 13 hours agorootparentIf we consider science as a way to document reality (among other things) then \"information -> decision\" is an elegant point of view (if information doesn't help make decision, it is just noise). https://news.ycombinator.com/item?id=41881872 It reminds me of \"brain (mostly) for movement \" (no decisions to make, no brain is necessary). Sea squirts being a prime example: \"As an adult, the sea squirt attaches itself to a stationary object and then digests most of its own brain.\" (Rodolfo R. Llinas I of the Vortex: From Neurons to Self) https://www.amazon.com/Vortex-Neurons-Rodolfo-R-Llinas/dp/02... reply LeonB 16 hours agorootparentprevAre you a libra? This sounds like something a libra might would say! /s “Repeatable identifiable categories of dubious usefulness” Yep, there’s a lot of this in modern science. “Is this method of classification useful? wrong question! All you need to ask is, is it easy to write a reproducible paper (or shareable web content) on this method of classification? Yes, yes it is!” There’s a lamppost problem at work here. reply colechristensen 16 hours agorootparentEh, it's more like the DSM was written primarily with the concept of billing and charting in mind. If you track and bill medical conditions, you need a consistent language for things that will be consistent across medical providers. It solves a problem, just not the problem most people think it solves. https://en.wikipedia.org/wiki/DSM-5 reply w23b07d28 20 hours agoparentprevIt's not true by its own definition, it's just that if you soak it in, you will actually start to see the patterns behind it among people. It's still going to be pseudoscience because there are a lot of variables, but it often works and even more often people don't know what to look at or what to put together, because you'll find really A LOT of articles on the Internet that try to run mechanics on certain types without understanding what or why. I assume that if you want to put this together more precisely, there is a lot of scope here. And lest it be said that I'm talking out of turn myself, I only became interested in this whole MBTI thing because an ENFP once told me I was an INTP after a few hours of talking about silly things. That's exactly what these tests once told me. Of course, these are still anecdotes, but we are sciency. Is it a problem that someone has catalogued autism in this way? Is it a question of lack of precision or bad direction? Am I asking the wrong questions? reply kijin 14 hours agorootparentIt's true by definition because MBTI splits people into two groups, four times in a row. Everyone is going to fall into one bucket or another, just as the last 4 bits of any integer is going to be one of 16. The question is whether the distinction has any practical uses. My guess as to why MBTI is more popular than other equally valid personality tests is that the splits are defined in such a way as to capture a few traits that people find the most irksome in their everyday relationships. You want to take a rational approach, but your SO gets all emotional? Well, that's a T vs. F thing. It might not be medically meaningful, but IMO it does help us appreciate that other people's minds might be wired differently. Not wrong, just different. Business idea: AI companions with custom personality out of the box. Choose the MBTI type you want! reply greesil 21 hours agoprevAs someone with a close family member who is autistic, I am always bothered by the phrase \"if you have met one person with autism, you have met one person with autism\". Autism as a diagnostic classification is so broad that the non verbal are lumped in with those with rigidity behavior, when at least to me they seem like they should just have a different diagnosis. This is not just playing semantics if they're able to correlate against specific sets of genes. This work seems highly relevant, IMO reply jtsiskin 21 hours agoparentThe expression you quoted is completely agreeing with you! It’s a play on the expected idiomatic ending “then you have met everyone with autism”, pointing out that the diagnosis is broad and everyone is different reply greesil 21 hours agorootparentWhy even have the word autism? It's almost meaningless. reply furyofantares 18 hours agorootparentA characteristic autistic trait is having a narrow and deep tunnel of attention. Perhaps so narrow and deep that you're unable to learn language, because it requires a more holistic processing, and less focus on individual components. Perhaps so narrow and deep that you're overwhelmed by sensation, processing every touch, sound, sight stimulus individually, leaving little energy to put everything together. Or perhaps only so narrow and deep that you are extremely focused on math. Or collecting insects. Or memorizing train routes. I don't know if this is an explanation. But it is extremely plausible for a wide variety of outcomes to be usefully categorized by a singular trait. reply cogman10 20 hours agorootparentprevThe symptoms cluster together and are related. Someone with sensory issues is also likely to have food aversions, for example. It's also useful for diagnostics and treatment. It means you don't have to fight insurance as much or need rediagnostics to get needed therapies. I don't need to get my child with food aversions, speech delay, and sensory issues a new diagnosis for each just because some people with autism don't have those issues. reply zug_zug 4 hours agorootparentI guess then my thought is that whether it's one disorder or 4 or any number perhaps is best understood as a statistical question. For example, if a parent exhibit autism symptom X (e.g. trouble understanding emotions), are there kids more likely to inherit symptom X, or ANY autism symptom. If X is uniquely heritable, then perhaps it's best as multiple disorders. But if X leads equally likely to X, Y, or Z then it's better understood as one disorder. reply mindslight 19 hours agorootparentprevThat sounds like a problem with the medical gatekeeping industry rather than anything fundamental. Like a blanket diagnosis of \"human\" would get you the same thing, but for the middlemen realizing that would completely destroy one of the levers they use to defraud. reply lox 18 hours agorootparentWith a prevalence rate ofit crowds out more nuanced understanding. Like even the word \"spectrum\", trying to add some depth to the pop culture model, is really just a fancy word for a single scalar. I just disagree with this take. For people with autism, the broad criteria help to serve as guideposts for common experiences shared by those with autism. When doing treatment, everyone gets into the specifics of what autism means for the individual. What you are complaining about is similar to someone complaining that cancer is too broad of a term. After all, the word cancer describes a spectrum of mutations and symptoms everywhere in the body. reply mindslight 2 hours agorootparentHow about for people \"without autism\" that have some of the characteristics (probably everyone), trying to examine their own mental workings (ideally more people) ? How about for people with \"mild autism\" that have now been labeled by the medical system as being distinct from people \"without autism\", even though the main difference was merely passing some arbitrary threshold? The difference with cancer is that cancer is an unequivocal negative. You can't be just \"a little cancerous\" and just embrace it. Whereas autism we're seemingly talking about variances in distinct components of what makes up intelligence. So setting some arbitrary threshold below which you're \"fine\" and above which you have a \"problem\" is really an artifact of the medical industry and larger economic system rather than actual mechanics. reply lox 11 hours agorootparentprevI don’t disagree that pop culture has distilled spectrum down into a magnitude, but that isn’t how the DSM describes it or how professionals diagnose it (or in my experience how they communicate it). The metaphor is supposed to be like the light spectrum not “less autism ranging to more autism”. Severity scale is distinct to interacting traits of social issues and restricted interests and repetitive behaviors (the spectrum bit). reply mindslight 2 hours agorootparentWhen I said single scalar, I was referencing the light spectrum - it is literally just less energy ranging to more energy (per photon). We just experience it so vibrantly as different colors (etc) because the difference between specific energies are quite important at the level of our existence. So unless there is a single underlying factor whose magnitude causes all of the different distinct traits of autism, it's a poor analogy. reply BoiledCabbage 1 hour agorootparentTwo beams of light of equal intensity and different frequency contain equal energy not differing amounts of energy. The actual difference in frequency is the composition of that energy. If of course you compare a dimmer beam of light with a brighter one the dimmer one will have less energy. So no less energy is due to lower intensity light, not due to different frequency. You can pretty trivially have 5 flashlights each with a different color of light and all with the same energy. reply mindslight 1 hour agorootparentSure. The different \"compositions\" of that energy is the spectrum - it's a single scalar. You're adding one more dimension of dim/bright, making the entire description be two scalars. Go look up the definition of \"spectrum\", and contrast with \"gamut\". reply idiotsecant 19 hours agorootparentprevSo it solves a problem, then? Its useful? Saying a problem would not be a problem if the universe was a little better is not particularly useful. reply _moof 15 hours agorootparentprevYou may be interested in reading Wittgenstein on trying to define what a \"game\" is. In short he found that there are no conditions necessary or sufficient to make something a game. Nevertheless, games exist. https://en.wikipedia.org/wiki/Family_resemblance reply bugglebeetle 12 hours agorootparentIt’s amazing how much Wittgenstein anticipated. You can see here how his philosophy anticipated meaning as being fundamentally a probabilistic superposition of the relationships between words, with no fixed form, predicting today’s model architectures: > for instance the kinds of number form a family in the same way. Why do we call something a \"number\"? Well, perhaps because it has a direct relationship with several things that have hitherto been called number; and this can be said to give it an indirect relationship to other things we call the same name. And we extend our concept of number as in spinning a thread we twist fibre on fibre. And the strength of the thread does not reside in the fact that some one fibre runs through its whole length, but in the overlapping of many fibres. reply Frummy 19 hours agorootparentprevEach seed may land in a unique place, being swept by the wind from the same tree. reply throwawaymaths 19 hours agorootparentprevBecause autism got a shit ton of research funding from scared parents a decade ago reply Spivak 21 hours agorootparentprev\"hot\" is still a meaningful word even though 100 F°, 1000 F°, and 1,000,000 F° aren't comparable at all. They're nonetheless still all experiencing heat. reply blargey 20 hours agorootparentYes, if we could pin it to a linear scale of Degrees Autistic (Farenheit), that could be estimated with reasonable precision for all day-to-day relevant values by feeling the nearby air on your skin, nobody would complain about \"Austism\" being too broad. reply o11c 19 hours agorootparentUsually it's at least 4 scales with no strong correlation (and a couple more that are correlated more). They do have them. Thus, it is incorrect to refer to Autism as a \"spectrum\". Instead, we should correctly call it a \"manifold\". reply Spivak 20 hours agorootparentprevAm I missing something you can though. That's actually kinda how it is. I detest the phrase \"high functioning\" but that group is roughly your outside temperatures. You'll notice the difference between 30° and 80° and the same temperature 72° can feel different in the summer, winter, before it rains, when it's humid/dry but is still the same intensity. Then there's 1000° degrees where (and this is someone I know) he stripped naked, ran through downtown, and yelled at random restaurant workers calling them fascists for not lettting him in and then got into a fight with the cops. I think broadly that's what the \"spectrum\" characterization is meant to convey. And you should expect this, in code there's one happy path and a million different ways to err, some more catastrophic than others. reply bbarnett 20 hours agorootparentprevIt seems to me that something could be hot enough, that you could vaporize before your nerves signal your brain. reply Muromec 18 hours agorootparentprevSo the normies have a way to explain weirdness and go on with their day. reply kelseyfrog 33 minutes agoparentprevTo me the point of the phrase was to counter prejudice and gatekeeping. \"You can't be autistic because I know another autistic person who does the opposite,\" is a sentiment that autistic people in my life have encountered. It doesn't extend to researchers because (arguably) researchers aren't in the business of telling people that because they are uncomfortable with behaviors associated with autism. reply saghm 21 hours agoparentprev> Autism as a diagnostic classification is so broad that the non verbal are lumped in with those with rigidity behavior, when at least to me they seem like they should just have a different diagnosis. As someone with a diagnosis, I've gotten into the habit of referring to myself as \"on the spectrum\" when discussing with people rather than using a specific term. It helps emphasize the fact that there's a range of potential manifestations, and (hopefully) helps remind people that their expectations based on past experience might not fit my behavior exactly. reply Jensson 21 hours agoparentprevMerging autism with asperger went the exact opposite direction of where it needed to. reply NeuroCoder 20 hours agorootparentThe problem was that a diagnosis of Asperger's was unreliable and therefore useless. We definitely need to identify individuals within the diagnosis of autism spectrum disorder that can reliably be identified and benefit from specific interventions. However, Asperger's did not provide that. reply faeriechangling 15 hours agorootparentNo, the problem was that psychiatrists could bill insurers more to treat autism than they could Asperger's. You aren't cynical enough. If it wasn't a scientific distinction, why are we still identifying autism subtypes? reply TeaBrain 13 hours agorootparentDo you have any information on patients could be billed more for an autism diagnosis? I've never heard this claim before. reply faeriechangling 11 hours agorootparentMostly anecdotal, my school psychologist back in the day sure believed it, and this would vary from place to place. She was a champion of \"You give children the diagnosis that gives them the services they need\". Autism being the one which gave children the services they needed, and she often expressed frustration at not being able to get such a diagnosis. In general Aspergers basically meant no verbal delays, where Autism meant verbal delay. Autism was also around longer as a diagnosis. In general, I think theres a reason they changed the name of Aspergers to Autism and not the other way around. reply spondylosaurus 59 minutes agorootparentI thought they changed the name in part because Hans Asperger was a Nazi collaborator. reply NeuroCoder 10 hours agorootparentprevIt can be helpful to get a diagnosis of autism for kids in public school. Kids end up needing additional one on one time and resources are limited. Those with the biggest problems are the first to be approved for these resources, and a formal diagnosis makes it easier to get that approval. reply didufis 14 hours agorootparentprevI think this is precisely why for-profit healthcare is wild. If it weren’t for ideology we could get behind socialised care and cut out all of the nonsense. reply TeaBrain 13 hours agorootparentprev>diagnosis of Asperger's was unreliable and therefore useless If this is true, then in what way is the diagnosis of autism reliable, that the Asperger's label was not? reply TeaBrain 16 hours agorootparentprevI'd argue that having the descriptor of \"asperger's\" is much more useful than simply having a blanket descriptor of \"autism\". Low functioning people who are described as having autism, have very little in common with most of the high functioning type. reply throwawaymaths 19 hours agorootparentprevThe problem was that asperger was a problematic physician at best. reply CalRobert 15 hours agorootparentprevIt almost feels like a cynical ploy to nerf autism so insurance companies don't have to pay for anything. There's a huge, huge, difference between \"I'm awkward\" and \"my kid cannot talk\". reply SpicyLemonZest 21 hours agorootparentprevThe reasons why the change was made (https://www.thetransmitter.org/spectrum/why-fold-asperger-sy...) still make sense to me. The autism spectrum is quite wide, and I'd 100% believe something meaningful coming from the source study, but the specific category of Asperger's was based on factors that don't seem to matter much and weren't being reliably evaluated. reply hcfman 13 hours agoprevI’m quite sick of all the studies that only talk about problems. If a side effect of some autistic conditions can mean an advantage their area of deep focus can this not also be celebrated ? Often even people with deep autism with associated problems achieve extraordinary things through their focus and persistence. Otherwise all is presented is negative with no positive. I’d quite like to be seen also from the positive side when it’s present thank you. I’m happy that I’m not neurotypical and please can’t they cancel trying to cure this. Sounds all too much like eugenics. reply faeriechangling 9 hours agoparentAutism is literally medically defined as a series of deficits, latest criteria = DSM-V-TR https://www.reddit.com/r/aspergirls/comments/th9hku/dsm5tr_n... The only positive thing said about autism in that entire gigantic write-up is \"Special interests may be a source of pleasure and motivation and provide avenues for education and employment later in life\" Why would doctors treat autism as being anything but negative with no positive when Autism is literally defined in that way? Of course, people since Leo Kanner and Hans Aspergers noted Autistics having extraordinary abilities, and people are vaguely aware of this, but doctors hold autistics in worse contempt than the general population mostly because you can't bill insurance to treat a \"difference\" or get a study grant to research a \"difference\". So the system they're in forces them to treat autistics as contemptible and even in need of curing. Besides that, psychiatry is after all the study of mental illness and disorders, not of mental differences, so there's a bias just from training. Legal, UN definition, trying to cure a mental disorder is not eugenics. It is logically the same idea as eugenics though, the UN just didn't outlaw this idea since the mentally disordered were considered sufficiently inferior. Autistics can also be banned from sperm banks. There's nothing to be done really as a mere not neurotypical internet dweller. The inertia of the status quo is like a train and many people benefit from it. The only real choice you have is to call yourself something like \"Not neurotypical\" instead of \"Autistic\". reply randomNumber7 7 hours agorootparentBecause people with autism have often a very high IQ and also are often very talented in some things (while beeing bad at others). This combination can be very useful for specialized taks. Like programming, math, research ... reply ywvcbk 5 hours agorootparent> people with autism have often a very high IQ I don’t think that’s necessarily true even if we only consider those with high-functioning autism (and the average is significantly lower if we include everyone). Some people with autism have a very high IQ, just like some people who don’t have it. e.g. https://pmc.ncbi.nlm.nih.gov/articles/PMC9058071/ reply dyauspitr 11 hours agoparentprevTry telling that to my neighbor whose kid constantly throws violent tantrums multiple times a week. It’s very unfortunate, it happened to some very nice people. reply tomrod 16 hours agoprevHierarchical clustering applied to brain scans. As a clustering output, it's useful to conceptualize but not definitive of what one can expect from behavior nor development. If I am not mistaken, this is the original paper.[0] [0] https://www.nature.com/articles/s41593-023-01259-x reply alilleybrinker 13 hours agoparentYou are mistaken. The work described here is based on this paper, currently published as a preprint: https://www.medrxiv.org/content/10.1101/2024.08.15.24312078v... This preprint is the first link in the article. reply skissane 17 hours agoprevI’m sceptical of research based on a single diagnostic label - such research assumes that the boundary between that specific diagnosis, other diagnoses, and no diagnosis, is valid - while (at least some) research which tests that assumption rather than making it, by including cohorts with other diagnoses and also no diagnosis, ends up challenging that assumption - e.g. https://www.nature.com/articles/s41398-019-0631-2 - but if the challenge is correct, how much validity is there in research which relies on it? reply kelseyfrog 26 minutes agoparentAs you should be. When we go spelunking for evidence of differences we're engaging in the reinforcement of social categories. When evidence of differences is found it becomes the justification for the category's existence. If that seems backwards and circular in logic, then you would be correct. The same interplay exists between sex and gender and well as a host of other categories. reply NeuroCoder 19 hours agoprevIt's important to note that models that use genome wide association analysis have demonstrated extremely high predictive value across large cohorts sharing geography but are very poor when applied on a geographically distinct population. This suggests that although autism has a strong association with genetics, neurophysiology unique to autism develops in the context of highly complex genetic associations that are likely subject genetic drift across population and time. When we have a a few genes of interest that are important in screening for a rare disease we accept that novel variants will continue to be identified throughout the years as more people are screened. Autism as a prevalence of 1-3%. I don't remember the exact number but I think something like 30% of autism diagnoses are believed to be secondary to fairly severe but distinct genetic syndromes. So when we talk about a subgroup of autism without clear etiology we are looking at a fraction of 70% of 3% of the population. We're approaching rare disease territory when we talk about subgroups within autism. A rare disease with a highly complex genetic association across the genome that is subject to genetic drift is not a good candidate for genetic screening. All that being said, studies like this may provide valuable insight into what microbiology is being influenced, even if we can't reliably predict which variants are responsible. Id love to see future investigations relate genetics to biomarkers instead of behavioral tests in autism. reply maeil 13 hours agoparentDid GPT help you write this, or do you just happen to write in the same style, conditioned by having to write research literature in that style? reply NeuroCoder 3 hours agorootparentIt's the latter plus typing on my phone while riding the bus. reply ruthmarx 16 hours agoparentprev> This suggests that although autism has a strong association with genetics, neurophysiology unique to autism develops in the context of highly complex genetic associations that are likely subject genetic drift across population and time. Wouldn't it be more likely local culture and brain plasticity are the cause of these differences? reply NeuroCoder 10 hours agorootparentAutism can be reliably diagnosed at 2.5 years of age, so we probably aren't looking at a strong cultural bias in in how kids are presenting with autism. These models using data from countries where the healthcare systems provides support for appropriate screening and testing further minimizing bias due to culture. In other words, one would not expect these models to be overfit to phenotypically distinct representations of autism as a result of culture. Variations in brain plasticity have been suspected of playing a role in autism for a long time. Brain plasticity may vary by region, throughout development, and at a molecular level. If population dependent variations in plasticity are indeed responsible for the lack of model generalizability, then the next step would be to do as I previously suggested. That is, identify where the many genome wide variations converge on biomarkers driving differences in plasticity. reply ruthmarx 9 hours agorootparent> Autism can be reliably diagnosed at 2.5 years of age, so we probably aren't looking at a strong cultural bias in in how kids are presenting with autism. These models using data from countries where the healthcare systems provides support for appropriate screening and testing further minimizing bias due to culture. I'm not talking about bias due to culture but culture having a real physical effect on development. It wouldn't be noticeable in toddlers but would in older children, if present. > Brain plasticity may vary by region, That was my point, that local culture can play a large role in shaping how autism can manifest. > then the next step would be to do as I previously suggested. That is, identify where the many genome wide variations converge on biomarkers driving differences in plasticity. I think it would make sense to evaluate adults from different cultures with same or similar autism types and compare and contrast their neurology. reply faeriechangling 15 hours agoprevGlad they're working away at this. I've stopped identifying as autistic despite having a dx because I was tired of the way it got me treated worse by the general public who presume I'm far more incompetent than I actually am and medical practitioners who do the same. Last time I sought help for a totally unrelated issue they asked if I had any prior dx and I was foolishly honest so I was subsequently referred to a day program for unemployed people with severe mental health issues. I'm full time employed so I couldn't even attend. This is the outcome of treating \"autistics\" as a monolith. After the DSM-IV they decided to consolidate 3 different autism spectrum diagnosis's into one. They said this was scientific, but IMHO, this was done so everybody with Aspergers or PDD-NOS to be eligible for the higher funding levels those categorised as \"Autistic\" got. Which has done nothing but take away funding from severely disabled people while causing more mildly impaired people to receive inappropriate and condescending treatment, I am fucking eligible for a full time day program where a social worker will be paid to chaperone me because I'm just so fucking needy that's just what needs to be done. This change was great for the psychiatrists through, who simply received more money as a result of making this diagnostic change, nice conflict of interest there. As a society, we decided having certain labels of disability entitled you to special legal rights and certain financial benefits. Where I live any autism diagnosis, no matter how severe or mild, gets you a flat amount of money, whereas other diagnosis's even if the condition is more severe than autism get nothing. What do you think parents, who can shop around from a variety of private psychiatrists, looking to access therapy for their child that autistic kids also get do? So you might get a kid with ADHD or Anxiety or some speech disorder, and not getting an autism DX, but getting treated like a moderately impaired autistic. I honestly loathe, absolutely loathe the status quo of autism diagnosis and treatment and am happy for any change to be made that splits the diagnosis up because it creates at least a hope of \"Severe autism\" receiving higher funding levels than the rest, and \"autism light\" starting to get similar funding levels to other disabilities so there isn't a perverse incentive. I have not made an online comment on any website in 1 month but this inspired me. reply __turbobrew__ 14 hours agoparentThis may be spicy, but I wonder if autism has a higher rate of occurrence in upper middle class homes which is why it has got so much more attention and funding than more serious illnesses as you say. reply sakjur 12 hours agorootparentIt seems to me that we’ve always had many people both on the spectrum and with ADHD, but our modern society has such stiff norms that the people who are the most involved with society sees these otherwise often fairly benign variations as diseases to be medicated away. Schools herd children through something so rigid that a lot of people are told they’re problematic, lazy, stupid, or insufficient in one of millions of ways. And then that continues into adulthood with work and, in a context familiar to many around this site, frameworks for how work should be done. People talk about estimates, and working in bursts or worrying about something your professionalism tells you is important but your boss tells you to ignore become a problem to fix. Finding a level in the hierarchy and a pace that works well for you and being content there is lacking ambition and being lazy. I guess when a society is sick, its members are diagnosed. reply faeriechangling 10 hours agorootparentCounterpoint: https://www.youtube.com/watch?v=dntgFIzNKrc First Reference to ADHD Found to be in 1753 by Kloekhof. ADHD has been observed since pre-modern times. Autism on the other hand is a modern condition, but I don't think there's really much dispute there's SOMETHING there because you see things like savants who are just inexplicable. However, the rate at which we're diagnosing people today is totally unprecedented. The DSM and ICD-11 are also more like medical dictionaries than rigorously scientific reflections of underlying biological reality. They describe what Autism and ADHD are, but the categorisation is largely based on convention, clinical convenience, and a desire to fit a certain nosology rather than actual science. I've been looking into the alternative frameworks like RDOC and HITOP. Anyways we're diagnosing people a lot more often nowadays, increasing the patient population, but still acting like research done on a much smaller patient population still holes up. Adaptive Behaviour Analysis therapy for instance is still insured in the United States based on research from the 90s when the average autistic child was very different than an average 2024 autistic child (not to say there hasn't been more research since than), and generally I see money and entrenched laws and bureaucratic guidelines and incentives as creating a sort of system which has no evidence of helping anybody which coincidentally results in a lot more money changing hands and more people getting government money. Anyways I think the current system we have where we pretend that it's useful to say that Elon Musk and some guy who smashes his head into the wall until it bleeds to self-stimulate have the same disability strains credulity. reply kreyenborgi 11 hours agorootparentprevIn Norway (with a completely different system of public health funding), children of immigrants have several times higher chance of being diagnosed as autistic. Results were \"adjusted for parents' education and income\". I have no idea how to interpret this. (Hopefully diagnoses didn't involve testing immigrant children on their command of the Norwegian language.) https://www.nrk.no/norge/barn-av-innvandrere-far-oftere-auti... https://pubmed.ncbi.nlm.nih.gov/36609392/ reply mock-possum 15 hours agoprevFour classes, to wit: > We identified one class … referred to as Social/Behavioral … that demonstrated high scores (higher difficulties) across core autism categories of social/communication and restricted/repetitive behaviors, as well as disruptive behaviors, attention, and anxiety, and no reports of developmental delays > A second class … called Mixed ASD with DD … shows more nuanced presentation within certain categories, with some features enriched and some depleted among the restricted/repetitive behavior, social/communication, and self-injury categories, and an overall strong enrichment of developmental delays > The last two classes scored consistently lower (fewer difficulties) and consistently higher than other autistic children across all seven categories. These two classes were termed Moderate Challenges and Broadly Impacted, respectively. reply 93po 21 hours agoprevsort of off topic, but does anyone else have the experience of consistently having bad interactions in real life any time autism is discussed? like the reasons are so varied but it's so consistently not great. i feel like it's rooted in a \"i know autism better than you and i feel threatened anytime something is expressed that differs from my own opinion/experience with it\". and sometimes people are offended with any opinion or anecdote or experience expressed on it at all in a \"don't mansplain autism to me\" sort of way (i'm not a man, just to be clear, and obv mansplaining isnt unique to men). not saying this to be unkind or mean to anyone. it just feels like such a super touchy topic. i started completely not engaging in conversation around it at all and pretending like i don't know anything about it. reply uncletaco 21 hours agoparentNo. The greatest interaction I had was when a kid didn’t hold a door for me and his mom said “I’m sorry he has Asperger’s and it’s a little heavy on the ass.” Then we ended up in the same waiting room for a while and she talked about accommodating her son and how they managed it. What was really interesting is the kid looked like it was the first time he heard his mom talk to someone else about it and him and I could tell he was embarrassed but really loving her that moment. reply cuttysnark 21 hours agorootparent> it's a little heavy on the ass I hadn't heard this idiom before so I looked it up and I couldn't find anything relevant. What does this phrase mean in this context? reply umanwizard 20 hours agorootparentThe first syllable of Asperger sounds like “ass”, and the mom was making a pun based on that fact (pointing out that her son’s Asperger’s made him an ass). reply cuttysnark 20 hours agorootparentSaying it out loud made it click. Thank you. reply vampirical 18 hours agoparentprevYou’re probably going to think I’m very presumptuous but I’m going to say this anyway in case it is helpful for you. If people are frequently offended when you speak on a topic you’re probably being offensive somehow through content or delivery. In my experience the thought “[they] feel threatened anytime something is expressed that differs from […]” is not accurate and it also turns off your brain on trying to figure out what is actually going on. I can recommend from personal experience a small apology and transition to more listening in that moment. If more feels appropriate, giving it some time and space and re-engaging gently to discover what went on for them in that moment can yield a lot of value for both sides. reply 93po 56 minutes agorootparenti appreciate the response, this is something i have already considered and i agree there could have been some element to my tone or delivery that contributes to their reaction, though i also don't necessarily claim responsibility for their reaction. i agree the wording on my post you're replying to comes across the way you're describing. i have more nuanced thoughts about this but it's a lot to type, but will end by saying thank you for the kindly worded comment reply Fnoord 15 hours agoparentprevMy wife and I are on the spectrum hence we read up on it regularly. Especially when we were first diagnosed. Lately we read more on it given our kids are likely on the spectrum with our oldest being in a diagnosis trajectory. One major thing which annoys me is when autism is being used as a curse-word. Such as 'no autism please' when looking for people to play with in an online game. The irony being this is possibly (non-diagnosed) high functioning autistic who don't want to play with low(er) functioning autistic. One thing to do in such a case is join and prove them wrong (that you are good enough) and then leave saying 'btw I am on the spectrum'. But it generally isn't worth the hassle. If someone uses such language, they're likely toxic, so I just avoid them. Plus, there is the ignore list. This works reasonably well. Though I also use this to ignore people whom I find under-performing. But one thing which comes to mind is that even though I am on the spectrum it is simply untrue you are compatible with other people who are on the spectrum. For example, when I am under stress I make certain sounds (coping) which my wife finds super annoying. I also remember a guy being hardcore Christian and on the spectrum (he gets annoyed when I curse), while we are agnostic. reply 93po 54 minutes agorootparent> But one thing which comes to mind is that even though I am on the spectrum it is simply untrue you are compatible with other people who are on the spectrum. the experiences i've heard from the people around me is that someone with neurodivergence is significantly less likely to be friends with someone else who also has it. im not making a generalization that this is commonly the case, it's just frequent feedback in my specific social circle reply nullc 11 hours agorootparentprev> One major thing which annoys me is when autism is being used as a curse-word. Such as 'no autism please' when looking for people to play with in an online game. I wouldn't read that as a curse word. We always have to abbreviate when we communicate. One way to read \"(no autism please)\" is \"I've had negative experiences playing with people exhibiting behaviors that I think others (rightly or wrongly) understand as autism and/or I've complained to players about their conduct and received autism as an excuse/justification. I'm not currently feeling up to dealing with that-- maybe due to my own pathologies--, so if you think you might behave that way please find someone else who is feeling more tolerant.\" And that's something we can sympathize with, even if sometimes feeling excluded can be disappointing, or knowing that someone might stereotype you might feel reductive and objectifying. But at the same time, no one is entitled to anyone elses time, and it's usually better that someone tells you their preferences rather than feeling those things and saying nothing. reply Fnoord 5 hours agorootparentHow would you feel if they'd say 'no disabilities please'? Or if you are POC and they say 'no black people please'? You'd feel excluded for something you are; something you cannot change. It is flat out discrimination. There's an easier variant for: \"I've had negative experiences playing with people exhibiting behaviors that I think others (rightly or wrongly) understand as autism and/or I've complained to players about their conduct and received autism as an excuse/justification. I'm not currently feeling up to dealing with that-- maybe due to my own pathologies--, so if you think you might behave that way please find someone else who is feeling more tolerant.\" And it is: 'no toxicity please'. Because autistic or not, toxicity has no excuse. You will not get away with it hiding behind a(ny) disability when you receive a warning and/or ban. Nor would you being a POC allow you to be toxic. reply 93po 53 minutes agorootparentjust wanted to say i understand your perspective and i think maybe the GP is maybe not fully understanding it. \"no autism please\" is really unkind for the reasons you laid out reply tapland 20 hours agoparentprevI see this but usually only when someone who has never saught a diagnosis or have any shared experiences keeps referring to their supposed autism in a community with a lot of autistic people who will backlash. Never seen a discussion go bad _about_ it. reply faeriechangling 15 hours agoparentprevYes because the diagnostic label is over broad to the point of meaninglessness and people have their own independently valid reasons for wanting autism to be interpreted as either a mild or severe disability because the diagnostic label is over broad to the point of meaninglessness. Autism and \"severe autism\" in particular need to be addressed using totally different words. reply ggm 16 hours agoparentprevIt is certainly one of several topics which really demand some caution to raise, such as religion, abortion, women's reproductive rights and the sexuality/gender debate, not to mention trigger warnings, campus DEI politics and academic freedom. I am always abashed when I walk into this space and frequently resent it when I am led into it by others. I like to imagine during the late 18th and early 19th century it was equally hard to have dispassionate discussions about slavery. In the 1920s alcoholism, and feminism. Since many years in many cultures homosexuality and according to Pierre telhard de Chardin almost always since we have looked back in time incest. I think he argued for a genetic component but perhaps not in as many words. Consider the Rosenbergs, or David Irvine, or Sir Anthony Blunt, and how rancorous debate got. Or, F.R. Leavis and the academic career paths for professors of English at that time. I knew academics whose careers were ruined by being in the wrong department at the wrong time by that. Or eugenics which until the Nazi rise to power was a tolerably normalised debate. Personally, I am glad we cast that into a different tone of voice but in context it might be indicative of like cases in times past. reply 23B1 17 hours agoparentprevThe hardest part for me is understanding the difference between legitimate diagnoses and people who use it as a sort of veneer for antisocial behavior. reply 93po 38 minutes agorootparenti mean the point of language is to communicate, saying \"i have autism\" is usually done to give context to someone's behavior or how to interact with them (though obviously autism is very different person to person). i think this is a tricky conversation because someone could argue the \"stolen valor\" of autism might cause issues when someone says they have autism, but doesn't, and then the person interacting with them uses it as a learning experience in how to interact with autistic people in the future. however i'm not really sure this holds up as a decent argument, because as it's said all the time and i said earlier, you can't make assumptions about how to interact best with any given person who has autism. you have to ask, \"is there anything i can do to make you comfortable or help?\", if it's appropriate to do so, and see what they in particular need. reply kayo_20211030 21 hours agoprev [–] I spend no time considering genetic variations, or genetic correlations, I deal with what's in front of me. This study is _almost_ backcasting; it might improve the model, but, not the outcome. Maybe the study is fine and valuable, and maybe it'll lead to something. Maybe? But, it does nothing in the present. Not in the here and now. reply tapland 20 hours agoparent [–] If we can find specific subtypes it would be extremely helpful. You can’t expect everything to be presented only with a complete solution. reply elcritch 20 hours agorootparent [–] Exactly, lumping in all autism into the same overall category when there could be very different underlying biological mechanisms would potentially block progress. For example, a medication that could work wonders on one subtype by affecting a biological mechanism unique to that subtype could be found not to meet clinical standards because it didn't work on the other subtypes or even make them worse. In other words, it's a confounding variable which needs to be discovered and characterized after which it could play significant role in advancing treatments and understanding. reply kayo_20211030 5 hours agorootparent [–] That is a fair point, or two. My frustration with studies of this type is that they provide very little guidance with respect to how to prevent a condition, how to fix a condition, or how to manage a condition. Nor, do they give me much hope that even a long way down the road will they do so. My focus, because of a family situation, is management. Whether there's one type, four types, or more than a thousand types is, to me, immaterial. Even if this study was a complete genetic explanation of the mechanism, it wouldn't help, because I don't work with genes, I work with gene expressions, manifestations and symptoms. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study categorizes autistic individuals into four subgroups based on shared traits and genetic variants, using data from over 5,000 autistic children.",
      "Each subgroup is linked to distinct biological pathways, with varying challenges in autism traits, from mild to severe difficulties, social challenges, or developmental delays.",
      "The study suggests that genetic profiles could help predict clinical milestones, and further data could refine these subtypes, as reported on medRxiv."
    ],
    "commentSummary": [
      "A study by Troyanskaya and colleagues analyzed traits and behaviors of 5,392 autistic individuals, identifying four autism subtypes using a statistical model similar to clustering in data analysis.",
      "The findings suggest that understanding autism's complexity through subtypes may be beneficial, though there is debate about whether psychological diagnoses effectively capture individual uniqueness.",
      "The discussion includes the broadness of the autism diagnosis, the merging of autism and Asperger's, and the challenges of creating scientifically valid and practically useful categories."
    ],
    "points": 134,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1729365562
  },
  {
    "id": 41894451,
    "title": "How to do distributed locking (2016)",
    "originLink": "https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html",
    "originBody": "Skip to content Martin Kleppmann Student Projects About/Contact Supporters How to do distributed locking Published by Martin Kleppmann on 08 Feb 2016. As part of the research for my book, I came across an algorithm called Redlock on the Redis website. The algorithm claims to implement fault-tolerant distributed locks (or rather, leases [1]) on top of Redis, and the page asks for feedback from people who are into distributed systems. The algorithm instinctively set off some alarm bells in the back of my mind, so I spent a bit of time thinking about it and writing up these notes. Since there are already over 10 independent implementations of Redlock and we don’t know who is already relying on this algorithm, I thought it would be worth sharing my notes publicly. I won’t go into other aspects of Redis, some of which have already been critiqued elsewhere. Before I go into the details of Redlock, let me say that I quite like Redis, and I have successfully used it in production in the past. I think it’s a good fit in situations where you want to share some transient, approximate, fast-changing data between servers, and where it’s not a big deal if you occasionally lose that data for whatever reason. For example, a good use case is maintaining request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per user ID (for abuse detection). However, Redis has been gradually making inroads into areas of data management where there are stronger consistency and durability expectations – which worries me, because this is not what Redis is designed for. Arguably, distributed locking is one of those areas. Let’s examine it in some more detail. What are you using that lock for? The purpose of a lock is to ensure that among several nodes that might try to do the same piece of work, only one actually does it (at least only one at a time). That work might be to write some data to a shared storage system, to perform some computation, to call some external API, or suchlike. At a high level, there are two reasons why you might want a lock in a distributed application: for efficiency or for correctness [2]. To distinguish these cases, you can ask what would happen if the lock failed: Efficiency: Taking a lock saves you from unnecessarily doing the same work twice (e.g. some expensive computation). If the lock fails and two nodes end up doing the same piece of work, the result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice). Correctness: Taking a lock prevents concurrent processes from stepping on each others’ toes and messing up the state of your system. If the lock fails and two nodes concurrently work on the same piece of data, the result is a corrupted file, data loss, permanent inconsistency, the wrong dose of a drug administered to a patient, or some other serious problem. Both are valid cases for wanting a lock, but you need to be very clear about which one of the two you are dealing with. I will argue that if you are using locks merely for efficiency purposes, it is unnecessary to incur the cost and complexity of Redlock, running 5 Redis servers and checking for a majority to acquire your lock. You are better off just using a single Redis instance, perhaps with asynchronous replication to a secondary instance in case the primary crashes. If you use a single Redis instance, of course you will drop some locks if the power suddenly goes out on your Redis node, or something else goes wrong. But if you’re only using the locks as an efficiency optimization, and the crashes don’t happen too often, that’s no big deal. This “no big deal” scenario is where Redis shines. At least if you’re relying on a single Redis instance, it is clear to everyone who looks at the system that the locks are approximate, and only to be used for non-critical purposes. On the other hand, the Redlock algorithm, with its 5 replicas and majority voting, looks at first glance as though it is suitable for situations in which your locking is important for correctness. I will argue in the following sections that it is not suitable for that purpose. For the rest of this article we will assume that your locks are important for correctness, and that it is a serious bug if two different nodes concurrently believe that they are holding the same lock. Protecting a resource with a lock Let’s leave the particulars of Redlock aside for a moment, and discuss how a distributed lock is used in general (independent of the particular locking algorithm used). It’s important to remember that a lock in a distributed system is not like a mutex in a multi-threaded application. It’s a more complicated beast, due to the problem that different nodes and the network can all fail independently in various ways. For example, say you have an application in which a client needs to update a file in shared storage (e.g. HDFS or S3). A client first acquires the lock, then reads the file, makes some changes, writes the modified file back, and finally releases the lock. The lock prevents two clients from performing this read-modify-write cycle concurrently, which would result in lost updates. The code might look something like this: // THIS CODE IS BROKEN function writeData(filename, data) { var lock = lockService.acquireLock(filename); if (!lock) { throw 'Failed to acquire lock'; } try { var file = storage.readFile(filename); var updated = updateContents(file, data); storage.writeFile(filename, updated); } finally { lock.release(); } } Unfortunately, even if you have a perfect lock service, the code above is broken. The following diagram shows how you can end up with corrupted data: In this example, the client that acquired the lock is paused for an extended period of time while holding the lock – for example because the garbage collector (GC) kicked in. The lock has a timeout (i.e. it is a lease), which is always a good idea (otherwise a crashed client could end up holding a lock forever and never releasing it). However, if the GC pause lasts longer than the lease expiry period, and the client doesn’t realise that it has expired, it may go ahead and make some unsafe change. This bug is not theoretical: HBase used to have this problem [3,4]. Normally, GC pauses are quite short, but “stop-the-world” GC pauses have sometimes been known to last for several minutes [5] – certainly long enough for a lease to expire. Even so-called “concurrent” garbage collectors like the HotSpot JVM’s CMS cannot fully run in parallel with the application code – even they need to stop the world from time to time [6]. You cannot fix this problem by inserting a check on the lock expiry just before writing back to storage. Remember that GC can pause a running thread at any point, including the point that is maximally inconvenient for you (between the last check and the write operation). And if you’re feeling smug because your programming language runtime doesn’t have long GC pauses, there are many other reasons why your process might get paused. Maybe your process tried to read an address that is not yet loaded into memory, so it gets a page fault and is paused until the page is loaded from disk. Maybe your disk is actually EBS, and so reading a variable unwittingly turned into a synchronous network request over Amazon’s congested network. Maybe there are many other processes contending for CPU, and you hit a black node in your scheduler tree. Maybe someone accidentally sent SIGSTOP to the process. Whatever. Your processes will get paused. If you still don’t believe me about process pauses, then consider instead that the file-writing request may get delayed in the network before reaching the storage service. Packet networks such as Ethernet and IP may delay packets arbitrarily, and they do [7]: in a famous incident at GitHub, packets were delayed in the network for approximately 90 seconds [8]. This means that an application process may send a write request, and it may reach the storage server a minute later when the lease has already expired. Even in well-managed networks, this kind of thing can happen. You simply cannot make any assumptions about timing, which is why the code above is fundamentally unsafe, no matter what lock service you use. Making the lock safe with fencing The fix for this problem is actually pretty simple: you need to include a fencing token with every write request to the storage service. In this context, a fencing token is simply a number that increases (e.g. incremented by the lock service) every time a client acquires the lock. This is illustrated in the following diagram: Client 1 acquires the lease and gets a token of 33, but then it goes into a long pause and the lease expires. Client 2 acquires the lease, gets a token of 34 (the number always increases), and then sends its write to the storage service, including the token of 34. Later, client 1 comes back to life and sends its write to the storage service, including its token value 33. However, the storage server remembers that it has already processed a write with a higher token number (34), and so it rejects the request with token 33. Note this requires the storage server to take an active role in checking tokens, and rejecting any writes on which the token has gone backwards. But this is not particularly hard, once you know the trick. And provided that the lock service generates strictly monotonically increasing tokens, this makes the lock safe. For example, if you are using ZooKeeper as lock service, you can use the zxid or the znode version number as fencing token, and you’re in good shape [3]. However, this leads us to the first big problem with Redlock: it does not have any facility for generating fencing tokens. The algorithm does not produce any number that is guaranteed to increase every time a client acquires a lock. This means that even if the algorithm were otherwise perfect, it would not be safe to use, because you cannot prevent the race condition between clients in the case where one client is paused or its packets are delayed. And it’s not obvious to me how one would change the Redlock algorithm to start generating fencing tokens. The unique random value it uses does not provide the required monotonicity. Simply keeping a counter on one Redis node would not be sufficient, because that node may fail. Keeping counters on several nodes would mean they would go out of sync. It’s likely that you would need a consensus algorithm just to generate the fencing tokens. (If only incrementing a counter was simple.) Using time to solve consensus The fact that Redlock fails to generate fencing tokens should already be sufficient reason not to use it in situations where correctness depends on the lock. But there are some further problems that are worth discussing. In the academic literature, the most practical system model for this kind of algorithm is the asynchronous model with unreliable failure detectors [9]. In plain English, this means that the algorithms make no assumptions about timing: processes may pause for arbitrary lengths of time, packets may be arbitrarily delayed in the network, and clocks may be arbitrarily wrong – and the algorithm is nevertheless expected to do the right thing. Given what we discussed above, these are very reasonable assumptions. The only purpose for which algorithms may use clocks is to generate timeouts, to avoid waiting forever if a node is down. But timeouts do not have to be accurate: just because a request times out, that doesn’t mean that the other node is definitely down – it could just as well be that there is a large delay in the network, or that your local clock is wrong. When used as a failure detector, timeouts are just a guess that something is wrong. (If they could, distributed algorithms would do without clocks entirely, but then consensus becomes impossible [10]. Acquiring a lock is like a compare-and-set operation, which requires consensus [11].) Note that Redis uses gettimeofday, not a monotonic clock, to determine the expiry of keys. The man page for gettimeofday explicitly says that the time it returns is subject to discontinuous jumps in system time – that is, it might suddenly jump forwards by a few minutes, or even jump back in time (e.g. if the clock is stepped by NTP because it differs from a NTP server by too much, or if the clock is manually adjusted by an administrator). Thus, if the system clock is doing weird things, it could easily happen that the expiry of a key in Redis is much faster or much slower than expected. For algorithms in the asynchronous model this is not a big problem: these algorithms generally ensure that their safety properties always hold, without making any timing assumptions [12]. Only liveness properties depend on timeouts or some other failure detector. In plain English, this means that even if the timings in the system are all over the place (processes pausing, networks delaying, clocks jumping forwards and backwards), the performance of an algorithm might go to hell, but the algorithm will never make an incorrect decision. However, Redlock is not like this. Its safety depends on a lot of timing assumptions: it assumes that all Redis nodes hold keys for approximately the right length of time before expiring; that the network delay is small compared to the expiry duration; and that process pauses are much shorter than the expiry duration. Breaking Redlock with bad timings Let’s look at some examples to demonstrate Redlock’s reliance on timing assumptions. Say the system has five Redis nodes (A, B, C, D and E), and two clients (1 and 2). What happens if a clock on one of the Redis nodes jumps forward? Client 1 acquires lock on nodes A, B, C. Due to a network issue, D and E cannot be reached. The clock on node C jumps forward, causing the lock to expire. Client 2 acquires lock on nodes C, D, E. Due to a network issue, A and B cannot be reached. Clients 1 and 2 now both believe they hold the lock. A similar issue could happen if C crashes before persisting the lock to disk, and immediately restarts. For this reason, the Redlock documentation recommends delaying restarts of crashed nodes for at least the time-to-live of the longest-lived lock. But this restart delay again relies on a reasonably accurate measurement of time, and would fail if the clock jumps. Okay, so maybe you think that a clock jump is unrealistic, because you’re very confident in having correctly configured NTP to only ever slew the clock. In that case, let’s look at an example of how a process pause may cause the algorithm to fail: Client 1 requests lock on nodes A, B, C, D, E. While the responses to client 1 are in flight, client 1 goes into stop-the-world GC. Locks expire on all Redis nodes. Client 2 acquires lock on nodes A, B, C, D, E. Client 1 finishes GC, and receives the responses from Redis nodes indicating that it successfully acquired the lock (they were held in client 1’s kernel network buffers while the process was paused). Clients 1 and 2 now both believe they hold the lock. Note that even though Redis is written in C, and thus doesn’t have GC, that doesn’t help us here: any system in which the clients may experience a GC pause has this problem. You can only make this safe by preventing client 1 from performing any operations under the lock after client 2 has acquired the lock, for example using the fencing approach above. A long network delay can produce the same effect as the process pause. It perhaps depends on your TCP user timeout – if you make the timeout significantly shorter than the Redis TTL, perhaps the delayed network packets would be ignored, but we’d have to look in detail at the TCP implementation to be sure. Also, with the timeout we’re back down to accuracy of time measurement again! The synchrony assumptions of Redlock These examples show that Redlock works correctly only if you assume a synchronous system model – that is, a system with the following properties: bounded network delay (you can guarantee that packets always arrive within some guaranteed maximum delay), bounded process pauses (in other words, hard real-time constraints, which you typically only find in car airbag systems and suchlike), and bounded clock error (cross your fingers that you don’t get your time from a bad NTP server). Note that a synchronous model does not mean exactly synchronised clocks: it means you are assuming a known, fixed upper bound on network delay, pauses and clock drift [12]. Redlock assumes that delays, pauses and drift are all small relative to the time-to-live of a lock; if the timing issues become as large as the time-to-live, the algorithm fails. In a reasonably well-behaved datacenter environment, the timing assumptions will be satisfied most of the time – this is known as a partially synchronous system [12]. But is that good enough? As soon as those timing assumptions are broken, Redlock may violate its safety properties, e.g. granting a lease to one client before another has expired. If you’re depending on your lock for correctness, “most of the time” is not enough – you need it to always be correct. There is plenty of evidence that it is not safe to assume a synchronous system model for most practical system environments [7,8]. Keep reminding yourself of the GitHub incident with the 90-second packet delay. It is unlikely that Redlock would survive a Jepsen test. On the other hand, a consensus algorithm designed for a partially synchronous system model (or asynchronous model with failure detector) actually has a chance of working. Raft, Viewstamped Replication, Zab and Paxos all fall in this category. Such an algorithm must let go of all timing assumptions. That’s hard: it’s so tempting to assume networks, processes and clocks are more reliable than they really are. But in the messy reality of distributed systems, you have to be very careful with your assumptions. Conclusion I think the Redlock algorithm is a poor choice because it is “neither fish nor fowl”: it is unnecessarily heavyweight and expensive for efficiency-optimization locks, but it is not sufficiently safe for situations in which correctness depends on the lock. In particular, the algorithm makes dangerous assumptions about timing and system clocks (essentially assuming a synchronous system with bounded network delay and bounded execution time for operations), and it violates safety properties if those assumptions are not met. Moreover, it lacks a facility for generating fencing tokens (which protect a system against long delays in the network or in paused processes). If you need locks only on a best-effort basis (as an efficiency optimization, not for correctness), I would recommend sticking with the straightforward single-node locking algorithm for Redis (conditional set-if-not-exists to obtain a lock, atomic delete-if-value-matches to release a lock), and documenting very clearly in your code that the locks are only approximate and may occasionally fail. Don’t bother with setting up a cluster of five Redis nodes. On the other hand, if you need locks for correctness, please don’t use Redlock. Instead, please use a proper consensus system such as ZooKeeper, probably via one of the Curator recipes that implements a lock. (At the very least, use a database with reasonable transactional guarantees.) And please enforce use of fencing tokens on all resource accesses under the lock. As I said at the beginning, Redis is an excellent tool if you use it correctly. None of the above diminishes the usefulness of Redis for its intended purposes. Salvatore has been very dedicated to the project for years, and its success is well deserved. But every tool has limitations, and it is important to know them and to plan accordingly. If you want to learn more, I explain this topic in greater detail in chapters 8 and 9 of my book, now available in Early Release from O’Reilly. (The diagrams above are taken from my book.) For learning how to use ZooKeeper, I recommend Junqueira and Reed’s book [3]. For a good introduction to the theory of distributed systems, I recommend Cachin, Guerraoui and Rodrigues’ textbook [13]. Thank you to Kyle Kingsbury, Camille Fournier, Flavio Junqueira, and Salvatore Sanfilippo for reviewing a draft of this article. Any errors are mine, of course. Update 9 Feb 2016: Salvatore, the original author of Redlock, has posted a rebuttal to this article (see also HN discussion). He makes some good points, but I stand by my conclusions. I may elaborate in a follow-up post if I have time, but please form your own opinions – and please consult the references below, many of which have received rigorous academic peer review (unlike either of our blog posts). References [1] Cary G Gray and David R Cheriton: “Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency,” at 12th ACM Symposium on Operating Systems Principles (SOSP), December 1989. doi:10.1145/74850.74870 [2] Mike Burrows: “The Chubby lock service for loosely-coupled distributed systems,” at 7th USENIX Symposium on Operating System Design and Implementation (OSDI), November 2006. [3] Flavio P Junqueira and Benjamin Reed: ZooKeeper: Distributed Process Coordination. O’Reilly Media, November 2013. ISBN: 978-1-4493-6130-3 [4] Enis Söztutar: “HBase and HDFS: Understanding filesystem usage in HBase,” at HBaseCon, June 2013. [5] Todd Lipcon: “Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1,” blog.cloudera.com, 24 February 2011. [6] Martin Thompson: “Java Garbage Collection Distilled,” mechanical-sympathy.blogspot.co.uk, 16 July 2013. [7] Peter Bailis and Kyle Kingsbury: “The Network is Reliable,” ACM Queue, volume 12, number 7, July 2014. doi:10.1145/2639988.2639988 [8] Mark Imbriaco: “Downtime last Saturday,” github.com, 26 December 2012. [9] Tushar Deepak Chandra and Sam Toueg: “Unreliable Failure Detectors for Reliable Distributed Systems,” Journal of the ACM, volume 43, number 2, pages 225–267, March 1996. doi:10.1145/226643.226647 [10] Michael J Fischer, Nancy Lynch, and Michael S Paterson: “Impossibility of Distributed Consensus with One Faulty Process,” Journal of the ACM, volume 32, number 2, pages 374–382, April 1985. doi:10.1145/3149.214121 [11] Maurice P Herlihy: “Wait-Free Synchronization,” ACM Transactions on Programming Languages and Systems, volume 13, number 1, pages 124–149, January 1991. doi:10.1145/114005.102808 [12] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: “Consensus in the Presence of Partial Synchrony,” Journal of the ACM, volume 35, number 2, pages 288–323, April 1988. doi:10.1145/42282.42283 [13] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues: Introduction to Reliable and Secure Distributed Programming, Second Edition. Springer, February 2011. ISBN: 978-3-642-15259-7, doi:10.1007/978-3-642-15260-3 If you found this post useful, please support me on Patreon so that I can write more like it! To get notified when I write something new, follow me on Bluesky or Mastodon, or enter your email address: I won't give your address to anyone else, won't send you any spam, and you can unsubscribe at any time. Subscribe Site RSS feed To find out when I write something new, sign up to receive an email notification, follow me on Bluesky or Mastodon, or subscribe to the RSS feed. I won't give your email address to anyone else, won't send you any spam, and you can unsubscribe at any time. My book My book, Designing Data-Intensive Applications, has received thousands of five-star reviews. I am an Associate Professor working on local-first software and security protocols at the University of Cambridge. If you find my work useful, please support me on Patreon. Recent posts 05 Jul 2024: Pudding: user discovery for anonymity networks 04 Jan 2024: 2023 year in review 12 Oct 2022: Verifying distributed systems with Isabelle/HOL 03 Jan 2022: Book Review: The Future of Fusion Energy 01 Sep 2021: Several podcast interviews Full archive Conference talks 30 May 2024 at Local-First Conference 27 Feb 2024 at Local First (LoFi) meetup 06 Nov 2023 at IETF-118 Decentralization of the Internet Research Group 19 Oct 2023 at KASTEL Distinguished Lecture Series 27 Sep 2023 at ACM Tech Talks Full archive Unless otherwise specified, all content on this site is licensed under a Creative Commons Attribution 3.0 Unported License. Theme borrowed from Carrington, ported to Jekyll by Martin Kleppmann.",
    "commentLink": "https://news.ycombinator.com/item?id=41894451",
    "commentBody": "How to do distributed locking (2016) (kleppmann.com)127 points by yusufaytas 8 hours agohidepastfavorite65 comments jroseattle 4 hours agoWe reviewed Redis back in 2018 as a potential solution for our use case. In the end, we opted for a less sexy solution (not Redis) that never failed us, no joke. Our use case: handing out a ticket (something with an identifier) from a finite set of tickets from a campaign. It's something akin to Ticketmaster allocating seats in a venue for a concert. Our operation was as you might expect: provide a ticket to a request if one is available, assign some metadata from the request to the allocated ticket, and remove it from consideration for future client requests. We had failed campaigns in the past (over-allocation, under-allocation, duplicate allocation, etc.) so our concern was accuracy. Clients would connect and request a ticket; we wanted to exclusively distribute only the set of tickets available from the pool. If the number of client requests exceeded the number of tickets, the system should protect for that. We tried Redis, including the naive implementation of getting the lock, checking the lock, doing our thing, releasing the lock. It was ok, but administrative overhead was a lot for us at the time. I'm glad we didn't go that route, though. We ultimately settled on...Postgres. Our \"distributed lock\" was just a composite UPDATE statement using some Postgres-specific features. We effectively turned requests into a SET operation, where the database would return either a record that indicated the request was successful, or something that indicated it failed. ACID transactions for the win! With accuracy solved, we next looked at scale/performance. We didn't need to support millions of requests/sec, but we did have some spikiness thresholds. We were able to optimize read/write db instances within our cluster, and strategically load larger/higher-demand campaigns to allocated systems. We continued to improve on optimization over two years, but not once did we ever have a campaign with ticket distribution failures. Note: I am not an expert of any kind in distributed-lock technology. I'm just someone who did their homework, focused on the problem to be solved, and found a solution after trying a few things. reply nh2 3 hours agoparentYou are right that anything that needs up to 50000 atomic, short-lived transactions per second can just use Postgres. Your UPDATE transaction lasts just a few microseconds, so you can just centralise the problem and that's good because it's simpler, faster and safer. But this is not a _distributed_ problem, as the article explains: > remember that a lock in a distributed system is not like a mutex in a multi-threaded application. It’s a more complicated beast, due to the problem that different nodes and the network can all fail independently in various ways You need distributed locking if the transactions can take seconds or hours, and the machines involved can fail while they hold the lock. reply fny 22 minutes agorootparentYou could just have multiple clients attempt to update a row that defines the lock. Postgres transactions have no limit and will unwind on client failure. Since connections are persistent, there’s no need to play a game to determine the state of a client. reply stickfigure 3 hours agoparentprevI think this illustrates something important, which is that: You don't need locking. You need . In your case, the constraint is \"don't sell more than N tickets\". For most realistic traffic volumes for that kind of problem, you can solve it with traditional rdbms transactional behavior and let it manage whatever locking it uses internally. I wish developers were a lot slower to reach for \"I'll build distributed locks\". There's almost always a better answer, but it's specific to each application. reply nasretdinov 3 hours agoparentprevSo basically your answer (and the correct answer most of the time) was that you don't really need distributed locks even if you think you do :) reply tonyarkles 47 minutes agorootparentHeh, in my local developer community I have a bit of a reputation for being “the guy” to talk to about distributed systems. I’d done a bunch of work in the early days of the horizontal-scaling movement (vs just buying bigger servers) and did an M.Sc focused on distributed systems performance. Whenever anyone would come and ask for help with a planned distributed system the first question I would always ask is: does this system actually need to be distributed?! In my 15 years of consulting I think the answer was only actually “yes” 2 or 3 times. Much more often than was helping them solve the performance problems in their single server system; without doing that they would usually just have ended up with a slow complex distributed system. Edit: lol this paper was not popular in the Distributed Systems Group at my school: https://www.usenix.org/system/files/conference/hotos15/hotos... “You can have a second computer once you’ve shown you know how to use the first one.” reply OnlyMortal 1 hour agoparentprevInteresting. We went through a similar process and ended up with Yugabyte to deal with the locks (cluster). It’s based on Postgres but performance was not good enough. We’re now moving to RDMA. reply wwarner 4 hours agoparentprevThis is the best way, and actually the only sensible way to approach the problem. I first read about it here https://code.flickr.net/2010/02/08/ticket-servers-distribute... reply hansvm 3 hours agorootparent> only sensible way That's a bit strong. Like most of engineering, it depends. Postgres is a good solution if you only have maybe 100k QPS, the locks are logically (if not necessarily fully physically) partially independent, and they aren't held for long. Break any of those constraints, or add anything weird (inefficient postgres clients, high DB load, ...), and you start having to explore either removing those seeming constraints or using other solutions. reply wwarner 3 hours agorootparentOk fair; I'm not really talking about postgres (the link i shared uses mysql). I'm saying that creating a ticket server that just issues and persists unique tokens, is a way to provide coordination between loosely coupled applications. reply zbobet2012 3 hours agorootparentYeah that's cookies. They are great. reply etcd 1 hour agoparentprevI guess this is embarassingly parralelizable in that you can shard by concert to different instances. Might even be a job for that newfangled cloudflare sqlite thing. reply apwell23 2 hours agoparentprevClassic tech interview question reply galeaspablo 7 hours agoprevMany engineers don’t truly care about the correctness issue, until it’s too late. Similar to security. Or they care but don’t bother checking whether what they’re doing is correct. For example, in my field, where microservices/actors/processes pass messages between each other over a network, I dare say >95% of implementations I see have edge cases where messages might be lost or processed out of order. But there isn’t an alignment of incentives that fixes this problem. Ie the payment structures for executives and engineers aren’t aligned with the best outcome for customers and shareholders. reply noprocrasted 4 hours agoparent> there isn’t an alignment of incentives that fixes this problem \"Microservices\" itself is often a symptom of this problem. Everyone and their dog wants to introduce a network boundary in between function calls for no good reason just so they can subsequently have endless busywork writing HTTP (or gRPC if you're lucky) servers, clients & JSON (de?)serializers for said function calls and try to reimplement things like distributed transactions across said network boundary and dealing with the inevitable \"spooky action at a distance\" that this will yield. reply sethammons 2 hours agorootparentI've worked with microservices at scale and it was fantastic. We couldn't break backwards compatibility with our API without a lot of coordination. Outside of that, you could deploy as frequently as needed and other services could update as needed to make use of new features. The monoliths I have worked in, very contrastingly, have had issues coordinating changes within the codebases, code crosses boundaries it should not and datastores get shared and coupled to (what should be) different domains leading to slow, inefficient code and ossified options for product changes. reply shepherdjerred 44 minutes agorootparentprevIf you're hand-writing clients/servers/serializers instead of generating them from schema definitions then you have more fundamental issues than using microservices. reply sethammons 6 hours agoparentprevThe path to fixing this requires first measuring and monitoring it, then establishing service level objectives that represent customer experience. Product and engineering teams have to agree on them. If the SLOs become violated, focus shifts towards system stability. Getting everyone onboard is hard and that is why good leadership is needed. When customers start to churn because bugs pop up and new features are slow or non existent, then the case is very easy to make quality part of the process. Mature leaders get ahead of that as early as possible. reply galeaspablo 5 hours agorootparentGood leadership is spot on! Agreed. The cynic part of me sees incentives that discourage mature leadership styles. Leaders tend to be impatient and think of this quarter’s OKRs as opposed to the business’ long term financial health. In other word the leaders of leaders use standard MBA prescribed incentive structures. reply mrkeen 6 hours agoparentprevI think there's a bit of an alignment of incentives: the edge cases are tricky enough that your programmers probably need to handle a lot of support tickets, which isn't good for anyone. But I don't see anyway to convince yesterday's managers to give us time to build it right. reply secondcoming 5 hours agoparentprev> 95% of implementations I see have edge cases where messages might be lost or processed out of order. Eek. This sort of thing can end up with innocent people in jail, or dead. [0] https://en.wikipedia.org/wiki/British_Post_Office_scandal reply noprocrasted 4 hours agorootparentThe problem (or the solution, depending on which side you're on) is that innocent people are in jail or dead. The people that knowingly allowed this to happen are still free and wealthy. So I'm not particularly sure this is a good example - if anything, it sets the opposite incentives, that even jailing people or driving them to suicide won't actually have any consequences for you. reply jojolatulipe 1 hour agoprevAt work we use Temporal and ended up using a dedicated workflow and signals to do distributed locking. Working well so far and the implementation is rather simple, relying on Temporal’s facilities to do the distributed parts of the lock. reply anonzzzies 2 hours agoprevI am updating my low level and algo knowledge; what are good books about this (I have the one written by the author). I am looking to build something for fun, but everything is either a toy or very complicated. reply cosmicradiance 2 hours agoparentSystem Design Interview I and II - Alex Xu. Take one of the topics and do it practically. reply dataflow 1 hour agoprev> The lock has a timeout (i.e. it is a lease), which is always a good idea (otherwise a crashed client could end up holding a lock forever and never releasing it). However, if the GC pause lasts longer than the lease expiry period, and the client doesn’t realise that it has expired, it may go ahead and make some unsafe change. Hold on, this sounds absurd to me: First, if your client crashes, then you don't need a timed lease on the lock to detect this in the first place. The lock would get released by the OS or supervisor, whether there are any timeouts or not. If both of those crash too, then the connection would eventually break, and the network system should then detect that (via network resets or timeouts, lack of heartbeats, etc.) and then invalidate all your connections before releasing any locks. Second, if the problem becomes that your client is buggy and thus holds the lock too long without crashing, then shouldn't some kind of supervisor detect that and then kill the client (e.g., by the OS terminating the process) before releasing the lock for everybody else? Third, if you are going to have locks with timeouts to deal with corner cases you can't handle like the above, shouldn't they notify the actual program somehow (e.g., by throwing an exception, raising a signal, terminating it, etc.) instead of letting it happily continue execution? And shouldn't those cases wait for some kind of verification that the program was notified before releasing the lock? The whole notion that timeouts should somehow permit the program execution to continue ordinary control flow sounds like the root cause of the problem, and nobody is even batting an eye at it? Is there an obvious reason why this makes sense? I feel I must be missing something here... what am I missing? reply winwang 1 hour agoparentThis isn't a mutex, but the distributed equivalent of one. The storage service is the one who invalidates the lock on their side. The client won't detect its own issues without additional guarantees not given (supposedly) by Redlock. reply dataflow 1 hour agorootparentI understand that. What I'm hung up on is, why does the storage system feel it is at liberty to just invalidate a lock and thus let someone else reacquire it without any sort of acknowledgment (either from the owner or from the communication systems connecting the owner to the outside world) that the owner will no longer rely on it? It just seems fundamentally wrong. The lock service just... doesn't have that liberty, as I see it. reply winwang 58 minutes agorootparentWhat if the rack goes down? But I think the author is saying a similar thing to you. The fenced token is essentially asserting that the client will no longer rely on the lock, even if it tries to. The difference is the service doesn't need any acknowledgement, no permission needed to simlly deny the client later. reply dataflow 39 minutes agorootparentTo be clear, my objection is to the premise, not to the offered solution. To your question, could you clarify what exactly you mean by the rack \"going down\"? This encompasses a lot of different scenarios, I'm not sure which one you're asking about. The obvious interpretation would break all the connections the program has to the outside world, thus preventing the problem by construction. reply neonbrain 1 hour agorootparentprevMy understanding is that the dataflow user was talking about a notification which the server is supposed to receive from the OS in the case of a broken client connection. This notification is usually received, but cannot be guaranteed in a distributed environment. reply neonbrain 1 hour agoparentprevThe assumption that your server will always receive RST or FIN from your client is incorrect. There are some cases when these packets are being dropped, and your server will stay with an open connection while the client on the remote machine is already dead. P.S. BTW, it's not me who downvoted you reply dataflow 1 hour agorootparentI made no such assumption this will always happen though? That's why the comment was so much longer than just \"isn't TCP RST enough?\"... I listed a ton of ways to deal with this that didn't involve letting the program continue happily on its path. reply eknkc 4 hours agoprevI tend to use postgresql for distributed locking. As in, even if the job is not db related, I start a transaction and obtain an advisory lock which stays locked until the transaction is released. Either by the app itself or due to a crash or something. Felt pretty safe about it so far but I just realised I never check if the db connection is still ok. If this is a db related job and I need to touch the db, fine. Some query will fail on the connection and my job will fail anyway. Otherwise I might have already lost the lock and not aware of it. Without fencing tokens, atomic ops and such, I guess one needs a two stage commit on everything for absolute correctness? reply candiddevmike 4 hours agoparentOne gotcha maybe with locks is they are connection specific AFAIK, and in most libraries you're using a pool typically. So you need to have a specific connection for locks, and ensure you're using that connection when doing periodic lock tests. reply egcodes 3 hours agoprevOnce I wrote a dist. lock blog using this resource. Here it is: https://medium.com/sahibinden-technology/an-easy-integration... reply hoppp 3 hours agoprevI did distributed locking with Deno, and Deno KV hosted by Deno Deploy. Its using foundationdb, a distributed db. The deno instances running on local devices all connect to the same Deno KV to acquire the lock. But using postgres, a select for update also works, the database is not distributed tho. reply jmull 6 hours agoprevThis overcomplicates things... * If you have something like what the article calls a fencing token, you don't need any locks. * The token doesn't need to be monotonically increasing, just a passive unique value that both the client and storage have. Let's call it a version token. It could be monotonically increasing, but a generated UUID, which is typically easier, would work too. (Technically, it could even be a hash of all the data in the store, though that's probably not practical.) The logic becomes: (1) client retrieves the current version token from storage, along with any data it may want to modify. There's no external lock, though the storage needs to retrieve the data and version token atomically, ensuring the token is specifically for the version of the data retrieved. (2) client sends the version token back along with any changes. (3) Storage accepts the changes if the current token matches the one passed with the changes and creates a new version token (atomically, but still no external locks). Now, you can introduce locks for other reasons (hopefully goods ones... they seem to be misused a lot). Just pointing out they are/should be independent of storage integrity in a distributed system. (I don't even like the term lock, because they are temporary/unguaranteed. Lease or reservation might be a term that better conveys the meaning.) reply cnlwsu 4 hours agoparentYou’re describing compare and swap which is a good solution. You’re pushing complexity down to the database, and remember this is distributed locking. When you have a single database it’s simple until the database crashes leaving you in state of not knowing which of your CAS writes took effect. In major systems that demand high availability and multi datacenter backups this becomings pretty complicated with scenarios that break this as well around node failure. Usually some form of paxos transaction log is used. Never assume there is an easy solution in distributed systems… it just always sucks reply zeroxfe 4 hours agoparentprev> This overcomplicates things... You're misinterpreting the problem described, and proposing a solution for a different problem. reply karmakaze 5 hours agoparentprevThis is known as 'optimistic locking'. But I wouldn't call it a distributed locking mechanism. reply jameshart 4 hours agorootparentOptimistic locks are absolutely a distributed locking mechanism, in that they are for coordinating activity among distributed nodes - but they do require the storage node to have strong guarantees about serialization and atomicity of writes. That means it isn’t a distributed storage solution, but it is something you can build over the top of a distributed storage solution that has strong read after write guarantees. reply zeroxfe 3 hours agorootparentThis is unconventional use of the term \"distributed locking\". This alternative just punts the hard part of locking to the storage system. reply karmakaze 4 hours agorootparentprevI normally see it as a version column in a database where it being with the data makes it non-distributed. I'm not even sure how it could be used for exclusive update to a resource elsewhere--all clients will think they 'have' the lock and change the resource, then find out they didn't when they update the lock. Or if they bump the lock first, another client could immediately 'have' the lock too. reply wh0knows 5 hours agoparentprevThis neglects the first reason listed in the article for why you would use a lock. > Efficiency: Taking a lock saves you from unnecessarily doing the same work twice (e.g. some expensive computation). If the lock fails and two nodes end up doing the same piece of work, the result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice). I think multiple nodes doing the same work is actually much worse than what’s listed, as it would inhibit you from having any kind of scalable distributed processing. reply karmakaze 5 hours agorootparentAs mentioned in the article, a non-100%-correct lock can be used for efficiency purposes. So basically use an imperfect locking mechanism for efficiency and a reliable one for correctness. reply jmull 5 hours agorootparent> and a reliable one for correctness To be clear, my point is don't use distributed locking for correctness. There are much better options. Now, the atomicity I mention implies some kind of internal synchronization mechanism for multiple requests, which could be based on locks, but those would be real, non-distributed ones. reply jmull 5 hours agorootparentprevSure, that's why I said you might introduce \"locks\" (reservations is a much better term) for other reasons. Efficiency is one, as you say. The other main one that comes to mind is to implement other \"business rules\" (hate that term, but that's what people use), like for a online shopping app, the stock to fulfill an order might be reserved for a time when the user starts the checkout process. reply bootsmann 5 hours agoparentprevWon't this lead to inconsistent states if you don't do monotonically increasing tokens? I.e. your storage system has two nodes and there are two read-modify-write processes running. Process 1 acquires the first token \"abc\" and process two also acquires the token \"abc\". Now process 1 commits, the token is changed to \"cde\" and the change streamed to node 2. Due to network delay, the change to node 2 is delayed. Meanwhile process 2 commits to node 2 with token \"abc\". Node 2 accepts the change because it has not received the message from node 1 and your system is now in an inconsistent state. Note that this cannot happen in a scenario where we have monotonically increasing fencing tokens because that requirement forces the nodes to agree on a total order of operations before they can supply the fencing token. reply computerfan494 4 hours agorootparentIn the above description of optimistic locking, it is assumed that it is impossible to issue the same token to multiple clients. Nodes can agree that a given token has also never been issued before just like a monotonically increasing value. The nice property about non-monitonically-increasing tokens is that nodes may generate them without coordinating if you can make other assumptions about that system. A good example is when nodes use an ID they were assigned beforehand as part of the token generation, guaranteeing that the leasing tokens they mint will not conflict with other nodes' as long as node IDs are not reused. reply bootsmann 2 hours agorootparentI have a hard time wrapping my head around what you are proposing here. Say client A requests data, they get the token a-abc. Then client B requests data, they get the token b-cde. Client A commits their write, does the storage reject it because they already issued another token (the one from client B) or does it accept it? reply computerfan494 1 hour agorootparentMy understanding of what the OP was discussing is an optimistic locking system where the nodes only accept commits if the last issued token matches the token included in the commit. While agreeing on the last token requested requires coordination, unlike monotonically increasing tokens you could have well-behaved clients generate token content themselves without coordination. That may or may not be useful as a property. reply bootsmann 1 hour agorootparentGot it, thank you for clarifying this. reply jmull 4 hours agorootparentprev\"node1\", \"node2\", and \"storage\" are three separate things in the distributed environment. Only storage accepts changes, and it's what verifies the incoming token matches the current token. So node2 doesn't get to accept changes. It can only send changes to storage, which may or may not be accepted by it. reply bootsmann 2 hours agorootparentIf the storage is a singular entity then this is not a distributed systems problem at all, no? reply eru 4 hours agoparentprevGit push's `--force-with-lease` option does essentially this. (Honestly, they should rename `--force-with-lease` to just `--force`, and rename the old `--force` behaviour to `--force-with-extreme-prejudice` or something like that. Basically make the new behaviour the default `--force` behaviour.) reply antirez 5 hours agoprev [–] I suggest reading the comment I left back then in this blog post comments section, and the reply I wrote in my blog. Btw, things to note in random order: 1. Check my comment under this blog post. The author had missed a fundamental point in how the algorithm works. Then he based the refusal of the algorithm on the remaining weaker points. 2. It is not true that you can't wait an approximately correct amount of time, with modern computers an APIs. GC pauses are bound and monotonic clocks work. These are acceptable assumptions. 3. To critique the auto release mechanism in-se, because you don't want to expose yourself to the fact that there is a potential race, is one thing. To critique the algorithm in front of its goals and its system model is another thing. 4. Over the years Redlock was used in a huge amount of use cases with success, because if you pick a timeout which is much larger than: A) the time to complete the task. B) the random pauses you can have in normal operating systems. Race conditions are very hard to trigger, and the other failures in the article were, AFAIK, never been observed. Of course if you have a super small timeout to auto release the lock, and the task may easily take this amount of time, you just committed a deisgn error, but that's not about Redlock. reply computerfan494 5 hours agoparentTo be honest I've long been puzzled by your response blog post. Maybe the following question can help achieve common ground: Would you use RedLock in a situation where the timeout is fairly short (1-2 seconds maybe), the work done usually takes ~90% of that timeout, and the work you do while holding a RedLock lock MUST NOT be done concurrently with another lock holder? I think the correct answer here is always \"No\" because the risk of the lease sometimes expiring before the client has finished its work is very high. You must alter your work to be idempotent because RedLock cannot guarantee mutual exclusion under all circumstances. Optimistic locking is a good way to implement this type of thing while the work done is idempotent. reply kgeist 4 hours agorootparent>because the risk of the lease sometimes expiring before the client has finished its work is very high We had corrupted data bacause of this. reply antirez 4 hours agorootparentprevThe timeout must be much larger than the time required to do the work. The point is that distributed locks without a release mechanism are in practical terms very problematic. Btw, things to note in random order: 1. Check my comment under this blog post. The author had missed a fundamental point in how the algorithm works. Then he based the refusal of the algorithm on the remaining weaker points. 2. It is not true that you can't wait an approximately correct amount of time, with modern computers an APIs. GC pauses are bound and monotonic clocks work. These are acceptable assumptions. 3. To critique the auto release mechanism in-se, because you don't want to expose yourself to the fact that there is a potential race, is one thing. To critique the algorithm in front of its goals and its system model is another thing. 4. Over the years Redlock was used in a huge amount of use cases with success, because if you pick a timeout which is much larger than: A) the time to complete the task. B) the random pauses you can have in normal operating systems. Race conditions are very hard to trigger, and the other failures in the article were, AFAIK, never been observed. Of course if you have a super small timeout to auto release the lock, and the task may easily take this amount of time, you just committed a deisgn error, but that's not about Redlock. reply computerfan494 4 hours agorootparentLocking without a timeout is indeed in the majority of use-cases a non-starter, we are agreed there. The critical point that users must understand is that it is impossible to guarantee that the RedLock client never holds its lease longer than the timeout. Compounding this problem is that the longer you make your timeout to minimize the likelihood of this from accidentally happening, the less responsive your system becomes during genuine client misbehaviour. reply antirez 3 hours agorootparentIn most real world scenarios, the tradeoffs are a bit softer than what people in the formal world dictates (and doing so they forced certain systems to become suboptimal for everything but during failures, kicking them out of business...). Few examples: 1. E-commerce system where there are a limited amount of items of the same kind, you don't want to oversell. 2. Hotel booking system where we don't want to reserve the same dates/rooms multiple times. 3. Online medical appointments system. In all those systems, to re-open the item/date/... after some time it's ok, even after one day. And if the lock hold time is not too big, but a very strict compromise (it's also a reasonable choice in the spectrum), and it could happen that during edge case failures three items are sold and there are two, orders can be cancelled. So yes, there is a tension between timeout, race condition, recovery time, but in many systems using something like RedLock the development and end-user experience can be both improved with a high rate of success, and the random unhappy event can be handled. Now the algorithm is very old, still used by many implementations, and as we are talking problems are solved in a straightforward way with very good performances. Of course, the developers of the solution should be aware that there are tradeoffs between certain values: but when are distributed systems easy? P.S. why 10 years of strong usage count, in the face of a blog post telling that you can't trust a system like that? Because even if DS issues emerge randomly and sporadically, in the long run systems that create real-world issues, if they reach mass usage, are known. A big enough user base is a continuous integration test big enough to detect when a solution has real world serious issues. So of course RedLock users picking short timeouts with tasks that take a very hard to predict amount of time, will indeed incur into knonw issues. But the other systemic failure modes described in the blog post are never mentioned by users AFAIK. reply computerfan494 2 hours agorootparentI feel like you're dancing around admitting the core issue that Martin points out - RedLock is not suitable for systems where correctness is paramount. It can get close, but it is not robust in all cases. If you want to say \"RedLock is correct a very high percentage of the time when lease timeouts are tuned for the workload\", I would agree with you actually. I even possibly agree with the statements \"most systems can tolerate unlikely correctness failures due to RedLock lease violations. Manual intervention is fine in those cases. RedLock may allow fast iteration times and is worth this cost\". I just think it's important to be crystal clear on the guarantees RedLock provides. I first read Martin's blog post and your response years ago when I worked at a company that was using RedLock despite it not being an appropriate tool. We had an outage caused by overlapping leases because the original implementor of the system didn't understand what Martin has pointed out from the RedLock documentation alone. I've been a happy Redis user and fan of your work outside of this poor experience with RedLock, by the way. I greatly appreciate the hard work that has gone into making it a fantastic database. reply bluepizza 5 hours agoparentprev [–] Could you provide links? reply saikatsg 4 hours agorootparent [–] http://antirez.com/news/101 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Martin Kleppmann criticizes the Redlock algorithm for distributed locking on Redis, highlighting its unsuitability for scenarios demanding correctness due to its dependence on timing assumptions and absence of fencing tokens.",
      "He advises using a single Redis instance for efficiency locks and a consensus system like ZooKeeper for locks requiring correctness, as Redlock's timing assumptions can lead to vulnerabilities such as network delays and process pauses.",
      "Kleppmann emphasizes the importance of understanding Redis's limitations and selecting the right tools for specific locking requirements."
    ],
    "commentSummary": [
      "In 2018, a team opted for Postgres over Redis for distributed locking in ticket allocation, valuing its reliability and simplicity.",
      "The team utilized a composite UPDATE statement for distributed locks, which improved accuracy and performance.",
      "The discussion underscores that many distributed systems can rely on traditional database transactions, and not all issues necessitate complex distributed solutions."
    ],
    "points": 127,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1729420697
  },
  {
    "id": 41890158,
    "title": "The Languages of English, Math, and Programming",
    "originLink": "https://github.com/norvig/pytudes/blob/main/ipynb/Triplets.ipynb",
    "originBody": "norvig / pytudes Public Notifications Fork 2.4k Star 22.8k Code Issues 21 Pull requests 7 Actions Projects Security Insights Files main Triplets.ipynb Breadcrumbs pytudes/ipynb /Triplets.ipynb Latest commit History History 584 lines (584 loc) · 19.3 KB File metadata and controls Preview Code Blame 584 lines (584 loc) · 19.3 KB Raw",
    "commentLink": "https://news.ycombinator.com/item?id=41890158",
    "commentBody": "The Languages of English, Math, and Programming (github.com/norvig)123 points by stereoabuse 23 hours agohidepastfavorite39 comments PaulDavisThe1st 7 minutes agoWithin hours to weeks to months, many LLMs will be better at solving the problem as originally given in TFA ... because of the existence of TFA. Not immediately clear what this means or if it is good or bad or something else. reply underdeserver 5 hours agoprevI'm just here to point out that since Python 3.10, you don't need to import anything from typing anymore, you could use the built-in types for annotation: from math import prod from itertools import combinations def find_products(k=3, N=108) -> set[tuple[int, ...]]: \"\"\"A list of all ways in which `k` distinct positive integers have a product of `N`.\"\"\" factors = {i for i in range(1, N + 1) if N % i == 0} return {ints for ints in combinations(factors, k) if prod(ints) == N} find_products() reply superlopuh 37 minutes agoparentThe typing version is still useful when you want to communicate that the result conforms to a certain interface, which doesn't include mutability in the case of Set, but not the exact type. Edit: I see that he imported the typing Set, which is deprecated, instead of collections.abc.Set, which is still useful, so your comment is correct. reply nick0garvey 15 hours agoprevI took a Udacity class by Norvig [1] and my abilities as a programmer clearly were improved afterward. His code here demonstrates why. It is both shorter and much easier to understand than anything the LLMs generated. It is not always as efficient as the LLMs (who often skip the third loop by calculating the last factor), but it is definitely the code I would prefer to work with in most situations. [1] https://www.udacity.com/course/design-of-computer-programs--... reply fifilura 19 minutes agoparentWorking extensively in SQL for a while also gives you another perspective of programming. There is just no way you can write this with a for loop in SQL since it does not (generally) have for loops. WITH all_numbers AS ( SELECT generate_series as n FROM generate_series(1, 108) as n ), divisors AS ( SELECT * FROM all_numbers WHERE 108 % n = 0 ), permutations as ( SELECT a.n as n1, b.n as n2, c.n as n3 FROM divisors as a CROSS JOIN divisors as b CROSS JOIN divisors as c ) SELECT * FROM permutations WHERE n1 * n2 * n3 = 108 AND n1It is both shorter and much easier to understand than anything the LLMs generated. I think this makes sense. LLMs are trained on average code on average. This means they produce average code, on average. reply earslap 16 hours agoprevIt is more obvious when taken to extreme: With the current feedforward transformer architectures, there is a fixed amount of compute per token. Imagine asking a very hard question with a yes/no answer to an LLM. There are infinite number of cases where the compute available to the calculation of the next token is not enough to definitively solve that problem, even given \"perfect\" training. You can increase the compute for allowing more tokens for it to use as a \"scratch pad\" so the total compute available will be num_tokens * ops_per_token but there still are infinite amount of problems you can ask that will not be computable within that constraint. But, you can offload computation by asking for the description of the computation, instead of asking for the LLM to compute it. I'm no mathematician but I would not be surprised to learn that the above limit applies here as well in some sense (maybe there are solutions to problems that can't be represented in a reasonable number of symbols given our constraints - Kolmogorov Complexity and all that), but still for most practical (and beyond) purposes this is a huge improvement and should be enough for most things we care about. Just letting the system describe the computation steps to solve a problem and executing that computation separately offline (then feeding it back if necessary) is a necessary component if we want to do more useful things. reply bytebach 2 hours agoprevRelated HN discussion - https://news.ycombinator.com/item?id=41831735 using Prolog as an intermediate target language for LLM output improves their 'reasoning' abilities. reply tmsh 4 hours agoprevWhat about o1? I think the world is sleeping on o1. Recently I misread a leetcode/neetcode problem (so I was curious that my version of the problem with an extra constraint could be solved in a different way). And 4o hallucinated incorrectly and double downed when asked follow up questions - but o1 solved it the first time what seemed like easily. It really is a major step forward. reply fifilura 45 minutes agoparentSo... Can you enlighten us how it went? reply segmondy 15 hours agoprevTried these with some local models and these are the ones that generated the program one shot, a few of them also generated the results correctly one shot. llama3.1-70b, llama3.1-405b, deepseekcoder2.5, gemma-27b, mistral-large, qwen2.5-72b. https://gist.github.com/segmond/8992a8ec5976ff6533d797caafe1... I like how the solution sort of varies across most, tho mistral and qwen look really similar. reply assadk 5 hours agoparentWhat specs does your machine have to run these models locally? reply __0x01 3 hours agoprevI thought this was going to be an essay on the impact of English, Math and Programming on humanity. I would place English (or all spoken/written languages in general) first, Math (as discovered) second, and programming languages last. reply ryandv 14 hours agoprevIt's worth noting that math and programming do not appear to be considered \"languages\" by much of the academic and/or neuroscientific literature; see [0] on the front page right now and my comments regarding the same [1]. [0] https://news.ycombinator.com/item?id=41868884 [1] https://news.ycombinator.com/item?id=41892701 reply riku_iki 55 minutes agoparent> It's worth noting that math and programming do not appear to be considered \"languages\" by much of the academic There is math term \"formal language\", and both math and programming perfectly fit into it reply nurettin 5 hours agoparentprevYes, calling an algorithm, a formal description of a generative function \"a language\" is clearly wrong. I don't think we need an academic dissertation on this. reply ryandv 4 hours agorootparentI take it you disagree with the conclusions from TFA then? > Sometimes a natural language such as English is a good choice, sometimes you need the language of mathematical equations, or chemical equations, or musical notation, and sometimes a programming language is best. Written language is an amazing invention that has enabled human culture to build over the centuries (and also enabled LLMs to work). But human ingenuity has divised [sic] other notations that are more specialized but very effective in limited domains. reply bbor 4 hours agoparentprevHuh, interesting. Programming languages were devised with Chomsky’s foundational theory of formal languages in mind, and they’re one of the few actual implementations of it. I read your comment and it seems your main thrust is that arithmetic activity lights up different brain regions than communicative activity, which I don’t personally see as a compelling basis for a definition of the word “language”. Of course, this is what Chomsky calls a “terminological dispute”, so I mean no offense and you’re ofc free to stand your ground that the only language is what appears in human brains! But if mathematical notation and programming languages aren’t languages, what are they…? Protocols? Recursively generative patterns? Maybe just grammars? The best move in any terminological dispute is “my terms are more useful”, so this seems like a good reason to keep language as it’s defined by the generative linguistics. Or, more broadly: Saussure approaches the essence of language from two sides. For the one, he borrows ideas from Steinthal and Durkheim, concluding that language is a 'social fact'. For the other, he creates a theory of language as a system in and for itself which arises from the association of concepts and words or expressions. Thus, language is a dual system of interactive sub-systems: a conceptual system and a system of linguistic forms. Neither of these can exist without the other because, in Saussure's notion, there are no (proper) expressions without meaning, but also no (organised) meaning without words or expressions. https://en.wikipedia.org/wiki/Theory_of_language reply ryandv 4 hours agorootparentI don't actually take any strong positions on the matter so long as the sense in which a term is used is clearly defined and agreed upon from the outset. This is merely an observation regarding what the literature considers \"language\" and this narrow definition's basis in brain structure contrasted with other forms of mental activity. But if I must, I suppose I am indeed assuming a stance by taking potshots at this narrower (albeit more precise) use of the word language by (obliquely) pointing to counterexamples that could be considered languages in their own right; the sweeping claim that \"language is not essential for thought\" seems far broader than the narrow sense in which the term is construed in the actual paper. reply YeGoblynQueenne 5 hours agoprev>> Only 2 of the 9 LLMs solved the \"list all ways\" prompt, but 7 out of 9 solved the \"write a program\" prompt. The language that a problem-solver uses matters! Sometimes a natural language such as English is a good choice, sometimes you need the language of mathematical equations, or chemical equations, or musical notation, and sometimes a programming language is best. Written language is an amazing invention that has enabled human culture to build over the centuries (and also enabled LLMs to work). But human ingenuity has divised other notations that are more specialized but very effective in limited domains. If I understand correctly, Peter Norvig's argument is about the relative expressivity and precision of Python and natural language with respect to a particular kind of problem. He's saying that Python is a more appropriate language to express factorisation problems, and their solutions, than natural language. Respectfully -very respectfully- I disagree. The much simpler explanation is that there are many more examples, in the training set of most LLMs, of factorisation problems and their solutions in Python (and other programming languages), than in natural language. Examples in Python etc. are also likely to share more common structure, even down to function and variable names [1], so there are more statistical regularities for a language model to overfit-to, during training. We know LLMs do this. We even know how they do it, to an extent. We've known since the time of BERT. For example: Probing Neural Network Comprehension of Natural Language Arguments https://aclanthology.org/P19-1459/ Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference https://aclanthology.org/P19-1334/ Given these and other prior results Peter Norvig's single experiment is not enough, and not strong enough, evidence to support his alternative hypothesis. Ideally, we would be able to test an LLM by asking it to solve a factorisation problem in a language in which we can ensure there are very few examples of a solution, but that is unfortunately very hard to do. ______________ [1] Notice for instance how Llama 3.1 immediately identifies the problem as \"find_factors\", even though there's no such instruction in the two prompts. That's because it's seen that kind of code in the context of that kind of question during training. The other LLMs seem to take terms from the prompts instead. reply akira2501 55 minutes agoprevWhen an author writes things like: \"But some of them forgot that 1 could be a factor of 108\" I struggle to take them seriously. The anthropomorphization of AI into something that can \"know\" or \"forget\" is ridiculous and shows a serious lack of caution and thinking when working with them. Likewise it leads people into wasting time on \"prompt engineering\" exercises that produce overfit and worthless solutions to trivial problems because it makes them feel like they're revealing \"hidden knowledge.\" Genuinely disappointing use of human time and of commercial electricity. reply owenpalmer 17 hours agoprevA formal notation for reasoning could possibly solve some reasoning issues for LLMs. Perhaps something like Lojban or symbolic logic. We don't have a lot of data for it, but it might be possible to synthetically generate it. On a dark note, I wonder if increasing AI reasoning capability could have dangerous results. Currently, LLMs are relatively empathetic, and seem to factor the complex human experience into it's responses. Would making LLMs more logical, cold, and calculating result in them stepping on things which humans care about? reply seanhunter 13 hours agoparent> A formal notation for reasoning could possibly solve some reasoning issues for LLMs. Perhaps something like Lojban or symbolic logic. We don't have a lot of data for it, but it might be possible to synthetically generate it. There's definitely anecdata supporting this. For some time chatgpt was better on a lot of arithmetic/logic type tasks if you asked it to generate a python program to do x than if you just asked it to do x for example. I haven't tested this specifically on the latest generation but my feeling is it has caught up a lot. reply Jianghong94 15 hours agoparentprevDue to the extreme data quantity requirement for pre-training, LLM effectively locks your reasoning language into the lowest common denominator, aka Python. Sure, people (maybe very smart) can come up with reasonable, effective, efficient notation; the problem is to train the model to use it properly. reply okibry 9 hours agoprevI have question, we still do not know what behind \"neuron network\" of machine. What if we gave them some syntax of a language (or syntax of a area) then ask them extract a general rule for that language ? Can them can do that ? reply albert_e 15 hours agoprevGut feel: doing this in two steps (1. write an algorithm for and 2. write code for) or even chain-of-thought prompting might yield better results. reply downboots 16 hours agoprev> The language that a problem-solver uses matters! Because the \"intelligence\" is borrowed from language (lower entropy) reply ainiriand 6 hours agoparentWhat are your thoughts on https://news.ycombinator.com/item?id=41868884 reply downboots 1 hour agorootparentAn analogy https://youtu.be/5H8aRCyEGnU reply itronitron 9 hours agoprevso, in conclusion, the training data containing 'math' that LLMs have access to is predominantly written as software code, and not as mathematical notation reply jll29 8 hours agoparentIt would be quite exciting to train a LLM with (OCR scans of) all mathematical journals, pre-prints, time-series etc. reply dandanua 3 hours agoprev [–] It's not the language it's the context. LLMs could give very different outputs depending on the supplied context. In this case, words \"python\" and \"program\" put it in a far better context to solve the problem. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Norvig/pytudes repository is a public project on GitHub with significant community engagement, evidenced by its 2.4k forks and 22.8k stars.",
      "It includes various resources such as code, issues, pull requests, and security insights, indicating active development and collaboration.",
      "A notable file within the repository, \"Triplets.ipynb,\" consists of 584 lines and is 19.3 KB in size, suggesting a substantial amount of code or data."
    ],
    "commentSummary": [
      "The discussion highlights the effectiveness of English, Math, and Programming languages in problem-solving with Large Language Models (LLMs), emphasizing Python's structured nature for expressing problems.- It is suggested that LLMs may perform better with programming languages due to extensive training on examples in these languages, though they face limitations in reasoning and computation.- The debate includes whether math and programming should be classified as languages, noting their formal language characteristics and the impact of language choice on LLMs' problem-solving efficiency."
    ],
    "points": 123,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1729366759
  }
]
