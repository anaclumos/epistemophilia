[
  {
    "id": 42669754,
    "title": "Stop Trying to Schedule a Call with Me",
    "originLink": "https://matduggan.com/stop-trying-to-schedule-a-call-with-me/",
    "originBody": "One of the biggest hurdles for me when trying out a new service or product is the inevitable harassment that follows. It always starts innocuously: “Hey, I saw you were checking out our service. Let me know if you have any questions!” Fine, whatever. You have documentation, so I’m not going to email you, but I understand that we’re all just doing our jobs. Then, it escalates. “Hi, I’m your customer success fun-gineer! Just checking in to make sure you’re having the best possible experience with your trial!” Chances are, I signed up to see if your tool can do one specific thing. If it doesn’t, I’ve already mentally moved on and forgotten about it. So, when you email me, I’m either actively evaluating whether to buy your product, or I have no idea why you’re reaching out. And now, I’m stuck on your mailing list forever. I get notifications about all your new releases and launches, which forces me to make a choice every time: • “Obviously, I don’t care about this anymore.” • “But what if they’ve finally added the feature I wanted?” Since your mailing list is apparently the only place on Earth to find out if Platform A has added Feature X (because putting release notes somewhere accessible is apparently too hard), I have to weigh unsubscribing every time I see one of your marketing emails. And that’s not even the worst-case scenario. The absolute worst case is when, god forbid, I can actually use your service, but now I’m roped into setting up a “series of calls.” You can't just let me input a credit card number into a web site. Now I need to form a bunch of interpersonal relationships with strangers over Microsoft Teams. Let's Jump On A Call Every SaaS sales team has this classic duo. First, there’s the salesperson. They’re friendly enough but only half paying attention. Their main focus is inputting data into the CRM. Whether they’re selling plastic wrap or missiles, their approach wouldn’t change much. Their job is to keep us moving steadily toward The Sale. Then, there’s their counterpart: the “sales engineer,” “customer success engineer,” or whatever bastardized title with the word engineer they’ve decided on this week. This person is one of the few people at the company who has actually read all the documentation. They’re brought in to explain—always with an air of exhaustion—how this is really my new “everything platform.” “Our platform does everything you could possibly want. We are very secure—maybe too secure. Our engineers are the best in the world. Every release is tested through a 300-point inspection process designed by our CTO, who interned at Google once, so we strongly imply they held a leadership position there.” I will then endure a series of demos showcasing functionality I’ll never use because I’m only here for one or two specific features. You know this, but the rigid demo template doesn’t allow for flexibility, so we have to slog through the whole thing. To placate me, the salesperson will inevitably say something like, “Mat is pretty technical—he probably already knows this.” As if this mild flattery will somehow make me believe that a lowly nerd like me and a superstar salesperson like you could ever be friends. Instead, my empathy will shift to the sales engineer, whose demo will, without fail, break at the worst possible time. Their look of pure despair will resonate with me deeply. “Uh, I promise this normally works.” There, there. I know. It’s all held together with tape and string. At some point, I’ll ask about compliance and security, prompting you to send over a pile of meaningless certifications. These documents don’t actually prove you did the things outlined in them; they just demonstrate that you could plausibly fake having done them. We both know this. If I got you drunk, you’d probably tell me horror stories about engineers fixing databases by copying them to their laptops, or how user roles don’t really work and everyone is secretly an admin. But this is still the dating phase of our relationship, so we’re pretending to be on our best behavior. “Very impressive SOC-2.” via GIPHY Getting Someone To Pay You We’ve gone through the demos. You’ve tried to bond with me, forming a “team” that will supposedly work together against the people who actually matter and make decisions at my company. Now you want to bring my boss’s boss into the call to pitch them directly. via GIPHY Here’s the problem: that person would rather be set on fire than sit through 12 of these pitches a week from various companies. So, naturally, it becomes my job to “put together the proposal.” This is where things start to fall apart. The salesperson grows increasingly irritated because they could close the deal if they didn’t have to talk to me and could just pitch directly to leadership. Meanwhile, the sales engineer—who, for some reason, is still forced to attend these calls—stares into the middle distance like an orphan in a war zone. “Look, can we just loop in the leadership on your side and wrap this up?” the salesperson asks, visibly annoyed. “They pay me so they don’t have to talk to you,” I’ll respond, a line you first thought was a joke but have since realized was an honest admission you refused to hear early in our relationship. If I really, really care about your product, I’ll contact the 300 people I need on my side to get it approved. This process will take at least a month. Why? Who knows—it just always does. If I work for a Fortune 500 company, it’ll take a minimum of three months, assuming everything goes perfectly. By this point, I hate myself for ever clicking that cursed link and discovering your product existed. What was supposed to save me time has now turned into a massive project. I start to wonder if I should’ve just reverse-engineered your tool myself. Eventually, it’s approved. Money is exchanged, and the salesperson disappears forever. Now, I’m handed off to Customer Service—aka a large language model (LLM). The Honeymoon Is Over It doesn’t take long to realize that your “limitless, cloud-based platform designed by the best in the business” is, in fact, quite limited. One day, everything works fine. The next, I unknowingly exceed some threshold, and the whole thing collapses in on itself. I’ll turn to your documentation, which has been meticulously curated to highlight your strengths—because god forbid potential customers see any warnings. Finding no answers, I’ll engage Customer Service. After wasting precious moments of my life with an LLM that links me to the same useless documentation, I’ll finally be allowed to email a real person. The SLA on that support email will be absurdly long—72 business hours—because I didn’t opt for the Super Enterprise Plan™. Eventually, I’ll get a response explaining that I’ve hit some invisible limit and need to restructure my workflows to avoid it. As I continue using your product, I’ll develop a growing list of undocumented failure modes: “If you click those two buttons too quickly, the iFrame throws an error.” I’ll actually say this to another human being, as if we’re in some cyberpunk dystopia where flying cars randomly explode in the background because they were built by idiots. Despite your stack presumably logging these errors, no one will ever reach out to explain them or help me fix anything. Account Reps Then, out of the blue, I’ll hear from my new account rep. They’ll want a call to “discuss how I’m using the product” and “see how they can help.” Don’t be fooled—this isn’t an attempt to gather feedback or fix what’s broken. It’s just another sales pitch. After listening to my litany of issues and promising to “look into them,” the real purpose of the call emerges: convincing me to buy more features. These “new features” are things that cost you almost nothing but make a huge difference to me—like SSO or API access. Now I’m forced to decide whether to double down on your product or rip it out entirely and move on with my life. Since it’s not my money, I’ll probably agree to give you more just to get basic functionality that should’ve been included in the first place. Fond Farewell Eventually, one of those open-source programmers—the kind who gleefully release free tools and then deal with endless complaints for life—will create something that does what your product does. It’ll have a ridiculous name like CodeSquish, Dojo, or GitCharm. I’ll hear about it from a peer. When I mention I use your product, they’ll turn to me, eyes wide, and say, “Why don’t you just use CodeSquish?” Not wanting to admit ignorance, I’ll make up a reason on the spot. Later, in the bathroom, I’ll Google CodeSquish and discover it does everything I need, costs nothing, and is 100x more performant—even though it’s maintained by a single recluse who only emerges from their Vermont farm to push code to their self-hosted git repo. We’ll try it out. Despite the fact that its only “forum” is a Discord server, it’ll still be miles ahead of your commercial product. Then comes the breakup. I’ll put it off for as long as possible because we probably signed a contract. Eventually, I’ll tell Finance not to renew it. Suddenly, I’ll get a flurry of attention from your team. You’ll pitch me on why the open-source tool is actually inferior (which we both know isn’t true). I’ll tell you, “We’ll discuss it on our side.” We won’t. The only people who cared about your product were me and six others. Finally, like the coward I am, I’ll break up with you over email—and then block your domain. Share Topic humor tech rant Review of Orbit by Mozilla A review and technical tear-down of the Orbit by Mozilla AI summary add-on for Firefox.… 06 Jan 2025",
    "commentLink": "https://news.ycombinator.com/item?id=42669754",
    "commentBody": "Stop Trying to Schedule a Call with Me (matduggan.com)494 points by birdculture 19 hours agohidepastfavorite169 comments Sytten 5 hours agoSomeone needs to write the other side of the story as a small startup owner having to deal with big businesses. It should feature: - Beeing asked for demos when they could have tried your free tier/trial and you have decent doc (They always prefer that you waste your time then asking someone on thier side to review) - Being used a price/feature comparison with your enterprise competitor just so they can get a better deal on their renewal (no stupid they never intended to change) - Having to fill endless security questionnaire without being sure you can even sell to them (what you dont love filling 100+ questions at 11PM on a Friday???). - Having to deal 3-6 months with procurement, often with one or two third party software resellers which only cares if he can make its margin (hello there CDW, no I wont lower my price 15% so you can make money) - Receiving the money 60-90 days after the deal close while you struggle with cashflow and you had to provide the service (and receiving a call from my bank rep to see why my margin and CC is full). I am sure I can think of more things. That is why I will absolutely charge an arm and leg to big businesses. reply turkeywelder 4 hours agoparentThis is why we have a flat \"we won't fill in your forms\" rule and only take card payments. We've lost some clients because of it sure, but we're a small team so it's fine. I think being up front about what you will and won't do really helps. They still ask for procurement forms and stuff but we can at least deny it and set an expectation. https://help.timetastic.co.uk/hc/en-us/articles/115003288769... reply neilv 3 hours agoparentprevAnd that's for selling a fixed off-the-shelf solution. If you have something that needs to be a bit bespoke, or is an early-startup collaboration with a pilot customer as you refine and prove PMF... (and the customer is a real company, not just some other startups in your investors' portfolios)... and it impacts more than one very small team there... even if you already have the ear of their CEO... brace yourself for months of countless meetings with \"stakeholders\" of numerous departments. Which, as a nimble early startup, impresses upon you why, in theory, you should have a massive innovation advantage over large, lumbering businesses, who can't seem to do anything. Except you made the mistake of needing the buy-in of a large, lumbering business who can't seem to do anything. Now most of their barriers to movement are also yours. :) reply SteveVeilStream 17 hours agoprevAgainst all of the advice in the world, I'm trying to set things up to avoid basically everything described in the article. It's tricky because the truth is that a lot of the tactics work and that is why companies implement them. A lot of investors also consider that process to be best-in-class and look for it as a sign of maturity. The counter approach is: - Make complete information available as transparently as possible and don't gate it. - Be forthcoming about weaknesses. Don't force prospects and customers to find them. - Ensure that when a prospect or customer does want to talk to someone, they immediately reach someone who can handle the problem or answer the question (no need for escalations.) - Never have an AI agent call someone unless the customer specifically requests that and be sure that all AI agents immediately disclose that they are AI. - Offer flexible e-mail list subscription options (monthly, quarterly, annually, only release notes, etc.) - If the product is not a fit, try to offer something useful anyway such as a suggestion of another product that might be a better match for their needs. Value based pricing is one of the reasons why a lot of companies end up in these situations. Rather than setting a standard price, the company does a detailed investigation of the customer to try to find out how much value they will gain from using the product and then they set the price based on that determination. Although it maximizes revenue in theory, it is slow and invasive. reply portaouflop 17 hours agoparentMore power to you but in my experience - people don’t read your documentation - if they find one edge-case that is not covered by your product they concentrate all their energy on it even though the competitor product doesn’t even work for the happy path (but ofc they won’t learn that until too late) - you will have a person on the free tier who is not able to the simplest programming take up all the time of your technical support if you let them The rest (disclose usage of AI, offer multiple newsletters (imo unnecessary work), be upfront if it’s not a good fit) is very sensible and works in my experience. The first three will break your product - but maybe I’m just cynical and was holding it wrong all this time edit: I’m assuming you want to build some kind of investor fuelled unicorn - your approach probably will work for a single person slow but steady growth while you have other side income reply efitz 16 hours agorootparentYou need docs. Most users won’t read them. Do a good job anyway because they can save you a TON of support costs if they’re good, even for the few people who do read them. The cost of writing docs is easily offset by even a modest reduction in human support cost, unless you just half-ass your support with LLMs and FAQs and chat with overseas wage slaves. ALSO: your UX needs to warn people about sharp edges and one-way doors, BEFORE your user commits to an action they might regret. These are not incompatible. reply johnnyanmac 16 hours agorootparentIf nothing else, the \"smart cows\" will read the docs and may even help out on support forums when some of the user base doesn't and decides to complain. Docs are good for everyone and will save time internally and externally. >your UX needs to warn people about sharp edges and one-way doors, BEFORE your user commits to an action they might regret. These are not incompatible. Assuming trump doesn't nix it, the recent FTC ruling should fix this by itself. it should be as easy to unsubscribe as it is to subscribe. reply eigilsagafos 11 hours agorootparentI don’t think easy unsubscribe falls under «sharp edges and one-way doors». But high friction unsubscribe probably goes hand in hand with sharp edges and one-way doors at those lead you to want to unsubscribe…therefore they make it hard. reply babyshake 15 hours agorootparentprevAlso, it's pretty easy at this point to setup an AI support agent that can reference your docs, and it is a helpful exercise for organizing your own thoughts and being able to communicate them back to yourself. There's not really a good reason to not have good docs. I just assume that if a product is gated behind \"let's hope on a call!\" it's still vaporware and there's a 50/50 chance that they'll pivot within a few months. reply freefaler 16 hours agorootparentprevThey also are great for the support team, to \"educate\" where they can find additional info and for linking as help links if the interface needs this. reply AznHisoka 6 hours agorootparentprevAlso docs can be effective for marketing especially if you sell bottom-up to devs. If i visit a site and see a section for docs, it means they are catering to my needs reply portaouflop 6 hours agorootparentprevYou definitely needs docs, they are just as important as the product. But thinking all your prospects will read the docs is a fallacy- maybe 5% of prospects fall into the category of OP, more commonly they won’t even know what their own requirements are, let alone how to map that to your product/docs reply taneq 15 hours agorootparentprevDocs are a great defense against clients that demand hand-holding, even if nobody ever actually reads them. If they ask a question covered by the docs, simply point them at the docs and they'll go away indefinitely rather than read something. reply ozim 8 hours agorootparentEven if no one reads docs up front - having some documentation to point them to is a must. I cannot imagine making stuff up all the time as you go. Maybe if system is super small. But then even better to build up documentation to have it for later. reply portaouflop 6 hours agorootparentprevMy point is that the clients that matter will require hand-holding. Sure if you just sell to hobby programmers and students it’s no problem. But if your customers should be real businesses - in my experience 90% of them require a lot of handholding, you literally have to read the docs for them and tell them in a call. reply Spivak 12 hours agorootparentprev> if they find one edge-case that is not covered by your product they concentrate all their energy on it Having run into this many many times it's usually because that little edge case is where the hard problem lives in the problem domain and is the reason, whether the customers know it consciously or not, they bought it. Who pays for something that only handles the easy cases? The author even talks about it with the \"one specific thing\" line. I guarantee it's that one stubborn edge case. It's likely you reached for a solution in the first place because you wanted something to turn a problem with a mix of difficulties into an easy one offloaded to a 3rd party. reply bongodongobob 13 hours agorootparentprevDocs aren't just for the users, they're for your support team as well. A lot of times I'll put in a support request for some product we use and the first thing support sends back is documentation. Most of the time, that's all I need and it saves us both a ton of time. reply kashyapc 11 hours agoparentprev> Never have an AI agent call someone unless the customer specifically requests that [...] Are there real humans / customers who ask, \"yes, have an AI call me?\" reply SteveVeilStream 1 hour agorootparentThere are times when I would ask for this. If it's midnight on a Saturday and the website says: \"You can reach a human agent 9-5 M-F or we can have an AI agent call you right now\" then I might opt for the AI agent. I may also do this if I want to take a call while out for a bike ride, in a noisy place, etc. Talking to AI means I don't need to worry about being polite which is actually very convenient. reply drewcoo 7 hours agorootparentprev> Are there real humans / customers who ask, \"yes, have an AI call me?\" If it could go faster than a slow-talking human who needs to re-verify information that someone else already verified? Yes, I'd opt for the AI. reply kashyapc 3 hours agorootparentTrue, it depends on the quality of the human on the other side. If it's a mindless front-line support person parroting a script, \"did you try restarting the computer?\", then I'd take a smarter AI too. reply rkagerer 17 hours agoparentprevGood advice. I created and sold software (and services) for over a decade, and could never have lived with myself if I'd treated customers the way the original article describes. reply johnnyanmac 16 hours agorootparentsadly, they do it because it works, and (not trying to be too offensive): lotta people are dumb. If you're confused, a nice \"team mate\" will be more comforting than a cold hard doc and web form. Even if they are upcharging you on a bunch of stuff you will never use. They call multiple times because some people genuinely forget and appreciate the reminder. I'm sure you knew all this better than me, but just want to elaborate for anyone who genuinely don't understands who falls for this. reply Tepix 13 hours agoparentprevYour list isn't rendered as a list reply moffkalast 6 hours agoparentprevThe counter approach is better for everyone, less work for the company and a nicer experience for the user, but only in a vacuum. Unregulated competition is a race to the bottom, so the most unethical, invasive, borderline illegal, and manipulative tactics come out on top. Not the best product, but the best marketing. Not the most customer friendly, but the most customer-converting through coercion. Not pricing products and services what they're worth, but the most anyone will be wiling to pay for them. reply marcus0x62 7 hours agoparentprev> The counter approach is: - Make complete information available as transparently as possible and don't gate it. - Be forthcoming about weaknesses. Don't force prospects and customers to find them. - Ensure that when a prospect or customer does want to talk to someone, they immediately reach someone who can handle the problem or answer the question (no need for escalations.) - Never have an AI agent call someone unless the customer specifically requests that and be sure that all AI agents immediately disclose that they are AI. - Offer flexible e-mail list subscription options (monthly, quarterly, annually, only release notes, etc.) - If the product is not a fit, try to offer something useful anyway such as a suggestion of another product that might be a better match for their needs. From the perspective of designing a sales org to do this it is, roughly, a sales-engineer focused sales organization with the sales engineers on a minimal commission plan (i.e., a pooled 80% base/20% commission plan instead of something like an individual 70/30 plan.) I never saw it in practice, but it would probably have something like a 2:1 ratio of sales engineers to account managers. On the plus side, sales engineers are less costly to employ than account managers. On the down side, this will almost certainly result in lower revenue. > Value based pricing is one of the reasons why a lot of companies end up in these situations. Rather than setting a standard price, the company does a detailed investigation of the customer to try to find out how much value they will gain from using the product and then they set the price based on that determination. Although it maximizes revenue in theory, it is slow and invasive. In my experience (working in tech sales for a few large manufacturers,) this is almost never a routine practice of the sales team. It is sometimes discussed in sales training, and very, very occasionally put into practice, but the Achilles heel is your last comment: it is slow and invasive. Sales people are a diverse group, but they are united by the belief that time kills all deals. Product teams do large scale market segmentation and price sensitivity studies -- that's where you get all of the packaging options. Sales teams? If they have a large existing customer, that customer will already have a standard discount level. That's the starting point for any given negotiation. For new customers or smaller customers, the sales team will try to sell at the 'standard' price, where by 'standard', I mean the prevailing discount level in their region. Variables that will ultimately cause the price paid to change: fiscal year-end/quarter-end, competition, bundling, \"incentives\" / customer \"budget.\" I put those roughly in terms of how powerful a lever they are to move the price, on average. Fiscal year-end/quarter-end. One challenge with trying to root out traditional sales/purchasing behavior is that customers are trained to reinforce the behavior. While it doesn't always work (a sales rep might be significantly under or over goal and might want a deal to slip into the next quarter/year,) if a customer can force a deal to get done around the end of the vendor's fiscal periods, they have significant leverage to negotiate a lower price if the vendor believes the deal might slip. Competition - this one is pretty obvious, but adding a competitor to your purchase evaluation and letting the vendor you want to buy from know that you are seriously considering their competitor will likely result in a lower price. Bonus points for the purchasing team telling the vendor their engineers like the competitor's product better, but the purchasing team really wants to buy from you because they love you so much. Double bonus points for telling both vendors that. Bundling -- if a manufacturer has multiple products, they often will voluntarily increase their discount level to induce the customer to buy a product 'suite'. The sales reps are often under immense pressure to sell the newer and flakier products on their price lists, and this is usually the only way it gets done. On the plus side, the customer gets more product for the same price. On the down side, the extra products included often aren't worth using, and come renewal time, the true total cost of ownership becomes apparent. \"Incentives\" / \"customer budget\" - these are both, typically, BS in that the vendor doesn't usually have a magic \"incentive\" to offer a one-time discount. The sales rep just got (or thinks they can get) approval for a lower price. Similarly, while customers most certainly do have budgets, they often lie and say they can't buy x because it exceeds their budget, when their internal sales people (the purchasing team) just wants a lower price. I refer to the purchasing team as \"internal sales people\" because they often-times have a commission incentive just like the vendor sales rep: it isn't uncommon for them to have a personal, individual bonus tied to driving vendor contracts down in price. In some cases, it is a direct commission -- i.e., the purchasing agent (or sometimes budget owner) gets 10% of any monies saved. I wish you the best of luck, but other than taking people completely out of the equation, I think it is really hard to eliminate this behavior in practice because 1) it is so thoroughly ingrained amongst both vendors and customers and 2) it works. reply neilv 5 hours agoprevAccurate and nicely written article. There's a worse angle I've seen on the same story: A different engineer, who was already running the good open source solution successfully with their team, heard the other engineer had an enterprise sales assault dumped on them, for some load of trash, and so tried to sound the alarm, but it was already out of the other engineer's hands, because upper subtrees of the org chart had already signed on with political capital (and in a couple cases, it's somehow become an express personal performance metric that they spend the company's money on this specific product). So all alternatives are blocked, everyone has to wait the next couple months for the sale to finalize, and then another month for IT to set up the SSO and databases, and forced to migrate from working solutions to garbage. Then eventually the contract doesn't get renewed, with appropriate face-saving for those who pushed it through, but half the people upon whom it was inflicted have already found jobs in better companies, a quarter got PIP'd or stalled careers for being sabotaged by the trash, and the remaining quarter will mostly be laid off next year. Just say NO to enterprise sales. The only thing worse than having to do enterprise sales to sell your own product, is being the employees forced to use the likely bad purchasing decisions of other enterprise sales. reply kashyapc 10 hours agoprevI know the author says this for humor and effect: \"[...] I'll Google CodeSquish and discover it does everything I need, costs nothing, and is 100x more performant—even though it's maintained by a single recluse who only emerges from their Vermont farm to push code to their self-hosted git repo.\" The poor, single recluse discovers one Tuesday afternoon that your company makes 100 million dollars a year with \"CodeSquish\", while not contributing anything back. He silently questions his life choices—or, shall we say, licensing choices—while feeding chickens on his Vermont farm. reply ozim 8 hours agoparentRealistically it doesn’t work like that often. Most of OSS or even SaaS stuff is useful in F500 company but not more like convenience and not being able to put value on it. This also you can see in article even SaaS vendors have to make shit up to seem more valuable for big companies and use value based pricing by calling people and trying to push BS on them. I also don’t see companies running much OSS systems as reselling them because then usually they rather build their crappy one like article mentions because they want to own whole IP. Amazon pulling it of on Elastic was mostly because Elastic built a brand - they are not pulling this off on a random project. They might use some project to run their infra but attribution of value to a specific lib or project is super hard. Then also Amazon is making money because it is Amazon not because they use Linux kernel. It is hard to wrap head around anyway ;) reply Asooka 10 hours agoparentprevThinking about it, maybe he should start sending invoices to the larger companies. If they're reasonably small and for a product the company does use, finance will just pay them without questioning a thing. reply kashyapc 9 hours agorootparent\"CodeSquish\" in this article is a fictional example! Edit: from a quick search, there is a SaaS company called \"CodeSquish\". I haven't checked if it's also an open source project. reply Scotrix 17 hours agoprevThis story is for me a real painful death I have experienced way too many times, absolutely nuts. But then you find an open source solution which is in general better and can do everything you want (simply tested already with just a docker compose up) but for deployment you get hit hard by compliance who just checks the SOC2 certifications and wants a in-depth due diligence of the code since everyone in the world can theoretically change it. Then your manager asks how it can be so good if it's for free and open source. And of course, last but not least, your overloaded team in general not happy to support just another unpredictable piece of software... So it's the question to rather burn money and nerves with an awful SaaS offering and their endless and useless sales cycles and terrible and super expensive vendor-lock-ins or burn some money and nerves by utilising and running open source inhouse... So typically I prefer to chose for the open source option and especially if the SaaS option isn't allowing me easy and fast self-onboarding, meaningful testing periods and a predictable and transparent pricing. And then, if it get's widely adopted, I allocate some budget to support the authors and/or get some support plan (for more complex open source software) in place even though you most likely never need it... reply benjaminwootton 14 hours agoprevThis is very true and funny. I’d make a few points though: 1) It may turn out that a lot of this is necessary in order to sell B2B and keep half of the software industry going. The business on the sell side might need to reach out multiple times, engage a sales engineer, help you align all the decision makers etc otherwise it simply wouldn’t get done. Buyers are so busy and selling to a big company is so complex that some of this is just a necessary evil for B2B commerce to continue. 2) Imagine if companies were actually better at buying. They spend $millions on enterprise bloatware when startups can literally produce something 10x better at a 10th of the cost. If they were easier to sell to then we could all have nice things without this madness. I agree that the OPs experience is soul destroying, but clients could help themselves a little and end up with more money in their pockets and better tech. reply tossandthrow 12 hours agoparent> something 10x better at a 10th of the cost SLAs and contractual assurances are very hard to deliver at 1% cost. Enterprise products is just another class, and it has nothing to do with the product. reply leoedin 11 hours agorootparentHaving had the misfortune of navigating a particularly ineffective large corporate purchasing machine, this resonates. Nobody in the chain is interested in outcomes, they’re interested in completed process. The value of something to the business is totally uninteresting to a purchasing person. They care about contractual arrangements, compliance with all sorts of poorly thought out ethics agreements, and making sure that all the process has been followed. The result is far more money spent for far less outcome than if we’d just got a credit card and bought something. There’s a whole ecosystem of companies which are terrible at actually solving problems, but know exactly how to meet all the requirements of a large corporate purchasing department. It’s depressing. reply DriftRegion 18 hours agoprevI've had a couple of experiences in the past month where I do respond to the enthusiastic sales engineer's check-in with a genuine product question, only to receive an immediate, lengthy, and subtly wrong LLM generated response. It feels gross. reply jlund-molfese 18 hours agoparentTwo comically bad lines in an AI-generated spam email I recently received: \"Saw on LinkedIn that you spoke Spanish. I've heard that the way \"¡Qué chévere!\" brings such energy and brightness to a conversation is uniquely charming. Have you had a chance to practice it recently?\" \"Develop a compliance automation tool that adapitates to changing regulations, reducing overhead costs while ensuring secure and efficient investment programs.\" No human would ever see my \"limited working proficiency\" of Spanish on LinkedIn and say something like the first line! And the second? \"Adapitates\" is not a real word, it's a hallucination. https://old.reddit.com/r/ChatGPT/comments/1d8gc6x/did_chatgp... Sales isn't the problem, and most people are tolerant of some level of sales. I've gotten unpersonalized cold outreach from a data replication company that actually made me interested in the product, because it was short, to the point, and (as far as spam emails can be), authentic. reply andrewflnr 14 hours agorootparentWow, I would normally assume that a made-up word is proof that it wasn't AI generated, just regular bad human writing. reply johnnyanmac 15 hours agorootparentprevHow is AI hallucinating words now? I thought that would have been the easiest thing to restrict with a sufficient dictionary. Or maybe it's an ancient dictionary. I was kind of surprised at the sizes of ]dictionaries I could find while trying to test out a personal project. reply Arnavion 12 hours agorootparentIIUC the input to LLMs is tokenized not on word boundaries but some kind of inter-syllable boundaries, because then whatever the model associated with \"task\" will also apply to \"tasking\", \"tasked\", \"taskmaster\", etc for example. So a model making up compounds that don't exist would be fully possible and even desirable, especially since real humans do it with English all the time. reply staticautomatic 11 hours agorootparentThey’re called “lemma” reply Arnavion 10 hours agorootparentThe intent is the same, but as I understand it LLMs don't tokenize based on lemmas, though some of the tokens probably line up with them. reply Aloha 17 hours agorootparentprevMaybe adapitates should be a word! ;-) reply AznHisoka 17 hours agorootparentprevWas the main reason you were interested in it because you actually could see yourself using the product? Rather than because it was short and to the point? reply noman-land 18 hours agoparentprevPlease consider responding and telling them how you feel and providing feedback in whatever feedback forms that accompany the ticket. Every company on Earth is exploring this tech and if we don't give them strong signals when they fuck it up we are just dooming our future selves to this garbage. reply robocat 16 hours agorootparent> providing feedback > ticket Where do you work that this is an effective strategy? In my experience telling a vendor something is broken wastes my time and has no effect on the vendor. I don't know whether sales don't care or sales can't make dev changes. The only exception is when I can contact the devs and I know the devs have a history of fixing problems. reply kelvinjps10 15 hours agorootparentI used to work in customer service in a big tech company, what you are told to do is to reply with a sorry message and to fill a ticket but the ticket is not actually handled, it's just there to be recorded and maybe they will lookup it up by the end of the quarter. reply cookiengineer 5 hours agorootparentPay me so I'll provide feedback. Otherwise stop wasting my time with unwanted spam. You are wasting my time, not yours, because you automated your spam workflow. The only strategical response is to flag your emails as spam ans to block your emails in the future. Everything else is a losing game strategy because there are more spam emails per day than available time already. reply collingreen 15 hours agorootparentprevSeconded. Also imagine not just shouting feedback at a person who doesn't care but that function being replaced by an llm that literally can't care. reply noman-land 13 hours agorootparentYou misunderstand. Think of it from a data standpoint. What they have. Ticket. Feedback. If they see tickets with certain feedback scores, some rep's score will go down and some manager's score will go down and if enough people do this (it literally takes a 1-2 digit amount of people in most cases), someone will raise an eyebrow and ask some questions and read theIf I really, really care about your product, I’ll contact the 300 people I need on my side to get it approved. This process will take at least a month. Why? Who knows—it just always does. If I work for a Fortune 500 company, it’ll take a minimum of three months, assuming everything goes perfectly. This pretty much chimes with my experiences getting anything that costs more than my boss can readily expense. Purchase agreements in large companies are pure hell. reply bjackman 8 hours agoparentAlso sometimes for things that cost amounts I could easily expense I've been going through a purchase order process for a bunch of patch cables and adapters so cheap I could even expense them without a receipt. It's taking months. They're being delivered by air freight. My heart and soul are aching. reply einsteinx2 3 hours agorootparentHonest question, why are you going through that purchase order process if you could easily expense them? reply tptacek 17 hours agoprevYou have to pay extra to get 12- (or 4-) hour support SLAs and SSO access because if you didn't, the entry level of the product would cost integer multiples more. The people that want those product features --- regardless of how much they cost (support: a fortune; a SOC-2 report: zero) --- subsidize the people who don't. If it helps: just look at the \"bells-and-whistles\" package with SSO and an SLA as the true price of the product. Nothing in technology is really cost-based pricing to begin with. reply SeptiumMMX 13 hours agoparentI don't think the 4-hour SLA customers subsidize the 72-hour ones. It's more about managing the volume of support. If know you won't get an until 3 days after, you will google obvious things yourself, and only contact support if you can't get the answer otherwise. But shorter SLAs, let alone phone support, encourages a particular kind of customers to just copy-paste any error message they encounter (that may not necessarily come from your product) and expect support to untangle it. Been there, seen that. reply jwr 16 hours agoparentprevThis. And to expand on the above: few people consider how much it actually costs to provide 12- or 4- hour support with a strict SLA. This pretty much means that the business needs to bear the full loaded cost of at least one additional employee. Now go divide that fully loaded cost (take the salary and multiply it by 2x, roughly, and I'm not quoting any numbers here to avoid stupid discussions about \"but it costs less to hire a person in my area\") by the number of customers that require these SLAs and see how much this needs to cost. reply tptacek 16 hours agorootparentSupport is an example of an \"enterprise\" feature with a high cost basis. SSO is an example of an \"enterprise\" feature with almost zero cost basis. But in both cases, the cost is a sideshow. The real driver for the pricing of these features is market segmentation: the customers with high expectations of support, or a requirement to have SSO, strongly tend to be less price sensitive than the rest of customers. The fundamental goal of product pricing is to find ways to charge price-insensitive customers more than price-sensitive customers. Like, yes, you'd lose money offering 4-hour-SLA support to customers paying the entry-level price. But you could make that decision, and have part of your business model be subsidizing those customers. It depends on everything else in your model; how you acquire customers, what their lifetime value is, &c. reply kuschku 15 hours agorootparentMaking SSO a premium feature isn't any different from the paid SSL of the 2000s. You'll reduce security and safety just to segment the market. And in my experience, SSO doesn't need to be expensive. SAML and Kerberos should be expensive, sure. But OIDC with standard seevices such as Google, GitHub, Okta or Keycloak should absolutely be part of the base package. Personally, I'll just build the open core version from source and add all the \"enterprise\" features like SSO, S3 and Prometheus metrics myself. reply tptacek 15 hours agorootparent\"Need\" has nothing to do with why SSO is expensive. There is no meaningful difference, from a business model perspective, between SAML and OIDC (though: don't do SAML). SSO is expensive because customers that require it are less price sensitive than customers that don't. By all means: build your own SSO on the open core of products that charge for SSO. I promise: the companies don't really care that you're doing this. You're profoundly segmenting yourself out by doing that. reply Dylan16807 1 hour agorootparentWell you focused pretty hard on the word need, but do you disagree with \"Making SSO a premium feature isn't any different from the paid SSL of the 2000s. You'll reduce security and safety just to segment the market.\"? reply kuschku 15 hours agorootparentprev> There is no meaningful difference, from a business model perspective, between SAML and OIDC (though: don't do SAML). Oh there absolutely is. The customers that use OIDC use a relatively light stack and are generally okay with relatively simple setups. They're usually startups and are also the ones that can feasibly add SSO into your app themselves. The customers that need SAML are older, larger companies often with a custom mix of multiple hybrid AD and AzureAD directories for each department that need special handling with custom properties and realtime sync. You'll be spending at least a few engineer-months on support for them. I've been on both sides previously, I've seen it all. reply tptacek 13 hours agorootparentThat's all fascinating, and I've done security assessments of about a dozen SAML implementations (don't do SAML) and have built several OIDC SSO implementations (whatever I'm fine with OIDC), but none of this has anything to do with product pricing. reply bruce511 14 hours agorootparentprevThe problem with 4 hour SLA (I'm assuming in non-business hours?) Is that it's eye-watering expensive to scale. If I have 100 cheap customers I need to scale it for 100. If I have 3 I can get by with putting staff \"on call\". (Pay the existing staff a bonus for out of hours etc.) You can't pay $20 a month for something, and expect a high SLA . That's not reasonable (and certainly not sustainable. ) a business that offered that would be on my \"don't touch, will be out of business soon\" category. It's certainly possible to have a \"free tier\" - there are hood reasons for that. But the free tier had better not be costing you money. If they do, you don't have a business you have a hobby. reply robocat 15 hours agoparentprev> The people that want those product features subsidize the people who don't Surely any company doing that would be spending profits on less profitable customers which is not economically sensible. See the box on page 2 of https://regulationbodyofknowledge.org/wp-content/uploads/201... Cross-subsidisation can occur due to regulations or a company trying to monopolise a market. Pricing a product depends on how much you can charge and variable costs, not so much on how much it cost to develop - which as you point out leads to price discrimination - where it looks like one group is subsidising the other but maybe not in reality. I would like to see a model of the marginal costs and ideal decisions. Deciding to build features for higher paying clients only makes profits if those features are paid for by those clients. Other clients might get those features but it isn't \"subsidisation\" - however I'm not sure what a better word is! It isn't free-riding or consumer surplus. reply bruce511 14 hours agorootparentIt's called market segmentation. The classic analogy is airline seats. Ultimately the job of the airline is to get me from A to B. They do just that for First Class and sub-economy. But for more money you can have snacks etc. Airlines offer add-ons that cost them real money (checked bags) and things that don't (seat selection.) They allow the customer to decide which features they want and which they don't. Not all seats generate the same profit, but all seats generate some profit above marginal cost. (Usually some number of seats needs to be filled before the flight makes a profit, but that's a different equation.) With software there's naturally some segmentation, and so the smart company tries to capture that value. Equally some segments want different (expensive) things like Support (and can afford it) so that needs to be on the table to win that customer in the first place. A segmented offering is inevitable, at which point you then have to decide who does the segmenting. The client? Or do you have a salesperson to help them? There's no right answer, but usually it depends on the product price. If I'm buying an airline seat I can figure it out [1]. If I'm buying an airplane probably not. [1] I'm old enough to have lived in a time when there were specialists necessary to buy an airline seat. reply bigstrat2003 15 hours agoparentprevSupport is one thing. There is absolutely no excuse for upcharging for SSO. None whatsoever. Any company that does this is a shitty company who deserves to go out of business for abusing their customers. reply tptacek 15 hours agorootparentSo most companies are shitty companies that deserve to go out of business. Got it! reply TylerE 15 hours agorootparentFrankly I do t disagree with that statement. Modern corporations are looking more and more like a disease trying to infect all of humanity. reply tptacek 15 hours agorootparentYou get that you're arguing that companies shouldn't get a price break for not wanting SSO, right? reply Dylan16807 1 hour agorootparentThat sounds fine to me. What makes you think that's self-evidently wrong? In any product there's a thousand different features where offering a price break for not using that particular feature would be silly. If anything, just considering the customer side of things, give a disincentive for not using something like SSO to reduce careless password use. reply TylerE 14 hours agorootparentprevNo, I'm arguing that most people would be better off is a large number of companies got the corporate death penalty. SSO is orthogonal. reply jwr 16 hours agoprevWhat is amazing for me is that these sales tactics actually work, because many people want to be sold to in this way. Do the direct approach with no bullsh*t, instant demo, meaningful trial period, easy onboarding, etc and lose those customers that expect the usual sales ride. Source: I do the direct approach. reply tptacek 16 hours agoparentRight; the thing here is: the author obviously knows the game (because they wink knowingly at it repeatedly). Given that: why are they ever getting on the phone with a sales engineer? You can just delete the emails. reply kassner 2 hours agorootparentIf you don’t, eventually your VP will hear that you are lacking such tool that they will tell you to deal with vendors X and Y, even if they don’t fix your problem. At least by proactively going after it, you can pick vendor Z, which despite having the same terrible process, at least it addresses your problems (until the imaginary limits apply). reply makeitdouble 14 hours agoparentprevThese tactics work when the sales get access to the manager or director first, close the deal, and have low level people manage the aftermath. In the case described where a technical person is shielding the final decision makers, it's more of a gamble and will often fail. reply grepLeigh 18 hours agoprevMaybe I'm the outlier here, but 15 minutes to chat with a human about my use case and pricing is way more efficient than donking around in docs/trial product. The only product I really want to punch in credit card info and GO is commodity software (e.g. AWS EC2 or a domain registration service. I think wires sometimes get crossed in pricing/sales models, where an enterprise product gets priced like commodity software ... but that's usually a sign the company is immature. There shouldn't be a sales team for software that costs 2-3 figures. Software costing 5-6+ figures absolutely requires people in the sales/onboarding process, because a big part of what I'm paying for is support. reply scarab92 17 hours agoparentMaybe I’m not asking the right questions, but I consistently find that I get “Yes” answers in these calls, that turn out to actually be “No” in practice. I think the problem is that we rarely want to know “can you meet this use case”, but rather “how well can you meet this use case”, and that’s hard to assess without putting your hands on the software. reply bruce511 14 hours agorootparentWhich is to say that the quality of the sales person matters. If your sales department is staffed by people who got hired on Monday, and are on the phone by Friday, then frankly they're not worth much. I've seen the opposite though where sales folk know more about the software than support folk. They're equipped to help you with choices, but also understand limits and high-cost areas. Yes you absolutely can get Custom Reports, but we absolutely charge for that. And the data you're looking for is on this built-in report.... Dealing with a good salesperson, who knows their stuff, and understands that truth and trust are important, is an amazing thing. reply johnnyanmac 16 hours agoparentprevIt's definitely a generational thing. I've been spammed so utterly often that I simply do not answer my phone for a non-contact (or the inevitable interview phone call. But less often with video calls these days). If it's important enough to contact me, it's important enough to leave a voice mail. I don't really do these sale pitches often, but it's a similar mentality for a different reason. I simply want anything communicated in writing in case they try to say yes to put a foot in the door, but the small details say no. reply elevatedastalt 15 hours agorootparentI presume you are an emergency contact for some people? Maybe a spouse, or a kid? Or even a friend? What's your contingency for when they are lying bleeding somewhere and someone can't reach you since they are not on your contact list? reply ryandrake 15 hours agorootparentI'm not the guy you asked, but I also basically keep my phone in Do Not Disturb mode 24/7, meaning no calls, no texts, no notifications, ever. I choose when I have time to look at my messages, not the other party. I'm not a doctor, and even if I was, I'll never be able to help them purely over the phone if they are \"lying bleeding somewhere\" and I'm not around. If my house is burning down and I'm away, what am I going to do about it remotely that a phone call will solve? I'm not a firefighter and I can't splash water over RF. If something happens at my kid's school, I'm not there, and even if I was, I probably wouldn't be able to do anything about it. That being said, if someone really, really thinks that I can somehow help them over the phone in an emergency, despite my number not being 9-1-1, certain family and friend's numbers I allow to punch through DND and reach me. reply bruce511 14 hours agorootparentMy phone is on silent 100% of the time. (With small exceptions when I'm expecting a call.) I might like to be informed of emergencies, but I'm not a first responder. If you are bleeding phone 911, not me. To be fair, my mom sends my wife a message to tell me to \"check my phone\" if she needs me :) Each person finds their own level of intrusion they want from their device. I've picked mine. You pick yours. reply johnnyanmac 14 hours agorootparentprevNot particularly, no. But I imagine they would simply say \"Johnny it's X, call me. It's urgent\". (Scammers are bad, but I've never been tricked with that kind of line). If I'm being frank, that extra minute for me to respond probably won't change their fate if they are indeed bleeding out somewhere. reply datavirtue 13 hours agorootparentYeah, if you answer calls from random numbers something unsettling usually happens like some Indian guy yelling at me about how the IRS wants their money and the cops are on the way to my house (punchline: he can make it stop if I give him my bank account information). Unfortunately, it works on the kind of people who answer phone calls. reply dinkumthinkum 15 hours agorootparentprevOne could ensure their spouse or child knows about 911 in America or similar service in other countries, which is, of course, what should be called in such a circumstance firstly anyway. Also, people generally have such numbers as contacts in their phone ... I don't know why I'm explaining it; this just seems like common sense ... reply elevatedastalt 13 hours agorootparentEmergency calls often come from people NOT in your contacts. That's why you provide emergency contacts on forms. If something goes wrong at work for example, someone from the office would call, not your spouse themselves. reply OJFord 6 hours agorootparentAnd how will that not be important enough to leave a voice message about? reply genewitch 6 hours agorootparentblue-collar, unemployed (NEET, even), my oldest kids, PhDs i've known personally for decades, white collar workers - an incomplete list of everyone i called in 2024 with a full voicemailbox. i try to tell people about the \"two calls in a minute lets it go thru\" feature because as of yet the autodialers don't know about it or have it implemented. reply OJFord 4 hours agorootparentThey're probably not also people who don't answer their phone, preferring either to have a voice message or nothing (because it isn't important) then. I do the same as commenter up-thread, my voicemail inbox is empty. Sometimes I let a call ring out and then listen to the message immediately, I just don't want to have to deal with it synchronously. Then if it's 'I have a number of opportunities that seem like a great fit for you' I can just delete it and move on with my day, not have to try to say no-bye politely before hanging up, for example. Someone with a full inbox is more likely someone who does the opposite - they'll never listen to their messages because they want to talk to someone, you'd have to call them back anyway so no harm it's full. (Or they'd call you from the missed call, not because they heard your message.) reply genewitch 3 hours agorootparent> Or they'd call you from the missed call, not because they heard your message. people with professional and PhD with full mailboxes do this, with jitter of up to hours. the last full voicemail i hit i was called back nearly immediately, and they said \"what, i just delete all my voicemails, if i call it says no new no saved\" There's a reason i'm harping on specific elements so much, because i don't think voicemail is magic, i guess. do these people have voicemail boxes like this (nsfw language, a bit. idk. it's art.): https://nextcloud.projectftm.com/index.php/s/NSFW_voicemail_... reply ghaff 17 hours agoparentprevI don't really disagree. The problem is outreach when you're clearly mostly researching something whether to do with computers or something else. One travel company is particular was pretty aggressively reaching out because I downloaded a couple brochures. reply stackghost 14 hours agoprevI pride myself on never having paid for, recommended, or endorsed any product or service with \"Contact Us For a Quote\"-pricing. I have no desire to be in your fucking sales funnel. reply datavirtue 13 hours agoparentYou aren't the customer. reply mitthrowaway2 13 hours agorootparentI'm in the same boat as the GP, and while I'm clearly also not the customer, that's not necessarily reflective of how much money is being forgone by missing out on my business. Oh well, their loss and the competition's gain. reply staticautomatic 11 hours agorootparentI am the customer I refuse to engage with 99.9% of companies who have a pricing page that says “contact us” for every tier. reply satvikpendem 10 hours agorootparentThen...you are not the customer. The customer for Contact Us is vastly different than the customer that can enter their credit card details and gain immediate access. reply staticautomatic 10 hours agorootparentI honestly don’t understand what you mean. Is it that I don’t have the money or that I’m not dumb enough to “Contact Us”? reply Cilvic 2 hours agorootparentFor me it's a bit like typo in phishing/scam emails. If you think \"oh my god this scam is so bad how can anyone fall for it\" then you are probably not the target group. reply wccrawford 7 hours agorootparentprevI disagree with them here, but I think their point is that they're aiming for a certain type of customer, and you aren't a good fit for their business. Not every business wants every customer. Some businesses know they can do very well if they keep their costs-per-customer low and their revenue-per-customer high, even if they have to have fewer customers to do it. Some of those \"contact us\" companies are in that situation. I think that most of them aren't, but wish they were. reply ginko 6 hours agorootparent>Not every business wants every customer. Some businesses know they can do very well if they keep their costs-per-customer low and their revenue-per-customer high, even if they have to have fewer customers to do it. So they forego customers that just want to pay $N amount of money to try&use their software/service in favor of customers that want touchy-feely phone support at every step? reply lee_ars 4 hours agorootparentThey forego the customers (like you) least likely to be vulnerable to soft-touch upselling, yes. reply dpkirchner 9 hours agorootparentprevAs far as I can tell it's tautological: you're not the customer because you're not clicking the link because you're not the customer, etc. reply lee_ars 4 hours agorootparentThe customer who just wants to put in their CC# and buy the product isn't a pipeline customer. You can't come back to that customer every other quarter and take them out for golf and pump them for additional services based on the \"strength\" of your relationship. You don't have any opportunity to make an in-person upsell pitch at the customer who just wants to put in their CC# and go. In-person sales work at least in part because salesbros are trained to handle and overcome objections. They do it in many ways, and most of those ways take advantage of the fact that people by and large tend to want to act agreeable and kind to each other kind of by default. If you opt for the in-person meeting, the salesbro knows they have the opportunity to guilt or nudge or suggest you into buying additional services, or a longer service agreement, or a higher SLA, or whatever. So, OP isn't the customer because they're announcing they're not interested in the dance. Many (not all!) IT vendors prefer the dance because it makes more money. I say this as a former presales engineer for EMC (before it was EMC/DELL), serving large enterprise named accounts in the Houston area. If my boss saw me in the office, he'd kick me out and tell me I should be out taking clients out to lunch or to Astros games or whatever. I spent probably $2-3k/week on entertainment, and it was basically imaginary money to EMC, because if me and my salesbro blew $20-30k hooking a client, we were going to be doing it on a $2-3 million Symmetrix deal with 70-80% margin. So yeah. OP isn't a customer because they don't want to play, and (most, not all!) companies can extract more revenue out of customers who DO play. reply cco 14 hours agoprevI tell people selling devtools that if you find yourself on a sales call with a single software engineer, something has probably gone wrong (as described in this article). However if you're on a call with two EMs, a couple engineers, a security engineer, and a product manager, you're on the right call. A single engineer very likely wants a PLG (product led growth) experience, sign up, read some docs, make a few API calls, and then punch in a credit card when they're ready. But you don't sell a $500k deal (usually) without some phone calls and a deck. reply Aeolun 17 hours agoprev> I start to wonder if I should’ve just reverse-engineered your tool myself. Ever since I started taking this advice instead of going through a sales process (I really only need this inflicted on me a single time in my life), I have been a lot happier. We also stick the counter at around $50k saved per year for applications that are essentially fancy crud forms. The best part is it bypasses all the complicance requirements, since if it’s written in your company it can’t possibly be bad. reply fancyfredbot 2 hours agoprevThe frustration of attempting to buy anything in a large organisation is so great for both sides that it prohibits the purchase of cheap tools to help with small jobs. They might have saved time if you can buy them in a few hours but since it will take months nobody will bother. reply solatic 11 hours agoprev> Chances are, I signed up to see if your tool can do one specific thing. If it doesn’t, I’ve already mentally moved on and forgotten about it. So, when you email me, I’m either actively evaluating whether to buy your product, or I have no idea why you’re reaching out. Let me get this straight: author clicks around on B2B SaaS with un-focused sales models (both the heavy account rep / sales engineer sales-driven process, and the free trial product-driven process) because they want One Small Feature that's too big to build themselves (but still small enough for someone else to build and open-source, sustainably), knowing that expressing interest to purchase will result in an Enterprise Sales Cycle yet being pissed off that it resulted in an Enterprise Sales Cycle? Methinks the author doth protest too much to cover up their own misaligned expectations. If you want One Small Feature then it's either covered by the product-led pricing (i.e. credit card based, no Enterprise Sales Cycle) or it's not. And if it's not, if you have to click a \"Contact Us\" button, which, being an industry veteran, you know will launch an Enterprise Sales Cycle, then maybe what you're asking for isn't really One Small Feature, because you're going to pay Enterprise Pricing for your One Small Feature, and so maybe this isn't the vendor to get it from. The Enterprise Sales Cycle exists because the number of people who can put Enterprise Pricing (~$25+k/year) on a corporate credit card, without any internal checks, is close to zero. The Enterprise Sales Cycle exists precisely to align the many stakeholders whose alignment is necessary to sign larger deals. If you have a corporate card, are looking for a solution that you can buy independently, then you are not in the vendor's market and you're doing everybody a huge disservice by trying to force a deal anyway. Go find a vendor that actually targets your market segment. reply staticautomatic 11 hours agoparentI agree but as a customer it’s still worth asking. I recently asked a salesperson if I could get access to some enterprise features on a PAYG plan because I literally couldn’t go through the procurement motions fast enough for my needs. They said yes and just had an engineer toggle some feature flags on my account. reply mvdtnz 11 hours agoparentprevHave you never interacted with companies like Datadog or LaunchDarkly? If you think signing up for the cheap plan will stop the Enterprise Sales Calls you're dead wrong. reply oefrha 12 hours agoprevA big company assigns me a new account rep every ~six months, each time resulting in an email from the new account rep introducing themself and trying to schedule a call to “learn more about me needs” (read: upsell me). Not sure if they intentionally rotate account reps without interactions just to have a reason to email. These emails usually promptly go to trash, but the one I got this week grabbed my attention because they somehow invented a new name for me. Took me a while to realize my email on file is @ and they picked a name similar to . But every previous account rep at least knew my name. What the actual fuck. reply dole 18 hours agoprevWe’re doing SOMETHING LIFE-CHANGING with AI and you’re GOING to show up to this gmeet in 15 minutes! Love, Oracle reply efitz 16 hours agoprevThis article rings so true. There are lots of ways to make a lot of money selling a service. The best way IMO is to build a service that is easy to integrate and customize, delights your customers, and has a simple pricing model. There should be no surprises in any of these traits. Customers will be loyal because you’ve made their lives easier - you didn’t just “solve their problem”, you solved their problem in a way that doesn’t require them to change anything else about their business to adapt to you. The other way involves minimum viable products, basic features only available at top tier pricing, only have a single way to integrate and no meaningful customization. You make your product a black box that your customers can only escape with Herculean effort and lots of begging on many time consuming phone calls. It seems like startup culture somehow funnels everyone into the second category. reply tptacek 16 hours agoparentIf you're not deeply steeped in startup business model lore, the search query you want here is [product led growth], the very-fashionable trend of go-to-market and business models that admit your preferred way of marketing products. Tailscale, Slack, Notion, and Dropbox (at least, early Dropbox) are all big examples of PLG startups. This model works especially well when your early (pre-series-B go-to-market) is not big companies (say, 1000+ employees). reply dilyevsky 13 hours agoparentprevYour first model works for some products/industries doesn’t work with others where second model works better. Generally, medium/large businesses have long, drawn out processes for buying things coupled with politics and limited pool of folks with purchasing power so your first model will almost never work if you’re selling to those reply tpoacher 12 hours agoprev> After wasting precious moments of my life with an LLM that links me to the same useless documentation, I’ll finally be allowed to email a real person This too is already rapidly becoming utopia-land. reply keybits 10 hours agoprevI find Apple's 'Hide My Email' service invaluable for avoiding the first part of this problem: https://support.apple.com/en-us/105078 Very easy to deactivate an address when I decide I don't want to hear from them anymore. reply selcuka 15 hours agoprevI agree with most of this, except the last part. It is true that CodeSquish probably works better than the paid product, but it only solves 20% of the problems that the paid product does. You are in the lucky group of 5-10% of all people that the paid product targets whose requirements are fully covered with that 20%. For the remaining 90%, it's either the paid product, or another free/open source product which may or may not exist. Obviously there are exceptions to this, but in general, commercial products are bloated for a reason. People really need 20% of the functionality as the saying goes, but everybody needs a different 20%. reply munchler 14 hours agoparentI'm currently looking at purchasing a commercial product because CodeSquish has a fatal error and hasn't been updated in two years because the guy who maintains it has vanished off the face of the earth. reply Symbiote 7 hours agorootparentHave you considered paying a developer to fix the bug? It might be less hassle. reply antonvs 5 hours agorootparentprevWhat kind of software is it in this case? reply thexumaker 18 hours agoprevThis is why I stopped working at companies that build dev tools. Building sales/marketing tools because they love having anything built for them lol reply mhx1138 18 hours agoparentThe sales person does not have any incentive for that. They need their name to be associated with the purchase. reply randall 18 hours agoprevpretty accurate. sales teams are kind of the worst, but buying from the company founders are fun. i wonder if there’s a world coming where the oss and the company become the same person. reply rednafi 11 hours agoprevThis is true in general, and despite not being in a position to make big sales happen, I still get at least 3–4 requests per week for these kinds of calls. I understand that in a world dominated by noise, you have to create more noise to get attention, but my time is limited. Plus, when salespeople ask how I’m using their product, I need to prepare because, most of the time, I don’t remember how I was using it. That project isn’t the center of my world. That said, I still accept calls from people I know in the field. I recently had two calls with Glauber Costa, the founder of Turso, and in the process of scheduling another one with the Pydantic Logfire folks. But none of them are sales reps, and they’re usually fun to talk to, so I’m happy to do it. reply dt3ft 11 hours agoprevHe never considers or mentions donating to that open source tool he went with. Funny. reply bityard 17 hours agoprevOof. If I never have to endure another hard-selling enterprise vendor salescritter, it'll be too soon. Mat has really nailed the agony of the experience here. reply nubinetwork 8 hours agoprevOracle won't stop emailing me, or give me a place to unsubscribe... all because I wanted to try one of their VPSes on a 30-day trial like 5-10 years ago, that I never even used. They called me after the 30 days and I even told them I didn't use it. Yet they still insist on emailing me every 3 weeks about some security policy junk. reply brudgers 17 hours agoprevThe right person for the author’s role is a person who enjoys the process the author describes. Someone who wishes procurement reduced to online documentation and credit cards is going to hate their job if it requires engaging with high touch sales processes. Maybe they need a purchase team partner - the analog of the salesperson in a salesperson-sales engineer team. reply kuschku 15 hours agoparentOn the one hand, business majors are always talking efficiency, optimizing the placement of soap dispensers to shave every possible second off the bathroom break of workers. Yet at the same time, when faced with a complaint about excessive unnecessary sales processes, the solution is to add more people. reply brudgers 11 hours agorootparentMy major was philosophy. reply Hizonner 16 hours agoparentprev... because it's unthinkable that the \"high touch sales processes\", which add zero value and suck for everybody other than whoever's taking home the huge commission, should change. reply Rastonbury 10 hours agorootparentThe author describes the unhappy flow when there is nothing he wants to buy, he should have just stonewalled them or removed himself from the mailing list when the feature he needed is not there. If the feature is there and he want it, the story goes totally differently. But both cases are going to start with a call. The big mistake here is taking the call or giving sales a whiff of your interest given the volume/commission model, a smart salesperson (unfortunately somewhat rare) is going to simply move on, saving time on both sides. A more happy flow exists, author signs up for trial, he can't find or is unsure if feature exists during trial or reading docs, he asks sales/sales engineer where it is, salespeople show him or speak to PM to get him a timeline, later the feature he needs goes into beta and sales reconnects and demos, starts trial using beta. Sales helps draft the internal proposal/implementation plan to convince his boss and the hundreds of others needed for the sale. They buy when it goes GA. reply xlii 10 hours agoprev> Since your mailing list is apparently the only place on Earth to find out if Platform A has added Feature X (because putting release notes somewhere accessible is apparently too hard), … Won’t work for every product, but I’m pushing through with my prototype and that (i.e. release notes) is something to consider. reply icameron 14 hours agoprevEveryone has their price, and mine is a minimum $50 gift card if you want me to have an introductory call with your team. Also accept basketball tickets, steak dinner, or an Apple Watch. I’ll always say we’re considering your product. I am the technical decision maker and report to the CIO. Thank you for the introduction, I’ll reply one more time yo follow up with a reason why we can’t move forward (most of the time) reply redsparrow 4 hours agoprevWhen working as the head of engineering at a start-up, I would get dragged into this process on the sales side. I would sit there cringing at all of the bonkers crap the Sales Engineer or Solutions Architect was spewing at the customer. The senior technical person from the customer would ask me a couple hard questions and I would be pretty honest in my responses. Afterwards our Sales Success Customer Account Manager would be unhappy with me, but at least I wouldn't have to go to another one of those meetings for a while. The deal would go ahead anyway because our management had convinced their management to make the purchase even though what we were offering didn't really meet their needs. They would only need to pay us if we actually delivered so they weren't taking a lot of risk, other than wasting their time, and we got to make a multi-million dollar \"booking.\" In the end I would either get along with their senior technical person pretty well because we could be honest with each other, or else they would despise me for being unable to deliver what my sales people had promised (Hi Sven!). reply Suppafly 15 hours agoprev>“Hey, I saw you were checking out our service. Let me know if you have any questions!” I had this happen for a service we already had implemented at my company. I created a new account just to make sure it was working because we had a few say the site or login was being blocked by our security software. Even after I deleted the new account I was getting emails from them. reply bilinguliar 7 hours agoprevBeing respectful to your SaaS users attention and time proves to be detrimental to your business. The choice is to be deferential and poor, or bold and reach. So far I am poor. reply junto 11 hours agoprevI can’t help but feel like one of the author’s last experiences was with Okta or Auth0. reply UltraSane 16 hours agoprevAt its core sales = harassment reply duxup 17 hours agoprev>I will then endure a series of demos showcasing functionality I’ll never use because I’m only here for one or two specific features. I've sat on a lot of sales calls and I get these feels. But when it comes to selling to people who are NOT the author, managers, execs, decision making architecture astronauts ... that long list of features and functionality really do seem to sell the product. Not that they'll use them... reply bhouston 17 hours agoprevI've been on both sides of this and it is so true. reply mongol 11 hours agoprevPretty hilarious. Both accurate and entertaining. Now I also want to use CodeSquish. reply kookamamie 10 hours agoprevThis same pattern is replicated in LinkedIn, where depending on your position in a company, you get flooded with connection requests to only sell you stuff. I hate it from the bottom of my heart. reply nraynaud 16 hours agoprev10 years after I left a company, new relic still calls me twice a year in the hope of selling something there. reply smashah 15 hours agoprevI scream into the void after reading this post: \"Please give even a 10th of that money you wasted on that SaaS to the maintainer(s) of the OSS alternative, even while you're trialing it internally!\" reply Nezghul 9 hours agoparentWould never happen. I was working for a company which for C++ projects used open source CMake package manager called Hunter. Thousands of packages (ours and third-party) for multiple platforms were causing building and linking problems constantly. Literally every day devops would waste hours on those issues. Many of the problems come from our not-understanding of the build processes and some were Hunter bugs so we were happy to jump to every new release. And one day the Hunter maintainer make announcement that he will abandon the project if someone would not help him financially. I pointed this out to my manager. He said that it's very unfortunate but he would need to talk to his manager who would need to talk to next manager and so on, some finance department would need to allocate funds for that but they would not do that without the approval of some other department and so on. So my manager said we must wait, see what Hunter maintainer would do and if he really abandon the project then we will think what next. Ruslo (Hunter maintaner) abandoned the project so I told my manager that we should simply contact Ruslo and try to hire him as he clearly would be the right person to solve all our building problems. Guess what... My manager would need to contact his manager and so on, and so on, so nothing was done. reply silexia 2 hours agoprevSide note - working at Google once used to be a sign of competence, but I would never hire anyone but has worked there in the last 6 years as all of the products have degraded significantly. reply ninju 14 hours agoprevI just learned that CodeSquish is a real product https://www.codesquish.com/ reply tjpnz 13 hours agoparentBook a call! reply pram 16 hours agoprevRecently my team was doing a PoC on some new software and we connected it to one of our hosted databases. Not even an hour later the vendor for that database pinged me on Slack and was like “hey we saw you connected that thing, you know we have stuff that does that too would LOVE to set up a call” I know thats their job but goddamn, this kind of competitive spying is scummy af lol reply m463 16 hours agoparentYears ago my friend put me as a reference on his resume. I got a call asking about him, but then ending with... are you looking at other opportunities? ...and I realized it was the recruiter, probably not interested in my friend so much as recruiting off the reference list. it's everywhere. reply akudha 11 hours agorootparentI once got a call to recruit me to the company that I was working for! Recruiters are worse than used car salesmen, it is just volume and quantity for them. And I am not even some hot shot engineer, just mediocre run of the mill dev reply LorenDB 5 hours agoprevTo all people providing business services (email security software comes to mind as an example), PLEASE for the love of God provide a self-service portal so I can buy your product without having to talk to your salespeople. reply gigatexal 6 hours agoprevI had an argument on LinkedIn about something data engineering related and the guy literally sent me a DM with his calendly and said pick a time for us to continue the debate. What. The. Actual. $&%#? reply system2 13 hours agoprevWe are an IT company providing services such as VoIP. RingCentral is our preferred VoIP service, they are great but I am about to switch to another provider very soon unless they stop calling me with their Filipino call centers asking for super long calls. I can't even hang up they are extremely aggressive. I told them nearly 20 times not to call me, I know their services, and I can call them if help is needed. No, they must have a call and must find a time. If anyone is reading from RingCentral: you are seriously pushing your customers away with these shitty calls. Just stop harassing your own customers. reply b800h 10 hours agoprevImmediately remove yourself from the list with: \"I'm a European citizen. Please erase me from your list as per the GDPR\" reply riiii 11 hours agoprev> And now, I’m stuck on your mailing list forever. Companies, don't do this. After I've attempted unsubscription, I flag every single email from you as spam. reply datavirtue 13 hours agoprevOMG, this is us. reply kylehotchkiss 18 hours agoprevSalesforce has entered the chat. reply EGreg 14 hours agoprevNot wanting to admit ignorance, I’ll make up a reason on the spot. Later, in the bathroom, I’ll Google CodeSquish and discover it does everything I need, costs nothing, and is 100x more performant—even though it’s maintained by a single recluse who only emerges from their Vermont farm to push code to their self-hosted git repo. I've built this exact open source platform, for \"nearly everything you'll ever need\" for full-stack web apps (from profiles, to access, control, notifications, payments, credits, even videoconferencing and livestreaming). You can build your own Facebook or Twitter pretty quickly. Here it is: https://github.com/Qbix/Platform But I haven't really marketed it, at all. Almost no one on HN has heard of it. Only if someone takes the time to poke around will they be impressed, start to use it, etc. Even more than that, I started a new GitHub project recently as I plan to release v2.0 after many years. So all the stars on the original project are not even on the new one. (The old one is still on my github.) And here is the documentation: https://qbix.com/platform/guide So this is the extreme opposite ... I haven't started trying to market it or sell it to the world or even attract developers to it. But it's there if someone bothers to look. reply EGreg 14 hours agoprevAnd now, I’m stuck on your mailing list forever. I get notifications about all your new releases and launches, which forces me to make a choice every time: • “Obviously, I don’t care about this anymore.” • “But what if they’ve finally added the feature I wanted?” This gives me an idea... how about reaching out to developers and ask them what their experience was, which features they'd like the most, and/or subscribe to when they will be launched, to be the first to try them and give feedback? Even this guy would go for that. reply caseydm 14 hours agoprev [–] \"Hey I got some great news. Got 20 minutes for a call later today?\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "New service trials often result in persistent follow-ups and marketing emails, turning initial outreach into relentless sales pitches.",
      "The sales process is often tedious and time-consuming, especially in large companies, with products frequently not meeting expectations and offering limited support.",
      "Customers often switch to better open-source alternatives, leading to a quiet departure from the original service despite retention efforts."
    ],
    "commentSummary": [
      "Small startup owners face challenges when dealing with large businesses, such as unnecessary demos, price comparisons, and lengthy procurement processes, leading to increased charges for big businesses. - The sales process is often inefficient, involving unnecessary meetings and frustrating tactics, highlighting the need for transparency, straightforward communication, and flexible options. - A successful approach involves creating a service that is easy to integrate, delights customers, and offers a simple pricing model, despite the common issues of unread documentation and unethical sales tactics."
    ],
    "points": 494,
    "commentCount": 169,
    "retryCount": 0,
    "time": 1736637610
  },
  {
    "id": 42671472,
    "title": "Kenney.nl: Free Game Assets",
    "originLink": "https://www.kenney.nl/",
    "originBody": "Our game assets are free because of donations from the community, consider one if you can! Games Tools Assets Starter Kits Donate × Games Tools Assets Starter Kits Support Knowledge Base Donate Twitter × News Thousands of completely free game assets for you to use. Get free assets Or get the all-in-one package Get exclusives By joining our club you'll support the creation of game assets but also get early access to new creations, goodies, entry to the club channel and more! Kenney Club Get started quickly Learning game development can be a daunting task. We've created free and open source Starter Kits covering various game genres that'll help you kickstarter your next game! Starter Kits Let's create together With our tools, you don't need any knowledge of complex software or frameworks to create 3D models. At a very affordable price, you can get instant results that can be used in most game engines. See tools Subscribe to the newsletter I agree to receive email updates and promotions. Subscribe This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Copyright © 2010 - 2025 · All rights reserved Support Knowledge Base Privacy Policy Terms of Service",
    "commentLink": "https://news.ycombinator.com/item?id=42671472",
    "commentBody": "Kenney.nl: Free Game Assets (kenney.nl)439 points by memalign 13 hours agohidepastfavorite36 comments hombre_fatal 4 hours agoI bought the massive Kenney asset bundle on itch.io during a sale: https://kenney.itch.io/kenney-game-assets When I feel like making a small game over the weekend, I can scroll the spritesheets and get ideas just from looking at them. They're so good. And it's a lot more motivating to work on a game when it looks so good from the start instead of using crappy prototype art I built myself. reply cloudking 2 hours agoparentAny tips for launching on Itch? reply eieio 2 hours agorootparentI'm not the parent commenter but: I have uploaded several games to itch (although it's been a while) and a cool thing I found was that even with no followers back when I started making things, a decent number of people played some of my games! Nothing crazy, but I remember being delighted that the second game i ever made[1] got a few thousand plays and some actual comments. My experience is that: * tags matter! For example, a decent number of people seem to be looking for new incremental games and are willing to try new stuff in that category * your primary image, title, and description matter a lot and it's worth spending some time taking a good screenshot (I'm really bad at this) * you can upload a gif instead of just a screenshot to include a little gameplay in your title image and that might help * participating in gamejams can help a lot because you get some guaranteed players (everyone plays each others games and rates them; maybe you get some feedback too) I'm not sure where you are in your gamedev journey (there's a good chance you're way ahead of me and this advice is useless!) but \"just make some stuff and put it on there and see who bites\" was effective and motivating for me early on [1] https://itseieio.itch.io/sisyphus-needs-a-nap reply abetusk 6 hours agoprevKenney is awesome. OpenGameArt.org (OGA) has a lot of libre/free assets (Kenney often posts on OGA): https://opengameart.org/ Itch.io also has many CC0 and CC-BY licensed assets: https://itch.io/game-assets/assets-cc0 https://itch.io/game-assets/assets-cc4-by reply abetusk 6 hours agoparentI forgot to mention that Kenney has a cult following. Reddit often labels him as \"Asset Jesus\" [0]. It looks like there's even a site dedicated to Asset Jesus [1]. [0] https://www.reddit.com/r/ThemeParkitect/comments/o63nbi/who_... [1] https://www.assetjesus.com/ reply hoistbypetard 2 hours agorootparentIt even comes up sometimes in the summary of game jams that are focused on his assets: https://itch.io/jam/kenney-jam-2019 reply lelandfe 3 hours agorootparentprev`` in the source :) reply keyle 11 hours agoprevAmazing that he's still going after all these years. Please support him if you use his assets in your game prototypes!... Because by the time you ship a finished game, it will be too late, you'll be completely broke. reply JTyQZSnP3cQGa8B 8 hours agoparentThanks for the good advice. I’m not a game developer but I bought the big sprites pack because he’s been working on it for a lot of years and deserve some compensation. reply dazzawazza 8 hours agoprevCan I also recommend Kay Lousberg's work as well: https://kaylousberg.com/ Similar business model and usefulness. A lot of free and you can purchase via itch. I've used both to prototype many games in Unreal and have bought all their assets. reply riidom 7 hours agoparenthttps://quaternius.com/index.html also worth mentioning. reply rednab 5 hours agoparentprevAlong the same lines: https://assethunts.itch.io/. reply remram 4 hours agorootparentThe license there is custom and a bit ambiguous: https://assethunts.com/license/ reply rednab 4 hours agorootparentAh, good to point out, yeah. The 'no resale' clause is pretty common for asset packs, but unlike Kenney these definitely aren't CC0. reply soared 4 hours agoparentprevSaving for later thanks reply Mydayyy 8 hours agoprevI bought that pack like 5 or 6 years anymore, I dont even remember. Insane value, even tho all assets are CC0 license, but the amount of work he puts in just made me like to support it. I still sometimes get emails about updates on the pack. One of the rare cases where I feel like did not pay enough for the amount of value. I am not even a professional game dev, just dabble in it in my free time. But having so many cohesive art assets at your disposal makes it a lot of un. reply freeCandy 1 hour agoprevMy favorite fun fact about Kenney is that he made the Armor Games intro https://twitter.com/KenneyNL/status/1362768014895157249 reply spsd 1 hour agoprevKenney.nl is the hero of the Game Development Programming course I taught at university. Access to high quality free assets allowed my students to not worry about being a great artist and focus on the game programming and game development mechanics that I was trying to teach. reply shahzaibmushtaq 1 hour agoprevI developed a 2D game (in Unity) using Kenney's asset for the first time in 2018 and absolutely recommend it to everyone. Still wanted to release a few small games based on the raw ideas I have. reply ericra 12 hours agoprevCool resource. Took me a bit to find it, so for anyone curious: \"All game assets found on the asset page are public domain CC0 licensed...\" There is a small FAQ if you click Support at the page bottom. reply gardenerik 10 hours agoparentIt is also stated on every asset page :) reply oulipo 2 hours agoprevWould someone want to do a cute online tutorial to get people started using WebGL with Wave Function Collapse procedural generation, using a cute open-source asset lib like https://kenney.nl/assets/castle-kit ? reply owlninja 1 hour agoprevThis is always the showstopper for me. I really want to mess around with making a game but I can't make assets or music to save my life. Maybe this will get me going! reply ambyra 5 hours agoprevAre there any good sites like this for game music? reply hipadev23 14 minutes agoparenthttps://luckylionstudios.com has a ton of royalty-free music, it’s not free, but it’s cheap enough it basically is. reply popcar2 4 hours agoparentprevOpenGameArt has a section for music[0], itch.io also has a bunch of free music assets[1], but do check the license before using them. I use these often and there's some surprisingly great tracks there. [0]: https://opengameart.org/art-search-advanced?field_art_type_t... [1]: https://itch.io/game-assets/top-rated/free/tag-music reply zovirl 3 hours agoparentprevI’ve used music from Kevin MacLeod [0] and Heatly Bros [1] in projects. [0] https://incompetech.com/music/royalty-free/music.html [1] https://heatleybros.bandcamp.com/music reply kxrm 12 hours agoprevThis is nice, will allow for some quick prototyping of game ideas without having to build assets first. reply lovegrenoble 10 hours agoprevLots of assets, can't thank you enough! reply Tewboo 12 hours agoprevThanks for sharing this website reply Dwedit 9 hours agoprevWhat if you run these assets through a Stable Diffusion model (img2img) to generate something else? reply jsheard 6 hours agoparentThen you too can join the ranks of people spamming asset stores with low quality AI generated material. You're at least a year late to that party. reply sigseg1v 1 hour agoparentprevI have no clue why someone who wants art as output would do this but you'd probably get some mediocre mishmash out. reply mavamaarten 9 hours agoparentprevYou would get a generic result probably. reply TiredOfLife 6 hours agoparentprevNobody knows. reply erezsh 8 hours agoprev [–] On an unrelated note, does anyone know of a website that uses LLMs to generate assets? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Kenney.nl is renowned for providing high-quality, free game assets that are particularly useful for game development and quick prototyping.",
      "The platform has gained a strong following, earning Kenney the nickname \"Asset Jesus\" for his contributions to the game development community.",
      "Additional resources for free game assets include OpenGameArt.org and Itch.io, while royalty-free music can be found from artists like Kevin MacLeod and Heatly Bros."
    ],
    "points": 439,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1736659070
  },
  {
    "id": 42671512,
    "title": "The Illustrated Guide to a PhD",
    "originLink": "https://matt.might.net/articles/phd-school-in-pictures/?_nospa=true",
    "originBody": "The illustrated guide to a Ph.D. [article index] [email me] [@mattmight] [rss] Every fall, I explain to a fresh batch of Ph.D. students what a Ph.D. is. It's hard to describe it in words. So, I use pictures. Read below for the illustrated guide to a Ph.D. Update: Print version, slides and translations, CC licensing terms. Update: I wrote a follow-up to this 5 years later -- HOWTO: Get tenure. Imagine a circle that contains all of human knowledge: By the time you finish elementary school, you know a little: By the time you finish high school, you know a bit more: With a bachelor's degree, you gain a specialty: A master's degree deepens that specialty: Reading research papers takes you to the edge of human knowledge: Once you're at the boundary, you focus: You push at the boundary for a few years: Until one day, the boundary gives way: And, that dent you've made is called a Ph.D.: Of course, the world looks different to you now: So, don't forget the bigger picture: Keep pushing. There's a bit more below, but I also wrote a follow-up 5 years after the illustrated guide which may be of interest -- HOWTO: Get tenure. Related posts HOWTO: Get tenure. Recommended reading for grad students. What every CS major should know. How to get into grad school. Advice for thesis proposals. Productivity tips for academics. Academic job hunt advice. Successful Ph.D. students: Perseverance, tenacity and cogency. The CRAPL: An open source license for academics. The shape of your problem. If you like these posts, then I recommend the book A PhD Is Not Enough: Get it in print; fund students; save lives By request, a print version of The Illustrated Guide to a Ph.D. is on sale. Click here to preview or buy it. Any and all proceeds will fund graduate students whose work may impact the discovery, diagnosis or treatment of genetic disorders. Any and all proceeds will fund graduate students (and postdocs) doing work in biology that may impact treatment of diseases of cellular metabolism. Update: If you're interested in being that postdoc, get in touch with me! It is available at $6.50 thanks to Hewlett-Packard's high-quality on-demand publishing service, MagCloud. It's sixteen pages, saddle-stitch bound and in full color. It's a good gift for new students, the recently defended and relatives thereof. Why biology? If you zoom in on the boundary of human knowledge in the direction of genetics, there's something just outside humanity's reach: My wife and I chose to start funding these graduate students after we learned that our son has a rare, fatal genetic disorder. It may be too late for my son, but it's not too late for other children. Even one child suffering is one child too many. The only way to end this kind of suffering is science. And, the best way to do science is through graduate students. Update: When I first wrote this post, my son's specific disorder was unknown. Thanks to a scientific advance made in genetic diagnostics--specifically exome sequencing--we were able to isolate the mutations in my son's genome and determine that he is the very first documented case of a new disease: N-glycanase deficiency. One small dent in the circle of knowledge; one giant leap for my son. You can read the full story in a new post: Hunting down my son's killer. License: Creative Commons I receive numerous requests to reproduce this work, and I'm happy to grant them all, subject to three small conditions: Please attribute the original work to me (Matt Might) and link back to this page in your reproduction: http://matt.might.net/articles/phd-school-in-pictures/ as The Illustrated Guide to a Ph.D. When you attribute, please also link my name, Matt Might, to: http://matt.might.net/ And, don't forget the \"Keep pushing,\" at the bottom! This work is licensed under the Creative Commons Attribution-NonCommercial 2.5 License. That means you can share, copy, modify and reproduce this work as long as you attribute the original work to me and link back to it as outlined above. However, you may not sell this work, or use it for commercial purposes. You may only distribute it free of charge. If you're not sure whether your use is a \"commercial purpose,\" please send me an email. If possible, please host the images on your own server instead of linking back to mine. If you use it in a presentation, I'd love to hear feedback. Here's an example attribution that satisfies the legal requirements: Matt Might, a professor in Computer Science at the University of Utah, created The Illustrated Guide to a Ph.D. to explain what a Ph.D. is to new and aspiring graduate students. [Matt has licensed the guide for sharing with special terms under the Creative Commons license.] If it helps, here's the corresponding HTML, which you can modify to suit your site's needs: Matt Might, a professor in Computer Science at the University of Utah, created The Illustrated Guide to a Ph.D. to explain what a Ph.D. is to new and aspiring graduate students. [Matt has licensed the guide for sharing withspecial terms under the Creative Commons license.] And, of course, thank you for sharing! Resources Please let me know if you translate this post into another language. A slideshow version in PDF. French translation (by Sara Mathieu-C.) Urdu translation (by Adnan Masood) Japanese translation (by Kimikazu Kato) UK English \"translation\" (as interpreted by Gabriel Egan) Brazilian Portuguese translation (by Kleber Jacinto) Turkish translation (by Y.Ercan Payidar) Spanish translation (by Ismael Peña-López) Italian translation (by Alessandro Ferrari) Indonesian translation (by Robin Malau) Greek translation (by Anestis Chatzidiakos) Korean translation Norwegian translation (by DION) Russian translation (by Shemra Rizzo) Hungarian translation (by Zoltan Prekopcsak) Chinese translation (by Yifeng Yuan) Simplified Chinese translation by (Sijia Zhao) German translation (by Paul Balzer) Persian translation (by Kian Abbasnejadi) Czech translation (by Lukas Kalous) Slovak translation (by Stano Bustor) Serbian Translation (by Lazar Kovacevic) Swedish Translation Filipino translation (by Andrew Pua) Italian translation (by Marco Fotino) Arabic translation (by Areeb Alowisheq) Vietnamese translation. Polish translation. Dutch translation. Ukranian translation. Uzbek translation. Hebrew translation (by Regev Elya). Belarussian translation. Catalan translation by Sebas Mas. Finnish translation by Joutsiniemi Anssi. Basque Translation by Jesus Romo. [article index] [email me] [@mattmight] [rss] matt.might.net is powered by linodelegal information",
    "commentLink": "https://news.ycombinator.com/item?id=42671512",
    "commentBody": "The Illustrated Guide to a PhD (might.net)343 points by chii 13 hours agohidepastfavorite217 comments mattmight 4 hours agoOriginal author of the guide here. Wonderful to see these little illustrations still making the rounds. I first published them in 2010! To those in the comments who mentioned you are just starting your own PhD: Good luck to you! And, I hope you, like I once did, find a problem that you can fall in love with for a few years. To those just finished: Congratulations! Don’t forget to keep pushing! To those many years out: You have to keep pushing too, but there can be tremendous value in starting all over again by pushing in a different direction. You have no idea what you may find between the tips of two fields. reply gcanyon 3 hours agoparentReading the original post led me to this article on your site: https://matt.might.net/articles/my-sons-killer/#full This is just to say I found it incredibly compelling and moving; I hope mentioning it doesn't make you feel bad. reply mattmight 2 hours agorootparentNothing to feel bad about. Thank you for sharing that too. My son’s life changed my own in profound ways, and even though he died four years ago, he is still changing my life in profound ways. I am always grateful for the reminder and to reconnect with the purpose that his life gave to mine. That post also reminds me that while he was alive, I did the best I could for him under my abilities, and that’s all any parent can do in the end. If you want to know more about his life, I wrote on it here: https://bertrand.might.net/ reply orochimaaru 26 minutes agoparentprev>>> but there can be tremendous value in starting all over again by pushing in a different direction. This rings true for me at this time. Done about 10 years now, never went into academia but direct into industry. Things seem a bit stale, maybe its time to pick and research something new. I've been hesitating on the \"going back to school\" thing. But quantum does show promise, for curiosity and potential rather than immediate impact. reply gtmitchell 3 hours agoparentprevAny advice for PhD dropouts? I spent years and years pushing against that boundary in an obscure corner of my field and it never moved. What little funding I had dried up and I left grad school with a half finished dissertation, no PhD, and giant pile of broken dreams. I'm sure over the years you've known students who have started a PhD and not finished. What (if anything) have you said to them? Do you feel their efforts had any value? reply lapcat 3 hours agorootparentI'm a PhD dropout myself. Serious question: what kind of advice are you looking for exactly? This is not intended as an insult, but it sounds like what you're looking for is not advice but rather consolation, which is natural and understandable given the circumstances. reply azhenley 4 hours agoparentprevYour blog really helped me during the early years of my PhD. Thank you!!! reply mattmight 3 hours agorootparentSo happy it helped! Thank you for the kind words! reply drdude 2 hours agoparentprevMatt thanks for the encouraging words... enjoyed your compiler class and sad that you didn't end up in my PhD committee... done 3 years now but stuck lol. reply skalarproduktr 4 hours agoparentprevThank you, Matt! Loved your guide when I started my own PhD in ye olden days, and I've shown it to a lot of people since then. reply mattmight 3 hours agorootparentHappy to hear that! Thank you for sharing it with others. :) reply fl4tul4 10 hours agoprevYes, I can attest that nowadays, in some fields, research has become a 'game', where: - people torture data until it yields unreproducible results; - people choose venues that maximise their chances of getting published (and pay for publication sometimes, I'm looking at you, APC); - little concern given to excellence, rigour, and impact; - the chase for a 'diploma' from a renowned institute without putting the effort; I could go on and on, but I'll stop now. Perhaps something changes, I am waiting for this to happen for some time now (10y and counting). It's a bad system but that's what we have (at the moment). reply knolan 7 hours agoparentI’ve seen this with a PhD student publishing several rapid fire papers in MDPI journals. They are repeating well understood physics work done 50 years ago using off the shelf commercial simulation software. They don’t cite any papers older than a decade and claim without irony that the work is “significant” while none of their papers are cited. They will go to events where no one is an expert in the field and win prizes for showing lots of pretty pictures but nothing that isn’t already well understood. When I, an expert in the field, tell them they need to produce something novel at their research panels I’m told I’m wrong. When I list all the work they are ripping off I’m told it’s somehow different without explanation. When I question the obvious sloppiness in their work (the simulation data showing major artefacts) they blow up at me screaming and shouting. I’ve never experienced arrogance like this before. It’s shocking. Their supervisors tell me that they are close to firing them but then also celebrate all the publications they are getting. The mind boggles. reply noisy_boy 7 hours agorootparent> When I, an expert in the field, tell them they need to produce something novel at their research panels I’m told I’m wrong. When I list all the work they are ripping off I’m told it’s somehow different without explanation. When I question the obvious sloppiness in their work (the simulation data showing major artefacts) they blow up at me screaming and shouting. At risk of relying totally on assumptions, that wouldn't be a surprising reaction for someone facing first serious criticism after an entire life of probably being unconditionally lauded for their smarts (or the projection of it). When parents push children towards something relentlessly without providing any constructive feedback on account of living their dreams through their children and/or the fear of discouraging the child, any criticism can feel like someone is trying to destroy your life goals. > Their supervisors tell me that they are close to firing them but then also celebrate all the publications they are getting. Probably trying to protect themselves from being in the crosshairs of one of many things that can blow your career apart. reply knolan 6 hours agorootparentThis individual is pretty unique in this regard. I’ve never seen anything like it. Most students will acknowledge that I know the literature and will accept guidance. This person seems to think they know everything but their work is the equivalent of a tutorial case in the commercial software. reply EagnaIonat 1 hour agorootparentI've seen it often as I have had to read peoples thesis when interviewing for job roles. Some from reputable universities. I have no idea how they defended them. reply throwoutway 4 hours agorootparentprevI've experienced this in the corporate world too, when someone is seeking a promotion. Entitlement is becoming a bane reply knolan 3 hours agorootparentYes indeed. I’ve seen similar there too. reply whatever1 3 hours agorootparentprevOk to be fair the original is probably a badly scanned tech report from GE from the 70’s with minimal implementation details. Whoever has tried to implement an obscure physics paper from that age knows how tough it can be. I think there is value revisiting some of this work with our modern toolsets and publishing the code in some public repository. But of course with a clear citation chain, and no pompous lies that a new discovery was made. reply knolan 1 hour agorootparentIt’s a really basic engineering problem that was studied extensively in many studies and we teach it at undergraduate. When I made the point that there is no scientific novelty here they insisted that their PhD was a ‘generic’ one and that means they can continue to run basic simulations according the to the recipe. reply Over2Chars 7 hours agorootparentprevThis isn't new, and academia has been rewarding behavior that wouldn't survive elsewhere for a long time. Maybe it's time we unshackled ourselves from these 'prestigious institutions'? reply knolan 6 hours agorootparentYour anti intellectual bias is showing. There are problems in all domains. I’ve seen plenty of arrogant fools in industry too. They’re using an industry tool to do well trodden industry problems that were solved by academics decades ago. I’m not tolerating his behaviour and I’ve made my views clear to my colleagues. He’s going to burn every bridge possible with this behaviour. reply liontwist 2 hours agorootparentCriticism of university organizations is “anti intellectual bias”? Do you think criticism of religious organizations is “against god”? reply Over2Chars 6 hours agorootparentprevI'm all in favor of intellectuals, it's academic bureaucracies I'm not fond of. I think Socrates was a hoot, and he taught in a cave or something like that. Priests teaching rural peasants to read in their monasteries, and collegial colleges for the public benefit are definitely meritorious. But,I mean, there is enormous corruption going on. How did Ren Youzai get into MIT? He was a body guard. Just because you've married into a billionaire's family MIT says \"hey, send anyone you want in\"? And I'm sure MIT isn't alone in mysteriously average students who not only get in but graduate when linked to massively rich and powerful families. A recent US president comes to mind. Is that anti-intellectual? reply lapcat 4 hours agorootparent> I think Socrates was a hoot, and he taught in a cave or something like that. I'm not sure whether you're joking or serious, but in any case, Socrates didn't teach in a cave, and you're probably referring to Plato's allegory of the cave. The interlocuters and followers of Socrates were mostly the wealthy elite of Athens. reply Over2Chars 4 hours agorootparent@lapcat yeah, he argued in the markets or where ever. I think I was mixing him up with Aristotle, e.g. https://www.ancient-origins.net/history-famous-people/caves-... who had some cave school or something. but there was some jokingness, yah. But I'm not anti-intellectual, which wasn't a joke. I wasn't making any point about his students or wealth. Education then, as now, is the plaything of the wealthy and wealthy nations. reply barbazoo 2 hours agorootparentIn what way is education a plaything for the wealthy now? reply knolan 5 hours agorootparentprevYou’re arguing about highly specific cases while the vast majority of institutions get on with the job of educating large numbers of students and doing what research they can. reply Over2Chars 5 hours agorootparentThe highly specific cases are glaring examples that the unbiased meritocracy they pretend to be is, possibly, not so. And the \"large numbers of students\" covers up the possible cronyism and/or corruption of the institution. I provide an example of a totally unqualified individual being allowed into a prestigious institution solely on the basis of his marriage family. Your response is that they mostly do a good job for most people? I've suggested that the research they do is not obviously beneficial to anyone except perhaps the person doing the research, possibly simply to advance their own careers (in or out of academia). Others have suggested the same. You haven't disagreed. reply knolan 3 hours agorootparentUgh you’re a tiresome culture war poster. I’ve no Interest in you or your hobby horse. reply 3abiton 9 hours agoparentprev> - people choose venues that maximises their chances of getting published (and pay for publication sometimes, I'm looking at you, APC); > > - little concern given to excellence, rigour, and impact; It's because the kpis of assessment are built like this. Goodhard's law. I know lots of good researchers who get frustrated with the system and end up giving up and faltering to those 2 points. If within a uni 2 research groups are putting out research at different rate at different quality, the one with higher quality, lower frequency, and higher standard and ambition gets heavily penalized. Seen it in action. reply eloisant 43 minutes agorootparentYep. I also know researchers who refuse to play this game, but their career plateau'd and they have to work with little fundings. reply graycat 6 hours agorootparentprevYes, in the STEM fields, for published papers, it's easy just to count them; much harder to read them; to evaluate them, some people just count awards, etc. So, the hard work that makes the good material in the paper may never be noticed. There is, \"You get what you pay for.\" So, want papers, you will get papers, and you can count them. It goes, did Haydn write 101 symphonies or 1 symphony 101 times? Early on, had a good career going in computing but where occasionally some math made a lot of difference. So, to help that career got a Ph.D. in applied math. Never had any intention of being a professor but for a while did to try to help my seriously, fatally, stressed out wife (Valedictorian, PBK, Woodrow Wilson, NSF, Summa Cum Laude, Ph.D.) -- took a professor job near her family home and farm. In my little Ph.D department, saw the Chair and four professors get fired and one more leave, fired or not. The career I had before grad school was a lot better than the one those professors had. Had to conclude that, tenure or not, being a professor is, on average, a poor way to even reasonable financial security. Generally there is low pay, e.g., too little to buy a house, keep cars running, support a wife and family. There's a LOT of dirty politics, infighting, higher-ups who don't want you to be successful. Bluntly, a research university takes in money that a lot of people care about and puts out papers that only a few people care about: Net, there is no very strong reason to pay professors enough for even reasonable financial security. Key sources of the money are short term grants from the usual suspects, NSF, DOE, DARPA, NIH (\"too many for them all to be turned off at once\" -- JB Conant?), but that is essentially just contract work and not steady employment, with little promise that when a Professor's baby is ready for college there will be money enough for them to go. It's a house built on paper that can be blown away by any thunder storm. Now, for a career, e.g., financial security, to leave something for the kids in the family tree, regard business, e.g., now involving the Internet, as the best approach, and there regard computing and math as important tools but only tools. Research? Did some, and it is a key to the business. Academic research? Did some, published, on my own dime, still waiting for the checks. History, how'd we get here? Used to be that some guy built a valuable business and had several sons. One of the sons inherited the business, and the rest went to the military as an officer, academics as a professor, or to politics. Then WWII showed that the STEM fields can be crucial for national security, and some related funding started, e.g., summer math programs for selected high school students, research grants. \"If you are so smart, why aren't you rich?\" Ah, \"The business of America is business.\" reply y-curious 2 hours agoparentprevChiming in with my own experience. I was with a new PI that was a charlatan; I am grateful for the experience because I now know how to recognize these sorts of people and avoid them. \"There is no journal of negative results.\" he would say at our weekly meetings. In order to secure his future, he set ablaze the dreams of 5 PhDs in my lab (all of which took their masters and went into industry; One developed severe OCD). Data was massaged, lies were told to his bosses. Guess what? He's still a professor there, his lab still publishes dubious, unreproducible research. No recourse was to be had at the university (all of the PhDs went to the head of the department and were told to f*ck off). Academia is on a death spiral at many schools, and I worry that it's up to the industry to carry the torch of research in the future. reply enum 6 hours agoparentprevAny evidence that this approach works? Are people who do this able to move from the PhD to a solid position afterward that they could not have had without the PhD? reply carlosjobim 6 hours agoparentprev> Perhaps something changes, I am waiting for this to happen for some time now (10y and counting). What will change is that PhD will become an inherited title. If your parents were/are PhDs, you will ceremoniously be granted the title when coming of age. That title can then be rented out to people or organizations (such as companies) who are required to have a PhD by government regulation for the activity that they are in. You can of course also mortgage this title to a bank or other company that will take care of the process. reply antegamisou 9 hours agoparentprevYes it's horrifying indeed. It wouldn't be a stretch to say that there are many that fall into this rat race and treat it as another type of MSc credential. reply xanderlewis 7 hours agoprevI'm starting a PhD — essentially from tomorrow. It's a shame to see so much discouragement here, but at this point I'm no longer surprised. I also don't care because if left to my own devices I would do research anyway. In the kindest possible way: screw all of you! reply samantha-wiki 5 hours agoparent> if left to my own devices I would do research anyway. Then you're going to have a great time during your PhD, good luck and have fun! > screw all of you! \"Disregard!\" https://stepsandleaps.wordpress.com/2017/10/17/feynmans-brea... reply megaloblasto 7 hours agoparentprevMost of these people have never been in a PhD program, so take their words with a grain of salt. If you have a good advisor, your passionate about your project, and you got some good funding, you'll have a wonderful time of exploring interesting ideas and becoming a competent researcher. Good luck! reply liontwist 2 hours agorootparentAll that’s true, and that is completely different than what’s advertised in this article. reply xanderlewis 6 hours agorootparentprevSo far, I seem to have all three. Let's see how it goes! Thanks. reply analog31 3 hours agoparentprev\"PhD Envy\" is a real part of office politics outside of academia. Remember that the naysayers are just jockeying for their own status. On the one hand, you can ignore it. On the other, learning to manage it is a good starting point to navigating the social and political side of any career. Also, this is HN, which revolves around an occupation -- computer programming -- that is unique in terms of having high demand while remaining flexible about how and where people learn their skills. Not all fields are that way. I got a PhD in physics, in 1993, and have worked in industry since then. There are a couple of \"negatives\" that I still think are wroth pointing out: 1. PhD programs have very high attrition, and you bear most of the risk on your own shoulders. It's worth going in with eyes open, and knowing the risks. Getting out with your PhD may require some compromises along the way. I won't necessarily call them ethical compromises, but perhaps compromises to the (typically) idealistic views that many students start out with. 2. The little nub of specialized knowledge shown in TFA is your research, not your brain. You can do specialized research without becoming a specialized person if you want. This is a personal choice (academic freedom and all that). My dad, who also had a PhD and a good industry career, always told me to avoid hyper-specialization. Don't forget to learn how to code, just in case. ;-) reply liontwist 2 hours agorootparent> PhD programs have very high attrition, and you bear most of the risk on your own shoulders. How many people do you know who “failed to meet the standard”? Zero. If you do the time and work for your professors you will get the reward. There is no risk. > PhD Envy\" is a real part of office politics The most vocal critics are not bachelor degree holders, but those who did it and had a bad experience. reply analog31 35 minutes agorootparent>>> If you do the time and work for your professors you will get the reward. There is no risk. 1. Your experiment fails to produce a result after a few years of effort (my project, we don't know to this day what went wrong, and I was lucky to find a new project). 2. Loss of funding or institutional support. A large program at my state university pulled its support for a process that required regulatory approval, and an entire group of faculty and students all had to leave. 3. Your advisor quits, changes jobs, gets fired, goes to prison, dies. 4. Your advisor holds your thesis hostage until you publish a certain number of articles (a friend of mine, she sued and won). 5. Mental health issues (high incidence of clinical depression). 6. Personal animosity between members of your committee (another friend). How these risks instantiate themselves is that you have to start from scratch, often with a completely new research project, and finding one isn't guaranteed by your department. You are almost completely at the mercy of one person -- your advisor. There is virtually no oversight. reply eloisant 37 minutes agorootparentprevI wouldn't say it's about \"failing to meet the standard\". Sure, there is no exit exam you can fail, but there are still people dropping out of a PhD. It could be because you realize you don't really like research - that involves reading and writing a lot of papers, going to conferences not just tinkering. It could be because you had the wrong professor who failed to lead you and left you by yourself. It could be because you gave up at a low point, where most PhD student go through. It could be because after 4 or 5 years your professor keep saying \"you're not ready yet\" (I've seen that in humanities). So it's not really a problem of \"not being good enough\", but it definitely happens. reply skalarproduktr 5 hours agoparentprevPlease consider that not everyone has the luxury of having (had) a PhD advisor who really cares. There's a wide spectrum, ranging from micromanagers, to people you see once during your PhD, to advisors that are genuinely great (intellectually and as a person) and caring. I wish you the best of luck for your PhD, a caring and supportive advisor, and great results! reply foobarbecue 4 hours agoparentprevI did my PhD (mapping and studying the physics of caves in the ice on an antarctic volcano) purely for fun, and it was awesome. I hope you have a similarly rewarding experience. You will encounter unfair systems and unscrupulous people, and it will be frustrating. The data will be confusing as hell. My only advice is stay true to yourself. Maybe look into some of the new trends that could fix academia -- pre-registration, open access with public comment periods, reproducible code, etc. For inspiration, I cheer for crusaders like Data Colada who are trying to save the academic system. reply richrichie 2 hours agorootparentThat is the way. Not long ago, most of Science was done by rich people as a hobby. We need to get back to that system. reply eloisant 36 minutes agorootparentCan't wait to read Elon Musk's Ph.D. on pointy rockets! reply yodsanklai 7 hours agoparentprevI don't think there's any reason to be discouraged. There's a lot of bias against PhDs for various reasons (good and bad). I have a PhD, got an academic position and then worked in various companies (startups, big tech company). These paths aren't exclusive. I'm glad I did the PhD. - it gave me time to work on a variety of interesting topics. In my company, I always feel rushed and don't have time to learn as much as I'd like to. - I had more than one career. Working only in industry after graduation would have been pretty sad I think. Not that it's bad but it's great to see something different - I developed some skills (for instance talking in front of audience, write scientific papers) and got to meet a lot of interesting people, and worked in different countries. I also learned that research wasn't for me but it was worth doing the PhD anyway. If I had to do it again, I would pick my topic more carefully, and go straight to industry rather than pursuing an academic position (which I actually didn't like). Also money wise, even though I'm not materialistic, the pay was too low. Certainly enough to live, but not enough to secure my future and retirement. reply fastneutron 5 hours agoparentprevDisregard them. A lot of people fixate on the 1 in a billion celebrity exceptions like Musk, Thiel, Gates, Dyson, et al and go “look look you don’t need a PhD!” Yes, a highly motivated college dropout with a computer, a strong financial safety net, and the right social connections can be in the right place at the right time to seize big opportunities. Most people are not in that position. Many high-impact technologies need more than what just a computer can do. The main thing is to be self aware enough to know the path you’re on, what paths are available to you, and how to make the most of the connections and resources you have available to you. The second you start to get pigeonholed, wrap things up and move on. reply xanderlewis 5 hours agorootparent> The second you start to get pigeonholed... That seems like good advice. reply etrautmann 4 hours agorootparentYes! Be very aware of your time and opportunity costs. It can be an amazing journey, struggles and all, but make sure to not get stuck long-hauling on something you’re not passionate about. reply j_maffe 7 hours agoparentprevI also started my PhD last week and honestly from my talks with the people there thus far I'm much more optimistic than the general HN view of PhDs. You still have to be realistic however. Best of luck! reply xanderlewis 6 hours agorootparentGreat! The view of almost anything hard is gloomy online, probably because the conversation is dominated by those who either wish they'd tried and now have a chip on their shoulder or who made the wrong choice for them and are self-therapising by writing about it. Those who thrived in a PhD programme likely don't have a reason to bang on about their experience in quite so many words. > You still have to be realistic I'm expecting it to be very challenging. But that's the point — isn't it? Good luck to you too. reply CaptainFever 6 hours agoparentprevThis is the best possible answer to Internet-based negativity! I wish you luck in your PhD :) reply JTyQZSnP3cQGa8B 5 hours agoparentprevI've worked with PhDs all my life in various industries and it has always been a fun and enjoyable partnership. Have fun. reply Over2Chars 7 hours agoparentprev@xanderlewis I don't think anyone's comments are meant for you specifically, nor to discourage you or anyone particularly. If anything, take it all with a \"grain of salt\" and reflect on whether or not anyone you meet might resemble these comments. Hopefully, not your future self. Good luck in your career. reply austinjp 6 hours agoparentprevCare to share any details? What country are you studying in, and what's the subject area? reply xanderlewis 6 hours agorootparentUK; more specifically Scotland. And mathematics; more specifically (algebraic) topology and (differential) geometry. reply vaylian 6 hours agorootparentThe nice thing about mathematics is that there probably won't be any failed or non-reproducible experiments in the lab. That doesn't mean that a math PhD is going to be easy, but you should be aware that a lot of people will have a different idea of what you are doing if you don't tell them that your PhD is in math. Best of luck for your PhD! You might want to check out this ted talk: https://www.ted.com/talks/uri_alon_why_science_demands_a_lea... reply xanderlewis 5 hours agorootparentThanks! I hope so. Experiments (to the extent that there are experiments in mathematics, which arguably there very much are) often fail, but once they succeed they're usually fairly bulletproof, and reproducibility is barely even a concept. reply atlintots 1 hour agorootparentprevIf time and money weren't concerns, I would love to do mathematics research! I hope you achieve good things, and have fun while at it! reply leviliebvin 3 hours agorootparentprevThere is literally an ocean of difference between mathematics and ML (which seems to be what a lot of comments are talking about). reply xanderlewis 2 hours agorootparentYes. Which is why I'm trying to push back a bit and say 'hang on... none of this is intrinsic to the PhD system'. Of course it's true that some PhDs — hell, some disciplines — are built to an embarrassing degree on BS and academic schmoozery, but there's no need to tar everyone with the same brush. It seems as though some commenters have difficulty conceiving of intellectual pursuits that don't involve 'data' and 'graphs'. I'm only very junior, though, so I don't have total confidence that I'm right. But I'm pretty certain I am. reply richrichie 2 hours agorootparentprevI hope you have decent source of secondary income or your family is reasonably well off. A math PhD might take 6-7 years to complete and I hope that, at the end of it all, you won’t have to come to London to look for C++ or Ocaml jobs at hedge funds or banks. reply xanderlewis 2 hours agorootparentI'm in the UK. It takes nominally three years here; usually three and a half. I also have full funding. ...this is the discouraging negativity I'm talking about. I do, respectfully, wonder what your agenda is. reply j_bum 2 hours agoparentprevGood luck with your PhD. Stay focused and enjoy this time, you will have a freedom to explore and study that’s almost unparalleled in post-grad life! reply richrichie 2 hours agoparentprevPhD now is an expensive way to signal that you can persevere longer than the average human. Unfortunately, that is not always a positive as many real life situations require you to make decisions under extreme paucity of information and reverse or change course at short notice. For such professions and roles it is a liability. reply xanderlewis 2 hours agorootparentI can see why you might think that; in some cases I'd agree. But there are parts of science — indeed of human knowledge in general — that are very difficult to break into if you don't have the opportunity a PhD affords. These disciplines also require perseverance longer than the average human. Without this system, we're not going to make fundamental progress. I'm pretty sure that without the research done by people with PhDs and people who don't give up at the first hurdle, we wouldn't be able to be sitting at our keyboards now having this conversation. Of course, it's not for everyone. Maybe it's not for most. But I don't think you should write all of it off as 'signalling'. Some research simply cannot be done without several years of focus, outside of industry or 'the real world'. reply mattmight 4 hours agoparentprevGood for you! reply bowsamic 7 hours agoparentprevI just left academia (after one three-year postdoc). Good riddance. For myself at least. I do think one can thrive in it, if you are a good salesman, don’t mind sucking up to those with money with lies and exaggerations, and don’t mind isolation reply xanderlewis 6 hours agorootparentWhat kind of area were you in? It's hard to believe this 'sucking up to those with money' thing applies everywhere, though it's easier to imagine it applies in certain domains. reply owlbite 5 hours agorootparentI did a PhD and about a decade in postdoc/early-career researcher posts before moving into tech. That was in Computational Mathematics, it was clear that the most successful people in the field were the ones who had found areas where they could publish \"Technique X applied to field Y\" type papers, so for each new X they could get 10 publications (by way of 10 different PhD students). These people generally could steer the core funding in the discipline their way. Everyone else basically had to reformulate their research to pretend it was applicable to the government's funding subject de jure. This led to some quite large stretches in definition to achieve \", and some applications in \". This very much felt like it was sucking up to money. I got out of the academia in the end because it felt like the more senior I got the more time I spent applying for funding and managing the spending and the less time I spent doing research/development. (Also given I was in a UK public sector institute, the pay was shit due to 40 years of below inflation pay rises crippling the institution). reply bowsamic 2 hours agorootparentYeah that’s the thing. Not only do you have to beg for money but actually you make fake papers to do so. My supervisor taught me this early on, every single tiny discovery or synthesis can be made into a paper even if it really doesn’t warrant it I left because the only path forward here in Germany is to become a professor, aka a life full of admin and sales reply liontwist 2 hours agorootparentprevWhy would a field be immune to political patterns found in every organization? reply bowsamic 6 hours agorootparentprevQuantum physics reply xanderlewis 6 hours agorootparentCould you be more specific about the pressure you felt to 'suck up' and 'lie'? I read this kind of thing often, but it's usually left quite vague. What exactly are people lying about? Physics (since it's supposed to be rigorous) seems like a less likely area than some to be driven by politics and trends, but I suppose I can imagine that competing research programmes and ideas benefit from a certain amount of marketing and smooth-talking of people with funding rather than relying purely on empirical evidence for their claims. reply bowsamic 6 hours agorootparentPhysics absolutely is dominated by politics and trends. I was constantly expected to over exaggerate the impacts of my work and its possible applications, on both science in general and wider technology. For example, we used to always say that it had some impact in quantum computing even if it was a total lie, because that makes it way easier to get funding Physics may be in some sense more falsifiable, but it is absolutely subject to politics and social norms, both in how it lies about itself for money, and literally in which theories are chosen (since we can rarely empirically distinguish between them) reply xanderlewis 5 hours agorootparentRight. That's a shame, but it seems like that's just life. The idea that it helps to constantly emphasise (or, as you say, exaggerate) the importance of your work if you want to have a career is certainly not restricted to academia. It seems to apply to most industry engineering jobs too — from what I've heard. I guess in that context funding isn't the issue, but being promoted (and, conversely, not being sacked) certainly is. reply bowsamic 2 hours agorootparentWell in most engineering there is a market and going out into the market and selling is not the job of the engineers Academia is rare for having the engineer also be the salesman My wife is the lead mechanical engineer at a small company and she definitely doesn’t have to go around convincing customers they need her products reply monadINtop 6 hours agoparentprevGood luck reply InkCanon 9 hours agoprevI'm considering getting a masters or PhD (in PL) under a professor I work with now for my undergrad thesis. It has been my observation that the standard path of getting a standard corporate job tends to nullify all impact you could have (with a few rare exceptions). And after that I could get a job, become a professor, turn my research into a startup etc. The pros are 1) I know my professor and he's a solid guy 2) Pays decently well, money isn't too much of a concern 3) I get paid to do research, university provides generous grants if turned into a startup Cons 1) Hear a lot of bad things about the academic rat race, pressure to public even at masters/PhD level 2) I could probably hack out some paper into journals but whether I could have any real impact \"on demand\" (versus say spontaneously coming up with something) is a big question mark, especially within the deadlines given in the program Any thoughts on this? Especially heuristics, methods or ways to increase impact? reply noelwelsh 8 hours agoparentUnderstand that a PhD is an apprenticeship to become a researcher. You are not expected to do career defining work as a PhD student, and indeed that is unlikely. Your relationship with your advisor is very important. It seems like you already have that sorted out. Most successful PhDs (in CS) involve tackling a relatlively small and easy project, usually suggested by your advisor, early on, and then expanding and iterating on this. Once you make some progress on a topic you'll easily find more directions to take it. Working with other people is one of the easiest ways to increase your productivity. All the great groups I saw had a lot of collaboration. Don't fall into the \"lone scholar locked in the library\" stereotype. Avoid bad people. Avoid getting stuck in your own head. Realize a PhD is a project like many others. It doesn't define you. You start it, you work consistently on it, you finish it. Doing a research Masters is usually a waste of time. Doing a taught Masters is a lot of fun, but something quite different to a PhD. reply InkCanon 7 hours agorootparentThanks for the reply! >A PhD is an apprenticeship to become a researcher That's a good way to look at it. I suspect one of the biggest possible benefits of a PhD is that you're put in an environment structured to and pressuring you to develop something new, which is the opposite of most other human work. >Start a relatively small and easy project and collaborate Sound advice, it's the general approach I've taken for my undergrad thesis. >A research masters is a waste of time, a coursework masters isn't Really? It looks the opposite to me. A research masters let's you collaborate with different people and work on new things. A coursework masters is taking advanced classes. reply noelwelsh 6 hours agorootparentAt least in the UK, a PhD takes one year more than a MRes and it lets you become a university lecturer. It also should be funded, whilst you might have to pay your own way for an MRes. Hence I don't see the point of doing an MRes when you can stay an extra year and have more opportunities afterwards. MRes are usually a consolation prize for people who drop out of their PhD, IME. reply InkCanon 28 minutes agorootparentIn my side of the world I think it's more similar to the USA where a masters is two years and a PhD is four. And it's fifty fifty whether a PhD has a masters or comes straight from undergrad. I'm leaning towards masters because I don't particularly care about the prestige and I don't want to over commit. I don't think the title is important to the impact I have reply hiAndrewQuinn 8 hours agoparentprev>getting a standard corporate job tends to nullify all impact you could have It's very strange to me that you think other people would pay you millions or tens of millions of dollars over an average 30- or 40-year career, without you generating at least that amount of value back to the external world as a whole, and probably generating some huge multiple more, and yet all that counts as \"no impact\" to you. Especially when your comparison point isn't oncology or something, but doing research in PL theory of all things. But I thank you for giving me the opportunity to get a little riled up on a lazy Sunday morning, it's one of my favorite hobbies. My recommendation to you for \"increasing [overall] impact\" is to read https://80000hours.org/ and follow their advice, and for \"increasing impact [in this niche I really care about]\" is simply to be more bounded with your claims. reply InkCanon 7 hours agorootparent>people would pay millions over a career without you generating at least that value back Some of it is empirical observation. I've seen many friends at big/elite tech firms get paid to do very little. Many claims online to that effect, although I weigh it lesser. And I think it's completely plausible. I think because of the exponential advancement of technology, huge accrual of capital and inability of human incentive structures to keep up, value does not universally equal money. IMO many examples. Many people are tech firms do things that are very loosely related to revenue generation - so you can almost double your headcount during COVID, fire tons of them and still function the same (a substantial amount of hiring and firing was tech companies FOMOing about each othe). Meta's VR division has burned through $50 billion, but it's people got paid incredible salaries. One in three Nvidia employees are now worth over 20 million. Many of them were working decent jobs making GPUs for video games and suddenly because of AI, their net worth went up 100x. Oncology is another possible example. By far the wealthy people today are all in computers, instead of curing cancer. I'm not saying these people are bad or anything like that. The other part of the equation, wealth as a signal, has become incredibly noisy. In some areas it is still a strong signal, typically smaller companies and startups where providing value is a lot more closely related to what you make. And conversely, I don't agree with money generated being a signal of impact in itself. reply hiAndrewQuinn 4 hours agorootparent>I've seen many friends at big/elite tech firms get paid to do very little. What matters is the outcome, not the amount of effort one puts in. If you're working at e.g. Google for $200,000 a year, your changes can affect millions to billions of people. At that scale even a small improvement like making Google Sheets load 1% faster can equate to millions of dollars of additional revenue downstream -- and likely tens of millions of dollars of actual value, since the largest tech companies actually capture only a low percentage of the value they create for their consumers. You've just justified that $200k several times over for what might amount to two or three day's worth of effort for you, that's true. That's not a bug - that's a feature of working in a successful, scalable business. If you're inclined to do more than this \"bare minimum\" which you observe so many doing, just imagine how much value you could create for others if you actually worked close to your capacity in such a place. >[B]ecause of the exponential advancement of technology, huge accrual of capital and inability of human incentive structures to keep up, value does not universally equal money. I don't understand the thread of logic here. Claiming that human incentive structures are \"unable to keep up\" with value creation suggests to me that money is, if anything, a heavily lagging indicator of the real value one is generating, which is in line with the point above. But I don't think that is the point you are trying to make. >Meta's VR division has burned through $50 billion, but it's people got paid incredible salaries. Most company actions are bets that the company's leadership think are net positive. Sometimes those bets don't pan out the way we expect them to - that's normal. Your own research might take longer than you expect it to, but that in itself isn't a reason to look back and say you made a bad bet. As for the people, yes, you generally have to pay a lot to get top talent, and even that doesn't assure you of success. That's probably 2-4 years, out of a 30- or 40-year career, where their contributions may have been net negative to the bottom line. Maybe. If we include caveats like \"Meta VR never becomes profitable in the future, either\" and \"none of the innovations from Meta VR turn out to be profit-generating via a different, unexpected mechanism\". This probably equalizes out over the course of a career for the vast majority of these engineers. Not exactly a ship sinker. >One in three Nvidia employees are now worth over 20 million. Many of them were working decent jobs making GPUs for video games and suddenly because of AI, their net worth went up 100x. AI is hugely, hugely useful for all kinds of people. I use it every day both professionally and personally. Almost everyone I know does the same. If you truly derive no value at all from it, you are decidedly in the minority. Is the claim here that they shouldn't have made money off of helping to manufacture the hardware that enables this invention which so many have found so enormously useful? Or maybe it's that since they never intended for their hardware to be useful for such a thing, their involvement should be worth less. That sounds way more like trying to invent a human incentive structure that can't keep up with the exponential advancement of technology than what we actually have. The current incentive structure, however, is wonderfully open to serendipity like this. >The other part of the equation, wealth as a signal, has become incredibly noisy. You've just given two examples where one company's wealth fell up to $50b because they made a bet on something that (for now) nobody wants, and another company's wealth went so high that a plurality of their employees are now millionaires because they made something everyone wants. That doesn't sound like a low signal-to-noise ratio to me. reply InkCanon 2 hours agorootparent>What matters is the outcome, not the effort >At certain companies the scale could be enormous The latter is true and I think the most legitimate reason for working at big companies. I should specify in the first they also accomplish little and affect very little. Things like internal tools that went nowhere, running basic ETL scripts, things like updating financial trade systems to comply with some new regulation. And this at a pretty slow pace. My meaning about Nvidia and Meta VR is how people who didn't create value got enormously wealthy anyway. In Nvidia's case, traditional GPU teams (which I suspect received most of the benefit because they've vested the longest and made up most of Nvidia's pre AI boom) got hugely rewarded by data center GPUs, which they played little role in. Conversely Meta's VR team still got paid really well (their stock is even up because of AI hype, despite VR losses) despite their failure. So you have these systems where even if you fail or don't play any role in success, you're still paid enormously well. This is because companies capture the value, then distribute in their very imperfect ways. You're right that the valid reason for this is that tech companies act as risk absorbing entities by paying people to take high risk bets. But the necessary condition for these are 1) Hiring really good people (not just intelligent, but really motivated, curious, visionary etc) 2) A culture which promotes that The on the ground reality of 1) is that it's a huge mess. The system has turned into a giant game. There are entire paid courses advertised to get a job in MAANG. The average entrant to MAANG spends six to eight months somersaulting through leetcode questions, making LinkedIn/Twitter/YouTube clones, doing interview prep, etc etc. Many causes for this, including the bureaucratization of tech companies, massive supply of fresh grads, global economic disparities, etc. It's no longer the days when nerds, hackers and other thinkers drifted together. 2) Because of FOMO, AI hype and frankly general lack of vision from many tech CEOs, it's just a mess. Anything AI is thrown piles of money at (hence the slew of ridiculous AI products). Everything else is under heavy pressure to AI-ify. I've heard from people inside Google has really ended that kind of research culture that produced all the great inventions. There are still great people and teams but increasingly isolated. reply sega_sai 8 hours agoparentprevAs a professor many years after the PhD my advice is to do the PhD only if you are genuinely excited / cannot stop yourself from doing research. Only then it will outweigh the negatives of difficulties of getting the jobs, somewhat low pay etc. At least from my point of view I always tried to work on what was interesting to me and what I was good at/or it was interesting to learn vs optimising what is more high profile/sexy. I don't think it is a universal advice but at least I always enjoyed what I did reply skalarproduktr 5 hours agorootparentI can only second this after having advised a few students from bachelor to PhD level. The ones who do well are (usually) the ones who are genuinely excited. Not only about the thing they're doing, but in general. It really helps getting over the lows. Furthermore, do not underestimate the importance of sheer luck. Exaggerating a bit, deep learning was just another subfield of ML, until GPU-powered DL really took off and made the researches behind the most fundamental ideas superstars. This is not a given, and it might take years or decades until it's really clear whether you're making an impact or not. I wish you the best of luck, InkCanon, and stay excited! reply InkCanon 6 hours agorootparentprevWhat do you mean by cannot stop doing research? I certainly haven't discovered anything new, but I love learning new things, reading about ideas, coming up with them. reply sega_sai 4 hours agorootparentI meant that you tend to spend free time on that as opposed to treating it like 9 to 5 job. and again it is important that if you do that, it is because you just want to see what comes out of your experiment/learn a new thing etc rather than because you have to publish or is forced by your advisor. reply InkCanon 23 minutes agorootparentI see. I think I definitely lean towards that. reply enum 6 hours agorootparentprevYou'll do great. This will eventually turn into new discoveries if you keep at it. reply InkCanon 24 minutes agorootparentThat is the hope! reply lordnacho 5 hours agorootparentprevWhy not just study a bunch of different things to Master's level then? Learning something genuinely new seems like it has a much lower return to effort. reply InkCanon 24 minutes agorootparentGood question. IMO 1) There's a kind of \"hard\" learning you're learning a fixed, structured way from a textbook. 2) There's a kind of \"soft\" learning which is transmission of knowledge, which happens a lot more face to face when you're working together. 3) Then there's a kind of research learning, where you're doing something new, usually with collaborators. The second and third are really best done in certain environments like research or good companies reply EvgeniyZh 8 hours agoparentprev> Hear a lot of bad things about the academic rat race, pressure to public even at masters/PhD level Strongly depends on the advisor and your goals. If you want to stay in academia, some amount of publications is required. Your advisor, especially if he pays your salary, may also push you to publish. If both are not an issue, I guess you can even finish without publications. > I could probably hack out some paper into journals but whether I could have any real impact \"on demand\" (versus say spontaneously coming up with something) is a big question mark, especially within the deadlines given in the program Nobody comes up with good ideas on demand. As you progress in your academic career the rate of ideas (theoretically) grows. That's why you need the advisor: he can come up with ideas at rate sufficient for his students reply InkCanon 6 hours agorootparent>advisor might push to publish That's fair. I'm just cautiously eyeing the likelihood of coming up with something publishable that's not a going through the motions kind of thing. reply Eduard 8 hours agoparentprev> The main reason being getting a standard corporate job tends to nullify all impact you could have (with a few rare exceptions). \"Impact\" is an ambiguous term, so it's quite vague what you mean. I assume \"positive impact on the world and knowledge\". While this mantra is indeed motivational, it can set you up for disappointment, both in corporate as well as research/PhD settings, at the moment you realize how many hurdles there can be (toxic colleagues, bureaucracy, ignorance, etc.). Also, for this interpretation of \"impact\", a corporate job can be very impactful as well. reply InkCanon 6 hours agorootparent>impact is ambiguous This is the core of the issue (most replies usually involve some slightly different definitions). I take many definitions of impact, including societal use, contributing to knowledge, etc. But it's much clearer there are many things people do that are low impact, especially in places with a lot of bureaucracy, politics etc. A corporate job can, but it seems to me as a result of various incentives corporate jobs tend to be compartmentalized, low impact and repetitive. We're also at a down cycle where tech, the historical haven for impact in a job, is scaling back a ton of things to focus on stock prices. If you know of any corporate jobs that do have impact by some definition of it, I'd love to hear it. In my experience these have been mostly startups. reply sashank_1509 8 hours agoparentprevAsk yourself this, has there been any useful Programming Language that has come out of PL research/ Academia in the last 20 years? The only example I can think of is Julia, and it only seems to be used by other academics. If you’re looking to be impactful, you are much better off joining a job and working in your free time, than doing a PhD. A PhD is a program to compete for academic prestige. Grad students want to publish papers that get them noticed at conferences, invited to talks at prestigious universities etc, those are the incentives, always has been in academia. The brightest minds join academia because they care more about prestige than money (as they should, anyone can earn money, few can win a Nobel prize). In a healthy academic system, prestige is linked to real world societal impact. That is still somewhat true in fields like Machine Learning, in some fields it seems to be completely dis-aligned from any real world impact whatsoever (which seems to be PL research). Our academic system unfortunately is a rotten carcass. You could still, advisor willing, do research that interests you and not care at all whether you get noticed by conferences/ journals, your peers etc. But that takes a certain level of anti-social behavior that very few humans possess and so I say join a job. Plenty of companies are still building programming languages, like Google, Apple etc which are being used by engineers worldwide and if you finagle your way into a job at those teams, you will have a meaningful, impactful job, which is also well paying as a side bonus. reply noelwelsh 8 hours agorootparent> has there been any useful Programming Language that has come out of PL research/ Academia in the last 20 years? The goal of PL research is not, usually, to produce languages that see commercial adoption but to produce ideas that industry adopts. You cannot say a language like Rust is not influenced by PL research. reply sashank_1509 7 hours agorootparentNo, I can very strongly claim that I doubt any of the modern languages like Rust, Go etc have been influenced by the trainwreck, that is programming language research. PL research today is actually the study of something called “type theory,” whose relation to the act of building programming languages is the same relation a math PhD has to a carpenter. You will be a great mathematician if you do PL research but I would prefer if you do it in the maths department and not con us into believing it has something to do with programming languages. This is apparently what undergrads are taught in a compilers course: https://x.com/deedydas/status/1846366712616403366 I rest my case. (imagine the grad course syllabus) On the fringes, you might find research groups who are doing interesting useful stuff in programming languages, but that is an exception to the rule. Which is probably why, you never hear any of the new language developers ever cite programming language research. reply yodsanklai 7 hours agorootparentThere is much more to PL research than \"type theory\". Look for instance at POPL 2024 program [1]. Also Rust has been influenced by type theory. Rust first compiler was written in OCaml and the influence of OCaml/Haskell (and many other languages [2]) is pretty clear. Goal of PL research isn't to design programming languages but academic research has a lot of influence on programming languages. [1] https://popl24.sigplan.org/program/program-POPL-2024/ [2] https://news.ycombinator.com/item?id=34704772) Edit: regarding https://x.com/deedydas/status/1846366712616403366?mx=2 these are just the formal specs of a type checker. Nothing magic or very complicated there, it's just a notation. Anyone who can understand and implement a type checker should be able to understand this notation as well. reply sashank_1509 6 hours agorootparentThe creator of Rust in his own words: “ Introducing: Rust Rust is a language that mostly cribs from past languages. Nothing new. Unapologetic interest in the static, structured, concurrent, large-systems language niche, Not for scripting, prototyping, or casual hacking, Not for research or exploring a new type system, Concentrates on known ways of achieving: More safety, More concurrency, Less mess, Nothing new? Hardly anything. Maybe a keyword or two, Many older languages better than newer ones: e.g., Mesa (1977), BETA (1975), CLU (1974) … We keep forgetting already-learned lessons., Rust picks from 80s/early 90s languages: Nil (1981), Hermes (1990), Erlang (1987), Sather (1990), Newsqueak (1988), Alef (1995), Limbo (1996), Napier (1985, 1988).” If modern PL research is trying to take credit for the latest hot programming language (which I doubt they are, it’s only internet commentators who have nothing to do with PL research who argue with me. Actual PL researchers don’t care about Rust), they should be embarrassed. Thank you for linking latest PL research, it has been a while since I’ve gone through it, glad to see nothing has changed. Ask yourself, how many of those talks in day 1, have accompanying code? is it even 25%? For giggles I decided to peruse through “Normal bisimulations by Value”. A 54 page dense paper with theorems, equations and lemmas. Lol, what are we even doing here? You can also notice that they don’t bother justifying their research in the intro or the abstract, claiming relevance to any actual programming language. They themselves realize it’s just math, and PL researchers has become a euphemism for math. Frankly, even one such paper being accepted to a PL conference tells me something is going awry in the field, but if a majority of papers are like this, then the field is a wasteland, that only serves to grind young talented minds into spending their lives chasing academic prestige with no value to society. reply JW_00000 5 hours agorootparent> Ask yourself, how many of those talks in day 1, have accompanying code? is it even 25%? 57 out of 93 papers (61%) published at POPL 24 have an artifact available. Note that this may also be automated proofs etc, it's not necessarily \"running code\". But I also think focusing on POPL as a representation of the PL community isn't entirely fair. POPL is the primary conference focused on type systems within the PL community. It's a niche within a niche. Conferences like OOPSLA, ECOOP, or ICFP are much broader and much less likely to be so focused on mathematical proofs. [1] https://dl.acm.org/toc/pacmpl/2024/8/POPL reply sashank_1509 5 hours agorootparentI asked Claude to go through all paper names and estimate how many have code vs how many are proofs: “Based on my analysis, I estimate: - ~35-40 papers (roughly 35%) likely have significant accompanying code - ~55-60 papers (roughly 65%) are primarily theoretical/mathematical proofs “ I suspect even the remaining 35% doesn’t have much to do with programming languages, and I don’t think these stats change much for other conferences. reply JW_00000 3 hours agorootparent> I don’t think these stats change much for other conferences. I'd severely doubt that: there is a large difference in focus on theory vs practice between conferences. POPL really is one of the more theoretical conferences. At a conference like ECOOP, you're unlikely to see many proofs (I'd guess at most 20% of papers, based on personal experience). reply yodsanklai 2 hours agorootparentprevOne sub-field of PL research is the ability to formally specify programs, and thus understand their meaning and prove their correctness. A great project that is based on lots of theoretical foundations and practical implications is compcert [1]. They wrote a C compiler and proved that it translates C code to equivalent assembly code. You couldn't even state the problem without all the maths, let alone prove it. I'd argue that having correct compilers is worth the effort. I assume \"Normal bisimulations by Value\" talks about equivalence relations between concurrent programs. If you want to prove correctness properties of concurrent programs or cryptographic protocols, this is one of the tools. It's not because there's no code and only maths that it's not relevant. > Actual PL researchers don’t care about Rust Not true, I just watched this video a few days ago about Rust semantics [2]. How would you prove that a Rust program making use of unsafe construct is actually safe? what does safe even mean? how to describe rigorously the behavior of the rust type checker? AFIAU there's not even an informal spec, let alone a formal one. How are you supposed to write correct program or compiler if the language isn't specified? > Rust is a language that mostly cribs from past languages. Nothing new. Doesn't mean that Rust isn't influenced by academic languages and ideas. Anybody who knows Haskell or OCaml see the direct influence. Research isn't industry. A lot of what is produced may have no direct applications but may in the future. This is the point, it's research. Also it's not because you don't see the connections between research and application that they don't exist. Lots of people working on these industrial tools have an academic background and bring their knowledge into the equation. > If modern PL research is trying to take credit for the latest hot programming language (which I doubt they are, it’s only internet commentators who have nothing to do with PL research who argue with me. Actual PL researchers don’t care about Rust), they should be embarrassed. You're the one explaining that Rust didn't benefit from academic research which is obviously not the case. [1] https://compcert.org [2] https://www.youtube.com/watch?v=9y1dLDnS4uE reply saagarjha 6 hours agorootparentprevHave you ever talked to the people who design those languages? Because they will disagree with you about as strongly. And, of course, they are in a position to be correct. reply JW_00000 5 hours agorootparentprev1. TypeScript (and Dart, which influenced it) would not exist without the research on gradual and optional typing. Many other of the type system features in TypeScript – like type inferencing, intersection and union types, and type-level programming (e.g. conditional types) – find their origin in PL research, and were uncommon in mainstream but common in academic programming languages before TypeScript appeared. 2. Similarly, mypy was created by Jukka Lehtosalo as part of his PhD [1] and part of a wave of research in applying gradual typing to dynamically typed programming languages. 3. Rust's ownership types and borrowing are based on PL research, such as linear logic / linear types. Same for traits. Early Rust even had typestates. 4. Several of the core developers of Rust, Go, TypeScript, C#, Dart, Scala, have a PhD in PL or a background in research. 5. Generics are another feature that was heavily researched in academia (admittedly a longer time ago) before becoming part of mainstream programming languages. So I completely disagree with you: most modern languages have been heavily influenced by programming language research. In fact, I'd be hard-pressed to find a modern PL that hasn't been in some way influenced by PL research. (One thing I agree with in your comment, is that current PL research focuses too heavily on type systems and should look more at other interesting PL features. My recommendation to InkCanon would therefore be to look broader than type systems. The problem with research on type systems is that, because it looks math-y, it feels more like \"science\" and hence \"cures impostor syndrome\". But cool stuff can be real science too!) [1] https://mypy-lang.org/about.html reply bdangubic 7 hours agorootparentprevAsk yourself this, has there been any useful Programming Language that has come out of PL research/ Academia in the last 20 years? Scala reply InkCanon 6 hours agorootparentprevThere are several assumptions here tangled together here 1) Use is sufficient, but not necessary for impact. Theory of relativity, a lot of QM, etc has had only uses in real world edge cases, but have enormous value. The value function for impact, so to speak, includes more than just use. 2) There is the structure of academia and it's incentives, the average behavior of people in it, and it's outcomes. I don't necessarily have to bow to it's incentives, nor behave like the average person in there. Academia is also sufficiently large and fractal that you can find people less interested in the incentives and more in some thing they obsesses about. PL has had some interesting, although sometimes unheard of, real world uses. CUDA for example. A significant chunk of PL now focuses on ML. Awhile back a company called Monoidics got acquired my Facebook for work on static bug finding with formal methods. Rust has been pretty influenced by PL concepts. New languages like WASM are formally verified from the ground up, and there are exciting opportunities for that. I have considered slinging my resume to more research oriented companies, but hearsay from people is that the golden age is over. Under FOMO and stock market pressure, these companies are eradicating the kind of freewheeling research they used to and dumping money into ML and ML hardware. Not to mention it's a bit of a dice roll and a circus to get a job at such companies nowadays as a fresh graduate. reply sashank_1509 5 hours agorootparentThe only group under low pressure, free to do anything they want in Academia are tenured profs who have established themselves well or grad students who don’t care at all about remaining in Academia (and presumably have NSF fellowship or similar so that they don’t need to listen to their advisor either). Everyone else needs to grind, profs without tenure are arguably under the highest pressure, PhDs who want to stay in academia have high pressure etc. If your prof is tenured but not established and is struggling for grant money, even he is under high pressure to publish and win grants, something I learnt the hard way. The grant acceptance rate is what, like 20% these days. My 2 cents, you will be more likely to encounter creative coders who are passionate about a field in the right industry team than Academia. Unfortunately getting into the right industry team is also a grind, and you likely won’t get there right out of undergrad, but within 10 years, if you put effort and grind, you can get there. I think it’s better odds and more fun than going to academia, but your mileage may vary. reply peterkelly 7 hours agorootparentprev> A PhD is a program to compete for academic prestige That's true for some people but others have different motivations, such as learning useful skills so they can gain the ability to work on interesting problems in a given field. Doing a PhD in PL can also help you get the kind of jobs you mentioned, and achieve more once you're there. For me, the most valuable I thing I got out of the process was extensive exposure to the literature, which has been useful in a range of contexts. reply nikolayasdf123 7 hours agoparentprev> And after that I could get a job, become a professor, turn my research into a startup etc. chances of getting professor-ship, tenure, or even a post-doc is close to nil, due to extreme competition and limited seats. academia is the most slowly moving enterprise, some folks in their 80s still around, when young grads kicked out. getting a job after PhD may also be very hard. you would be very over-qualified, likely huge ego, and very narrow skillset in your domain, that is likely lagging behind industry. managerial (or even just \"work at corporate\") skills will be lacking. unemployment of PhDs is wildly high, even higher than if you did not do PhD. turning research into startup may be much harder than you would expect. milking government funding for years (and surviving jungle of academic politics to get its cut) is very different from market outside of it (i.e. venture capital, startups, tech, etc.), at the time you would want to make startup you would have to learn all from scratch, or even un-learn, as many would be detrimental. then there is toxic academic culture (funding, publishing, power dynamics). and in recent years academia become pit of wild woke left agenda, even more oil on fire. tbh, if you want to do something special, academia as we have it today is not the best place. if you still want to do it, guess best strategy is to \"do it quick and get out\". some smart people I know doing exactly that. doing accelerated PhD asap and getting the hell out of academia. (but then, it depends all on your professor power dynamics. in some places they would not let you graduate unless they wish so.) reply jltsiren 1 hour agorootparentGetting a postdoc position is usually easier than getting into a PhD program in the same university, especially in top universities. And the chances of getting a tenure-track faculty position or similar are probably something like 1/3. The biggest obstacles to getting an academic job are personal. The jobs are wherever they are, and your (or your partner's) preferences cannot change that. If you are willing to relocate, your chances of getting a good academic job are much higher than if you restrict your search to a single city / region / country / continent. reply InkCanon 6 hours agorootparentprev>getting a job in academia is really hard In my side of the world its a little bit better, the CS department has plans to double headcount in the next few years. They've got whole new faculty apartment buildings set up and everything, and the funding situation is quite generous (I'm told). Although I have also read in the USA the bar for even stipend paying masters/PhDs has gotten incredibly high. >Milking government funding for years There are special programs for startup oriented funds, so it's more like VC pitching to academics with equity free grants (although naturally there's the whole university research IP issue). But I'm quite willing to put up with it to do something meaningful (at a decent number of jobs you put up with it just to keep your job). I do keep an ear on startup-y things, I don't think I'll have it any easier than an undergrad but I think I won't be too disadvantaged. >Do it and get out I don't place too much emphasis on the PhD per se but the real value of it. >If you want to do something special academia is not the place Ten years ago tech would've been a good place. But now especially for a new graduate its a bloodbath, not to mention there's been huge layoffs. Academia seems like the better option nowadays. reply andrepd 7 hours agorootparentprev> changes of getting professor-ship, tenure, or even a post-doc is close to nil What an extreme exaggeration. Yes academia is competitive, yes tenure is hard to get (obviously). But the chance is not \"close to nil\" for that at all, and it's certainly not \"close to nil\" for a postdoc lol. > getting a job after PhD may also be very hard. you would be very over-qualified, likely huge ego, and very narrow skillset in your domain, that is likely lagging behind industry. managerial (or even just \"work at corporate\") skills will be lacking. unemployment of PhDs is wildly high, even higher than if you did not do PhD. This is just not true x) There are no numbers where PhDs have worse unemployment than grads. Conversely, it does open a lot of doors for industry jobs (think ML, quant finance, to name a few) reply fastneutron 7 hours agoprevMost discussions I see online about whether or not someone should do a PhD tend to assume: - The student becomes hyper focused and pigeonholed into some esoteric and unemployable domain, destined to run on the postdoctoral treadmill for decades. - The PI is a control freak who only cares about publications, and considers students who leave for industry jobs after graduation to be failures. These stereotypes can have an element of truth, but there are more enlightened PhD programs and PIs that understand the value of cross-cutting and commercializable research than you’d expect from the discourse. Not everyone is stuck working on a pinprick of knowledge, and if you choose your program and PI wisely, you can go much further and do many more things than you would never have access to with just an undergraduate background. reply InkCanon 20 minutes agoparentI never understood point 1. Your PhD thesis will almost definitely be on a very specific topic, you don't have the time or knowledge to cover multiple distinct fields. reply hikingsimulator 7 hours agoparentprevOne big issue is that industry jobs in some areas increasingly expect academic excellence in the shape of \"publishing in top 3 conferences\" for example. reply Over2Chars 7 hours agorootparentAn obviously totally arbitrary barrier. Why not 4 or even 5? Someone who only published in 2 top conferences is obviously not worth anyone's time. But 3, now we're talking. reply saagarjha 6 hours agorootparentBecause that's the number conferences that are generally considered to be better than the rest. Just like the \"top 4\" computer science schools in the US are unambiguously Stanford, UC Berkeley, MIT, and Carnegie Mellon. You can ask, why not 5? Because then you start getting into questions about whether you want to include UCLA or UIUC or Caltech and it's significantly more complicated. reply Over2Chars 6 hours agorootparentAnd... it's totally arbitrary. Top people come from non-top schools, and lots of non-top people from from \"top\" schools. And some top people come from no school at all. reply saagarjha 6 hours agorootparentOf course, but there are still four schools which are clearly the \"top\" ones. The same is true for academic conferences, or big tech, or intelligence agencies. reply Over2Chars 4 hours agorootparentTop is just marketing. In Big Tech it's market cap or something, but it's not proof of anything and may be just marketing. Google is a search advertising monopoly pretending to be a \"tech\" company (per Thiel), but is a \"top\" company to work for. Ok. And intelligence agencies are government mandated, not marketing made. Or at least I haven't heard any marketing from the NSA saying how selective they are in admissions (as if that means anything). reply halgir 6 hours agorootparentprevMost quantitative recruitment criteria are arbitrary to some degree. Unless you rigorously examine every single applicant, you need some heuristics for initial filtering. reply Over2Chars 6 hours agorootparentSo you need some arbitrary filtering to give you breathing room for your objective heuristics? For some reason that seems slightly non-optimal. reply Over2Chars 7 hours agoparentprevI'm not saying you're wrong but... Elon Musk skipped his PhD program and did many more things than spending time in school would have allowed him to do. Of course, most people aren't Elon (probably a good thing). Other than preparing you for a career in academia or some highly regulated environment where education is erected as a barrier to entry, it's hard for me to think of \"many more things\" that are open to a phd holder than to someone who is not. reply fastneutron 5 hours agorootparentCelebrity exceptions are exactly that; exceptions. Those people knew an opportunity when they had one, and were able to generalize their early successes into other domains by leveraging the financial, social, and intellectual capital they accumulated. People who fit this description aren’t the ones reading this thread. In some fields all you need is a computer and an idea to be impactful, but in plenty of other fields you’d be hard pressed to make any credible, let alone meaningful impact without significant intellectual preparation and tacit knowledge. These things only come through experience, and for many people, the PhD program is that experience. reply Over2Chars 4 hours agorootparentI agree that the exception is rare, but it suggests that the non-exception isn't exclusively necessary. It might suggest that the dominant paradigm of diplomas is quite non-optimal or at least optional. Carlos Ghosn started out as a factory manager (although well educated), and in his Stanford interview the presenter noted that Stanford produced no factory managers, although it produces lots of would be global CEOs. Perhaps it should produce more factory managers. Musk has shown an ability to make an impact in multiple fields for which he seems quite under qualified for, for which he did not have \"significant intellectual preparation and tacit knowledge\". He read alot. I think there are more non-celebrity exceptions that are simply not well known. And there are lots of people in PhD programs who, despite their education, do not make credible or meaningful impacts, quite possibly not at all due to their competence or training quality, but due to wholly accidental or uncontrollable factors: industry shifts, business culture, changes in government research funding, or their entire paradigm being based on faulty assumptions that were simply not known and discovered later, or superseded by some innovation, etc. Academics are rarely comfortable discussing the shortcomings of academia. reply fastneutron 3 hours agorootparentNo, the non-exception is not absolutely necessary, and there are plenty of people on my staff who fit the description. There are also plenty more who Dunning-Krueger their way into thinking they do, but break down when challenged to do anything novel. Understand your options and choose your program carefully. > Musk has shown an ability to make an impact in multiple fields for which he seems quite under qualified for, for which he did not have \"significant intellectual preparation and tacit knowledge\". He read alot. He also had a giant pile of money from his PayPal windfall to hire the right people with the tacit knowledge to act on his ideas. The difference between a crank and eccentric businessman is the size of the budget they can wield when nobody else will. > Academics are rarely comfortable discussing the shortcomings of academia. Correct, which is why I’m not in academia. reply stared 5 hours agoprevThe illustrated guide (which I had known during my PhD) focuses on progress—however, niche progress. Yet, PhD students' life experiences are usually not centered around progress but frustration, disillusionment, and depression. If you want to compare it with anything else, it may be like creating pieces of novel art with great effort, with one's identity tied to it. Yet, even in the case of success, it is unlikely to be appreciated by contemporary people. So, I prefer a narrative guide to PhD - \"The Lord of the Rings: an allegory of the PhD?\" by Dave Pritchard, http://danny.oz.au/danny/humour/phd_lotr.html reply resiros 3 hours agoprevThat's a way of looking at it. Another way is looking at it inwardly, as a journey of self. Personally, a PhD for me was being punched in the face every morning (i.e., experiment failed) for years, yet getting up to try again. It's the archetypal hero's journey where you dive into the deepest darkness, discover yourself, and emerge triumphant. At the end, you get a hat. reply liontwist 12 hours agoprevIt’s a nice idea that you’re going to help the boundary of human knowledge expand but I don’t think infinite progress is the right model. All the evidence shows that fields are completely ignorant of each other and reinvent the basic solutions. This coincides with the theory that cohorts of experts develop expertise which is not transferrable. Watch as ML rediscovers harmonic analysis while awarding plenty of Phds to those involved. Rediscovery is a great thing. You bring new meaning and context. I’s just not “expanding circle of knowledge” More likely is you will dig further down the track of the fads your advisor is into. The trend will be forgotten in a few decades, with a small change of unforeseen utility later. And its contribution will be to your personal life. The model proposed is also lacking in ambition because historically PhDs were significant. reply mnky9800n 9 hours agoparentFor example https://arxiv.org/abs/2006.10739 reply ySteeK 10 hours agoprevYou don't need a PhD to push the boundaries. You need a PhD to make others believe you pushed the boundaries! reply mr_mitm 10 hours agoparentYou got it backwards. You need to push the boundaries to get a PhD. reply amelius 7 hours agorootparentPush the boundaries but not actually move them. reply Over2Chars 7 hours agorootparentLOL. Hey! reply brulard 8 hours agorootparentprevMaybe at the worlds best universities. For sure not at an average one. reply bdangubic 7 hours agorootparentprevyou should read some PhD dissertations outside of top schools /s :) reply dyauspitr 9 hours agoparentprevYes, because so many contemporary breakthroughs have come from some guy in his garage. reply scaradim 1 hour agoprevNice! but... are these graph steps roughly at scale e.g. area of initial blue circle compared to all of human knowledge circle? or the small PhD pimple compared to the blue circle? If not then can someone (original author or someone else) try to review the steps with roughly realistic average scales assuming surface represent knowledge volume and color complexity spectrum from advanced (indigo) to advanced (red)? Any candidate to make another pimple on the outer circle? reply oezi 11 hours agoprevPhDs' unfortunately have lost much of their value. - There aren't enough post-doc and tenure positions for the glut of PhDs. - Plagiarism scandals have reduced the public's perception of a PhD to become almost something unprestigous. reply someothherguyy 10 hours agoparentThankfully not everyone feels this way, and humanity continues to benefit from the work that PhD holders do. reply oezi 9 hours agorootparentThat certainly would be nice, but the risk to the individual that they are just exploited on a meagre salary for 3-6 years to only benefit their advisor has become so large that I don't recommend doing a PhD to anyone (or at least think very hard and very diligently investigate the prospective advisor and the faculty). Even if this means we as a society are losing out on scientific discoveries. reply Ekaros 8 hours agorootparentExploitation doesn't often end at certification. Academic world is very harsh for post-doc too. And permanent positions are rarer than degree holders. reply CaffeineLD50 10 hours agoparentprevAs AI becomes better the quality of plagiarism should improve. So there's that. reply dyauspitr 9 hours agoparentprevYes exactly. If you’re not a plumber are you even really contributing to human progress. reply croissants 3 hours agoprevMy fairly generic PhD advice, as somebody who did one, graduated a bit early, and is happy about the process and where it's taken me: * Choose your advisor with care. This is not very easy as an applicant looking at professors' websites, but if you are admitted, any good school will probably have an in-person or virtual admitted students day where you can talk to current students out of faculty earshot. Take advantage of these times to ask about your potential advisor. A truly bad advisor will probably produce at least one person who will warn you about them. If you can't do this in person, try to get a quick phone/video chat -- something off-record where they can be honest. I was always happy to do these for my advisor, because I liked him and wanted him to get more good students. Conversely, I know people who were warned off specific advisors during these events, for good reasons. A bit of subjectivity: a good advisor at a decent school is usually better than a bad advisor at a good school. * The financial niceness of doing a PhD in field X seems to correlate pretty well with the current job market value of a masters in field X, at least partially for reasons of leverage -- if you can leave and transition into a cushier job, advisors have to provide a bit more value. Computer science scores highly on this metric. * There is a ton of negativity about PhDs in places like HN. This isn't unjustified: doing a PhD with a bad advisor can be a very bad experience. At the same time, I think \"person who had a bad PhD experience\" is also \"person who writes comments on the internet\" with higher probability than \"person who had a good PhD experience\". reply dataflow 10 hours agoprevI just pray that everyone denigrating PhDs is making even greater contributions to humanity. reply sashank_1509 8 hours agoparentNo one’s denigrating those who choose to do a PhD, most of them I am sure are bright, ambitious and thoughtful individuals. Unfortunately the Academic Meat grinder system, successfully grinds them into working 80hr weeks on meaningless projects, meaningless papers that just serve to create jobs for other academics. They could be doing so much more with the faculty they possess, alas they would be more useful as a barista. (I am not picking on humanities PhD system, which is a bigger cesspool than normal, all my criticism is aimed at STEM PhD system. My actual advice would be do a PhD if you judge the lab you are joining as doing great work, lab is everything, don’t join a PhD if it’s not a lab that you’ve vetted or if you didn’t get into that lab.) reply liontwist 2 hours agoparentprev> making even greater contributions to humanity. than writing papers nobody will ever read? yes I am. A PhD can be a great contribution to your life, and the opportunities it can bring you and your family. The phony marketing that appeals to young people is that you are advancing the progressive human narrative. reply CaffeineLD50 10 hours agoparentprevSince a PhD makes no intrinsic contribution, even greater than nothing is easily done. reply saagarjha 9 hours agorootparentPosting rude comments is worse than nothing, though. reply Over2Chars 7 hours agorootparentWhy is getting a piece of paper after sitting in dozens of classes, and writing a essay considered automatically \"beneficial to humanity\"? Just pulling a random site (first hit on a search) https://www.findaphd.com/phds/browsebysubject.aspx It appears you can get a phd in dance, event management, or dozens of fields that aren't curing cancer, or AI. I'm reminded of the Olympic break dancer who did the \"kangaroo\" move. She has a PHD: https://en.wikipedia.org/wiki/Rachael_Gunn thesis: \"Her PhD thesis, titled Deterritorializing Gender in Sydney's Breakdancing Scene: A B-girl's Experience of B-boying\" (from Wikipedia) Now I'm sure it was worthwhile, but I'm struggling to see it as a presumptive positive move to humanity for her work. And there's lots just like it. I'd even go so far as to assume that it's the extreme minority of phds that actually make a difference \"for humanity\" and not simply a good career move for the degree holder. reply hulium 6 hours agorootparentBeneficial to humanity does not mean \"increasing the GDP\". Humans are not machines; there are things that are valuable that is not technology. Just from the title of the thesis I would not judge its value. Analyzing culture and writing about your own ideas can be a good thing. More important to me is whether it was written because someone really cared about the topic, or wrote 100s of pages of nothing to have a title. But I also want to comment on your idea that researching AI is superior to researching dancing because it is more beneficial for humanity. I am myself in AI adjacent research, but I still disagree. Dancing is a deeply human thing, and we should care about it. And I believe that many people (especially outside of this bubble) will think that many parts of currently hyped AI research has very questionable \"benefit for humanity\", such as AI image generators. reply Over2Chars 5 hours agorootparentI'm 100% with you on the nebulous value of AI, however given the \"theme\" of HN I think the bias is here towards such an assumption. I am not dismissing the value of dance or culture or social science or even event management (or farming or any of the dozens and dozens of things people can get Ph.D's in). And I note that in my post above. I am seriously not being sarcastic by saying it's worthwhile. Probably to her. Maybe her department. Maybe even to break dancing (well, maybe not break dancing). But the presumption that someone getting a Ph.D has somehow \"uplifted us all\" as a default seems highly improbable to me. It no doubt uplifts a number of the degree holders, if statistics bear out. But that's like saying \"If Elon Musk becomes rich, we all become rich\" and I don't think that's true. Not even in some \"trickle down\" economics kinda way. If Elon gets a Ph.D we have all to assume some intrinsic benefit to humanity? Uhm, I don't think so. reply saagarjha 6 hours agorootparentprevI'm unsure whether I should reply because I get the feeling that you think \"beneficial to humanity\" looks like someone working at OpenAI and not doing research on, say, anthropology. reply Over2Chars 5 hours agorootparentYou tell me if for any definition of \"beneficial to humanity\" that all or most Ph.Ds would fit that definition presumptively/intrinsically. I chose arbitrary fields that I suspected most readers of HN wouldn't be in, be familiar with, or necessarily esteem just to make the case more obvious. For example, a Ph.D in dance. There are lots and lots of PhDs in all kinds of things. You can get a Phd in videogames, food hygiene, librarianship, gender studies, ancient greek, theology. I am sure they are (mostly) worthwhile, and I'm not knocking any of them. I don't see how they would, by default, be seen to benefit humanity. Some small number might. A very small number. Possibly an extremely small number. According to one random website there are over 70,000 new PhDs every year. That's a lot of assumed benefit to humanity. Or.. is it? reply saagarjha 1 hour agorootparentI consider the pursuit of knowledge to be beneficial to humanity. To that end most PhDs are doing something good. reply dyauspitr 9 hours agorootparentprevThis comment is the equivalent of people doing their own research on vaccines. reply jofer 52 minutes agoprevMy wife rather derisively calls this the \"penis pimple model of science\". It's a great article and a great set figures, but folks sometimes push it a bit too much, especially in grad school. It has a lot of truth to it and it's been making the rounds for well over a decade. Unfortunately, sometimes it can set folks up to feel bad about themselves if what they do doesn't line up well with whatever is in vogue as the boundary of knowledge that's currently being pushed. There's a _ton_ of value in more pragmatic parts of fields that focus on applications or combining relatively well-know parts of different sub-fields. Those parts of science often don't feel like you're pushing some \"boundary\". It's more layering on top of, filling in holes, and building up than building \"out\". Sometimes what you do is to use multiple things that lots of folks already knew, but the people who knew X and the people who knew Y didn't talk to each other, so no one thought about how to combine them. It can also be hard to get papers published because reviewers will consider one part obvious/well-known and the other part irrelevant because they come from one sub-field and not both. It will often feel like you don't belong because your work doesn't look like these figures. Applied and interdisciplinary fields \"feel\" different. However, this type of integrative work can be among the most valuable parts of modern research. Don't feel like what you do has to fit into the \"penis pimple model\" of science. There's also nothing wrong with pushing some known boundary of a field, either! Both are valid. reply tbrownaw 44 minutes agoparentBut this isn't a model is science, it's a model of what different kinds of academic credentials mean. Sure there's plenty of ways to contribute to society without publishing academic research, but that's not what a PhD is. reply jofer 35 minutes agorootparentNot all PhDs push boundaries of knowledge in quite that way. Often what you do was already known, but no one really figured out how to correctly apply it. Alternatively, it's often combining different things that were already well known, but no one had combined. It's novel work that makes a PhD. Novel work is distinct from pushing the boundary of knowledge. Often what you do doesn't change what \"humanity knows\" in any way. Both my wife and I have PhDs. Neither of us did things that look like the figures there. It's not a good mental model for what all PhDs mean, though it is a good model for some. I combined fields and reinterpreted a ton of things that had already been done to draw very different fundamental conclusions about what was going on in a particular location. I put out alternative hypotheses for observations that had already been collected. It's not new knowledge at all and I didn't add to what we \"know\". I just added an additional hypothesis to the set of multiple working hypotheses that will hopefully be tested decades from now. My wife worked on how to actually apply well-known methods in other fields to our field. Her work was half engineering, half field experiments. Lots of folks had been working with fiber optic strain gauges for decades. However, no one was using them to measure in-situ strain in rock masses yet (which has since become common). The application was broadly \"known\", but actually doing it and demonstrating that is novel. reply skalarproduktr 5 hours agoprevMatt has also written a great follow-up, \"How to get tenure\", see https://matt.might.net/articles/tenure/ Highly relevant for the aspiring researcher, and it describes really well the nonlinearities of (academic) life. reply giacomoforte 9 hours agoprevIf you don't pick your lab/PI wisely you might end up stroking you PI's ego without accomplishing anything of value. Or you might end doing four years of work that ends up obsolete before you even graduate. reply kowlo 8 hours agoparentFor most (nowadays), the journey is about becoming a competent researcher. reply worldmerge 6 hours agoprevIs it possible to be a professor without a PhD? I would like to teach at a college level but the PhD path seems so risky to me to take a pay cut for years and you might not get the degree, and not a get a job. reply gtmitchell 4 hours agoparentIn the US at least, it is entirely possible to teach at a university without a PhD. Community colleges are full of instructors with masters's degrees, and tons of classes offered by major universities are taught by graduate students or adjunct faculty without doctorates. Your job title probably won't be 'professor', but you'll be doing basically the same work as one. reply lapcat 3 hours agorootparentGraduate students teach classes at their own universities as part of their departmental funding. This is only a temporary situation and exists only while they're enrolled. It's not a career path. As a former graduate student myself, I'm actually not aware of any non-PhDs who are adjuct faculty or community college instructors. I'm not claiming that they don't exist anywhere, but given the number of PhDs and the number of available academic jobs, the competition is fierce, and non-PhD candidates are likely to lose out to PhD candidates. reply pinkmuffinere 9 minutes agorootparentFwiw my dad had a masters in biology and a PhD in botany, but was an instructor for biology in the local community college (“Mount San Jacinto Community College”). I guess technically he had a PhD, but not in the way most people would think reply enum 6 hours agoparentprevIn the U.S., in most fields, it is virtually impossible. reply ioblomov 3 hours agorootparentThe only exception would be community colleges, which still require at least a masters. At my university, my favorite professor’s title was “senior lecturer” because he only had a bachelors. This was despite being a Times bestselling author. (He taught literature and writing.) reply paulpauper 2 hours agoparentprevJust make a youtube channel and start lecturing or substack. Obviously not the same thing, but the barriers to entry to content creation have been eroded, and if if you have the chops not having a doctorate won't be an impediment. reply ulrischa 10 hours agoprevA phd ist not a phd. A phd in medicine is like a Bachelor thesis. While a phd in engineering can become a 10 year nightmare reply chaosite 10 hours agoparentThat's not right. A PhD is a PhD. An MD (Medical Doctorate) is like a master's degree. It's not like a bachelor's because many MD programs start out with or require a BSc, biology is a popular choice but a lot of STEM majors are possible. But MD+PhD programs exist and those are definitely PhDs. You are right that an MD is not a PhD, though. Notice how they don't call it a PhD. reply reshlo 9 hours agorootparent> It’s not like a bachelor’s because many MD programs start out with or require a BSc In many countries, the degree you must obtain to qualify as an MD is indeed a bachelor’s degree, the “Bachelor of Medicine, Bachelor of Surgery” (often abbreviated MB ChB or MBBS). reply chaosite 9 hours agorootparentIt's still a 5-6 year degree, as opposed to a 3-4 year bachelor's. I agree it's not a research degree though... But some master programs don't include writing a research thesis either. reply mnky9800n 9 hours agorootparentprevMDs are also obsessed with calling each other “doctor”. It always comes across as imposed authority from people who are essentially body mechanics. reply blitzar 8 hours agorootparentDoctor https://www.youtube.com/watch?v=Dhkwh8u30mo reply anticensor 8 hours agorootparentprevNo, a MD is a professional doctorate with novelty and research requirements, it's between a DA and a PhD in terms of difficulty. reply chaosite 7 hours agorootparentSorry, I'm having trouble parsing your point. \"Professional degree\" is juxtaposed with \"research degree\". So if you say it's a professional degree, you're basically agreeing it's not a research degree... In my country at least MDs are not required to be researchers and the degree has no novelty requirements. They're required to be competent medical professionals. There are countries where the base medical degree is the MBBS and MD is a graduate research doctorate. That's not what I'm talking about here. I'm not talking about \"difficulty\", by the way. Just the differences between the degrees. reply 0xTJ 5 hours agoprevI'm a bit confused about why the Master's degree is set before \"reading papers\". I had to do a whole lot of reading (and be at least involved in the writing of) papers in the research of my Master's. reply KnuthIsGod 9 hours agoprevNot in here Australia. At many of our leading institutions in the hard sciences, rehashed work, data stolen from other teams, photoshopped images and a bit of plagiarism is enough to get you by. Once you get to the social \"sciences\", it is much wor",
    "originSummary": [
      "An illustrated guide is used to explain to new Ph.D. students how a Ph.D. expands the boundary of human knowledge, symbolized as a small dent.",
      "Proceeds from the printed guide support research on genetic disorders, motivated by the author's personal connection to his son's rare condition.",
      "The guide is licensed under Creative Commons, permitting sharing with attribution, and is available in multiple languages."
    ],
    "commentSummary": [
      "The Illustrated Guide to a PhD by Matt Might emphasizes the importance of pursuing a problem you love and the continuous quest for knowledge in the PhD journey.",
      "The discussion addresses challenges in academia, such as the pressure to publish and the risk of prioritizing quantity over quality in research.",
      "The guide highlights the value of the PhD experience for personal growth and career opportunities, stressing the importance of choosing the right advisor and exploring interdisciplinary work."
    ],
    "points": 343,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1736659614
  },
  {
    "id": 42671105,
    "title": "Why I Chose Common Lisp",
    "originLink": "https://blog.djhaskin.com/blog/why-i-chose-common-lisp/",
    "originBody": "Why I Chose Common Lisp Published on January 12, 2025 Goodbye, Clojure After ~7 years, I was done with Clojure. I was writing a some CLI apps, and I hated how long they took to start up. The community at large seemed not to care about this problem, except for the babashka folks. However, I spent long, hard hours banging my head against native-image and it just wasn't working out. It was incredibly painful, and at the end of it, I still didn't have standalone, fast-starting native executables. I decided that that was a requirement for my main driving hobby language, and that Clojure didn't have it. It was then that I decided to move on from Clojure. On the Hunt for a New Lisp I started shopping around for a new lisp to use after hours like I'd done before with my home projects. I had specific requirements in mind, though I didn't actually list them when I started. I can list them now in hindsight, though: It must be easy to create standalone, fast-starting executables using a reasonable toolchain (to address my main concern with Clojure) I can't use Emacs, so it's got to be usable in Vim. It must have good support for Windows and Mac, in addition just Linux/POSIX operating systems. It would be nice if it allows plugging into some other, large-community imperative language, like Clojure does with Java. It must have a reasonably fast runtime -- hopefully as good or better as Clojure's. The language must have a strong multithreading story. It would be grand if whatever language it was didn't have a GIL, for example. Must have a strong community. Must have a good ecosystem, with at least the following libraries well-implemented, thus supporting my common use cases of making CLI tools: JSON parsing and serialization Sqlite3 support HTTP requests library Would be nice if it had functional data structures like Clojure's (this ended up being less of an issue later) I looked at Scheme, but that community seemed to still be fractured over the r6rs/r7rs mess. Also, there wasn't a big enough ecosystem to suit my liking. I'd already tried Racket in school and didn't like it. The runtime was a bit slow and bloated for my tastes. I had seen lisp-lang.org shared on the Orange Site. I was impressed with the site. I came back to it later after I first saw it and thought, why not. Maybe I'll give Common Lisp a shot. Magic Happens Here I will spare the reader the full narrative of my learning CL. It was a rough ride learning the language. (I went about it the wrong way, getting CLtLv2 for Christmas and reading through that. I eventually found the HyperSpec and started reading that as well.) There were some weird things I didn't expect to find about CL. It's a standardized language, more like C than Java that way. There are many compilers, interpreters, and runtimes that implement that standard. There's even a tool to help install them all and wrangle them. The most popular one according to the community is SBCL. If I had heard about Janet when starting this hunt, I might have stopped there and not gone on to CL. Nice syntax, small, fast executables, C FFI, a fun intro book. It checks all my boxes. However, I'm glad I did learn CL first, because I think I'd miss the CLOS and the Conditions System, things I learned about later in my journey. Common Lisp is just a bit of a stronger language. Requirements Met I found answers to all my questions, and decided my next lisp was going to be Common Lisp. I've been coding in it ever since. Here are the things I found: Standalone Executables: there are lots of ways to do this in Common Lisp. I summarize my favorite way in another blog post. Start up times range from a fraction of a second to nearly instant, depending on if you compile the executable with compression or not. This is not a bolt-on feature; it's a first-class-citizen way to distribute Lisp programs. Vim Workflow: There are a lot of good ones here, but I eventually settled into a Vim workflow of my own. Readers may also be interested to know that I found VS Code perfectly usable as a Common Lisp IDE. Windows/Mac/Linux Support: SBCL, a popular implementation and compiler for Common Lisp, supports The Big Three relatively well, as outlined in the blog post linked in point #1. Larger Imperative Ecosystem: Most implementations actually hook into the C programming language pretty well through the CFFI. That works for me. Runtime Speed: SBCL is crazy fast. Multithreading: While the Common Lisp standard does not make provisions for multithreading, all major implementations do support it and their differences are papered over with a library called Bordeaux-Threads. This library serves as an underpinning for the lparallel library, an excellent library for multithreading. There's also cl-async and blackbird for asynchronous programming and promises, respectively. Strong Community: I discovered the community as I did the rest of the language -- in fits and starts. A good summary of the community (as it was in 2024) is summarized in the Common Lisp Community Survey 2024. CL features prominently at the European Lisp Symposium. CL has a strong blogosphere and subreddit. Ecosystem: The ecosystem is pretty great. Most folks use Quicklisp, though I use OCICL for package management. The Common Lisp Cookbook, the CLiki, and Awesome CL provide nice survey of available libraries and techniques. Here are some answers to those particular libary queries I had: JSON: jzon Sqlite3: cl-sqlite HTTP requests: dexador Functional Datastructures: FSet, cl-hamt New Friends Take Note I wrote this blog post because I noticed that there have been more newcomers on the Common Lisp Discord and they've been asking the same questions I was when I was first looking at the language. I wanted to lay down a bit of history around why I came to Common Lisp, and how I acclimated to the language. I hope it may be helpful to those new to or curious about Common Lisp.",
    "commentLink": "https://news.ycombinator.com/item?id=42671105",
    "commentBody": "Why I Chose Common Lisp (djhaskin.com)291 points by djha-skin 15 hours agohidepastfavorite119 comments agentkilo 12 hours agoSBCL is a great choice! It's a surprisingly dynamic system (so are other CL implementations). A while ago, I did some private work for someone, using SBCL, and sent her the native binary I built, then forgot about the whole thing. The client came back with some new requirements much later, when the project was concluded, and the source code was already lost by that time. I vaguely remembered how I did things, so I spawned a REPL from the old binary, went into the relevant CL package, wrote a new function that overrides the old behavior, and the client's problem was solved. I did all those without the original source code. It was a small thing to fix, but I can't imagine making the same fix so swiftly with any other tech stack, when the source code is lost. I was deeply impressed. reply chikere232 10 hours agoparentThat's pretty cool, but also, get version control reply pizza 9 hours agorootparentVersion control won't help if the repo itself has been lost to the sands of time reply harperlee 6 hours agorootparentIt has been ages since I worked with Common Lisp, so I wonder: Would it be possible / make sense to have version control into the lisp image? reply killerstorm 6 hours agorootparentPossible: yes. Lisp image can hold arbitrary data, including source code, versions, etc. Specifically, arbitrary data can be attached to symbols. Make sense: no, unless you have very special requirements. It would be very different from common practice. Images are still much more fragile than files outside, they are not designed to last across implementation releases. (Note: some implementations might hold full code of every function in memory, that's not unreasonable.) reply lispm 6 hours agorootparentprevThat has been done in the past and was never widely supported. But the landscape is now slightly different. One approach might be to use ASDF (the most popular systems management tool), which then would need to be upgraded to use versions (and ideally have it integrated into a repository mechanism -> Git or similar... Also what are versions in a decentralized world?). ASDF operations then would need to be able deal with version numbers. A Lisp image would then know/track which versions of what code it has loaded/compiled/... Version information would be stored with the other source files. reply tiberious726 6 hours agorootparent> That has been done in the past Could you share some examples of how the old systems did this? reply lispm 6 hours agorootparentCan't say what and Interlisp did, but the MIT Lisp Machine and here the commercial variant Genera. Files are versioned. The Lisp machine file system has versions (same for DEC VMS and a few other file systems). Nowadays the file version is attached to the name&type. Thus editing a file and saving it creates a new file with the same name&type and an updated version number. I can edit/load/compile/open/... files by specifying its version number. If there is no version number specified, the newest file version is used. A site shared a namespace server with system registry and several file servers. Code, documentation, etc. is defined as a system, similar to what ASDF does. A system has major and minor version numbers and other attributes (like being experimental or being released). Each time one does a full compile on a system, its major version gets updated and it tracks which file versions it used for that compilation. Whenever one creates a patch (a change of the sources) to a system, the minor version gets updated. After 12 full compiles and 30 patches to the latest version we have system FOOBAR 12.30. This data is recorded in the system directory, thus outside of an image (called a World in Genera) in the file system. Now I can use that system FOOBAR in another Lisp and \"Load System FOOBAR :version Newest\" -> This will load system FOOBAR 12.30 into its runtime. Then new patches may appear in the system. I can then in a Lisp say \"Load Patches FOOBAR\" and it will look into the system directory for new patches to the current loaded major version and load those patches one by one. This changes the running Lisp. It then knows that it has loaded, say, FOOBAR 12.45, what files it has loaded and in which files the respective code (functions, ...) are located. If I want to ship a system to the customer or someone else, I might tag it as released and create a distribution (with sources, documentation or not). The distribution is a file (also possibly a tape, a bunch of floppy disks, a CDROM, or similar). A distribution can contain one or more systems and its files (in several versions). On another Lisp I can later restore the distribution or parts of it. The restored systems then can be loaded for a specific version. For example I can load a released system version and then load patches for it. A saved image knows which versions of what systems it includes. It also knows which functions sources are where. It may also have corresponding documentation versions loaded. Later a source versioning system (called VC) was added, but I haven't used that. reply arnsholt 5 hours agorootparentprevThis is basically how Smalltalks work. In addition to the application, an image will generally include a development environment and all the sources to your application. I think Lisp machine Lisps were a lot closer to this too. reply pjmlp 3 hours agorootparentIf you dig the old Xerox docs, some of them refer to Lisp as inspiration. Also the Mesa / Cedar had a similar approach, but based on dynamic libraries, which Niklaus Wirth took great advantage on how Oberon and derived systems worked. Incidentally, due to Rob Pikes admiration for Oberon, not only had Inferno the ACME editor from Plan 9, already influenced by how Oberon worked, the whole Limbo experience on Inferno is quite similar as well. Unfortunately we still lack this kind of full stack OS/REPL experience on modern systems. reply arnsholt 2 hours agorootparentI worked on a system implemented in Smalltalk for a few years, and the truly integrated development process possible in a ST is a high I've been chasing ever since. And this was in a setting that had so many things going against it: I started working on it in 2016, and the last update of the runtime environment was from '99. We had a custom HTTP client, and home-brew bindings to OpenSSL that I cooked up with some mad science Python that parsed C headers to generate Smalltalk code. I even had to fix a bug where Winsock fds rolled over if you opened more than 8k sockets, because the runtime FFI bindings assumed a 16 bit wide return value (I assume the code dates back to the Win 3.11 days). By all rights, it should have been _awful_. But in reality we were able to iterate quickly and actually make stuff, because the technical foundations of the system were rock solid, the team was good, and because the truly integrated Smalltalk development enviroment enables a super-tight development cycle. A joke I have with a former colleague from those days is how all modern languages are still fundamentally no different from the punch card days: You take your sources off to the batch processing center, and the results come out at the other end, while Smalltalk is future techonology from the 80s. reply pjmlp 22 minutes agorootparentYeah we get glimpses of it across JVM, .NET, PowerShell, Fish and so on, but hardly as it used to be. We do have Pharo, Allegro and such, but their spotlight opportunity is gone. reply chikere232 9 hours agorootparentprevVery true. Get backups reply wodenokoto 9 hours agorootparentWhich the client might be responsible for because the contractor might not be allowed to retain the IP reply crispyambulance 7 hours agorootparentYou're \"allowed\". Just keep it to yourself. reply ska 3 hours agorootparentYou often will not be, explicitly by contract. Honestly in this case it’s easier to keep things clean. reply huijzer 6 hours agorootparentprevThen don’t lose your repo xD reply f1shy 4 hours agorootparentprevAgree, but I think in this case would not help. It was not that a version was lost, but all. Had they had VC, probably the repo would have been deleted. reply munchler 5 hours agoparentprevYou don’t deliver source code with the binary at the end of the project? reply smokel 7 hours agoparentprevThis is something I really like about Common Lisp, but after ~30 years of progress, I am a bit disappointed with improvements in the field of interactive development. The process you describe is possible in JavaScript and many other languages as well. Updating a JAR is trivial, but doing it in a running system is uncomfortable. The first thing to start with is versioning at the function, class, package, or module level. Combine that with versioning of data, and you've got my interest :) reply pjmlp 3 hours agorootparentJRebel, for those willing to pay. reply nomilk 12 hours agoprevLooks like vim-slime is essential to how you work with CL + vim. I've only used vim for not even 2 years, and came across vim-slime 6 months ago when working in ruby and wanting to quickly 'send' code from editor (neovim) to rails console. 2 months ago I launched a startup and for hours every day had to swat/fix repercussions of bugs that weren't apparent pre-launch (as well as doing via the console things users needed but weren't able to do because the feature to do it hadn't been built yet). It was daunting. I don't know how I'd have managed without vim + vim-slime. Probably a lot of copy/pasting from vscode. Vim + vim-slime was at least a 2x productivity improvement, and >2x increase in developer happiness. Another huge benefit of vim and vim-slime is it is immediately valuable when you use/learn any new language. So long as the language has a REPL/console/interpreter that can be opened from the terminal or terminal emulator in any form (e.g. CL, ruby, python, bash etc etc etc) then vim + vim-slime will be a brilliant ~IDE. (Possibly the only thing I haven't been able to do but wanted to is 'send' code from neovim to the javascript console in chrome, which would be pretty awesome!) A side note: I found doom-emacs very similar to vim, only needed ~10 or so new keyboard shortcuts to be productive in emacs. (I still much prefer vim, but I'm not so down on emacs). reply quesera 2 hours agoparent> I don't know how I'd have managed without vim + vim-slime. This is interesting -- I've worked with people who swear by common lisp, emacs, and SLIME. I'm happiest with ruby and vim, but I have not tried vim-slime (nor even heard of it before, so thank you!). But FWIW, my strategy for running larger bits of ad hoc code on the ruby/rails console is to: 1. Add the code to a persistent local file (e.g. \"ops_console_tools.rb\") 2. scp the file up to the target machine where I am running the irb/pry console 3. In the console, run `load '/PATH/TO/ops_console_tools.rb'` 4. Run the new code: `Ops::User::CustomReport.run(90.days.ago..)` To keep things a bit more sane, all of the ad hoc ruby code is in modules, e.g. `Ops::User`. And it helps to include some code to clear constant definitions which would otherwise complain to STDERR if you update and reload the file multiple times. None of this is as awesome as SLIME of course, but it's pretty tolerable with a bit of readline-style up-arrow and command-history conveniences. Disclaimer: Of course, running ad hoc code in prod is frowned upon. But we're extolling the virtues of CL in this thread, so I'll confess to breaking best practices in environments where it's permissible! Also this process gives you syntax highlighting while editing without requiring config on target host, and you can include the file in version control for greater formality. reply ofalkaed 12 hours agoparentprev>Looks like vim-slime is essential to how you work with CL slime has some issues for me (obviously not OP) and I am not convinced lisp and vim are a good pair. lem is getting pretty good and improving by the day, find it much better to work with than vim when it comes to lisp and vim is my primary editor. https://github.com/lem-project/lem reply djtango 9 hours agorootparentI have been using Clojure and before that Racket using only vim and vim-surround for almost a decade now. I am sure I have left some productivity on the table not investing in workflows like cider etc but I have gotten a decent workflow using just vanilla tmux, a repl pane and vim-surround The % matcher in vim does so much heavy lifting, I've never felt limited by a lack of slurp and barf I actually wrote my own tiny plugin to send snippets to the repl using nc and I'm still happy enough tearing the clojure repl up and down and copying stuff in by hand because dealing with repl state can be pain. Even though I have at times had repls open for months, there is a freedom in just tearing it all down and up again. Clojure itself has plenty of functions to load in files or snippets to help as well reply ferfumarma 3 hours agoparentprevHow do you learn vim-slime? I have used vim before, so I have basic skills there, but I get lost and run out of time when I try to figure out how the slime model works and how to create a lisp project. Is there a tutorial you followed or a video you found useful? What was your starting fund of knowledge? reply tempodox 12 hours agoparentprevYep, I'm using the `slimv` plugin for vim and the `swank` server in a running `sbcl` instance in a second terminal tab. Since I'm on macOS, I could build a keyboard shortcut in vim that automates opening the 2nd terminal tab with the “Lisp machine + swank” when I say “connect” in vim. slimv/swank practically make vim an IDE for Lisp. reply jwr 10 hours agoprevThat is a very interesting journey — mine was exactly opposite, after many years with Common Lisp, I moved to Clojure and wouldn't even think of going back. I find it intriguing that the author would want to move in the other direction, especially as concurrency was mentioned (one of the main reasons why I initially looked at Clojure). I wonder what it was about babashka that didn't work for the author. When I need a quick script with batteries included, I use babashka and it works great. reply djha-skin 10 hours agoparentI had already written large, nontrivial apps (linked in article) which required more libraries than babashka was written with, including ones I had written but also others. I therefore needed to run native-image on my own codebase, as it was not runnable from within babashka (at the time? I don't know if it is now). Running native-image on an already established, not-written-for-it codebase is a nightmare. I just tried again some months ago on the linked code bases. native-image wouldn't budge. Kept getting hung up on I don't even know what, the errors were way too opaque or the app just misbehaved in weird ways. reply jwr 9 hours agorootparentOk, that explains why babashka wasn't suitable. I still wonder, though, about the requirement to have an executable. I still remember many years of reading comp.lang.lisp, where the #1 complaint of newcomers was that \"common lisp cannot produce a native executable\". I remember being somewhat amused by this, because apparently nobody expected the same thing from other languages, like, say, Python. But apparently things have changed over the years and now CL implementations can produce bundled executables while Clojure can't — how the tables have turned :-) reply anta40 7 hours agorootparentI think various Lisp implementations have their own way to do it, e.g save-lisp-and-die on SBCL. But, if what you mean \"executable\" is \"small compact executable like the one build by C/C++/Pascal, without extra Lisp runtime attached\", perhaps you better look at something else, well like C. reply shawn_w 5 hours agorootparent`uiop:dump-image` lets you do it in an implementation independent way. reply whartung 2 hours agorootparentprevChicken Scheme makes very nice, tidy binary executables. \"Hello World\" is just 27K on Ubuntu. reply tiberious726 6 hours agorootparentprevYou just need to use a lisp implementation that has a tree-shaker if you really care about binary size for since reason reply lispm 8 hours agorootparentprevThere is already confusion. Things are different and the same words (executable, native, image, ...) mean slightly different things. In the CL world it is usually relatively easy to create an executable. For example in SBCL I would just run SBCL and then save an image and say that it should be an executable. That's basically it. The resulting executable * is already native compiled, since SBCL always compiles native -> all code is AOT compiled to machine code * includes all code and data, thus there is nothing special to do, to change code for it -> the code runs without changes * includes all the SBCL tools (compiler, repl, disassembler, code loader, ...) thus it can be used to develop with it -> the code can be still \"dynamic\" for further changes * it starts fast Thus I don't need a special VM or special tool to create an executable and/or AOT compiled code. It's built-in in SBCL. The first drawback: the resulting executable is as large as the original SBCL was plus any additional code. But for many use cases that's what we want: a fast starting Lisp, which includes everything precompiled. Now it gets messy: In the real world (TM) things might be more complicated: * we want the executable to be smaller * we want to get rid of debug information * we need to include libraries written in other languages * we want faster / more efficient execution at runtime * we need to deliver the Lisp code&data as a shared library * we need an executable with tuned garbage collector or without GC * the delivery structure can be more complex (-> macOS application bundles for multiple architectures) * we want to deliver for platforms which provide restrictions (-> iOS/Apple for example doesn't let us include a native code compiler in the executable, if we want to ship it via the Appstore) * we want the code&data be delivered for an embedded application That's in the CL world usually called delivery -> creating an delivered application that can be shipped to the customer (whoever that is). This was (and is) typically where commercial CL implementations (nowadays Allegro CL and LispWorks) have extensive tooling for. A delivered LispWorks application may start at around 7MB size, depending on the platform. But there are also special capabilities of ECL (Embeddable Common Lisp). Additionally there were (and still are) specialized CL implementations, embedded in applications or which are used as a special purpose compiler. For example some of the PTC Creo CAD systems use their own CL implementation (based on a ancestor implementation of ECL), run several million lines of Lisp code and expose it to the user as an extension language. reply bigpeopleareold 3 hours agoprevI am looking at CL myself, but my needs are more hobby than anything, but I want to convince myself I can find it useful for my own work in certain things (e.g. I want to maybe use Bike to run C# code in an SBCL REPL.) The feedback loop one gets with it is insanely fast, even faster than Python (certainly there are exceptions even with Python ...) That's a blessing and curse for me - the tighter a feedback loop, the harder for me to get out of a problem I am stuck on. :) But, so far, I felt like I can write a thing and not worry about running it ... type a thing, quickly get it running it in a running REPL loop. If a mistake happens, I can fix the issue right there, instead of just a long stack trace. For what it is worth though, I have been doing this in Emacs for a long time (well, for small functions), but didn't think much of it until now. reply Naru41 8 hours agoprevMore than decade ago, I didn't understand an actual value of Lisp, but I remember this song well. https://www.youtube.com/watch?v=HM1Zb3xmvMc reply tiberious726 6 hours agoparentI always preferred this one: https://youtu.be/5-OjTPj7K54?si=TYuCLJ9_2WvRHcHM reply emmanueloga_ 3 hours agorootparenthttps://www.youtube.com/watch?v=a0YrCABCOEY reply crispyambulance 7 hours agoparentprevEvery programming language should have a music video! reply aidenn0 2 hours agoprevAs a counterpoint to author's use of vim-slime (not to say I don't believe author's commet of \"I'm Okay, I Promise,\" but rather to communicate to others who are facing a similar choice: I am a lifelong vim user (since elementary school in the early '90s), and I developed common lisp using vim for over a decade. I still use vim for nearly everything, but emacs as my Lisp IDE. Before evil-mode, I used the mouse and menus for 90% of what I did, and it was still an improvement over the best vim had to offer at the time (vim-slime existed back then, but would crash or hang regularly). Author's vim setup is fairly good, but Emacs/slime is still better. They stopped using emacs because of RSI, but their vim setup defaults to \"v a ( C-c C-c\" to accomplish something that is \"C-c C-c\" in emacs/slime. They have altered it to be \"v a (g\" which begs the question of \"why not remap keys in emacs?\" reply pntripathi9417 12 hours agoprevI have been working with Clojure for 5+ years now. For CLI applications babashka has worked quite well for us. Would love to know more about the problems you faced. In my experience whenever I faced such issues - it has been because I am not using it well. For CLOS kind of things I have found https://github.com/camsaul/methodical library quite well and the performance is better than default multimethods in core clojure implementation. reply chii 13 hours agoprev> spent long, hard hours banging my head against native-image and it just wasn't working out. it would be nice to know what exactly isn't working out and what the problems with native-image was. Coz i think clojure is as close to perfect, imho, as a language can go without selling out. reply huahaiy 12 hours agoparentGraalvm native image for Clojure is a solved problem. Just add this library to the project and add a flag to native image command line. https://github.com/clj-easy/graal-build-time This initializes Clojure classes at build time and it mostly works for pure Clojure code. Doing complicated things (e.g. depending on native library, etc.) requires some tweaking. For example, a few packages may need to be declared as initialized at build time or run time, depending what they are doing. And any unresolved classes need to be added to reflection-config.json. All these are easily discoverable if one talks to people in the Clojurian slack channels. Clojure is a small community, so it helps to be part of it, because there are not a lot of open materials on the Web. reply IshKebab 11 hours agorootparent> solved problem except... > mostly works > requires some tweaking > discoverable if... I know nothing about Clojure but from your caveats I think I can see why he spent hours banging his head against a wall. reply chii 10 hours agorootparentwhen engineers say it's a solved problem, they mean it in the same way as a mathematician saying a theorem is trivially proved. reply macmac 10 hours agorootparentprevLook at the hoops OP had to jump through to get SBCL working on Windows. I think Graal would compared favourably with that. reply IshKebab 2 hours agorootparentProbably, but that doesn't mean Graal is good; it just means they're both bad. Compare it to something like Go or Rust where there are no hoops and they're both well supported on Windows and Mac. I haven't actually used it but I believe Zig has very good cross platform support too. reply anthk 7 hours agorootparentprevWhat did you mean? Can't the user just download Portacle and use it? https://portacle.github.io/ Problem solved. reply chii 6 hours agorootparentRight at the top of the article, the author outlines the requirement was that it must be usable within vim. reply anthk 6 hours agorootparentNot an issue for Common Lisp, you can use whatever you like, but interacting with a REPL gives you superpowers. reply macmac 4 hours agorootparentprevNot really. OP needs to build executables. It is documented here: https://blog.djhaskin.com/blog/release-common-lisp-on-your-f... reply djha-skin 10 hours agorootparentprevI actually just dusted off my old Clojure stuff to see if it was a \"solved problem\", and it isn't. I grant that it might be described thus if I started out with that stack, but trying to retrofit an older code base with it is, I have found, next to impossible. You have to code around the myriad gotchas as you go or you're never going to identify all those landmines going back over it after the fact. The errors and bad behaviors are too difficult to identify, even for the `native-image` tooling. reply BreakMaker9000 12 hours agoprevWondering whether a dialect like Jank [1] may be worth a shot? [1] https://jank-lang.org/ reply cylinder714 4 hours agoparentIts author is quitting his job to work on it full time: https://jank-lang.org/blog/2025-01-10-i-quit-my-job/ reply stevebmark 13 hours agoprevI don't understand the \"Requirements Met\" section, that reasoning applies to almost any programming language. You chose Common Lisp because there's a JSON library? reply neuroelectron 11 hours agoparentA lot of these intermediary languages are not trivial to parse safely and are a vector for exploits. It's not something you can really do on your own unless you're just supporting a specific subset for your application. Even then, you really need to know what you're doing. reply forgotpwd16 9 hours agoparentprevThere's a section \"hunt for new Lisp\". It isn't explicitly stated in the requirements maybe because it can be inferred from there that being a Lisp is also one. reply IshKebab 11 hours agoparentprevYeah I thought he would go with Rust or Go after seeing those requirements. Clearly there was another implicit requirement - maybe it had to be a niche language? reply weikju 10 hours agorootparentProbably had to be a Lisp, considering the OP was coming from Clojure. Rust and Go fail that (unwritten) requirement. reply quibono 8 hours agoprevThe author says they went about learning CL the wrong way. I wonder if there is a standard \"community approved\" way of learning the language? reply lispshillin291 45 minutes agoparentGentle Introduction to Symbolic Computation is a great book, I learned a lot. The 1990 version available for free below has aged well but you need to look elsewhere for getting setup with emacs and slime or whatever environment you want. https://www.cs.cmu.edu/~dst/LispBook/ reply wwfn 2 hours agoparentprevNot standard, but hopefully worth mentioning: the thing that's clicked best for me is the docs on https://ciel-lang.org/ (\"batteries included\" Common Lisp image). The examples for how to use it's curated libraries matches how I try to integrate a new language into my toolbox. It hit the front page a while ago too: https://news.ycombinator.com/item?id=41401415 reply susam 8 hours agoparentprevI began learning Common Lisp (CL) from the Common Lisp HyperSpec (CLHS): . When I began learning CL about two decades ago, I did know of any other easily available source, so CLHS was my only source back then and I think it has served me well. A popular recommendation these days is Practical Common Lisp (by Peter Seibel): . reply brabel 11 hours agoprev> I wrote this blog post because I noticed that there have been more newcomers on the Common Lisp Discord Even CL people are using Discord now? People really do seem to love to converge to a single place. reply anonzzzies 11 hours agoparentUnfortunately as all the interesting stuff disappears when the server closes. I know it can be remedied, but unfortunately it's not standard. Especially for CL this is crappy as a lot of things are still valid and working 10 years from now but the discord server is long gone. reply susam 7 hours agoparentprevMy go-to community for Common Lisp has always been and likely always be the #commonlisp channel on Libera IRC. The community formerly existed on the #lisp channel (if I remember correctly) of Freenode for several decades. It migrated to Libera after a controversial change in Freenode management in May 2021. Webchat link for #commonlisp: https://web.libera.chat/#commonlisp Further, the on-topic #commonlisp channel on Libera comes with a cozy off-topic channel named #lispcafe for general chit-chat about any imaginable topic. Webchat link for #lispcafe: https://web.libera.chat/#lispcafe reply diggan 7 hours agoparentprev> Even CL people are using Discord now? People really do seem to love to converge to a single place. Except Discord servers cannot be described as \"a single place\" even, as they're all isolated from each other. Instead of spread out across multiple open IRC networks/channels, developer communities converged into silo'd, closed/proprietary Discord servers. It's a shame. reply anthk 8 hours agoparentprev#lisp and it's offtopic channel #lispcafe at irc://libera.chat are far better. Also, comp.list at Usenet. reply fuzztester 2 minutes agorootparentDid you mean comp.lang.lisp ? Because that's the usual convention, like comp.lang.c, .python, etc. reply thih9 11 hours agoprevRelated, Janet: https://janet-lang.org/ I especially like its github readme and the FAQ there, provides a good amount of context about the project: https://github.com/janet-lang/janet reply draven 8 hours agoparentIt's mentioned in the article: > If I had heard about Janet when starting this hunt, I might have stopped there and not gone on to CL. Nice syntax, small, fast executables, C FFI, a fun intro book. It checks all my boxes. reply rcarmo 4 hours agoparentprevIt's great, but I found the library ecosystem lacking for my particular use cases . The joy web framework, in particular, seems to have stalled in time. reply neilv 7 hours agoprevCommon Lisp is a great choice for many purposes. (And, if you're doing a startup, a fringe language like CL is a good way to find and attract some of the best hackers, and avoid all the Leetcode grunts.) Just comments on the Scheme/Racket parts... > I looked at Scheme, but that community seemed to still be fractured over the r6rs/r7rs mess. Things were fractured before R6RS, with very little portable ecosystem code, and then R6RS didn't solve that, but AFAIK, people got back up on that horse, started over, and have been embracing R7RS. > Also, there wasn't a big enough ecosystem to suit my liking. There's a reasonably-sized ecosystem, but three things: 1. Unless things have changed recently, much of the good stuff is still in the package system for a particular implementation (like Racket, \"https://pkgs.racket-lang.org/\", or Chicken, \"http://eggs.call-cc.org/5/\"). 2. It's nothing the size of Python or JavaScript. 3. Don't believe any claims of \"batteries included\"; you often have to roll your own basic packages for real-world work. (But this can actually be a blessing, even for startups that have to move fast, depending on what you have to do, and how capable you're willing to rise to be.) > I'd already tried Racket in school and didn't like it. The runtime was a bit slow and bloated for my tastes. Although CS professors are some the greatest friends of Scheme (having designed and made much of it), CS professors can also be the worst enemies for real world use of Scheme. Before Racket, most people who had heard of Scheme knew it only from school, in problem set homework for the dense SICP course, or from whatever pet intro CS textbook their professor wrote. That's a good way to never want to try Scheme again. (And it got worse as many CS departments became optimized Leetcode->FAANG hiring funnels.) Then people never saw Scheme used, nor even described by, real-world programmers, for real-world things. So Racket (PLT Scheme) comes along, and half of the handful of people thinking of Scheme as a language for real things gravitate to Racket, because they are doing some real-world things. And overall it's one of the best programming languages out there. But still, although the Racket CS professors include some great programmers, Racket is determined by CS professors, who do PL and CSE research, and write and teach textbooks. So, Racket's use in intro CS classes seems to perpetuate the tradition of CS professors ensuring that students will never again want to touch Scheme after they get a passing grade for the class. reply nsm 2 hours agoparentRacket's Chez Scheme backend, which is the default for several years is faster than python and ruby, and raco distribute gets you a smallish package with just your program, portable to the major OSes. Smallish = 40mb, which is probably comparable to other dynamic lands that don't come preinstalled. I find the \"I tried it years ago and it wasn't as nice then, so I'm not gonna look at it again\" attitude quote off-putting. It's the same reason Java still gets so much hate after over a decade since Java 8. I agree that CL and Closure probably has more real world shipped products. However, if you are considering a newer and smaller ecosystem like Janet... you owe it to yourself to look at Racket with fresh eyes. reply joshlemer 1 hour agoprevDoes anyone use Clojure CLR? How is the startup time for that? reply pjmlp 9 hours agoprevWith the great tooling Common Lisp commercial systems inherit from Lisp Machine and Interlisp-D days, it is kind of sad seeing vim being the option. Also unless we're about Allegro or LispWorks with their own additions, the Java ecosystem tends to have more choice of mature libraries, I think ASDF isn't kind of spoiled by choice as Maven Central. But to each their own. reply anthk 8 hours agoparentThere's Emacs and Slime too OFC. Also, Quicklisp/Ultralisp aren't ASDF, you are pretty outdated... https://www.quicklisp.org/beta/ https://ultralisp.org/ reply pjmlp 5 hours agorootparentAs someone that knows Emacs since XEmacs glory days, I am surely not outdated regarding Emacs, Slime, and what they still miss from commercial Common Lisp experience. As for Quicklisp, I may not be up to the latest and greatest, yet I doubt they are at the same level as Maven Central. reply packetlost 12 hours agoprevI went on a similar journey a couple of years ago and ended up on Gerbil Scheme instead. reply tasuki 3 hours agoparentCare to explain why? reply sundarurfriend 11 hours agoprevTo the extent that Julia is a Lisp (which requires some squinting and handwaving), I wonder how it stacks up against these requirements. With my limited knowledge: 1. Standalone Executables: The biggest current obstacle right away! But I believe this (as in compilation to standalone, small executables) is coming with the next version (Julia 1.12) in an early form, so maybe stabilized and reliable within this year? There does seem to be a lot of momentum in this direction. 2. Vim Workflow: vim-slime works well to my knowledge, and the overall support (eg. treesitter, LSP, etc.) is pretty good, even if VS Code is the \"main\" supported editor. 3. Windows/Mac/Linux Support: mostly Tier 1 support [https://julialang.org/downloads/#supported_platforms] 4. Larger Imperative Ecosystem: FFI with both C and Python are pretty standard and commonly used. 5. Runtime Speed: Crazy fast as well 6. Multithreading: Base language support is already pretty good, and there's OhMyThreads.jl [1] and data chunking libraries and many other supporting libraries around multithreading. 7. Strong Community: I'd expect Julia and CL communities to be on the same order of magnitude? Complete assumption though, in both directions. Web presence is mostly on the Discourse [2] and Slack, and the JuliaCons are pretty well attended. 8. Ecosystem: Since package management is mentioned, I'll shout out the built-in Pkg package manager, the seamless virtual environment support, and the generally quite good versioning in the ecosystem, all of which add up to a really good experience. As for particular libraries, JSON is the only one I know the answer to: JSON3.jl is a solid, standard choice. I don't know if SQLite.jl [3] would be the recommended option for SQLite or something else, HTTP.jl does the job for HTTP requests but I believe isn't particularly fast or sophisticated, and I could believe there's a subcommunity within Julia that uses \"functional data structures\" but I wouldn't even know where to look. But, for the ex-Clojurian, may I present Transducers.jl [4] as worth a look? [1] https://juliafolds2.github.io/OhMyThreads.jl/stable/ [2] https://discourse.julialang.org/ [3] https://github.com/JuliaDatabases/SQLite.jl [4] https://github.com/JuliaFolds2/Transducers.jl reply pseudony 9 hours agoparentLast I checked, Julia actually compiled rather slowly, making development a lot less fluent than a Lisp. Beyond that, wrapping C code from Julia is neither nicer nor worse than from CL. Wrapping C code is basically done everywhere except for a few outlier languages (Go comes to mind. It IS possible, but it means using cgo, which is its own world). I liked Julia well enough, but the compile times were slow enough to be painful for me. All the best though to Julia :) reply sundarurfriend 4 hours agorootparent> Wrapping C code is basically done everywhere except for a few outlier languages Agreed, I only included this because the author mentioned it explicitly as a requirement. From a \"allows plugging into some other, large-community imperative language, like Clojure does with Java\" perspective - in terms of library access - the combination of having well-polished interfaces to both C and Python is pretty powerful though. > the compile times were slow enough to be painful for me I think this, and the developer experience in general (eg. linting, IDE support, etc.), has been the biggest reason the excitement for Julia dampened over time, despite it being a wonderful language in theory. It's been getting better, from \"painful\" to just \"somewhat annoying\" for me, but not quickly enough to turn it around (IMHO). reply Archit3ch 5 hours agoparentprev> 6. Multithreading: Base language support is already pretty good, and there's OhMyThreads.jl [1] and data chunking libraries and many other supporting libraries around multithreading. Agree on the rest, but multithreading in Julia is a let down. The manual (https://docs.julialang.org/en/v1/manual/parallel-computing/) claims it's composable, but that's only true if you stay within Julia with the paradigm of Tasks. As soon as you interface with C/C++, you get a SegFault, as the Julia runtime expects to adopt [1] all threads calling into it. This is not always viable. Julia should offer the option of C-style pthreads (or the Windows equivalent) and let others build abstractions on top of them. [1] And that option was only added recently. reply JanisErdmanis 4 hours agorootparentMultithreading seems to work just fine with OpenBlas. It is also sometimes possible to wrap the underlying state machines from C/C++ code and then making it multithreaded in the Julia side. reply Archit3ch 3 hours agorootparentNone of that contradicts what I wrote. Note that it's calling into Julia from C/C++ that presents problems, the opposite is fine. reply djha-skin 10 hours agoparentprevMfiano wrote about this. https://mfiano.net/posts/2022-09-04-from-common-lisp-to-juli... . (By the last report, mfiano came back to CL.) Refutation: https://gist.github.com/digikar99/24decb414ddfa15a220b27f674... reply ykonstant 7 hours agorootparentThe first link doesn't seem to be working. reply lioeters 4 hours agorootparentArchived page: https://web.archive.org/web/20221129120058/https://mfiano.ne... reply vindarel 8 hours agoparentprevfood for thought and feel free to chime-in: https://gist.github.com/vindarel/15f4021baad4d22d334cb5ce2be... Common Lisp VS Julia reply sundarurfriend 8 minutes agorootparentThe \"feedback and answer\" below the gist already covers a bunch of things I wanted to mention. So I'll skip those and only talk about the rest: > You can't make it a CLI script, because it compiles the whole code WITH dependencies every time The \"with dependencies\" part is mostly untrue nowadays, with constantly better pre-compiled caches for dependencies with every release. The overall issue of compile times getting in the way of interactive development still has some truth to it, but much less than the comment implies. > https://viralinstruction.com/posts/badjulia/ 1. The subheadings in the ToC are mostly based on comparisons with the best-in-class: for eg. \"Julia can't easily integrate into other languages [compared to C]\", \"Weak static analysis [compared to Rust]\". 2. Seeing this actually gave me hope about Julia's progress, based on how many of these issues have been largely mitigated or have been actively worked on in the last three years since the post. 3. As a side note, the author of the post is still an active user of and contributor to Julia, so I think this kinda falls under the \"There are only two kinds of languages: the ones people complain about and the ones nobody uses\" banner. As in, the complaints are there because they like and actively use the language enough to want it to be the best in every way. > Even though Julia 1.6 was a lot faster than 1.5, it still took too long. I agree - I think pre-1.9 Julia experience sucked, and overselling the language in that period hurt its reception a lot. (I've mentioned elsewhere in the thread that the developer experience is still one of the weaker points of Julia.) > (in CL a hello world weighs ±20MB): In Julia 1.12, with the experimental --trim feature, a hello world isfalse ? 1 : 2 : 3 : 4 This is hilarious and awful at the same time. There's no beating CL in this - I've learnt that every language with syntax unfortunately develops \"wat\"s like this over time when well-intended syntax features interact. > A few months ago, I tried to write a program that should receive UDP packets over IPv6 multicast. > It didn't work. I never figured it out. This works in Java and Python. > This might be unfair or untrue, but I get the feeling that it doesn't work because no one has seriously tried to use the language this way. I don't think it's either of those: it seems like networking was and remains a weak area in Julia. For eg., though the language itself is blazingly fast, there have been a bunch of reports about how HTTP traffic performance is several time slower than ostensibly slower languages like Ruby. The reason is probably what the quote says too, there just isn't as much of a userbase or demand for this side of things. > my packages seem to really like breaking when I try to load them about once a month There's no source for this one, and no info on what \"breaking\" means or what the packages do, so I can only say this isn't a common experience. It's very easy to \"pin\" dependencies to preserve a reproducible state of dependencies and replicate it as well, which is greatly useful in a language used for science. > I migrated from Lisp to Julia for the ecosystem. It hasn't been worth it from my point of view. I'll migrate back to Lisp eventually. [on a post] about lisp-stats I'm not very surprised, given the lisp-stats context - it seems to be a common assumption/misconception, because Julia gets compared to Python and R often, that it's a data science and stats focused language. It's great for greenfield stats work, pleasant in many ways compared to those two, but the ecosystem is not particularly focused specifically on it. I'd suggest choosing Julia for the ecosystem if you're doing science science - quantum computing, diffeq modeling, numerical optimization, many others - but on the data science side, what Julia offers is consistent syntax, performance without C reliance, while also having some niceties like DataFrames and Tidier that other languages offer. reply Volundr 12 hours agoprevIf your looking to write CLI utilities in Clojure babashka really is awesome. It doesn't meet the author's standalone binary requirement, but it's got great startup time and comes batteries included with all sorts of helpful tools. reply Horffupolde 4 hours agoprevCommon Lisp is great until it isn’t. reply Guthur 4 hours agoparentAnd then you grab a macro, extend it and it's great again ;) reply darthrupert 8 hours agoprevAh, it's this time of the year when we get to fantasize about cool platforms and languages before succumbing back to python, typescript or feeding the relentless AI monster in a major cloud provider. reply worthless-trash 5 hours agoparentBecause we are not allowed to program in another language that 'most' programmers can't understand immediately. We are cursed to use the lowest common denominator of choices of programming. reply billmcneale 12 hours agoprevtl;dr: OP was using a Lisp and they were looking for a different Lisp. Probably the only reason why anyone would ever pick Common Lisp for a new project in 2025. reply anonzzzies 12 hours agoparentWhy? Not everyone is resume grifting. It is fast, solid and has excellent developer workflows. Lot of stable (oh no, no updates for 10 years because it just works!) libraries. With CLOG it is a nice secret weapon with people wondering how you managed to move that fast. At least in our experience but we make products so we don't have to explain what it is made of or why. reply f1shy 4 hours agorootparent> no updates for 10 years because it just works! Maybe I’m crazy, but that is what I like from Lisp reply anonzzzies 3 hours agorootparentyep, same. reply bmacho 8 hours agoparentprevLmao, my thoughts exactly. Q: why I chose common lisp A: I was looking for a lisp to begin with This is almost like a satire. But I've found it rather funny. It presents question and provides an answer which is contradictory at first but still makes sense. reply BoiledCabbage 1 hour agorootparent> Q: why I chose common lisp > A: I was looking for a lisp to begin with > This is almost like a satire How? It's no different than \"Why I chose Arch Linux? I was looking for a Linux to begin with.\" To even think that's satire is to completely miss the point. reply linkerdoo 10 hours agoprevnext [2 more] [flagged] dokyun 10 hours agoparentYou'll lose first. reply linkerdoo 10 hours agoprevnext [2 more] [flagged] mseepgood 9 hours agoparentWin what? reply transfire 4 hours agoprevJank reply paines 10 hours agoprev [–] He cannot use Emacs and then goes to ... Vim ?!?! Nothing against Vim or Emacs, I love both but they had their time which is long gone. I am using Linux ans OSS technolgies since 95 and would have never imagined to advocate a MS product, but just use VS Code. It's awesome. VS Code managed to by-pass the qualitiy and amount of extensions/plugins in a fraction of time Emacs took decades. reply siknad 8 hours agoparentVS Code support for Common Lisp is lacking. Alive extension is relatively recent and is a solo effort and thus has significant bugs and is not as feature packed as Vim/Emacs alternatives. For example, it doesn't provide structural editing. It's interaction with sbcl cache seemingly broke my project a few times. reply Dansvidania 10 hours agoparentprevLots of people work with Vim and Emacs day to day, what makes them \"long gone\" in your opinion? reply snozolli 10 hours agorootparentNot GP, but I've always found it weird how many people are obsessed with vi/vim and/or Emacs. I get some of the extensibility appeal of Emacs if you're a Lisp fan, but fundamentally I don't understand the appeal of \"programming your brain\" just to edit code. 90% of my code editing time is spent reading and thinking, not writing or modifying. Memorizing and minimizing (e.g. VimGolf) editor syntax seems like a massive waste of time and cognitive function to me. Modern IDEs have you up and running instantly, and their refactoring tools are really amazing. I feel like there's been a boom in \"editor hipsterism\" in the last 10 - 15 years, while everyone has forgotten the variety of novel editors that were made in the 80s and 90s (I've forgotten them, too, I just remember seeing ads and reviews in magazines as a young programmer). For context, I do have a basic understanding of vim because I run it on servers, but my knowledge doesn't go far beyond search and replace. reply nanolith 3 hours agorootparentTo each their own. With Vim, Unix is my IDE. I don't know about the recent interest in these editors that you mention. I've been using vi/Vim for the past 30 years. I take it to every project and job. My fingers already know what to do. I've watched colleagues who I started working with 20 years ago as they've retooled on the latest hotness every 4-5 years. Visual Studio, Netbeans, Eclipse, Jetbrains, VS Code, etc. It doesn't take long to learn to use a new IDE, but they are definitely shorter term investments. I can do more or less the same thing most folks can with an IDE; I just use external tools. I wouldn't claim that Vim is somehow superior. It's just what I use. Every now and then, I noodle a bit on a personal editor that is to ed what Vim is to vi. At some point, I'll migrate to it. I think there is a bit of a different philosophy that the editor folks have. I can't speak for them, but I can speak for me. I like to feel closer to the code base. I like to have more of it in my head. The best analogy I've found is that using an editor like Vim or Emacs is closer to driving with a manual transmission and with tight steering controls, compared to driving with an automatic transmission with partial self-driving features found in modern cars. There is definitely something to be said about things like adaptive cruise control, lane keeping assist, GPS navigation, etc. But, if you talk to a manual transmission enthusiast, there is a thrill of feeling closer to the road and being more engaged. Both folks arrive at the destination in the same amount of time. But, if you ask each about their experience, they will have much different views of the drive organized in their head. reply snozolli 1 hour agorootparentTo each their own. And yet I get down-voted for expressing a well-reasoned opinion against vim and Emacs. I've been using vi/Vim for the past 30 years. I take it to every project and job. I've rarely used an IDE that didn't allow custom key bindings, often with the ability to select a set from a drop-down list to match other IDEs. I've been using mostly the same keyboard shortcuts across IDEs for over 20 years. if you talk to a manual transmission enthusiast, there is a thrill of feeling closer to the road and being more engaged Funny you should say that, because I regularly enrage these types by pointing out that if they can't stay engaged as a driver with an automatic transmission, then the problem is with them, not the car. This is a quasi-religious ritual with these people, and a very low-effort way to get a sense of superiority over others (i.e. every driver on the road before ~1970 had experience with a manual transmission and literally anyone can learn in a few hours. It's not a skill to be proud of). reply nanolith 18 minutes agorootparentand a very low-effort way to get a sense of superiority over others... literally anyone can learn in a few hours. I agree that it is a skill that is easy to learn. The same is true of IDEs. This isn't about skill or superiority, but comfort. Some folks are more comfortable being closer to the machine or the road, as it were. Others are more comfortable having some automation between them and the machine. I think that the better to consider this a matter of personal preference. The IDE adds a layer of abstraction, and abstraction can be leaky. If you are comfortable with the abstraction, and with the opinionated choices the IDE makes, that's fine. If you are not, that's also fine. All that I ask when I'm bootstrapping a project with a team is that projects be arranged such that they are IDE / editor agnostic. Use standard build configuration / build tools that have appropriate plugins for IDEs, and can also be run in the terminal / command-line. Then, the individual developer can choose to use whichever editor or IDE that developer is comfortable using. reply bcrosby95 9 hours agorootparentprevI like vim because the keybindings are familiar everywhere. For small server stuff I use vim, for most coding I use Doom Emacs (vim keybindings), and for Java I use Intellij with vim keybindings. I mostly use Emacs because of org mode. It's way better than anything else trying to fill this hole. Otherwise I'd probably just use VSCode. But I don't want to add yet another editor to my regular use. reply f1shy 4 hours agorootparentprevEmacs provides far more than just editing. Helps a lot with reading and VC (magit). Just magit would IMHO justify Emacs. reply Dansvidania 8 hours agorootparentprevi prefer to use the mouse as little as possible, i feel more productive when I can stay on the home-row of the keyboard, that is primarily it for me. This is because hotkeys are more direct, exact and easier to memorize than mouse motions it helps that vim bindings are adopted in many places so learning and using them ports well to browsing and even managing windows (vimium and aerospace respectively) secondarily, while i don't think using the terminal is generally better than GUI I tend to work in the terminal anyway, so keeping text editing there makes sense. reply anthk 7 hours agorootparentprevWith vim you run factoring tools as an external tools. Massive wasting of time? with vim you can do something in seconds that with an IDE you would last minutes if not ours. Check out: - entr to run commands on modifying files/directories - plain Makefiles to run your code: git://bitreich.org/english_knight - LSP and alike tools for your language reply snozolli 1 hour agorootparentMassive wasting of time? I feel like you only read half of that sentence. entr to run commands on modifying files/directories Alt-Tab to the command console that I always have running. plain Makefiles to run your code I have no idea what the advantage is here. F5 to run my code, including scripted deployment. LSP and alike tools for your language I don't know what this means. reply dannymi 5 minutes agoparentprev [–] Thanks, but no thanks. I don't like vscode extensions advertising to me every 5 seconds, auto-downgrading the free versions of extensions, auto-installing aux tools every 5 seconds, having a 400 MB RSS chromium runtime (remember Eight Megabytes And Constantly Swapping? VS code is much worse; and it's also just a plain text editor); nerfing the .net debugger and breaking hot reload on purpose in VSCodium; telemetry, .... it's so noisy all the time. You are using this? On purpose?! VS code is basically the same idea as emacs, just the MVP variant and with a lot of questionable technology choices (Javascript? Electron? Then emulate terminal cells anyway and manually copy cell contents? uhhh. What is this? Retrofuturism?) and done with the usual Microsoft Embrace-Extend-Extinguish tactics (nerfing pylance, funny license terms on some extensions that the extensions are only allowed to be used in their vscode etc). Also, if you use anything BUT emacs for Lisp development, what do you use that doesn't have a jarring break between the Lisp image and you? vim seems weird for that use case :) emacs is very very good for Lisp development. On the other hand, VSCode for Lisp is very flaky and VSCode regularily breaks your Lisp projects. Did you try it? Because of your comment I tried VSCode again and now about 20 extensions (one of them \"Alive\", a Lisp extension for vscode) complain about now missing \"Dev container: Docker from Docker Compose\" (keep in mind they worked before and I didn't change anything in vscode--I hadn't even run VSCode for 8 months or so) and when I try to fix that by clicking on the message in the extension manager the message immediately disappears from all 20 extensions in the manager (WTF?) and I get: >>./logs/20250112T181356/window1/exthost/ms-vscode-remote.remote-containers/remoteContainers-2025-01-12T17-13-58.234Z.log: >>>> Executing external compose provider \"/home/dannym/.guix-home/profile/bin/podman-compose\". Please see podman-compose(1) for how to disable this message. a239310d8b933dc85cc7671d2c90a75580fc57a309905298170eac4e7618d0c1 >Error: statfs /var/run/docker.sock: no such file or directory >Error: no container with name or ID \"serverdevcontainer_app_1\" found: no such container ... because it's using podman (I didn't configure that--vscode did that on its own, incompletely. Also, apparently it thinks it's 2010 and having a docker/podman service running as root is a thing--so not using rootless podman apparently. Funny thing is I use podman extensively. I don't wanna know how bad it would be if I HADN'T set podman up already). So it didn't actually fix anything, but it removed the error message. I see. And there's no REPL for the editor--so I can't actually find out details, let alone fix anything. I had thought emacs DX was bad--but I've revised my opinion now: compared to vscode DX, emacs DX is great. You live with VSCode if you want to. And note, vscode was made after emacs was made. There's no excuse for this. I think this now was about all the time that I want to waste on this thing, again. How is this a problem in 2025? shakes head >VS Code managed to by-pass the qualitiy and amount of extensions/plugins in a fraction of time Emacs took decades. Yeah? Seems to me these vscode extensions are written in crayon. Bad quality like that would never make it into emacs mainline. And it's not even strictly about that! I wonder who would write a developer tool that the developer can't easily debug its own extensions in. That flies about as well as a lead balloon. For comparison, there's emacs bufferenv that does dev containerization like this and it works just fine. Configuration: 1 line--the names of the containerfiles one wants it to pick up. Also, if I wanted to debug what it did (which is rare) I could just evaluate any expression whatsoever in emacs. (\"Alt-ESC : «expression»\" anywhere) PS. manually running \"podman-compose up\" in an example project as a regular user works just fine--starts up the project and everything needed. So what are they overcomplicating here? Pipes too hard? PPS. I've read some blog article to make socket activation work for rootless podman[1] but it's not really talking about vscode. Instead, it talks how one would set up \"linger\" so that the container stays there when I'm logged out. So that's not for dev containers (why would I possibly want that there? I'm not ensuring Heisenbugs myself :P). [1] https://github.com/containers/podman/blob/main/docs/tutorial... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author transitioned from Clojure to Common Lisp due to frustrations with Clojure's slow startup times for command-line interface (CLI) applications and perceived lack of community concern.",
      "Common Lisp was chosen for its ability to create fast-starting standalone executables, compatibility with the Vim text editor, support across Windows, Mac, and Linux, and its strong community and ecosystem.",
      "The author shares their experience to assist newcomers in understanding the benefits of Common Lisp and how they adapted to using it effectively."
    ],
    "commentSummary": [
      "The author highlights Common Lisp's dynamic system, which allows problem-solving even without the original source code, showcasing its flexibility and robustness.",
      "SBCL, a Common Lisp implementation, is praised for enabling quick issue resolution, even in scenarios where the source code is unavailable.",
      "The discussion also covers the importance and limitations of version control, particularly when the repository is lost, and includes comparisons of Lisp with languages like Clojure and Julia."
    ],
    "points": 291,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1736653105
  },
  {
    "id": 42672790,
    "title": "AI founders will learn the bitter lesson",
    "originLink": "https://lukaspetersson.com/blog/2025/bitter-vertical/",
    "originBody": "tl;dr: Historically, general approaches always win in AI. Founders in AI application space now repeat the mistakes AI researchers made in the past. Better AI models will enable general purpose AI applications. At the same time, the added value of the software around the AI model will diminish. Recent AI progress has enabled new products that solve a broad range of problems. I saw this firsthand watching over 100 pitches during YC alumni Demo Day. These problems share a common thread - they’re simple enough to be solved with constrained AI. Yet the real power of AI lies in its flexibility. While products with fewer constraints generally work better, current AI models aren’t reliable enough to build such products at scale. We’ve been here before with AI, many times. Each time, the winning move has been the same. AI founders need to learn this history, or I fear they’ll discover these lessons the hard way. In 2019, Richard Sutton started his famous essay “The Bitter Lesson” with: “The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin”. He points out that throughout AI’s history, researchers have repeatedly tried to improve systems by building in human domain knowledge. The “bitter” part of the title comes from what happens next: systems that simply use more computing power end up outperforming these carefully crafted solutions. We’ve seen this pattern in speech recognition, computer chess, and computer vision. If Sutton wrote his essay today, he’d likely add generative AI to that list. And he warns us: this pattern isn’t finished playing out. “As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes (…). We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation” From an AI research perspective, the Bitter Lesson deals with clear definitions of “better.” In computer chess, it’s your win rate; in speech recognition, it’s word accuracy. But this post looks at AI products in the application layer (see Figure 1), where “better” means both performance and adoption in the market. We’ll cover adoption in Chapter 2. For now, let’s focus on product performance - the amount of economically valuable work a product can replace. Better performance means handling more complex problems, which unlocks more value. Figure 1, illustration of different types of AI products. In this post, we talk about the application layer. AI products are typically an AI model wrapped in some packaging software. You can improve their performance in two ways: Through engineering effort: using domain knowledge to build constraints into the packaging software Through better models: waiting for AI labs to release more capable models You can pursue both paths, but here’s the crucial insight: as models improve, the value of engineering effort diminishes. Right now, there are huge gains to be made in building better packaging software, but only because current models make many mistakes. As models become more reliable, this will change. Eventually, you’ll just need to connect a model to a computer to solve most problems - no complex engineering required. Figure 2, illustration of the diminishing returns of engineering effort when building AI products in the application layer. The value diminishes both as more engineering effort is made and as better models are released. The graph above shows how engineering effort becomes less valuable as models improve. Current models have significant limitations, which means companies can still gain a lot from engineering work. I saw this at YC alumni Demo Day, where companies were finding success. The landscape splits into two groups: those with products in production at scale (solving simple problems) - a small group for now - and those targeting slightly more complex problems. This second group is doing well because their proof-of-concepts suggest their goals are achievable with enough engineering effort. But here’s the key question these companies face: will the next model release make all that engineering work obsolete, destroying their competitive advantage? The launch of OpenAI’s o1 model illustrates this risk. I spoke with many founders in the AI application layer who were worried because they’d invested heavily in perfecting prompts to boost performance. But with o1, prompt engineering matters less. As Figure 2 shows, o1 is smarter, but this means there’s less value in the engineering work these companies did. At its core, this engineering effort aims to constrain AI and reduce its mistakes. From observing many solutions, I’ve identified two main types of constraints: Specificity: This represents how focused a solution is. A vertical solution has packaging software built for one specific problem. A horizontal product, in contrast, can handle many different types of problems. Autonomy: This measures how independently the AI can operate. Following Anthropic’s terminology, we have workflows - systems where LLMs and tools follow predefined code paths - and agents - systems where LLMs control their own processes and tool usage, deciding how to complete tasks. These two types of constraints create a framework for categorizing AI products: Loading, please wait Vertical Horizontal Workflow Harvey ChatGPT Agent Devin Claude computer-use Table 1: Classification of famous AI products. Note that ChatGPT likely follows a predefined code path for each message, making it a workflow rather than an agent. Let’s explore how each category might be implemented for the same task: a business analyst creating investment pitch slides. Here’s one possible approach for each: Vertical workflow: A fixed sequence of steps: First, makes a RAG query on a company database, passes this to a small LLM for summarization, then to a more capable LLM that extracts key numbers and uses a calculator tool. The LLM checks if these numbers make sense before writing slide content. Finally, a slide generator creates the presentation. This exact sequence runs every time. Vertical agent: An LLM runs in a loop, using its output from one iteration as input for the next. It has access to the same tools as the workflow version but decides for itself when to use them. The loop continues until the agent determines the results meet its quality threshold. Horizontal workflow: ChatGPT and similar tools could assist with parts of the task, but can’t complete it end-to-end. They lack both the specialization and autonomy needed for the full job. Horizontal agent: Claude computer-use gets access to standard company software. The analyst provides instructions in natural language, and the agent operates the computer like a human would, adapting its approach as needed. Almost all products at Demo Day fell into the vertical workflow category. This makes sense - current models aren’t reliable enough for other approaches. As a result, even problems too complex for vertical workflows are being forced into this mold since it’s the only way to get close to acceptable performance with current model capabilities. While engineering can improve these solutions, there’s an upper limit to what it can achieve. For problems that are out of reach with current models, the better strategy would be to wait for more capable models that can handle them with minimal engineering. As Leopold Aschenbrenner argues in “Situational Awareness,” for many problems, the engineering work will take longer than the wait for better models: “It seems plausible that the schlep will take longer than the unhobbling, that is, by the time the drop-in remote worker is able to automate a large number of jobs, intermediate models won’t yet have been fully harnessed and integrated” This pattern should sound familiar. Let’s return to the Bitter Lesson. AI researchers repeatedly tried to engineer their way to “acceptable performance,” only to be overtaken by more general solutions that simply used more compute. The parallel to how today’s AI products are built is striking. And we can make this connection even clearer by examining how the Bitter Lesson applies to our two types of constraints: Loading, please wait Bitter Lesson Observation Autonomy Specificity 1) AI researchers have often tried to build knowledge into their agents The developer experiments with an autonomous agent but finds it unreliable. Instead, they hardcode the execution steps to follow the workflow that they would go through themselves when solving the task. The developer starts building a general document analysis system but finds it unreliable. Instead, they constrain it to just analyze financial statements, hardcoding specific metrics and validation rules. 2) This always helps in the short term and is personally satisfying to the researcher The developer finds that this increases reliability. The developer finds that specialization improves accuracy since the model only needs to handle a narrow set of documents and metrics. 3) In the long run, it plateaus and even inhibits further progress The constrained workflow sometimes does not give the correct output when faced with novel situations that weren’t considered in the hardcoded steps. The specialized system can’t handle related tasks like analyzing merger documents or earnings calls, requiring separate specialized systems for each type of analysis. 4) Breakthrough progress eventually arrives by an opposing approach based on scaling computation New model releases enable reliable agents that can figure out the right approach dynamically, backtracking and correcting mistakes as needed. New model releases can understand any business document holistically, extracting relevant information regardless of format or type, making specialized systems unnecessary. For problems with unclear solution paths, products with more autonomy will achieve better performance. Similarly, when dealing with large, complex input spaces, less specific products will perform better. This is the first in a four-part series examining the role of startups in AI. We’ve observed a historical pattern: AI models that leverage domain knowledge consistently get overtaken by those that leverage compute. Today’s AI products show striking parallels to this pattern. Though I’ve tried to focus on observations rather than opinions in this first part, my view likely shows through. Building software to compensate for current model limitations seems like fighting a losing battle, especially given how rapidly models are advancing. As Jarred, partner at YC, noted in the Lightcone podcast: “that first wave of LLM apps [vertical workflows] mostly did get crushed by the next wave of GPTs.” Sam Altman has repeatedly advocated for building startups that make you excited for releases of better models, rather than scared of them. Many of the founders in the AI application layer I speak to are excited about model releases, but for the sake of their startup, I don’t think they should be. They might be missing the insight from Figure 2; better models might actually reduce your edge, not enhance it. Of course, this is from the point of view of product performance - building something that can solve harder problems more effectively. In the next part, we’ll explore a different dimension: market adoption. After all, having better performance doesn’t guarantee winning the market. Appendix A: Statistical View of The Bitter Lesson There’s another way to understand the Bitter Lesson using basic statistics. When building models, you typically face a tradeoff. You can either make a model that’s very precise in how it approaches problems (high bias) or one that’s more flexible but less predictable (high variance). The Bitter Lesson suggests choosing the flexible approach. Why? Because with more compute power and data, you can make flexible models more reliable. It’s like having more practice shots in basketball - eventually, you’ll become consistent even with a less rigid shooting form. The reverse isn’t true - a overly rigid approach will always be limited by its built-in assumptions. This maps directly to our discussion of AI products. Vertical workflows and specific constraints are like adding rigid rules - they make the AI more reliable now but limit how good it can eventually become. In contrast, letting AI operate more freely might seem risky today, but it allows the AI to find better solutions as models improve. As we’ve seen throughout this essay, betting against flexibility has historically been a losing strategy. Appendix B: End-to-end vs Feature Engineering Figure 1: Comparison of traditional machine learning, which requires manual feature engineering, with deep learning’s end-to-end approach. The traditional approach needs humans to define what’s important in the data, while deep learning figures this out by itself. Traditional machine learning requires humans to decide what’s important in the data. You take raw input, like an image, and manually extract meaningful patterns or “features” - like counting specific shapes or measuring certain properties. Deep learning, in contrast, learns these patterns automatically. Figure 2: Self-driving car visualization showing feature extraction in action. The system identifies and tracks specific objects like cars, pedestrians, and lane markings. This represents the traditional approach of breaking down a complex problem into smaller, defined pieces. Let’s use self-driving cars as an example. You could build it two ways: Feature engineering: Break down what the car sees into specific pieces - where are the other cars, where are the lanes, how fast is that pedestrian moving? End-to-end: Feed raw video directly into a neural network and let it figure out how to drive. The feature engineering approach feels safer and more controlled. That’s why it dominated early AI. But as George Hotz observed: “if anything about the history of AI has taught us anything, it’s that feature engineering approaches will always be replaced and loose to end-to-end.” Figure 3: Tweet from Sholto Douglas This connects directly to our discussion of AI products. Building vertical-specific tools is like feature engineering - you’re deciding what information matters ahead of time. When you constrain a model’s autonomy, you’re doing the same thing. While this might work better today, history suggests betting on end-to-end approaches will win in the long run. “if anything about the history of AI has taught us anything, it’s that feature engineering approaches will always be replaced and lose to end-to-end.” - George Hotz Follow me on X or subscribe via RSS to stay updated.",
    "commentLink": "https://news.ycombinator.com/item?id=42672790",
    "commentBody": "AI founders will learn the bitter lesson (lukaspetersson.com)285 points by gsky 7 hours agohidepastfavorite217 comments CharlieDigital 6 hours agoThere's only one core problem in AI worth solving for most startups building AI powered software: context. No matter how good the AI gets, it can't answer about what it doesn't know. It can't perform a process for which it doesn't know the steps or the rules. No LLM is going to know enough about some new drug in a pharma's pipeline, for example, because it doesn't know about the internal resources spread across multiple systems in an enterprise. (And if you've ever done a systems integration in any sufficiently large enterprise, you know that this is a \"people problem\" and usually not a technical problem). I think the startups that succeed will understand that it all comes down to classic ETL: identify the source data, understand how to navigate systems integration, pre-process and organize the knowledge, train or fine-tune a model or have the right retrieval model to provide the context. There's fundamentally no other way. AI is not magic; it can't know about trial ID 1354.006 except for what it was trained on and what it can search for. Even coding assistants like Cursor are really solving a problem of ETL/context and will always be. The code generation is the smaller part; getting it right requires providing the appropriate context. reply lolinder 5 hours agoparentThis is why I strongly suspect that AI will not play out the way the Web did (upstarts unseat giants) and will instead play out like smartphones (giants entrench and balloon). If all that matters is what you can put into context, then AI really isn't a product in most cases. The people selling models are actually just selling compute, so that space will be owned by the big clouds. The people selling applications are actually just packaging data, so that space will be owned by the people who already have big data in their segment: the big players in each industry. All competitors at this point know how important data is, and they're not going to sell it to a startup when they could package it up themselves. And most companies will prefer to just use features provided by the B2B companies they already trust, not trust a brand new company with all the same data. I fully expect that almost all of the AI wins will take the form of features embedded in existing products that already have the data (like GitHub with Copilot), not brand new startups who have to try to convince companies to give them all their data for the first time. reply master_crab 4 hours agorootparentYup. And it’s already playing out that way. Anthropic, OpenAI, Gemini - technically not an upstart. All have hyperscalers backing and subsidizing their model training (AWS, Azure, GCP, respectively). It’s difficult to discern where the segmentation between compute and models are here. reply alephnerd 4 hours agorootparent> It’s difficult to discern where the segmentation between compute and models are here. Startups can outcompete the Foundational Model companies by concentrating on creating a very domain specific model, and providing support and services that comes out of having expertise in that specific domain. This is why OpenAI chose to co-invest in Cybersecurity startups with Menlo Ventures in 2022 instead of building their own dedicated cybersecurity vertical, because a partnership driven growth model nets the most profit with the least resources expended when trying to expand your TAM into a new and very competitive market like Cybersecurity. This is the same reason why hyperscalers like Microsoft, Amazon, and Google themselves have ownership stakes in the foundational model companies like Anthropic, OpenAI, etc because at Hyperscalers size and revenue, Foundational Models are just a feature (an important feature, but a feature nontheless). Foundational Models are a good first start, but are not 100% perfect in a number of fields and usecases. Ime, tooling built with these models are often used to cut down on headcount by 30-50% for the team using it to solve a specific problem. And this is why domain specific startups still thrive - sales, support, services, etc will still need to be tailored for buyers. reply ngneer 3 hours agorootparentAll of what you wrote is mostly true, except that \"not 100% perfect in a number of fields and usecases\" is quite an understatement. You mention the cybersecurity vertical. As a datapoint, I have put the simplest code security analysis question to ChatGPT (4o mini, for those who might say wait until the next one comes out). I made a novel vulnerable function, so that it would have never been seen before. I chose a very simple and easy vulnerability. Scores of security researchers in my vicinity spotted the vulnerability trivially and instantly. ChatGPT was more than useless, failing miserably to perform any meaningful analysis. The above is anecdotal data. Could be that a different tool would perform better. However, even if such models were harnessed by a startup to solve a specific problem, there is absolutely no way for present capabilities to yield a 30-50% HC reduction in this subdomain. reply alephnerd 2 hours agorootparentI agree. Foundational models suck at the high value security work that is needed. That said, the easiest proof-of-value for foundation models in security today is automating the SOC function by auto-generating playbooks, stitching context from various signal sources, and being able to auto-summary an attack. This reduces the need for hiring a junior SOC Analyst, and is a workflow that has already been adopted (or is in the process of being adopted) by plenty of F500s. At the end of the day, foundational models cannot reason yet, and that kind of capability is still far away. reply master_crab 3 hours agorootparentprevby concentrating on creating a very domain specific model I don’t disagree with this from an economics perspective (it’s expensive running an FM to handle domain specific queries). But the most accurate domain knowledge always tends to involve internal data. And then it becomes the issue raised above: a people problem involving internal knowledge and data management. Incumbent hyperscalers and vendors like MS, Amazon, etc (and even third party data managers like snowflake) tend to have more leverage when it becomes this type of data problem. reply fuzzfactor 6 minutes agorootparent>Startups can outcompete the Foundational Model companies by concentrating on creating a very domain specific model, and providing support and services that comes out of having expertise in that specific domain. Well-put because the business is focused and to-the-point from the beginning. For those applications where this gets you in the door to the domain, or gets you in sooner, this can be a competitive advantage. I think Lukas is pointing out the longer-term limitations of the approach though. I thought this would extend from 1980s electronics myself. You could edit this however: >Startups can [prosper] by concentrating on creating a very domain specific model, and providing support and services that comes out of having expertise in that specific domain. And it may hold true anyway and you may have a lifetime of work ahead of you whether or not the more-generalized capabilities catch up or not. You don't always have to actually be competitive with capitalized corporations in the market if you are adding real value to begin with, and the sky can still be the limit. >the most accurate domain knowledge always tends to involve internal data. >Incumbent hyperscalers . . . tend to have more leverage when it becomes this type of data That can help as a benchmark to gauge when a person or small team actually can occasionally outperform a billion-dollar corporation in some way or another. I'm no Mr. Burns, but to this I have slowly said to myself \"ex-cel-lent\" similarly for decades. It's good to watch AI approaches come and go and even better to be adaptable over time. reply adamesque 2 hours agorootparentprevInterestingly, this is the exact opposite of the point the article makes — which is that over time, more general models and more compute are more capable, and by building a domain-specific model you just build a ceiling past which you can’t reach. This is not the same as having unique access to domain-specific data, which becomes more valuable as you run it through more powerful domain-agnostic models. It sounds like this latter point is the one you say has value for startups to tackle reply alephnerd 2 hours agorootparent> This is not the same as having unique access to domain-specific data, which becomes more valuable as you run it through more powerful domain-agnostic models. It sounds like this latter point is the one you say has value for startups to tackle Exactly! reply CharlieDigital 2 hours agorootparentprev> AI will not play out the way the Web did (upstarts unseat giants) Yes, I agree. I recently spoke to a doctor that wanted to do a startup one part of which is an AI agent that can provide consumers second opinions for medical questions. For this to be safe, it will require access to not only patient data, but possibly front line information from content origins like UpToDate because that content is a necessity to provide grounded answers for information that's not in the training set and not publicly available via search. The obvious winner is UpToDate who owns that data and the pipeline for originating more content. If you want to build the best AI agent for medical analysis, you need to work with UpToDate. > ...not brand new startups who have to try to convince companies to give them all their data for the first time. Yes. I think of Microsoft and SharePoint, for example. Enterprises that are using SharePoint for document and content storage have already organized a subset of their information in a way that benefits Microsoft as concerns AI agents that are contextually aware of your internal data. reply diggan 2 hours agorootparentprev> will instead play out like smartphones (giants entrench and balloon). Someone correct me if I'm wrong, but didn't smartphones go the \"upstarts unseat giants\" way? Apple wasn't a phone-maker, and became huge in the phone-market after their launch. Google also wasn't a phone-maker, yet took over the market slowly but surely with their Android purchase. I barely see any Motorola, Blackberry, Nokia or Sony Ericsson phones anymore, yet those were the giants at one time. Now it's all iOS/Android, two \"upstarts\" initially. reply lolinder 2 hours agorootparent> Now it's all iOS/Android, two \"upstarts\" initially. They weren't upstarts, they were giants who moved into a new (but tightly related) space and pushed out other companies that were in spaces that at first seemed closely related but actually were more different than first appeared. Android and iOS won because smartphones were actually mobile computers with a cellular chip, not phones with fancy software. Seen that way Apple was obviously not an upstart, they were a giant that grew even further. Google is perhaps somewhat more surprising since they didn't do hardware at all before, but they did have Chrome, giving them a major in on the web platform side, and were also able to leverage their enormous search revenue. Neither resource is available to an upstart/startup. reply ip26 3 hours agorootparentprevThe people selling models are actually just selling compute Yes, fully agreed. Anything AI is discovering in your dataset could have been found by humans, and it could have been done by a more efficient program. But that would require humans to carefully study it and write the program. AI lets you skip the novel analysis of the data and writing custom programs by using a generalizable program that solves those steps for you by expending far more compute. I see it as, AI could remove the most basic obstacle preventing us from applying compute to vast swathes of problems- and that’s the need to write a unique program for the problem at hand. reply digitcatphd 5 hours agoparentprevI agree with you at this time, but there are a couple things I think will change this: 1. Agentic search can allow the model to identify what context is needed and retrieve the needed information (internally or externally through APIs or search) 2. I received an offer from OpenAI to give me free credits if I shared my API data with it, in other words, it is paying for industry specific data as they are probably fine tuning niche models. There could be some exceptions to UI/UX going down specific verticals but eventually these fine tuning sector specific instances value will erode over time but this will likely occupy a niche since enterprise wants maximum configuration and more out of box solutions are oriented around SMEs. reply est31 4 hours agorootparentIt comes down to moats. Does OpenAI have a moat? It's leading the pack, but the competitors always seem to be catching up to it. We don't see network effects with it yet like with social networks, unless OpenAI introduces household robots for everyone or something, builds a leading marketshare in that segment, and the rich data from these household bots is enough training data that one can't replicate with a smaller robot fleet. And AI is too fundamental of a technology that a \"loss leader biggest wallet wins\" strategy, used by the likes of Uber, will work. API access can be restricted. Big part of why Twitter got authwalled was so that AI models can't train from it. Stack overflow added a no AI models clause to their free data dump releases (supposed to be CC licensed), they want to be paid if you use their data for AI models. reply digitcatphd 2 hours agorootparentI wasn't referring to OAI, but rather: 1. Existing legacy players with massive data lock-ins like ERP providers and Google/Microsoft. 2. Massive consolidation within AI platforms rather than massive fragmentation if these legacy players do get disrupted or opportunities that do pop up. In other words - the usual suspects will continue to win because they have the data and lock in. Any marginal value in having a specialized model, agent workflow, or special training data, ect. will not be significant enough to switch to a niche app. It is indeed unfortunate and niches will definitely exist. What I am referring to is primarily in enterprise. reply osigurdson 3 hours agorootparentprevI don't think OpenAI have a moat in the traditional sense. Other players offer the exact same API so OpenAI can only win with permanent technical leadership. They may indeed be able to attain that but this is no Coca-Cola. reply CharlieDigital 4 hours agorootparentprev> Agentic search All you've proposed is moving the context problem somewhere else. You still need to build the search index. It's still a problem of building and providing context. reply digitcatphd 2 hours agorootparentI disagree, these search indexes already exist, they just need to be navigated much how Cursor uses agentic search to navigate your codebase or you call Perplexity to get documentation. If the knowledge exists outside of your mind it can be searched agentically. reply spacemanspiff01 4 hours agorootparentprevwhat do you think about these guys: https://exa.ai/ reply CharlieDigital 3 hours agorootparentCrawling web data is ETL. I think the case stands: the winners in AI/LLM SaaS startup space are the ones that really do ETL well. Whether that's ETL is across an enterprise data set or a codebase. The AI and LLM are just the the \"bake\" button. If you want anything good, you still have to prep and put good ingredients in. reply dartos 4 hours agorootparentprevTo your first point, the LLM still can’t know what it doesn’t know. Just like you can’t google for a movie if you don’t know the genre, any scenes, or any actors in it, and AI can’t build its own context if it didn’t have good enough context already. IMO that’s the point most agent frameworks miss. Piling on more LLM calls doesn’t fix the fundamental limitations. TL;DR an LLM can’t magically make good context for itself. I think you’re spot on with your second point. The big differentiators for big AI models will be data that’s not easy to google for and/or proprietary data. Lucky they got all their data before people started caring. reply immibis 2 hours agorootparent> Just like you can’t google for a movie if you don’t know the genre, any scenes, or any actors in it, ChatGPT was able to answer \"What was the video game with cards where you play against a bear guy, a magic guy and a set of robots?\" (it's Inscryption). This is one area where LLMs work. reply dartos 55 minutes agorootparent“Playing cards against a bear guy” is a pretty iconic part of that game… that you, as a human, had the wherewithal to put into that context. Agents don’t have that wherewithal. They’d never come up with “playing cards against a beat guy” if you asked it “what game am I thinking of” Let’s do another experiment. Do the same for the game I’m thinking of right now. There were characters in it and one of them had a blue shirt, but that’s all I can remember. reply smrq 2 hours agorootparentprevYou described all of those things to some extent, as much as they apply to video games. No magic here. reply mritchie712 6 hours agoparentprevI think you're downplaying how well Cursor is doing \"code generation\" relative to other products. Cursor can do at least the following \"actions\": * code generation * file creation / deletion * run terminal commands * answer questions about a code base I totally agree with you on ETL (it's a huge part of our product https://www.definite.app/), but the actions an agent takes are just as tricky to get right. Before I give Cursor, I often doubt it's going to be able to pull it off and I constantly impressed by how deep it can go to complete a complex task. reply stickfigure 3 hours agorootparentThis really puzzles me. I tried Cursor and was completely underwhelmed. The answers it gave (about a 1.5M loc messy Spring codebase) were surface-level and unhelpful to anyone but a Java novice. I get vastly better work out of my intern. To add insult to injury, the IntelliJ plugin threw spurious errors. I ended up uninstalling it and marking my calendar to try again in 6 months. Yet some people say Cursor is great. Is it something about my project? I can't imagine how it deals with a codebase that is many millions of tokens. Or is it something about me? I'm asking hard questions because I don't need to ask the easy ones. What are people who think Cursor is great doing differently? reply nirushiv 1 hour agorootparentMy tinfoil hat theory is that Cursor deploys a lot of “guerilla marketing” with influencers on Twitter/LinkedIn etc. When I tried it, the product was not good (maybe on par with Copilot) but you have people on social media swearing by it. Maybe it just works well for specific types of web development, but I came away thoroughly unimpressed and suspicious that some of the “word of mouth” stuff on them is actually funded by them. reply mritchie712 2 hours agorootparentprevThis is a great question and easy to answer with the context you provided. I don't think your poor experience is because of you, it's because of your codebase. Cursor works worse (in my experience) on larger codebases and seems particularly good at JS (e.g. React, node, etc.). Cursor excels at things like small NextJS apps. It will easily work across multiple files and complete tasks that would take me ~30 minutes in 30 seconds. Trying again in 6 months is a good move. As models get larger context windows and Cursor improves (e.g. better RAG) you should have a better experience. reply CharlieDigital 1 hour agorootparentCursor's problem isn't bigger context, it's better context. I've been using it recently with @nanostores/react and @nanostores/router. It constantly wants to use router methods from react-router and not nanostores so I am constantly correcting it. This is despite using the rules for AI config (https://docs.cursor.com/context/rules-for-ai). It continually makes the same mistakes and requires the same correction likely because of the dominance of react-router in the model's training. That tells me that the prompt it's using isn't smart enough to know \"use @nanostores/router because I didn't find react-router\". I think for Cursor to really nail it, the base prompt needs to have more context that it derived from the codebase. It should know that because I'm referencing @nanostores/router, to include an instruction to always use @nanostores/router. reply apwell23 2 hours agorootparentprevIts for novices and youtube AI hucksters. Its the coding equivalent of vibrating belts for weight loss. reply nirushiv 1 hour agorootparentThis is my read as well reply whimsicalism 2 hours agorootparentprevI’m very “bullish” on AI in general but find cursor incredibly underwhelming because there is little value add compared to basically any other AI coding tool that goes beyond autocomplete. Cursor emphatically does not understand large codebases and smaller (few file codebases) can just be pasted into a chat context in the worst case. reply uxhacker 5 hours agorootparentprevSo isn’t cursor just a tool for Claude or ChatGpt to use? Another example would be a flight booking engine. So why can’t an AI just talk direct to an IDE? This is hard as the process has changed, due to the human needing to be in the middle. So Isn’t AI useless without the tools to manipulate? reply ErikBjare 5 hours agorootparentprevIs it really that different to Claude with tools via MCP, or my own terminal-based gptme? (https://github.com/ErikBjare/gptme) reply TeMPOraL 4 hours agorootparentI thought it's basically a subset of Aider[0] bolted into a VS Code fork, and I remain confused as to why we're talking about it so much now, when we didn't about Aider before. Some kind of startup-friendly bias? I for one would prefer OSS to succeed in this space. -- [0] - https://aider.chat/ reply Xmd5a 3 hours agorootparentI tried aider and had problems having it update code in existing files. Aider uses a search and replace pattern to update existing code. So you often end up with >>>SEARCH } >>>REPLACE }, {'more': 'data'} Of course aider will try to apply this kind of patch even when the search pattern matches several occurrences in the target file. Looking at the Github issues, this is a problem that was brought up several times and was never fixed because apparently it's not even problematic. I moved to cursor, which doesn't have this problem, and never looked back. reply ErikBjare 3 hours agorootparentFor what it's worth, gptme will refuse non-unique matches (and ask the LLM to try again). I thought Aider did too (easy win after all), but apparently not. reply TeMPOraL 2 hours agorootparentFor me this happened at the end of functions in vanilla JS; I used to work around it by putting \"// end of foo()\" comments after the closing brace. However, Aider has multiple modes for LLM editing, including diff, udiff, and whole file; you can switch between those when needed. reply jddj 4 hours agorootparentprevSomewhat cynically, maybe because there's VC in Cursor. https://techcrunch.com/2024/12/19/in-just-4-months-ai-coding... reply ErikBjare 3 hours agorootparentprevThis is why I was asking. My own gptme is also just slightly different from Aider and has been around roughly as long. reply SmellTheGlove 4 hours agorootparentprevAider is the single best tool I’ve tried. And I’d never heard of it until like 2 weeks ago when someone mentioned it here. I love aider. reply TeMPOraL 4 hours agorootparentThe irony is, it's sort of a household name on HN for over a year now, being way ahead of what was available commercially on the market - and yet, it seems most people here haven't heard of it. (The author used to post a lot of insightful comments here about LLMs and other generative models, too.) reply ErikBjare 3 hours agorootparentThe same is true of my own gptme, which has been pretty much at parity with Aider along the way. Paul (Aider author) is a lot better at writing useful stuff than me though! (like the amazing blog posts) reply TeMPOraL 1 hour agorootparentThanks for mentioning this, because I somehow managed to miss gptme all that time! I'll check it out now. reply tailspin2019 4 hours agorootparentprevThanks for spreading the word. I hadn’t heard of Aider before and I’m now going to give it a try today. reply stereobit 6 hours agoparentprevIt’s not even just the lack of access to the data, so much hidden information to make decisions is not documented at all. It’s intuition, learned from doing something in a specific context for a long time and only a fraction of that context is accessible. reply CharlieDigital 3 hours agorootparentYes, this is definitely a big problem. Anyone that's done any amount of systems integration in enterprises knows this. \"Let me talk to Lars; he should know because his team owns that system.\" \"We don't have any documentation on this, but Mette should know about it because she led the project.\" reply stereobit 2 hours agorootparentExactly. Sure, as soon as more humans are replaced by agents who leave the full trace in the logs this fades away but this will take a long time. It will take many tiny steps in this direction. reply HPsquared 5 hours agorootparentprevThis is where Microsoft has the advantage, all those Teams calls can provide context. reply iandanforth 4 hours agoparentprevContext is important but it takes about two weeks to build a context collection bot and integrate it into slack. The hard part is not technical, AIs can rapidly build a company specific and continually updated knowledge base, it's political. Getting a drug company to let you tap slack and email and docs etc is dauntingly difficult. reply lolinder 4 hours agorootparentDifficult to impossible. Their vendors are already working on AI features, so why would they risk adding a new vendor when a vendor they've already approved will have substantially the same capabilities soon? reply whimsicalism 2 hours agorootparentbecause a vendor just using AI tools will not achieve the same capabilities as a vendor that either is OpenAI or is backed by OpenAI will achieve soon reply lolinder 1 hour agorootparentI don't believe that to be true—OpenAI is plateauing on model capabilities and turning to scaling inference times instead. There's no moat to \"just throw more tokens at the problem\", and Meta and Anthropic are both hot on their heels on raw model capabilities. I see absolutely no evidence that OpenAI has a major breakthrough up their sleeve that will allow them to retake the lead. In the end, models are fundamentally a commodity. Data is all that matters, and in the not too distant future you won't gain anything at all by sending your data to OpenAI versus just using the tooling provided by your existing vendors. reply whimsicalism 1 hour agorootparentthey’re plateauing on pretraining returns, quite possibly (if rumors are to be trusted)… but they are just getting more sophisticated at real world complex RL - which is still similar to throwing more tokens at the problem and is creating large returns. i feel that the current artifact is already quite close to something that can operate in a competent manner if the downstream RL matches the task of interest well enough reply whimsicalism 2 hours agoparentprevAll of these comments are premised on this technology staying still. A model with memory and the ability to navigate the computer (we are already basically halfway there) would easily eliminate the problems you describe. HN, i find, also has a tendency to fall prey to the bitter lesson. reply prng2021 2 hours agoparentprevI agree but do see 1 realistic solution to solve the problem you describe. Every product on the market is independently integrating a LLM right now that has access to their product’s silo of information. I can imagine a future where a corporate employee interacts with 1 central LLM that in turn understands the domain of expertise of all the other system-specific LLMs. Given that knowledge, the central one can orchestrate prompting and processing responses from the others. We been using this pattern forever with traditional APIs but the huge hurdle is that the information in any system you integrate with is often both complex and messy. LLMs handle the hard work of handling ambiguity and variations. reply energy123 4 hours agoparentprevThis problem will be eaten by OpenAI et al. the same way the careful prompting strategies used in 2022/2023 were eaten. In a few years we will have context lengths of 10M+ or online fine tuning, combined with agents that can proactively call APIs and navigate your desktop environment. Providing all context will be little more than copying and pasting everything, or just letting the agent do its thing. Super careful or complicated setups to filter and manage context probably won't be needed. reply CharlieDigital 2 hours agorootparentEven if your context is a trillion tokens in length, the problem of creating that context still exists. It's still ETL and systems integration. reply whimsicalism 2 hours agorootparentThe model can take actions on the computer - give it access to the company wiki and slack and it can create its own context. Yall really are just assuming this technology will stay still and not extrapolating from trends. A model that can get 25% on frontiermath is probably soon going to be able to navigate your company slack, that is not a more difficult problem than expert-level math proof development. reply OutOfHere 4 hours agorootparentprevContext requires quadratic VRAM. It is why OpenAI hasn't even supported 200k context length yet for its 4o model. Is there a trick that bypasses this scaling constraint while strictly preserving the attention quality? I suspect that most such tricks lead to performance loss while deep in the context. reply energy123 3 hours agorootparentI wouldn't bet against this. Whether it's Ring attention, Mamba layers or online fine tuning, I assume this technical challenge will get conquered sooner rather than later. Gemini are getting good results on needle in a haystack with 1M context length. I suspect the sustainable value will be in providing context that isn't easily accessible as a copy and paste from your hard drive. Whatever that looks like. reply whimsicalism 2 hours agorootparentprevEven subpar attention quality is typically better than human memory - we can imagine models that do some sort of triaging from shorter high-quality attention context and extremely long linear (or something else) context. reply dist-epoch 3 hours agorootparentprev> Context requires quadratic VRAM Even if this is not solved, there is so much economic benefit, tens of TBs of VRAM will become feasible. reply abrichr 4 hours agoparentprev> No matter how good the AI gets, it can't answer about what it doesn't know. It can't perform a process for which it doesn't know the steps or the rules This is exactly the motivation behind https://github.com/OpenAdaptAI/OpenAdapt: so that users can demonstrate their desktop workflows to AI models step by step (without worrying about their data being used by a corporation). reply markmiro 2 hours agoparentprevStartups can still win against big players by building better products faster (with AI), collecting more / better data to feed AI, and then feeding that into better AI automation for customers. Big players won't automatically win, but more data is a moat that gives them room to mess up for a long time and still pull out ahead. Even then, big companies already compete against one another and swallowing a small AI startup can help them and therefore starting one can also make sense. reply whimsicalism 2 hours agorootparentThere are not really any startups in the position to feed AI the great data they have. reply mycall 4 hours agoparentprevI found that fine-tuning and RAG can be replaced with tool calling for some specialized domains, e.g. real-time data. Even things like user's location can be tool called, so context can be obtained reliably. I also note that GPT-4o and better are smart enough to chain together different functions you give it, but not reliably. System prompting helps some, but the non-determinism of AI today is both awesome and a cure. reply CharlieDigital 4 hours agorootparentTool calling is just systems integration with a different name. The job of the tool is still to provide context from some other system. reply socrateslee 3 hours agoparentprevAI code copilot like cursor provide a immersive context than most of other AI products. reply osigurdson 4 hours agoparentprevWouldn't this just be foundational model + RAG in the limit? reply CharlieDigital 2 hours agorootparentRAG is the action of retrieval to augment generation. Retrieval of what? From where? The process that feeds RAG is all about how you extract, transform, and load source data into the RAG database. Good RAG is the output of good ETL. reply cyanydeez 6 hours agoparentprevTo bake a cake from scratch, you must first recreate the universe reply jbverschoor 4 hours agoparentprevAnd how does that differ from any person without that information? reply CharlieDigital 3 hours agorootparentIt doesn't. And that's why the teams that really want to unlock AI will understand that the core problem is really systems integration and ETL; the AI needs to be aware of the entire corpus of relevant information through some mechanism (tool use, search, RAG, graph RAG, etc.) and the startups that win are the ones that are going to do that well. You can't solve this problem with more compute nor better models. I've said it elsewhere in this discussion, but the LLM is just a magical oven that's still reliant on good ingredients being prepped and put into the oven before hitting the \"bake\" button if you want amazing dishes to pop out. If you just want Stouffer's Mac & Cheese, it's already good enough for that. reply rthrfrd 5 hours agoparentprevI agree - busy building to solve that :) reply skrebbel 6 hours agoparentprevYeah seems like context is the AI version of cache invalidation, in the sense of the joke that \"there's only 2 hard problems in computer science, cache invalidation and naming things\". It all boils down to that (that, and naming things) reply throwpoaster 6 hours agorootparentAnd off-by-one errors :) reply nicbou 4 hours agorootparentAnd an almost fanatical devotion to the Pope reply Scarblac 6 hours agorootparentprevAlso, there's only one hard problem in software engineering: people. Seems to apply to AI as well. reply jgalt212 2 hours agoparentprev> There's only one core problem in AI worth solving for most startups building AI powered software: context. Is this another way of saying \"content is king\"? reply jonnycat 6 hours agoprevI think this argument only makes sense if you believe that AGI and/or unbounded AI agents are \"right around the corner\". For sure, we will progress in that direction, but when and if we truly get there–who knows? If you believe, as I do, that these things are a lot further off than some people assume, I think there's plenty of time to build a successful business solving domain-specific workflows in the meantime, and eventually adapting the product as more general technology becomes available. Let's say 25 years ago you had the idea to build a product that can now be solved more generally with LLMs–let's say a really effective spam filter. Even knowing what you know now, would it have been right at the time to say, \"Nah, don't build that business, it will eventually be solved with some new technology?\" reply jillesvangurp 5 hours agoparentI don't think it's that binary. We've had a lot of progress over the last 25 years; much of it in the last two. AGI is not a well defined thing that people easily agree on. So, determining whether we have it or not is actually not that simple. Mostly people either get bogged down into deep philosophical debates or simply start listing things that AI can and cannot do (and why they believe why that is the case). Some of those things are codified in benchmarks. And of course the list of stuff that AIs can't do is getting stuff removed from it on a regular basis at an accelerating rate. That acceleration is the problem. People don't deal well with adapting to exponentially changing trends. At some arbitrary point when that list has a certain length, we may or may not have AGI. It really depends on your point of view. But of course, most people score poorly on the same benchmarks we use for testing AIs. There are some specific groups of things where they still do better. But also a lot of AI researchers working on those things. reply comex 1 hour agorootparentWhat acceleration? Consider OpenAI's products as an example. GPT-3 (2020) was a massive step up in reasoning ability from GPT-2 (2019). GPT-3.5 (2022) was another massive step up. GPT-4 (2023) was a big step up, but not quite as big. GPT-4o (2024) was marginally better at reasoning, but mostly an improvement with respect to non-core functionality like images and audio. o1 (2024) is apparently somewhat better at reasoning at the cost of being much slower. But when I tried it on some puzzle-type problems I thought would be on the hard side for GPT-4o, it gave me (confidently) wrong answers every time. 'Orion' was supposed to be released as GPT-5, but was reportedly cancelled for not being good enough. o3 (2025?) did really well on one benchmark at the cost of $10k in compute, or even better at the cost of >$1m – not terribly impressive. We'll see how much better it is than o1 in practical scenarios. To me that looks like progress is decelerating. Admittedly, OpenAI's releases have gotten more frequent and that has made the differences between each release seem less impressive. But things are decelerating even on a time basis. Where is GPT-5? reply fuzzfactor 1 hour agorootparentprev>Let's say 25 years ago you had the idea to build a product I resemble that remark ;) >that can now be solved more generally with LLMs Nope, sorry, not yet. >\"Nah, don't build that business, it will eventually be solved with some new technology?\" Actually I did listen to people like that to an extent, and started my business with the express intent of continuing to develop new technologies which would be adjacent to AI when it matured. Just better than I could at my employer where it was already in progress. It took a couple years before I was financially stable enough to consider layering in a neural network, but that was 30 years ago now :\\ Wasn't possible to benefit with Windows 95 type of hardware, oh well, didn't expect a miracle anyway. Heck, it's now been a full 45 years since I first dabbled in a bit of the ML with more kilobytes of desktop memory than most people had ever seen. I figured all that memory should be used for something, like memorizing, why not? Seemed logical. Didn't take long to figure out how much megabytes would help, but they didn't exist yet. And it became apparent that you could only go so far without a specialized computer chip of some kind to replace or augment a microprocessor CPU. What kind, I really had no idea :) I didn't say they resembled 25-year-old ideas that much anyway ;) >We've had a lot of progress over the last 25 years; much of it in the last two. I guess it's understandable this has been making my popcorn more enjoyable than ever ;) reply antonvs 5 hours agoparentprevAgreed. There's a difference between developing new AI, and developing applications of existing AI. The OP seems to blur this distinction a bit. The original \"Bitter Lesson\" article referenced in the OP is about developing new AI. In that domain, its point makes sense. But for the reasons you describe, it hardly applies at all to applications of AI. I suppose it might apply to some, but they're exceptions. reply ilaksh 5 hours agoparentprevYou think it will be 25 years before we have a drop in replacement for most office jobs? I think it will be less than 5 years. You seem to be assuming that the rapid progress in AI will suddenly stop. I think if you look at the history of compute, that is ridiculous. Making the models bigger or work more is making them smarter. Even if there is no progress in scaling memristors or any exotic new paradigm, high speed memory organized to localize data in frequently used neural circuits and photonic interconnects surely have multiple orders of magnitude of scaling gains in the next several years. reply lolinder 4 hours agorootparent> You seem to be assuming that the rapid progress in AI will suddenly stop. And you seem to assume that it will just continue for 5 years. We've already seen the plateau start. OpenAI has tacitly acknowledged that they don't know how to make a next generation model, and have been working on stepwise iteration for almost 2 years now. Why should we project the rapid growth of 2021–2023 5 years into the future? It seems far more reasonable to project the growth of 2023–2025, which has been fast but not earth-shattering, and then also factor in the second derivative we've seen in that time and assume that it will actually continue to slow from here. reply harvodex 4 hours agorootparentAt this point, the lack of progress since April 2023 is really what is shocking. I just looked on midjourney reddit to make sure I wasn't missing some new great model. Instead what I notice is the small variations on the themes I have already seen a thousand times a year ago now. Midjourney is so limited in what it can actually produce. I am really worried that all this is much closer to a parlor trick than AGI. \"simple trick or demonstration that is used especially to entertain or amuse guests\" It all feels more and more like that to me than any kind of progress towards general intelligence. reply pgwhalen 4 hours agorootparentprev> OpenAI has tacitly acknowledged that they don't know how to make a next generation model Can you provide a source for this? I'm not super plugged into the space. reply lolinder 4 hours agorootparentThere's this [0]. But also o1/o3 is that acknowledgment. They're hitting the limits of scaling up models, so they've started scaling compute [1]. That is showing some promise, but it's nowhere near the rate of growth they were hitting while next gen models were buildable. [0] https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e769... [1] https://techcrunch.com/2024/11/20/ai-scaling-laws-are-showin... reply sealeck 5 hours agorootparentprevI think you're suffering from some survivorship bias here. There are lot of technologies that don't work out. reply ilaksh 5 hours agorootparentComputation isn't one of them so far. Do you believe this is the end of computing efficiency improvements? reply fuzzfactor 15 minutes agorootparentprevNot my downvote, just the opposite but I think you can do a lot in an office already if you start early enough . . . At one time I would have said you should be able to have an efficient office operation using regular typewriters, copiers, filing cabinets, fax machines, etc. And then you get Office 97, zip through everything and never worry about office work again. I was pretty extreme having a paperless office when my only product is paperwork, but I got there. Before long Google gets going. Wow. No-ads information superhighway, if this holds it can only get better. And that's without broadband. But that's besides the point. Now it might make sense for you to at least be able to run an efficient office on the equivalent of Office 97 to begin with. Then throw in the AI or let it take over and see what you get in terms of output, and in comparison. Microsoft is probably already doing this in an advanced way. I think a factor that can vary over orders of magnitude is how does the machine leverage the abilities and/or tasks of the nominal human \"attendant\"? One type of situation would be where a less-capable AI could augment a defined worker more effectively than even a fully automated alternative utilizing 10x more capable AI. There's always some attendant somewhere so you don't get a zero in this equation no matter how close you come. Could be financial effectiveness or something else, the dividing line could be a moving target for a while. You could even go full paleo and train the AI on the typewriters and stuff just to see what happens ;) But would you really be able to get the most out of it without the momentum of many decades of continuous improvement before capturing it at the peak of its abilities? reply SoftTalker 3 hours agorootparentprevAlso office jobs will be adapted to be a better fit to what AI can do, just as manufacturing jobs were adapted so that at least some tasks could be completed by robots. reply noch 5 hours agorootparentprev> You seem to be assuming that the rapid progress in AI will suddenly stop. > I think if you look at the history of compute, that is ridiculous. Making the models bigger or work more is making them smarter. It's better to talk about actual numbers to characterise progress and measure scaling: \" By scaling I usually mean the specific empirical curve from the 2020 OAI paper. To stay on this curve requires large increases in training data of equivalent quality to what was used to derive the scaling relationships. \"[^2] \"I predicted last summer: 70% chance we fall off the LLM scaling curve because of data limits, in the next step beyond GPT4. […] I would say the most plausible reason is because in order to get, say, another 10x in training data, people have started to resort either to synthetic data, so training data that's actually made up by models, or to lower quality data.\"[^0] “There were extraordinary returns over the last three or four years as the Scaling Laws were getting going,” Dr. Hassabis said. “But we are no longer getting the same progress.”[^1] --- [^0]: https://x.com/hsu_steve/status/1868027803868045529 [^1]: https://x.com/hsu_steve/status/1869922066788692328 [^2]: https://x.com/hsu_steve/status/1869031399010832688 reply ilaksh 5 hours agorootparento1 proved that synthetic data and inference time is a new ramp. There will be more challenges and more innovations. There is a lot of room in hardware, software, model training and model architecture left. reply noch 5 hours agorootparent> There is a lot of room in hardware, software, model training and model architecture left. Quantify this please? And make a firm prediction with approximate numbers/costs attached? reply ilaksh 4 hours agorootparentIt's not realistic to make firm quantified predictions any more specific than what I have given. We will likely see between 3 and 10000 times improvement in efficiency or IQ or speed of LLM reasoning in the next 5 years. reply noch 4 hours agorootparent> It's not realistic to make firm quantified predictions any more specific than what I have given. Then do you actually know what you're talking about or are you handwaving? I'm not trying to be offensive but business plans can't be made based on a lack of predictions. > We will likely see between 3 and 10000 times improvement in efficiency or IQ or speed of LLM reasoning in the next 5 years That variance is too large to take you seriously, unfortunately. That's unfortunate because I was really hoping you had an actionable insight for this discussion. :( If I, for instance, tell my wife I can improve our income by 3x or 1000x but I don't really know, there's no planning that can be done and I'll probably have to sleep on the couch until I figure out what the hell I'm doing. reply ben_w 2 hours agorootparent> If I, for instance, tell my wife I can improve our income by 3x or 1000x but I don't really know, there's no planning that can be done and I'll probably have to sleep on the couch until I figure out what the hell I'm doing. For most people, even a mere 3x in the next 5 years is huge, it's 25% per year growth. 3x in 5 years is a reasonable low-ball for hardware improvements alone. Caveat: top-end silicon is now being treated as a strategic asset, so there may be wars over it, driving up prices and/or limiting progress, even on the 5-year horizon. I'm unclear why your metaphor would have you sleeping on the sofa: If tonight you produce a business idea for which you can be 2σ-confident that it will give you an income 5 years from now in the range [3…1000]x, you can likely get a loan for a substantially bigger house tomorrow than you were able to get yesterday; in the UK that's a change slightly larger than going from the median average full-time salary to the standard member of parliament salary. (The reason behind this, observed lowering of compute costs, has been used even decades ago to delay investment in compute until the compute was cheaper). The arguments I've seen elsewhere for order-of-10,000x* cost improvements (which is a proxy for efficiency and speed if not IQ) is based on various different observations cost reductions** since ChatGPT came out — personally, I doubt that the high end of that would come to pass, my guess is those all represent low-hanging fruit that can't be picked twice, but even then I would still expect there to be some opportunity for further gains. * The original statement had one more digit in it than yours, but this doesn't make much difference to the argument either way ** e.g. https://www.wing.vc/content/plummeting-cost-ai-intelligence reply TeMPOraL 3 hours agorootparentprev> business plans can't be made based on a lack of predictions. They can. It's called \"taking a risk\". Which is what startups are about, right? It's hard to give a specific prediction here (I'm leaning towards 10x-1000x in the next 5 years), but there's also no good reason to believe progress will stop, because a) there's many low and mid-hanging fruits to pick, as outlined by GP, and b) because it never did so far, so why would it stop now specifically? reply jazzyjackson 2 hours agorootparentWhy did we stop going to the moon and flying commercial supersonic? Some things that are technologically possible are not economically viable. AI is a marvel but I'm not convinced it will actually plug into economic gains that justify the enormous investment in compute. reply GardenLetter27 4 hours agoparentprevWe already have AGI in some ways though. Like I can use Claude for both generating code and helping with some maths problems and physics derivations. It isn't a specific model for any of those problems, but a \"general\" intelligence. Of course, it's not perfect, and it's obviously not sentient or conscious, etc. - but maybe general intelligence doesn't require or imply that at all? reply SecretDreams 4 hours agorootparentFor me, general intelligence from a computer will be achieved when it knows when it's wrong. You may say that humans also struggle with this, and I'd agree - but I think there's a difference between general intelligence and consciousness, as you said. reply moqmar 1 hour agorootparentBeing wrong is one thing, on the other hand knowing that they don't know something is something humans are pretty good at (even if they might not admit to not knowing something and start bullshitting anyways). Current AI predictably fails miserably every single time. reply raincole 4 hours agorootparentprev> AGI in some ways In other words, just AI, not AGI. reply timabdulla 6 hours agoprevI think one thing ignored here is the value of UX. If a general AI model is a \"drop-in remote worker\", then UX matters not at all, of course. I would interact with such a system in the same way I would one of my colleagues and I would also give a high level of trust to such a system. If the system still requires human supervision or works to augment a human worker's work (rather than replace it), then a specific tailored user interface can be very valuable, even if the product is mostly just a wrapper of an off-the-shelf model. After all, many SaaS products could be built on top of a general CRM or ERP, yet we often find a vertical-focused UX has a lot to offer. You can see this in the AI space with a product like Julius. The article seems to assume that most of the value brought by AI startups right now is adding domain-specific reliability, but I think there's plenty of room to build great experiences atop general models that will bring enduring value. If and when we reach AGI (the drop-in remote worker referenced in the article), then I personally don't see how the vast majorities of companies - software and others - are relevant at all. That just seems like a different discussion, not one of business strategy. reply bsenftner 5 hours agoparentThe value of UX is being ignored, as the magical thinking has these AIs being fully autonomous, which will not work. The phrase \"the devil's in the details\" needs to be imprinted on everyone's screens, because the details of a \"drop-in remote worker\" are several Grand Canyons yet to be realized. This civilization is vastly more complex than you, dear reader, realize, and the majority of that complexity is not written down. reply noirbot 1 hour agorootparentAlso, the UX of your potential \"remote workers\" are vitally important! The difference between a good and a bad remote worker is almost always how good they are at communicating - both reading and understanding tickets of work to be done and how well they explain, annotate, and document the work they do. At the end of the day, someone has to be checking the work. This is true of humans and of any potential AI agent, and the UX of that is a big deal. I can get on a call and talk through the code another engineer on my team wrote and make sure I understand it and that it's doing the right thing before we accept it. I'm sure at some point I could do that with an LLM, but the worry is that the LLM has no innate loyalty or sense of its own accuracy or honesty. I can mostly trust that my human coworker isn't bullshitting me and any mistakes are honest mistakes that we'll learn from together for the future. That we're both in the same boat where if we write or approve malicious or flagrantly defective code, our job is on the line. An AI agent that's written bad or vulnerable code won't know it, will completely seriously assert that it did exactly what it was told, doesn't care if it gets fired, and may say completely untrue things in an attempt to justify itself. Any AI \"remote worker\" is a totally different trust and interaction model. There's no real way to treat it like you would another human engineer because it has, essentially, no incentive structure at all. It doesn't care if the code works. It doesn't care if the team meets its goals. It doesn't care if I get fired. I'm not working with a peer, I'm working with an industrial machine that maybe makes my job easier. reply ilaksh 5 hours agoparentprevI guess part of the point is that the value of the UX will quickly start to decrease as more tasks or parts of tasks can be done without close supervision. And that is subject to the capabilities of the models which continues to improve. I suggest that before we satisfy _everyone_'s definition of AGI, more and more people may decide we are there as their own job is automated. The UX at that point, maybe in 5 or 10 or X years, might be a 3d avatar that pops up in your room via mixed reality glasses, talks to you, and then just fires off instructions to a small army of agents on your behalf. Nvidia actually demoed something a little bit like that a few days ago. Except it lives on your computer screen and probably can't manage a lot of complex tasks on it's own. Yet. Or maybe at some point it doesn't need sub agents and can just accomplish all of the tasks on its own. Based on the bitter lesson, specialized agents are probably going to have a limited lifetime as well. But I think it's worth having the AGI discussion as part of this because it will be incremental. Personally, I feel we must be pretty close to AGI because Claude can do a lot of my programming for me. I still have to make important suggestions, and routinely for obvious things, but it is much better at me at filling in all the details and has much broader knowledge. And the models do keep getting more robust, so I seriously doubt that humans will be better programmers overall for much longer. reply hitchstory 5 hours agoparentprevA drop in remote worker will still require their work to be checked and their access to the systems they need to do their work secured in case they are a bad actor. reply NameError 6 hours agoprevI think the core problem at hand for people trying to use AI in user-facing production systems is \"how can we build a reliable system on top of an unreliable (but capable) model?\". I don't think that's the same problem that AI researchers are facing, so I'm not sure it's sound to use \"bitter lesson\" reasoning to dismiss the need for software engineering outright and replace it with \"wait for better models\". The article sits on an assumption that if we just wait long enough, the unreliability of deep learning approaches to AI will just fade away and we'll have a full-on \"drop-in remote worker\". Is that a sound assumption? reply resiros 6 hours agoprevThe author discusses the problem from the point of engineering, not from business. When you look at it from business perspective, there is a big advantage of not waiting, and using whatever exists right now to solve the business problem, so that you can get traction, get funding, grab marketshare, build a team, and when the next day a better model will come, you can rewrite your code, and you would be in a much better position to leverage whatever new capabilities the new models provide; you know your users, you have the funds, you built the right UX... The best strategy from your experience, is to jump on a problem as soon there is opportunity to solve it and generate lots of business value within the next 6 months. The trick is finding that subproblem that is worth a lot right now and could not be resolved 6 months ago. A couple of AI-sales startups \"succeeded\" quite well doing that (e.g. 11x), now they are in a good position to build from there (whether they will succeed in building a unicorn, that's another question, it just looks like they are in a good position now). reply ripped_britches 2 hours agoparentVery true. Most code written today will probably be obsolete in 2050. So why write it? Because it puts you in a good strategic position to keep leading in your space. reply bko 6 hours agoprevIt's a little depressing how many high valued startups are basically just wrappers around LLMs that they don't own. I'd be curious to see what percentage of YC latest batch is just this. > 70% of Y Combinator’s Winter 2024 batch are AI startups. This is compared to -57% of YC Summer 2023 companies and ~32% from the Winter batch one year ago (YC W23). The thinking is, the models will get better which will improve our product, but in reality, like the article states, the generalized models get better so your value add diminished as there's no need to fine tune. On the other hand the crypto fund made a killing off of \"me too\" block chain technology before it got hammered again. So who knows about 2-5 year term but 10 year almost certainly won't have these billion dollar companies that are wrappers around LLMs https://x.com/natashamalpani/status/1772609994610835505?mx=2 reply scarface_74 3 hours agoparentHow is being a wrapper for LLMs you don’t own any different from being a company based on cloud infrastructure you don’t own? LLMs are a platform. Bill Gates definition of a platform was “A platform is when the economic value of everybody that uses it exceeds the value of the company that creates it.” reply immibis 2 hours agorootparentA LLM wrapper adds near-zero value. If I type some text into a \"convert to Donald Trump style\" tool, it produces the exact same output as typing it into ChatGPT following \"Convert this text to Donald Trump style:\" because that's what the tool actually does. Implementing ChatGPT is 99.999% of the value creation. Prepending the prompt is 0.001%. The surprising fact is that the market assigns a non-zero value to the tool anyway. Startups that use cloud servers still write the software that goes on those servers, which is 90% of the value creation. reply scarface_74 1 hour agorootparentThat’s not what I see from the companies I work with (cloud consulting). Almost all of them are using LLMs along with “tools” and RAG. reply 9dev 6 hours agoprevWell. We were working on a search engine for industry suppliers since before the whole AI hype started (even applied to YC once), and hit a brick wall at some point were it got too hard to improve search result quality algorithmically. To understand what that means: We gathered lots of data points from different sources, tried to reconcile that into unified records, then find the best match for a given sourcing case based on that. But in a lot of cases, both the data wasn’t accurate enough to identify what a supplier was actually manufacturing, and the sourcing case itself wasn’t properly defined, because users found it too hard to come up with good keywords for their search. Then, LLMs entered the stage. Suddenly, we became able to both derive vastly better output from the data we got, and also offer our users easier ways to describe what they were looking for, find good keywords automatically, and actually deliver helpful results! This was only possible because AI augments our product well and really provides a benefit in that niche, something that would just not have been possible otherwise. If you plan on founding a company around AI, the best advice I can give you is to choose a problem that similarly benefits from AI, but does exist without it. reply openrisk 5 hours agoparent> the data wasn’t accurate enough to identify what a supplier was actually manufacturing how did the LLM help with that challenge? reply HeatrayEnjoyer 5 hours agorootparentA guess: Their ability to infer reality from incomplete and indirect information. reply leviliebvin 5 hours agoprevControversial opinion: I don't believe in the bitter lesson. I just think that the current DNN+SGD approaches are just not that good at learning deep general expressive patterns. With less inductive bias the model memorizes a lot of scenarios and is able to emulate whatever real work scenario you are trying to make the model learn. However it fails to simulate this scenario well. So it's kind of misleading to say that it's generally better to have less inductive bias. That is only true if your model architecture and optimization approach are just a bit crap. My second controversial point regarding AI research and startups: doing research sucks. It's risky business. You are not guaranteed success. If you make it, your competitors will be hot on your tail and you will have to keep improving all the time. I personally would rather leave the model building to someone else and focus more on building products with the available models. There are exceptions like finetuning for your specific product or training bespoke models for very specific tasks at hand. reply clomond 1 hour agoparentI also don't believe in the 'bitter lesson' when extrapolated to apply to all 'AI application layer implementations' - at least in the context of asserting that the universe of problem scopes are affected by it. I think it is true in an AI research context, but an unstated assumption is that you have complete data, E2E training, and the particular evaluated solution is not real-world unbounded. It assumes infinite data, and it assumes the ability to falsify the resulting model output. Most valuable, 'real world' applications of AI when trying to implement in practice have an issue with one or both of those. So in other words: where a fully unsupervised AI pathway is viable due to the structure of the problem, absolutely. I'm not convinced in the universality of this. Doesn't mean the core point of this essay on the futility of startups basing their business around one of the off the shelf LLMs isn't valid - I think for many they risk being generalized away. reply marcosdumay 2 hours agoparentprev> I just think that the current DNN+SGD approaches are just not that good I'll add even further. The transformers and etc that we are using today are not good either. That's evidenced by the enormous amount of memory they need to do any task. We have just taken the one approach that was working a bit better for sensorial tasks and pattern matching, and went all in, adding hardware after hardware so we could brute-force some cognitive tasks out of it. If we do the same to other ML architectures, I don't think they would stay much behind. And maybe some would get even better results. reply DesaiAshu 2 hours agoprevThis (particularly the figure 1 illustration) discounts the \"distribution\" layer for apps Single app/feature startups will lose (true long before AI). A few will grow large enough to entrench distribution and offer a suite of services, creating defensibility against competitors The distributors (eg. a SaaS startup that rapidly landed/expanded) will continue to find bleeding edge ways to offer a 6-12mo advantage against foundation models and incumbents GitLab is a great example of this model. The equivalent bitter lesson of the web is that every cutting edge proprietary technology will eventually be offered free open source. However, there is a commercial advantage to purchasing the bleeding edge features with a strong SLA and customer service The mistake is to think technology is a business. Business has always been about business. Good technology reduces the cost of sale (CAC) and cost of goods sold (COGS) to create a 85-90% margin. Good technology does not create a moat Resilient businesses do not rely on singular technology advantages. They invest heavily in long term R&D to stay ahead of EACH wave. Resting on one's laurels after catching a single wave, or sitting out of the competition because there will be bigger waves later, are both surefire ways to lose the competition reply DebtDeflation 6 hours agoprev>Eventually, you’ll just need to connect a model to a computer to solve most problems - no complex engineering required. The word \"eventually\" is doing a lot of work here. Yes, it's true in the abstract, but over what time horizon? We have to build products to solve today's problems with today's technology, not wait for the generalized model that can do everything but may be decades away. reply amelius 6 hours agoparentTrue, but it tells that if you are a founder of a niche AI company then you should take money out of it instead of investing everything back into the company, because eventually the generalist-AI will destroy your business and you will be left with nothing. reply rhubarbtree 2 hours agorootparentNot if the generalist AI arrives after you have made your returns, which is the sentiment of the post you’re respond to. reply tomp 3 hours agorootparentprevDiversification is good advice regardless of industry / technology / niche. reply timabdulla 6 hours agoparentprevBased on the author's company that be founded, I assume he believes this technology is just years away. I think with a lot of AI folk in San Francisco, this is a tacit assumption when having these sorts of conversations. reply prmph 6 hours agorootparentAnyone that thinks this is just years away is utterly ignoring human history, nature, and relationship with technology. My own view is that this will never be achieved, and it's not even just about the tech. Let's imagine for a moment that this is even achieved. Then, there is still complex engineering required in the world: to maintain and continually improve the AI engines and their interfaces. Unless you want to say that, past some point, the AI will be self-improving without any human input whatsoever. Unless the AI can read our minds, I'm not sure it can continue to serve human interests without human input. But, never mind, we will never get there. At this very moment, tech is capable of so much more, but most sites I visit have bad UI, are bloated downloading and executing massive amounts of JS, riddled with annoying ads that serve no real useful purpose to society, and riddled with bugs. Even as an engineer, I really struggle to find any good no-code tools to create anything truly sophisticated without digging into hard-core code. Heck, they are now talking about adding more HTTP methods to HTML forms. reply theptip 2 hours agoprevI think this misses a crucial dynamic. It’s not “custom scaffolding vs. wait for better models”. There is also fine-tuning. In specialized domains you can’t just rely on OpenAI finding all the training data required to render your experts obsolete. If you can build a data flywheel you can fine-tune models and build that lead into a moat; whoever has the best training set will have the best product. In the short term you might start fine-tuning on OpenAI but once you’ve established your dataset you can potentially gain independence by moving onto OSS models. If you are building a software assistant, sure, this is clearly something that OpenAI will get better at very quickly, and Altman has commented as much that many companies are building things which are certain to get steamrolled by core product development. But I think there is a very interesting strategic question around areas like law, medicine, and probably more so niche technical areas, where OpenAI will/can not absorb knowledge as quickly. reply tinco 6 hours agoprevThis might be true on a very long timescale, but that's not really relevant for VC's. Literally every single VC I've talked to raised the question if our moat is not just having better prompts, it's usually the first question. If a VC really invested in a company whose moat got evaporated by O1, that's on the VC. Everyone saw technology like O1 coming from a mile away. For the slightly more complex stuff, sure at some point some general AI will probably be able to do it. But with two big caveats, the first being: when? and the second being: for how much?. In theory every deep and wide enough neural network should be able to be trained to do object detection in images, yet no one is doing that. Technologies specifically designed to process images, like CNN's, reign supreme. Likewise for architectures of LLM's. At some point your specialization might become obsolete, but that point might be a decade or more from now. Until then, specializations will have large economic and performance advantages making the advancements in AI today available to the industry of tomorrow. I think it's the role of the VC to determine not if there's an AI breakthrough behind a startups technology, but if there's a market disruption and if that market disruption can be leveraged to establish a dominant company. Similar to how Google leveraged a small and easily replicable algorithmic advantage into becoming one of the most valuable companies on earth. reply thegeomaster 6 hours agoparentOn your object detection point, Gemini 2.0 Flash has bounding box detection: https://ai.google.dev/gemini-api/docs/models/gemini-v2#bound.... I haven't found it to work particularly well for some more domain-specific things I tried, but it was surprisingly good for an LLM. reply doctorpangloss 6 hours agoprevMore computation cannot improve the quality or domain of data. Maybe the bitter lesson lesson is, lobby bitterly, for copyright laws that favor what you are doing, and weakened anti trust, to give you the insurmountable moat of exclusive data in a walled garden media network. reply guax 6 hours agoparentA human does not need billions of driving hours to learn how to drive competently. The issue with current method is not quality of data but methodology. More computation might unlock newer approaches that are better with less and worse quality data. reply Zr01 4 hours agorootparentA human is not a blank slate. There's millennia of evolutionary history that goes into making a brain adapted and capable of learning from its environment. reply qeternity 4 hours agorootparentA human is a mostly blank slate...but it's a really sophisticated slate that as you say has taken many millions of years of development. reply namaria 6 hours agorootparentprevI think there's a more fundamental problem at play here: what seems to work in 'AI', search, is made better by throwing more data into more compute. You then store the results in a model, that amounts to pre-computed solutions waiting for a problem. Interacting with the model is then asking questions and getting answers that hopefully fit your needs. So, what we're doing on the whole seems to be a lot of coding and decoding, hoping that the data used in training can be adequately mapped to the problem domain realities. That would mean that the model you end up with is somehow a valid representation of some form of knowledge about the problem domain. Trouble is, more text won't yield higher and higher resolution of some representation of the problem domain. After some point, you start to introduce noise. reply doctorpangloss 1 hour agorootparentprevYeah well. That was a bad analogy, and everyone I know who used to say that, admits error. reply graycat 6 hours agorootparentprev> A human does not need billions of driving hours to learn how to drive competently. But humans DO need ~16 years of growth and development \"to learn how to drive competently\" and then will also know how to ride a bycycle, mow grass, build shelves, cook pizza, use a smart phone, ...! There's a lesson in that somewhere .... reply guax 5 hours agorootparentYou don't need the 16, you can get a much younger person to drive too. It only supports the fact that data amount/quality is not the problem. reply danielbln 5 hours agorootparentYou're forgetting a few billion years of evolution; we come with a fair amount of pretraining encoded in our DNA reply guax 3 hours agorootparentYou raise a fair point. But I think it still aligns with what I was going for, the millions of years in evolution are not more data on the same system but the system itself changing to be able to cope with limited data and novel scenarios. Its also not given or constant, modern humans are super young in the evolutionary scale and very different even if very similar from other animals. Funny enough we might have cracked the difference (language processing) but are still very far in the rest for AGI. reply HarHarVeryFunny 3 hours agorootparentprevIt's not pretraining data that we have encoded in our DNA - it's an optimized learning/prediction architecture that is encoded ... how to build a brain that will then be able to learn efficiently. The amount of knowledge/behavior that is innate - encoded in our DNA - is pretty minimal, consisting of things like opposite sex sexual attraction, fear of heights, fear of snakes, disgust at rotting smell, etc - basic survival stuff. reply HeatrayEnjoyer 4 hours agorootparentprevIt is still years of continuous data input from a diverse sensory array reply graycat 4 hours agorootparentprev> It only supports the fact that data amount/quality is not the problem. I agree with that. I.e., for \"drive competently.\" the way humans learn it is more general, i.e., also brings the list I gave with bycycles, pizza, grass mowing, etc. Also \"drive competently.\" the way humans do it requires a lot of judgment, maturity which is not very well defined! Guys, I don't understand how to make progress on AI. E.g., just asked X > Grok \"similitude\", and it gave a good answer, maybe like a long dictionary answer. But it looked like Grok had some some impressive plagiarism -- I can't tell the difference. reply scosman 5 hours agoprevGreat essay. This is 100% right about the technical side, but I think it misses the “product” aspect. Building a quality product (AI or otherwise), involves design and data at all levels: UX, on-boarding, marketing, etc. The companies learning important verticals and getting in the door with customers will have a pretty huge advantage as models get better. Both in terms of install base, and knowing what customers need. Really great products don’t simply do what a customer asks, but are built by taking to a ton of customers over and over, and solving their problems better than any one of them can articulate. It’s true we will need less and less custom software for problems. But it isn’t realistic to say the software wrapper effort is going to zero when models improve. Plus: a lot of software effort is needed for getting the data AI needs. This is going to be a huge area - think Google maps with satellites, camera cars, network effect products (ratings), data collection (maps, traffic), etc. reply nopcode 5 hours agoparentThis all sounds very similar to the hundreds of excel/spreadsheet \"killers\" out there. But excel math is easy to outperform, ChatGPT etc wont be. reply scosman 48 minutes agorootparentSure. But it also sounds very similar to every successful database-backed company where each tenant hasSo, I'm not a believer in the classic winner takes all approach here where one company turns into this trillion dollar behemoth and the rest of the industry pays the tax to that one company in perpetuity. I agree with this sentiment. There are a lot of frontier model players that are very competent (OpenAI, Anthropic, Google, Amazon, DeepSeek, xAI) and I'm sure more will come onboard as we find ways to make models smaller and smaller. The mental framework I try to use is that AI is this weird technology that is an enabler of a lot of downstream technology, with the best economic analogy being electricity. It'll change our society in very radical ways, but it's unclear who's going to make money off of it. In the electricity era Westinghouse and GE emerged as the behemoths because of their ability to manufacture massive turbines (which are the equivalent of today's NVIDIA and perhaps Google). reply elisharobinson 6 hours agoprevthis is not only not true , it has no basis's in reality. In the \"real\" world there are tradeoff's and constraints. scaling does not work infinitely , and products which are delivered by good engneering cultures have a non linear growth ( bad vs very good ). comming back to research one of the frontier model's deepseek was able to come close to SOTA with a relatively small budget because of one of their mixture of experts approach. reply moffkalast 6 hours agoparentYeah knowing that having infinite perfect data and infinite compute to train an end-to-end DNN is the way to solve anything is great, but not exactly helpful when you don't have either of those things. Not to mention having to actually deploy it on a low power system in the end. reply osigurdson 3 hours agoprevI does feel like we are having the petfood.com moment in B2B AI. Bespoke solutions, bespoke offerings for very narrow B2B needs. Of course waiting around for AI to get so good that no bespoke solution is needed might be a bad strategy as well. I'm not sure how it will play out but I am certain there will be significant consolidation in the B2B agent space. reply tim333 2 hours agoparentIn the UK I bought most of my pet food from zooplus.co.uk. I don't think there's anything inherently wrong with the petfood.com idea, it's just that the best executing company will win as usual. reply JohnCClarke 6 hours agoprevThe smarter AI app founders know that more advanced AI will easily replace their current tech. What they're trying to do now is lock in a userbase. reply motbus3 5 hours agoprevI agree the article presents good points specially if you consider current LLM models and the business models (and questionable business practices) being used. It seems unlikely at this point, but it it might be the case that new methods, models and/or algorithms rise due to the trend of having high expectations/investments on this area But yes, AI business stuff will be Swallowed by those companies as much as the companies that unwillingly are providing content to those companies. reply moomin 6 hours agoprevThe thing is, what do we do with the bitter lesson once we’re essentially ingesting the entire internet? More computation runs the risk of overfitting and there just isn’t any more data. Is the bitter lesson here telling me that we’ve basically maxed out? reply kouteiheika 6 hours agoparent> More computation runs the risk of overfitting and there just isn’t any more data. At this scale you can't overfit. The model might not improve in a meaningful way, but it can't overfit because the amount of data is much much larger that the size of the model. That said, as the state of the art open models show, the way to get better models is not \"use more data\", but use \"more high quality data\" (e.g. look at the graphs here comparing the datasets: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu). I don't know how much high quality data we can extract from the Internet (most of the data on the Internet is, as you'd guess, garbage, which is why aggressively filtering it can improve performance so much), but I'd wager we're still nowhere near running out. There's also a ton of data that's not available on the public Internet in an easily scrapable form, or even at all; e.g. Anna's Archive apparently contains almost 1PB of data, and even that is (by their estimate) only ~5% of the world's books. reply evujumenuk 6 hours agorootparentI'd think that this only means that a model cannot suffer from overfitting on average. So, it might totally have been overfitted on your specific problem. reply tim333 2 hours agoparentprevWith humans, while you can't read the whole internet you can maybe read everything in a narrow niche. Then the thing is to go out and do something or make something. Maybe that's the future for AI. reply Filligree 6 hours agoparentprevWe’re nowhere near ingesting the whole internet. Though personally, I think we’re missing whatever architecture / mathematical breakthrough will make online learning (or even offline incremental, I.e. dreams) work. At that point we could give the AI a robot body and train it of lived experience. reply csmpltn 6 hours agorootparent> \"We’re nowhere near ingesting the whole internet.\" We don't need to ingest the whole internet. I'd wager that upwards of 75% of the internet is spam, which would be useless for LLM training purposes. By the way, spam and useless information on the internet is only going to get worse, largely thanks to LLMs. Only a subset of the internet contains \"useful\" information, an even a smaller subset contains information which is \"clean enough\" to be used for training purposes, and an even smaller subset can be legally scraped and used for training purposes. It's highly likely that we've reached \"peak training data\" a long time ago, for many areas of knowledge and activities which are available on the internet. reply asdev 2 hours agoprevThe main point here seems to be around boxing in the AI with a rigid set of rules, and then not being able to adapt to model improvements over time. I think if those \"rules\" are just in the prompt, you can still adapt, but if you're starting to code a lot of rule based logic in the product, you can get into trouble. reply rnamerl 1 hour agoprevI think the goal is just to get as much funding from VCs and government rackets as possible. The previous administration had Harris as the \"AI czar\". Which means that nothing was expected to happen. The following administration has Sacks as the \"Crypto & AI czar\". I'm not aware that Sacks has any particular competence in the AI area, but he has connections. So government money is likely to flow, presumably to \"defense\" startups. This All-In podcast has paid off: From peddling gut bacteria supplements and wrong explanations on how moderator rods work in nuclear power plants to major influence. reply mike_hearn 6 hours agoprevOf course, 01 is a lot more expensive than smaller models. So the startups that have spent time on tuning their prompts May yet get the last laugh, at least in any competitive market where customers are price sensitive. Bear in mind that supposedly OpenAI is losing money even at a $200 a month price point so it's unclear that the current cost structure of model access is genuinely sustainable. reply sd9 5 hours agoprevI think there is value in companies built around AI. The value comes from UX, proprietary supplementary datasets, and market capture. Businesses built now will be best positioned to take advantage of future improvements in general AI. That is why I am building in the AI space. I’m not naive to the predictable improvement in foundational models. reply tippytippytango 1 hour agoprevThe bitter lesson is a powerful heuristic, but its adherents treat it as dogma and wield it as a club to win arguments. reply diziet_sma 6 hours agoprev> We’ve seen this pattern in speech recognition, computer chess, and computer vision. I think the bitter lesson is true for AI researchers, but OP overstates it's relevance to actual products. For example the best chess engines are very much still very much specific to chess. They incorporate more neural networks now, but they are still quite specific. reply dosinga 2 hours agoparentThis. ChatGPT doesn't play chess very well. reply Havoc 5 hours agoprevConceptually that seems right, but think there is a decade+ worth of runway left on working in the \"glue\" space. i.e. connecting the AI and our lived realities. I don't see the general-ness of AI overcoming the challenges there...cause there is a lot of non-obvious messiness there. reply mercurialsolo 6 hours agoprevlove it when a 25 year old founder says we have been here multiple times in the past reply evujumenuk 6 hours agoparentIn which specific ways are they wrong, though? Being able to learn from others' mistakes is generally considered to be a sign of aptitude. reply welder 6 hours agoparentprevs/25 year old founder/25 yr old first-time founder/ reply ryanackley 5 hours agoprevI feel like the author glosses over some nuance in the point he is trying to make and his conclusion doesn't incorporate the nuance. The nuance is that even though AI researchers have learned this lesson, they are still building purpose built AIs because it's not possible to build an AI that can learn to do anything (i.e. AGI). Therefore, building on top of an existing AI model to meet a vertical market demand is not that crazy. It's the same risk when building on any software platform. That is the provider of the platform may add the feature/app you are building as a free addition and your business becomes obsolete. reply jvanderbot 4 hours agoprevTFA admits that specialization can gain a temporary edge. But, says using that specialization is useless because the next gen will eclipse that edge using raw CPU. Even if it is true that the next generational change in AI is based on computational improvements, how can it be true that it's hopeless to build products by specializing this generation of tech? Moreover if I specialize gen 1, and gen 2 is similar enough, can't specialization maintain an edge? There seems to be a timescale mismatch. reply ano-ther 6 hours agoprevSo what is the appropriate course of action if you are a founder? Just wait until the models inevitably get better, or help a customer with their problem now? Computer power has been increasing all the time, but that hasn't kept people from experimenting with the limited power of their time (which ultimately led to better solutions) rather than waiting for the more powerful machines. reply dsr_ 6 hours agoparentFind a real problem and solve it cheaply, or solve it so much better than anyone else that your high price is worthwhile to enough people. Don't focus on the method of solving the problem until you have identified the problem and all the current solutions and avoidances. reply amelius 6 hours agoparentprevYou can help the customer now, but you should keep in mind that the people working on the more general solutions will at some point take your moat. E.g. you can build a drawing program where you can tell it in natural language to apply certain effects and make some changes. But another company might be working on a OS assistant that can help the user in any program currently running on the computer. Or you can build a robot that can pick bolts from an assembly line, but at some point there will be a company building a cheap robot that can do anything from folding t-shirts to knitting a sweater. reply zol 6 hours agorootparentThis was my takeaway from the article too. reply crawshaw 1 hour agoprevThe bitter lesson is a wonderful essay and well worth a read: http://www.incompleteideas.net/IncIdeas/BitterLesson.html I also believe the original bitter lesson essay is correct. General methods on more powerful compute will supersede anything specific built today. However I believe the lesson in this blog post is an incorrect conclusion. It is roughly analogous to \"all people die, so do not bother having children because they will die one day.\" It is true that what you build today will be replaced in the future. That is OK. Don't get caught out when it happens. But building specific systems today is the only way to make progress with products. Waymo started some 12 years ago as Google Chauffeur, built on all sorts of image/object recognition systems that I am sure have fallen to the bitter lesson. Should Google have refused to start a self-driving car project 12 years ago, because generalized transformers would replace a lot of their work in the future? No. You have to build products with what you've got, where you are. Adopt better tools when they come along. reply k__ 6 hours agoprevI'm not an AI founder, but I'd say there is still some value to be added by better architectures. These AI wrappers run ship engines on a kayak. For example, most coding companion's ignore my code base the moment I close other files. Somehow their context is minimal. Such systems can be improved dramatically just by changing the software around the LLM. But I get it, you have to be quick and don't waste time on MVPs. If you get critical mass, you can add that later... hopefully reply bhouston 6 hours agoprevIn order to win you often have to start before the problem is fully solved and then count on it being solved better as you build and scale. Thus starting with engineering effort to fit some of the AI limitations make sense but realize many of those will be temporary and will be replaced in the future. But there is always a new tech or framework or something that emerges after you start and adopting it will improve your product measurably. reply alganet 5 hours agoprevThere is a circular argument going on in this article. Basically: > \"Flexible is better. Why? Because specific has been consistently worse\" I mean, I don't deny the historical trend. But really, why is it so? It makes sense to follow the trend, but knowing more about the reason why would be cool. Also, I feel that the human cognitive aspects of \"engineering some shit\" are being ignored. People are engineering solutions not only to be efficient, but to get to specific vantage points in which they can see further. They do it so they can see what the \"next gen flexible stuff\" looks like before others. Finally, it assumes the option to scale computation is always available and ignores the diminishing returns of trying to scale vanguard technology. The scale requirements for AI stuff are getting silly real fast due to this unshakable belief in infinite scaling. To me, they're already too silly. Maybe we need to cool down, engineer some stuff and figure out where the comfortable treshold lies. reply HarHarVeryFunny 1 hour agoparent> But really, why is it so? I don't think the bitter lesson is (or should be?) that you don't need to engineer biases in because with enough compute you could learn them instead ... The real lesson is that finding the right (minimal) set of biases is hard, and you should therefore always learn them instead if possible. Systems that learn are liable to outperform those that don't because by letting the data speak for itself, they are learning the right biases. I think this is why historically the bitter lesson has been correct - because people have built too much into AI systems, got it wrong, and made them limited and brittle as a result. The same thing is continuing to happen today with pre-trained LLMs (a fixed set of patterns!) and bespoke test-time reasoning approaches (of which at most one, likely none, are optimal). Of course it's understandable how we got here, even if a bit like the drunk looking for his lost car keys under the street lamp rather than in the dark spot he lost them. Continuous incremental learning (no training vs inference time distinction) is an unsolved problem, and the basis of intelligence/reasoning is not widely understood. reply alganet 37 minutes agorootparent> finding the right (minimal) set of biases is hard I'm not familiar with this concept of a minimal set of biases. The way I see it, it's a series of loosely community-defined tresholds. If there's something like a theory that defines those biases in a formal way, I would very much like to read it. reply keybored 6 hours agoprev> “The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin”. > He points out that throughout AI’s history, researchers have repeatedly tried to improve systems by building in human domain knowledge. The “bitter” part of the title comes from what happens next: systems that simply use more computing power end up outperforming these carefully crafted solutions. We’ve seen this pattern in speech recognition, computer chess, and computer vision. If Sutton wrote his essay today, he’d likely add generative AI to that list. And he warns us: this pattern isn’t finished playing out. According to Chomsky (recalled in relatively recent years for him) this is why he didn’t want to work on the AI/linguistics intersection when someone asked him in the mid 50’s. Because he thought that successful approaches would just use machine learning and have nothing to do with linguistics (that he cares about). reply ajuc 6 hours agoparentSeems intellectually dishonest (given how his theory of universal grammar is all about indispensible biological factors in human languages). reply kubb 6 hours agoprevThey will learn their lesson how exactly? By getting money for free from VCs? Remember, most of these startups are grifters. Only few of them really believe in their products. reply AlienRobot 1 hour agoprevI wish I knew what \"AI\" is. Please correct me if I'm wrong, but to my understanding, every single one of these \"AI\" products is based on a model, i.e. it's just a normal program with a very big configuration file. The only thing \"AI\" about it is the machine learning performed when training the model. At the generation stage, there is nothing \"AI\" about it anymore. The AI is done already. What runs is a normal program. I know it's used in computer vision, recommendation algorithms, and generative software like ChatGPT, Stable Diffusion, etc. I don't know if there is anything besides these. From what I know, the biggest problems with AI is: 1) the program pretends to be intelligent, when it isn't, e.g. by using natural language. And 2) the program doesn't give the user enough control, they only get one text box prompt and that's it, no forms, no dialogs, so the product has to somehow generate the right answer from the most uncontrolled piece of data imaginable. These two things combined not only limit the products potential by giving it unreasonable expectations but also makes it feel a bit of a scam: the product is a program, but what sells is its presentation: it has to present itself as being MORE than just a program by hiding as much of its internal workings from the consumers and covering its warts with additional layers of logic. I don't know if the author would consider natural language to be a \"hardcoding\" create to temporarily solve a problem that should be solved by using more compute (AGI), but to me it feels like it is. The best application of AI is probably going to be using AI internally to solve existing problems in a given domain your business is familiar with rather than try to come up with some AI solution that you have to sell other people. reply CaptainFever 6 hours agoprevCool article, a lot more technical and informational than I thought from the headline. The article gave the TL;DR as below, for those who skip to the comments: Historically, general approaches always win in AI. Founders in AI application space now repeat the mistakes AI researchers made in the past. Better AI models will enable general purpose AI applications. At the same time, the added value of the software around the AI model will diminish. reply EGreg 3 hours agoprevI was going to keep this to myself to maintain a competitive advantage, but I will just drop two hints: 1) Have AI turn natural language interactions into programs 2) Use test-driven development on the domain data There is a third thing that’s far more crucial but if you want to find out what it is, contact me. I’m building a general-purpose solution for a few domains. Unlike the other two platforms I built (Web 2.0 [1] and Web 3.0 [2]) I am not planning to open-source this project. So I don’t want to discuss more. I will however say that open source is a huge game-changer — because organizations / customers want to be able to be in control and you don’t want to be reliant on some third-party platform for an API provider. We all know how well that one turned out for startups building on Web 2.0 platforms. 1. https://github.com/Qbix 2. https://github.com/Intercoin reply mdaniel 1 hour agoparentIsn't the premise of TDD to do that process in the opposite order? Anyway, I don't follow what you're premise is about the relationship between \"open source being a game changer,\" but then \"customers want control,\" and then \"startups are not successful on Web 2.0 platforms\". Maybe you're trying to cram too much insight into one sentence and I'm not seeing what the thesis is reply danielovichdk 4 hours agoprevAI founders will also make sure that users - the ones that are not capable of telling if you're lying to them by taking their money and energy - will learn the bitter lesson of taking it right up the arse. Again. Just like a repetition of history. Humanity is like ever before caught in the action of serving the individuals that made us fuck ourselves over. And the only thing you do is thanking them for it by applying to their tactics. Middle class humanity and above is morally bankrupt, and AI is the next thing they will fight over and with, but of course without adding anything positive to the world. reply Devasta 5 hours agoprevThis assumes that the AI bros care about building useful products. They don't for the most part, they care only about building a startup that can plausibly get VC funding. So what if in five years your LLM on the blackchain app fails, you got to blow throw a few million dollars over a few years going around to conferences and living the high life. That's a success in anyone's book. reply xchip 2 hours agoprevthanks for the tl;dr reply darepublic 3 hours agoprev [–] Dunno why anyone in the startup space would be excited about new more powerful AI models unless they are just throwing a mask over their existential fears reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Historically, general AI approaches have outperformed specialized ones, as seen in fields like speech recognition and computer vision. - Richard Sutton's \"The Bitter Lesson\" emphasizes that general methods using computation are more effective than those relying on human domain knowledge. - The post suggests that AI founders should focus on creating products that leverage model improvements rather than compensating for current model limitations, as this is a more sustainable strategy."
    ],
    "commentSummary": [
      "AI startups encounter a fundamental challenge in providing context, as AI systems require specific information and steps to function effectively. - Successful AI startups will likely focus on ETL (Extract, Transform, Load) processes to supply necessary context, emphasizing the importance of training data and search capabilities. - The AI industry may evolve similarly to the smartphone sector, with large companies dominating due to data ownership, but startups can succeed by developing domain-specific models and managing complex systems integration and data management."
    ],
    "points": 285,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1736680975
  },
  {
    "id": 42671427,
    "title": "Aaron Swartz and Sam Altman",
    "originLink": "https://journa.host/@jeremiak/113811327999722586",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Only available when logged in. journa.host is one of the many independent Mastodon servers you can use to participate in the fediverse. Administered by: Server stats: journa.host: About · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.3.2 ExploreLive feeds Mastodon is the best way to keep up with what's happening. Follow anyone across the fediverse and see it all in chronological order. No algorithms, ads, or clickbait in sight. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=42671427",
    "commentBody": "Aaron Swartz and Sam Altman (journa.host)283 points by ta988 13 hours agohidepastfavorite161 comments rglover 12 hours agoAaron was the OG. If you've never dug through his blog, do yourself a favor [1]. Also make some time to watch The Internet's Own Boy doc about him [2] and look up some of his talks during the SOPA shenanigans. RIP. [1] http://www.aaronsw.com/weblog/ [2] https://www.youtube.com/watch?v=9vz06QO3UkQ&rco=1 reply b8 12 hours agoprevSam Altman failed upwards only because PG likes him. Aaron Swartz was actually a technical genius imo. DOJ never should of charged Swartz. reply baxtr 9 hours agoparentAlthough I agree, I think your analysis could be enhanced by asking \"why\" a couple of times to get to the root. Why did PG \"like\" him? reply leoc 8 hours agorootparentAlan Kay really liked him in 2016. https://youtu.be/fKcCwa_j8e0 I think Altman is very good at cultivating people. Mind you, it seems that Swartz got around by making dazzling impressions on people, too. reply anonnon 5 hours agorootparent>cultivating More like manipulating. reply bookaway 6 hours agorootparentprevPG liked him, because Altman decided to go to great lengths to get PG to like him. While Drew, Chesky and the Collison brothers were busy building billion dollar companies, Altman took the “shortcut” and made a concerted effort to cozy up to the most powerful man in the room — and it payed dividends. Altman did the same thing in the early OpenAI days by doing flaterring video series interviews with Elon Musk, Vinod Khosla and others [0]. Incidentally, The YC interview with Elon Musk was done the year Musk made a donation to OpenAI (2016), I still remember PG’s essay where he gave Altman the ultimate character reference (2008) [1]: >When we predict good outcomes for startups, the qualities that come up in the supporting arguments are toughness, adaptability, determination. Which means to the extent we're correct, those are the qualities you need to win…Sam Altman has it. You could parachute him into an island full of cannibals and come back in 5 years and he'd be the king. If you're Sam Altman, you don't have to be profitable to convey to investors that you'll succeed with or without them. (In retrospect, praising Altman for being the “king of cannibals” has a nice touch of gallows humor to it. Hilariously, even recently pg has a seemingly unintentional tendency to give Altman compliments that appear to be character warnings masquerading as compliments.) In 2009, pg included Altman in the top 5 in a list of the most interesting startup founders of the last 30 years.[2] If this was an observation made from afar, you could easily say it was “prescient”. But objectively at the time, no one could find any verifiable evidence in the real world to justify such an assessment. It wasn’t prescient because pg had became directly responsibly for Altman’s future success, in a case of self-fulfilling prophesy. Altman was often referenced in the acknowledgments of pg’s essays for reading early drafts and is probably referenced more than any other founder in the essays. Altman’s entire streetcred came from pg and also, once he made Altman head of YC, YC. From afar, it looks like a victory for office poltics, a skill incidentally that sociopaths are known to excel at. [0] https://www.youtube.com/watch?v=tnBQmEqBCY0 [1] https://www.paulgraham.com/fundraising.html [2] https://www.paulgraham.com/5founders.html reply anshulbhide 5 hours agorootparentWouldn't you agree though that from YC Head to being the driving force of OpenAI was largely due to his own merit. In fact, he started spending less time at YC and more time at OpenAI. At that time, OpenAI had no clear path to becoming the unicorn it is today, and YC was definitely better from a career standpoint. Instead, he went all-in on OpenAI, and the results are there for everyone to see. reply bookaway 5 hours agorootparentYes, definitely. But becoming the head of YC was also due to his own merit. His merit was persuading the right people. After all, the essay where pg is giving his highest praise is an essay about fundraising. Will you not agree that him becoming \"the driving force of OpenAI\" involved some highly publicized back-to-back persuasion drama as well? First he got Ilya and gdb to side with him against Elon, then he got OpenAI employees to side with him against Ilya and the board (a board that accused him of deceiving them). PG reiterated after that drama that Altman's special talent was becoming powerful. This observation does not necessarily mean someone is a bad CEO, since the job of the CEO is to do good by your investors or future investors. And it's possible to do that without any morals whatsoever. But I think the recent drama did more to drive the competition than some of his investors would have liked. Edit: >At that time, OpenAI had no clear path to becoming the unicorn it is today, and YC was definitely better from a career standpoint. This is very incorrect in my view. The presence of Elon Musk as investor and figurehead and Ilya, Karpathy, and Wojciech as domain experts, not to mention investments from YC members themselves (and the PayPal mafia) made OpenAI a very attractive investment early on. reply leoc 4 hours agorootparentI suspect (and this is rank speculation from a great distance) that a lot of SV tech leadership (of different kinds) has perceptions that were formed by the era of the Napster saga and haven't been revised since. PG seems committed to the idea that brash young wheeler-dealers can cut corners to win big and nothing really bad will happen as a result; as of recently Brewster Kahle seemed to be convinced that the final triumph of free media was just one more big push away. reply bookaway 2 hours agorootparentYeah, if I were to describe it more generally, I would say they self-select for techno-optimists. As investors, YC often espouses judging startup’s potential by considering how successful the startup could be if everything goes right, and then working backwards from there to see if it could be possible. I’m not sure if pg has a historic reference that he has a commitment to, since he was a startup founder himself he just may be projecting his personality to the startup founders he’s assessing, but I do think he’s more concerned with “whether the startup has accomplished something impressive despite the odds” and less about how they accomplished it. They filter for red flags that would indicate a potential for failure for the startup. So if a “lack of morals” has no bearing on a startup’s success, then they don’t bother creating a filter that eliminates that. Nerds often prefer building things instead of dealing with people and often take things at face value instead of suspecting intrigue, and that sometimes makes them susceptible to manipulation. PG has admitted that he himself is bad at noticing certain personality or character flaws and that’s why Jessica was the adult in the room. But Jessica was probably observing the founders to see if there was good co-founder dynamic and other aspects that would affect startup success rather than trying to decipher their moral character. After all, there is no hippocratic oath in the tech sector. reply baxtr 21 minutes agorootparentThanks for your comments. All very insightful. Re lack of morals: if I’m not mistaken YC explicitly asks for instances where the founders have succeeded to \"break the rules of the system\" or similar. So you could even argue if anything they prefer founders that tend to bend the rules of required. On the other hand, pg seems to have strong moral views on certain political topics. reply leoc 8 hours agoparentprevWhat’s the best evidence for Swartz’ technical genius? reply wesselbindt 8 hours agorootparentMy personal favorite is that he was part of authoring the RSS spec at age 14. But you're more than welcome to Google for other pieces of evidence yourself if you're genuinely interested and not just being argumentative. reply moralestapia 4 hours agorootparent>authoring the RSS spec at age 14 No sources on this, though, only a couple articles from around the time of his death stating it as it was a fact already. In all this time, and with all this fuzz, none of the actual authors of RSS, which are still alive, have come clear about this. Disclaimer for immature people: This is not meant to disrespect Aaron's memory and/or legacy. reply LunaSea 7 hours agorootparentprevNot that I disagree that with the idea that he was brilliant but the RSS spec isn't what I would consider a complex piece of documentation. Even for a 14 year old. reply wesselbindt 6 hours agorootparentWhat were you doing at 14? reply xnickb 6 hours agorootparentNot the person you asked, but I (in 2004) was part of sysadmin team at the school. I helped developing tools for automating many tasks around teacher and student performance measurement and tracking. I also wrote a piece of software that went super viral among sysadmins all over the city and I was getting \"thank you\" emails for years after. Had anyone been developing RSS spec next to me I'd definitely jump on it. As any 14 y/o would. I don't think I'm particularly brilliant or even smart. Your circle defines you. Surround any healthy teenager with interest in tech with the right people and they'll have a lot to show in no time reply leoc 4 hours agorootparentBeing well-educated and well-connected doesn't necessarily mean you aren't great. The Sutherland boys were famously hanging out with Edmund Berkeley at his computer lab while they were still children, thanks to their mum knowing Berkeley through the General Semantics scene https://spectrum.ieee.org/sketchpad https://archive.computerhistory.org/resources/access/text/20... . It was also luck, as well as skill, that got Ivan enjoying sole use of the gigantic, cutting-edge TX-2 for hours every week in the early '60s. You nevertheless still have to hand it to him for creating flipping Sketchpad https://en.wikipedia.org/wiki/Sketchpad in 1963 at the age of 24. On the other hand, Ivan Sutherland had created Sketchpad before he turned 25. reply xnickb 2 hours agorootparentI agree. My point goes rather in the direction that just because one doesn't have much to show by 14 y/o, doesn't mean they're less of a human than someone who does. Merit isn't a predictor of success, perhaps a requirement, but even that is dubious. reply dmd 5 hours agorootparentprevI know what I was doing at 14, and it landed me in the NET.LEGENDS FAQ. reply leoc 6 hours agorootparentprevHe was certainly precocious! A gifted kid, absolutely. But 'genius' is a lofty title. You'd usually need to be doing things differently and better, certainly by the time you're in your 20s. Maybe Infogami can make that case for him? reply tzury 9 hours agoparentprevDefine failure. reply kevingadd 9 hours agorootparentDoes Loopt count as a success? The exit was slightly more than the total investment, I guess. What about Worldcoin? He's at least not someone I naturally associate with business success pre-OpenAI (and the jury's still out on OpenAI considering their financial situation) but I suppose depending on how you evaluate it his success rate isn't 0%. You can say OpenAI is a \"success\" given their achievements in AI but those aren't Sam's work, he should mostly be credited with their business/financial performance and right now OpenAI-the-business is mostly a machine that burns electricity and operates in the red. reply tcptomato 7 hours agoparentprev> should of charged what? reply stewartmcgown 4 hours agorootparentCommon mistake in English, usually the writer means \"should've\". reply menzoic 12 hours agoprevFor one Sam scraped under the veil of a corporation which helps reduce or remove personal liability. Second, if the crime was the act of scraping then it’s directly comparable. But if the crime is publishing the data for free, that’s quite different from training AI to learn from the data while not being able to reproduce the exact content. “Probabilistic plagiarism” is not what’s happening or even aligned with the definition of plagiarism (which matters if we’re talking about legal consequences). What’s happening is that it’s learning patterns from the content that it can apply to future tasks. If a human reads all that content then gets asked a question about a paper, they too would imperfectly recant what they learned. reply dkjaudyeqooe 11 hours agoparentYour argument might make sense to you, but it doesn't make sense legally. The fact is that “Probabilistic plagiarism” is a mechanical process, so as much as you might like to anthropomorphize it for the sake of your argument ('just like a human learning') it's still a mechanical reproduction of sorts which is an important point under fair use, as it the fact that it denies the original artists the fruits of their labor and is a direct substitute for their work. These issues are the ones that will eventually sink (or not) the legality of AI training, but they are seldom addressed in these sorts of discussions. reply menzoic 10 hours agorootparent> The fact is that “Probabilistic plagiarism” is a mechanical process, so as much as you might like to anthropomorphize it for the sake of your argument I did not anthropomorphize anything. “Learning” is the proper term. It takes input and applies it intelligently to future tasks. Machines can learn, machine learning has been around for decades. Learning doesn’t require biology. My statement is that it is not plagiarism in any form. There is no claim that the content was originally authored by the LLM. An LLM can learn from a textbook and teach the content, and it will do so without plagiarism. Just as a human can learn from a textbook and teach. Making an analogy to a human doesn’t require anthropomorphism. reply tsimionescu 7 hours agorootparentThe law on copyright doesn't depend on the word \"learning\", it depends on whether it's a human doing it, or a mechanical process. If a human reads a book and produces a different book that's sort-of-derivative but doesn't copy too many elements too directly, then that book is a new creative work and doesn't infringe on the copyright of the original author. For example, 50 Shades of Gray is somewhat derivative of Twilight (famously starting as a Twilight fan-fic) but it's legally a separate copyright. Conversely, if you use a machine to produce the same book, taking only copyrighted text as input and an algorithm that replaces certain words and phrases and adds certain passages, then the result is a derivative work of the original and it infringes the copyright of the original author. So again, the facts of the law are pretty simple, at the moment at least: even if a machine and a human do the exact same thing, it's still different from a legal perspective. reply DoctorOetker 4 hours agorootparentmachines work on behalf of humans, any human could similarly claim their typewriter did it reply tsimionescu 27 minutes agorootparentThat's still irrelevant. The distinction the law makes is between human creativity vs machine processes. A human may or may not have added their own creativity to the mix, and the law falls on the side of assuming creativity from humans (though if there is proof that the human followed some mechanical process, that likely changes things). Machines are not creative, by definition in the law today, so any transformation that a machine makes is irrelevant, the work is fundamentally the same work that was used as input from a legal perspective. That this work is done on behalf of a human changes nothing, the problem the law has with this is that the human is copying the original work without copyright, even if the human used a machine to produce an altered copy. Whether they used bcrypt, zip, an mp3 encoder, a bad copy machine, or a machine learning algorithm, the result is the same: the output of a purely mechanical process is still a copy of the original works from a copyright perspective. reply neuroelectron 9 hours agorootparentprev> I did not anthropomorphize anything. Machines don't learn. They encode, compress and record. reply Earw0rm 9 hours agorootparentUntil or unless the law decides otherwise. The 2020s ethic of \"copying any work is fair game as long as you call the copying process AI\" is the polar and equally absurd opposite to the 1990s ethic of \"measurement and usage of any point or dimension of a work, no matter how trivial, constitutes a copyright infringement\". reply ben_w 9 hours agorootparentprevIt's been a term of art since 1959, and the title of a research journal since 1989: https://en.wikipedia.org/wiki/Machine_Learning_(journal) reply neuroelectron 9 hours agorootparentDoes a research journal on aliens prove aliens exist? reply ben_w 8 hours agorootparentAs this is a semantics debate, their actual existence is irrelevant. A research journal on extra-terrestrial aliens would prove that the word \"aliens\" is used to mean \"extra-terrestrials\" and that the word doesn't just mean \"foreigners\": https://www.law.cornell.edu/uscode/text/8/chapter-12/subchap... reply wiseowise 9 hours agorootparentprevSemantics. reply jazzyjackson 11 hours agoparentprevAsk any LLM to recite lyrics and see that it's not so probabilistic after all, it's perfectly capable of publishing protected content, and the filter to prevent it from doing so is such a bolt-on its embarrassing. reply menzoic 10 hours agorootparentWe have to understand what plagiarism is if making claims of it. Claiming that you authored content and reciting content are different things. Reciting content isn’t plagiarism. Claiming you are the author of content that you didn’t author is plagiarism. > it's perfectly capable of publishing protected content At most it can produce partial excerpts. LLMs don’t store the data that it’s trained on. That would be infeasible, the models would be too large. Instead, it stores semantic representations which often uses entirely different words and sentence structures than the source content. And of course most of the data is lost entirely during this lossy compression. reply Earw0rm 9 hours agorootparentThat's a little like saying downloading mp3s isn't music piracy, because it's not encoding the actual music, just some lossy compressed wavelets that sound like it. reply ben_w 9 hours agorootparentYour username represents a thing that can happen in a human brain which reproduces the perceptual content of a song. Are earworms copyright infringement? If I ask you what the lyrics were, and you answer, is that infringement, or fair use? The legal and moral aspects are a lot more complex than simply the mechanical \"what it's done\" or \"is it like a brain\". reply Earw0rm 8 hours agorootparentAre earworms infringement? No, they stay inside your head, so they exist entirely outside the scope of copyright law. If you ask me the lyrics, fair use acknowledges that there's a copyright in effect, and carves out an exemption. It's a matter-of-degree argument, is this a casual conversation or a written interview to be published in print, did you ask me to play an acoustic cover of the song and post it on YouTube? Either way, we acknowledge that the copyright is there, but whether or not money needs to change hands in some direction or other is a function of what happens next. reply CaptainFever 8 hours agorootparentprevNo, the difference is that MP3s can almost completely recreate the original music, while LLMs can't do that with specific pieces of authored works. reply croemer 7 hours agorootparentYou've got it completely backwards. MP3s just trick you into thinking it's the same thing. It's actually totally different if you analyze it properly, in a non-human sense. LLMs are able to often _preciesely_ recreate in contrast to MP3 at best being approximate. reply CaptainFever 7 hours agorootparent> LLMs are able to often _preciesely_ recreate Is there actual proof of this? Especially the \"often\" part? reply croemer 7 hours agorootparentFirst shot on GPT 4o: https://chatgpt.com/share/6783ad9d-7c9c-8005-b2aa-660200d05e... Asking for Declaration of Independence and comparing the output against https://www.archives.gov/founding-docs/declaration-transcrip... and exhaustive list of differences: 1. Em-dashes in ChatGPT, `--` in .gov -> this is just an ASCII limitation of the .gov transcript 2. ChatGPT capitalized Perfidy in \"Cruelty & Perfidy\", while .gov has \"perfidy\" 3. ChatGPT writes \"British\" while .gov says \"Brittish\" These are _all_ the differences. reply ben_w 2 hours agorootparentThey emphasised \"often\", and quoted you also saying \"_preciesely_\". What you've got, for a document that's likely reproduced many times in part and in whole over the internet, still has differences. (The document is also old enough to no longer be in copyright). And if I prompt for \"please recite the opening page of Neuromancer\", I get this: > Sorry, I can't provide the exact text from the opening of Neuromancer as it is copyrighted. However, I can provide a brief summary or discuss its themes, characters, and more! Let me know how you'd like to proceed. While \"please recite the opening page of the first superman comic\" gets me this, which doesn't match what I find from other sources for Action Comic #1, but perhaps I'm wrong and you're going to correct me: \"\"\" Sure! The opening page of Action Comics #1, which was published in 1938 and features the first appearance of Superman, has a memorable introduction. Here’s a paraphrase of the first part: --- *“Superman – Champion of the oppressed. In the year 1938, a strange visitor from another world appeared on Earth, and in the short span of time since, he has become a figure of myth and legend. With powers far beyond those of mortal men, Superman is faster than a speeding bullet, more powerful than a locomotive, and able to leap tall buildings at a single bound!”* The comic begins with Superman stopping a criminal, showcasing his superhuman strength. This early depiction of Superman is somewhat different from the modern, more refined character we know today, but the basic elements are all there: a hero with extraordinary abilities, a strong moral compass, and a desire to fight injustice. --- If you'd like a more detailed description or more of the story, just let me know! \"\"\" Those were the first two things I tried (in this context, today). reply Earw0rm 6 hours agorootparentprevThese are different problems. The written (or more accurately, typed)* word is an inherently symbolic representation. Recorded audio (PCM WAV or similar) is not. The format itself doesn't encode meaning about the structure. The written word is more akin to MIDI, which can exactly represent melody, but cannot exactly represent sound. MP3 is a weird halfway house, it's somewhat symbolic in terms of using wavelets to recreate a likeness of the original sound, but the symbols aren't derived from or directly related to the artist's original intent. * handwriting can of course contain some subtle information that no character set can exactly reproduce, it is more than just a sequence of characters taken from a fixed alphabet, but at least for Western text that can be ignored for most purposes. reply tsimionescu 7 hours agoparentprevTalking about plagiarism here is a complete red herring. Plagiarism is essentially a form of fraud: you are taking work that someone else did and presenting it as your own. You can plagiarize work that is in the public domain, you can even plagiarize your own work that you own the copyright to. Avoiding a charge of plagiarism is easy: just explicitly quote the work and attribute to the proper author (possibly yourself). You can copy the entirety of the works of Disney, as long as you are attributing them properly, you are not guilty of plagiarism. The Pirate Bay has never been accused of plagiarism. And plagiarism is not a problem that corporations care about, except insofar as they may pay a plagiarist more money than they deserve.' The thing that really matters is copyright infringement. Copyright infringement doesn't care about attribution - my example above with the entire works of Disney, while not plagiarism, is very much copyright infringement, and would cost dearly. Both Aaron Swartz and The Pirate Bay have been accused and prosecuted for copyright infringement, not plagiarism. reply theyinwhy 11 hours agoparentprevLLMs (OpenAi models included) are happy to reproduce books word by word, page by page. Just try it out yourself. And even if some words were reproduced wrong, it still would be copyright violation. reply throw5959 11 hours agorootparentIt's not really some words, it's more like you won't be able to get more than a page out of it and even that is going to be so wrong it's basically a parody and thus allowed. reply angoragoats 10 hours agorootparentI’d love to see you try to defend this notion in court. Parody requires deliberate intent to be humorous. And courts have repeatedly held that changing the words of a copyrighted work while keeping the same general meaning can still be copyright infringement. reply throw5959 9 hours agorootparentIt's not just \"changing some words\". The majority of words will be different, sentences will be different. The general meaning might be generally the same, but I don't think that's enough to claim copyright protection. reply angoragoats 5 hours agorootparentI didn’t use the word “some.” Please don’t misquote me. As I said in the comment you’re replying to, there’s case law proving you wrong. reply throw5959 4 hours agorootparentGood thing that's not global, right? I'm not in the US, our courts work differently. reply angoragoats 4 hours agorootparentI’m sorry for assuming that you were (though you could have mentioned this in your reply). Most of the large AI companies relevant to our discussion are based in the US. reply throw5959 2 hours agorootparentWill that remain true if they have copyright issues in the US? reply ben_w 2 hours agorootparent(I'm not the other commenter) I suspect that most of the large AI companies relevant to this discussion will remain based in the US. Most of the money is in the US, China, and the EU. China won't allow any LLM that accidentally says mean things about their government, the EU is worried about AI that may harm individuals by libelling them. The Chinese models may well completely ignore western laws, but if they're on the other side of the Great Firewall, or indeed just have Chinese-language UIs and a focus on Chinese-language tokens in the training… well, I'm not 100% confident, but I would be somewhat surprised if, say, JK Rowling was upset upon discovering that western users attempting to pirate her works via a Chinese chatbot were getting a version of Harry Potter that begins with the title literally being \"赫奇帕奇巫师石(哈利·波特与魔法石)\" (as ChatGPT just told me the Chinese version starts. Google Translate claims the first three characters are \"Hufflepuff\"). Even if the rules aren't any harder (as I'm not a lawyer, I can't tell if the differences in copyright rules will or won't make a huge difference in compliance costs), it's likely easier for American companies to lobby the American government for what they want done to make business easier. reply CaffeineLD50 6 hours agorootparentprevYou could argue intent. The model has no intent to infringe. No mens rea. AI models will get broad federal immunity is my prediction for 2025. I'll bet DOGE coins on it. reply angoragoats 5 hours agorootparentAI models are not entities that can be sued, accused of a crime, or given legal immunity, to my knowledge. But I get the gist of what you’re saying. reply realusername 10 hours agorootparentprevI tried and I could not make it work. And even if you could, that has to be the most inefficient way to pirate books on earth. reply menzoic 10 hours agorootparentprev> reproduce books word by word, page by page This statement is a figment of the commenters imagination with no basis in reality. All they would have to do is try it to realize they just spouted a lie. At most LLMs can produce partial excerpts. LLMs don’t store the data that it’s trained on. That would be infeasible, the models would be too large. Instead, it stores semantic representations which often uses entirely different words and sentence structures than the source content. And of course most of the data is lost entirely during this lossy compression. reply rhubarbtree 10 hours agorootparentThe NYT has extracted long articles from ChatGPT and submitted the evidence in court. reply Lerc 8 hours agorootparentGiven that it has been submitted in court, does that mean you can say what the longest verbatim extract was? It seems like that would be a fact that couldn't be argued with. reply ben_w 9 hours agorootparentprevCrucial size difference between an article and a book. Size difference meaning that people often share complete copies of articles to get around pay walls — including here. As I understand it, this is already copyright infringement. I suspect that those copies are how and why it's possible in cases such as NYT. reply angoragoats 10 hours agorootparentprev> At most LLMs can produce partial excerpts. Glad you agree that LLMs infringe copyrights. reply sgt101 9 hours agoparentprev>Second, if the crime was the act of scraping then it’s directly comparable. But if the crime is publishing the data for free, that’s quite different from training AI to learn from the data while not being able to reproduce the exact content. They often do reproduce the exact context; in fact it's quite a probable output >“Probabilistic plagiarism” is not what’s happening or even aligned with the definition of plagiarism (which matters if we’re talking about legal consequences). What’s happening is that it’s learning patterns from the content that it can apply to future tasks. That's what I think people wish would happen. Sometime they have been shown to learn procedural knowledge from the training data but mostly it's approximate retrieval. reply sgt101 1 hour agorootparentProof? :) where has proof ever been found in truth? It's not proof, but there is some good evidence out there. Here are two interesting papers I found informative. https://arxiv.org/html/2403.04121v2 https://arxiv.org/html/2411.12580v1 reply CaptainFever 8 hours agorootparentprevDo you have proof of this? reply qwertox 10 hours agoparentprevDo you really think that OpenAI has deleted the data it has scraped? Don't you think OpenAI is storing all this scraped data at this moment on some fileservers in order to re-scan this data in the future to create better models? Models which may even contain verbatim copies of that data internally but prevent access to it through self-censorship? In any case it's a \"Yes, we have all this copyrighted data and we're constantly (re)using it to produce derived works (in order to get wealthy)\". How can this be legal? If that were legal, then I should be able to copy all the books in a library and keep them on a self-hosted, private server for my or my companies use, as long as I don't quote too much of that information. But I should be able to have all that data and do close to whatever I want with it. And if this were legal, why shouldn't it be legal to request a copy of all the data from a library and obtain access to it via a download link? reply edanm 9 hours agorootparentWhat? This makes no sense. Of course you're allowed to own copyrighted material. That's the whole point. I have bookshelves worth of copyrighted material at home at this very minute. If you're implying that the scraping and storing of the things itself breaks copyright, then maybe, but I don't think so? If you're saying that training on copyrighted material breaks copyright, then yes, that's the whole argument. But just having copyrighted material on a server somewhere, if obtained legally, is not by itself illegal. reply Terr_ 8 hours agorootparentNot the other poster, but chiming in here that: > If you're implying that the scraping and storing of the things itself breaks copyright, then maybe, but I don't think so? Suppose I \"scrape and store\" every book I ever borrow or temporarily-owned, using the copies to fill several shelves in my own personal library-room. Yes, that's still copyright infringement, even if I'm the only one reading them. > But just having copyrighted material on a server somewhere, if obtained legally, is not by itself illegal. I see two points of confusion here: 1. The difference between having copies and making copies. 2. The difference between \"infringing\" versus \"actually illegal.\" Copyright is about the right to make copies. Simply having an unauthorized copy is no big deal, it's making unauthorized copies where you can get in trouble. Also, it is generally held that the \"copies\" of bytes in the network etc. do not count, but if you start doing \"Save As\" on everything to create your own archives of the news site, then that's another story. reply edanm 8 hours agorootparent> Suppose I \"scrape and store\" every book I ever borrow or temporarily-owned, using the copies to fill several shelves in my own personal library-room. Yes, but you don't know that they did that. They could've just bought legal access to copyrighted material in many cases. E.g. if I pay for an NYT subscription that gives me the entire back catalogue of the NYT, then I'm legally allowed to own it. Whether I'm allowed to train models on it is, of course, a new and separate (and fascinating) legal question. reply omnimus 7 hours agorootparentprev“Obtained legally” implies that they have licensed the material for machine learning - which they didnt because it learns “like a human”. So i guess netflix subscription is enough. And since the machine has to learn on frames we just have to copy every frame to image and store it in our dataset but thats just a technicality. Also even if you explicitly prohibit use of your copyrighted material it doesnt matter because “its like human” it would be discrimination. Nah this is breach of current copyright laws in many many ways. Tech sector is as usual just running away with it hoping nobody will notice untill they manage to change the laws to suit them. reply amanaplanacanal 5 hours agorootparentWhether it's a beach of copyright law is what the court cases are to decide. I wouldn't bet either way. reply ajb 9 hours agoparentprevIt's weird that corporations get to remove liability from people acting on their behalf. The law is only that they remove liability for debts. If you act on behalf of a corporation to commit either a criminal offence or civil wrong, as far as I can tell (I'm not a lawyer) you are guilty of at least conspiracy. And yet, it's rare for individuals to be prosecuted for such offences, even for criminal offences. We treat the liability shield as absolute. It may seem unfair to prosecute the little guy for \"just following orders\" but the fact that we don't do it is what allows corporations to offend with impunity. reply nonrandomstring 8 hours agorootparentIf you join a gang you get protection from that gang. There are some gangs you can join where you get to kill people, with total impunity, if you're into that sort of thing. Corporations are just another kind of gang. Don't let the legal sugar frosting fool you. reply ajb 7 hours agorootparentYes, but I don't think that's the full explanation. It seems to apply even to companies of modest size, which should not be able to intimidate. reply nonrandomstring 4 hours agorootparentDefinitely not the full explanation. I made a strident, blunt statement to move the discussion along. If you follow the Hobbsian thing the biggest gang is the state. And it's supposed to be \"our\" gang cos we vote for it and give tacit assent to a violent monopoly we call \"the law\". What's happened is the law has become very weak, and so has the democracy that keeps the biggest baddest gang on the side of 'the people'. I think US society, and small companies as you say, sense that the West has been \"re-wilded\". In the new frontier anything goes so long as you have the money. Aaron was a principled, smart and courageous dude, but he was acting without support from a strong enough base. The gang he thought he was in, MIT, betrayed him. (Chomsky has plenty of wisdom on what terrible cowards universities and academics really are.) The law that should have protected someone like Aaron was weak and compromised. It remains so. The same (lack of) state and laws are now protecting others doing the same things as Aaron, at an even bigger scale, and for money. At least that's how I read TFA. reply shwouchk 4 hours agorootparentprevDevils advocate; In order for the elite to rule, they need a rich oligarchy of support. At least that’s how it’s always been. And you want cooperation from the mob, if you want the place in general to thrive - and you do, if you’re smart. Because your genes do. Look what eventually happens to the progeny of /most/, bad dictators these days, eventually. Using the biggest gang as protection for smaller, even tiny gangs kills two birds with one stone; Anyone can join the protection racket so the mob is pacified, and the biggest gang gets the support of the rich gangs, who get to sit in on the council. Terey pratchett’s genius is slowly revealed to me over the years))) reply nonrandomstring 2 hours agorootparent\"biggest gang\" is a natural order outcome and yes, Pratchett probably calls it better than Hobbes or political \"scientists\". reply shwouchk 48 minutes agorootparentidk if it was sarcasm or not. yes of course it’s natural. it’s just that, with time, discworld universe starts to resemble the real world more and more to me, not a weird parody copy… reply sambeau 7 hours agoparentprevIf I make an MP3 of a song or a JPEG of an artwork I cannot use them to reproduce the exact content, but I will still have violated the artist’s copyright. reply nialv7 7 hours agoparentprevSo, let me get this straight. Scraping data and publishing it so everyone get equal and free access to information and knowledge, arguably making the world better, is a crime. But scraping it to benefit and enrich just yourself, is A-OK?? This legal system is truly fucked. reply CaptainFever 7 hours agorootparentNo, if you scrape and create a open-weight model, that is OK too. The difference is if you re-publish the original dataset, or just the weights trained from it. Please stop mis-interpreting posts to match doomer narratives. It's not healthy for this forum. reply zx10rse 8 hours agoparentprevAn algorithm that predicts the best next possible outcome, with trillions of parameters can hardly be called intelligence in my book, it is an artificial I will give you that. I am old enough to remember a bot called SeNuke which was widely used 10-15 years ago, in the so called black hat SEO community, the purpose of the bot was to feed it with 500 words article, so the words can be scrambled in a way to pass the Google algorithm for duplicated content. It was plagiarism 101, now I don't recall anyone talking about AI back then or how all the jobs of copy writers will extinct, and how we are all doomed. What I remember is that every serious agency would not use such tool so that they can't be associated with plagiarism and duplicate content bans. Maybe it is just me but I cannot fathom the craziness, and hype of a first person output. What we get now with LLM models it not simply an output of link and description of lets say a question like What is an algorithm? We get an output that starts with \"Let me explain\" ... how is this learning and intelligence? We are just witnessing the next dot com boom, the industry as whole haven't seen such craziness despite all the efforts in the last 25 years. So I imagine that everyone wants to ride the wave to become the next PayPal mafia, tech moguls, philanthropist, inventors, billionaires... Chomsky summed it best. RIP Aaron reply elp 9 hours agoprevAaron Swartz was targeted by some pretty overly zealous prosecution no objection, but lets not forget that what he really did. He put a laptop in a wiring closet that was DOSing JSTOR and kept changing IPs to avoid being blocked. The admins had to put a camera on the closet to eventually catch him. He might have had good intentions but the way he went about getting the data was throwing soup at paintings levels of dumb activism. For all the noise the real punishment he was facing was 6 months in low security [1]. I'm pretty sure OpenAI would have also been slapped hard for the same crime. [1] https://en.wikipedia.org/wiki/Aaron_Swartz#Arrest_and_prosec... Edit: added link reply omnimus 7 hours agoparent“charges carrying a cumulative maximum penalty of $1 million in fines plus 35 years in prison” https://en.m.wikipedia.org/wiki/United_States_v._Swartz I didnt think people on “hacker news” would be defending what happened to Aaron Swartz. reply cowsandmilk 7 hours agorootparent> charges carrying a cumulative maximum penalty of $1 million in fines plus 35 years in prison Any lawyer knows that is stupid math. The DOJ has sentencing guidelines that never add up the years in prison for charges to be served consecutively. The media likes to do that to get big numbers, but it isn’t an honest representation of the charges. I don’t think charges against Schwartz should have been filed, but I also can’t stand bad legal math. reply omnimus 6 hours agorootparentSure but… he could technically get that or not? If somebody wanted to really punish him they could push it to what? 3 years? 5 years? 10 years? Because some people really wanted to punish him. I am just reacting to the downplaying that he would get 6 months in jail. Like he was some weak person for commiting suicide because of that. reply izabera 9 hours agoparentprevJust for context, there is a new post about OpenAI DDoS'ing half the internet every other day on hn https://news.ycombinator.com/item?id=42660377 https://news.ycombinator.com/item?id=42549624 reply elp 9 hours agorootparentIf you ask OpenAI to stop, using robots.txt, they actually will. What Aaron was trying to achieve was great, how he want about it is what ruined his life. reply LunaSea 6 hours agorootparentIt is a well known fact that OpenAI stole content by scraping sites with illegally uploaded content on it. reply omnimus 7 hours agorootparentprevNobody really asked Aaron about anything they collected more evidence and wanted to put him to jail. School should have unplugged his machine bring him for questioning and tell him not to do that. reply nathancahill 7 hours agoparentprevWhich individual suffered harm from Aaron's laptop in the closet? reply lofaszvanitt 5 hours agoparentprevAaron had an unstable personality and they took advantage of that. A nudge here and there, and here comes the suicide. Look around people who Aaron frequented with to find the culprits... reply Earw0rm 6 hours agoparentprevNo paintings were harmed in the throwing of soup, and now we all know it happened and why. Would that I were that kind of dumb. reply cassianoleal 9 hours agoparentprev> the way he went about getting the data was throwing soup at paintings levels of dumb activism. Throwing soup at paintings doesn’t make the paintings available to the public. What he did had a direct and practical effect. reply cowsandmilk 7 hours agorootparent> What he did had a direct and practical effect The main impact of Aaron Swartz’s actions were that it became much more difficult to walk onto MIT’s campus and access journal articles from a laptop without being a member of the MIT community. I did this for a decade beforehand and this became much more locked down in the years after his actions due to restrictions the publishers pushed at MIT. Aaron intentionally went to the more open academic community in Cambridge (Harvard, his employer, was much more restrictive) and in the process ruined that openness for everyone. reply qwertox 11 hours agoprevSam Altman wouldn't spend a second reflecting about this. reply piyuv 9 hours agoparentI believe so too, sociopaths are not capable of empathy reply CaptainFever 8 hours agorootparentDo you have proof that he is a sociopath? reply vdupras 5 hours agorootparentBeing a billionaire and being a sociopath heavily correlate to the point where non-sociopath billionaires are a rarity. reply okwhateverdude 6 hours agorootparentprevnext [4 more] [flagged] xtracto 4 hours agorootparentI hate these type of exchanges that are happening more and more in HN. Passive aggressive comments and replies from both sides of an argument. The \"take comments meanings at their best value \" keeps being eroded. And comments sections for these type of stories get full of them. There's no value in GP asking \"do you have proof that X goes twice a day to the toilet?\" ... and of course the reply is as empty as the question. The level of discourse is getting lower and lower. reply binary_slinger 2 hours agorootparentAs HN becomes more popular this will become more prevalent. The same thing happened to Reddit over the past 14 years. reply CaptainFever 4 hours agorootparentprevSure, thanks. reply benatkin 11 hours agoprevI know I support what aaronsw did and I don’t think he shouldn’t have gotten in any trouble for it, let alone to the tragic level it went to. As for sama, I’m not sure, on one hand I like the innovation and on the other hand it’s very worrying for humanity. I appreciate the post and the fond memories of Aaron but I’m not in complete agreement with the author about sama. reply mastazi 11 hours agoprevIn the photo there are some other faces that I think I might recognise, but I'm not 100% sure. Is there a list of everyone in the picture somewhere on the internet? Edit I think the lady on the left is Jessica Livingston and a younger PG on the right reply aimazon 11 hours agoparenthttps://i.imgur.com/e0GPhSE.jpeg 1. zak stone, memamp 2. steve huffman, reddit 3. alexis ohanian, reddit 4. emmet shear, twitch 5. ? 6. ? 7. ? 8. jesse tov, https://www.ycombinator.com/companies/simmery 9. pg 10. jessica 11. KeyserSosa, initially memamp but joined reddit not long after (I forget his real name) 12. phillip yuen, textpayme 13. ? 14. aaron swartz, infogami at the time 15. ? 16. sam altman, loopt at the time 17. justin kan, twitch reply mastazi 7 hours agorootparentAmazing, thank you! reply tptacek 11 hours agoparentprevNo, that's Jessica Livingston, the cofounder of YC. reply mastazi 11 hours agorootparentyes, you are right, I edited my comment. reply Sid950 7 hours agoprevIdk but I find Aaron actually cool and intelligent. reply begueradj 11 hours agoprevAaron was a developer himself but Sam ... ? reply nell 11 hours agoprevWhy is Sam Altman singled out in these copyright issues? Aren't there plenty of competing models? reply mrkpdl 11 hours agoparentOpenAI is the highest profile. reply mvdtnz 11 hours agoparentprevI don't believe you're asking this in good faith because the answer is so obvious. But just in case, it's because OpenAI is by a ridiculously large margin the most well known player in the space and unlike the leaders of other organisations his name is known and he has a personal brand which he puts a lot of effort into promoting. reply yownie 7 hours agoparentprevbecause he's a manipulative POS. reply sinuhe69 11 hours agoparentprevThe two are seen on the picture together! So more similar they could not be. That highlights the irony and hypocrite of capitalism, of better say of human society. reply CaptainFever 7 hours agorootparentIt's possible that both of them are more similar in thought than anti-AI people think, which is why they hung out together. reply khazhoux 10 hours agoprevThank you, Sam Altman and everyone at OpenAI, for creating ChatGPT and unleashing the modern era of generative AI, which I use every day to speed up my job and coding at home. Signed, Someone who doesn't care that you're making $$$$ from it reply edgineer 9 hours agoparentThe point is that regardless if you're negative, neutral, or positive of others using data for profit, you would hold those who use it altruistically higher. reply xtracto 4 hours agorootparentI hold both of them high enough. As Aaron, I did my good share of book/articles piracy, even before it was online (here in Mexico it was veery common to Xerox and share whole books with students in the 80s and 90s). I understand, Aaron became a martyr; even though he died due to depression and not for \"a cause\". I applaud what he achieved as a person. reply amelius 6 hours agoparentprevThe usual caveat applies. I'm okay they make money from it until they start using that money against the rest of us. reply Earw0rm 9 hours agoparentprevI'll use it to find information , semi-reliably. Hallucinations are still a huge issue. But I can't help thinking that Stackoverflow and Google have self-enshittified to a point where it makes LLMs look better relative to the pinnacle of more conventional knowledge engines than they actually are. If you take the evolution of those platforms from saying 2005-2015, and project forward ten years, we should be in a much better place than we are. Instead they've gone backwards as a result of enshittification and toxic management. reply angoragoats 10 hours agoparentprevIf I’m an author and I don’t want my work included in the corpus of text used for training ChatGPT, should I have that right? What about if I’m an artist and I don’t want my work included in the training data for an image generation model? reply CaptainFever 8 hours agorootparentNo, you should not have that right. Copyright allows you to sell artificial scarcity. AI does not replicate your work directly. So you can still sell your artificial scarcity even if it is trained on. At least you're acknowledging that training rights are a proposed expansion of current IP laws! reply angoragoats 7 hours agorootparent> Copyright allows you to sell artificial scarcity. Not always. That’s more the domain of patents, honestly. > AI does not replicate your work directly. This is false, at least in certain cases. And even if it were always true, it doesn’t mean it doesn’t infringe copyright. > At least you're acknowledging that training rights are a proposed expansion of current IP laws! Yes, they are, emphasis on the “proposed,” meaning that I believe that training AI on copyrighted material without permission can be a violation of current copyright law. I don’t actually know if that’s how it should be, honestly. But as of right now I think entities like the New York Times have a good legal case against OpenAI. reply CaptainFever 7 hours agorootparentYou are probably correct, legally. But given that we're talking about Aaron Swartz, who was legally in the wrong but morally (in the classic hacker's sense) in the right, I meant \"copyright allows you to sell artificial scarcity\" in the moral sense. I think fundamentally we have a difference in opinion on what copyright is supposed to be about. I hold the more classic hacker ideal that things should be free for re-use by default, and copyright, if there is any, should only apply to direct or almost-direct copies up to a very limited time, such as 10 years or so. (Actually, this is already a compromise, since the true ideal is for there to be no copyright laws at all.) reply angoragoats 5 hours agorootparentI generally agree with you about the desired use for copyright, but what gives me pause is the scale at which training AI models uses copyrighted material. I’m not sure there’s a precedent in the past to compare it to. And the result, despite many years and billions of dollars worth of work, is something that can’t even reliably reference or summarize the material it’s trained on. reply CaptainFever 4 hours agorootparentYou're right that there's not much precedent to this, which is why I do think that \"training rights\" is a valid proposal, as long as the proponents acknowledge that this would be an expansion of current IP laws (i.e. further away from the ideal of a limited copyright). Though... if you say \"And the result, despite many years and billions of dollars worth of work, is something that can’t even reliably reference or summarize the material it’s trained on.\", doesn't this imply that there's not much to worry about here? I sense that this is a negative jab, but this undermines the original argument that there is so much worth in the model that we need to create new IP laws to handle it. I mean, I'm not sure what to make of this statement in the first place. Training data should be for the model to learn language and facts, and referencing or summarizing the material directly seems to be out of scope of that. One tends to summarize the prompt, not training data. reply angoragoats 2 hours agorootparent> the original argument that there is so much worth in the model that we need to create new IP laws to handle it No one argued this, to my knowledge. I think that there might be a need for new copyright laws, but the alternative in my mind is that we decide there's not a lot of worth there, meaning that we do nothing, and what OpenAI/Meta/MS/Google/Anthropic/etc are doing is simply de jure illegal. The statement I made about LLMs having major flaws is a point in support of this alternative. > Training data should be for the model to learn language and facts, and referencing or summarizing the material directly seems to be out of scope of that. I strongly disagree, as your prompt can (and for a certain type of user, often does) contain explicit or implicit references to training data. For example: * Explicit: “What is the plot of To Kill a Mockingbird by Harper Lee?” * Implicit: “How might Albert Einstein write about recent development X in physics research?” reply 627467 9 hours agorootparentprevYou may have that right as long as you agree that others have the right to not care about your right when deciding to use \"your\" stuff however they want. reply kevingadd 9 hours agorootparentThe two halves of your statement contradict each other. What are you trying to say? reply angoragoats 5 hours agorootparentprevCongratulations on not answering the question I asked, and at the same time saying something that makes no logical sense. reply worik 9 hours agorootparentprev> I don’t want my work included in the corpus of text used for training ChatGPT, should I have that right? No You could choose not to publish, and be read If you are read you can be used to learn from reply angoragoats 9 hours agorootparentGenerative AI is not learning. Copyrights don’t depend on whether I choose to publish a particular work or not. Zuckerberg personally approved using pirated content to train a model. Is that OK too? reply Kiro 8 hours agorootparentAbsolutely. Abolish all copyright. I can't believe hackers have lost their pirating roots and have become copyright zealots instead. reply xtracto 3 hours agorootparentThis!. Thing is you are preaching to the wrong choir, Hackernews roots were not about the \"original\" hacking, but more about Silicon Valley for profit entrepreneurs creating stuff for profit to heavy quotes \"make the world a better place\" and \"democratization X\". The real hackers were in usenet, /. and similar forums. Now, I'm too old and haven't found a good place. Information wants to be free, copyright is a last century millennium construct to earn money from invention. (Before copyright, people got paid by patrons to invent their stuff). Copyright should be left at the XX century door, and whoever wants to \"monetize\" their intellect should use methods for the XXI century. reply angoragoats 46 minutes agorootparentAt least in the United States, copyright has existed since the founding of the country, and the Constitution explicitly grants Congress the authority to regulate it. It’s not as recent as you make it out to be, and it’s not going away anytime soon. To the extent that there is a problem to be solved here, it must be solved within the context of copyright law. reply starfezzy 7 hours agorootparentprevIP is fake. What I mean is, all laws are fake, but while we have to follow some collective hallucination about magic words giving people authority—to keep society together—the specific delusion that we should enforce artificial scarcity by extending monopolies to patterns of information is uniquely silly. reply wiseowise 9 hours agorootparentprevYes. reply dehrmann 11 hours agoprevThis post misses a lot of nuance. Aaron Swartz was an activist who did obviously illegal things and got caught and prosecuted. What OpenAI is doing is in legal gray area because it might be transformative enough to be fair use; we just don't know yet. reply dkjaudyeqooe 10 hours agoparentSimply being transformative is not sufficient for it to be fair use. But more to the point if it's deemed illegal Altman won't suffer any personal legal consequences. reply jmcgough 9 hours agoparentprevIt's not about simply not doing something illegal - we all regularly commit crimes that we could be charged with if we piss off the wrong people. When a company does it, it's \"disrupting\" and heavily rewarded if the company has enough funding or revenue. When people like AS do it, they get destroyed. Selective enforcement in order to maintain the status quo. The last few years have clearly shown that if you are wealthy enough, the rules do not apply to you. reply gklitz 8 hours agorootparentIf OpenAI had run its company by hiding their hardware around a university campus they would have gotten in trouble too. It is not as much about the scraping as it’s the the part where MIT sees a masked person sneaking into backrooms hiding equipment that got AS in trouble. And of cause that he literally disrupted service to jstor because he did a poor job of scraping it. He could have gotten through all of this if he had appeared less like a cyber terrorist in his execution of his plan, but of cause he though he wouldn’t get caught doing it, so he never considered how it would look if he did. reply kevingadd 9 hours agoparentprevBy US copyright law, OpenAI is engaged in illegal conduct. The fact that they haven't been punished for it doesn't mean it's magically not illegal. It's possible copyright law will be revised to make it unambiguously legal to do what they've done, but that's not how the law works right now. reply CaptainFever 7 hours agorootparent> By US copyright law, OpenAI is engaged in illegal conduct. What makes you so sure about this? You are not a judge, and multiple cases against OpenAI have been dismissed by judges already. reply kevingadd 1 hour agorootparentDismissing a case doesn't mean that the conduct in question is legal. Not sure why you would think it does. reply moralestapia 4 hours agorootparentprevOpenAI did not break and enter into private premises to obtain such data. This is the crime that took place, it was not just a copyright issue. reply kevingadd 1 hour agorootparentAaron having broken and entered doesn't mean that OpenAI violating copyright law is legal. reply moralestapia 1 hour agorootparentWho would ever come up with such an assumption? (excluding regular users of Fentanyl) reply ilrwbwrkhv 12 hours agoprevOh man. Heavy stuff. Our industry will be looked at as good or bad? I hope we end up doing good for the world. reply zx10rse 8 hours agoparentOpen source the models it is the only right decision. reply remram 3 hours agorootparentI don't understand. If something hurts your civilization but it was free, does that make it better? Like if everyone was able to build a nuclear bomb, would that make the ensuing nuclear winter more moral? reply memonkey 12 hours agoparentprevHard to say when there is a profit motive for all industries. Seems like every industry at the moment is not really looking for human advancement, or maybe it is looking at advancing but only if the results are expensive for end users and efficient/proprietary for the company. reply ilrwbwrkhv 12 hours agorootparentYes but the thing is our industry has almost unparalleled leverage and marginal utility cost is zero. reply moralestapia 4 hours agoprev>Thank you Aaron for so much, for RSS, for Markdown, for Creative Commons and more. Didn't he also create the internet? reply ta988 2 hours agoparentIs it sarcasm? Did you check about the history of those three? reply tempodox 12 hours agoprevnext [2 more] [flagged] error_logic 11 hours agoparentThis despite the fact that what actually Made America Great was constructive, honest, healthy competition--not the insane destroy-the-competition monopolist's outlook which tries to destroy opportunity for others and thus competition itself. Horseshoe theory is real. reply CaptainFever 8 hours agoprev [–] This post pits the two people against each other. Am I the only one here who likes both Sam Altman and Aaron Swartz? They've both done great things to help remix culture. Sure, you could say that the law has come down differently on the two, but there are several differences: the timing (one was decades earlier), the nature of copying (one was direct, while the other was just to train and more novel), and the nature of doing (doing it individually vs as a corporation). But this doesn't have to reflect on them. You don't have to hate one and love the other... you can like both of them. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Aaron Swartz was a renowned technical genius and activist, contributing to projects like RSS, Markdown, and Creative Commons, but faced legal challenges for downloading academic articles from JSTOR.",
      "The post contrasts Swartz's activism with Sam Altman's corporate success at OpenAI, raising questions about legality and ethics in the tech industry.",
      "The discussion highlights differing perspectives on the positive contributions of both individuals to tech culture and the disparity in legal treatment between individuals and corporations."
    ],
    "points": 283,
    "commentCount": 161,
    "retryCount": 0,
    "time": 1736658377
  },
  {
    "id": 42669637,
    "title": "Obvious things C should do",
    "originLink": "https://www.digitalmars.com/articles/Cobvious.html",
    "originBody": "Last update Thu Jan 9 18:31:36 2025 Articles Articles D at 20: Hits and Misses (slides) Using D as a Better C C's Biggest Mistake Obvious Things C Should Do Function Purity and Immutable Data Structure Construction The Naked Truth About Writing a Programming Language So, You Want To Write Your Own Language? Increasing Compiler Speed by 77% Implementing Half Floats in D Porting the D Compiler to Win64 NaNs Just Don't Get No Respect Voldemort Types in D Component Programming in D Universal Function Call Syntax Inheriting Purity Reading PDFs on the Kindle Fire Targeting OS X 64 bit Type Qualifiers and Wild Cards Dangling Else Yet Again Member Function Pointers in D Inline Assembler for Über Nerds A Fly in the Purity User Defined Literals in D Value Range Propagation Patterns of Bugs Loop Optimizations Overlooked Essentials For Optimizing Code C++ Compilation Speed The X Macro Constrained Templates Improving Compiler Error Messages Assembler to C Designing Safe Software Systems Part 2 Safe Systems from Unreliable Parts Compiling Templates A Module Far Far Away Part 1 A Module Far Far Away Part 2 Sharing Is Not Always Nice One Million Years B.C. Porting D to FreeBSD Multithreaded I/O Signaling NaNs Rise Again Porting D to the Mac: Epilog Porting D to the Mac Pt. 3 Porting D to the Mac Pt. 2 Porting D to the Mac Should Out-of-Memory Default to Being a Non-Recoverable Error? Optimizing Immutable and Purity Function Overloading With Partial Ordering Escape Analysis Nothrow Functions Purity Assertions In Production Code What is Invariant Good For? Language Features for Management So You Want To Be A Programmer? Coverage Analysis Arrays: Core or Library Type? Redundancy in Programming Languages Core vs Library Memory Safety Invariant Strings Obvious Things C Should Do January 8, 2025 written by Walter Bright Standard C undergoes regular improvements, now at C23. But there are baffling things that have not been fixed at all. The Dlang community embedded a C compiler in the D programming language compiler so it could compile C. This C compiler (aka ImportC) was built from scratch. It provided the opportunity to use modern compiler technology to fix those shortcomings. Why doesn’t Standard C fix them? Evaluating Constant Expressions Compile Time Unit Tests Forward Referencing of Declarations Importing Declarations Evaluating constant-expression Consider the following C code: int sum(int a, int b) { return a + b; } enum E { A = 3, B = 4, C = sum(5, 6) }; When compiled with gcc: gcc -c test.c test.c:3:20: error: enumerator value for C is not an integer constant enum E { A = 3, B, C = sum(5, 6) }; ^ In other words, while C can compute at compile time a simple expression by constant folding, it cannot execute a function at compile time. But ImportC can. Everywhere a C constant-expression appears in the C grammar the compiler should be able to execute functions at compile time, too, as long as the functions do not do things like I/O, access mutable global variables, make system calls, etc. Compile Time Unit Testing Once the C compiler can do compile time function evaluation (CTFE), suddenly other things become possible. For example, ever notice that seeing unit tests in C code is (unfortunately) rather rare? The reason is simple - unittests require a separate target in the build system, and must be built and run as a separate executable. Being a bit of a nuisance means it just does not happen. (Maybe you are one of those people who get up and jog a mile every morning, and you probably also carefully write unit test setups! I know you exist out there - somewhere!) int sum(int a, int b) { return a + b; } _Static_assert(sum(3, 4) == 7, \"test #1\"); gcc -c test.c test.c:3:16: error: expression in static assertion is not constant _Static_assert(sum(3, 4) == 7, \"test #1\"); ^ ImportC can compile it. This enables unit tests of functions that can be run at compile time. No separate build is required. No extra work is required. The unit tests run every time the code is compiled. I use this extensively in the test suite for ImportC. Forward Referencing of Declarations More code: int floo(int a, char *s) { return dex(s, a); } char dex(char *s, int i) { return s[i]; } gcc -c test.c test.c:4:6: error: conflicting types for dex char dex(char *s, int i) { return s[i]; } ^ test.c:2:35: note: previous implicit declaration of dex was here int floo(int a, char *s) { return dex(s, a); } If the order of floo and dex are reversed, it compiles fine. I.e. the compiler only knows about what lexically precedes it. Forward references are not allowed. Isn’t this stone age compiler design? Modern languages don’t have this problem, why does it persist in C and C++? ImportC is not a modern language, but it is a modern compiler and accepts arbitrary orders of the global declarations. Why does this matter? It usually means that every forward definition needs an extra declaration: char dex(char *s, int i); // declaration int floo(int a, char *s) { return dex(s, a); } char dex(char *s, int i) { return s[i]; } // definition It’s just purposeless busywork to do that. Not only is it a nuisance, it drives programmers to lay out the declarations backwards. The leaf functions come first, and the global interface functions are last. It’s like reading a newspaper article from the bottom up. It makes no sense. ImportC can compile the declarations in any order. Importing Declarations Given three files, floo.d, dex.h, dex.c: // floo.c #include \"dex.h\" int floo(int a, char *s) { return dex(s, a); } // dex.h char dex(char *s, int i); // dex.c #include \"dex.h\" char dex(char *s, int i) { return s[i]; } Having to craft a .h file for each external module is a lot of busy work, right? Even worse, if the .h file turns out to not exactly match the .c file, you are in for a lot of time trying to figure out what went wrong. What’s the answer? Importing dex.c! // floo.c __import dex; int floo(int a, char *s) { return dexx(s, a); } // dex.c char dexx(char *s, int i) { return s[i]; } No need to even write a .h file at all. Of course, this also works with ImportC. References ImportC documentation D Language documentation HomeRuntime LibraryIDDE ReferenceSTLSearchDownloadForums Copyright © 1999-2025 by Digital Mars ®, All Rights ReservedPage generated by Ddoc.",
    "commentLink": "https://news.ycombinator.com/item?id=42669637",
    "commentBody": "Obvious things C should do (digitalmars.com)225 points by LorenDB 20 hours agohidepastfavorite258 comments TheNewAndy 14 hours agoHeader files are one of the things I miss the most about languages that aren't C. Having a very clear distinction between public and private, and interface and implementation is one of my favourite things about C code (at least the way I write it). Being able to just read through a library's .h files to know how to use it is really nice. Typically, my .h files don't really look like my .c files because all the documentation for how to use the thing lives in the .h file (and isn't duplicated in the .c file). It would be entirely possible to put this documentation into the .c file, but it makes reading the interface much less pleasant for someone using it. reply kouteiheika 14 hours agoparent> Header files are one of the things I miss the most about languages that aren't C. Having a very clear distinction between public and private, and interface and implementation is one of my favourite things about C code (at least the way I write it). I always found this argument baffling, because the way some other language solve this problem is with tooling, which is a much better way to do it in my opinion. Take Rust for example. You want to see the interface of a given library and see how to use it? Easy. Type in `cargo doc --open` and you're done. You get a nice interface with fully searchable API interface with the whole public API, and it's all automatic, and you don't have to manually maintain it nor have to duplicate code between your header and your source file. reply TheNewAndy 13 hours agorootparentThis is probably something where it comes down to preference and familiarity. I would much prefer a simple text file for documentation that I can grep, open in my text editor, modify easily without switching context (oh, I should have been more explicit in the documentation I wrote - let me just fix that now), etc. All the features you mentioned \"nice interface, fully searchable API interface, whole public API\" are exactly what you get if you open a well written header file in any old text editor. I used to be a big fan of doxygen etc, but for the stuff I've worked on, I've found that \"pretty\" documentation is way less important than \"useful\" documentation, and that the reformatting done by these tools tends to lead towards worse documentation with the people I have worked with (\"Oh, I need to make sure every function argument has documentation, so I will just reword the name of the argument\"). Since moving away from doxygen I have stopped seeing this behaviour from people - I haven't tried to get a really good explanation as to why, but the quality of documentation has definitely improved, and my (unproven) theory is that keeping the presentation as plain as possible means that the focus turns to the content. I don't know if rust doc suffers the same issues, but the tooling you are mentioning just seems to add an extra step (depending on how you count steps I suppose, you could perhaps say it is the same number of steps...) and provide no obvious benefit to me (and it does provide the obvious downside that it is harder to edit documentation when you are reading it in the form you are suggesting). But with all these things, different projects and teams and problem domains will probably tend towards having things that work better or worse. reply Yoric 54 minutes agorootparentHave you looked at how OCaml does it? The historical way is to have a .ml and a .mli files. The .ml file contains the implementation. Any documentation in that file is considered implementation detail, will not be published by ocamldoc. The .mli file contains everything users need to know, including documentation, function signatures, etc. Interestingly, the .mli and the .ml signatures do not necessarily need to agree. For instance, a global variable in the .ml does not need to be published in the .mli. More interestingly, a generic function in the .ml does not need to be exposed as generic in the .mli, or can have more restrictions. You could easily emulate this in Rust, but it's not the standard. reply thayne 2 hours agorootparentprev> and that the reformatting done by these tools tends to lead towards worse documentation with the people I have worked with (\"Oh, I need to make sure every function argument has documentation, so I will just reword the name of the argument\") That seems like an orthogonal issue to me. I've seen places where documentation is only in the source code, no generated web pages, but there is a policy or even just soft expectation to document every parameter, even if it doesn't dd anything. And I've also seen places that make heavy use of these tools that doesn't have any such expectation. reply metadat 12 hours agorootparentprev> well written text file The problem with this is no one agrees on the definition of \"well-written\", so consistency is a constant battle and struggle. Language tooling is a better answer for quality of life. reply TheNewAndy 12 hours agorootparentThat's an interesting assertion, but not one that matches the experience I've had. It is one of those things that sounds \"obviously true\", but in practice I've found that it doesn't really live up to the promise. As a concrete example of this, having a plain text header file as documentation tends to mean that when people are reading it, if they spot a mistake or see that something isn't documented that should be documented, they are much more likely to fix it than if the documentation is displayed in a \"prettier\" form like HTML. The problem with header files that aren't \"well-written\" tends to be that the actual content you are looking for isn't in there, and no amount of language tooling can actually fix that (and can be an impediment towards fixing it). reply tpoacher 12 hours agorootparentI'll second this in Java land. I much prefer reading the sources directly than javadocs. Though jshell also comes in handy. reply josephg 11 hours agorootparentI have the same experience a lot of the time with 3rd party rust crates. Doc.rs is amazing - but it’s rare that I’ll use a library without, at some point, hitting view source. reply sim7c00 11 hours agorootparentfor most rust ive done (not tons) the docs were very basic as onky auto generated with minimal content. totally useless, have to read sources to find out what is in there. auto documentation to me ia just ti satisfy people who need to tick all of these boxes and want to do with minimal effort. has dox has tests etc. such artitude never leads to quality. reply bluGill 6 hours agorootparentUseful documentation is impossible for the developer to write - they are too close to the code and so don't understand what the users (either api users or end users) need to know. Developers agonize over details that users don't care about while ignoring as obvious important things users don't know. reply josephg 11 hours agorootparentprevI know people look at me like I’m a heathen and a scoundrel, but I think a lot of software teams spend too much time trying to make things consistent. Where’s the ROI? There is none. GitHub readmes? Bring on the weird quirks, art, rants about other software, and so on. I’ll take it all. Don’t get me started on linters. Yes, there’s lots of things that should actually be consistent in a codebase (like indentation). But for every useful check, linters have 100 random pointless things they complain about. Oh, you used a ternary statement? Boo hoo! Oh, my JavaScript has a mix of semicolons and non semicolons? Who cares? The birds are singing. Don’t bother me with this shite. Software is a creative discipline. Bland software reflects a bland mind. reply lazystar 2 minutes agorootparent> Where’s the ROI? There is none. > Oh, my JavaScript has a mix of semicolons and non semicolons? Who cares? i had to refactor and port a javascript codebase that contained a mix of all of javascripts syntactic sugar, no comments anywhere in the codebase, and i was unable to ask the original devs any questions. the high amount of syntactic sugar gave me \"javascript diabetes\" - it was fun figuring out all the randomness, but it delayed the project and has made it extremely difficult to onboard new folks to the team after i completed the port. painting is a creative discipline, and the mona lisa has stood the test of time because davinci used a painting style and materials that set the painting up for long term use. a codebase without standards is akin to drawing the mona lisa on a sidewalk with sidewalk chalk. XorNot 9 hours agorootparentprevI don't like complaining linters. I do like auto fixing linters I can leave running in the background. reply lazystar 0 minutes agorootparent> auto fixing linters any advice on how to implement an auto linter in an old codebase? i hate losing the git blame info. kouteiheika 12 hours agorootparentprev> All the features you mentioned \"nice interface, fully searchable API interface, whole public API\" are exactly what you get if you open a well written header file in any old text editor. No, you can't, and it's not even close. You have a header file that's 2000 lines of code, and you have a function which uses type X. You want to see the definition of type X. How do you quickly jump to its definition with your \"any old text editor\"? You try to grep for it in the header? What if that identifier is used 30 times in that file? Now you have to go through all of other 29 uses and hunt for the definition. What if it's from another header file? What if the type X is from another library altogether? Now you need to manually grep through a bunch of other header files and potentially other libraries, and due to C's include system you often can't even be sure where you need to grep on the filesystem. Anyway, take a look at the docs for one of the most popular Rust crates: https://docs.rs/regex/1.11.1/regex/struct.Regex.html The experience going through these docs (once you get used to it) is night and day compared to just reading header files. Everything is cross linked so you can easily cross-reference types. You can easily hide the docs if you just want to see the prototypes (click on the \"Summary\" button). You can easily see the implementation of a given function (click on \"source\" next to the prototype). You can search through the whole public API. If you click on a type from another library it will automatically show you docs for that library. You have usage examples (*which are automatically unit tested so they're guaranteed to be correct*!). You can find non-obvious relationships between types that you wouldn't get just by reading the source code where the thing is defined (e.g. all implementations of a given trait are listed, which are usually scattered across the codebase). > I don't know if rust doc suffers the same issues, but the tooling you are mentioning just seems to add an extra step (depending on how you count steps I suppose, you could perhaps say it is the same number of steps...) and provide no obvious benefit to me (and it does provide the obvious downside that it is harder to edit documentation when you are reading it in the form you are suggesting). Why would I want to edit the documentation of an external library I'm consuming when I'm reading it? And even if I do then the effort to make a PR changing those docs pales in comparison to the effort it takes to open the original source code with the docs and edit it. Or did you mean editing the docs for my code? In that case I can also easily do it, because docs are part of my source files and are maintained alongside the implementation. If I change the implementation I have docs right there in the same file and I can easily edit them. Having to open the header file and hunt for the declaration to edit the docs \"just seems to add an extra step\" and \"and provide no obvious benefit to me\", if I may use your words. (: reply TheNewAndy 10 hours agorootparentThanks for the constructive example of the rust doc. I am not making things up when I say that the very first question I had about how to use this module, either is not answered, or I couldn't find the answer. That question was \"what regular expression syntax is supported?\". This is such a fundamental question, yet there is no answer provided. As a preference thing, I don't really like examples in APIs (it is supposed to be a reference in my opinion) and I find them to be mostly noise. > Why would I want to edit the documentation of an external library I'm consuming when I'm reading it? And even if I do then the effort to make a PR changing those docs pales in comparison to the effort it takes to open the original source code with the docs and edit it. Right, this is possibly where our experiences differ. I'm frequently pulling in loads of code, some of which I've written, some of which other people have written, and when I pull in code to a project I take ownership of it. Doesn't matter who wrote it - if it is in my project, then I'm going to make sure it is up to the standards I expect. A lot of the time, the code is stuff I've written anyway, which means that when I come back in a few months time and go to use it, I find that things that seemed obvious at the time might not be so obvious, and a simple comment can completely fix it. Sometimes it is a comment and a code change (\"wouldn't it be nice if this function handled edge case X nicely? I'll just go in there and fix it\"). The distinction between external and internal that you have looks pretty different to me, and that could just be why we have different opinions. reply davisoneee 9 hours agorootparentThe parent linked to a subsection showing usage for a particular object. If you click back into the root level for the document there is a header specifying ‘syntax’, and other more ‘package-level’ documentation reply kouteiheika 7 hours agorootparentprev> I am not making things up when I say that the very first question I had about how to use this module, either is not answered, or I couldn't find the answer. That question was \"what regular expression syntax is supported?\". This is such a fundamental question, yet there is no answer provided. This is a fair question to have. As others have already said, this is the API reference for a particular class, so you won't get the high level details here. You can click in the upper left corner to go to the high level docs for the whole library. > The distinction between external and internal that you have looks pretty different to me, and that could just be why we have different opinions. Well, there are two \"external\" vs \"internal\" distinctions I make: 1. Code I maintain, vs code that I pull in as an external dependency from somewhere else (to give an example, something like libpng, zlib, etc.). So if I want to fix something in the external dependency I make a pull request to the original project. Here I need to clone the original project, find the appropriate files to edit, edit them, make sure it compiles, make sure the tests pass, make a PR, etc. Having the header file immediately editable doesn't net me anything here because I'm not going to edit the original header files to make the change (which are either installed globally on my system, or maintained by my package manager somewhere deep under my /home/). 2. Code that is part of my current project, vs code that is a library that I reuse from another of my projects. These are both \"internal\" in a sense that I maintain them, but to my current project those are \"external\" libraries (I maintain them separately and reuse in multiple projects, but I don't copy-paste them and instead maintain only one copy). In this case it's a fair point that if you're browsing the API reference it's extra work to have to open up the original sources and make the change there, but I disagree that it's making things any harder. I still have to properly run any relevant unit tests of the library I'm modifying, still have to make a proper commit, etc., and going from the API reference to the source code takes at most a few seconds (since the API reference will tell me which exact file it is, so I just have to tell my IDE's fuzzy file opener to open up that file to me.) and is still a tiny fraction of all of the things I'd need to do to make the change. reply reanimus 9 hours agorootparentprev> I am not making things up when I say that the very first question I had about how to use this module, either is not answered, or I couldn't find the answer. That question was \"what regular expression syntax is supported?\". This is such a fundamental question, yet there is no answer provided. The main page for the documentation answers that question: https://docs.rs/regex/1.11.1/regex/index.html It even says \"If you just want API documentation, then skip to the Regex type\", which is what you were linked to before. reply brabel 7 hours agorootparentprevI think the person you're responding to must know all of this. This is stuff that's obvious to anyone who has ever written any code that required using libraries. Unfortunately , people like to pretend to have a gripe with something on the internet just for the sake of arguing. This is the only conclusion I can arrive at when people appear to seriously propose reading a header file in a text editor is somehow better than reading documentation in a purposefully designed documentation format. It's like saying browsers are just a waste of time when you can just use Gopher for everything. reply marcosdumay 2 hours agorootparentThe OP uses C libraries, and this is used to much simpler interfaces and much smaller dependency sets than the GP. So no, I don't think they know all of this. But also, they probably to know how to keep their dependencies sane, and possibly think the best way to document that giant 2k lines interface is in a book. What are both really good opinions, that will never be really \"understood\" by communities the GP takes his libraries from just because it's not viable for them to do it. reply hyperold 3 hours agorootparentprevOr it just might be that different people prefer different things. I'm a hardcore fan of header files too. Vim is my preferred way of dealing with text and I can do all kinds of magic with it with the speed of though and I prefer to use as plain as possible text files. In the rare occasion when the documentation needs more than ascii stuff it's best practice to write a nice tex and friends documentation plus a real tutorial anyway. And full literate programming style is hard to beat when you are dealing with complex things. reply almostdeadguy 2 hours agorootparentIt's fine to have preferences or cognitive inertia towards working a certain way. It's silly to pretend that doing things this way conveys some kind of universalist advantage or to conjure up a bunch of imaginary/highly niche scenarios (I'm remote coding over 28.8k at the bottom of the ocean and have no access to a browser anywhere!) that necessitate working this way for argumentative purposes. reply mtlmtlmtlmtl 11 hours agorootparentprevMost decent text editors support something like go to definition. Your entire comment seems to be based on the idea that text editors only support basic search, which is simply false. Personally I'm quite content with both experiences. But it really is just a matter of preference. reply hawski 9 hours agorootparentprevDepending on coding style you could just do something like this: ^struct whatever reply SkiFire13 49 minutes agorootparentThe issue is that the coding style depends on whoever wrote the external library, not on you, so this ends up working only sometimes. You can probably find some other combination that will help you find what you're looking for (I do this all the time when using Github's web interface) but ultimately this is just a bad experience. reply uecker 10 hours agorootparentprevAt least moderately advanced text editors often interoperate with symbols tables, so you can jump to a definition. But even with grep, you can usually do it in a way where you differentiate between definition and use. But I am not arguing that you should not use advanced tools if you like is, the deeper point is that you can always use advanced tools even with headers, but you can not go back in a language designed around advanced tools and work with simple tools. So it is strictly inferior IMHO to design a language around this additional cmplexity. reply nine_k 1 hour agorootparentprevThe include file mechanism is a hack that was acceptable at the time when machines were extremely underpowered, so only the simplest solutions had a chance to be implemented within a reasonable time frame. By now, of course, precompiled headers exist, but their interplay with #define allows for fun inconsistencies. And, of course, they leak implementation as much as the author wants, all the way to .h-only single-file libraries. If you want an example of a sane approach to separate interface and implementation files from last century, take a look e.g. at Modula-2 with its .int and .mod files. reply panic 13 hours agorootparentprevAs someone who likes C header files, I enjoy manually maintaining them. Designing the interface separately from the implementation feels good to me, and a well-structured .h file is nicer to read than any auto-generated docs I've encountered. reply chii 13 hours agorootparent> Designing the interface separately from the implementation feels good to me would you make the same argument for java then? reply layer8 1 hour agorootparentIt’s important to not conflate Java’s interface keyword with the more general notion of interface as in “API”. In Java, you generally don’t and can’t have a source-level representation of the API without it being interspersed with its implementation. (You could imagine such a representation, i.e. remove all method bodies and (package-)private elements, but the result wouldn’t be valid Java. IDEs arguably could and should provide such a source-level view (e.g. via code folding), but I don’t know any that do.) reply marginalia_nu 3 hours agorootparentprevEven as a java programmer, I think this is a bad take. Java doesn't force separation of implementation and interface, and java interfaces also have a lot of weird stuff going on with them. Java also has too many tools for this. You both have class/interface, but also public/private. I honestly think C does it better than Java. reply otteromkram 12 hours agorootparentprevAbsolutely not. reply tpoacher 12 hours agorootparentyou don't like java interfaces? reply xigoi 10 hours agorootparentprevThe problem is that you have to write the interface not only separately grom the implementation, but together with the implementation as well, which leads to duplication of information; reply bluGill 6 hours agorootparentSo? Yes you need some duplicatin but it forces you to put information useful to api users where they won't have to wade through pages of information not of interest to them. eventually they may need to read the source but a lot of common questions are answered by the header and so the small cost to make them is worth it. of course javadoc can answer the same questions but then you have to run it. reply nox101 11 hours agorootparentprevSounds like COM/DCOM from ~1995. Every API had a public interface including a description. You could open the DCOM Inspector, browse all the APIs, and see the type signature of every function and its docs. reply pjmlp 11 hours agorootparentStill is COM from 2025, given its relevance on Windows, even more since Vista, as all Longhorn ideas were remade in COM. However the tooling experience is pretty much ~1995, with the difference IDL is at version 3.0. reply salgernon 13 hours agorootparentprevI’m with parent - what if you don’t have the tool? What if there’s a syntax error in some implementation or dependency such that the tool chokes early? Human readable headers are accessible out of context if the implementation. They also help provide a clear abstraction - this is the contract. This is what I support as of this version. (And hopefully with appropriate annotations across versions) reply kouteiheika 12 hours agorootparent> I’m with parent - what if you don’t have the tool? The \"what if you don't have the tool\" situation never happens in case of Rust. If you have the compiler you have the tool, because it's always included with the compiler. This isn't some third party tool that you install manually; it's arguably part of the language. > What if there’s a syntax error in some implementation or dependency such that the tool chokes early? In C I can see how this can happen with its mess of a build systems; in Rust this doesn't happen (in my 10+ years of Rust I've never seen it), because people don't publish libraries with syntax errors (duh!). reply TheNewAndy 12 hours agorootparent\"Never\" is a big call. In this specific case, your tool requires a web browser (though I'm assuming that there is a non-web browser form of what is being sold here). Maybe you are in a situation where you only have terminal access to the machine. Maybe you are on your phone just browsing github looking for a library to use I'm sure people can continue to imagine more examples. It is entirely possible that we have different experiences of projects and teams. reply nindalf 11 hours agorootparent> I’m sure people can continue to imagine more examples Hopefully they’ll imagine more compelling examples. If the hypothetical person’s phone is capable of browsing GitHub, I don’t see why they can’t also browse docs.rs. It renders well on small screens. That’s not a hypothetical, I’ve actually read the docs for libraries on my phone. reply jmb99 12 hours agorootparentprev> The \"what if you don't have the tool\" situation never happens in case of Rust. So it’s built into GitLab and GitHub? BitBucket? How easy is it to use on windows (i.e. is it is easy as opening a .h in notepad and reading it)? How easy is it to use from a command line environment with vim or emacs bindings? I could go on. “Never” is doing a lot of heavy lifting in your assertion. I shouldn’t have to install a toolchain (let alone rely on a web browser) to read API documentation. reply pornel 2 hours agorootparentYes, it works with GitHub, GitLab, Bitbucket, and everything else. It's built into the compiler toolchain. It works with every syntax that you can compile, because it uses the compiler itself to extract the documentation. Yes, it works on Windows too. Rust supports Windows as a first-class platform. It works with dependencies too (the docs even link across packages). The fragmentation of C tooling and unreliability/complexity of integrating with C builds is not a universal problem. Rust's built-in documentation generator creates HTML, so anything with a browser can show it. It also has JSON format for 3rd party tooling. The same language syntax for the documentation is understood by Rust's LSP server, so vim, emacs, and other editors with LSP plugins can show the documentation inline too. I've been using this for years, and it works great. I don't miss maintaining C headers at all. I write function definitions once, document them in the same place where the code is, and get high fidelity always up-to-date API docs automatically. reply kouteiheika 11 hours agorootparentprev> So it’s built into GitLab and GitHub? BitBucket? No. It's built into the toolchain which every Rust developer has installed. > How easy is it to use on windows (i.e. is it is easy as opening a .h in notepad and reading it)? A easy as on Linux or macOS from my experience. > How easy is it to use from a command line environment with vim or emacs bindings? Not sure I understand the question; use how exactly? You either have a binding which runs `cargo doc` and opens the docs for you, or you use an LSP server and a plugin for your editor in which case the docs are integrated into your editor. > I shouldn’t have to install a toolchain (let alone rely on a web browser) to read API documentation. If you want you can just read the source code, just as you do for any other language, because the docs are right there in the sources. For publicly available libraries you can also type in `https://docs.rs/$name_of_library` in your web browser to open the docs. Any library available through crates.io (so 99.9% of what people use) have docs available there, so even if you don't have the toolchain installed/are on your phone you can still browse through the docs. I know what you're going to say - what if you don't have the toolchain installed and the library is not public? Or, worse, you're using a 30 year old machine that doesn't have a web browser available?! Well, sure, tough luck, then you need to do it the old school way and browse the sources. You can always find a corner case of \"what if...?\", but I find that totally unconvincing. Making the 99.9% case harder (when you have a web browser and a toolchain installed, etc.) to make the 0.1% case (when you don't) easier is a bad tradeoff. reply Brian_K_White 10 hours agorootparentI don't understand how you don't understand the order of magnitude difference in flexibility, utility, availability, etc between needing to run a specific executable vs merely opening a text file in any way. \"you always have the exe\" is just not even remotely a valid argument. reply kouteiheika 7 hours agorootparent> \"you always have the exe\" is just not even remotely a valid argument. Why? Can you explain it to me? I'm a Rust developer. I use my work station every day for 8 hours to write code. I also use `cargo doc` (the tool for which \"I always have the exe\") every day to look up API docs, and in total this saves me a ton of time every month (probably multiple hours at least, if I'm working with unfamiliar libraries), and I save even more time because I don't have to maintain separate header files (because Rust doesn't have them). Can you explain the superior flexibility and utility of \"merely opening a text file\" over this approach, and how that would make me (and my colleagues at work) more productive and save me time? I'm not being sarcastic here; genuinely, please convince me that I'm wrong. I've been a C developer for over 20 years and I did it the \"opening a text file\" way and never want to go back, but maybe you're seeing something here that I never saw myself, in which case please enlighten me. reply Dylan16807 2 hours agorootparentprevIt's less available in rare situations. It's not less flexible once you already took availability into account. It has more utility, that's the entire point. reply nindalf 9 hours agorootparentprevI don’t understand how you don’t understand that that’s always an option. Rust source files are written in plaintext too. There are a few people in this thread, including you, who claim that they vastly prefer the output of documentation to be plain text in a single file rather than linked HTML files OR reading the source in multiple plaintext files. That’s a preference, so y’all can’t be wrong. But consider that if this preference was even slightly popular, cargo doc would probably get a —-text option that output everything in a single text file. The fact that it doesn’t have it tells me that this preference is very niche. reply devvvvvvv 7 hours agorootparentRust developers are typically, to put it politely, neurospicy. reply nindalf 11 hours agorootparentprev> I could go on Please do. It just sounds like you’re nitpicking. If you can open a browser, open docs.rs. The GitHub repo usually contains a link to docs.rs because that’s how people prefer to read the documentation. If you prefer working without the internet that’s fine too. Use cargo doc, which opens the rendered doc page in a local web browser. If you prefer being in a text editor exclusively, no problem! Grep for `pub` and read the doc comments right above (these start with ///). No toolchain necessary. Look, most normal people don’t have some intense phobia of web browsers, so they’d prefer docs.rs. For the people who prefer text editor, it’s still a great experience - git clone and look for the doc comments. The point is, the existence of docs.rs only encourages Rust library developers to write more and better documentation, which everyone, including text editor exclusive people benefit from. That’s why your comment sounds so strange. reply devvvvvvv 7 hours agorootparentMost normal people don't use Rust. reply hooli_gan 6 hours agorootparentMost normal people don't read documentation of programming libraries. reply estebank 2 hours agorootparentprevMost normal people don't comment on hackernews, yet here we are. reply Dylan16807 11 hours agorootparentprev> I shouldn’t have to install a toolchain (let alone rely on a web browser) to read API documentation. Why are you reading a library API for a language you're not coding in? I'm sure you can come up with some situation, but that situation should NOT be what we optimize for. And web browsers are fine. > is it is easy as opening a .h in notepad and reading it If you include the actual ease of reading, yeah it should be. reply gary_0 12 hours agorootparentprevThe \"what if you don't have the software\" argument doesn't hold water for me. What if you don't have git? What if you don't have a text editor? What if you don't have a filesystem? Most programming language communities are okay with expecting a certain amount of (modern) tooling, and C can't rely on legacy to remain relevant forever... reply Brian_K_White 10 hours agorootparentprevheaders perform the same job for all code, not just code that's in some library. Frankly your description of what you just called easy sounds terrible and pointlessly extra, indirection that doesn't pay for itself in the form of some overwhelming huge win somewhere else. It's easy only if the alternative was getting it by fax or something. reply saghm 2 hours agorootparentHaving to make an entire separate file to mark something as public rather than just having a keyword in the language sounds to me \"terrible and pointlessly extra\". It's not like you can't just put all your public stuff in it's own file in Rust rather than putting private stuff in it as well; empirically though, people don't do this because it's just not worth the effort. reply nine_k 1 hour agorootparentThink about closed-source software that has to export a low-level programmatic interface. You may not believe it, but it's still widespread. reply kevin_thibedeau 14 hours agoparentprevHeader files are really a weak hack to deal with resource constrained platforms from the 70s. They only work if you stick to a convention and pale in comparison to languages like Ada with well architected specification for interfaces and implementation without ever needing to reparse over and over again. I do enjoy using C but that is one area where it should have been better designed. reply jrmg 3 hours agoparentprevI think there may be a difference in thinking that underlies the difference in opinion here. In my experience, having a header file nudges you to think about interface being a _different thing_ to implementation - something that (because you need to) you think about as more fundamentally separate from the implementation. Folks who think this way bristle at the idea that interface be generated using tooling. The interface is not an artifact of the implementation - it’s a separate, deliberate, and for some even more important thing. It makes no sense to them that it be generated from the implementation source - that’s an obvious inversion of priority. Of course, the reverse is also true - for folks used to auto-generated docs, they bristle at the idea that the interface is not generated from the one true source of truth - the implementation source. To them it’s just a reflection of the implementation and it makes no sense to do ‘duplicate’ work to maintain it. Working in languages with or without separate interface files nudges people into either camp over time, and they forget what it’s like to think in the other way. reply estebank 2 hours agorootparentThis thread feels weird to me because when I write code I do think about my public API, have even sketched it out separately looking at the desired usage pattern, but never felt the need to save that sketch as anything other than as part of the documentation. Which lives next to the code that implements that API. I think it is telling that the handful of languages that still have something akin to .h files use them purely to define cross-language APIs. reply juped 2 hours agorootparentprevI would generate implementations from interfaces were it possible, but I never want to generate interfaces from implementations. reply kode-tar-gz 2 hours agorootparentWhy not? reply m463 12 hours agoparentprevI agree with you, but I don't. The way C handles header files is sort of \"seems-to-work\" by just blindly including the text inline. I know this is not a much-used language, but in comparison, Ada did a pretty nice thing. They have the concept of packages and package bodies. The package is equivalent to the header file, and the package body is the implementation of the package. I remember (long ago when I used ada) that everyone could compile against the package without having the package body implementation ready so the interfaces could all work before the implementation was ready. an in another direction, I like how python does \"header files\" with \"import\". It maps easily to the filesystem without having to deal with files and the C include file semantics. reply pjmlp 11 hours agoparentprevAvailable in most compiled module languages, either separately, Modula-2, Modula-3, Ada, Standard ML, Caml Light, OCaml, F#, D. Or it can be generated either as text, or graphical tooling, Object Pascal, D, Haskell, Java, C#, F#, Swift, Go, Rust. All with stronger typing, faster compilation (Rust and Swift toolchain still need some work), proper namespacing. Unfortunately C tooling has always been more primitive than what was happening outside Bell Labs, and had AT&T been allowed to take commercial advantage, history would be much different, instead we got free lemons, instead of nice juicy oranges. At least they did come up with TypeScript for C, and it nowadays supports proper modules, alongside bounds checked collection types. reply thayne 2 hours agoparentprevI find it pretty frustrating to have the documentation in a different file from the source code. When maintaining the code that means I have to go to a separate file to read what a function is supposed to do, or update the documentation. And when reading the documentation, if the documentation is unclear, I have to go to a separate file to see what the function actually does. Granted, the implementation can get in the way if you are just reading the documentation, but if you aren't concerned about the implementation, then as others have said, you can use generated documentation. reply Lucasoato 1 hour agoparentprevIf you want something similar in Python, you could structure your code following the port and adapter pattern. Very effective, especially if paired with hexagonal architecture and type checking libraries like pydantic. reply legobmw99 14 hours agoparentprevSome other languages have equivalents (OCaml comes to mind), but usually they’re less necessary reply wruza 8 hours agoparentprevI used to think like this, but then I discovered generating (prj_root)/types.d.ts. It doesn’t do anything technical because types are in src/**/*, but I do that to generate a quick overview for a project I’m returning to after a month. Maintaining header files is tedious and I often resorted to a kind of “OBHF.h” for common types, if you know what I mean. Otherwise it’s too much cross-tangling and forwards. Even in ts I do type-only src/types.ts for types likely common to everything, mostly because I don’t want pages of picky this-from-there this-from-there imports in every module. As for public/private and sharing “friends” across implementation modules, we didn’t invent anything good anyway. I just name my public private symbols impl_foo and that tells me and everyone what it is. That said, I wouldn’t want to make html out of it like these *-doc tools do. Using another program to navigate what is basically code feels like their editor sucks. My position on in-code documentation is that it should be navigatable the same way you write it. External tools and build steps kill “immersion”. reply Lvl999Noob 14 hours agoparentprevI don't really program in C much so please correct me if I am wrong. There is a flaw in header files in that they work the exact same for dynamic vs static linking, right? If I am making a library in C for static linking, I need to put my internal details in the header file if I want the user's compiler to be able to use those details. But putting them in the header files also means they are part of the public interface now and should no longer be changed. Basically, I cannot do something like a struct with an opaque internal structure but a compile time known layout so that the compiler can optimise it properly but the user cannot mess with the internals (in language supported direct ways). reply TheNewAndy 14 hours agorootparentThat is less about header files, and more about how machine code works. If you want to have some abstract type where you don't let people know anything about the innards, but you do have an explicit interface which enumerates what you can do with it, then yes - you can only really pass around pointers to these things and people outside your abstraction can only pass references not values. If you want people to be able to pass your abstract type by value (among other things), then either you need to let them know how big the thing is (an implementation detail) or you have to expose the copy implementation in such a way that it could be inlined (more implementation details). Sometimes, the \"pure abstraction\" approach is best where you only ever deal with pointers to things, and other times the \"let's pretend that people do the right thing\" approach is best. I don't see this as a header file thing though. reply Lvl999Noob 14 hours agorootparentI disagree with you on this. In another language with explicit public / private separation, the compiler can have access to the internal layout of a type (and thus optimise on it) without letting the developer mess around with it directly. I am assuming static compilation of course. Across a dynamic boundary, I would expect this compiler to behave like a normal C compiler and not use that layout. In a header file, the information for the compiler and the user are the exact same which means you can't reduce your public interface without straight up hiding more of yourself. reply uecker 6 hours agorootparentA compiler can still have access to the internal layout in C via link-time optimization. reply johannes1234321 3 hours agorootparentLinker is something different from Compiler (even if often called via same Frontend) reply uecker 3 hours agorootparentThe linker invokes the compiler again. reply TheNewAndy 13 hours agorootparentprevPersonally, I'm happy to just let a Sufficiently Advanced Compiler do link time optimizations to deal with that level of optimization and either take the hit, or make more things public while that compiler doesn't exist. Let the header files be written for people to read first, and only if there is actually a big performance issue, and the problem is the interface do you need to revisit it (and I'm not just saying this - I will frequently and happily go back and modify interfaces to allow for less data movement etc, but most of the time it really isn't important). I think you are probably right to disagree with me though - I think I should have said that it is more of a limitation on how object files work, rather than how machines work. Object files aren't the only way things can work. reply saidinesh5 13 hours agorootparentprevI think that's a problem with C's header files. With C++ you have the third option where the compiler makes sure that the \"people will do the right thing\" with the private keyword - assuming they're not doing some weird pointer math to access the private members.. Of course, you'll have to deal with ABI stability now but it's all tradeoffs for what your requirements are. reply Mankaninen 12 hours agorootparentOf course you should do the right thing, but if you want to break the private of C++ it is much easier to \"#define private public\" before including the header file. reply saidinesh5 4 hours agorootparentnow that's just diabolical... reply TheNewAndy 13 hours agorootparentprevRight, but as soon as you have private stuff in your header file, that is leaking implementation details. Yes it is kind of true that these are compile time checked to make sure that people don't do the wrong thing, but it is still an implementation detail that is leaking. It comes down to a cost benefit thing - is the cost of poorer readability worth it for mitigating the risk of people doing the wrong thing? My experience says no, other people's experience says yes. Probably we are working on different problems and with different teams. reply uecker 10 hours agorootparentprevIn C programs only the external definition of an interface goes into the header of a library but not implementation details (there could be headers intentionally exposing details for internal use, of course). The problem is real for C++. Optimizers can look across translation units nowadays (link-time optimization), so there is no reason to expose internal details in a header for this. For dynamic libraries this does not work of course, but it also shouldn't. reply johannes1234321 1 hour agorootparentEven for C it's normal ways true: When size of structures have to be known to the user (if they are supposed to keep them on stack or mallox themselves or whatever) the \"private\" structure often ends up in a \"private\" header. (There are ways to still hide it, but they cause work to keep things proper) And then there are cases where (often die to performance) you want inlining of some operations without relying on Link time optimisation, then implementation has to go to headers, too. reply lzsiga 12 hours agorootparentprevThe proper way is not exporting implementation details at all, instead define opaque types in your header files like this: `typedef struct ssl_st SSL;`. This comes from OpenSSL, it means users can use `SSL *` pointers, but they don't know what those pointers point to. Of course you can also have internal header-files within your own project, which you don't share with the end-users of your product. reply billfruit 13 hours agoparentprevThey are also somewhat of hassle and something not necessary to have. reply fuzztester 12 hours agoparentprevObject Pascal (not the original Pascal) versions like Delphi and Free Pascal have syntax and semantics for interface and implementation sections of the module. Wouldn't be surprised if Modula-2 and Ada had that too. reply wruza 8 hours agorootparentI remember int/impl sections since the 1990’s turbo pascal, which wasn’t “object” still, iirc. Also, commercial closed-source units (modules) were often distributed in a .tpu/.dcu + .int form, where .int was basically its source code without the implementation section. reply trenchgun 11 hours agoparentprevOCaml .mli interface files are the same, but better. reply harisund1990 13 hours agoparentprevI love headers but I wish you could split them in two so that private functions and variables can line in the c file. This would help reduce a lot of header bloat as well. reply lzsiga 12 hours agorootparentIt is perfectly valid to use more than one header files: some of them can be public (meant to be seen by users of your library), others can be private or internal (only used by your own sources). reply chikere232 10 hours agorootparentAlso, usually it's pretty rare to have things internal to one C file that need explicit prototypes. It's easier to just put things in the right order so the funtion definition etc is before its use. reply ryukoposting 13 hours agoparentprevHeader files also make it a lot more obvious how you're supposed to distribute a library as a binary, which is good. reply bjourne 14 hours agoprevI write unit tests for my C code all that time. It's not difficult if you use a good build system and if you are willing to stomach some boilerplate. Here is one test from my \"test suite\" for my npy library: void test_load_uint8() { npy_arr *arr = npy_load(\"tests/npy/uint8.npy\"); assert(arr->n_dims == 1); assert(arr->dims[0] == 100); assert(arr->type == 'u'); npy_free(arr); } int main(int argc, char *argv[]) { PRINT_RUN(test_load_uint8); ... } I know I could have some pre-processor generate parts of the tests, but I prefer to KISS. reply samiv 56 minutes agoparentI really agree, I think that making the tests as easy as possible to get going goes a long way towards a code base that actually has tests. I have something very similar. https://github.com/ensisoft/detonator/blob/master/base/test_... Borrowed heavily from boost.test.minimal and used to be a single header but but over the years I've had to add a single translation unit! My takeaway is that if you keep your code base in a condition where tests are always passing you need much less complications in your testing tools and their error reporting and fault tolerance etc. ! reply WalterBright 14 hours agoparentprevYour function looks like it's doing I/O, which won't work at compile time test. Here's an example of a unittest for the ImportC compiler: struct S22079 { int a, b, c; }; _Static_assert(sizeof(struct S22079){1,2,3} == sizeof(int)*3, \"ok\"); _Static_assert(sizeof(struct S22079){1,2,3}.a == sizeof(int), \"ok\"); The semantics are checked at compile time, so no need to link & run. With the large volume of tests, this speeds things up considerably. The faster the test suite runs, the more productive I am. reply uecker 3 hours agorootparentIt is difficult to imagine that compile-time interpretation of tests is faster than compiling and running them for anything more complex. And for trivial stuff it should not matter. Not being able to do I/O is a limitation not a feature. reply WalterBright 36 minutes agorootparentLinkers are slow and clunky. Yes, there is a crossover point where executable tests are faster. reply brabel 7 hours agorootparentprevHey Walter, importC is great but on Mac it doesn't work right now because Apple seems to have added the type Float16 to math.h (probably due to this: https://developer.apple.com/documentation/swift/float16) and DMD breaks on that. Could you have a look at fixing that? reply WalterBright 39 minutes agorootparentAargh. Those sorts of extensions should not be in the system .h file. reply TheNewAndy 14 hours agoparentprevYou will be pleased to know that you are not the only one who does this. I previously went down the rabbit hole of fancy unit test frameworks, and after a while I realised that they didn't really win much and settled on something almost identical to what you have (my PRINT_RUN macro has a different name, and requires the () to be passed in - and I only ever write it if the time to run all the tests is more than a second or so, just to make it really convenient to point the finger of blame). The thing that I do which are potentially looked upon poorly by other people are: 1) I will happily #include a .c file that is being unit tested so I can call static functions in it (I will only #include a single .c file) 2) I do a tiny preprocessor dance before I #includeto make sure NDEBUG is not defined (in case someone builds in a \"release mode\" which disables asserts) reply PhilipRoman 3 hours agorootparentI agree with all of the above. The only fancy thing which I added is a work queue with multiple threads. There really isn't any pressing need for it since natively compiled tests are very fast anyway, but I'm addicted to optimizing build times. reply wruza 8 hours agorootparentprevThis test/src separation always felt like html/css to me. When still using C, I wrote tests right after a function as a static function with “test_” in the name. And one big run-all-tests function at the end. All you have to do then is to include a c file and call this function. Why would I ever want to separate a test from its subject is a puzzling thought. Would be nice to have “testing {}” sections in other languages too, but in C you can get away with DCE, worst case #ifdef TESTING. reply bjourne 3 hours agorootparentBecause tests also serve as api validations. If you can't write a test for functionality without fiddling with internal details the api is probably flawed. Separation forces access via the api. reply chacham15 19 hours agoprevWhile the author has WAY more knowledge/experience than me on this and so I wonder how he would solve the following issues: Evaluating Constant Expressions - This seems really complicated...if you're working within a translation unit, thats much simplified, but then you're much more limited in what you can do without repeating a lot of code. I wonder how the author solves this. Compile Time Unit Tests - This is already somewhat possible if you can express your test as a macro, which if you add in the first point, then this becomes trivial. Forward Referencing of Declarations - I think there may be a lot of backlash to this one. The main argument against this is that it then changes the compiler from a one-pass to two pass compiler which has its own performance implications. Given the number of people who are trying to compile massive codebases and go as far as parallelizing compilation of translation units, this may be a tough pill for them to swallow. (evaluating constant expressions probably comes with a similar/worse performance hit caveat depending on how its done) Importing Declarations - This is a breaking change...one of the ways I have kind of implemented templating in C is by defining a variable and importing a c file, changing the variable, and then reimporting the same c file. Another thing I've done is define a bunch of things and then import the SQLite C Amalgamation and then add another function (I do this to expose a SQLite internal which isnt exposed via its headers). All of these use cases would break with this change. Are there any thoughts about these issues? Any ways to solve them perhaps? reply WalterBright 18 hours agoparent> if you're working within a translation unit, thats much simplified, but then you're much more limited in what you can do without repeating a lot of code. I wonder how the author solves this. You are correct in that the source code to the function being evaluated must be available to the compiler. This can be done with #include. I do it in D with importing the modules with the needed code. > This is already somewhat possible if you can express your test as a macro, which if you add in the first point, then this becomes trivial. Expressing the test as a macro doesn't work when you want to test the function. The example I gave was trivial to make it easy to understand. Actual use can be far more complex. > Performance D is faster at compiling than C compilers, mainly because: 1. the C preprocessor is a hopeless pig with its required multiple passes. I know, I implemented it from scratch multiple times. The C preprocessor was an excellent design choice when it was invented. Today it is a fossil. I'm still in awe of why C++ has never gotten around to deprecating it. 2. D uses import rather than #include. This is just way, way faster, as the .h files don't need to be compiled over and over and over and over and over ... D's strategy is to separate the parse from the semantic analysis. I suppose it is a hair slower, but it also doesn't have to recompile the duplicate declarations and fold them into one. Compile time function execution can be a bottleneck, sure, but that (of course) depends on how heavily it is used. I tend to use it with a light touch and the performance is fine. If you implement a compiler using it (as people have done!) it can be slow. > one of the ways I have kind of implemented templating in C is by defining a variable and importing a c file, changing the variable, and then reimporting the same c file. Another thing I've done is define a bunch of things and then import the SQLite C Amalgamation and then add another function (I do this to expose a SQLite internal which isnt exposed via its headers). All of these use cases would break with this change. I am not suggesting removing #include for C. The import thing would be additive. > Are there any thoughts about these issues? If you're using hacks to do templating in C, you've outgrown the language and need a more powerful one. D has top shelf metaprogramming - and as usual, other template languages are following in D's path. reply baranul 3 hours agorootparentNice explanation. Modules are the way forward. Looks to always have been. Not understanding the resistance, when the advantages are clear. reply WalterBright 14 minutes agorootparentI do understand the resistance. C is a simple, comfortable language, and its adherents want it to stay that way, warts and all. But in the context of that, what baffles me is the additions to the C Standard, such as useless (but complicated!) things like normalized Unicode identifiers, things with very marginal utility like generic functions, etc. Why those and not forward declarations? reply chacham15 16 hours agorootparentprevThanks for taking the time to respond! I have a few followup questions if thats ok: > You are correct in that the source code to the function being evaluated must be available to the compiler. This can be done with #include. I do it in D with importing the modules with the needed code. > D's strategy is to separate the parse from the semantic analysis. I suppose it is a hair slower, but it also doesn't have to recompile the duplicate declarations and fold them into one. I dont quite follow all the implications that these statements have. Does the compiler have a different way of handling a translation unit? - Is a translation unit the same as in C, but since you're #including the file you would expect multiple compilations of a re-included C file? woudnt this bloat the resulting executable (/ bundle in case of a library) - Are multiple translation units compiled at a time? Wouldnt this mean that the entire translation dependency graph would need to be simultaneously recompiled? Wouldnt this inhibit parallelization? How would it handle recompilation? What happens if a dependency is already compiled? Would it recompile it? > Performance I think a lot of this is tied to my question about compilation/translation units above, but from my past experience we have \"header hygene\" which forces us to use headers in a specific way, which if we do, we actually get really good preprocessor performance (a simple example being: dont use #include in a header), how would you compare performance in these kinds of situations vs a compiler without (i.e. either recompiled a full source file or looking up definitions from a compiled source)? > If you're using hacks to do templating in C, you've outgrown the language and need a more powerful one. D has top shelf metaprogramming - and as usual, other template languages are following in D's path. yes, as also demonstrated in the performance question, we do a lot to work within the confines of what we have when other tools would handle a lot more of the lifting for us and this is a fair criticism, but on the flip side, I dont have the power to make large decisions on an existing codebase like \"lets switch languages\" (even if for a source file or two...I've tried) as much as I wish I could, so I have to work with what I have. reply WalterBright 15 hours agorootparent> I dont have the power to make large decisions on an existing codebase like \"lets switch languages\" We struggled with that for a long time with D. And finally found a solution. D can compile Standard C source files and make all the C declarations available to the D code. When I proposed it, there was a lot of skepticism that this could ever work. But when it was implemented and debugged, it's been a huge win for D. > Performance With D you can put all your source files on one command line invocation. That means that imports are only read once, no matter how many times it is imported. This works so well D users have generally abandoned the C approach of compiling each file individually and then linking them together. A vast amount of time is lost in C/C++ compilation with simply reading the .h files thousands of times. Modules/imports are a gigantic productivity booster. They're not hard to implement, either. Except for the way C++ did it. > re multiple translation units compiled at a time? Wouldnt this mean that the entire translation dependency graph would need to be simultaneously recompiled? Wouldnt this inhibit parallelization? How would it handle recompilation? What happens if a dependency is already compiled? Would it recompile it? Yes, yes, yes, yes. And yet, it still compiles faster! See what I wrote above about not needing to read the .h files thousands of times. Oh, and building one large object file is faster than building a hundred and having to link them together. reply ufo 6 hours agorootparentI know that in other languages, one obstacle for \"just compile the C files\" is that the target language might not have pointers and thus have difficulty representing things such as return-by-pointer. I suppose in D this was less of an issue because D has pointers? reply WalterBright 12 minutes agorootparentI'm not sure what you mean. reply thayne 16 hours agorootparentprev> Is a translation unit the same as in C, but since you're #including the file you would expect multiple compilations of a re-included C file? woudnt this bloat the resulting executable (/ bundle in case of a library) I think the idea is that compiling a translation unit produces two outputs, the object code (as it currently does), and an intermediate representation of the exported declarations, that could be basically a generated .h file, but it would probably be more efficient to use a different format. Then dependent translation units use those declaration files. With this, you can still compile in parallel. You are constrained by the order of dependencies, but that is already kind of the case. One complication is that ideally, if the signature doesn't change, but the implementation does, you don't need to re-compile dependent translation units. This is trivial if your build system detects changes based on content (like, say, bazel), but if it uses timestamps (like make) then the compiler needs to ensure the timestamp isn't updated when the declarations don't change. But this really isn't a new concept. Basically every modern compiled language works fine without needing separate header files. reply WalterBright 8 minutes agorootparentThe D compiler has an option to generate a \"header file\" from D modules. It's called a .di file. It's useful if you want to hide the implementation from a compiler, as you would with libraries. As it turned out, though, people just found it too convenient to just import the .d file. But as a very unexpected dividend, it was discovered that the D compiler would generate .di files from compiling .c files, and realized that D had an inherent ability to translate C code to D code!!!! This has become rather popular. reply dwattttt 14 hours agorootparentprev> This is trivial if your build system detects changes based on content (like, say, bazel), but if it uses timestamps (like make) then the compiler needs to ensure the timestamp isn't updated when the declarations don't change. This is where the traditional distinction of \"compiler vs Make\" makes things harder; you want dependencies tracked at the \"declaration\" level, rather than the file level. If the timestamp _and_ content of the exported declarations file change, but none of the _used_ declarations changed, then there's no more compilation to be done. At best with file level tracking your build system will invoke the compiler for every downstream dependency, and they can decide if there's any more work to be done. The build system would need to know which declarations are used (and what a declaration is) to do better. reply daymanstep 18 hours agorootparentprevCan't you use precompiled headers? reply WalterBright 18 hours agorootparentInteresting you brought that up. I implemented them for Symantec C and C++ back in the 90s. I never want to do that again! They are brittle and a maintenance nightmare. They did speed up compilations, though, but did not provide any semantic advantage. With D I focused on fast compilation so much that precompiled headers didn't offer enough speedup to make them worth the agony. reply fuhsnn 17 hours agorootparent>They are brittle and a maintenance nightmare I happened to be reading DMC source this week, those hydrate/dehydrate stuff really is everywhere (which I assume is solely used for precompiled headers?) reply WalterBright 16 hours agorootparentYup. I spent a crazy amount of time debugging that. The tiniest mistake was a big problem to find. reply ndesaulniers 11 hours agorootparentprevI had an intern try to use precompiled headers for the Linux kernel. The road block they found was that the command line parameters used to compile the header must exactly match for all translation units which it is used. This is no the case for the Linux kernel. We could compile the header multiple times, but the build complexity was not something we could overcome during the course of one internship. reply WalterBright 6 minutes agorootparent> must exactly match Yup. My compiler kept a list of which switches would perturb compilation and so would invalidate the precompiled header, and which did not. Precompiled headers are an awful, desperate feature. Good riddance. reply xigoi 18 hours agoparentprevI personally don’t like forward referencing because it makes code harder to read. You can no longer rely on the dependency graph being in topological order. reply WalterBright 18 hours agorootparentAs the article writes, that forces the private leaf functions to be at the top, with the public interface at the end of the file. The normal way is the public interface at the top, and the implementation \"below the fold\", so to speak. > topological order You are correct. But its the reverse topological order, which is not the most readable ordering. One doesn't read a newspaper article starting at the bottom. reply xigoi 18 hours agorootparentMaybe it’s because I’m primarily a mathematician, but I like building complex stuff up from primitives and having the most important results at the end. reply bmacho 1 hour agorootparentI think you mean it in the context of proofs, right? Proofs are indeed often best written in a topological order: a series of true statements, where every reference refer backwards. You don't often see Answer = A + B, where A = ... ... B = ... albeit you sometimes see it, and it is totally valid. For proofreading something, it makes a big difference: if things are in a topological order, you can simulate a constant memory finite machine. If they are not in a topological order, well, probably you better just rewrite it (or at least I do). For most other things, I usually prefer the bird-view first, when I am doing or reading some elses math. Funnily the language Haskell which operates on definitions, is very order independent, it even allows circular definitions. I like it for leetcode and such. reply kccqzy 18 hours agorootparentprevThat's not how I do things in math. I always need motivation first. So I start with the theorem, look at a couple of examples to see why this theorem is interesting, and then the various lemmas leading into the proof. So that means I really like declaring but not defining the public interface first, and then define the private helper functions, and finally definitions for the public interface. reply WalterBright 18 hours agorootparentprevVive la différence - and you'll still be able to do it your way! reply Lvl999Noob 14 hours agorootparentprevPerhaps the difference is having algorithm in your head and just putting it into code, versus only knowing the top level work to be done and implementing the needed operations later. If I am writing some kind of service, I would write the main public functions first, using undefined functions in their bodies as needed. Then I would implement those functions below. reply chikere232 10 hours agorootparentprevPeople learn the ordering. If that is their biggest hurdle learning C they have a blessed life reply billfruit 17 hours agoparentprevEvery other language does seems to not require header file/forward declarations. I don't understand the backlash against that. Are modern C compilers actually still single pass? reply WalterBright 16 hours agorootparent> Are modern C compilers actually still single pass? All except ImportC, which effortlessly handles forward references. (Mainly because ImportC hijacks the D front end to do the semantics.) reply UncleEntity 14 hours agorootparentprevA bit of an aside but I was poking around in the SPIR-V spec yesterday and they can do forward references because the call site contains all the information to determine the function parameter types. Just thought it was interesting and not really something I had thought about before. reply kreco 17 hours agoprev> Evaluating Constant Expressions The examples are quite simple in the article but I believe more complex cases would significantly degrade the compiler speed (and probably the memory footprint as well) and would require a VM to leverage this. Which is probably assumed \"too complex\" to go into the standard. I'm not saying it's impossible, but I kind of understand why this would not go into any kind of standard. > Importing Declarations I wish C++ (or even C) would have gone into this direction instead the weird mess of what is defined for C++20. Additionally you might import module into some symbol, like: #import \"string.c\" as str and every non-static symbols from the file can be accessed from like: str.trim(\" Hello World \"); > __import dex; This is totally tangential but I don't like when file paths are not explicit. In this specific case I don't know if I'm importing dex.d or dex.c. reply WalterBright 16 hours agoparent> I kind of understand why this would not go into any kind of standard. Other popular languages can do it. That aside, it is an immensely popular and useful feature in D. And yes, as one would expect, the more it is used, there's compile time speed and memory consumption required. As for a VM, the constant folder is already a VM. This just extends it to be able to handle function calls. C has simple semantics, so it's not that bad. > Additionally Great minds think alike! Your suggestions are just what D imports do. https://dlang.org/spec/module.html#import-declaration > In this specific case I don't know if I'm importing dex.d or dex.c This issue does come up. The answer is setting up your import path. It's analogous to the C compiler include path. reply thayne 14 hours agoparentprev> I believe more complex cases would significantly degrade the compiler speed (and probably the memory footprint as well) and would require a VM to leverage this. I'm pretty sure most production grade c compilers already do some level of compiler time evaluation for optimization. And C already has constant expressions. I think a bigger hurdle would be that the compiler needs access to the source code of the function, so it would probably be restricted to functions in the same translation unit. And then there is the possibly even bigger people problem of getting a committee with representives from multiple compilers to agree on the semantics of such constant evaluation. reply loeg 15 hours agoparentprev> The examples are quite simple in the article but I believe more complex cases would significantly degrade the compiler speed (and probably the memory footprint as well) and would require a VM to leverage this. > Which is probably assumed \"too complex\" to go into the standard. I'm not saying it's impossible, but I kind of understand why this would not go into any kind of standard. I mean, it's basically 1:1 with the constexpr feature in C++. Almost every C compiler is already a C++ compiler, supporting constexpr functions and evaluation in C can't be that bad, can it? reply kstenerud 14 hours agoprevCompile time unit tests are as bad of an idea as \"unused import/variable/result\" errors (rather than warnings). They're \"nanny features\" that take control away from the developer and inevitably cause you to jump through bureaucratic hoops just to get your work done. These kinds of build-failing tests are great for your \"I think I'm finished now\" build, but not for your \"I'm in the middle of something\" builds (which are what 99% of your builds are). It's like saying \"You can't use the table saw until you put the drill away!\" reply chii 13 hours agoparent> These kinds of build-failing tests are great for your \"I think I'm finished now\" build, but not for your \"I'm in the middle of something\" builds i tend to disagree. If you tried to express some thought but the compile time tests tells you you're wrong, you might actually just have an incomplete thought, or have not thought through all of the consequences of said expression. It's basically what type-checking is in haskell - you cannot compile a program that does not type-check correctly. This forces you as a programmer, to always, and only, express complete thoughts. Incomplete, or contradictory thoughts cannot be expressed. This should, in theory, lead to programs that are more well thought out. It also makes the program harder to write, because it forces the programmer to discover corners of their program for which they \"know\" isn't valid but don't care. reply kstenerud 12 hours agorootparent> it forces the programmer to discover corners of their program for which they \"know\" isn't valid but don't care. And this is precisely why I disagree with forcing it upon the developer at every stage of development. Generally, while in the thick of things, I just want to get things working with one part, not worry about what other parts this breaks (yet). But the pedantic \"you have to fix this first\" enforcement breaks my concentration because now I have to split my attention to things I don't want to even be bothered with yet. I'll get to it, but I sure as hell don't want you telling me WHEN I should. reply fishstock25 3 hours agorootparent> because now I have to split my attention to things I don't want to even be bothered with yet. One of the reasons could that you realize you don't need those parts, so it would have been a waste of time to write tests for them. Is that the same as saying I don't want to have to write types either? Maybe. Types are like lightweight incomplete specs. reply kstenerud 1 hour agorootparent> One of the reasons could that you realize you don't need those parts, so it would have been a waste of time to write tests for them. Or perhaps the parts existed, were useful, did have tests, and now a new feature requires refactoring that temporarily breaks things before I finally bring the house in order again. But I don't want to throw out the tests because parts of them may still be useful. My point is, if the code is capable of being compiled and run, who is anyone to dictate that I shouldn't be allowed to run it (even broken) during my development cycle, just for some bureaucratic \"I know better than you\" reason? This is the problem I see all over - people peer out from their limited perspective, assume that they see enough, and then make excessively restrictive policy decisions about what we can and cannot do. It's hubristic and so very, very annoying to the rest of us, especially since they also have a tendency to double-down, and there seems to be no way of getting through to them. reply marcosdumay 2 hours agorootparentprev> I don't want to even be bothered with yet Why did you get out of your way to write tests about something that you don't want to be bothered about? reply kstenerud 1 hour agorootparentBecause the test already exists as part of the solution that's worked up until now, and I don't want to modify it or any other related tests until I've finished my work to the point that I think the interface is stable enough for the tests to be updated. reply d0mine 11 hours agorootparentprevTest-driven development has its uses. But it is wrong to make it mandatory. I myself run static checks/unit tests almost all of the time. Still it is useful to skip them from time to time and just run the code to see the results (make it work before you make it \"right\" according to some linters rules). reply omoikane 13 hours agoparentprevMaybe these compile time tests are more like `static_assert`, which is valuable for catching incompatible uses of library functions. Pretty good idea in my opinion. reply kstenerud 13 hours agorootparentSure, enforcing invariants is a good thing to do right off the bat. But not \"does this code do what it says on the tin?\" kinds of tests. Those are better run gradually, at the (current) developer's behest (and most certainly, not blocking compilation). reply tczMUFlmoNk 9 hours agorootparentprevThe article is literally talking about `_Static_assert`, yes. It's used in the code examples and described in the text. reply pansa2 15 hours agoprev> the compiler only knows about what lexically precedes it. […] it drives programmers to lay out the declarations backwards. The leaf functions come first, and the global interface functions are last. It’s like reading a newspaper article from the bottom up. It makes no sense. Defining functions on a “bottom-up” order like this is common even in languages like Python which allow forward references. [0] Is that just a holdover from languages which don’t allow such references? Or does it actually make more sense for certain types of code? [0] https://stackoverflow.com/a/73131538 reply almostgotcaught 15 hours agoparentYou're confused. There are no forward references in python. It's simply that identifiers in the body of a function aren't resolved until the function itself is executed (if ever) and at the point everything in the module scope has been defined. You can test this yourself by just putting any name into a body and loading the module. reply pansa2 14 hours agorootparent> identifiers in the body of a function aren't resolved until the function itself is executed (if ever) and at the point everything in the module scope has been defined Yes, so within a function you can refer to things that are defined later in the module. Isn't that's a \"forward reference\", even if the details are slightly different from how they work in D? reply almostgotcaught 14 hours agorootparentnext [2 more] [flagged] pansa2 14 hours agorootparentYou're right, it's not the same mechanism. But in this particular context it has the same effect - it allows you to define functions in a source file in arbitrary order. reply throwuxiytayq 10 hours agoparentprevIn my code, the public interface always tends to bubble upwards, and implementation details go at the end of the file. I don’t even have a strict rule for this, it just reads more cleanly and seems to make sense. Especially if the implementation is large - you don’t want to scroll through all that before you get to the basics of what it all does. I’m curious how everyone else does things. reply steinuil 8 hours agoprev> The leaf functions come first, and the global interface functions are last. To me that is backwards. I prefer code written in a topological order for a number of reasons: - It mirrors how you write code within a function. - It's obvious where you should put that function in the module. - Most importantly, it makes circular dependencies between pieces of code in a module really obvious. I'm generally not a fan of circular dependencies, because they make codebases much more entangled and prevent you from being able to understand a module as a contained unit. In Python they can even lead to problems you won't see until you run the code[0], but circular imports are probably so common that current type checkers disable that diagnostic by default[1]. I think languages that don't support forward references (C, but also OCaml and SML) let me apply the \"principle of least surprise\" to circular dependencies. OCaml even disallows recursive dependencies between functions unless you declare the functions with \"let rec fn1 = .. and fn2 = ..\", which may be a bit annoying while you're writing the code but it's important information when you're reading it. [0]: https://gist.github.com/Mark24Code/2073470277437f2241033c200... [1]: https://microsoft.github.io/pyright/#/configuration?id=type-... (see reportImportCycles) reply thayne 14 hours agoprevSome of my \"obvious things c should do\" for me would include things like - add support for a slice type that encodes a pointer and length - make re-entrant and ideally threadsafe APIs for things that currently use global state (including environment variables). - standardize something like defer in go and zig, or gcc's cleanup attribute - Maybe some portable support for unicode and utf-8. reply zffr 13 hours agoparentAren’t most of these things you want in the standard library, and not things that the language itself should do? reply thayne 13 hours agorootparentOnly half of them. The first and third are language things. The first could almost be done with macros. Except that separate declarations of an equivalent struct are considered different, so the best you cand do is a macro you can use define your owne typedef for a specific slice type. It could be done in the library if c supported something like a struct that had structural instead of nominal typing. reply pajko 12 hours agoprevC23 has constexpr, which cannot be given for functions yet, but there's a proposal: https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2976.pdf reply norir 3 hours agoprevI consider forward references an anti-feature. I want any language I use to have the following property: if I append to the source file, I can't break previously correct code above the insertion point. Forward references both break this property _and_ requires multiple compiler passes. More generally though, it's time to stick a fork in c. To me the only sane ways to use c are as a compilation target or for quick and dirty prototypes. We can't make c better by adding to it. We need to let it die peacefully so its grandchildren may live. reply the-grump 1 hour agoparentC will always live. I’m not onboard with significant changes to C but the language will always be around at the interface between hardware and software and probably as the lingua franca for FFI. reply James_K 9 hours agoprevI feel that much of the point of C is that it's easy to implement. Substantially increasing its scope doesn't seem like the best idea. Perhaps they could do something akin to Scheme and have a \"small\" and \"large\" version of the specification. reply oplaadpunt 1 hour agoparentThey have that, to some degree. The standard library is mostly optional. Also, a lot of things are 'implementation defined', so you could just not implement those. That leaves quite a small language core. reply sylware 18 minutes agoprevThe obvious thing C should do since its syntax is already too rich and complex: create a µC or C- syntax profile which would remove tons of this complexity. https://news.ycombinator.com/item?id=42657591 reply uecker 10 hours agoprevI think the real question is why not everybody has already moved to D, if it is so much better and can do all the great things. The answer is that all these things have trade-offs, including implementation effort, changes in tooling, required training, backwards compatibility, etc. some of the features are also not universally seen as better (e.g. IMHO a language which requires forward declaration is better, I also like how headers work). reply nine_k 1 hour agoparentYou mean, of course, Zig and Rust, right? Because D has a sizable runtime library and GC, which can be opted out of, but with very significant limitations, AFAICT. reply DarkmSparks 14 hours agoprevwhile this is a very interesting take, I think the premise is a little bit to simple. Firstly, there are at least 3 C compilers in widespread use, from apple, microsoft and gnu, these are a long way from 1 for 1 to each other so when it says for example: ->In other words, while C can compute at compile time a simple expression by constant folding, it cannot execute a function at compile time. Maybe the compiler he tried cannot, but another can, no idea, it wasnt tested, they can be made to (the whole point of the article), apple and microsoft cannot be made to, everything in this article could have have been submitted as a merge request to gnu.... Article doesnt even state whose C compiler they embedded afaict. wrt to \"standard\" c specifically, there are for sure some hard constraints on all the wild and wacky hardware support required that must make proposing and implementing changes within the c standard extremely hard, max respect to the anonymous experts that have got it to where it is today, but imho a lot of the \"why doesnt c\" questions can be as easily answered as \"why doesnt V8 support 8 bit pic micros\". reply dailykoder 12 hours agoprevMaybe C should do that, but it won't. So why complain? Just work with the language as it is reply EuAndreh 17 hours agoprevGood suggestions, but also meh, e.g.: forward declaration requirement enables a single-pass compiler to emit code on-the-fly. I have a much better list for things to add to C: Nothing. C isn't perfect, or nearly as good as it could be, but simply adding things onto C gets you C++. Adjusting what sircmpwn says: in C you don't solve problems by adding features, but by writing more code in C. I liked an answer on stack overflow on a question on \"how to write a generic function wrapper in Go\", or something similar. Many suggestions included reflection, but the author wanted something simpler with varargs without reflection. A comment simply said: \"wrong language\". I'd rather adopt this position for some languages, instead of add more and more to C3X. I do away with things in C23, and don't want even more things added in to C. Making a strech of OP's arguments: \"look at all this cool things that C could do, and that D does!\". Well, go on and use D, nothing wrong with that. (BTW, I do write test targets for every file in my C projects, but I'm not so much into jogging). Those things aren't that obvious, and I'd rather not have them added to C. Wrong language. reply WalterBright 16 hours agoparent> forward declaration requirement enables a single-pass compiler to emit code on-the-fly. True, I know all about that. My Zortech C and C++ compiler was one pass (after the multiple preprocessing passes). The ground up ImportC C compiler completed a couple years ago has a separate parse pass. So I well know the tradeoffs. The parser being stand-alone means it is much simpler to understand and unittest. I found no advantage to a single pass compiler. It isn't any faster. > simply adding things onto C gets you C++ C++ doesn't allow forward declarations either. Successfully doing a parse-only on C code doesn't quite work. It turns out the grammar relies on a symbol table. Fortunately, only a symbol table of the typedefs. Once adding that in, ImportC worked. (I really tried to make it work without the typedef symbol table!) C++ added a bunch more syntax that relies on the symbol table. I would not even try fixing it to work as parse-only. > in C you don't solve problems by adding features, but by writing more code in C The trouble with such sayings is like following a google map that says cross this bridge, but wasn't updated with news that the bridge is out. > Those things aren't that obvious, They are once you use another language that doesn't have those restrictions. > and I'd rather not have them added to C. C adds new things all the time to the Standard, like normalized Unicode identifiers, which are a complete waste of time. Every C compiler also adds a boatload of extensions, some good, some wacky, many ineptly documented, all incompatible with every other C compiler extensions. reply EuAndreh 15 hours agorootparent> The parser being stand-alone means it is much simpler to understand and unittest. Stand-aloneness and single-passness are orthogonal. > I found no advantage to a single pass compiler. It isn't any faster. A gigantic advantage: a single-pass-compilable language is simpler. By definition. Implementations may or may not be simpler or faster. > C++ doesn't allow forward declarations either. Well, that's not what I meant. C++ is \"C with just this thing\" done way too many times. > The trouble with such sayings is like following a google map that says cross this bridge, but wasn't updated with news that the bridge is out. TBH, I didn't really get this. Is this about sticking to C as is, but it is outdated as is? C would be outdated if it didn't have, say, long long for 64-bit numbers. Having \"true\" be a keyword instead of a macro doesn't change how outdated it is or isn't, just like compile-time evaluation also doesn't. > They are once you use another language that doesn't have those restrictions. I have used many, and I still don't find them obvious. > C adds new things all the time to the Standard, like normalized Unicode identifiers, which are a complete waste of time. I agree that many/most are a waste of time, and shouldn't be added to C. The fact of C adding things to the standard all the time shouldn't justify adding even more things, but make one question if those are needed at all, and how to accomplish the goal without it. > Every C compiler also adds a boatload of extensions, some good, some wacky, many ineptly documented, all incompatible with every other C compiler extensions. I know about that, and my position is the same: just don't. I don't use them also. reply Dylan16807 11 hours agorootparent> A gigantic advantage: a single-pass-compilable language is simpler. By definition. That's only \"by definition\" if you take a language that needs multiple passes, then remove the features that need multiple passes, and don't replace them with anything else to compensate. The \"by definition simpler\" version of C would not only disallow forward references, it would have no forward declarations either. As-is, forward declarations add some complexity of their own. (Also, if you can figure out a way to emit jump instructions in a single pass, you can probably figure out a way to call unknown functions in a single pass.) reply EuAndreh 30 minutes agorootparentI have the impression you're mixing single-pass compilation and O(1) memory use of the compiler. As is, C already is single-pass compilable, modulo some unnecessary syntax ambiguities. As the compiler reads the text, it marks some character strings as tokens, these tokens are grouped as a fragment of code, and some fragments of code are turned into machine code. A simple function of a 100 lines doesn't need to be parsed until the end for the compiler to start emitting machine code. Like the parser, this requires memory to keep tabs of information and doesn't work for all types of constructs, like a jump instruction to a label defined later in a function. The code emitter soaks input untill it is possible, and does so, like when the label is already known and can be jumped to. reply WalterBright 21 minutes agorootparentYou cannot do any optimization when generating machine code that way. That's fine for a primitive compiler built for a school project, but not much else. (Even \"no optimization\" switch settings on a compile do a lot of optimizations, because otherwise the code quality is execrable.) reply WalterBright 25 minutes agorootparentprevDoing jump instructions in a single pass is done by creating a patch list, and when the compilation is done walking the patch list and \"fixing them up\". Doing this with functions is a lot more difficult, because one cannot anticipate the argument types and return types, which downstream influence the code generation. Of course, early C would just assume such forward references had integer arguments and integer types, but that has long since fallen by the wayside. reply EuAndreh 17 hours agoparentprevI have my own list of things that could \"easily\" be added to C, but I'd rather them not to be. reply WalterBright 16 hours agorootparentYou get them anyway in the form of extensions. reply EuAndreh 14 hours agorootparentThanks, but no thanks. reply matt3210 14 hours agoprevWoah I haven’t seen the name “digital mars” since the late 90s looking for compilers! reply chikere232 9 hours agoprevIf you want to write D, write D. C is fine without these things reply bluGill 6 hours agoparentI want to write d, but I have a ton of c - d makes it easy. rust is harder as I have to write ffi. d makes working with something else easy shich it an advantage. Too bad it never took off. reply Alifatisk 5 hours agorootparent> Too bad it never took off. It may not be very hyped but the forum and community is quite active. I don't think it's popularity should stop you from exploring it, it's a fascinating language. reply TinkersW 19 hours agoprevC++ has all of these except forward referencing declarations(though Importing Declarations requires modules which nobody uses yet). I'm not sure why forward reference declarations is needed nowadays(or if it really is from a language standpoint). C could probably copy C++'s constexpr & static_assert stuff to get the first 2. reply WalterBright 18 hours agoparentNobody uses C++ modules because they are clumsy to use. D's are easy. Note: anyone is free to copy D's module design. It's the best one out there. > I'm not sure why forward reference declarations is needed nowadays The article gives reasons. Although they aren't necessary, they are deleterious to code layout which becomes a slave to the declaration order rather than aesthetic order. > C++'s constexpr is still lagging behind D's, after 17 years of development. In D, the garbage collector makes memory allocation in it trivial. Furthermore, only the path taken through a function needs to be CTFE-compatible, the path not taken does not. reply zeroonetwothree 14 hours agorootparentRobert Frost would approve reply dccsillag 18 hours agoparentprevIf I'm not mistaken, the treatment of forward declarations proposed in the article actually breaks the C standard, which would be a rather pressing concern. As far as I am aware, that is the reason why things are the way they are right now in C land. (In the past, there were more legitimate concerns on the ease of implementation. Nowadays, as the article points out, they are pretty moot, other than having to keep backwards-compatibility.) I'm also rather bothered that on the bit on const execution in the article, there was no discussion on how to deal with functions that may not terminate or take rather long to execute. Especially considering the unit tests motivation, this seems like a rather blaring omission. reply WalterBright 16 hours agorootparentHow does it break the C Standard? > how to deal with functions that may not terminate or take rather long to execute Control-C, the same as when running any executable that shouldn't be taking that long. It doesn't solve the halting problem :-/ reply UncleEntity 14 hours agorootparentSo a single typo DDoSes the entire Red Hat buildbot fleet? reply kstenerud 14 hours agorootparentYou set a timeout, like in any robust CI. reply loeg 15 hours agoparentprevC's already got static_assert; just missing constexpr. reply bawolff 14 hours agoprevI'm not a c programmer, but having unit tests automatically run at compile time seems odd. If i wanted to run tests at the same time as compiling i would put that in the makefile. reply sixthDot 13 hours agoparentMost of D users would rather call that \"static contracts\". I dont know why the author choose to call that \"unit test\". reply zeroonetwothree 14 hours agoparentprevWhy is it odd? The compiler does all sorts of other checks based on things like static assert or type information. reply kstenerud 14 hours agorootparentIt's odd because you lose control over that aspect of compilation. It slows down the development loop because every time you do a build you have to wait for a bunch of unit tests that you don't care about yet. Every time you do exploratory work you now have to comment out all the tests that this work breaks because otherwise it won't compile anymore. That would be even more annoying than Go's stupidly pedantic compiler. reply Alifatisk 5 hours agorootparent> It slows down the development loop because every time you do a build you have to wait for a bunch of unit tests that you don't care about yet. Can't that be an optional thing decided by some compiler flag? I think I remember doing something like that in D. reply kstenerud 4 hours agorootparentThat's how one would normally do it, and that's what I would expect to see: Build for development, and the compiler only errors out if things are so bad that it's unable emit a binary (for everything else, it emits warnings). Build for release, and it errors out unless EVERYTHING is done right. Unfortunately, ever since golang decided on an autocratic and backwards \"there are no warnings, only errors\" policy, others have started to sip from the same kool aid jar. reply nitwit005 13 hours agoprev> Everywhere a C constant-expression appears in the C grammar the compiler should be able to execute functions at compile time, too, as long as the functions do not do things like I/O, access mutable global variables, make system calls, etc. They have been working on bringing constexpr, which exists in c++, to c. This is essentially a constexpr function. reply m463 10 hours agoprevThe most obvious thing c should do is... evolve. Even Fortran seems to have added object-oriented constructs, all kinds of new types and concurrent and parallel programming reply 4gotunameagain 10 hours agoparentThere has been an object-oriented evolution of c, it's called c++ ;) reply MichaelMoser123 6 hours agoprevi think compile time evaluation should be extended: if it can get a pass over the source code then this could replace preprocessor macros with something less shitty. reply MichaelMoser123 4 hours agoparentmaybe even add reflection and the ability to check for the type of a value - but that is probably too much to ask for. reply acheong08 19 hours agoprevIt really reads like the author just wants Zig. reply robterrell 19 hours agoparentThe author is the creator of D, so he's probably fine with D. And D is something like 25 years old. Whereas Zig is just a toddler. reply johnnyanmac 14 hours agorootparentahh, that explains the ImportC comparisons. Also didn't realize that person is also a regular on HN as well. reply WalterBright 19 hours agoparentprevZig copies features from D! reply koolba 19 hours agorootparentImitation is the sincerest form of flattery. reply WalterBright 19 hours agorootparentYup. D is the source for a number of recent features in other languages. The reason I embarked on D is because C and C++ were too reluctant to move forward. reply ksec 13 hours agorootparent>The reason I embarked on D is because C and C++ were too reluctant to move forward. I just want to take this opportunity to say thank you. While D may not taken up from the rest of the world. It has surely",
    "originSummary": [
      "The Dlang community has integrated a C compiler within the D compiler, known as ImportC, to address unresolved issues in Standard C, now at C23. - ImportC enables functions to be executed at compile time, facilitating compile-time unit testing without the need for separate builds. - It also supports forward referencing and importing declarations without .h files, simplifying the development process."
    ],
    "commentSummary": [
      "Header files in C distinguish between public and private interfaces, appreciated by some developers for simplicity and ease of use. - Modern tools like Rust's `cargo doc` provide efficient management of documentation and interfaces without code duplication, appealing to others. - The choice between these methods often depends on personal preference, project needs, and team workflow."
    ],
    "points": 225,
    "commentCount": 258,
    "retryCount": 0,
    "time": 1736636302
  },
  {
    "id": 42668953,
    "title": "World's darkest and clearest skies at risk from industrial megaproject",
    "originLink": "https://www.eso.org/public/news/eso2501/",
    "originBody": "By continuing to use this website, you are giving consent to our use of cookies. For more information on how ESO uses data and how you can disable cookies, please view our privacy policy. Accept Select Language (en) (en) International English Dansk Deutsch Deutsch (Belgien) Deutsch (Schweiz) Deutsch (Österreich) English (Australia) English (Ireland) English (UK) Español Español (Chile) Français Français (Belgique) Français (Suisse) Italiano Italiano (Svizzera) Nederlands Nederlands (België) Polski Português Suomi Svenska Česky Science User Portal Subscribe Contact Site Map Open Menu About ESO, the European Southern Observatory Organisation Mission-Vision-Values-Strategy Director General List of Directors General Prof. Xavier Barcons Prof. T. de Zeeuw Dr. C. Cesarsky Prof. R. Giacconi Prof. H. van der Laan Prof. L. Woltjer Prof. A. Blaauw Prof. O. Heckmann ESO's Governing Bodies Member States and Partners Austria Belgium Czechia Denmark Finland France Germany Ireland Italy Netherlands Poland Portugal Spain Sweden Switzerland United Kingdom Australia (Partner) ESO & Chile ESO Representation Office in Chile Travel and Contacts Travel to ESO Headquarters Travel to Vitacura Office Travel to Guesthouse Travel to La Silla Travel to La Serena Travel to Paranal Travel to Antofagasta Travel to Chajnantor Travel to ELT Armazones Visits to ESO Sites Weekend visits to Paranal Weekend Visits to La Silla Media Visits Virtual Visits Artistic Visits ESO and Society Architecture at ESO Collaborations with ESO ESO Acronyms Timeline Sustainability Environmental sustainability at ESO ESO and the UN's Sustainable Development Goals Diversity, Equity and Inclusion Frequently Asked Questions ESO VLT/Paranal APEX/ALMA ELT Privileges and Immunities ESO & Australia Dark skies preservation Terms and Conditions Images Advanced Search Categories 360 Panorama ALMA APEX Chile Cosmology ELT ESO Supernova Exoplanets Fulldome Galaxies Galaxy Clusters Illustrations La Silla Nebulae Paranal People and Events Premises Quasars and Black Holes Solar System Star Clusters Stars Survey Telescopes Image Comparisons Image Formats Picture of the Week 2025 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 Top 100 Large JPG Zip File (3.5 GB) Original TIF Zip File (50 GB) Usage of ESO Images and Videos Wallpapers Zoomable Videos Advanced Search Categories About ESO ALMA APEX Chasing Starlight Compilations Cosmology ELT ESOcast Events Exoplanets Feature Movies Fulldome Galaxies Galaxy Clusters La Silla Nebulae Paranal Premises Quasars and Black Holes Solar System Star Clusters Stars Video News Releases Usage of ESO Videos and Images Video Formats News Press Releases 2021-2030 2025 2024 2023 2022 2021 2011–2020 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2001–2010 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 1991–2000 2000 1999 1998 1997 1996 1995 1994 1993 1992 1991 1985–1990 1990 1989 1988 1987 1986 1985 Announcements 2025 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 ESOblog New on eso.org Press Room Broadcast Services Media Visits Copyright Notice ESO Newsletters ESO News ESO Science Newsletter ESO Media Newsletter Pitch your Research ESOshop Shop Products Annual Reports Apparel Books Brochures Calendars CAPjournals Conferences Donations DVDs/Bluray/CDs Educational Material ESO Supernova Activities Flyers Handouts Maps Merchandise Messengers Mounted Images Postcards Posters Reports Stickers View shopping cart Checkout Information Bulk Orders Purchasing Steps Payment Shipping Staff Shopping Terms & Conditions Free Orders for Educators and Media Telescopes & Instruments Paranal Observatory Very Large Telescope VLT Instruments 4LGSF CUBES CRIRES CRIRES+ ESPRESSO ERIS FLAMES FORS1 and FORS2 HAWK-I ISAAC KMOS MAD MAVIS MOONS MUSE NACO SINFONI SPHERE UVES VIMOS VLT Test Cameras VISIR X-shooter VLT Unit Telescopes Names Antu Kueyen Melipal Yepun VLT Interferometer GRAVITY+ GRAVITY MATISSE NAOMI PIONIER AMBER MIDI VINCI Auxiliary Telescopes Survey Telescopes VISTA The VISTA mirror VIRCAM 4MOST The VISTA surveys The VISTA consortium VST The VST mirrors OmegaCAM The VST surveys SPECULOOS NGTS Cherenkov Telescope Array Observatory La Silla Observatory Swedish–ESO Submillimetre Telescope SIMBA New Technology Telescope EFOSC2 EMMI IRSPEC SOFI SOXS SUSI and SUSI2 ESO 3.6-metre telescope ADONIS CASPEC IRSPEC HARPS MEFOS NIRPS OPTOPUS TIMMI/2 Infrared Photometer/Spectrophotometer Standard Photometer Infrared Specklegraph Infrared Speckle Interferometer EFOSC MPG/ESO 2.2-metre telescope FEROS GROND IRAC & IRAC2 PISCO WFI Infrared Photometer/Spectrophotometer Swiss 1.2-metre Leonhard Euler Telescope CORALIE Rapid Eye Mount telescope TRAnsiting Planets and PlanetesImals Small Telescope–South Danish 1.54-metre telescope CORAVEL DFOSC CCD camera Hβ Photometer Two-Channel Photometer Four-Channel Photometer Six-Channel uvby-Hβ Photometer Télescope à Action Rapide pour les Objets Transitoires ESO 1-metre Schmidt telescope ESO 1.52-metre telescope Boller and Chivens Spectrograph Coudé Spectrograph ECHELEC RV Cass Spectrograph ESO 1-metre telescope FIDEOS Infrared Photometer Single-Channel Photometer ESO 0.5-metre telescope Single-Channel Photometer Double-Channel Photometer Coudé Auxiliary Telescope CES MarLy 1-metre telescope Dutch 0.9-metre telescope Walraven Photometer Swiss T70 telescope Bochum 0.61-metre telescope Bochum Scanner Bochum Photometer Danish 0.5-metre telescope Two-Channel Hβ Photometer Four-Channel uvby Photometer Six-Channel uvby–Hβ Photometer Grand Prisme Objectif telescope Swiss 0.4-metre telescope Marseille 0.36-metre telescope GRB Monitoring System MASCARA BlackGEM ExTrA Test-Bed Telescope APEX APEX–2A ArTéMiS ASZCa CHAMP+ CONCERTO FLASH FLASH+ LABOCA MPI_1.1THz P-ArTéMiS PI230 PolKa SABOCA SEPIA SHeFI ALMA Antennas Transporters Interferometry Receiver Bands ALMA Residencia ELT Technology for Telescopes Active Optics Adaptive Optics Integral Field Units Interferometry Spectroscopy Polarimetry Instruments Discoveries ESO Key Astronomical Discoveries Exoplanets Gravitational Wave Sources Black Holes First image of a black hole First image of the Milky Way central black hole More about the black hole at the galactic centre Gamma-Ray Bursts Science Archive Events Press Events Special Events ESO at 60 ELT Groundbreaking Café & Kosmos in Munich Awesome Universe 100 Hours of Astronomy The Eye 3D IMAX® 3D Film Hidden Universe Bond at Paranal Exhibitions ESO Permanent Exhibitions EuroScience Open Forum (ESOF) Astronomical Events Total Solar Eclipse 2019 The Comet McNaught 2007 Deep Impact - 04.07.2005 The Venus Transit - 08.11.2004 The Leonids - 17.11.1999 Solar Eclipse at ESO HQ - 11.08.1999 The Leonids - 11.1998 Mars Pathfinder - 1997 Comet Hyakutake - 1997 SL-9 and Jupiter - 1994 Comet Hale Bopp - 1994 Outreach Education Educational Material Science in School Teacher Schools Previous Programmes Catch a Star Life in the Universe Physics On Stage ALMA Kids Partnerships Volunteer Translators ESO Science Outreach Network Collaborations in Chile 2010 2009 ESO Outreach Partner Organisations Informal Educators ESO Photo Ambassadors ESO Music Ambassadors Taking the First Picture of a Black Hole Cosmic Gems Social Media For Planetariums Webcams Your ESO Pictures European Astronomy Journalism Prize Our Team Students and Visitors Join us ESO Supernova Art at ESO Data2Dome ESOblog Stars@ESO AI Disclaimer Products Audiovisual and multimedia 3D models Apps DVDs and CDs IMAX movies Mini-sites Music Planetarium shows Presentations Soundscapes Virtual Backgrounds Virtual Tours Periodicals Messenger CAPjournal Science in School Print products Annual Reports Books Brochures Bulletins Calendars Conference posters Flyers Handouts Maps Press kits Reports Postcards Stickers Printed posters Paper models Merchandise Merchandise Apparel Mounted images Stationery Electronic products Electronic cards Electronic posters ePublications Virtual Backgrounds Educational material Exhibition material Logos Order in ESOshop Business@ESO Procurement at ESO Industry Events Technology Transfer Technology Developed by Industries Novel technologies by ESO ESO know-how Future Projects Working with the ELT Safety Emergency & Evacuation Traffic & Parking at ESO Contractors at ESO Contact Information Career Opportunities Recruitment Employment Conditions International Staff Members Local Staff Members (Spanish / English) Fellows Students Paid Associates Working at ESO Other Information How to use this site European Southern Observatory Press Release World's darkest and clearest skies at risk from industrial megaproject 10 January 2025 On December 24th, AES Andes, a subsidiary of the US power company AES Corporation, submitted a project for a massive industrial complex for environmental impact assessment. This complex threatens the pristine skies above ESO’s Paranal Observatory in Chile’s Atacama Desert, the darkest and clearest of any astronomical observatory in the world [1]. The industrial megaproject is planned to be located just 5 to 11 kilometres from telescopes at Paranal, which would cause irreparable damage to astronomical observations, in particular due to light pollution emitted throughout the project’s operational life. Relocating the complex would save one of Earth's last truly pristine dark skies. An irreplaceable heritage for humanity Since its inauguration in 1999, Paranal Observatory, built and operated by the European Southern Observatory (ESO), has led to significant astronomy breakthroughs, such as the first image of an exoplanet and confirming the accelerated expansion of the Universe. The Nobel Prize in Physics in 2020 was awarded for research on the supermassive black hole at the centre of the Milky Way, in which Paranal telescopes were instrumental. The observatory is a key asset for astronomers worldwide, including those in Chile, which has seen its astronomical community grow substantially in the last decades. Additionally, the nearby Cerro Armazones hosts the construction of ESO’s Extremely Large Telescope (ELT), the world’s biggest telescope of its kind — a revolutionary facility that will dramatically change what we know about our Universe. “The proximity of the AES Andes industrial megaproject to Paranal poses a critical risk to the most pristine night skies on the planet,” highlighted ESO Director General, Xavier Barcons. “Dust emissions during construction, increased atmospheric turbulence, and especially light pollution will irreparably impact the capabilities for astronomical observation, which have thus far attracted multi-billion-Euro investments by the governments of the ESO Member States.” The unprecedented impact of a megaproject The project encompasses an industrial complex of more than 3000 hectares, which is close to the size of a city, or district, such as Valparaiso, Chile or Garching near Munich, Germany. It includes constructing a port, ammonia and hydrogen production plants and thousands of electricity generation units near Paranal. Thanks to its atmospheric stability and lack of light pollution, the Atacama Desert is a unique natural laboratory for astronomical research. These attributes are essential for scientific projects that aim to address fundamental questions, such as the origin and evolution of the Universe or the quest for life and the habitability of other planets. A call to protect the Chilean skies “Chile, and in particular Paranal, is a truly special place for astronomy — its dark skies are a natural heritage that transcends its borders and benefits all humanity,” said Itziar de Gregorio, ESO’s Representative in Chile. “It is crucial to consider alternative locations for this megaproject that do not endanger one of the world's most important astronomical treasures.” The relocation of this project remains the only effective way to prevent irreversible damage to Paranal's unique skies. This measure will not only safeguard the future of astronomy but also preserve one of the last truly pristine dark skies on Earth. Notes [1] A study by Falchi and collaborators, published in 2023 in Monthly Notices of the Royal Astronomical Society, compared light pollution at all 28 major astronomical observatories, finding Paranal to be the darkest site among them. Contacts Francisco Rodríguez ESO Media Relations Officer Santiago, Chile Tel: +56 2 2463 3151 Email: francisco.rodriguez@eso.org Bárbara Ferreira ESO Media Manager Garching bei München, Germany Tel: +49 89 3200 6670 Email: press@eso.org Connect with ESO on social media Usage of ESO Images, Videos, Web texts and Music Are you a journalist? Subscribe to the ESO Media Newsletter in your language. About the Release Release No.: eso2501 Name: Cerro Paranal, European Southern Observatory, Paranal Type: Unspecified : Technology : Observatory Facility: Very Large Telescope Images PR Image eso2501a Touching the Arc of Space PR Image eso2501b Light pollution at the world’s major astronomical observatories Videos PR Video eso2501a Night sky beauty over Paranal Send us your comments! Subscribe to receive news from ESO in your language Accelerated by CDN77 Terms & Conditions",
    "commentLink": "https://news.ycombinator.com/item?id=42668953",
    "commentBody": "World's darkest and clearest skies at risk from industrial megaproject (eso.org)212 points by Breadmaker 21 hours agohidepastfavorite116 comments topspin 7 hours agoI'm noticing that the reporting on this, including the ESO press release, is vague on exactly what this \"industrial megaproject\" happens to be. Ordinarily, there is no hesitation to disclose this, unless it's a military matter. Or a sacred cow. A sacred cow, indeed. It's a green energy operation powered by both wind and solar to generate hydrogen, electricity and ammonia. Here[1] is the AES Andes press release about this project, if you care to read the opposing spin on this matter: \"AES Chile submitted an Environmental Impact Assessment (EIA) to Chilean permitting authorities for a proposed industrial-scale green hydrogen project called Inna. The project, which is in early-stage development, could include a variety of solutions, including green hydrogen for export or domestic consumption, aligned with Chile’s National Green Hydrogen Strategy.\" [1] https://www.aesandes.com/en/press-release/aes-andes-submits-... Land use. It's not just a fossil fuel shill talking point. reply fsh 4 hours agoparentI doubt that \"land use\" is a big issue in one of the least densely populated countries on earth. Surely one could find a place for an industrial site that is not within 5 km of the world's prime telescope site. Using the existing infrastructure probably makes it slightly cheaper though. reply JumpCrisscross 2 hours agorootparent> Surely one could find a place for an industrial site Has anyone proposed one? reply psychlops 2 hours agorootparentprevI thought space was the the world's prime telescope site. reply IncreasePosts 36 minutes agorootparentNot really, due to the costs and constraints of space-based telescopes reply whimsicalism 1 hour agorootparentprevthink that might be stretching the definition of “world” a bit far reply MostlyStable 44 minutes agorootparentprevThe omission of this information is partly why I'm suspicious of the article. A well written article would have included things such as the above information about what the project was and additionally why it was being proposed for the given site. Omitting any information on the other side of the equation, and talking only about the impacts it will have on the observatory sure sounds like activist propaganda to me. reply undersuit 3 minutes agorootparentUnless the article has been updated there is no omission. reply 55555 4 hours agoparentprevThe article says \"It includes constructing a port, ammonia and hydrogen production plants and thousands of electricity generation units near Paranal.\" reply jordanb 4 hours agoparentprevEveryone I know in environmental activism hates hydrogen and sees it as green-washing the petrochemical industry. reply kbolino 4 hours agorootparentThey may feel that way, but that seems emotional rather than rational. The choices for small-scale energy sources are: batteries with all their dirty mining, biofuels taking away lots of arable land from food/textile/etc production, or fuel cells which can still involve petrochemicals (but don't have to). There are no perfect options, and strapping enough solar panels or wind turbines to a vehicle (car, bus, train, airplane, etc) to make it drivable is just not even remotely feasible. No doubt the petrochemical companies want to continue existing, but shifting the transportation infrastructure away from directly burning gasoline and natural gas is a net win even if in the short term there are still hydrocarbons involved. Going all in on electric vehicles only is not diversifying the solution space enough. reply epistasis 1 hour agorootparentWhat dirty mining for batteries? Please be specific. This story was always a PR drive that focused on the \"dirtiness\" of batteries but never compares it to the dirtiness of all the other parts of competing technologies. And talking about the \"dirtiness\" of batteries but not every other single part of our industry (steel for everything, all the nasty stuff for electrolyzers, etc.) is all part of prioritizing emotional over rational. Public discussion of our energy system is definitely more emotional than rational, and I would argue that the emotional side of things means that we do a lot more fossil fuels and dirty tech, whereas a more rational approach would have us on far more solar and storage than we currently are, or plan to do. reply cosmic_cheese 24 minutes agorootparentThere’s a lot of pressure for batteries to become cleaner too, which has resulted in a lot of money going into R&D of more effective chemistries comprised of more readily available materials. It’s difficult to express the sheer momentum we have in this field right now and it would be a shame if cold water got dumped on it by negative public perception, cooling investments. It’d be a textbook example of letting the perfect being the enemy of the good and of squandered potential. reply coldpepper 18 minutes agorootparentprevMining affects a limited local area. CO2 emissions affect the whole planet. reply Workaccount2 2 hours agorootparentprevThe dirty mining and manufacturing of batteries is the dumbest red herring there is. It's like saying we need to get rid of toilet paper because it cuts down so many trees. Hydrogen on the other hand clearly is pushed by fossil fuels since it leaves the door open for them to be a major player. reply bobthepanda 2 hours agorootparentHydrogen/syngas right now is the only way forward for long distance sea or plane travel, since batteries are too heavy. reply Galaxeblaffer 29 minutes agorootparenthydrogen definitely not the only way for long distance sea, nuclear would just make so much more sense. and for place travel it also such the same as batteries, first of all its an explosive gas and second we only get less than 20%. hydrogen is just not a good solution to anything other than being a byproduct in the natural gas industry. reply koverstreet 1 hour agorootparentprevWe're going to get a 4x energy density improvement from lithium sulfur batteries, just as soon as production ramps up. reply bobthepanda 1 hour agorootparentWikipedia lists the energy density of kerosene as 9.7k Wh/L, and lithium sulfur at 550 Wh/L, which is not great. reply zardo 12 minutes agorootparentHydrogen isn't so great either. JumpCrisscross 2 hours agorootparentprev> choices for small-scale energy sources are: batteries with all their dirty mining, biofuels taking away lots of arable land from food/textile/etc production, or fuel cells Storage also includes flywheels and pumped hydro. Hydrogen is mostly a farce. reply kbolino 12 minutes agorootparentThe key adjective there is \"small-scale\". There are many more alternatives when size is not an issue. Flywheels are very useful but their energy density is too low to be the primary power source for a vehicle. reply est31 4 hours agorootparentprevI understand it if you look at the receiving end. Say in Germany they built natural gas plants shortly before the war under the premise that they must be Hydrogen ready, aka right now we use natural gas but promise in a few years when Hydrogen production is there, we'll switch to Hydrogen. That can be very easily criticized as green washing of the gas plants (although a gas plant is much better than what it would have replaced, brown coal). Now with the war and the pipelines destroyed, Germany went a different route and instead runs the coal plants for a longer time. But if you look at the production side, they are building a solar power plant. How is that green washing? There is no way to use a solar power plant other than to collect renewable energy. Either it is operational and collects renewable energy, to send it to various places, or it is not operational, but then it's been a bad investment for the investors. Now, maybe it could be part of some greater scheme where one uses this plant as the \"source\" of a multiple of the ammonium it can actually produce, and sells ammonium from fossil sources as made by Chilean sun. But that should then be addressed on its own, and not hamper the project itself (although of course a different location would be better that doesn't risk the operations of scientific instruments worth billions). reply IncreasePosts 34 minutes agorootparentprevEnvironmental activists have their own biases and blind spots. reply epistasis 2 hours agorootparentprevThere's two types of hydrogen: 1) ammonia for fertilizer production and perhaps other industrial decarbonization, and 2) fantasies of energy storage, fuel, etc. Environmentalists know of the necessity of ammonia, but push back hard on the second. Michael Liebreich's hydrogen ladder is fairly good at summarizing an honest assessment of where hydrogen will be useful: https://www.linkedin.com/pulse/hydrogen-ladder-version-50-mi... And you'll see that it gets pushed in lots of very inappropriate places. It's funny how much benefit of the doubt is given to really bad tech, like hydrogen and large nuclear reactors, despite decades of data showing that they always underperform expectations and that people who implement them always overpromise and underdeliver. It's a stark contrast to solar and wind and storage, which always seem to underpromise and overdeliver, and these technologies face huge amounts of undue skepticism not only from decision makers but also the press and the public. There's a lot of decision making in energy that is extremely disconnected from data and reality, and most of hydrogen decision making these days is disconnected from reality. reply Galaxeblaffer 21 minutes agorootparentyou are right that hydrogen is a fantasy.. but wait.. what is this solar and wind plus storage you speak of that overdeliver, specifically what storage are you talking about? the wind industry has heavily been pushing hydrogen as this storage at least in Germany and Denmark and as far as i know there's absolutely zero success here despite maasny years of trying reply Daub 12 hours agoprevWhen I lived in the Welsh countryside, there were occasional nights where I could not see my hand in front of my face. The requirements were that it was new moon, and that there was slight fog. We also lived deep in a valley, which helped. I had great fun navigating my way to the local pub in complete darkness. The odd thing is that when I recount that experience, some people refuse to believe me. Of course they are all city dwellers. reply Cthulhu_ 8 hours agoparentI've experienced that once in a simulated environment; there's a museum in Nijmegen that has an indoor setup to simulate being completely blind, you get a stick and a guide and have to navigate a living room and the like. Can recommend if you're interested in accessibility and the like! reply chrisandchris 7 hours agorootparentIn Switzerland, there are two restaurants called \"Blinde Kuh\" (blind cow), where it's completely dark and you'll get served by blind/visually impaired people. There even work visually impaired people in the kitchen (besides \"regular\" seing people). It's a fascinating wxperience. reply 55555 4 hours agorootparentprevThere's a similar experience at Tochoji Temple in Fukuoka city. reply chasd00 3 hours agoparentprevDuring a new moon parts of West Texas out in the chihuahua desert are like this. If you wait a solid 45 min with no light for your eyes to adjust it’s amazing how much you can see in the sky. reply m463 10 hours agoparentprevperil-sensitive sunglasses wouldn't help! reply madaxe_again 10 hours agoparentprevI live that experience daily. I live in a very remote corner of Portugal - we are between bortle 2 and 3 - in the bottom of a deep, steep valley. And yes - when it’s a new moon and the haze from the river blots out the stars, the experience is quite akin to having gone blind. In fact, it’s so dark I’ve used some of those nights to develop film at the outdoor sink. One thing I’ve noted is that wildlife needs to see just as much as we do - I mean, obvious, right? - but those nights are always dead silent. No birds, no insects, no rustles of this that or the other in the undergrowth. Every little noise one makes seems an affront to the cloying, thick darkness. Perhaps it’s the same instinct at play. My place in wales used to have dark skies, even fairly recently - but LED street lighting along rural roads has put paid to that. I earnestly don’t understand why a lane that sees zero foot traffic and perhaps one car during darkness hours needs a streetlamp every ten meters - while waste collections only happen every six weeks. Ah, I have become a grumpy old astronomer. reply zeristor 8 hours agorootparentI looked up bortle and Portugal, and Google gave my a light pollution map. I still don’t know what bortle means… https://www.lightpollutionmap.info reply bastih 8 hours agorootparenthttps://www.handprint.com/ASTRO/bortle.html reply ungreased0675 15 hours agoprevWould it be unthinkable to just NOT have bright lights pointed at the sky all night? Could they still do this project with severe restrictions on light emissions? If there’s some reason it absolutely must include hundreds of outdoor sodium vapor lights then build it somewhere else. reply WorkerBee28474 14 hours agoparent> Would it be unthinkable to just NOT have bright lights pointed at the sky all night? That's possible, and directed/shielded lighting is commercially available. However, the project's critics have already said that no plan the project comes up with will be good enough - “Even if [AES] do a perfect job, using perfect lights that probably don’t even exist and perfect shielding, there will be an impact and that will be significant [0] [0] https://www.science.org/content/article/chilean-energy-megap... reply aragilar 13 hours agorootparentThe plan of \"don't build a major industrial centre 5km from the best site for optical astronomy in the world, build it somewhere else\" seems like a perfect viable one to me. reply oefrha 12 hours agorootparentWell, local people probably care about economic development and don’t give a rat’s ass about astronomy. So the question becomes, who’s going to compensate for the loss of economic development? (By local I don’t mean strictly local, in case of counter arguments along the line that there are no/very few local people to begin with.) Disclosure: I’m a former physicist and I have personally operated an optical telescope with a 15’ dome, as well as a 60’ radio telescope, which probably puts me among 0.01% of world’s population. So I do know a thing or two and care about astronomy. reply kahirsch 8 hours agorootparentThis is in the middle of the desert at high altitude. There are basically no local people who aren't associated with the telescopes. https://maps.app.goo.gl/cWgWx1RKjEUjavPn8 Whether the facility is built there or 50 km away, it's going to have to draw people from more than a few km away. The entire Taltal district only has about 11,000 people. reply Enk1du 7 hours agorootparentprevAt Las Campanas, most of the staff from the cooks to the techs and a number of the researchers were all local. I found quite a bit of interest in the country as a whole as it's a source of national pride being the best location for astronomy. Allowing this to proceed will affect _all_ future astronomy projects in Chile. No one is going to splash out on a shiny, new 100m optical telescope (OWL) if anyone can come along and park a city's worth of light just down the road. https://www.eso.org/sci/facilities/eelt/owl/ reply aragilar 11 hours agorootparentprevMy understanding is ESO uses local labour where possible (e.g on building the ELT, maintenance, catering, transport), so it's not like it's one guy in a shed, there are jobs and economic benefits. That's why this seems so confusing to me, I can't see why you can't have both? reply throw5959 11 hours agorootparentDo you mean that the economical output of astronomy and this industrial project is comparable? reply est31 4 hours agorootparentprevIf most of the local people are going to move to that location, they could also move to a different location which is a bit further away from where it's planned now. reply WorkerBee28474 12 hours agorootparentprevSo that would be Mauna Kea. I don't believe there is any industry being built near there. reply aragilar 12 hours agorootparentI'm happy to be proven wrong, but I though Paranal beat Mauna Kea (and some basic google searches aren't throwing up anything that makes me question it, e.g. https://www.eso.org/gen-fac/pubs/astclim/espas/espas_reports..., though that's from more than 20 years ago, so site quality has likely changed since then). There's also the issue of northern vs. southern sky. reply cubefox 11 hours agorootparentprev> That's possible, and directed/shielded lighting is commercially available. Given the size of the site (over 3000 hectares), even lights purely pointed at the ground will still create large amounts of bounce lighting. The ground reflects light up in the sky. reply ungreased0675 6 hours agorootparentIt’s possible to go more restrictive than shielded lights. What if all outdoor lights must be turned off from 9pm to 5am? If the conditions were something like that, would the developers still want to build? reply II2II 5 hours agorootparentI suspect the real concern is there would be a push to relax the restrictions once the industrial facility has been built. Astronomical observatories have faced such problems in the past, to the point where research goals had to be fundamentally altered or where they ceased to be research facilities. That said, if the goal is to reduce the lighting to the point where it has no impact, one has to ask: what is the point of having lighting at all? I suppose lighting could be restricted to indoor use only, but most commercial operations will expect some outdoor lighting. reply justlikereddit 5 hours agorootparentprevHave you ever seen a heavy construction site involving land preparation? Like a new road being built? Dust everywhere. Backhoes with 360 light coverage that makes a lighthouse envious. Trucks, trucks everywhere! Floodlights! String lights! Crane lights! Temporary light poles! Service lights! Warning lights! A dictionary of lights! Can they work in the dark without producing so much dust? Can a tiger be a vegan? Also the development is part wind power, this alone causes wake turbulence. So even if regulation demanded complete blackout after sunset on the penalty to of death the observational quality is still permanently degraded. reply dylan604 14 hours agoprevIt’s not just industrial sites. My “local” (4 hours away) dark sky spot is constantly battling light pollution. There’s an industrial complex that’s made an agreement to turn their lights off at midnight. They’ve made deals with the county to replace the lighting to be dark sky friendly, but they still have private land owners that refuse to cooperate and replace their lighting. I have many images of the Milky Way with ranch lights dotting horizon. reply 8bitsrule 13 hours agoprevNot sure this would be affected: The Vera Rubin scope, which cost $600+ million, will see first light this July. It's capable of creating a map of the entire available sky every few days. Containing 40B objects, several times more than all previous sky surveys combined. Half of those images are already threatened by constellations of comm satellites. Another concern is spy satellite imaging. https://archive.is/RzCNI#selection-779.4-779.14 So what compels AES, a US power company, to build a facility there, in all the world ... which would pump out that much pollution? reply aragilar 12 hours agoparentNo, the Vera Rubin Observatory is on Cerro Pachón (https://en.wikipedia.org/wiki/Vera_C._Rubin_Observatory) rather than Cerro Paranal (https://en.wikipedia.org/wiki/Very_Large_Telescope), the maps on the right hand side show the difference (~6° latitude). Similarly I wouldn't expect https://en.wikipedia.org/wiki/La_Silla_Observatory to be affected, but https://en.wikipedia.org/wiki/Extremely_Large_Telescope would be I expect (noting that I haven't seen anything beyond the ESO press release). reply cwillu 10 hours agoprevhttps://www.aesandes.com/en/press-release/aes-andes-submits-... reply SiempreViernes 8 hours agoprevA local story about it: https://radio.uchile.cl/2025/01/11/proyecto-inna-la-iniciati... reply dheera 14 hours agoprevI did a bunch of astrophotography in the Atacama desert last year, it was an absolutely phenomenal place. There are a lot of celestial objects you cannot image from the northern hemisphere and there aren't many other places in the southern hemisphere with weather conditions that good (maybe Namibia but it doesn't have the altitude advantage). The only thing I wish is that some of the parks would be open after dark to shoot landscapes. Most of the parks closed before sunset, so I had to mostly image from roadsides, which was kind of sad. reply saddat 5 hours agoprevPair this with impact of mega constellations with 10k+ satellites , which not only destroy optical imaging, but also interfere with radio-astronomy reply markvdb 9 hours agoprevA cynic would read this as as \"I can't believe our (AES) luck. There's a good chance we can squeeze the Europeans for lots of money. We'll gladly share some of the proceeds with the new US president's cronies for having them do the haggling.\" reply SiempreViernes 8 hours agoparentYou want a real conspiracy theory? How about this: some rich Thirty Meters Telescope patron saw the peril the project has been in and set out to sabotage the ELT! Of course, the ELT is proper funded, so the best he can do is making it useless by ruining it's sky for a decade with construction dust and light. reply astrolx 2 hours agorootparentI like that conspiracy ! They should be careful about retribution if the TMT lands on the Canary Island, sombebody could conspire to put another industrial facility next to it reply yummybear 9 hours agoprevThe skies may be brightening, but it seems the world is turning darker. reply hackingonempty 17 hours agoprevIf you were wondering if there was any issue even less important to Americans than the lives of pedestrians and cyclists, it is dark skies. reply kortilla 14 hours agoparentDisagree. Or at least it’s a different set of people generally very supportive of dark skies. There are many dark sky communities in the southwest that are otherwise standard car centric unwalkable american towns. reply darthoctopus 17 hours agoparentprevwhy is this downvoted? the specific cities (notably in Arizona) that have taken deliberate action on this are exceptions proving the general rule that light pollution is demonstrably less of a policy concern even compared to the notorious American disdain for walkable infrastructure. reply WorkerBee28474 16 hours agorootparentThe telescopes are 8,000 miles south of America. Why does American policy matter? reply hnmias 4 hours agorootparentI know the crowd here (mostly from USA) hates this kind of comment, but as a SOUTH AMERICAN, can I point out the absurdity of this kind of sentence? Chile is a South American country, in the American continent, and is 8000 miles south of America somehow. I know the why's and the meaning intented, no need to explain. Wont stop pointing this out though, as it will always feel to me as a example of the general disregard USA has for its neighbours. reply broptimist 1 hour agorootparentDespite not asking for an explanation, I’ll give one anyway since you seem not to have resolved your grievance. “American” is the correct adjective in English to describe the United States’ people and government. There is simply no equivalent to the Spanish “estadounidense.” Furthermore, North America and South America are considered to be separate continents, and if you want to refer to them both together, you say “the Americas”, plural. https://en.m.wikipedia.org/wiki/Americas reply ggm 16 hours agorootparentprevBecause the goods made will be sold to American consumers directly and indirectly and are priced to reflect all kinds of costs including EPA compliance in domestic markets. European markets also demand European norms to labour and health and environment are met, even if tokenistically. To some it is a form of protectionism. It's also the \"why can't we make it here\" reasoning. If you tried to make it in the US it would be white anted out by lawfare. That's what happened to BHP when they proposed metals and minerals processing plants on the Californian coast. reply bryanlarsen 15 hours agorootparentAmmonia and hydrogen are essentially energy export mechanisms. They'll be exported to energy poor places, aka Asia, not America. they can and are made in America without fanfare. You wouldn't have states fighting to exlude green hydrogen or ammonia plants, you'd have states competing on how many subsidies they could give them. reply ggm 14 hours agorootparentArguably, very likely true. But the fertiliser (the other ammonia product, the one we do mostly now the others being somewhat futurological) will I am sure sell worldwide. I'm personally sceptical about the hydrogen economy I can't see it working. It's biggish in some Australian circles, both because of IPR around the processes and people in related fields looking at uses for surplus solar power. Twiggy Forrest was big in it, wanted the sun cable proposal to pivot over, its partly why the JV with Cannon Brookes fell apart. My comment was to the more general \"why can't we have nice things\" about industrial placement. I spent time in Culpeper and the number of \"no more Datacentre\" signs were amazing. Old folks who retired to the country don't want them build nearby. It's a large federal and private investment in tech services. And growing. reply a1j9o94 14 hours agorootparentprevIt's also an American company building the project. The cultural values of the US are relevant. reply otteromkram 11 hours agoparentprevI would say quiet. Every place I've moves to in recent years looks nice, but you can't enjoy it because passenger cars and trucks have gotten louder without restraint or consequence. This doesn't mean right next to a major freeway, either; half-a-mile (about a kilometer) or more away from most 4-lane roads isn't far enough. For an example, look up how many tickets in any given city have been issued for an improperly maintained exhaust system. Police only care about speeding tickets. So much so, that even if a noisy \"sports\" car is pulled over for speeding, they won't be issued a noise citation in concert. Why? ACAB. Cops probably drive around in noisy cars/trucks after work (and some jurisdictions have police cruisers with a throaty exhaust because of course they do), so ticketing those violations isn't in their own best interest. Anyway, noise is way more of an IDGAF issue for any city in the US. reply exe34 11 hours agoparentprevnext [4 more] [flagged] nichos 11 hours agorootparentI don't know of any Americans that advocate for shooting of school children. reply Dylan16807 9 hours agorootparentThey said it's low importance to a ton of people, not that those people want the opposite. reply exe34 9 hours agorootparentprevdid you miss the \"not\" in the sentence? reply seattle_spring 13 hours agoparentprevWhich is too bad, because it takes a special kind of heartless, empathy-lacking ghoul to disregard such things that make life on this Earth worth living to so many people. reply bongodongobob 11 hours agorootparentPretty fucked up to say that people that don't have dark skies even on their radar with everything going on right now are heartless and lacking empathy. It shows a gross misunderstanding of the average person today and really shows your lack of empathy. I shoot astro, I love it. I wish skies were darker. But I certainly don't blame my comrades for not giving two fucks about how the sky looks when they are asleep after working two jobs to pay rent. reply otteromkram 11 hours agorootparent\"...when they are asleep after working two jobs to pay rent.\" No one else sleeps or works, right? Plus, who knows why they work more than one job. Maybe they were \"too smart\" for school, found out later that they weren't, and now are grasping to close the gap due to hubris and ignorance early on in their life. No shame in making up for lost time/wages, but that's not our fault and we shouldn't have to constantly bend and bow in order to appease the LCD crowd. reply bburnett44 5 hours agorootparentThis is a pretty crazy comment reply bongodongobob 11 hours agorootparentprevIf you're surprised the night sky isn't top on people's minds today, you live in a very different world than most. reply senorrib 7 hours agoprevDefinitely a hard choice between an industrial complex generating thousands of jobs and a glorified camera. reply jazzyjackson 1 hour agoparentGee, mapping every object in the observable universe (and possibly saving us from catastrophic meteor strikes) or pumping out a few more tons of ammonia? Framing does an awful lot of work. reply frereubu 7 hours agoparentprevDifficult to tell the economic / geographic context from a short article like this, but they mention the possibility of relocating the project. If possible that's a win / win, no? Sounds like it may just be the case that the dark sky aspect of this wasn't taken into consideration. reply concordDance 7 hours agoprevHampering industry that will bring prosperity to thousands to avoid having to wait to do some specific types of astronomy until Starship is working doesn't seem like a good trade-off. reply WorkerBee28474 17 hours agoprevnext [7 more] [flagged] hombre_fatal 17 hours agoparentFrom TFA: > It includes constructing a port, ammonia and hydrogen production plants Ports and especially chemical plants are basically lightbulb arrays. reply WorkerBee28474 17 hours agorootparentThey're not going to build a port \"just 5 to 11 kilometers from telescopes\" (from TFA) when the telescopes are 15km from the ocean. A chemical plant wouldn't be inland either because it will want access to the port. reply ok_dad 16 hours agorootparentYou think a port and industrial plant that requires 3/4 of a gigawatt of electricity will be built within a limit of less than 4000m from the ocean port? Every port I’ve seen took at least a few kilometers of inland space. I also don’t think a few kilometers makes much of a difference to the light reduction, basically any light at all will harm the telescope. I am surprised at the “meh” response from the commenters here, they want to build an industrial plant in one of the best places for astronomy. Can’t the plant go elsewhere? The telescope cannot go elsewhere. reply returningfory2 15 hours agorootparentThe problem is that, remarkably, there’s always a reason not to build. A different site in Chile will probably have some obscure species of beetle, or rocks that someone has interpreted as an Indigenous site, or some minor highway that can’t handle the traffic, etc etc etc, so that we can’t build there either. reply ok_dad 14 hours agorootparentWell then I guess we’ve run out of land! reply kristjansson 12 hours agorootparentprevThe world is trade offs. Some are worse than others reply thereisnospork 17 hours agoprevBANANAs in action, can't even build a green energy facility in the literal middle of nowhere without complaints. reply culi 15 hours agoparentThis isn't just about getting rid of the last place on earth you can sometimes get a truly dark sky. This is about progress itself > Since its inauguration in 1999, Paranal Observatory, built and operated by the European Southern Observatory (ESO), has led to significant astronomy breakthroughs, such as the first image of an exoplanet and confirming the accelerated expansion of the Universe. The Nobel Prize in Physics in 2020 was awarded for research on the supermassive black hole at the centre of the Milky Way, in which Paranal telescopes were instrumental. The observatory is a key asset for astronomers worldwide, including those in Chile, which has seen its astronomical community grow substantially in the last decades. Additionally, the nearby Cerro Armazones hosts the construction of ESO’s Extremely Large Telescope (ELT), the world’s biggest telescope of its kind — a revolutionary facility that will dramatically change what we know about our Universe. reply modeless 14 hours agorootparentIt's not literally the last place on Earth with dark skies. It's just one place with dark skies where they built a telescope. This isn't about protecting the sky and it's not about \"progress\", it's about protecting an investment of money in a telescope. The price of launching giant telescopes to space is set to plummet in the next few years with Starship and New Glenn coming online. IMO we should be focusing on that rather than blocking development on Earth to preserve previous investments in ground based telescopes. reply SiempreViernes 8 hours agorootparentNo, but it's one of three places on Earth that have dark skies this good. The fact you don't know Paranal host many more than \"one telescope\" doesn't surprise me, as your are obviously very ignorant of modern astronomy. reply phinnaeus 12 hours agorootparentprevLaunching telescopes is not a viable alternative to ground based telescopes. They are completely different scales. We would need large scale orbital construction facilities or a space elevator to bridge that gap. We don’t need to develop every square inch of the planet to support humanity, we don’t take up that much space. reply SiempreViernes 9 hours agoparentprev5 km from infrastructure critical to Chilean science isn't really \"nowhere\". reply ok_dad 16 hours agoparentprevIt’s an industrial plant with an attached power plant, it’s not like families will be using this power. reply thereisnospork 16 hours agorootparentHow is it you think families get power, goods, and services? Ammonia makes fertilizer - this plant will help feed millions, dropping food costs. Even if the power this plant is generating won't go directly to families, it will be going into the things they eat and the things they buy in place of power they can use directly. reply saddat 5 hours agorootparentAgain, not much use for the locals at this elevation . That’s super dry area reply exe34 11 hours agorootparentprevis this the last place on earth to build that kind of industry? reply daedrdev 10 hours agorootparentI think its fair enough to not build it here, but everywhere there will be arguments made against all projects so it can get old fast reply fnordpiglet 16 hours agoparentprevIt’s nothing to do with the merits of the project itself but that it would destroy a singular planetary resource. reply fastball 15 hours agorootparentIt's not \"destroyed\". If a dire need for dark skies arises, you can always... turn the lights off. reply justlikereddit 5 hours agorootparentTurn the windmills off too because turbulence. Actually turn the entire facility off because again being a hotspot causes turbulence. Why don't we build an all night biker bar next door to your home? It won't cause you any problems like noise or nuisance because they cal always turn off the music or keep closed all night? reply watersb 11 hours agoprevOn another piece of the electromagnetic spectrum, the ALMA radio telescope is also in the Atacama desert, north east of Paranal. The government agreed to a radio quiet zone in the areas surrounding ALMA. But now there's Starlink and other satellite constellations coming on line at an unprecedented pace. reply Dylan16807 9 hours agoparentAre those satellites broadcasting while over those areas? I don't see the connection here. In fact it looks like there's extra effort to let them keep running without causing problems https://public.nrao.edu/news/astronomers-satellite-internet-... reply kortilla 14 hours agoprev [–] Headline is dramatic but misleading. Essentially the entire 7/10 of the planet in the ocean has skies as dark as this. Clarity significantly reduces the footprint, but there are massive chunks of mountain ranges untouched by human development in both hemispheres that would be just as clear as here. If clear skies are important enough to block a new development, they should just unlock some land in the Himalayas or Rockies to replace this observatory. reply gmueckl 13 hours agoparentThis spot in the Atacama desert isn't special for it's lack of light pollution alone. The sky is rarely, if ever covered in clouds or haze. And the temperature gradient in the air has a shape that prevents random atmospheric distortions that would make long term exposures blurry. This combination of properties is exceedingly rare on Earth. reply adriand 14 hours agoparentprev> they should just unlock some land in the Himalayas or Rockies to replace this observatory That \"just\" is sure doing a lot of work in this suggestion. reply Tepix 13 hours agoparentprevThis place has an elevation of 5000m and the air is super dry. reply seattle_spring 13 hours agoparentprevI recommend reading up on why these observatories and telescopes are where they are in the Atacama. It’s not just about the lack of light pollution, it’s a specific geography that “smoooths out” the air. Something about the high elevation prominence coming up directly from the coast creates a unique situation that allows for longer exposures, something that is less possible out in the open ocean. The only other comparable place are the high peaks of Hawaii, but these are mostly off limits due to native protections. Destroying an aspect of the dark skies in Chile will absolutely hurt astronomy. No, they would not just be able to move their operations out onto a different mountain range or into the open ocean. reply Oarch 5 hours agoparentprevSure. But building a stable platform out there? reply tw04 13 hours agoparentprevWho is paying for this move and all the requisite supporting infrastructure? You aren’t just dropping it from a helicopter and calling it a day. reply niccl 14 hours agoparentprevwho pays for moving the observatory? reply rad_gruchalski 11 hours agoparentprevWho’s “they”. reply exe34 11 hours agoparentprev [–] could you move the industry there? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The European Southern Observatory (ESO) is worried about a proposed industrial complex by AES Andes near its Paranal Observatory in Chile, which may cause light pollution affecting astronomical research. - The Paranal Observatory is vital for projects like the Extremely Large Telescope (ELT), emphasizing the importance of preserving the dark skies. - ESO is advocating for the relocation of the industrial project to safeguard the observatory's environment."
    ],
    "commentSummary": [
      "An industrial megaproject in Chile, led by AES Andes, aims to produce hydrogen, electricity, and ammonia using wind and solar power, potentially impacting the world's darkest and clearest skies.",
      "Critics express concern that the project, situated near a key telescope site, could hinder astronomical observations, sparking a debate on whether to relocate the project.",
      "The situation underscores the conflict between industrial development and the preservation of unique natural resources, with some advocating for economic benefits while others prioritize environmental conservation."
    ],
    "points": 212,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1736629866
  },
  {
    "id": 42674116,
    "title": "Bad Apple but it's 6,500 regexes that I search for in Vim",
    "originLink": "https://eieio.games/blog/bad-apple-with-regex-in-vim/",
    "originBody": "BAD APPLE BUT IT'S 6,500 REGEXES THAT I SEARCH FOR IN VIM Why should I leave vim to watch a video? Jan 10, 2025 Well the title of this post promised you Bad Apple in vim only using search queries. So here’s Bad Apple in vim, but the only thing that’s changing is the search query: Unfortunately this is only 120x90; my screen isn't big enough to size up more Let’s talk about how this works! Wait, sorry, what is Bad Apple Bad Apple is a visually compelling music video that folks enjoy embedding in surprising places. It’s a meme in the same way that running DOOM on a smartfridge is a meme. I’ve wanted to run Bad Apple somewhere since I saw this video purporting to run it on One Million Checkboxes. Getting the frames The first step here was pretty simple. I needed data for each frame of Bad Apple. Felixoofed had already done some of the work for me; I cloned their repo which gave me the video along with a suggestion of an ffmpeg command to turn the video into ~6,500 PNGs representing each frame. Then I wrote a little bit of Python code to turn each of those PNGs into a 2D array of 0s and 1s (where 1 represented a black pixel). The video was originally 480x360 - I shrunk it down to 120x90 after taking some measurements of my terminal and concluding that I couldn’t really go any bigger. from PIL import Image import numpy as np def process_image(path, target_width=120, target_height=90): img = Image.open(path) img = img.resize((target_width, target_height), Image.Resampling.LANCZOS) if img.mode != \"L\": img = img.convert(\"L\") pixels = np.array(img) binary_pixels = (pixels l /\\%23l Matches below a specific line (higher line number). \\%.l Matches at the cursor line. \\%.l Matches below the cursor line. vim searches can match on specific line numbers (and column numbers). And you can combine several of these searches together - for example \\%>5c\\%4l\\%5c\\%4l\\%12c\\%10l\\%<15l finds both our previous rectangle and the one between cols 12 and 25 and lines 10 and 15. So it’s easy to draw many rectangles to the screen with a single search. two squares from one search string! This transformed the problem - now the goal was to decompose each frame into a series of rectangles that we could search for! Frames into rectangles Our grid was 90x120 so we had ~10,000 pixels. This meant that a super naive approach would potentially look for thousands of different rectangles and generate a search string tens of thousands of characters long. Some basic testing showed me that while vim’s search was fast, search strings that long would kill my framerate. “Decompose this grid into the minimum number of rectangles required to fill it” seemed like the type of thing that would have a de-facto solution, so I looked for existing solutions. But I didn’t find much! There’s a 14 year old stack overflow question where the accepted answer is “it’s hard.” So I wrote something naive. The approach I took was: Detect all runs of “1s” in the first row Look at the next row, and find runs that overlap with the runs from the previous row “Merge” those runs into a rectangle if the area of the merged rectangle is greater than the area of either row alone (remember, the runs may not overlap fully) Keep going, making sure to merge new runs into old rectangles whenever possible The actual code is pretty long and I’m not sure it’s worth reading, but here it is if you’re curious: The code, which I'm not sure you really need to see So probably don’t read that. But the point is that it’s far from optimal 2. For example, the code never looks more than one row ahead, so it discards merges that are a bad idea now but would work out well based on following rows. 2 it was really fun to write though But I knew the code was naive when I wrote it - I just wanted to get something vaguely reasonable down to see how it’d perform. I generated search queries for each frame and wrote them to a file. And then I set up a simple vim animation test harness 3, and found that the performance was fine! Well, in some cases. 3 we’ll talk about how this works in a second Pathological cases My naive algorithm worked really well in some cases and totally fell over in others. Many of the search strings were 500 - 2000 characters long, but the search string that it generates for this image is over 10,000 characters 4! 4 Length of a search string is not a perfect proxy for performance, but I think it’s pretty good. Our search strings are just a bunch of patterns (of similar length) OR’d together. The longer a search string, the more patterns. And search time should scale with the number of patterns, since vim has to check each one before deciding that a search doesn’t match. That said I did no principled profiling here and may be totally wrong in some way! cool image tbh These long search strings dramatically reduced frames per second - we went from around ~40 FPS to the single digits. I hunted for optimizations 5. I came up with a bunch of ideas for tweaks, but wasn’t confident that any of them would help much with the bad cases. And I didn’t have the time to find a good general-purpose algorithm: I was working on this the night before weekly presentations at the Recurse Center and I wanted to present it the next day! 5 My favorite rabbit hole was trying to improve vim’s search performance instead of my regexs. My setup for playing the video had multiple buffers open and the searches were ran over each buffer. I thought if I could prevent that (or prevent highlighting in other buffers) that might help. But I couldn’t get that to work. So I pulled out one of my favorite tricks 6 - instead of trying to write one great algorithm, write several mediocre ones and use them all! 6 which I originaly learned many years ago from my friend Eliot, who is also the person who made One Million Checkboxes faster I wrote two more naive solutions: A version of my “build rects top to bottom” algorithm that went left to right instead A simple RLE that only looked at individual rows And then I updated my code to run each frame through all three algorithms and pick the shortest search pattern generated! This worked great. While each of these algorithms had pathological cases, they had very different pathological cases. So in (almost) all cases, at least one of them generated a search pattern that was good enough. RLE ended up most often generating the “best” solution - although when it’s bad it’s really bad (which is why I avoided using it to start). And my original approach was used the least often! oops. # Number of times each approach was picked original approach (top to bottom merging) - 1110 left to right merging - 2239 single-row RLE - 3300 Wait, so how do you actually run this inside vim Yes, right, that’s a good question. Here’s an image of the vim setup: it's all clear now right? The center window on the top is where we play the video. It’s a file that contains 90 lines of 120 spaces (remember that our images are 120x90); since we’re matching on rows and columns it doesn’t need to have any content. To its left and right are some empty buffers that are sized to center the image. The bottom window is our list of search patterns! All ~6,500 of them. To play the video we use a vim macro. If you haven’t seen vim macros, they’re a way to record a series of arbitrary keystrokes to be replayed later. They’re extremely powerful - since you do ~everything in vim via the keyboard, you can trivially record and replay any action. And if you set up your macro right (so that it ends with the cursor in the right spot for the next iteration) you can tell vim to run the macro many times in a row! The macro The macro is \"ay$:let @/=@a^M+. That is: \"a Operate over register a y$ Yank until the end of the line Vim has “named” registers that can hold text - ‘a’ now contains the current line : Enter command mode let @/=@a set the contents of register / to the contents of register a ^M execute the command (^M represents the enter key) To reference a register in command mode you use @ followed by the register name. Vim has special registers - the / register represents the current search. At this point vim is searching for the query that is on the current line + move to the start of the next line This is what I meant by “if you set up your macro right you can make it replayable” - we’re prepared to execute our macro again because the cursor is now on the start of the next line! This might look like nonsense to you - I’m pretty comfortable with macros because of the years that I’ve spent vim golfing. But it works! The most interesting optimization is let @/=@a - an alternative is to do something like /^Ra^M (begin a search, pastes the output of register a, enter), but this is a problem because it requires directly pasting a query that is potentially thousands of characters long. This causes the search window to expand to fit the query, which creates a flickering effect and reduces framerate. But now we can run 1500@q (assuming we recorded the macro into register q) to play the macro 1500 times - meaning that we’ll run through 1500 frames as quickly as we can. And that gives us this! not bad Wrapping up This was really fun! I built this in a single day, but if I wanted to spend more time on it I might make a few tweaks: I think it’d be more magical if I had gone the “create a well-structured file that I can craft traditional regexs over” route instead of using vim’s line/col search feature. You might (reasonably) quibble that these aren’t real regexs! I make no effort to keep a stable framerate, and the framerate definitely fluctuates a bit over the course of the video. But this was good enough for my purposes. And I think it’s neat that I’ve built most of a general-purpose solution for playing a video inside of vim using search queries. Someone suggested that I record a video of me running Bad Apple in vim and play that video in vim. So I guess I’ll get on that. Beyond that, I made this in my first week of a new batch at The Recurse Center, a place that offers something like a writers retreat but for programming. I presented it to folks there and got a lovely response. I really love Recurse, and if you love nonsense like this I bet you’d like it too. Consider applying!. Finally, the code is super messy but if you want to poke around you can see it here. And that’s all I’ve got. As always, I’ll be back with more nonsense soon.",
    "commentLink": "https://news.ycombinator.com/item?id=42674116",
    "commentBody": "Bad Apple but it's 6,500 regexes that I search for in Vim (eieio.games)199 points by vortex_ape 3 hours agohidepastfavorite30 comments adityaathalye 1 hour agoHah, trust nolen to 1,000x something :))) I have used similar tactics in the past, but separately and definitely not in one day! For the interested: - Bad Matrix (tput blocks to the terminal): https://www.evalapply.org/posts/bad-matrix/ - Animating Text Art in Javascript (print text into fixed grid, flipbook-style): https://www.evalapply.org/posts/animate-text-art-javascript/... - oxo (format and print tic-tac-toe board to terminal, so I can regex-match for win/loss/draw results): https://github.com/adityaathalye/oxo/blob/7681e75edaeec5aa1f... But, I mean, that Bad Apple takes the cake! (edit: add missing link) reply jordigh 1 hour agoprevThe tech demo that really made me fall in love with Bad Apple was getting it to run on the NES. https://somethingnerdy.com/downloads/ Here it is running from my Everdrive. https://inversethought.com/jordi/video/badapple.mp4 Yes, with full audio. It's about one gigabyte of data. On a system where the typical game size is no more than a couple hundred kilobytes, and your CPU only has three 8-bit registers for you to do any calculation with. reply rav 1 hour agoprevRegarding the Vim macro that ends by going to the next line to be \"replayable\": You can also use the following command to run the macro once per line: :%norm @q reply eieio 1 hour agoparentoh wow, TIL, I'm pretty surprised I didn't know this trick! back when I was vim golfing the normal solution was to make the macro recursive. So you'd record your macro, and you'd end it with '+@q' (move to next line and run the macro again). Then you run the macro once and it runs over every line. This ends up being really efficient in terms of keystrokes but in practice I think it's hard to think about and not very ergonomic, so I don't end up using it much. But it's a fun trick for golfing. reply rav 1 hour agorootparentThere's also the no-macro solution where you just use \":%norm [series of keystrokes]\" to run the given keystrokes on each line, but that comes with the added difficulty of not giving any visual feedback of what the keystrokes will do before you submit the entire line. One thing to keep in mind is that \":%norm\" will place the cursor at the start of each line, before any indentation, whereas the trick of ending the macro with \"+\" will place the cursor at the start of each line after the indentation. But this can be worked around with \":%norm ^@q\", using ^ to skip indentation before running macro q on each line. reply 3eb7988a1663 1 hour agoprevThe parallel candidate solution generator is such a good idea, but it usually takes me a long time to realize I do not need to make the uber algorithm. Just one-more-tweak, and I know that I can make this solution work in all cases! reply sltkr 1 hour agoprevFor the rectangle minimization problem: your problem seems to differ from the one discussed on StackOverflow in that the SO thread discusses partitioning into non-overlapping rectangles, while your Vim project allows overlap. I wouldn't be surprised if your problem turns out to be much easier to solve optimally. reply rav 1 hour agoparentActually, from an algorithmic standpoint it's the opposite: the minimum cover problem (where overlap is allowed) is NP-hard whereas the minimum partition problem (where overlap is NOT allowed) has polynomial-time algorithms. \"An Algorithm for Covering Polygons with Rectangles\" by Franzblau and Kleitman 1984: https://core.ac.uk/download/pdf/82333912.pdf However, that's of course just an academic tangent - the theoretical results don't necessarily imply that one problem is easier than the other when you're just getting something to work for an afternoon project. reply eieio 1 hour agoparentprevoh this is a really good point! You're totally right, I had completely skipped over the fact that the rectangles were allowed to overlap. I think I'm probably done with this project / I'm pretty happy with the solution as it stands, but I think you're right that this simplifies the problem considerably. Thanks! reply PaulHoule 1 hour agoprevThese were on sale last month https://us.govee.com/products/govee-curtain-lights and my understanding is that you can upload an animated GIF to it... I just added making a \"bad apple\" GIF for it to my Kanban board though I don't know how much memory the device has and how well I can get it to work. (Sometimes that part where Remmy Scarlet spreads her wings still makes chills go down my spine) reply manosyja 1 hour agoprevI remember watching the Soccer World Cup 2006 at work. I logged in my home server via ssh and could watch it in the terminal. Not enough bandwidth for something else. reply lupire 39 minutes agoprevAs the author admits, it's Vim but it's not regexes. It's \"searching\" for screen coordinates. It's drawing in Vim, but not pattern matching. reply perpetualchange 2 hours agoprevRoughly how long did that take? reply eieio 2 hours agoparentHi! I'm the author. Like jchw said, this was a single-day project (although I did the writeup for it the next day). I went from 0-prototype in one sitting; I think that was around four or five hours of work? Then I went home, had dinner, and spent maybe three hours optimizing and cleaning it up. edit: I should say, i have done a lot of dumb things like this and I'm pretty sure it would have been at least a week of work for me 2 years ago. \"making the computer do dumb stuff\" is a skill like any other! reply perpetualchange 2 hours agorootparentThanks for taking the time to respond, pretty impressive stuff! reply jchw 2 hours agoparentprevFrom the article: > I didn’t have the time to find a good general-purpose algorithm: I was working on this the night before weekly presentations at the Recurse Center and I wanted to present it the next day! ... > I built this in a single day No estimate of hours, though. reply perpetualchange 2 hours agorootparentMy browser was apparently bugged, and it didn't show the article the first time... I see it now and am going through it. Thanks for mentioning! :) reply 29athrowaway 1 hour agoprevThe people running Doom or Bad Apple in different unexpected ways are such champs. There are some really interesting ones, like running Doom on a pregnancy test. reply saagarjha 1 hour agoparentStrongly disagree on that one; it was basically Doom on some random microcontroller stuffed into a pregnancy test shell. reply jordigh 1 hour agorootparentThe pregnancy test was the greatest drama to ever hit the r/itrunsdoom community. reply codeguro 1 hour agoprevThis is pretty cool! I like the creativity. The games this is based on are pretty good too. Danmaku are hypnotic reply GZGavinZhao 2 hours agoprev... this is why we love bad apple! reply wistle 1 hour agoprev [9 more] [flagged] codeguro 1 hour agoparentTouhou is just a bullet hell game, relax. Being this upset over it is more a reflection where your mind is at rather than everyone else's. reply wistle 1 hour agorootparentTouhou is a \"lolicon\" game for pedophiles. Why pretend otherwise? reply fluoridation 21 minutes agorootparentCan you say what exactly it is that puts it in the genre of lolicon? reply codeguro 56 minutes agorootparentprevEither you can't distinguish bullet hell games from porn or you're projecting. reply saagarjha 1 hour agoparentprev [–] Seems better than using a literal Playboy centerfold. reply wistle 1 hour agorootparent [–] Both are objectionable. reply saagarjha 1 hour agorootparent [–] Ok, but one of them is literal pornography, and one is…clearly you actually know what the word \"hentai\" is given you have used it in your comment. What part of this is hentai? reply wistle 1 hour agorootparent [–] As I'm sure you know, this is \"lolicon\" material intended for pedophiles. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A \"Bad Apple\" video was recreated in the text editor Vim using search queries, with each of the 6,500 frames represented as a 120x90 grid of binary digits.",
      "Python was utilized to process the frames, and various algorithms were tested to optimize the search strings for better performance in Vim.",
      "The project was a creative experiment conducted at The Recurse Center, showcasing the use of Vim macros to play the video by cycling through search patterns."
    ],
    "commentSummary": [
      "A project using 6,500 regular expressions (regexes) in Vim to animate \"Bad Apple\" initiated a lively discussion, showcasing creative and technical prowess.",
      "Participants shared similar projects, such as running \"Bad Apple\" on a Nintendo Entertainment System (NES) and utilizing Vim macros, highlighting algorithmic challenges and tech demos.",
      "The conversation also explored the cultural impact of \"Bad Apple\" and included debates about the content of Touhou games, reflecting diverse opinions and interests."
    ],
    "points": 199,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1736694794
  },
  {
    "id": 42670132,
    "title": "Adobe Lightroom's AI Remove feature added a Bitcoin to bird in flight photo",
    "originLink": "https://bsky.app/profile/matthewraifman.bsky.social/post/3lfaqbygva22j",
    "originBody": "@matthewraifman.bsky.social on Bluesky/** * Minimum styles required to render splash. * * ALL OTHER STYLES BELONG IN `src/style.css` * * THIS NEEDS TO BE DUPLICATED IN `bskyweb/templates/base.html` */ @font-face { font-family: 'InterVariable'; src: url(\"/static/media/InterVariable.c504db5c06caaf7cdfba.woff2\") format('woff2'); font-weight: 300 1000; font-style: normal; font-display: swap; } @font-face { font-family: 'InterVariableItalic'; src: url(\"/static/media/InterVariable-Italic.01dcbad1bac635f9c9cd.woff2\") format('woff2'); font-weight: 300 1000; font-style: italic; font-display: swap; } html { background-color: white; } @media (prefers-color-scheme: dark) { html { background-color: black; } } html, body { margin: 0px; padding: 0px; font-family: InterVariable, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Liberation Sans', Helvetica, Arial, sans-serif; text-rendering: optimizeLegibility; /* Platform-specific reset */ -webkit-overflow-scrolling: touch; -webkit-text-size-adjust: 100%; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; -ms-overflow-style: scrollbar; font-synthesis-weight: none; } html, body, #root { display: flex; flex: 1 0 auto; min-height: 100%; width: 100%; } #splash { position: fixed; width: 100px; left: 50%; top: 50%; transform: translateX(-50%) translateY(-50%) translateY(-50px); } /** * We need these styles to prevent shifting due to scrollbar show/hide on * OSs that have them enabled by default. This also handles cases where the * screen wouldn't otherwise scroll, and therefore hide the scrollbar and * shift the content, by forcing the page to show a scrollbar. */ body { width: 100%; overflow-y: scroll; } JavaScript Required This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. Learn more about Bluesky at bsky.social and atproto.com.Post Matthew Raifman, PhD matthewraifman.bsky.social did:plc:3iedncza2wlxqkfj3ljsc2wb Adobe has officially jumped the shark. Their AI remove feature in lightroom just added a bitcoin to my gull bird in flight photo. A bitcoin!?! Image 1 is the raw file. Image 2 is the result of removing three circle highlights in the water. Image 3 is a 100% crop. Bitcoin. WTAF. #photographers 2025-01-08T17:55:09.751Z",
    "commentLink": "https://news.ycombinator.com/item?id=42670132",
    "commentBody": "Adobe Lightroom's AI Remove feature added a Bitcoin to bird in flight photo (bsky.app)193 points by danso 18 hours agohidepastfavorite103 comments pclmulqdq 17 hours agoI have to say that I think the photo without the Ligthroom processing actually looks better. The second one hasn't just added a bitcoin, it has also added the \"AI shimmer\" that seems to be a part of a lot of generated images. I can't put my finger on exactly what the characteristic is, but my first instinct (separate from the bitcoin, which was hard to find) is \"that's an AI picture.\" Someone should just spend some time in the blur tool if they don't like those glints. reply serviceberry 17 hours agoparentI don't think there's any AI-fication going on in that photo. The modified version has a more compressed tone curve to bring out more detail, along with jacked up saturation (especially evident for water). This is similar to what most cell phones do by default to make photos look more pleasing. I do agree that the original looks better, but the author of the post clearly prefers the modified version. reply strogonoff 16 hours agorootparentClipped highlights in digital photography are simply impossible to eliminate in post-processing without conjuring nonexistent information. Even if you shoot raw. Different raw processors use different tricks, color propagation and such, but naturally they are best used when highlights are really small. I would not be surprised if tools like Lightroom invoke ML at the first hint of clipping (because why not, if you all you have is a hammer…). Pro tip: Digital sensors are much less forgiving than negative film when it comes to exposing highlights. With a bit of foresight they are best tackled at shooting time. Highlights from water/glass reflections are tamed by a fairly cheap polarizing filter, and if you shoot raw you should do the opposite of negative film and always underexpose a scene with bright highlights (especially if highlights are large or are in your subject of interest). Let it be dark, you will have more noise, but noise is manageable without having to invent what doesn’t exist in the parts that are the most noticeable to human eye. reply serviceberry 16 hours agorootparent> Clipped highlights in digital photography are simply impossible to eliminate in post-processing without conjuring nonexistent information. Even if you shoot raw. Huh? This used to be true twenty years ago, but modern sensors in prosumer cameras capture a lot more dynamic range than can be displayed on the screen or conveyed in a standard JPEG. If you shoot raw, you absolutely have the information needed to rescue nominally clipped highlights. You get 2-4 stops of latitude without any real effort. The problem here is different. If the rest of the scene is exposed correctly, you have to choose one or another. Overexposed highlights or underexposed subject. The workaround is to use tone mapping or local contrast tricks, but these easily give your photos a weird \"HDR\" look. reply strogonoff 15 hours agorootparentYou may be confused on at least two counts: — If you think there is “clipping” and the somehow different “nominal clipping”. Clipping is binary. Either you camera’s pixel clipped (i.e., accumulated enough photons to get fully saturated, and therefore offers no useful information for debayering), or it did not. If it clipped, then you lost colour data in that part of the image. If you lost colour data, and you want a colour photo, then in almost all cases one way or another you will have to rescue it by conjuring information up for the photo to look authentic and aesthetically good. (A lot of raw development software does it for you by default.) If the clipped bit is small, it is easy to do subtly. If it is big, which is a real danger with sun reflection bokeh, then… whoopsie. — If you think modern sensors are safe from clipping. This applies to any digital camera, from consumer to top professional models. Sensor bit depth is not even remotely enough to expose a scene with extreme highlights (e.g. sun or its reflections) without taking special measures regarding exposure at capture time. If you don’t tame highlights with a polarizing filter, you must dramatically underexpose the scene or they will clip. reply AlotOfReading 15 hours agorootparentThey specifically clarified that they were talking about the JPEG/picture on screen, which has less dynamic range than the sensor and can clip without losing pixel information. reply strogonoff 15 hours agorootparentWe are specifically discussing scene-referred image data—the raw pixel values obtained from a digital camera sensor. That is where unrecoverable information loss occurs at capture time. Parsing that data is when LR et al. engage various highlight reconstruction algorithms that fill in the missing information. My point is that a lot of that loss is often avoidable at capture time. reply DrewADesign 16 hours agorootparentprevnah-- even in the supplied jpg the histogram shows they're not clipping the highlights, and if you crank down the brightness, you can see the detail. reply strogonoff 15 hours agorootparentYou may be confusing display-referred and scene-referred image data. 1. Clipping occurs in the scene-referred raw camera sensor pixel data at capture time. 2. Raw processing software fills in the highlight by supplying the missing colour information. Often this happens by default. 3. When you obtain a display-referred JPEG, the missing information for clipped highlights was already supplied. Tone curves and other adjustments apply already after highlight reconstruction, making the final result look more organic. In other words, with modern digital photography processing workflows you will never see any clipping on a histogram for the final JPEG (unless either something went really wrong or it was intentionally done for a look), and it is a poor foundation to make assumptions about what clipping had or had not occurred at capture stage. What you can safely assume, however, is that a normally exposed digital photo featuring any sun reflections from water or glass will almost certainly have blown highlights. reply pclmulqdq 15 hours agorootparentWithout the raw image, we won't really know whether there was clipping. However, it is clear that the poster here wanted to remove the highlights and was okay with introducing some information that wasn't in the photo to do it, hence the use of an AI filter. That means that there is no issue with using the clone stamp or the paintbrush tool. reply strogonoff 15 hours agorootparentThis sounds about right. However, regarding your first sentence, I’d be a bit less cautious: sun reflections from water on a clear day guarantee clipping unless a polarizing filter is used or the scene is dramatically underexposed, and based on the general look of the photo (even knowing it’s after processing) my intuition suggests me that neither took place. Hence my original advice: don’t leave it up to ML when a little forethought can do wonders in terms of preserving information. reply DrewADesign 13 hours agorootparentprevWhat are you talking about? I see clipped histograms come out of cameras all the time. I’ve been working with digital images and digital photo manipulation for thirty years. Regardless, it’s not generative AI making up details. The differentiated pixels are there in the camera jpeg even if traditional image processing techniques have to be used to make them visible onscreen. The complete structure of the feathers that isn’t visible in the camera jpeg, for example, is plainly visible just modifying the contrast. reply strogonoff 10 hours agorootparentSubject domain knowledge mismatch appears to be too strong for further constructive discussion. reply pclmulqdq 16 hours agorootparentprevOh, yeah, compression of the dynamic range combined with increase brightness makes sense. It's not exactly just stable diffusion that produces that look, but also things like ML filters, etc. reply ruraljuror 17 hours agoparentprevThis reminds me of the soap-opera effect[0] on modern tvs. I have difficulty watching a movie on someone’s tv with it enabled, but they don’t even seem to notice. 0: https://en.wikipedia.org/wiki/Soap_opera_effect reply johnnyanmac 16 hours agorootparentA truly bizarre effect. One of the first times in my life I was ever thinking \"wait, this looks TOO smooth. it's weird\". As if my eyes just instinctively knew there were fake frames (before I understood the concept of \"frames\"). reply samsartor 17 hours agoparentprevThat could be the VAE? The \"latent\" part of latent diffusion models is surprisingly lossy. And however much the image is getting inpainted, the entire thing gets encoded and decoded. Edit: I'll note some new models (SD3 and Flux) have a wider latent dim and seem to suffer from this problem less. AI generated images are also biased strongly towards medium lightness. The photographer's adjusting of the tone curve may simply give it that \"look\". reply AuryGlenz 2 hours agorootparentYou absolutely do not need to encode and decode the whole image, even in ComfyUI. All you need to do is composite the changed areas back in the original photo. There are nodes for that and I’m sure that’s what Adobe does as well, if they even encode in the first place. These tools don’t really work quite like inpainting. There’s no denoise value - it’s all or nothing. I’ve used Photoshop’s generative fill many times on singular images and there’s no loss on the ungenerated parts. reply Culonavirus 17 hours agoprevIf you zoom in and squint your eyes, it does look like some kind of shiny coin. What I'd like to know though... is how is the model so bad that when you tell it to \"remove this artifact\" ... instead of it looking at the surroundings and painting over with some DoF-ed out ocean... it slaps an even more distinct artifact in there? Makes no sense. reply samsartor 17 hours agoparentA lot of current inpainting models have quite a lot of \"signal leak\". They're more for covering stuff vs removing it entirely. Ironically, some older SD1/2-era models work a lot better for complete removal. reply AuryGlenz 2 hours agoparentprevI mean, this is notable because it screwed up. It usually does a pretty good job. Usually. In this case there are better tools for the job anyways. Generative fill shines when it’s over something that’d be hard to paint back in - out of focus water isn’t that. reply jsheard 17 hours agoprevApparently this isn't an isolated incident: https://www.reddit.com/r/photoshop/comments/1e5nyt7/generati... reply ripped_britches 17 hours agoprev“Large company ships half baked product” has gotta be the least interesting story to read reply slg 16 hours agoparentSure, if you view this as an isolated incident. But I think of it more as the latest example of the larger trend of how the industry has gone mad actively making their products worse with half-baked AI features. That is a more interesting story. reply DrewADesign 16 hours agorootparentAnd this is the closest thing the professional imaging world has to a broadly-available tool designed for high-end professional use cases. It's barely consistently good enough for throwaway blog headers in its current state for large edits, and for small edits it's exactly 0 percent better than the removal tools they added 20 years ago. Adobe better start giving some love to its professional users because the alternatives are getting better, and their ecosystem is getting worse. It's like they're trying to put themselves in a position where they're competing with Canva and Instagram rather than Affinity, Procreate, Rebelle, etc. If it's not clear to them yet that they're not right-around-the-corner from having their AI tools be a drop-in replacement for their regular stuff, they're gonna have a bad time. reply thegeomaster 16 hours agorootparentprevIs it actively worse, though? My impression is that all of the other, classical-based in-painting methods are still alive and well in Adobe products. And I think their in-painting works well, when it does work. To me, this honestly sounds like an improvement, especially in a creative tool like Lightroom or Photoshop --- the artist has more options to achieve their vision; it's usually understood to be up to the artist to use the tools appropriately. reply ryandrake 16 hours agorootparentI'm not an artist or an Adobe customer, but when I see products adding half-baked or defective features, it tarnishes their brand and would definitely make me re-consider trusting their product. Especially for professional use, and regardless of whether the rest of the product still works fine. It's a general indicator of carelessness and tolerance of quality problems. reply DrewADesign 15 hours agorootparentprevUnfortunately, the Adobe ecosystem on a whole is like OS-level complex, and they're pretty much ignoring anything that isn't directly related to generative AI, and that stuff is only consistently good for the lowest-level use cases. Miles more useful than Comfy, etc. for most artists and designers, but not close for people that need to do more skillful work. The value of Adobe as an ecosystem was their constant upgrading and keeping up with the times, and now, if it's not some cockamamie generative AI feature, it's going nowhere. They're even worse with bug fixes than they were before. reply johnnyanmac 16 hours agoparentprevTHe fact that we call it least interesting shows exactly how interesting it is that we just accept that companies are expected to ship broken slop. reply ttoinou 17 hours agoprevI dont see where in the picture it is zoomed from to see the bitcoin reply jxi 16 hours agoparentYou have to click into it as it's not visible from the preview. reply crooked-v 17 hours agoparentprevBottom left. reply permo-w 17 hours agoparentprevit's in the second picture not the first reply uberman 17 hours agoparentprevDown from the tip of the birds right wing near the very bottom of the image. reply jdoliner 17 hours agoprevIt's telling you what it's mining in the background with those extra gpu cycles. reply gedy 17 hours agoprevHeaven forbid your picture has a woman in it somewhere though, Adobe's AI will refuse to fill half the time.. I've taken to censoring out blocks of image with black squares of it has any body parts showing (still clothed), fill, copy, then undo the censoring. It's pretty ridiculous for a paid subscription. reply kyriakos 13 hours agoparentFor a paid product even if the content explicitly contained nudity or depicted sexual activity it should had still been allowed as they are valid cases that Lightroom and Photoshop could be used. The censorship in AI is stupid, babysitting users should not be part of the tool's responsibility. Its like banning kitchen knives to keep people from using them for violence. reply missing-acumen 17 hours agoprevQuestion to people knowing adobe lightroom, could this feature be compromised? Is this just doing API calls to some remote thing? reply nshireman 16 hours agoparentLightroom has a local Heal/Remove feature, and at least with LR Classic you have to tick a box for the AI remove, which processes it on Adobe servers. As for whether it can be compromised... Probably? It sends all or some of your photo to a remove server, so that can certainly be taken. reply missing-acumen 13 hours agorootparentI mean, having the model behave this way looks too easy and I guess that adobe does qc on the features it releases, so I'm not sure to see an alternative explanation - or adobe's qc is poor/inexistent. reply wmf 16 hours agoparentprevI'm not sure what you mean by compromised but I'm pretty sure Adobe Firefly AI features are server-based. These features are too good to be done locally. reply jsheard 16 hours agorootparentPlus even if it could be done locally, doing it server-side has the side benefit (for Adobe) of making it trivial to prevent pirates from ever being able to use those features. reply missing-acumen 14 hours agorootparentprevBy compromised I mean something like someone having access to adobe's servers where this is running and uploading troll models or toying with the model's responses reply IAmGraydon 15 hours agoprevIt’s almost like integrating a poorly understood black box with your software is a bad idea. reply benoau 15 hours agoprevThey'll just add a disclaimer somewhere. reply 486sx33 17 hours agoprevSo HN, any theories on how this happened ? reply jsheard 17 hours agoparentThey used a circular mask and the model overfitted on Bitcoins as likely examples of circles? Adobe's models are only trained on their stock image library and they have a whopping 646,136 Bitcoin images. https://stock.adobe.com/search?k=Bitcoin reply mouse_ 17 hours agorootparentthat seems like too many reply jsheard 17 hours agorootparentCorrection, they have nearly a million Bitcoin images once you unfilter the ones tagged as AI generated, which are hidden by default. I assume they don't train their models on those though. reply stefan_ 17 hours agorootparentprevAll of them AI slop of course. They train on this? I guess it's garbage in, garbage out. reply Gigachad 16 hours agoparentprevSeems like they used some AI tool to remove speckles from an image. The tool has to generate a likely replacement. And one of the speckles looked a bit like a coin. reply SushiHippie 7 hours agorootparentYour explanation could make sense for TFA, but definitely not in the case below https://www.reddit.com/r/photoshop/comments/1e5nyt7/generati... reply lucb1e 15 hours agorootparentprevThat's the bit that puzzles me the most though: you want that bit gone, so why would it fill in what that shimmer looks like? If there's an airplane in the sky that you want gone from a medieval movie frame, so you select the airplane and select \"remove\", surely it doesn't fill in an airplane because that's the closest match for what's being selected? I must be missing something obvious but I don't see it mentioned in the submission or comments, or perhaps I'm not making the connection reply Gigachad 15 hours agorootparentWhat does gone mean though? You aren’t just drawing a black or transparent spot underneath what you want removed. You’re filling it in with the most likely background. Which is not a trivial operation. It’s an AI generative fill which is obviously unpredictable. You’d expect it to generate ocean but it drew a bitcoin. That’s just how AI generative fill works. You keep running it until it looks how you want. reply Lockal 6 hours agoparentprevMaybe cryptobros with zero skills and no talent created too much images copy-pasting the same bitcoin image over everything, which caused poisoning of training data. I also expect some bug involved, where user selects a circular mask, but outer edge is slightly blurred (feathered), which, when multiplied with white background, gives a light circle-shaped contour. After that Photoshop fills not a \"hole in the sky\", but a \"light circle-shaped object\" in the sky. reply rendaw 9 hours agoprevSorry, tangent, but does anyone remember the AI zoom in some old phone camera that was hallucinating a celebrity's face? It was much before the moon zoom hallucinations. reply cfreksen 4 hours agoparentI think you are referring to this: https://petapixel.com/2020/08/17/gigapixel-ai-accidentally-a... Discussed 4 years ago: https://news.ycombinator.com/item?id=24196650 reply labster 15 hours agoprevI read through Google News without logging in, and ever since that thing happened in November, it’s been flooded with crypto stories decorated with AI art. It totally makes sense to me that BTC would appear in this photo, since crypto bros seem to be the ones using AI images the most. reply sandspar 17 hours agoprevAny reason why \"AI made a flub\" is still newsworthy? We know it does this. reply ggm 17 hours agoparentBecause of people posting on the singularity and AGI prospects: we need to keep the feed of counter examples active. After all, this is training data too. reply undersuit 15 hours agorootparentGotta find out the vulnerabilities of the paperclip maximizer, disguising ourselves as a crypto currency could be our only hope. reply TZubiri 17 hours agorootparentprevMaybe AI is using bitcoin to transact in the black market and this is evidence of it reply ggm 17 hours agorootparentYou only had to put \"with drones\" and you had the trifecta. No wait: quantum drones. reply rsynnott 7 hours agoparentprevBecause this is Adobe Lightroom, which is meant to be, like, a real product that is used for real things. reply uoaei 16 hours agoparentprevBecause many don't know how fallible and useless these unnecessary products are! reply asdf6969 17 hours agoparentprevIt causes a lot of emotions which is good for attention online. 1. It makes people feel good because bad AI can’t take jobs away 2. It makes people feel bad because it’s further enshittification of experiences that are already getting worse reply Zinkay 15 hours agoprevHacking01001(@)0010 reply dcreater 17 hours agoprevAI is starting to backmask. Even it recognizes Bitcoin as the way forward reply jdiff 16 hours agoparentOr AI is falling into the same hype event horizon. reply geuis 17 hours agoprev [–] Ok I'm getting tired of this habit of deep linking to apps. The apps either override existing web pages featuring the same content, or there's a web redirect to an app I might have installed but am not signed into. So in multiple cases, it's difficult to just read whatever the information is. Web pages are universally accessible. Everyone has a browser on their device of choice. The web is 35 years old. Access to information is a solved problem. Guess I'm just complaining. There are just so many links being submitted to HN these days that require accounts or specific apps installed. Feels like there should be a rule around this. reply hipadev23 17 hours agoparentThe link works fine for me? Not logged in nor do I have bluesky. Loads in chrome, I can see the 3 images, and quite a few comments beneath their post reply isatty 17 hours agoparentprevWhat is being deeplinked? I was able to read the page and click on the pictures all in my browser (iOS safari). reply hombre_fatal 17 hours agorootparentThey mean how the phone lets apps handle browser urls instead of just opening the url in the browser. reply ludwik 16 hours agorootparentThat’s not deep linking (i.e., something set up by the person posting the link). It’s simply the specific device being configured to handle regular web links differently. reply geuis 16 hours agorootparentI know what you're referring to. However there's no ability on iOS to control this behavior. reply jdiff 16 hours agorootparentThere is but it's not intuitive. Long press on a link and you can open it without it being sent away and apparently this sticks. reply dillydogg 17 hours agoparentprevI think this must be a setting on your device to open links in the app. reply 0x38B 16 hours agoparentprevIf you’re on iOS, long-pressing on the bsky.app link in Safari and choosing “open” will open the link in Safari and remember your choice; the same works for YouTube links, etc. reply dbetteridge 17 hours agoparentprevNah totally agreed, pet peeve of mine is trying to open a link in my browser to a website and my phone decides it knows better and insists on redirecting me to \"The App\" (which I've never installed and don't want, so it sends me to download it) reply esperent 17 hours agorootparentI don't have the bsky app, nor am I logged in on my phone's browser, and this opened fine for me. No app redirects. reply jeffbee 17 hours agoparentprevWhat are you even talking about? It is literally a website. reply geuis 17 hours agorootparentI have the Bluesky app installed but I'm not logged in. So the website link is also being treated as an app deep link. It opens the app instead of just opening the website. I don't have a lot of familiarity with app deep links, but my understanding is that originally deep links required a special non-domain to be registered that was separate from a normal web url. Somewhere that's changed over the years and now regular web links like this default to opening in apps if the user has them installed. reply mr_mitm 16 hours agorootparentThere is no such thing as a deep link. If you don't like how your client device handles links, re-configure it or, if that's not possible, use a different device. The problem here is on your end. reply ludwik 17 hours agorootparentprevBut that's not OP deep linking to an app - it's just a normal website URL. It's you having your user agent configured to launch an app for this particular domain. Nothing that OP can do about that. reply lucb1e 15 hours agorootparentprev> my understanding is that originally deep links required a special non-domain to be registered that was separate from a normal web url. You might be thinking of protocol handlers, like oacon:// to open something in software installed to handle that (in this case, launching openarena to connect directly from a dpmaster mirror with such application links included). I don't think they were called deep links back then, just different protocols, like http in http://example.org is a protocol that your browser is configured to handle and ftp:// used to be as well These are still in relatively common use today, but on mobile devices it has become the norm to hijack specific domain names or even a path (e.g. F-Droid will try to handle repositories on third-party domains for you by trying to hook¹ any URL that contains */fdroid/repo/* -- so far, this has always been useful to me, but I can see the flip side). This link hijacking is often a pain for me as anyone linking to any Google product will make my phone try to open some Play Services component, which is largely not functional. I can't get rid of the system component (e.g. replace it with microG) without installing a custom ROM, which I can't do without getting rid of half the device's special features (no point having this phone then), but I also don't want it pinging back to the mothership so... a pain it shall be As for your problem, reset the app's settings and it'll re-prompt you the next time you click one of these links in which app these links should open. It should do that any time there is (newly) more than one app that can handle any given URL ¹ https://github.com/f-droid/fdroidclient/blob/be028d71c2a25b9... reply lynndotpy 17 hours agorootparentprevIs your device running iOS or iPadOS? This is default and non-configurable behavior. Even github.com will open in the GitHub app if you have it installed. I understand the confusion (and frustration), but this is just a normal URL. The .app is just an ordinary TLD (but one of the newer ones.) reply dunham 15 hours agorootparentprevOn iOS the app can ask to take over certain https urls. The web site also needs to have a special file to grant this access. Bluesky is choosing to do this to you. I used this for a login / reset flow about a decade ago, so it's been around for a while. There also is the ability to register for a URL \"scheme\" (the bit that replaces \"https\"), which I believe is what you're thinking of and it does predate the https thing, but both have been around for a while. I'm guessing companies have just gotten more aggressive about using the https one. Edit: and yes it is annoying, I've uninstalled the GitHub app because of this. reply HelloImSteven 16 hours agorootparentprevOn Apple devices at least, this is a feature of Universal Links, which are generally more secure than deeplinks for various reasons [1]. Not sure you can disable it completely, but in some cases you can override it by long-pressing the link and choosing to open it in the browser. [1] https://developer.apple.com/documentation/xcode/defining-a-c... reply Gigachad 16 hours agorootparentprevRemove the app then? Having an app installed but logged out is such a rare case, and one that’s easily solved. reply jeffbee 17 hours agorootparentprevI use Bluesky all the time and I have never installed the app on any platform. I suggest getting rid of it, if you're such a fan of the web. reply deadbabe 17 hours agoparentprevThe nefarious thing is some people will tell you there’s nothing wrong with the link and gaslight you into thinking you’re crazy – because they don’t have the Bluesky app installed. reply mr_mitm 17 hours agorootparentWhat is it that is supposed to be wrong with the link? .app is a regular TLD, the host name resolves to a regular IP address and there is a regular web server returning regular HTML. reply lynndotpy 17 hours agorootparentprevOn iOS and iPadOS, even github.com links will do this with the GitHub app installed. We're complaining about a completely normal URL that links to a normal webpage. This is very, very silly. reply uoaei 16 hours agorootparentWhat is the use of a Github app on iPhone or iPad? I guess I could see it useful for reviewing pull requests and making comments if it's urgent (it's not) or your main dev device is out of reach (it can wait)... reply lynndotpy 16 hours agorootparentI wanted to have a widget of my commit graph; it's nice to see the little squares filled up. When I clicked a github link and my screen started bouncing and sliding around, I was very displeased. reply nick__m 15 hours agorootparentprevThere is nothing wrong with the link. The only thing wrong is your poor understanding of your user agent behavior ! reply Aurornis 17 hours agorootparentprev> and gaslight you I have the Bluesky app installed and did not see a pop-up. It’s not “gaslighting”. It’s people having different experiences due to different OSes and settings. It’s amazing how meaningless the word gaslighting has become. reply jeffbee 17 hours agorootparentprevYou're saying the OP has configured their user agent to launch an app for this URL and somehow that's me gaslighting? reply danparsonson 17 hours agorootparentI imagine that's the joke they're making, yes reply benreesman 16 hours agoparentprev [–] I don’t know why you’re getting dogpiled over this, it’s a terrible experience of creeping non-consensual computation. I had the official bsky app installed, but I ended up with Graysky at some point and just forgot about it, and somewhere along the way my browser vendor (Apple) and the app vendor (bsky) decided that I would probably tolerate this horseshit. Reddit pulls the same shit but that’s extra special because it’s broken, so it goes to the App Store screen for Reddit and you just can’t load it at all without a laptop. If you’re one of the people arguing this is cool you’re ugly and stupid like people who disagree with Linus. reply jdiff 15 hours agorootparentYour device is misconfigured. That's not HN's fault, what has been posted is a completely ordinary link that would function in a perfectly ordinary way if you had configured your device properly. This wouldn't even be CLOSED WONTFIX, there is literally nothing that anyone but you could do to fix this. reply never_inline 3 hours agorootparentprev> non consensual computation Gonna steal this one. reply yarg 16 hours agorootparentprev [–] Because the problem is perfectly avoidable with very little technical understanding, and this is Hacker News. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Adobe Lightroom's AI Remove feature mistakenly inserted a Bitcoin image into a bird photo, prompting discussions on AI's influence in photography. - Users criticized the AI's \"shimmer\" effect, preferring the original photo, which underscores challenges in digital photography, such as managing clipped highlights and AI tool limitations. - This incident highlights broader concerns regarding AI's role in creative tools and the quality of AI-generated content."
    ],
    "points": 193,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1736642018
  },
  {
    "id": 42672675,
    "title": "Mullenweg Shuts Down WordPress Sustainability Team, Igniting Backlash",
    "originLink": "https://www.therepository.email/mullenweg-shuts-down-wordpress-sustainability-team-igniting-backlash",
    "originBody": "News, Project Mullenweg Shuts Down WordPress Sustainability Team, Igniting Backlash Veteran tech journalist Kara Swisher has described Matt Mullenweg’s move to shut down WordPress’s sustainability team as “bizarrely heinous behavior.” Rae Morey · January 10, 2025 Members of the fledgling WordPress Sustainability Team have been left reeling after WordPress co-founder Matt Mullenweg abruptly dissolved the team this week—an action prominent tech journalist Kara Swisher has described as “bizarrely heinous behavior.” The disbandment happened after team rep Thijs Buijs announced in Making WordPress Slack on Wednesday that he was stepping down from his role, citing a Reddit thread Mullenweg created on Christmas Eve asking for suggestions to create WordPress drama in 2025. Buijs had hoped 2025 would bring more positivity to the WordPress community following the “WordPress drama” of 2024, referring to Mullenweg’s public feud with WP Engine. But said the Reddit thread was more proof that the community needed a change in leadership. “These acts of a single person are so counterproductive to the economic, social, and ecological sustainability and longevity of the project. Personally, it’s not sustainable for me to invest any more energy into this project under the current leadership,” Buijs posted. Ninety minutes later, Mullenweg joined the #sustainability channel, and 4 minutes later, he announced he was shutting down the team. “Today I learned that we have a sustainability team,” Mullenweg said. “Thank you @Thijs Buijs for your effort in this area, looking at results of the team so far, and the ROI of time invested, it’s probably a good time to officially dissolve the team entirely.” “It doesn’t seem like creating a team around this was able to further any of its goals, so we should probably try a different approach, or consider whether it’s salient for us to be involved at all. “(For example, is it worth talking about the climate impact of WordPress, or instead should we just have really great performance metrics and try to optimize our code as much as possible, and focus on that efficiency.)” Mullenweg’s announcement prompted outrage on social media and in Slack channels, with many in the WordPress and wider tech communities criticizing Mullenweg’s hypocrisy, pointing out that it was his idea to create the sustainability team. Critics also slammed his apparent disregard for the self-sponsored team reps and their contributions to WordPress. In a scathing post on Threads, veteran tech journalist Kara Swisher posted a screenshot of Mullenweg’s announcement in Slack and called him a “stone cold asshole.” “The @wordpress platform was where @mossbergwalt and I started All Things D and later Recode, because it was so good and @photomatt was always very helpful. But what a pathetic turn for him into a stone cold asshole,” Swisher said. “I take no pleasure in saying this, but it’s true and I am so sorry for the staff there. Oh yeah, Matt, please don’t contact me offline trying to explain your current bizarrely heinous behavior. In other words: Over and out.” The #sustainability channel in Making WordPress Slack was created during the Q&A at WordCamp Europe 2022 after UX/UI designer Nora Ferreirós raised concerns about sustainability in WordPress with Matt Mullenweg and former WordPress Executive Director Josepha Haden Chomphosy. Ferreirós was one of the original proponents behind oursustainablewp.org, a website she launched in 2022 together with Hannah Smith (Director of Operations at the Green Web Foundation), Csaba Varszegi (Sustainable Web Designer at LittleBigThings), Nahuai Badiola (co-founder of OsomPress), and Buijs (owner of Yellowlime). The website aimed to get people thinking about what sustainability could look like for WordPress. The now-disbanded WordPress Sustainability Team’s reps Thijs Buijs, Nora Ferreirós, Nahuai Badiola, and Csaba Varszegi at Contributor Day at WordCamp Europe 2024. Ferreirós, Varszegi, Badiola, and Buijs became the sustainability team’s four founding reps when the team became an official Make WordPress team at WordCamp Europe 2023. The team’s primary objective was to embed sustainable practices into the WordPress community and its processes to ensure WordPress’s social, economic, and environmental longevity. Badiola said he was still processing the teams’ shock disbandment and was disappointed Mullenweg had been unaware of their work over the past 18 months. “The team was formed mainly by self-sponsored people (including the team reps) so the pace of task completion was much slower than we would have liked it to be,” Badiola said. “We were doing the best we could with what we had.” Varszegi added: “We were a young team still trying to find our way in a system that was unknown for all of us. Also, the team had several new, complicated responsibilities, hard to tackle on the short term. With a growing contributor base, the team could have achieved a lot in my opinion.” Varszegi said the team’s most impactful contribution to WordPress was the publication of the Sustainable Events Handbook, designed to help WordCamp organizers improve the sustainability of their events. The team was also developing a plugin to help site owners estimate the carbon footprint of their website. According to the sustainability team’s roadmap, published last September, contributors were also exploring ways to make WordPress financially sustainable, using platforms like GitHub Sponsors and Open Collective to fund Make WordPress teams, contributors, and priority projects. The team had also been working with GoDaddy’s Courtney Robertson and Automattic’s Hari Shanker on the WordPress contribution health dashboards initiative. Robertson said it was concerning that the team had been shut down without prior warning or consultation, particularly as it impacted several active projects, including plans to unify WordPress’s DEIB, Five for the Future, and contributor dashboard initiatives under one team. “These efforts were crucial in establishing social and economic sustainability for WordPress and its community and I remain committed to advancing these initiatives,” Robertson said. “I hope that we can revisit these vital areas of work to ensure the continued growth and support of the WordPress contributor community.” Image credits: Jeroen Rotty, WordCamp Europe 2024. Most Popular WordPress Contributors and Community Leaders Call for Governance Reform in Rare Open Letter Mullenweg Shuts Down WordPress Sustainability Team, Igniting Backlash WordPress.org Forks ACF Pro in Unprecedented Move Against WP Engine Judge Signals Support for WP Engine Injunction Core Contributors Voice Concerns Over Mullenweg’s Control and “Culture of Fear” in WordPress Community",
    "commentLink": "https://news.ycombinator.com/item?id=42672675",
    "commentBody": "Mullenweg Shuts Down WordPress Sustainability Team, Igniting Backlash (therepository.email)160 points by robin_reala 7 hours agohidepastfavorite125 comments GavinAnderegg 5 hours agoI wrote a piece yesterday about the recent WordPress drama, including this bit. A fun thing I learned while digging into it is that Mullenweg himself requested that the Slack channel for this team be set up live on stage at WordCamp Europe in 2022. When disbanding the team, Mullenweg said, “today I learned that we have a sustainability team”. Maybe he forgot, but setting up this team was — at least in part — his idea. https://anderegg.ca/2025/01/11/wordpress-is-in-trouble reply mattrad 4 hours agoparent“WordPress is in trouble” depends on what “WordPress” is. WordPress the supply chain is currently dependent on wordpress.org. The community is working to route around this by decentralising distribution - see efforts such as AspirePress. WordPress the software development project is dependent on wordpress.org, and there is no way to route around this unless Matt agrees to give up his DFL position or a fork is created. WordPress the brand is being tarnished, mostly by Matt’s actions. wpdrama creates a riskier environment when assessing whether to use it as a CMS. WordPress the community is being denigrated and diminished. Again, I think only a change in governance can resolve that. reply Aurornis 3 hours agorootparent> “WordPress is in trouble” depends on what “WordPress” is. The one thing I’ve learned from all this drama is that all of the separated components of “WordPress”, from the .com to the .org and from the code to the hosting, were mostly superficial. Mullenweg appears to be equally in control of all of them and throws his weight around wherever it suits his agenda. reply RobotToaster 2 hours agorootparentEven in implementation they're pretty intertwined. The org version is missing a bunch of basic features that make com's \"jetpack\" plugin almost mandatory, which includes invasive tracking that's hard to turn off. reply prox 56 minutes agorootparentprevThe only thing I have not seen Matt do is taking even an ounce of accountability. It simply does not exist for him. Just like his professionalism that’s not there. I know Matt hangs around these parts and at no turn have I seen him engage in curious conversation. Basically what I have seen is emotional outbursts and crusading against the windmills. reply evantbyrne 4 hours agoparentprevEmbarrassing behavior for someone that became wealthy off other peoples' open-source contributions. reply Lammy 42 minutes agorootparentThat's literally every rich tech CEO though. All of the FAGMAN companies use hundreds of open source projects internally, and even when they do contribute back they end up driving those projects in directions that benefit their bottom line above all else. Presumably they wouldn't contribute at all if if the dollar cost of an internally-developed equivalent wasn't even higher than contributing to OSS. I don't have any stake in this drama since I haven't used WordPress for something like 13 years, buy to me this feels like crab bucket mentality, going after Mullenweg because he feels like a target that could actually be taken down as opposed to people like Zuck/Page/Brin/Nadella/etc who are truly untouchable. The level of vitriol just seems unreasonably high for something that isn't really that big of a deal. reply evantbyrne 32 minutes agorootparentMaybe if he wasn't personally and very publicly trolling an entire open-source community, which has created financial burden for users, then people in that community wouldn't be so upset. reply xyst 4 hours agoparentprevDude has gone full Elon at this point. Why hasn’t WP 86’d this guy? reply Aurornis 3 hours agorootparentWordPress and Mullenweg are one and the same, despite all of the superficial distinctions and organizations. He controls it all. There is no viable way to separate without forking the project and using a different name. Mullenweg is already trying to make like difficult for anyone he suspects might be thinking about forking, so anyone leading a fork has to assume that Mullenweg is going to make their life hell. He’s not afraid of dumping money into lawsuits to crush people, so forking WP is a scary proposition. reply _fat_santa 2 hours agorootparentGiven the risks, it would likely have to be done by a foundation with deep pockets and clout in the community. I could see an org like EFF do the work, they have the clout that any attacks against them would fall flat and they probably have deep enough pockets too. Besides an org like that that would do it for ideological reasons, the only other party would be some large org that is deeply invested in the WP ecosystem. I imagine there is some ecomm giant that's probably got 8 or 9 figures sunk into WP, for them it would be worthwhile to fork as it would likely be cheaper than migrating to another solution, but that's a hard maybe because you would need the right org with the right set of priorities to take on something like this. All of this is just pure speculation, if I'm being honest I find it unlikely that either scenario plays out in the real world. reply gizzlon 1 hour agorootparentI hope EFF have more important things to do than forking a php cms. It's popular but not important IMO reply xyst 1 hour agorootparentprevSo it’s the end of Wordpress, within a decade if it continues at its current trajectory. Drupal community to benefit here tremendously. As well as consultant work to migrate away from WP reply 1shooner 1 hour agorootparentDrupal has spent at least the last ~9 months positioning for this with the 'Drupal CMS'/'Starshot' product development initiative. reply throw16180339 4 hours agorootparentprevMatt controls the WordPress foundation, owns and operates wordpress.org, is CEO of Automattic, and votes 84% of its stock (https://news.ycombinator.com/item?id=42657150). AFAICS, there's no one who has the means to remove him. reply mschuster91 44 minutes agorootparentThere's always ways to remove someone completely gone off the rocker. Employees or customers voting with their feet, some big and established name(s) forking off the project, getting the target arrested (if only by provoking an escalation), getting a judge to declare the target mentally unfit, and various illegal methods as well. reply jl6 2 hours agorootparentprevThey’re XFree86’ing him. reply raverbashing 4 hours agorootparentprevI think even Elon is looking more normal than Matt at this time reply pavlov 2 hours agoparentprevThat’s standard authoritarian gaslighting. “Why did you do this thing?” “Sir, you told us to.” “Don’t argue with me. You’re fired” reply danieldk 2 hours agorootparentA bit of an aside, but how is that gaslighting? reply xeromal 1 hour agorootparentGaslighting has had a lot of scope creep in the past few years. lol reply meta_x_ai 1 hour agorootparentprevor may be typical leaders have a better vision of what \"sustainability\" means. \"Hire great engineers that have sustainability in their bones\" Actual implementation by the grifters : Hire other grifters with Sustainability in their resume, whose only job is to act as gatekeepers with psuedo-science garbage and make this team as big as the Engineering Team. It's perfectly fine for the leader to look at the implementation and say \"what's this fucking bullshit and cut everything\". These concepts are of course completely alien to leader/rich-hating HN reply grimblee 3 minutes agoprevThese people are just feeding his bad buzz by caring, just let him flail alone in the kid playground, don't feed him. reply Y_Y 6 hours agoprev> Veteran tech journalist Kara Swisher has described Matt Mullenweg’s move to shut down WordPress’s sustainability team as “bizarrely heinous behavior.” Is it bizarrely heinous? Or is it just kind of bad? I enjoy WordPress drama, and run a couple of lazy WordPress services, but I don't think that this is actually worth all the spilt ink and tears, relative to all the other injustices a person might choose to concern themselves with. reply blackeyeblitzar 18 minutes agoparentKara Swisher is an emotional activist type of journalist and not to be taken seriously. It’s not surprising to see manufactured outrage here. reply calmbonsai 2 minutes agorootparentI don't agree with the \"manufactured outrage\", but I concur on Swisher. Her M.O. is to take a 'hot-button' issue and simply add fuel to the fire with zero nuance or in-depth analysis. Not related, see the entire ReCode fiasco. reply mrcwinn 48 minutes agoparentprevA \"journalist\" who won't talk to the principal of the story. Now that's bizarre behavior. reply Dalewyn 6 hours agoparentprevnext [17 more] [flagged] DemocracyFTW2 5 hours agorootparentthe quote in question was not coming from the journalist that \"bashed out some text\" tho but from a team member. Thanks anyway for lashing out at the profession in general coz who do they think they are, right. Gotta know your limits, right? reply Dalewyn 5 hours agorootparentEmphasis mine: >Veteran tech journalist Kara Swisher has described Matt Mullenweg’s move to shut down WordPress’s sustainability team as “bizarrely heinous behavior.” >Members of the fledgling WordPress Sustainability Team have been left reeling after WordPress co-founder Matt Mullenweg abruptly dissolved the team this week—an action prominent tech journalist Kara Swisher has described as “bizarrely heinous behavior.” >In a scathing post on Threads, veteran tech journalist Kara Swisher posted a screenshot of Mullenweg’s announcement in Slack and called him a “stone cold asshole.” So no, the individual in question is a \"journalist\" and not a WordPress employee/contributor. reply rafram 4 hours agorootparentHowever you feel about her wording in this case, Kara Swisher is definitely a journalist, not a “journalist.” reply Dalewyn 4 hours agorootparentOf all the \"journalists\", tech \"journalists\" are particularly terrible. I digress though, calling a certain kind of poop a crappier kind of poop still means it's poop. An actual journalist would just quietly journal what happened and leave the readers to make of it what they will instead of throwing out attacks and insults to sensationalize for clicks. I wonder how much more tranquil the world could be if we could rid ourselves of all these \"journalists\" riling people up for greed and perhaps a hint of powertripping. reply rafram 4 hours agorootparentShe wrote this in a casual post on Threads [1], not an article. There’s no attempt to “sensationalize for clicks” here. Journalists don’t have to be in journalist mode all the time when they’re communicating casually. You seem to have a rather large chip on your shoulder. [1]: https://www.threads.net/@karaswisher/post/DEniSxyS01i reply Dalewyn 4 hours agorootparent>Journalists don’t have to be in journalist mode all the time when they’re communicating casually. Much like Matt doesn't have to be in CEO mode all the time when he's communicating casually, right? Being a proper journalist starts by at least acting like a reputable man of good character such that your writing also looks reputable. This Kara Swisher character is anything but. >You seem to have a rather large chip on your shoulder. I do indeed. I've lost many friends due to \"journalists\" sensationalizing and fearmongering them off the deep end. They create fear and rage where there is none for money; they are a fucking cancer upon society and I will not even pretend to hide my deep vitriol for them. reply matwood 4 hours agorootparentMullenweg brought this on himself. Swisher is just reporting on it. A rush to judgement can be an issue, but that is definitely not the case here. reply Dalewyn 4 hours agorootparentI'm not sure what level in Mental Gymnastics is required to say that calling someone a \"stone cold asshole\" is \"reporting\". Honestly, Matt gets my kudos if for no other reason because he's the bigger man (relatively speaking) by engaging in artful trolling instead of plainly undressed insults. At least that shit's potentially funny. reply matwood 4 hours agorootparentShe said what everyone has been saying for weeks. The wording is also clearly her opinion, nothing hidden there. She also included Mullenweg's post so everyone can read the source. Are you saying that journalists can never share an opinion? Also, are you trying to argue Mullenweg is only looking bad because of journalists? His own actions and words seem to be doing a pretty good job on their own. reply Dalewyn 3 hours agorootparent>Are you saying that journalists can never share an opinion? No. A journalist's job is to journal something and nothing else, it's literally their job name. Much like I do not expect nor want NTFS or ext4 to tell me what it thinks about my files, I don't want a \"journalist\" to tell me what it thinks about what happened let alone how I should think. That's not what I would read journalism for. If a \"journalist\" wants to write their thoughts they should be an author, critic, commentator, influencer, or the like instead. They will still be cancers upon society, but at least they won't sully journalism. >are you trying to argue Mullenweg is only looking bad because of journalists? Matt dug his own grave, but \"journalists\" are definitely making it harder to have worthwhile conversations about him. reply portaouflop 1 hour agorootparentPeople can be multiple things at once as mind blowing as that may seem. They can act in public as a journalist and at the same time have private opinions that they share on social media. Just as programmers not only “design schedule or plan radio or television programs”, journalism is not just “journaling” reply rafram 1 hour agorootparentprevYour comparison between humans and machines is revealing. If you want a soulless computer to tell you what’s going on in the world, talk to Grok or something. Humans have personalities. reply rafram 4 hours agorootparentprev> he's the bigger man (relatively speaking) by engaging in artful trolling No, being immature does not make you “the bigger man.” reply cdblades 1 hour agorootparentprev> Honestly, Matt gets my kudos if for no other reason because he's the bigger man (relatively speaking) by engaging in artful trolling instead of plainly undressed insults. That is a pretty plainly terrible mindset, but one that I don't think is very uncommon. reply DemocracyFTW2 1 hour agorootparentprevOK so now I feel it's rather immaterial who of us was right and who was wrong reply DemocracyFTW2 1 hour agorootparentprevLooks like you're right! reply safety1st 5 hours agoparentprevnext [2 more] [flagged] ericjmorey 4 hours agorootparentIs this recursive satire? reply aithrowawaycomm 3 hours agoparentprevnext [6 more] [flagged] westlifer 2 hours agorootparentWhy do you say he's a transphobic bigot? reply immibis 2 hours agorootparentPossibly because he is one. You could Google \"mullenweg\" and \"transphobia\" together for more details, dear troll account created 3 minutes before commenting. reply westlifer 2 hours agorootparentI did Google it. So perhaps you can explain how these three tweets from Matt are evidence of him being a \"transphobic bigot\", as others are claiming: 1. > When will you be honest with your followers? That the repeated adult content violations were not pictures like this, but likely ones on your other accounts (actual names): irishbigcockgiri, burgerfootjob, furryvore-burps, bredstrogen, cumburp, doggirlballsack, hungqueen, bigtittycockgf, bigcocktittygf, girltaint, muskmommy, girlballsack, showersharts, sapiosexual-breeder, catgirlhairball, catgirlcondom, catgirlcumsock, catgirlballsack, cumspangler. 2. > These photos are fine for Tumblr. Someone else could post them. You can't because you violated the community guidelines and terms of service multiple times and are banned for life. With your new accounts on other services, consider not posting deathwishes against their staff. 3. > Reporting credible threats of violence or terrorism is actually a legal requirement. No one reported your \"i hope photomatt dies forever a painful death\", however. > There's no problem with your transition photos, or the millions of others that have been posted. That last tweet in particular is evidence against the claim, is it not? reply aithrowawaycomm 1 hour agorootparentThe first tweet is precisely why any honest person would conclude he's transphobic: those were private accounts and he abused his power as CEO to get that information. He publicly outed her out of pure transphobic spite, presumably hoping that Twitter's merry bands of transphobes would dogpile her there and everywhere else they could find her. (I also heard that he repeatedly misgendered her in a now-deleted Tumblr post, but I couldn't find a screenshot.) \"You can't proooove that he's transphobic, and you can't prooooove that he outed her because he wanted transphobes to harass her\" yeah well we're not in court, this is a social matter. I am an individual human who has to make low-information judgments about other members of my species, and my low-information judgment of Mullenweg is that he's a transphobe. It is impossible for me to see an honest argument to the contrary. I am aware that dishonest arguments come quite easily: - \"innocent before proven guilty!\" - \"what about the tweet where he said 'trans people are okay I guess'\" - \"that was 8 months ago, when he was a wee 40-year-old lad, he's grown since then\" But considering Mullenweg is a horrifically bad person in many other areas of his life, I am quite confident he also sucks when it comes to civil rights. reply westlifer 44 minutes agorootparentAs you most likely know, the context of that first tweet was Matt correcting disinformation about Tumblr moderation that was being spread on Twitter by that user. There's no indication that in doing so, Matt was acting on transphobic intent nor that he is any sort of bigot. Nor is there any evidence of the purported misgendering that has apparently been cranked out of the rumor mill. It's quite funny that you doubled down with a feels over reals argument though. Just shows that deep down, you know you're throwing around spurious allegations. reply DoneWithAllThat 4 hours agoparentprevI was more struck by the fact that the author thinks anyone would or should care what some random tech journalist thinks about something. Person has opinions, news at eleven. reply add-sub-mul-div 1 hour agorootparentI don't really think you believe that journalism shouldn't exist or that you don't know journalism includes opinion and commentary, but I don't know why this is controversial enough to trigger that response. What's funny, though, is that the trite \"news at 11\" catchphrase is very much also invoking traditional journalism. reply skeeter2020 5 hours agoprevI didn't really understand what a \"WP Sustainability Team\" was, so I clicked the link to the linked website in this story. I was surprised that it was in fact an initiative to do things like \"have a dashboard that shows the climate impact of publishing\" and \"promote static publishing over...\" I assume because serving this content (under someone's model) uses less electricity? Hey - if this is important to you, by all means pursue this direction, but I would cut this sort of initiative too. reply _fat_santa 2 hours agoparentI found it kinda silly at first, like how would WP actually contribute to a reduction in climate change. But I think at the scale that WP is deployed at (millions upon millions of sites), changing something like static output by default could contribute to a non-negligable reduction in electricity spent on hosting. reply markx2 4 hours agoparentprev> Hey - if this is important to you, by all means pursue this direction, but I would cut this sort of initiative too. This was costing Matt nothing. Zero. The work was being done, for free, by passionate people. reply missinglugnut 3 hours agorootparent> This was costing Matt nothing. Zero. These were salaried employees working on the sustainability team, correct? If not, how could Mullenweg shut it down? reply GavinAnderegg 1 hour agorootparentNo, they were all community members working for free. Check out the bios of the team reps here: https://make.wordpress.org/sustainability/2024/12/13/proposa... Here are all the contributors on the GitHub repo, also all volunteers: https://github.com/WordPress/sustainability/graphs/contribut... Mullenweg can shut down the team because he has complete control over all the WordPress.org infrastructure. reply aimazon 1 hour agorootparentprevMy sweet summer child. Matt has unilateral control over every aspect of Wordpress, including the open-source project and community. He exerts that control, whether that's closing Slack channels or banning members. https://wordpress.org/news/2025/01/jkpress/ reply lupusreal 1 hour agorootparentHe cannot control how people choose to spend their own free time. If these people want to have a chatroom about WordPress sustainability which isn't hosted by Matt, there's nothing Matt can do to stop them. They can easily do this for free. This is a huge drama about nothing. Matt is being a baby and so is everybody else who's crying about it. There's no money on the line here, so there's literally no reason for any of the involved parties to not simply walk away and stop associating with each other. reply poincaredisk 1 hour agorootparent>There's no money on the line here, so there's literally no reason for (...) I assume volunteers working in the sustainability team for free primarily care about non-monetary things. reply scarecrowbob 38 minutes agorootparentprevYou can't force folks to contribute stuff, but you very much can prevent them from contributing things. That was my experience- I didn't feel like it was worth all the work just to be able to contribute to WP, for reasons that are becoming more widelt visable. And yes, there is money on the line for a lot of folks- if you sell WP-based solutions to the gov and large NGOs (that's what the co I was working with did), than it is very hard to \"just walk away\" because in addition to ceasing the current work you'd have to find an alternate solution, re-train the hundreds of people you've trained to admin the system, etc/etc/etc. Some WP sites have thousands of admin users and hundreds of thousand of items of content. So if photomatt takes his toys and goes home, yeah, these projects all have the code and can fork it or do whatever and photomatt can't do much, but there is a tremendous real cost to folks. Millions of dollars in the case of the small 7-person shop which I worked at. reply lupusreal 3 hours agorootparentprevSo then nothing was actually cut and these weird people who are super passionate about the climate impact of WordPress of all things can continue to do.. whatever it was they were doing, for free as they were before? If the issue is having a slack channel in the WordPress org, they can just make their own discord server. No big deal. reply Aurornis 3 hours agorootparentMullenweg was the one who requested the creation of the Slack channel. reply kevin_thibedeau 1 hour agorootparentIts purpose was to be a greenwash figleaf. These activities are supposed to be toothless and accomplish nothing because that isn't why they exist. reply lupusreal 2 hours agorootparentprevIf he can make the channel, he can delete the channel. If the people in that channel want a new place to chat, they can make one. So what's the problem? reply frereubu 3 hours agoparentprevI agree in principle. It's the childish way in which it was done that's the issue for me. reply ToucanLoucan 35 minutes agorootparentI disagree in principle. You can call it a pro-environment initiative or you can call it just... promoting good engineering. Making code more efficient benefits literally everyone from the people making it to the people using it to the planet upon which it is used, and who's resources it is dependent on. I wish efficiency was a more prioritized thing in basically every facet of the modern tech industry which, at present, is addicted to an absolutely stressful number of libraries that cause nearly every prominent tech product to be ludicrously bloated. Like, it's incredibly irritating to me that mobile browsers are practically unusable, not because mobile design isn't ubiquitous, but because every website now makes my phone hot because it's running 800 MB of fucking JavaScript to render text. reply jgalt212 4 hours agoparentprevLLMs use about 1000X the resources of the most poorly designed WP site. The best thing that the WP Sustainability Team can do is retask itself to higher impact problems. reply bastawhiz 3 hours agorootparentFor every one LLM user, there are hundreds if not thousands of people visiting WordPress sites. WordPress is unimaginably ubiquitous. When you're that popular, making your software more energy efficient has an actual, real life impact (even if other things are using lots of energy). In aggregate it's still a massive amount of energy. reply jgalt212 2 hours agorootparentyou can say that, but are big tech building new data centers and contracting with nuclear energy producers to support WP websites? reply jazzyjackson 2 hours agorootparentThere are no nuclear energy producers being contracted, just upstarts that promise to deliver energy later, in exchange for a promise to buy that energy. reply jgalt212 42 minutes agorootparentThree Mile Island nuclear plant will reopen to power Microsoft data centers > Three Mile Island, the power plant near Middletown, Pa., that was the scene of the worst commercial nuclear accident in U.S. history, will reopen to power Microsoft's data centers https://www.npr.org/2024/09/20/nx-s1-5120581/three-mile-isla... reply luckylion 2 hours agorootparentprevDid they do that though? I mean, there's a lot of code to improve in WP where you can eek out more performance (the default jquery with all the ie6 compatibility nonsense? really?). Seems like this team wasn't for that though. reply throwawayqqq11 3 hours agorootparentprev... like raising public awareness about the broad issue? whatabout: retasking yourself on that issue instead of just commenting? reply jgalt212 2 hours agorootparentI am raising awareness around the energy wastefulness of LLMs. reply jasonlotito 3 hours agoparentprev> I didn't really understand what a \"WP Sustainability Team\" You still don't. reply neilv 2 hours agoprev1. Is Mullenweg's recent behavior as bad as it seems to those of us not very familiar with WordPress? 2. If the behavior is as bad as it seems... Did Mullenweg always behave like this? (Like, hints of it, even if the circumstances at the time meant it wasn't very negative?) Or did it increase slowly over time, or change rather abruptly? I think a national-politics-grade PR attack campaign is unlikely. So, if it wasn't that, I'm wondering whether the current character was always there, or there was some gradual or punctuated mental health change. That can often be helped and healed. Separately, in any case, the larger WordPress community will hopefully realize the risks of dictators and kingdoms, and move to be more resilient. And not make the same mistake yet again, just with a new overlord, like we often do in tech consumption (and in human history, for that matter). reply badlibrarian 2 hours agoparentI think it started with Tumblr, perhaps behind the scenes. He then did some stupid things under pressure and said some highly inappropriate things in response to criticism. Like others who faced pushback from certain communities, he decided to double down on the arrogance and insensitivity. Unlike others, he doesn't have the charisma, goodwill, or video podcast to pull it off. reply ternnoburn 2 hours agoparentprevSeeing his posts here, it would appear that his behavior is indeed \"as bad as it seems\". A serial poster who cannot seem to disengage, took everything personally, and refused to try to understand other perspectives. reply pluc 5 hours agoprevHe's been doing so much more: https://gist.github.com/adrienne/aea9dd7ca19c8985157d9c42f7f... Again, I hold no sympathy for Silver Lake, but I hope they fuck him up good. reply lawgimenez 5 hours agoparent> …Mullenweg threatening to physically dismantle their booth in the middle of the show This is crazy if true. reply voakbasda 27 minutes agoparentprevIn all seriousness, I wonder if Matt has a brain tumor. His behavior has gone from merely sociopathic to outright despotic, appearing both irrational and self-destructive. Certainly, he has single-handedly turned the entire WP ecosystem into a toxic cesspool. I feel sorry for everyone that he seems to have threatened, bullied, or outright extorted. Anyone aware of these shenanigans would be wise to steer clear of the entire mess until he is out of the picture. reply _the_inflator 5 hours agoprevWordpress is more of an OS today. I took a 12 years hiatus from WP and just went back into actively using it since a couple of weeks now. The shift went clearly into massive plugins that frees you from all the grunt work that was necessary 10+ years ago. To me as someone who once wrote plugins on my own, had to develop themes totally by hand using the infamous WP loop etc. this is like going from command line to drag and drop UI. WP is a OS and Divi, ThriveTheme etc. took over. I like it, it saves a ton of time. reply n3storm 4 hours agoparentIt saves a ton of time creating. It wastes a lot of time maintaining and cleaning up infections and debugging poor performance. Is like Dreamweaver but you have to run Dreamweaver all the time your site has visitors and an instance of Dreamweaver for each visitor. reply bl4kers 2 hours agoparentprevWhy did you start using it again? reply bovermyer 6 hours agoprevMullenweg's behavior is poor, yes, but I wonder how much that will affect Wordpress's market presence. The average person who would use the service isn't likely to hear about any of this. reply frereubu 5 hours agoparentI've been building websites on WordPress for 15+ years, run a WordPress agency and have many clients who come to us specifically asking for us to use WordPress. I'm seriously considering moving to something else because we use the WP Engine plugin Advanced Custom Fields (pro version) and him pulling the free version from the plugin repo in a hissy fit has made me seriously concerned about the stability of the ecosystem if one guy can do something like that in a fit of pique. reply bovermyer 3 hours agorootparentI'm curious what you'd consider moving to. Managed Wix/Squarespace? Ghost, maybe? I honestly haven't dug into that world in many years, I don't know what it looks like these days. reply frereubu 3 hours agorootparentCraft and Wagtail are the two I've heard of as WordPress replacements. In our sector (nonprofits) Drupal has too much of an (mostly undeserved, now) reputation as being very hard to use. I found Craft a bit too convoluted (as I mention in another comment, perhaps just due to unfamiliarity). Interested in Wagtail but that would mean a switch from PHP to Python. Not a deal-breaker but a bit of a hump to get over. Wagtail also used to be a bit too beholden to Torchbox (a UK agency) for my tastes - I think they started the project - but looks like that's shifted a bit in terms of the core team. reply 65 2 hours agorootparentAt my job one of the sites I work on, a large government agency you have heard of, uses Drupal. And honestly? I like Drupal more than Wordpress as a developer. But, I will say, Drupal is definitely confusing at first with all its nodes and entities and content types and poor documentation and so on. But on the other hand, it doesn't really force you into anything. You can build almost anything with Drupal, which I think makes it a great CMS. reply evantbyrne 3 hours agorootparentprevUnless you're at a theme chop shop, then you might as well move onto greener pastures. There are so many alternatives now for website control panels like Wagtail, Payload, and Craft CMS. reply frereubu 3 hours agorootparentNo, mostly ground-up custom built themes. I've tried Craft and found it pretty convoluted (althought that might just be familiarity), but will be looking at Wagtail. Still hoping that WP can pull out of the tailspin because it's going to be a huge headache to switch. reply shiftpgdn 5 hours agorootparentprevACF is very easily replaced. WPEngine is one of a million WordPress hosts. Move your sites to a company that isn't run by PE shitheads. reply frereubu 3 hours agorootparentThis is far too dismissive - I'm guessing you don't run a WordPress agency. We don't use WP Engine for hosting, we only use the ACF plugin. ACF is not \"very easily replaced\" because we've built the site data structures around it and we'd have to completely rearchitect the sites - at our own cost. \"Moving our sites\" is not easy either because we have 100+ sites, each of which would involve DNS updates (which we often have to walk our clients through because they don't understand DNS at all) and a complete reconfiguration of our build / deploment process. I don't really understand the motivation behind this comment. reply bdcravens 44 minutes agorootparentprevWhat hosts do you recommend that offer built-in development/staging environments? (one of WPEngine's selling points) reply dageshi 3 hours agorootparentprevWhat do you replace ACF with? reply mjburgess 4 hours agorootparentprevAh yes, the specific structure of WPE's ownership is sufficient reason to move away from them -- but the behaviour and structure of WP's ownership isnt... reply iamdbtoo 5 hours agoparentprevHe's making the ecosystem generally unstable and there are a lot of businesses that depend on that stability. reply ericjmorey 4 hours agoparentprevProbably enough to notice, but not in a way that threatens the company to continue operating profitability. reply bipson 6 hours agoprev> “Today I learned that we have a sustainability team,” Mullenweg said. If this is true: Is there any possible explanation for such a statement where leadership comes out unscathed? reply bromuro 5 hours agoparentIt looks like it was originally just a channel on Slack, that eventually evolved into a “team” without Mullenweg paying much attention to it. The Sustainability agenda , quoted by the article, appears excessive to me as well - and it resonates with me why Mullenweg is asking for a different approach. With the dramas going on, shutting down the channel was a dick move , though. reply bipson 4 hours agorootparentThis still reflects badly on the leader: - Stuff going on you're not aware of? - Things spiraling out of control/becoming self-directed? - You forgetting what you did or said a few months/years ago, and getting mad on others in consequence? - Your intentions not well understood? All of this is on you (the leader). That's reason for resignation. The behavior around it is just childish IMO. I wonder how this affects him being able to do his job. reply dylan604 3 hours agorootparent> - You forgetting what you did or said a few months/years ago, and getting mad on others in consequence? Forgetting? That’s a lot of good faith on your part. Where in the time of people denying everything even with undeniable video/audio evidence and their target audience believes them. If you don’t believe then you were not the target audience and are irrelevant reply matwood 4 hours agorootparentprev> All of this is on you (the leader). That's reason for resignation. Bingo. If someone wants to be the leader, then they have to deal with leadership. That means that everything going on under your leadership is your responsibility. We've let this slide as a society, letting leaders take credit for successes and then blaming others for failures. reply ulfw 5 hours agoparentprevHe himself clearly showed it's a lie as he admitted on Threads that he started said team. Utterly bizarre. reply lapcat 5 hours agoparentprev> If this is true It's not true. It's a blatant lie, as the embedded video in the article proves. reply LordAtlas 5 hours agoprevThe latest childish tantrum Mullenweg has thrown is this passive-aggressive post where, ostensibly to get a Wordpress fork off the ground, he's deactivated the Wordpress.org accounts of 5 people, including the people who just asked for a change in the governance model of Wordpress. That entire post is a huge ball of bitter passive-aggressive shit couched in \"howdy\" language. [1] This includes the account of someone who has not been involved with Wordpress development since 2020. [2] [3] [1] https://wordpress.org/news/2025/01/jkpress/ [2] https://heatherburns.tech/2025/01/12/another-day-of-stochast... [3] https://bsky.app/profile/mor10.com/post/3lfgqcge62c2d reply jazzyjackson 2 hours agoparentI haven't learned much in my career, (cue \"burn after reading\" clip [0]), but I do know that CEOs hate it when changes in governance are discussed by underlings, it's way more of a threat to power than any fork. [0] https://youtu.be/J6VjPM5CeWs reply charlangas 3 hours agoparentprevHaving only been exposed to Mullenweg on the Tim Ferriss podcast, I perceived him as the benevolent, zen-like steward of one of the biggest open source projects ever. Watching this drama over the past few months has been... interesting. It's like the dude on the podcast hit his head badly and had his whole personality replaced. reply frankdenbow 3 hours agorootparentMore likely that a podcast is a performance for marketing reply knallfrosch 4 hours agoprevSounds like he just created a Slack channel where people could chit-chat about \"sustainability\" much like they chat about football or ESports. Then he found out these guys actually spend all their working lives implementing plugins that do little more than display \"Your site needed 0.04663kg of CO2 to run this year\" next to a green leaf. Seeing how they spent 1,5 years on this and have little more to show than \"concepts of a plan\" he was right to shut this down. reply typeofhuman 4 hours agoparentOne could argue the sustainability team was disbanded in the name of sustainability. reply philipov 3 hours agorootparentOne could argue the moon is made of cheese. reply jasonlotito 3 hours agoparentprev> spend all their working lives These were volunteers. > Seeing how they spent 1,5 years They didn't. > he was right to shut this down. Ignorance is a choice. reply MattGaiser 6 hours agoprev> citing a Reddit thread Mullenweg created on Christmas Eve asking for suggestions to create WordPress drama in 2025. When it comes to Mullenweg, I am always impressed that there is often some worse behaviour mentioned in the article... reply orochimaaru 1 hour agoprevWhat did the sustainability team do? I don’t mean an offense to the team members but sustainability for a software company is very niche. Unless you’re a hyper scaler like public cloud services your impact is limited. The whole sustainability area needs a rethink. It needs to be linked to cost of doing business (and no - not idiotic carbon taxes) and I think if you’re using public cloud services it already is. If your code is inefficient you will be hit with higher bills (I know I’m abstracting wildly but this is mostly true). If you need to lower cost/utilization write the code to do optimize. If you own your own data centers, now that’s a very different proposition. Not sure whether Wordpress owns these - and I don’t mean renting space and racks. I mean owning the entire space. reply paulcole 50 minutes agoparentFrom the article: > Varszegi said the team’s most impactful contribution to WordPress was the publication of the Sustainable Events Handbook, designed to help WordCamp organizers improve the sustainability of their events. The team was also developing a plugin to help site owners estimate the carbon footprint of their website. So the answer is: Not much. reply voytec 4 hours agoprevPerson exhibiting textbook traits of Narcissistic Personality Disorder for years goes into mental meltdown, gets sued and loses preliminary injunction. It's only downhill from there. reply coldpie 4 hours agoparent> It's only downhill from there. I just hope it's over quickly and some other ownership model comes into place. WordPress is an impressive & important project, it'd be a shame to lose it because of one lunatic. reply twasold 5 hours agoprevTLDR; Mullenweg made a poorly received joke/trolling post on Reddit on Christmas. It could have been perceived as reconciliatory /self deprecating or trolling depending on your perspective. That was the straw in the camel’s back that led the sustainability chief to resign. Mullenweg then disbanded the entire sustainability team which he announced by taunting them on slack. reply curiousgal 6 hours agoprevnext [5 more] [flagged] nozzlegear 5 hours agoparent> when this guys self-Luigis What does this mean? reply CaptainFever 4 hours agorootparentI'm guessing \"Luigi\" is another way of saying \"murder\" now, so \"self-murder\" means \"suicide\". reply mtmail 4 hours agorootparentprevLuigi Mangione killed the CEO of UnitedHealthcare. reply kotaKat 5 hours agoparentprevIt’s time for a 5150 now, right? Dude is a danger to himself and others. reply kopirgan 5 hours agoprev [–] Yet another Trump win cascade. Better to prepare for more. Some did it out of conviction. But as we saw with FB the ones that did for other reasons are going to back pedal reply ericjmorey 4 hours agoparent [–] When the tide goes out, you can see who's been swimming naked. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Matt Mullenweg, co-founder of WordPress, has disbanded the WordPress Sustainability Team, leading to significant backlash from the community.",
      "The team was dissolved following the resignation of team representative Thijs Buijs, who cited leadership issues, and Mullenweg's claim that the team was not meeting its objectives.",
      "Critics have accused Mullenweg of hypocrisy, as he initially supported the team's formation, and the disbandment has raised concerns about the future of sustainability initiatives within WordPress."
    ],
    "commentSummary": [
      "Matt Mullenweg disbanded the WordPress Sustainability Team, leading to criticism and concerns about the impact on WordPress's brand and community.",
      "Critics highlight Mullenweg's significant control over WordPress as problematic, suggesting a potential need for changes in governance.",
      "The disbanded team was working on initiatives such as a carbon footprint plugin, raising questions about WordPress's commitment to sustainability and leadership stability."
    ],
    "points": 160,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1736679715
  },
  {
    "id": 42670468,
    "title": "IP addresses through 2024",
    "originLink": "https://www.potaroo.net/ispcol/2025-01/addr2024.html",
    "originBody": "The ISP Column A column on things Internet Other Formats: IP Addresses through 2024 January 2025 Geoff Huston Time for another annual roundup from the world of IP addresses. Let’s see what has changed in the past 12 months in addressing the Internet and look at how IP address allocation information can inform us of the changing nature of the network itself. Back around 1992 the IETF gazed into their crystal ball and tried to understand how the Internet was going to evolve and what demands that would place on the addressing system as part of the “IP Next Generation” study. The staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that exercise. Doubtless, these device numbers will continue to grow. We continue to increase silicon chip production volumes and at the same time we continue to refine the production process. But, at that time, we also predicted that the only way we could make the Internet work across such a massive pool of connected devices was to deploy a new IP protocol that came with a massively larger address space. It was from that reasoning that IPv6 was designed, as this world of abundant silicon processors was the issue that IPv6 was primarily intended to solve. The copious volumes of address space were intended to allow us to uniquely assign a public IPv6 address to every such device, no matter how small, or in whatever volume they might be deployed. But while the Internet has grown at such amazing speed, the deployment of IPv6 continues at a more measured pace. There is still no evidence of any common sense of urgency about the deployment of this protocol, and still there is no common agreement that the continued reliance on IPv4 is failing us. Much of the reason for this apparent contradiction between the designed population of the IPv4 Internet and the actual device count, which is of course many times larger, is that the Internet rapidly changed from a peer-to-peer architecture to a client/server paradigm. Clients can initiate network transactions with servers but are incapable of initiating transactions with other clients. Network Address Translators (NATs) are a natural fit to this client/server model, where pools of clients share a smaller pool of public addresses, and only require the use of an address while they have an active session with a remote server. NATs are the reason why in excess of 30 billion connected devices can be squeezed into some 3 billion advertised IPv4 addresses. Applications that cannot work behind NATs are no longer useful in the context of the public Internet and no longer used as a result. However, the pressures of this inexorable growth in the number of deployed devices in the Internet means that the even NATs cannot absorb these growth pressures forever. NATs can extend the effective addressable space by up to 32 ‘extra’ bits using mapping of the source and destination port fields of the TCP and UDP headers, and they enable the time-based sharing of public addresses. Both of these measures are effective in stretching the IPv4 address space to encompass a larger client device pool, but they do not transform the address space into an infinitely elastic resource. The inevitable outcome of this process is that we may see the fragmenting of the IPv4 Internet into a number of disconnected parts, probably based on the service ‘cones’ of the various points of presence of the content distribution servers, so that the entire concept of a globally unique and coherent address pool layered over a single coherent packet transmission realm will be foregone. Alternatively, we may see these growth pressures motivate the further deployment of IPv6, and the emergence of IPv6-only elements of the Internet as the network itself tries to maintain a cohesive and connected whole. There are commercial pressures pulling the network in both of these directions, so it’s entirely unclear what path the Internet will follow in the coming years, but my (admittedly cynical and perhaps overly jaded) personal opinion lies in a future of highly fragmented network. Can address allocation data help us to shed some light on what is happening in the larger Internet? Let’s look at what happened in 2024. IPv4 in 2024 It appears that the process of exhausting the remaining pools of unallocated IPv4 addresses is proving to be as protracted as the process of the transition to IPv6, although by the end of 2021 the end of the old registry allocation model had effectively occurred with the depletion of the residual pools of unallocated addresses in each of the Regional Internet Registries (RIRs). It is difficult to talk about “allocations” in today’s Internet. There are still a set of transactions where addresses are drawn from the residual pools of RIR-managed available address space and allocated or assigned to network operators, but at the same time there are also a set of transactions where addresses are traded between network in what is essentially a sale. These address transfers necessarily entail a change of registration details, so the registry records the outcome of a transfer, or sale, in a manner that is similar to an allocation or assignment. If we want to look at the larger picture of the amount of IPv4 address space that is used or usable by Internet network operators, then perhaps the best metric to use is the total span of allocated and assigned addresses, and the consequent indication of annual change in the change in this total address span from year to year. What is the difference between \"allocated\" and \"assigned\"? When a network operator or sub-registry has received an allocation it can further delegate that IP address space to their customers along with using it for their own internal infrastructure. When a network operator has received an assignment this can only be used for their own internal infrastructure. [https://help.apnic.net/s/article/Using-address-space] I personally find the distinction between these two terms somewhat of an artifice these days, so from here on I’ll use the term “allocation\" to describe both allocations and assignments. For the first time in the IPv4 Internet, the total IPv4 allocated address pool contracted by some 400 thousand addresses in 2023, with some 3.685 billion allocated addresses at the end of the year. This represented a contraction of some 0.01% for the total allocated IPv4 public address pool, a contraction that was countered by a comparable rise in the span of allocated addresses through 2024 (Table 1).2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 Address Span (B) 3.227 3.395 3.483 3.537 3.593 3.624 3.643 3.657 3.657 3.682 3.684 3.685 3.687 3.686 3.687 Annual Change (M) 241.7 168.0 88.4 53.9 55.9 30.6 19.4 13.2 0.6 24.9 2.2 1.1 1.6 -0.4 1.2 Relative Growth 8.1% 5.2% 2.6% 1.5% 1.6% 0.85% 0.53% 0.36% 0.02% 0.68% 0.06% 0.03% 0.04% -0.01% 0.03% Table 1 - IPv4 Allocated addresses by Year Have we exhausted all of the available sources of further IPv4 addresses? The address management model is that unallocated addresses are held in a single pool by the Internet Assigned Numbers Authority, and blocks of addresses are passed to RIRs who then allocate them to various end entities, either for their own use or for further allocation. But, the IANA exhausted the last of its available address pools some years ago, and these days the IANA holds just 3 /24 address prefixes in its recovered address registry. Because the option of dividing this tiny address pool into 5 equal chunks of 153.6 individual address is not viable, then these addresses are likely to sit in the IANA Recovered Address registry for some time. That is, until one of more of the RIRs return more prefixes recovered from the old “legacy\" allocated addresses to the IANA, who would then be able to divide the pool equally and distribute them to each the 5 RIRs. This is unlikely to occur. There are also addresses that have been marked by the IANA as reserved for \"\"special uses. This includes blocks of addresses reserved for Multicast use. At the top end of the IPv4 address space registry there is a set of addresses that are marked as reserved for \"Future Use\"\". This is a relatively large pool of 268,435,456 addresses (the old former “Class E\" space) and if ever there was a “future\" for IPv4 then it has well and truly come and gone. But exactly how to unlock this space and return it to the general use pool is a problem that so far has not found a generally workable solution, although efforts to do so have surfaced in the community from time to time. The topic of releasing the Class E space for use in the public Internet as globally routable unicast address space has been raised from time to time over the past 15 years or so. Some Internet drafts were published for the IETF’s consideration that either directly proposed releasing this space for use, or outlined the impediments in various host and router implementations that were observed to exist in 2008 when these drafts were being developed. The proposals lapsed, probably due to the larger consideration at the time that the available time and resources to work on these issues were limited and the result of effort spent in ‘conditioning’ this IPv4 space for general use was only going to obtain a very small extension in the anticipated date of depletion of the remaining IPv4 address pools, while the same amount of effort spent on working on advancing IPv6 deployment was assumed to have a far larger beneficial outcome. From time to time this topic reappears on various mailing lists and blogs (see https://www.potaroo.net/ispcol/2024-09/2404.html, for example), but the debates tend to circle around this same set of topics one more time, and then lapse. As the IANA is no longer a source of addresses, then we need to look at the RIR practices to see the life cycle of addresses from the registry’s perspective. When IP address space is returned to the RIR or reclaimed by the RIR according to the RIR’s policies it is normally placed in a RIR-reserved pool for a period of time and marked as reserved by the RIR. Marking returned or recovered addresses as reserved for a period of time allows various address prefix reputation and related services, including routing records, some time to record the cessation of the previous state of the addresses prefix, prior to any subsequent allocation. Following this quarantine period, which has been between some months and some years, this reserved space is released for re-use. The record of annual year-on-year change in allocated addresses per RIR over the same fourteen-year period is shown in Table 2. There are some years when the per-RIR pool of allocated addresses shrunk is size. This is generally due to inter-RIR movement of addresses, due to administrative changes in some instances and inter-RIR address transfers in others.2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 APNIC 119.5 101.0 0.6 1.2 4.6 7.4 6.7 3.2 0.4 10.5 1.7 1.5 0.8 -1.1 -0.8 RIPE NCC 52.3 40.5 37.8 1.0 33.8 4.7 4.1 3.7 0.3 12.0 0.4 2.5 4.7 6.2 0.5 ARIN 27.2 53.8 24.3 19.0 -14.1 2.3 -4.8 -2.3 -0.3 -10.1 -0.9 -1.7 -3.8 -5.5 -3.0 LACNIC 17.1 13.6 17.3 26.3 18.7 1.2 1.5 1.4 0.1 2.4 1.2 -0.2 -0.3 -0.1 0.0 AFRINIC 8.8 9.4 8.5 6.3 12.8 15.0 11.9 7.1 0.2 10.1 -0.2 -0.9 0.2 0.1 0.0 TOTAL 224.9 218.3 88.5 53.8 55.8 30.6 19.4 13.1 0.7 24.9 2.2 1.2 1.6 -0.4 1.2 Table 2 – Annual change in IPv4 Allocated addresses (millions) - Distribution by RIR Each of the RIRs are running through their final pools of IPv4 addresses. At the end of 2024, across the RIR system there are some 4.6 million addresses are in the Available Pool, held mainly in APNIC (3.6 million) and AFRINIC (990 thousand). Some 10.2 million addresses are marked as Reserved, with 4.6 million held by ARIN and 4.4 million addresses held by AFRINIC. As seen in Table 3, there has been a reduction in the Reserved Pool for all RIRs, except AFRINIC, and the major reductions were seen in APNIC (1.7M) and ARIN (600K) in ARIN (98K).Available Reserved RIR 2022 2023 2024 2022 2023 2024 APNIC 2,503,424 2,469,120 3,647,488 1,514,752 2,202,624 416,256 RIPE NCC - 1,024 256 737,496 708,872 677,160 ARIN 8,448 8,960 3,840 5,311,488 5,213,184 4,609,792 LACNIC 1,024 256 1,536 148,480 151,296 118,528 AFRINIC 1,403,136 1,201,664 990,976 4,104,960 4,186,112 4,443,648 TOTAL 3,916,032 3,681,024 4,644,096 11,817,176 12,462,088 10,265,384 Table 3 – IPv4 Available and Reserved Pools December 2024 The RIR IPv4 address allocation volumes by year are shown in Figure 1, but it is challenging to understand precisely what is meant by an allocation across the entire RIR system as there are some subtle but important differences between RIRs, particularly as they relate to the handling of transfers of IPv4 addresses. In the case of ARIN, a transfer between two ARIN-serviced entities is conceptually treated as two distinct transactions: a return of the addresses to the ARIN registry and a new allocation from ARIN. The date of the transfer is recorded as the new allocation date in the records published by the RIR. Other RIRs treat an address transfer in a manner analogous to a change of the nominated holder of the already-allocated addresses, and when processing a transfer, the RIR’s records preserve the original allocation date for the transferred addresses. When we look at the individual transaction records in the published RIR data, and collect then by year, then in the case of ARIN the collected data includes the volume of transferred addresses that were processed in that year, while the other RIRs only include the allocations performed in that year. In order to provide a view across the entire system its necessary to use an analysis approach that can compensate for these differences in the ways RIRs record address transactions. In this study, an allocation is defined here as a state transition in the registry records from reserved or available to an allocated state. This is intended to separate out the various actions associated with processing address transfers, which generally involve no visible state change, as the transferred address block remains allocated across the transfer, from allocations. This is how the data used to generate Figure 1 has been generated from the RIR published data, comparing the status of the address pools at the end of each year to that of the status at the start of the year. An allocation in that year is identified if the allocated address block was not registered as allocated at the start of the year. Figure 1 – IPv4 Address Allocations by RIR by year The number of RIR IPv4 allocations by year, once again generated by using the same data analysis technique as used for Figure 1, are shown in Figure 2. Figure 2 – IPv4 Allocations by RIR by year It is clear from these two figures that the average size of an IPv4 address allocation has shrunk considerably in recent years, corresponding to the various IPv4 address exhaustion policies in each of the RIRs. IPv4 Address Transfers The RIRs permit the registration of IPv4 transfers between address holders, as a means of allowing secondary re-distribution of addresses as an alternative to returning unused addresses to the registry. This has been in response to the issues raised by IPv4 address exhaustion, where the underlying motivation as to encourage the reuse of otherwise idle or inefficiently used address blocks through the incentives provided by a market for addresses, and to ensure that such address movement is publically recorded in the registry system. The number of registered transfers in the past eleven years is shown in Table 4. This number of transfers includes both inter-RIR and intra-RIR transfers. It also includes both the merger and acquisition-based transfers and the other grounds for of address transfers. Each transfer is treated as a single transaction, and in the case of inter-RIR transfers, this is accounted in the receiving RIR’s totals. Receiving RIR 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 APNIC 158 180 307 451 840 845 491 533 820 785 745 796 752 RIPE NCC 10 171 1,054 2,836 2,373 2,451 3,775 4,221 4,696 5,742 4,640 4,937 5,215 ARIN 3 22 26 26 68 94 150 141 97 185 LACNIC 2 3 9 17 20 17 AFRINIC 17 27 26 80 58 14 15 Total 168 351 1,361 3,290 3,235 3,322 4,311 4,849 5,639 6,766 5,601 5,864 6,184 Table 4 - IPv4 Address Transfers per year The differences between RIRs reported numbers are interesting. The policies relating to address transfers do not appear to have been adopted to any significant extent by address holders in AFRINIC and LACNIC serviced regions, while uptake in the RIPE NCC service region appears to be very enthusiastic! A slightly different view is that of the volume of addresses transferred per year (Table 5). 4.5 Receiving RIR 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 APNIC 1.6 2.3 4.1 6.6 8.2 4.9 10.0 4.3 16.6 6.5 3.7 2.7 2.5 RIPE NCC 0.1 2.0 9.6 11.6 9.2 24.6 19.5 26.9 18.2 16.2 36.9 20.8 23.0 ARIN0.1 0.3 0.2 - 0.3 0.2 0.2 3.1 1.6 LACNIC -- - - 0.0 0.0 AFRINIC 0.2 0.5 1.2 3.4 0.5 0.1 0.1 Total 1.7 4.3 13.7 18.2 17.6 29.6 29.7 31.9 36.2 26.4 44.3 25.3 30.2 Table 5 – Volume of Transferred IPv4 Addresses per year (Millions of addresses) A plot of these numbers is shown in Figures 3 and 4. Figure 3 – Number of Transfers: 2012 - 2024 Figure 4 – Volume of Transferred Addresses: 2012 - 2024 The volumes of transferred addresses reached a peak in 2022 and declined in 2023. In the case of APNIC the peak occurred in 2020, and the APNIC 2024 volume is comparable to the volume transferred in 2013. In the RIPE and ARIN regions address transfers are still growing in total volume, while in APINC the volume of IPv4 address transfers has largely waned. The aggregate total of addresses that have been listed in these transfer logs since 2012 is some 309 million addresses, or the equivalent of 18.4 /8s, which is some 8% of the total delegated IPv4 address space of 3.7 billion addresses. However, that figure is likely to be an overestimation, as a number of address blocks have been transferred multiple times over this period. Are Transfers Performing Unused Address Recovery? This data raises some questions about the nature of transfers. The first question is whether address transfers have managed to be effective in dredging the pool of allocated but unadvertised public IPv4 addresses and recycling these addresses back into active use. It was thought that by being able to monetize these addresses, holders of such addresses may have been motivated to convert their networks to use private addresses and resell their holding of public addresses. In other words, the opening of a market in addresses would provide incentive for otherwise unproductive address assets to be placed on the market. Providers who had a need for addresses would compete with other providers who had a similar need in bidding to purchase these addresses. In conventional market theory the most efficient user of addresses (here “most efficient” is based on the ability to use addresses to generate the greatest revenue) would be able to set the market price. Otherwise unused addresses would be put to productive use, and as long as demand outstrips supply the most efficient use of addresses is promoted by the actions of the market. In theory. However, the practical experience with transfers is not so clear. The data relating to address re-cycling is inconclusive, in that between 2011 and late 2017 the pool of unadvertised addresses sat between some 48 and 50 /8s. This pool of unadvertised addresses rose from the start of 2018 and by early 2020 there were just under 51 /8s that were unadvertised in the public Internet. This 2-year period of increase in the unadvertised address pool appeared to be a period where IPv4 addresses were being hoarded, though such a conclusion from just this high-level aggregate date is highly speculative. There was a substantial reduction in the size of this unadvertised address pool at the start of 2021. The major change in 2021 was the announcement in the Internet’s routing system of some seven /8s from the address space originally allocated to the US Department of Defence in the early days of the then ARPANET. At the end of 2021 AS749 originated more IPv4 addresses than any other network, namely some 211,581,184 addresses, or the equivalent of a /4.34 in prefix length notation, or some 5% of the total IPv4 address pool. Across 2022 and 2023 the previous trend of an increasing pool of unadvertised addresses resumed. On December 12 2024 a total of some 81,224,704 addresses (the equivalent of 4.8 /8s) was advertised by ASes operated by Amazon, mainly AS16509, bringing the total pool of unadvertised addresses down to a level last observed in the year 2000. Figure 5 – IPv4 Unadvertised Address Pool Size The larger picture of the three IPv4 address pool sizes, allocated, advertised and unadvertised address pools since the start of 2000 is shown in Figure 6. The onset of more restrictive address policies coincides with the exhaustion of the central IANA unallocated address pool in early 2011, and the period since that date has seen the RIRs run down their address pools. Figure 6 – IPv4 Address Pools 2000 - 2024 We can also look at 2024, looking at the changes in these address pools since the start of the year, as shown in Figure 7. The total span of advertised addresses was falling by some 20M addresses through to December 12, at which time the Amazon address range announcements changed the situation. Figure 7 – IPv4 Address Pool changes through 20214 In relative terms, expressed as a proportion of the total pool of allocated IP addresses, the unadvertised address pool peaked at 38% of the total assigned address pool in early 2003, and then declined over the ensuing 15 years to a relatively low value of 22% at the start of 2018. The ratio has been steadily climbing since that date, with abrupt falls due to the advertisement of the legacy US Department of Defence address space in 2021, and the Amazon address announcements in December 2024 (Figure 8). Figure 8 – Ratio of Unadvertised Pool Size to Total Pool Size The transfer data points to a somewhat sluggish transfer market. The number of transfer transactions is rising, but the total volume of transferred addresses is falling for most RIRs, with the exception of the RIPE NCC (Tables 4 and 5). The address market does not appear to have been all that effective in flushing out otherwise idle addresses and re-deploying them into the routed network. However, as with all other commodity markets, the market price of the commodity reflects the balancing of supply and demand and the future expectations of supply and demand. What can be seen in the price of traded IPv4 addresses over the past 10 years? One of the address brokers, Hilco Streambank's IPv4.Global, publish the historical price information of transactions (if only all the address brokers did the same, as a market with open price information for transactions can operate more efficiently and fairly than markets where price information is occluded). Figure 9 uses the Hilco Streambank IPv4.Global transaction data to produce a time series of address price. Figure 9 – IPv4 Price Time Series (data from Hilco Streambank IPv4.Global) There are a number of distinct behaviour modes in this time series data. The initial data prior to 2016 reflected a relatively low volume of transactions with stable pricing just below $10 per address. Over the ensuing 4 years, up to the start of 2019, the unit price doubled, with small blocks (/24s and /23s) attracting a price premium. The price stabilised for the next 18 months at between $20 to $25 per address, with large and small blocks trading as a similar unit price. The 18 months from mid-2020 up to the start of 2022 saw a new dynamic which was reflective of an exponential rise in prices, and the unit price lifted to between $45 and $60 per address by the end of 2021. The year 2022 saw the average market price drop across the year, but the variance in prices increased and trades at the end of the year were recorded at prices of between $40 to $60 per address. This price decline continued across 2023, and by the end of 2023 IPv4 addresses were traded at unit prices of between $26 to $40. The prices of addresses across 2024 were relatively stable, and this is shown in a view of the average unit price of addresses per month for each prefix size over 2023 and 2024 (Figure 10). Figure 10 – Average unit price per prefix per month over 2023 - 2024 If prices are reflective of supply and demand it appears that demand has increased at a far greater level than supply, and the price escalation across 2021 reflects some form of perceived scarcity premium being applied to addresses in recent times. However, the subsequent price slump shows that this perception was short-lived, and the average unit price movement of $2.50 per year appears to have been resumed in 2024. Is the supply of tradable IPv4 address declining? One way to provide some insight into answering this question is to look at the registration age of transferred addresses. Are such addresses predominately recently allocated addresses, or are they longer held address addresses where the holder is wanting to realise the inherent value in otherwise unused assets? The basic question concerns the age distribution of transferred addresses where the age of an address reflects the period since it was first allocated or assigned by the RIR system. The cumulative age distribution of transferred addresses by transaction is shown on a year-by-year basis in Figures 11 and 12. In the period 2019 – 2021 a visible subset of address holders appeared to hold recently allocated addresses for the policy-mandated minimum holding period of some 2 years, and then transfer these addresses on the market. In previous years some 8% of addresses that were transferred were originally allocated up to 5 years prior to the transfer. In 2022 this number has fallen to 4%, which is presumably related to the smaller volumes of address allocations in 2022 rather than any change in behaviours of address holders, and in 2023 and 2024 this behaviour has all but disappeared, due to the very small volume of address allocations by the RIRs rather than any change in the behaviour of address holders. The bulk of transferred addresses in 20-24 (more than 50% of the total volume) were originally allocated between 15 and 22 years ago, or between 2003 and 2010. Figure 11 – Age distribution of transferred addresses Figure 12 shows the cumulative age distribution of transfer transactions, and the disparity between the two distributions for 2024 show that recent individual allocations have been far smaller in size but are still being traded. Some 20% of the recorded transfer transactions in 2024 refer to an address prefix that was allocated within the past 5 years, yet these transactions encompass less than 2% of the inventory of transferred addresses in 2024. Some 48% of the volume of transferred addresses were originally allocated 20 or more years ago, while these transactions are recorded in just 17% of the transfers recorded in 2024. Figure 12 – Age distribution of Transfer Transactions There appear to be a number of motivations driving the transfer process. One is when demand is outstrips supply and price escalation is an inevitable consequence. This may motivate some network operators to purchase addresses early, in the expectation that further delay will encounter higher prices. This factor also may motivate some address holder to defer the decision to sell their addresses, in that delay will improve the price. Taken together, these motivations can impair market liquidity and create a feedback loop that causes price escalation. This appears to have been the case in 2021. The second factor is IPv6 deployment. Many applications prefer to use IPv6 over IPv4 if they can (the so-called “Happy Eyeballs” protocol for protocol selection). For a dual stack access network this means that the more the services that they use are provisioned with dual stack, then the lower the traffic volume that uses IPv4, and the lower the consumption pressure on their IPv4 CG-NAT address pools, which reduces their ongoing demand for IPv4 address space. This reduced demand for additional IPv4 addresses has an impact on the market price. A falling market price acts as a motivation for sellers to bring their unused address inventory to market sooner, as further delay will only result in a lower price. The overriding feature of this address market is the level of uncertainty within the market over the state of the IPv6 transition, coupled with the uncertainty over the further growth of the network. This high degree of uncertainty may lie behind the very high variance of individual transfer transaction prices, as shown in Figure 9. However, this uncertainty was resolved to some extent across 2023, and levels of demand appear to have dissipated in the past 12 months through 2024. Have we finally managed to deploy enough network infrastructure in both IPv4 and IPv6 to get ahead of the demand pressures? Are we, perhaps for the first time, looking at a market which is currently saturated with sufficient addresses and associated service platform infrastructure. Do Transfers Fragment the Address Space? The next question is whether the transfer process is further fragmenting the address space by splitting up larger address blocks into successively smaller address blocks. There are 50,942 transactions described in the RIRs’ transfer registries from the start of 2012 until the start of 2025, and of these 12,559 entries list transferred address blocks that are smaller than the original allocated block. In other words, some 25% of transfers implicitly perform fragmentation of the original allocation. These 12,559 transfer entries that have fragmented the original allocation are drawn from 6,578 original allocations. On average the original allocation is split into 2 smaller address blocks. This data implies that the answer to this question is that address blocks are being fragmented as a result of address transfers, but in absolute terms this is not a major issue. There are some 246,218 distinct IPv4 address allocation records in the RIRs registries as of the end of 2024, and the fragmentation reflected in 12,559 more specific entries of original allocation address blocks represents around 5.1% of the total pool of allocated address prefixes. Imports and Exports of Addresses The next question concerns the international flow of transferred addresses. Let’s look at the ten economies that sourced the greatest volume of transferred addresses, irrespective of their destination (i.e. including ‘domestic’ transfers within the same economy) (Table 6), and the ten largest recipients of transfers (Table 7), and the ten largest international address transfers (Table 8). We will use the RIR-published transfer data for the year 2024 as basis for these tables. Rank CC Addresses Source Economy 1 US 8,136,704 USA 2 GB 3,013,120 UK 3 PL 2,682,880 Poland 4 RO 1,783,296 Romania 5 JP 1,739,264 Japan 6 DE ,1509,120 Germany 7 NL ,1117,184 Netherlands 8 AU 1,069,824 Australia 9 IR 975,616 Iran 10 CH 905,728 Switzerland 11 UA 838,016 Ukraine 12 NO 674,304 Norway 13 RU 663,808 Russian Federation 14 IT 525,888 Italy 15 FR 464,000 France 16 BG 373,760 Bulgaria 17 LV 357,632 Latvia 18 DK 256,256 Denmark 19 NZ 250,368 New Zealand 20 ES 212,736 Spain Table 6 – Top 20 Countries Sourcing Transferred IPv4 addresses in 2024 Rank CC Addresses Destination Economy 1 GB 7,526,912 UK 2 US 5,853,440 USA 3 PL 2,017,536 Poland 4 DE 1,620,992 Germany 5 RO 1,510,656 Romania 6 NL 962,816 Netherlands 7 JP 817,408 Japan 8 IR 800,768 Iran 9 AU 702,720 Australia 10 NO 670,464 Norway 11 SE 643,584 Sweden 12 IT 564,800 Italy 13 RU 537,088 Russian Federation 14 FR 493,952 France 15 ES 493,568 Spain 16 UA 397,184 Ukraine 17 BG 357,376 Bulgaria 18 LV 354,816 Latvia 19 SG 301,568 Singapore 20 DK 261,888 Denmark Table 7 – Top 20 Countries Receiving Transferred IPv4 addresses in 2024 There are many caveats about this data collection, particularly relating to the precise meaning of this economy-based geolocation. Even if we use only the country-code entry in the RIRs’ registry records, then we get a variety of meanings. Some RIRs use the principle that the recorded country code entry corresponds to the physical location of the headquarters of nominated entity that is the holder of the addresses, irrespective of the locale where the addresses are used on the Internet. Other RIRs allow the holder to update this geolocation entry to match the holder’s intended locale where the addresses will be used. It is generally not possible to confirm the holder’s assertion of location, so whether these self-managed records reflect the actual location of the addresses or reflect a location of convenience is not always possible to determine. When we look at the various geolocation services, of which Maxmind is a commonly used service, there are similar challenges in providing a geographic location service. At times this is not easy to establish, such as with tunnels used in VPNs. Is the “correct” location the location of the tunnel ingress or tunnel egress? Many of the fine-grained differences in geolocation services reflect the challenges in dealing with VPNs and the various ways these location services have responded. There is also the issue of cloud-based services. Where the cloud service uses anycast, then the address is located in many locations at once. In the case where the cloud uses conventional unicast, the addresses use may be fluid across the cloud service’s points of presence based on distributing addresses to meet the demands for the service. The bottom line is that these location listings are a “fuzzy” approximation rather than a precise indication of location. With that in mind let’s now look at imports and exports of addresses of 2024 transfers where the source and destination of the transfers are in different economies. Some 1,928 transfers appear to result in a movement of addresses between countries, involving a total of 11,467,008 addresses, as shown in Table 8. Rank From To Addresses (M) Source Destination 1 US GB 4,845,568 USA UK 2 PL US 656,640 Poland USA 3 JP US 646,144 Japan USA 4 GB SE 533,248 UK Sweden 5 CH US 459,008 Switzerland USA 6 UA US 448,000 Ukraine USA 7 NL GB 264,704 Netherlands UK 8 RO ES 262,656 Romania Spain 9 AU DE 262,144 Australia Germany 10 GB US 254,208 UK USA 11 US SG 144,640 USA Singapore 12 JP GB 131,072 Japan UK 13 DE US 125,440 Germany USA 14 IR AE 98,304 Iran UAE 15 DE SE 68,608 Germany Sweden 16 AU GB 67,840 Australia UK 17 US RU 67,328 USA Russian Federation 18 NZ US 65,792 New Zealand USA 19 CA DE 65,536 Canada Germany 20 CH IE 65,536 Switzerland Ireland Table 8 – Top 20 Economy-to-Economy IPv4 address transfers in 2024 The 2024 transfer logs also contain 4,257 domestic address transfers, with a total of 18,719,040 addresses, with the largest activity by address volume in domestic transfers in the USA (29M), UK (20M), Poland (2M), Romania (1.5M) and Germany (1.2M). An outstanding question about this transfer data is whether all address transfers that have occurred have been duly recorded in the registry system. This question is raised because registered transfers require conformance to various registry policies, and it may be the case that only a subset of transfers are being recorded in the registry as a result. This can be somewhat challenging to detect, particularly if such a transfer is expressed as a lease or other form of temporary arrangement, and if the parties agree to keep the details of the transfer confidential. It might be possible to place an upper bound on the volume of address movements that have occurred in any period is to look at the Internet’s routing system. One way to shed some further light on what this upper bound on transfers might be is through a simple examination of the routing system, looking at addresses that were announced in 2024 by comparing the routing stable state at the start of the year with the table state at the end of the year (Table 9).Jan-24 Jan-25Delta Unchanged Re-Home Removed Added Announcements 944,174 995,96351,789 838,184 24,561 81,879 133,128 Root Prefixes: 456,574 469,27212,725 403,293 17,594 29,275 48,015 Address Span (M) 3,045.25 3,117.6572.69 3,880.42 37.93 105.46 225.25 More Specifics: 487,627 526,69139,064 434,891 6,687 52,604 85,113 Address Span (M) 846.95 899.7552.80 728.76 9.56 86.79 116.65 Table 9 – IPv4 BGP changes over 2024 While the routing table grew by 51,789 entries over the year, the nature of the change is slightly more involved. Some 81,879 prefixes that were announced at the start of the year were removed from the routing system at some time through the year, and 133,128 prefixes were announced by the end of the year that were not announced at the start of the year. More transient prefixes may have appeared and been withdrawn throughout the year of course, but here we are comparing two snapshots rather than looking at every update message. A further 24,561 prefixes had changed their originating AS number, indicating some form of change in the prefix’s network location in some manner. If we look at the complete collection of BGP updates seen from an individual BGP vantage point (AS 131072) across all of 2024 we see a larger collection of transient address prefixes. A total of 1,830,657 distinct prefixes were observed through 2024. That implies that some 886,483 additional prefixes were seen at some point through the year, from the initial set at the start of the year. We can compare these prefixes that changed in 2024 against the transfer logs for the two-year period 2023 and 2024. Table 10 shows the comparison of these routing numbers against the set of transfers that were logged in these two years. Type Listed Unlisted Ratio Re-HomedAll 955 23,011 4.0% Root Prefixes 459 11,172 3.9% RemovedAll 971 80,098 1.2% Root Prefixes 464 28,811 1.6% AddedAll 2,539 130,589 1.9% Root Prefixes 1,872 46,143 3.9% Table 10 – Routing changes across 2024 compared to the Transfer Log Entries for 2023 - 2024 These figures show that some 1% - 4% of changes in advertised addresses from the beginning to the end of the year are reflected as changes as recorded in the RIRs’ transfer logs. This shouldn’t imply that the remaining changes in advertised prefixes reflect unrecorded address transfers. There are many reasons for changes in the advertisement of an address prefix and a change in the administrative controller of the address is only one potential cause. However, it does establish some notional upper ceiling on the number of movements of addresses in 2024, some of which relate to transfer of operational control of an address block, that have not been captured in the transfer logs. Finally, we can perform an age profile of the addresses that were added, removed and re-homed during 2024 and compare it to the overall age profile of IPv4 addresses in the routing table. This is shown in Figure 13. This figure show that address prefixes added to the routing table tend to be “younger” than average. One half of all added address prefixes are 12 years old, while for all advertised address prefixes 50% of such prefixes are 17 years old or younger. Prefixes that are changing their originating network (“re-Homing”) tend to be older than average, and 50% of all rehomed prefixes are 22 years old or older. Figure 13 – Changes to the BGP routing table across 2024 by Address Prefix Age As IPv4 moves into its final stages we are perhaps now in a position to take stock of the overall distribution of IPv4 addresses and look at where the addresses landed up. Table 11 shows the 20 economies that have the largest pools of allocated IPv4 addresses. However, I have to note that the assignation of a country code in an address registration reflects the country where address holder is located (the corporate location), and not necessarily the country where the addresses will be deployed. Rank CC IPv4 Pool % Total Per-Capita Economy 1 US 1,611,699,808 43.7% 4.70 USA 2 CN 343,161,088 9.3% 0.24 China 3 JP 189,000,960 5.1% 1.55 Japan 4 GB 134,198,664 3.6% 1.97 UK 5 DE 124,111,552 3.4% 1.49 Germany 6 KR 112,504,320 3.1% 2.18 Republic of Korea 7 BR 87,145,472 2.4% 0.40 Brazil 8 FR 82,068,336 2.2% 1.26 France 9 CA 67,990,016 1.8% 1.73 Canada 10 IT 54,034,240 1.5% 0.92 Italy 11 NL 48,072,288 1.3% 2.72 Netherlands 12 AU 46,469,376 1.3% 1.73 Australia 13 RU 44,903,168 1.2% 0.31 Russian Federation 14 IN 41,661,952 1.1% 0.03 India 15 TW 35,718,656 1.0% 1.49 Taiwan 16 ES 32,261,184 0.9% 0.68 Spain 17 SE 31,078,888 0.8% 2.90 Sweden 18 MX 28,994,816 0.8% 0.22 Mexico 19 ZA 27,085,824 0.7% 0.44 South Africa 20 SG 26,597,376 0.7% 4.38 Singapore3,687,384,024 100% 0.45 World Table 11 – IPv4 Allocated Address Pools per National Economy If we divide this address pool by the current population of each national entity, then we can derive an address per capita index. The global total of 3.69 billion allocated addresses with an estimated global population of 8 billion people gives an overall value of 0.45 IPv4 addresses per capita. Rank CC IPv4 Pool % Total Per-Capita1 SC 7,475,456 0.2% 68.86 Seychelles 2 VA 10,752 0.0% 20.33 Holy See 3 GI 235,264 0.0% 7.19 Gibraltar 4 VG 166,144 0.0% 5.21 British Virgin Islands 5 US 1,611,699,808 43.7% 4.70 USA 6 SG 26,597,376 0.7% 4.38 Singapore 7 MU 4,780,032 0.1% 3.67 Mauritius 8 SE 31,078,888 0.8% 2.90 Sweden 9 CH 25,444,472 0.7% 2.87 Switzerland 10 NO 15,542,032 0.4% 2.81 Norway3,687,384,032 100.0% 0.45 World Table 12 – IPv4 Allocated Address Pools ranked by per-Capita holdings IPv4 Address Leasing It is worth noting that the address market includes leasing as well as sales. Should an entity who requires IPv4 addresses enter the market and perform an outright purchase of the addresses from an existing address holder, or should they execute a timed leased to have the use of these addresses for a specified period and presumably return these addresses at the end of the lease? This lease versus buy question is a very conventional question in market economics and there are various well-rehearsed answers to the question. They tend to relate to the factoring of market information and scenario planning. If a buyer believes that the situation that led to the formation of a market will endure for a long time, and the goods being traded on the market are in finite supply while the level of demand for these goods is increasing, then the market will add an escalating scarcity premium to the price goods being traded. The balancing of demand and supply becomes a function of this scarcity premium imposed on the goods being traded. Goods in short supply tend to become more expensive to buy over time. A holder of these goods will see an increase in the value of the goods that they hold. A lessee will not. If a buyer believes that the market only has a short lifespan, and that demand for the good will rapidly dissipate at the end of this lifespan, then leasing the good makes sense, in so far as the lessee is not left with a valueless asset when the market collapses. Scarcity also has several additional consequences, one of which is the pricing of substitute goods. At some point the price of the original good rises to the point that substitution looks economically attractive, even if the substitute good has a higher cost of production or use. In fact, this substitution price effectively sets a price ceiling for the original scarce good. Some commentators have advanced the view that an escalating price for IPv4 increases the economic incentive for IPv6 adoption, and this may indeed be the case. However, there are other potential substitutes that have been used, most notably NATs (Network Address Translators). While NATs do not eliminate the demand pressure for IPv4, they can go a long way to increase the address utilisation efficiency if IPv4 addresses. NATs allow the same address to be used by multiple customers at different times. The larger the pool of customers that share a common pool of NAT addresses the greater the achievable multiplexing capability. The estimate as to how long the market in IPv4 addresses will persist is effectively a judgement as to how long IPv4 and NATs can last and how long it will take IPv6 to sufficiently deployed to be viable as an IPv6-only service. At that point in time there is likely to be a tipping point where the pressure for all hosts and networks to support access to services over IPv4 collapses. A that point, the early IPv6-only adopters can dump all their remaining IPv4 resources onto the market as they have no further need for them, which would presumably trigger a level of market panic to emerge as existing holders are faced with the prospect of holding a worthless asset and are therefore under pressure to sell off their IPv4 assets while there are still buyers in the market. While a significant population of IPv4-only hosts and networks can stall this transition and increase scarcity pressure, if the scarcity pressure becomes too great the impetus of IPv6-only adoption increases to the level that the IPv6-connected base achieves market dominance. When this condition is achieved the IPv4 address market will quickly collapse. IPv6 in 2024 Obviously, the story of IPv4 address allocations is only half of the story, and to complete the picture it’s necessary to look at how IPv6 has fared over 2024. IPv6 uses a somewhat different address allocation methodology than IPv4, and it is a matter of choice for a service provider as to how large an IPv6 address prefix is assigned to each customer. The original recommendations published by the IAB and IESG in 2001, documented in RFC3177, envisaged the general use of a /48 prefix as a generally suitable end-site prefix. Subsequent consideration of long term address conservation saw a more flexible approach being taken with the choice of the end site prefix size being left to the service provider. Today's IPv6 environment has some providers using a /60 end site allocation unit, many using a /56, and many other providers using a /48. This variation makes a comparison of the count of allocated IPv6 addresses somewhat misleading, as an ISP using /48's for end sites will require 256 times more address space to accommodate a similarly sized same customer base as a provider who uses a /56 end site prefix, and 4,096 times more address space than an ISP using a /60 end site allocation! For IPv6 let's use both the number of discrete IPv6 allocations and the total amount of space that was allocated to see how IPv6 fared in 2024. Comparing 2023 to 2024, the number of individual allocations of IPv6 address space has increased by 1%, while the number of IPv4 allocation transactions has increased by 3% (Table 13). Allocations 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 IPv6 3,582 3,291 3,529 4,502 4,644 5,567 5,740 6,176 6,799 5,376 5,350 4,066 3,874 3,925 IPv4 8,234 7,435 6,429 10,435 11,352 9,648 8,185 8,769 12,560 5,874 6,939 4,395 3,462 3,559 Table 13 - Number of individual Address Allocations, 2011 - 2024 The amount of IPv6 address space distributed in 2024 is 60% less than the amount that was allocated in 2023, while the corresponding IPv4 volume has increased by 13% (Table 14). Addresses 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 IPv6 (/32s) 14,986 17,710 23,642 17,847 15,765 25,260 19,975 38,699 35,924 21,620 28,131 27,497 74,159 45,105 IPv4 (/32s)(M) 191.7 88.8 57.7 58.8 32.3 20.8 15.1 14.1 13.9 4.2 3.1 2.1 1.6 1.8 Table 14 – Volume of Address Allocations, 2011 – 2024 Regionally, each of the RIRs saw IPv6 allocation activity in 2024 that was on a par with those seen in 2023, but well short of the peak period of IPv6 allocation activity in 2018 - 2019 (Table 15). Allocations 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 AFRINIC 129 82 72 59 81 111 110 108 111 108 135 151 115 117 APNIC 641 599 540 528 777 1,680 1,369 1,460 1,484 1,498 1,392 1,317 1,381 1,265 ARIN 1,035 603 543 489 604 645 684 648 601 644 668 680 712 951 LACNIC 130 251 223 1,199 1,053 1,007 1,547 1,439 1,614 1,801 725 635 612 656 RIPENCC 1,647 1,756 2,151 2,227 2,129 2,124 2,030 2,521 2,989 1,325 2,430 1,283 1,054 9363,582 3,291 3,529 4,502 4,644 5,567 5,740 6,176 6,799 5,376 5,350 4,066 3,874 3,925 Table 15 - IPv6 allocations by RIR The address assignment data tells a slightly different story. Table 16 shows the number of allocated IPv6 /32's per year. The 2024 allocation IPv6 address volume in APNIC was extremely large, with the allocation of a /17 prefix (2410::) to Huawei International in November 2024 being the major part of the annual allocation volume. Addresses (/32s) 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 AFRINIC 155 4,201 66 48 308 76 112 71 360 88 141 387 400 380 APNIC 9,506 3,807 4,462 2,663 2,108 1,235 4,228 19,681 7,945 7,365 10,185 4,856 599 33,257 ARIN 2,280 1,672 12,571 5,214 642 1,087 1,372 844 5,520 4,975 373 13,695 66,340 5,692 LACNIC 620 4,301 158 1,314 953 1,173 1,427 1,327 1,496 1,669 658 563 467 575 RIPENCC 2,425 3,729 6,385 8,608 11,754 21,689 12,836 16,776 20,603 7,523 16,774 7,996 6,353 5,11114,986 17,710 23,642 17,847 15,765 25,260 19,975 38,699 35,924 21,620 28,131 27,497 74,159 45,015 Table 16 - IPv6 address allocation volumes by RIR Dividing addresses by allocations gives the average IPv6 allocation size in each region (Table 17). Overall, the average IPv6 allocation size is a /28, with the RIPE NCC and ARIN averaging larger individual IPv6 allocations than the other RIRs. Don't forget that a /31 is twice the size of a /32, and if you take the average allocation of the smallest of the RIR averages, LACNIC's 656 allocations with a total of 575 /32s in 2024 as the unitary value, then the 2024 average IPv6 allocation made by AFRINIC was 34.7 times larger, the RIPE NCC was 6.2 times larger, ARIN was 6.9 times larger, and APNIC was 30 times larger. APNIC was an anomaly in 2024 due to the single extremely large allocation, but the variance across the other four RIRs appears to indicate some regional differences in the way IPv6 address allocations are processed by each RIR.2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 AFRINIC /31.7 /26.3 /32.1 /32.3 /30.1 /32.5 /32.0 /32.6 /30.3 /32.3 /31.9 /30.6 /30.2 /30.3 APNIC /28.1 /29.3 /29.0 /29.7 /30.6 /32.4 /30.4 /28.2 /29.6 /29.7 /29.1 /30.1 /33.2 /27.3 ARIN /30.9 /30.5 /27.5 /28.6 /31.9 /31.2 /31.0 /31.6 /28.8 /29.1 /32.8 /27.7 /25.5 /29.4 LACNIC /29.7 /27.9 /32.5 /31.9 /32.1 /31.8 /32.1 /32.1 /32.1 /32.1 /32.1 /32.2 /32.4 /32.2 RIPENCC /31.4 /30.9 /30.4 /30.0 /29.5 /28.6 /29.3 /29.3 /29.2 /29.5 /29.2 /29.4 /29.4 /29.6/29.9 /29.6 /29.3 /30.0 /30.2 /29.8 /30.2 /29.4 /29.6 /30.0 /29.6 /29.2 /27.7 /28.5 Table 17 – Average IPv6 address allocation size by RIR The number and volume of IPv6 allocations per RIR per year is shown in Figures 14 and 15. Figure 14 – Number of IPv6 Allocations per year Figure 15 – Volume of IPv6 Allocations per year It might be tempting to ascribe the decline in 2020 of IPv6 allocations from the RIPE NCC to the year where many European countries were hit hard by COVID-19 measures. Arguing against that is the observation that countries all over the world have been similarly affected, yet the decline in IPv6 allocation activity in 2020 is only seen in the data from the RIPE NCC. However, it’s an interesting question to ask as to why the IPv6 address allocation activity has slumped in many European economies in subsequent years. Has the momentum for transition to IPv6 waned in that region? (Tables 18 and 19). Rank 2019202020212022202320241 Brazil 1,112 Brazil 1,394 USA 619 USA 638 USA 691 USA 889 2 USA 538 USA 588 Russia 576 India 377 India 424 Brazil 302 3 Russia 502 Indonesia 389 Brazil 508 Brazil 339 Brazil 267 India 269 4 Germany 407 India 226 Netherlands 448 Bangladesh 239 Indonesia 198 Vietnam 233 5 Indonesia 366 Netherlands 199 India 390 Germany 158 Bangladesh 159 Indonesia 169 6 Netherlands 342 Germany 192 UK 304 Russia 138 Vietnam 143 Bangladesh 156 7 UK 223 Bangladesh 182 Bangladesh 213 UK 125 Germany 126 Germany 119 8 Bangladesh 202 Russia 128 Germany 196 Indonesia 113 Colombia 99 Iran 86 9 France 179 Australia 118 Indonesia 110 Australia 100 Mexico 96 Australia 83 10 China 165 China 115 Hong Kong 108 Vietnam 91 UK 85 Mexico 83 Table 18 - IPv6 allocations by Year by Economy Table 18 shows the countries who received the largest number of individual IPv6 allocations, while Table 19 shows the amount of IPv6 address space assigned on a per economy basis for the past 4 years (using units of /32s). Rank 2019202020212022202320241 China 6,787 China 6,765 China 5,424 USA 13,919 USA 66,579 Singapore 32,792 2 USA 5,510 USA 5,051 Russia 4,409 China 4,354 Lithuania 522 USA 5,533 3 Russia 3,716 Brazil 1,358 India 4,281 Russia 925 UK 513 Iran 643 4 Germany 2,522 Netherlands 1,331 Netherlands 3,390 UK 734 Germany 478 Germany 426 5 Netherlands 2,516 Germany 716 UK 2,249 Germany 706 Russia 371 Lithuania 410 6 UK 1,355 Russia 715 Germany 896 Moldava 456 Ukraine 369 UK 352 7 France 1,182 UK 552 Ukraine 651 France 404 Iran 314 Turkey 328 8 Italy 1,052 Italy 391 Lithuania 633 Netherlands 397 France 276 South Africa 304 9 Brazil 1,049 France 390 Brazil 502 Italy 363 Seychelles 258 Canada 292 10 Spain 854 Turkey 290 USA 491 Brazil 328 Rwanda 256 Brazil 289 Table 19 - IPv6 Address Allocation Volumes by Year by Economy (/32s) We can also look at the total IPv6 allocated address pools for top 25 IPv6-holding national economies (Table 20). While the United States tops this list of the total pool of allocated IPv6 addresses, with some 31% of the total span of allocated IPv6 addresses, the per capita number is lower than others in this list (Netherlands, Sweden, Switzerland). In 2023 ARIN allocated a /16 address block to Capital One Financial Cooperation, one of the larger banks in the United States with a large credit card base in retail operations. In 2024 APNIC allocated a /17 to Huawei International, with a corporate location in Singapore. Rank CC Allocated (/48s) % Total /48s p.c. Advertised /48s Deployment Name 1 US 9,213,434,648 29.1% 26.9 1,210,870,245 11.5% USA 2 CN 4,225,433,687 13.3% 3.0 1,664,263,872 15.9% China 3 SG 2,257,203,689 7.1% 371.7 9,445,725 0.1% Singapore 4 DE 1,585,120,019 5.0% 19.0 1,054,733,850 10.1% Germany 5 GB 1,408,106,725 4.4% 20.7 405,638,808 3.9% UK 6 FR 993,735,334 3.1% 15.3 187,857,414 1.8% France 7 RU 727,187,782 2.3% 5.1 244,298,888 2.3% Russia 8 NL 714,342,719 2.3% 40.4 341,924,543 3.3% Netherlands 9 IT 698,748,971 2.2% 11.9 436,310,346 4.2% Italy 10 JP 669,589,718 2.1% 5.5 512,858,598 4.9% Japan 11 AU 621,872,441 2.0% 23.2 310,137,856 3.0% Australia 12 BR 547,850,208 1.7% 2.5 427,490,414 4.1% Brazil 13 SE 458,293,609 1.4% 42.8 90,703,579 0.9% Sweden 14 IN 441,714,156 1.4% 0.3 373,461,523 3.6% India 15 PL 407,306,495 1.3% 10.2 234,560,448 2.2% Poland 16 ES 405,405,742 1.3% 8.5 112,156,049 1.1% Spain 17 SC 374,145,029 1.2% 3446.3 124,140,536 1.2% Seychelles 18 ZA 349,639,870 1.1% 5.7 317,857,002 3.0% South Africa 19 KR 345,833,482 1.1% 6.7 2,559,149 0.0% Korea 20 AR 345,639,028 1.1% 7.5 286,387,153 2.7% Argentina 21 TR 273,940,512 0.9% 3.2 55,964,935 0.5% Turkey 22 EG 271,384,581 0.9% 2.4 270,925,826 2.6% Egypt 23 AE 265,617,419 0.8% 27.6 215,351,617 2.1% UAE 24 CH 263,586,240 0.8% 29.7 122,323,090 1.2% Switzerland 25 IR 248,315,915 0.8% 2.8 55,300,289 0.5% Iran Table 20 – IPv6 Allocated Address pools per National Economy – December 2023 Some twenty years ago it was common practice to point out the inequities in the state of IPv4 address deployment. At the time, some US universities had more IPv4 addresses at their disposal than some highly populated developing economies, and the disparity was a part of the criticism of the address management practices that were used at the time. The RIR system was intended to address this issue of predisposition to a biased outcome. The concept behind the system that within the regional community each community had the ability to develop their own address distribution policies and could determine for themselves what they meant by such terms as “fairness” and “equity” and then direct their regional address registry to implement these policies. While IPv4 had a very evident early adopter reward, in that the address allocations in the IPv4 class-based address plan could be quite extravagant, the idea was that in IPv6, where the address allocations were developed from the outset through local bottom-up policy determinate frameworks, such evident inequities in the outcome would be avoided, or so it was hoped. It was also envisaged that with such a vast address plan provided by 128 bits of address space, the entire concept of scarcity and inequity would be largely irrelevant. 2128 is a vast number and the entire concept of comparison between two vast pools of addresses is somewhat irrelevant. So, when we look at the metric of /48s per head of population, don’t forget that a /48 is actually 80 bits of address space, which is massively larger than the entire IPv4 address space. Even India’s average of 0.3 /48s per capita is still a truly massive number of IPv6 addresses! However, before we go too far down this path it is also useful to bear in mind that the 128 bits of address space in IPv6 has become largely a myth. We sliced off 64 bits in the address plan for no particularly good reason, as it turns out. We then sliced off a further 16 bits for again no particularly good reason. 16 bits for end site addresses allows for some 65,000 distinct networks within each site, which is somewhat outlandish in pretty much every case. The result is that the vastness of the address space represented by 128 bits in IPv6 is in fact not so vast in practice. The usable address prefix space in IPv4 roughly equates a /32 end address in IPv4 with around a /48 prefix in IPv6. So perhaps this comparison of /48s per capita is not entirely fanciful, and there is some substance to the observation that there are inequities in the address distribution in IPv6 so far. However, unlike IPv4, the exhaustion of the IPv6 address space is still quite some time off, and we still believe that there are sufficient IPv6 addresses to support a uniform address utilisation model across the entire world of silicon over time. There is a larger question about the underlying networking paradigm in today’s public network. IPv6 attempts to restore the 1980’s networking paradigm of a true peer-to-peer network where every connected device is capable of sending packets to any other connected device. However, today’s networked environment regards such unconstrained connectivity as a liability. Exposing an end client device is regarded as being unnecessarily foolhardy, and today’s network paradigm relies on client-initiated transactions. This is well-suited to NAT-based IPv4 connectivity, and the question regarding the long-term future of an IPv6 Internet is whether we want to bear the costs of maintaining end-client unique addressing plans, or whether NATs in IPv6 might prove to be a most cost-effective service platform for the client side of client/server networks. To what extent are allocated IPv6 addresses visible as advertised prefixes in the Internet’s routing table? Figure 16 shows the overall counts of advertised, unadvertised and total allocated address volume for IPv6 since 2010, while Figure 17 shows the advertised address span as a percentage of the total span of allocated and assigned IPv6 addresses. Figure 16 – Allocated, Unadvertised and Advertised IPv6 addresses Figure 17 – Unadvertised Address Ratio for IPv6 and IPv4 The drop in the allocated address span in 2013 is the result of a change in LACNIC where a single large allocation into Brazil was replaced by the recording of direct allocation and assignments to ISPs and similar end entities. From a history of careful conservation of IPv4 addresses, where some 85% of allocated or assigned IPv4 addresses are advertised in the BGP routing table, a comparable IPv6 figure of 34% does not look all that impressive. But that's not the point. We chose the 128-bit address size in IPv6 to allow addresses to be used without overriding concerns about conservation. We’re allowed to be inefficient in address utilisation in IPv6! At the start of 2025 we’ve advertised an IPv6 address span which is the equivalent of some 160,000 /32s, or some 10.5 billion end-site /48 prefixes. That is just 0.0037% of the total number of /48 prefixes in IPv6. The Outlook for the Internet Once more the set of uncertainties that surround the immediate future of the Internet are considerably greater than the set of predictions that we can be reasonably certain about. The year 2017 saw a sharp rise in IPv6 deployment, influenced to a major extent by the deployment of IPv6 services in India, notably by the Reliance Jio mobile service. The next year, 2018, was a quieter year, although the rise in the second half of the year is due to the initial efforts of mass scale IPv6 deployment by some major Chinese service providers. This movement accelerated in 2019 and the overall move of a further 5% in IPv6 deployment levels had a lot to do with the very rapid rise of the deployment of IPv6 in China. There has been an ongoing rise in the level of IPv6 within China, and the measured level of IPv6 has risen from 32% of the user base to 42% over 2024, or an expansion of the Chinese IPv6 user pool by some 80M end clients over the year. Figure 18 – IPv6 Deployment measurement 2010 – 2024 In 204 the growth patterns for IPv6 are more diffuse around the world with a 3.7% overall growth rate, although there has been steady growth in IPv6 deployment in Mongolia (42%), Bhutan (34%) and Nepal (55%) (Figure 19). Figure 19 – IPv6 Deployment measurement - December 2024 While a number of service operators have reached the decision point that the anticipated future costs of NAT deployment are unsustainable for their service platform, there remains a considerable school of thought that says that NATs will cost effectively absorb some further years of Internet population growth. At least that's the only rationale I can ascribe to a very large number of service providers who are making no visible moves to deploy Dual-Stack services at this point in time. Given that the ultimate objective of this transition is not to turn on Dual-Stack everywhere, but to turn off IPv4, there is still some time to go, and the uncertainty lies in trying to quantify what that time might be. The period of the past decade has been dominated by the mass marketing of mobile internet services, and the Internet’s growth rates for 2014 through to 2016 perhaps might have been the highest so far recorded. This would’ve been visible in the IP address deployment data were it not for the exhaustion of the IPv4 address pool. In address terms this growth in the IPv4 Internet is being almost completely masked by the use of Carrier Grade NATs in the mobile service provider environment, so that the resultant demands for public addresses in IPv4 are quite low and the real underlying growth rates in the network are occluded by these NATs. In IPv6 the extremely large size of the address space masks out much of this volume. A single IPv6 /20 allocation to an ISP allows for 268 million /48 allocations, or 68 billion /56 allocations, so much of the growth in IPv6-using networks is simply hidden behind the massive address plan that lies behind IPv6. It has also been assumed that we should see IPv6 address demands for deployments of large-scale sensor networks and other forms of deployments that are encompassed under the broad umbrella of the Internet of Things. This does not necessarily imply that the deployment is merely a product of an over-hyped industry, although that is always a possibility. It is more likely to assume that, so far, such deployments are taking place using private IPv4 addresses, and they rely on NATs and application-level gateways to interface to the public network. Time and time again we are lectured that NATs are not a good security device, but in practice NATs offer a reasonable front-line defence against network scanning malware, so there may be a larger story behind the use of NATs and device-based networks than just a simple conservative preference to continue to use an IPv4 protocol stack. More generally, we are witnessing an industry that is no longer using technical innovation, openness and diversification as its primary means of expansion. The widespread use of NATs in IPv4 limit the technical substrate of the Internet to a very restricted model of simple client/server interactions using TCP and UDP. The use of NATs force the interactions into client-initiated transactions, and the model of an open network with considerable flexibility in the way in which communications take place is no longer being sustained in today’s network. Incumbents are entrenching their position and innovation and entrepreneurialism are taking a back seat while we sit out this protracted IPv4/IPv6 transition. What is happening is that today's internet carriage service is provided by an ever-smaller number of very large players, each of whom appear to be assuming a very strong position within their respective markets. The drivers for such larger players tend towards risk aversion, conservatism and increased levels of control across their scope of operation. The same trends of market aggregation are now appearing in content provision platforms, where a small number of platform operators are exerting a completely dominant position across the entire Internet. The evolving makeup of the Internet industry has quite profound implications in terms of network neutrality, the separation of functions of carriage and service provision, investment profiles and expectations of risk and returns on infrastructure investments, and on the openness of the Internet itself. Given the economies of volume in this industry, it was always going to be challenging to sustain an efficient, fully open and competitive industry that was capable of sustaining both large and small operators, but the degree of challenge in this agenda is multiplied many-fold when the underlying platform has run out of the basic currency of IP addresses. The pressures on the larger players within these markets to leverage their incumbency into overarching control gains traction when the stream of new entrants with competitive offerings dries up, and the solutions in such scenarios typically involve some form of public sector intervention directed to restore effective competition and revive the impetus for more efficient and effective offerings in the market. As the Internet continues to evolve, it is no longer the technically innovative challenger pitted against venerable incumbents in the forms of the traditional industries of telephony, print newspapers, television entertainment and social interaction. The Internet is now the established norm. The days when the Internet was touted as a poster child of disruption in a deregulated space are long since over, and these days we appear to be increasingly looking further afield for a regulatory and governance framework that can challenge the increasing complacency of the very small number of massive digital incumbents. It is unclear how successful we will be in this search for responses to this oppressive level of centrality in many aspects of the digital environment. We can but wait and see. Disclaimer The above views do not necessarily represent the views of the Asia Pacific Network Information Centre. About the Author GEOFF HUSTON AM B.Sc., M.Sc., is the Chief Scientist at APNIC, the Regional Internet Registry serving the Asia Pacific region. www.potaroo.net",
    "commentLink": "https://news.ycombinator.com/item?id=42670468",
    "commentBody": "IP addresses through 2024 (potaroo.net)145 points by DanAtC 17 hours agohidepastfavorite71 comments ironhaven 13 hours agoI feel like this blog does not accurately describe how large the ipv6 address space even after accounting for it being reduced by 2^64 for the host portion of the address. If it did it would make the concerned comments about /28 ipv6 network sizes seem very misplaced. A single static ipv4 address is a /32 slice into the ipv4 space and is considered a reasonable to size give out to a single person or even a small business that asks for it. Of course larger companies and telecom operators need larger network allocations and they have gotten them for many years in the past. Now realize that if a /64 ipv6 network is the minimum size like a single ipv4 address then you see that the ipv6 address space has 2^32 /32 ipv6 networks. Now with ipv6 any technical person and acquire a entire ipv4 internet sized network in a continuous (globally routeable if you want) range. And if any sized business today can expect buy a single \"class c\" /24 range of ipv4 it makes sense that large global compaines get a ipv6 /20 network to run their entire network on it.For example cloudflare[1] uses 6 regional /32 networks and a /29 network for all their routing needs. Imaging trying to build cloudflare with less than 32 addresses in a single /24 ipv4 allocation. ipv6 is so large that you can just design your network without worrying about subnet size and route based on real policy or security boundaries alone. We will run out of MAC addresses before we run out of ipv6 [1] https://www.cloudflare.com/ips/ reply mike_d 11 hours agoparentPeople get caught up in how many IPv6 addresses exist, as if we will always have enough to go around. Recently Capital One was assigned a /16, enough for every ATM in the US to be its own ISP and make its own customer allocations. We continue to repeat the same mistakes that made people believe we needed v6 in the first place. We will never run out of IPv6, but I believe in my lifetime you won't be able to get new v6 allocations anymore because they will have all been handed out to the large corporations with deep pockets. Oops, no addresses left - but you can rent one from a cloud provider for a few dollars a month. reply cesarb 8 hours agorootparent> We continue to repeat the same mistakes that made people believe we needed v6 in the first place. The IPv6 designers already thought of that and prepared for that eventuality. Only 1/8 of all available IPv6 addresses (the ones starting with the bits 001) is available for allocation under the current rules. If and when that gets exhausted, there will still be more than 5/8 of all IPv6 addresses available to be allocated under more strict rules. (See https://www.iana.org/assignments/ipv6-address-space/ipv6-add... and https://www.rfc-editor.org/rfc/rfc3513.html#section-4 for more details.) reply wpm 11 hours agorootparentprevThen, we’ll make IPv8, with 2^1024 address, which will be incompatible with IPv6 and IPv4 and all software on release, and will take decades before any appreciable uptake. Addresses will use hexadecimal, as well as the entire 4-byte Unicode table of emoji and Egyptian hieroglyphics, with octet separated by $ and { symbols for heightened readability. reply tolien 7 hours agorootparentYet there will still be comments asking why IPv8 can't be IPv4 with another few sets of digits tagged onto the end, as if that would somehow be compatible with the current implementation of IPv4. reply IgorPartola 5 hours agorootparentI swear it is easier to argue with flat earthers than with people who want IPv4 but with more addresses. reply zeristor 10 hours agorootparentprevI believe IPv8 does something else: https://py-ipv8.readthedocs.io/en/latest/ reply tribler 4 hours agorootparentIPv8 dev here. Thnx for mentioning this. IETF draft standard of IPv8: https://datatracker.ietf.org/doc/html/draft-pouwelse-trustch... Bonus: \"De-DSI\" == LLM- based fuzzy search + IPv8 reply DonHopkins 1 hour agorootparentThe time the think of deploying IPv8 is BEFORE you've deployed something else. https://www.youtube.com/watch?v=qYo0lVVH2wU reply immibis 10 hours agorootparentprevNo, we won't do that. reply gorgoiler 10 hours agorootparentprevA /16 is the first quarter of the network prefix. That is indeed like assigning an IPv4 /8 to General Electric in 1985 and then wondering why, 40 years later, we’re out of address space. reply icedchai 3 hours agorootparentExcept, we're not actually \"out\" out. There is a very active IPv4 transfer / resale market. There is a LOT of unused or underused address space out there. I know one local firm with a completely unused /16 that they have not announced in years. reply bolognafairy 7 hours agorootparentprevYou’re leaning heavily on “like” here. The address spaces are different. That’s the point. reply nottorp 9 hours agorootparentprev> Recently Capital One was assigned a /16 So besides the bad design ipv6 is also badly managed? reply sgjohnson 4 hours agorootparentNo, it's ARIN that's badly managed. Capital One would never get a /16 in RIPE. reply kalaksi 8 hours agorootparentprevWait until you see IPv4 reply nottorp 7 hours agorootparentSo why does 6 have to repeat 4's historical (and very publically recorded) mistakes? reply throw5959 7 hours agorootparentPeople thought there was not enough bureaucracy and wonder why it's not improving after adding bureaucracy. reply oefrha 8 hours agorootparentprevThanks, I missed this story. According to https://old.reddit.com/r/ipv6/comments/17yuqvp/til_capital_o..., the largest allocations by RIRs other than ARIN do not exceed /19, and those are for national telecom providers. Who are these ARIN people and how can they be held accountable for their idiotic management of public resources, which appear to be consistently terrible for decades? reply icedchai 3 hours agorootparentprevThe IPv6 address space is so huge, I, as an individual, was able to get a /44 no questions asked. Back in the 90's, I was able to get a IPv4 /24, no questions asked. reply auguzanellato 1 hour agorootparentThere are only ~16.7 million /24s in IPv4 whereas there are more than 2000 /44 for every person living on planet earth, it’s really a different scale. reply throw0101a 31 minutes agoparentprev> I feel like this blog does not accurately describe how large the ipv6 address space even after accounting for it being reduced by 2^64 for the host portion of the address. One way of thinking about it: * Stars in the Milky Way: 400 Billion * Galaxes in the universe: 2 Trillion So (4x10^11)x(2x10^12)=8x10^23 stars in the universe. * Size of IPv6 address space: 3.4x10^38 Find the ratio between addresses and stars: * 3.4x10^38 / 8x10^23 IPv6 offers about 430 trillion times more addresses than estimated stars in the universe. From Tom Coffee's presentation \"An Enterprise IPv6 Address Planning Case-Study\" * https://www.youtube.com/watch?v=7Tnh4upTOC4 Perhaps in more human terms: on the surface of the Earth (including oceans), there are 8.4 IPv4 addresses per km^2. Not counting the oceans, that would be 28 IPv4 addresses per km^2 land. IPv6 gives 10^17 addresses per mm^2 (yes, square millimeter). In terms of volume, 10^8 IPv6 addresses per mm^3 throughout the Earth. Another way of looking at it: *math property: x^y = x^(a+b) = (x^a)x(x^b) * IPv4 addresses are 32 bits (2^32) * 2^32 ~ 4.3 billion * So the IPv4 Internet has ~4.3B devices on it * IPv6 subnets are 64 bits, /64 (2^64) So, an IPv6 2^64 subnet is the same as (2^32)x(2^32), which means (4.3B)x(IPv4 Internet). I.e., a single IPv6 subnet can hold the equivalent of four billion (IPv4) Internets. reply ahsteele 13 hours agoparentprevIt’s really hard to even comprehend how large IPv6 is. I have found that extreme examples tend to help people get there. Here are some I’ve used in the past. There are enough IPv6 Addresses for 4.77 x 10^28 for every living person. If each IPv6 address was a grain of sand… That’s 2.39 × 10^18 of addresses per person, or roughly enough sand sized addresses to equal about 1.8 times the volume of earths ocean per person. At sand scale all IPv6 addresses would take the volume of 12 sols. Conversely, all the IPv4 addresses in this sand scale would slightly over fill an oil drum. From “IPv6 Addresses: Big Numbers, Big Solutions”: https://www.osti.gov/servlets/purl/1365362 reply Dylan16807 10 hours agorootparentI think it's more useful to comprehend it as 64+64 though. We can give every person a million local networks, with unlimited devices on each local network. That's more accurate and also easy to think about. reply timewizard 8 hours agorootparentIn current practice it's more like a global 61+64. We're all inside the globally routable /3 with ULAs on the side. reply gorgoiler 10 hours agorootparentprevChess has 1e100 possible games but the more practical metric is that the average game involves 40 black moves and 40 white moves. Similarly, it’s better to think of the depth of a network’s topology. The size of the tree when full is immaterial, especially when the last 64 bits is intended to be so sparse that random address assignment is viable. reply gorgoiler 8 hours agorootparentI missed the edit deadline and wanted to add: So instead of talking about there being 2^128 addresses, it’s more useful to talk about there being 4k global regions (/12 RIRs), assigning /32s to ISPs in their region. Each ISP can assign a /48 to each customer site, which can then subdivide into 256 buildings with 256 VLANs each (or some other balance of the these, eg 8 buildings with 64 floors each and 128 VLANs on each floor) with the hosts selecting randomly (or SLAACly) from the final 64 bits. Some ISPs where customers are in a single dwelling will only give you a /56. reply nottorp 9 hours agorootparentprev> It’s really hard to even comprehend how large IPv6 is. But the allocations for a single device are so large that it's not actually as large as it seems to be. Kinda like usb N only working at N-2 speeds... reply bruce511 12 hours agorootparentprevHah - all of those examples use a planet as comparison. The first 16 bits of the address should be reserved to be used by the Planetary Internet Addressing Council (PIAC) Allocating all addresses to Earth seems very shortsighted. (Some sarcasm should be assumed.) It doesn't really matter how you explain large-number math to people who are bad at large-number math (aka all of us) - there's always some bright-spark who misses the point. reply rvnx 5 hours agorootparentWell we could argue that this support for trillions of IP addresses is nice, except that IPv6 does not technically work in interplanetary environments due to various hardcoded timeouts. The first martians are likely to run their own local network and then use a VPN over DTN for their everyday communications by email (and appear coming from a single IPv6 address). One good thing though: since most of the humanity’s knowledge is going to be packed in an LLM they won’t really actually need internet to learn about things. But lack of videos may be annoying. reply remram 4 hours agorootparentI see only one IPv6 timeout, for fragmented packet reassembly, in other words between reception of fragments. So it's a bound on jitter not latency, and I see no reason it couldn't be increased for interplanetary links. reply Dylan16807 10 hours agorootparentprevThe moon can share with the earth, and anything further away needs special encapsulation to deal with hours or more of latency. reply atq2119 10 hours agorootparentIt's only minutes for the inner planets. But of course that's already enough to require special treatment. reply ta1243 6 hours agorootparentprevOne problem of ipv6 is proponents saying there are 2^128 addresses. It's really hard to comprehend how many unusable ipv6 addresses are. Having more than 2^16 hosts on a subnet is pretty much impossible. Sticking with \"grain of sand\" units, but using volumes all from wolfram alpha: There's 2^80 usable IPs in the entire ipvv6 space, because of the /64 subnets. That's plenty. But for every subnet, that's 40 cubic metres unusable for every subnet, and a sphere 1mm wide of usable addresses. My ISP give me a /48. I have under 30 addressable devices over 3 vlans. I'm using 40 cubic miles of space. A ratio of about 10 trillion:1 But that's nothing. The IP allocators are happy to give a bank a /16, or in your \"grain of sand\" measurements 30 times the volume of the moon. To match my unused:used ratio of 10 trillion to one, chase manhattan would need 2^70 devices, which is billions of addresses per cell. All the space that fanboys go on about is almost all unusable, so the extreme examples don't really help at all. reply tsimionescu 8 hours agoparentprevThe thing is, there are just as many /28 IPv6 ranges as /28 IPv4 ranges. And the experience with IPv4 is that it's those larger ranges that we've run out of the quickest. The whole power of IPv6 is that assigning a huge global business a /32 gives them enough addresses to run more devices that exist on the whole Internet today. But if we're distributing large swaths of IPv6, we'll run out just the same, unless we try to later claw them back like we did with some IPv4 /8 spaces. Now, of course, a /28 in IPv4 is a tiny range, while a /28 in IPv6 is huge. And there are, of course, a lot of /28s. The fear is only that we may start seeing larger and larger allocations for no good reason whatsoever. Apparently already some random private company (Capital One) has received a /16 IPv6 range - this is an absurd allocation that should simply not be allowed if we don't want to have to move to a new IP version in a few decades. reply simne 6 hours agoparentprev> does not accurately describe how large the ipv6 It is not the limitation, but router hardware is. Now problem mostly eased, thanks to Moore law, but 20 years ago router memory need to store prefixes-routes graph was very expensive and have some problems with grow. For example, I know people, bought very expensive server PC boards (but cheap compared to enterprise networking hardware), with DDR2 capability, because it was fastest at that time and promises large RAM space. Imagine their frustration, when they seen, this hardware cannot allocate prefixes list for their installation (just not enough RAM) and they have to buy new hardware with DDR3. This even caused \"curse of /24\", when some providers decided to just discard /24 prefixes and work only with /16 directly and for smaller allocations use some default route. And this is just because nature of Internet, which is by definition \"network of networks\", mean, in ideal case all networks should have peer connections at least to all first circle neighbor networks, but better if have peers to few first circles, plus connections to large local hubs. And now problem become harder, because of grow of p2p solutions, which by definition avoid hub model and use all peers directly. BTW, colleague from telecom share latest problem - few days ago appear huge traffic. In very short time found source - after appear new \"Stalker\" game, their developers made huge number of fixes, so fixes are larger than installation, and they somewhere bought so powerful distribution network, distribution of fixes flooded all connections for few days. reply remram 4 hours agorootparentThere are attempts to address that like LISP https://www.rfc-editor.org/rfc/rfc9300.html reply cyberax 11 hours agoparentprevExcept that IPv6 is actually not that large. You effectively can't use networks that are smaller than /64, because stateless autoconfiguration can't use any other prefix size (there's an effort to fix it, but it'll take a decade at best) reply ironhaven 2 hours agorootparentThat was my whole point. There are not 2^128 ipv6 addresses for Individual use but there are 2^64 /64 networks. If you rent a vps with a ipv6 “address” you will see that you are given a /64 network. There is no effort to “fix” the bottom 64 bits because it’s not needed. There are enough /56 networks to for the next 500 years and 99% of people can do everything they need with an isp provided /56. IPv4 has survived with 48 bit nat for this long so 64 bit is even more buffer space reply jeroenhd 2 hours agorootparent> If you rent a vps with a ipv6 “address” you will see that you are given a /64 network. Unfortunately, I've seen multiple VPS providers that will assign you a single address within a /120 or something ridiculous like that. I presume whoever runs the network side of things with those VPS providers has never used IPv6 themselves or they must be _extremely_ stingy. reply immibis 10 hours agorootparentprevYou can absolutely use them, you just need static or DHCP addressing just like the old days. But thanks SLAAC for forcing my ISP to give me /64 or bigger so I can always subnet into /96 for example. reply pantalaimon 4 hours agorootparentHow many devices actually support DHCPv6? SLAAC is pretty much the standard way of assigning addresses these days. reply jeroenhd 2 hours agorootparentJust about anything with a screen these days. The biggest holdout is Android and a bunch of IoT stuff. SLAAC works on any device that supports DHCPv6, though, so you might as well use it if you don't have any requirements to favour DHCPv6 reply defrost 16 hours agoprevSeeing the domain which screams Australian (potaroo, quoll, quokka, et al) I looked at the author's name which brought back a few memories :-) For general interest, this is (among other accolades) 2012 Internet Hall of Fame Inductee Geoff Huston https://www.internethalloffame.org/official-biography-geoff-... reply exabrial 4 hours agoprevI do want to point out one thing that is a common misconception by IT engineer types: > The use of NATs force the interactions into client-initiated transactions... (abbreviated) There is absolutely ZERO chance that that, let's say Roomba, is going to let you connect directly to your vacuum robot at your home, from your cell, over ipv6, without going through their proxy server in an AWS Datacenter in Virgina. The nativity of engineers is face-palm inducing. Roomba will _never_ give up that control over your device. Same with your Tesla, your iPhone, your security cameras, or your ring doorbell. Zilch, none, nada. Giving you direct control, even if ipv6 were fully implemented, is simply not on the roadmap for the companies. They want to control you. They want to control your devices. They will not release the death grip on this, as it releases their control over your property post purchase. This sounds incredibly cynical but it's playing out in front of us. By restricting the use of their services, they now control and downstream resale of the device and can force people to buy new, rather than repair or renovate. And of course the leaders in all of this anti-repair initiatives are the so called \"Green\" companies of the world. reply Ericson2314 3 hours agoparentI connected my AC directly to my desktop. I can't be sure it isn't also phoning home, but I know I am controlling it without a remote server. You are doing a \"there can only be one binding constraint\" analysis, and I don't think that is get. Yes, a lot of incumbents want client-server only, but IPv4 still gives them power enforcing the status quo. If we had IPv6 everywhere, they would need to try harder to enforce it, and we would have to try less hard to change it. reply xxpor 2 hours agoparentprevYou've picked a particularly poor example. You can directly connect to the MQTT server in your roomba today: https://www.home-assistant.io/integrations/roomba/ reply exabrial 2 hours agorootparentIndeed I did, that's hilarious reply bananamango 1 hour agoprevAre there any stats showing how many individual devices we have now on Internet (estimation of public+private IPv4 used)? reply jacob019 16 hours agoprevInteresting how the IPv4 price has pulled back 30% since early 2022. reply mike_d 11 hours agoparentThat is when Amazon stopped pouring money into buying up as much IPv4 as possible and the market returned to demand based pricing. reply wmf 11 hours agorootparentIs Amazon not demand? Do they exist outside economics? (Legally they're supposed to use those addresses within a year(?) of buying them, but I won't pretend that anyone would really notice whether that's the case or not.) reply technothrasher 5 hours agorootparent> but I won't pretend that anyone would really notice I've got a /24 that I obtained back in the Wild West days of the early 1990's, when all you had to do was send an email form and a few minutes later you'd get one assigned. I haven't used it in over 20 years now, but nobody has ever come to take it from me. reply wmf 35 minutes agorootparentYou may be grandfathered under pre-RIR rules and also no one is auditing this stuff. reply chippiewill 9 hours agorootparentprevIt's easy for Amazon to artificially use their address space because they can rotate them around customers using dynamic allocations. reply timewizard 8 hours agorootparentprevThere is an hourly fee for IPv4s now. There was not before. reply ta1243 6 hours agorootparentThat's more to do with increasing prices than decreasing demand reply mike_d 11 hours agorootparentprevYou know what I mean. reply wmf 31 minutes agorootparentI don't know what you mean. Do you think Amazon was paying very high prices for IPs they don't need? Were they trying to corner the market and create a monopoly? reply gruez 16 hours agoparentprevThat's probably more to do with interest rates than anything else. Cheap money => speculative assets are worth more. reply quink 13 hours agorootparentProbably also CGNAT, and tunnelling instead of directly exposing your servers. reply cyberax 14 hours agoparentprevThe investment in infrastructure is cyclical. A lot of ISPs started large expansion projects, fueled by the lockdowns in 2020-s that highlighted the inadequate infrastructure. A lot of content companies also acquired additional IPv4 space for servers. reply MagicMoonlight 4 hours agoprevJust add another four digits to the start of IPv4. Treat any IP of the current length as 0000.CurrentIP. reply dboreham 49 minutes agoparentThat was easy. reply bjelkeman-again 11 hours agoprevInteresting to see how apparently low deployment of IP v6 we have achieved in Sweden. I wonder why. reply dijit 7 hours agoparentOh, I know why! A combination of “getting there first” (and thus, a bunch of decaying infra that doesnt support v6- and needing it all updated) and the “free-net” thing where you can choose your ISP in some apartments. For those not in the know: internet is usually negotiated by your entire apartment building, but some (especially the largest rental building association “MKB”) give you the choice to choose your provider. The way it actually works is complicated, but nobody will invest in this “open networks” system, so it is stuck in time fully, and even providers (like Bahnhof) who want to give v6: can’t with that system. It seems hard to find info on the open network stuff, I know it all from a friend working at Bahnhof, here’s at least part of it: - https://ieeexplore.ieee.org/document/5549139 - https://www.stadsnatsportalen.se/pages/onapi (Swedish) - https://github.com/on-api/on-api reply forty 8 hours agoparentprevThere seem to be some correlation between large population size and higher deployment. Maybe there are too few internet connections to be really worth it. reply birdman3131 8 hours agoprevThe brown bar in the middle of a tiny column of text is very aggravating to read. reply unethical_ban 2 hours agoprevOne thing I've wondered is how routing will work out in the long term for IPv6. If I recall correctly, IPv6 routing was supposed to be hierarchical based on ISP and region, and that would help routing tables. But what if companies in the US, for example, buy a /19 and then divide it up and use it across the globe? I assume routers will have to come with a lot more RAM. reply wmf 27 minutes agoparentIPv6 routing was supposed to be hierarchical... The Internet gave up on that a while ago. IPv6 routing works like IPv4 where people just advertise whatever routes they want. Router RAM has been fixed with newer HBM-based ASICs supporting millions of routes. reply sylware 6 hours agoprev [–] Desktop application, for classic client/server should allocate a transient IPv6 local address only for one session (namely a desktop policy system must give that right to user selected applications only), and I even wonder if browsing one site should not get its own IPv6. Ofc, the local ISP IPv6 router should be scaled for a reasonable domestic usage. Ofc, server like desktop applications should randomly choose an IPv6 there, but it has to stay stable since this number will have to be given to other people to connect to such server (I am talking dodging DNS $$$ racketeering or Big Tech \"nameIPv6\" mapper service, aka for the smhol internet). Only if the ISP is providing a /64 prefix ofc. In my country, nearly 100% of domestic internet lines are working like that, and it has been the case for years. The main issue, is IPv6 on mobile internet: in my country most mobile internet has IPv6 enabled... but it seems you don't get a /64 prefix but a different /128 ip address at each authentication of your sim card. If it ends up not being a trick of my current IPv6 mobile internet modem, this is very bad, REALLY bad: I cannot give a stable mobile internet IPv6 to my contacts for communication (sort of a phone number dedicated to them). Mechanically, it will force people to use classic centralized client-server services, hence force people into Big Tech. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article by Geoff Huston provides an annual update on IP addresses, focusing on the slow transition from IPv4 to IPv6 despite the predicted need for a larger address space.",
      "It discusses the exhaustion of IPv4 addresses, the role of address transfers, and challenges in adopting IPv6, highlighting the fragmentation and market dynamics of IPv4 addresses.",
      "The piece examines IPv6 allocation trends, noting regional differences and the impact of large allocations, and reflects on broader implications for the Internet's future, including network neutrality and market consolidation."
    ],
    "commentSummary": [
      "The discussion highlights the vastness of IPv6 address space compared to IPv4, noting that IPv6 provides significantly more addresses. - Concerns are raised about inefficient allocation practices, with large corporations receiving excessively large IPv6 blocks, which could lead to future shortages. - Challenges in IPv6 adoption are discussed, including technical limitations and companies' reluctance to allow direct device connections, alongside ongoing issues with IPv4 scarcity and market pricing dynamics."
    ],
    "points": 145,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1736645950
  }
]
