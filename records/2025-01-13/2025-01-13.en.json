[
  {
    "id": 42676432,
    "title": "Uv's killer feature is making ad-hoc environments easy",
    "originLink": "https://valatka.dev/2025/01/12/on-killer-uv-feature.html",
    "originBody": "Uv has a killer feature you should know about Jan 12, 2025 In my view, neither performance nor trying to be Python-aligned is what sets uv apart. Don’t get me wrong — try switching from uv to Poetry, and you’ll quickly notice how sluggish it (poetry) feels. uv goes extra miles to adhere to PEPs, and IMHO it’s the go-to package manager for Python these days. But these aren’t the features that surprised me most. There’s one small feature I initially overlooked that truly makes uv intriguing: Imagine doing some ad-hoc scripting in Python 3.12. You run python, and you’re in the REPL, ready to go. But what if you need to pull in a dependency, like Pandas? Here’s where it gets interesting. You would either: Run pip install pandas, potentially modifying your global environment, or Take the proper route: Create a virtual environment Activate the virtual environment pip install pandas Run python This gets even more interesting if you need some other Python version than your global one. You would then use something like pyenv, install the version and set that version as local. So, worst case: pyenv install 3.12. pyenv local 3.12. python -m venv .venv. source .venv/bin/activate. pip install pandas. python. With uv, it’s just 1 command: uv run --python 3.12 --with pandas python easy to remember, and no trace left behind. Happy scripting! HN",
    "commentLink": "https://news.ycombinator.com/item?id=42676432",
    "commentBody": "Uv's killer feature is making ad-hoc environments easy (valatka.dev)454 points by astronautas 22 hours agohidepastfavorite381 comments nharada 22 hours agoI really like uv, and it's the first package manager for a while where I haven't felt like it's a minor improvement on what I'm using but ultimately something better will come out a year or two later. I'd love if we standardized on it as a community as the de facto default, especially for new folks coming in. I personally now recommend it to nearly everyone, instead of the \"welllll I use poetry but pyenv works or you could use conda too\" reply poincaredisk 22 hours agoparentI never used anything other than pip. I never felt the need to use anything other than pip (with virtualenv). Am I missing anything? reply NeutralCrane 21 hours agorootparentCouple of things. - pip doesn't handle your Python executable, just your Python dependencies. So if you want/need to swap between Python versions (3.11 to 3.12 for example), it doesn't give you anything. Generally people use an additional tool such as pyenv to manage this. Tools like uv and Poetry do this as well as handling dependencies - pip doesn't resolve dependencies of dependencies. pip will only respect version pinning for dependencies you explicitly specify. So for example, say I am using pandas and I pin it to version X. If a dependency of pandas (say, numpy) isn't pinned as well, the underlying version of numpy can still change when I reinstall dependencies. I've had many issues where my environment stopped working despite none of my specified dependencies changing, because underlying dependencies introduced breaking changes. To get around this with pip you would need an additional tool like pip-tools, which allows you to pin all dependencies, explicit and nested, to a lock file for true reproducibility. uv and poetry do this out of the box. - Tool usage. Say there is a python package you want to use across many environments without installing in the environments themselves (such as a linting tool like ruff). With pip, you need to install another tool like pipx to install something that can be used across environments. uv can do this out of the box. Plus there is a whole host of jobs that tools like uv and poetry aim to assist with that pip doesn't, namely project creation and management. You can use uv to create a new Python project scaffolding for applications or python modules in a way that conforms with PEP standards with a single command. It also supports workspaces of multiple projects that have separate functionality but require dependencies to be in sync. You can accomplish a lot/all of this using pip with additional tooling, but its a lot more work. And not all use cases will require these. reply driggs 19 hours agorootparentYes, generally people already use an additional tool for managing their Python executables, like their operating system's package manager: $> sudo apt-get install python3.10 python3.11 python3.12 And then it's simple to create and use version-specific virtual environments: $> python3.11 -m venv .venv3.11 $> source .venv3.11/bin/activate $> pip install -r requirements.txt You are incorrect about needing to use an additional tool to install a \"global\" tool like `ruff`; `pip` does this by default when you're not using a virtual environment. In fact, this behavior is made more difficult by tools like `uv` if or `pipx` they're trying to manage Python executables as well as dependencies. reply athrun 19 hours agorootparent> sudo apt-get install python3.10 python3.11 python3.12 This assumes the Python version you need is available from your package manager's repo. This won't work if you want a Python version either newer or older than what is available. > You are incorrect about needing to use an additional tool to install a \"global\" tool like `ruff`; `pip` does this by default when you're not using a virtual environment. True, but it's not best practice to do that because while the tool gets installed globally, it is not necessarily linked to a specific python version, and so it's extremely brittle. And it gets even more complex if you need different tools that have different Python version requirements. reply coldtea 18 hours agorootparent>This assumes the Python version you need is available from your package manager's repo. This won't work if you want a Python version either newer or older than what is available. And of course you could be working with multiple distros and versions of the same distro, production and dev might be different environment and tons of others concerns. You need something that just works across. reply nightpool 17 hours agorootparentSurely you just use Docker for production, right? reply nicoburns 13 hours agorootparentYou almost need to use Docker for deploying Python because the tooling is so bad that it's otherwise very difficult to get a reproducible environment. For many other languages the tooling works well enough that there's relatively little advantage to be had from Docker (although you can of course still use it). reply wombatpm 12 hours agorootparentprevAnd how do you know everything is ok when you build your new docker image? reply mixmastamyk 13 hours agorootparentprev>> You are incorrect about needing to use an additional tool to install a \"global\" tool like `ruff`; `pip` does this by default when you're not using a virtual environment. >True, but it's not best practice to do that because while the tool gets installed globally, it is not necessarily linked to a specific python version, and so it's extremely brittle. \"Globally\" means installed with sudo. These are installed into the user folder under ~/.local/ and called a user install by pip. I wouldn't call it \"extremely brittle\" either. It works fine until you upgrade to a new version of python, in which case you install the package again. Happens once a year perhaps. The good part of this is that unused cruft will get left behind and then you can delete old folders in ~/.local/lib/python3.? etc. I've been doing this over a decade without issue. reply TeMPOraL 7 hours agorootparent> \"Globally\" means installed with sudo. These are installed into the user folder under ~/.local/ and called a user install by pip. > It works fine until you upgrade to a new version of python, in which case you install the package again. Debian/Ubuntu doesn't want you to do either, and tell you you'll break your system if you force it (the override flag is literally named \"--break-system-packages\"). Hell, if you're doing it with `sudo`, they're probably right - messing with the default Python installation (such as trying to upgrade it) is the quickest way to brick your Debian/Ubuntu box. Incredibly annoying when your large project happens to use pip to install both libraries for the Python part, and tools like CMake and Conan, meaning you can't just put it all in a venv. reply mixmastamyk 1 hour agorootparentNot Debian specific. The braindead option was added by pip to scare off newbies. No one with the most basic of sysad skills is “bricked” by having to uninstall a library. Again have not experienced a conflict in over 15 years. Use the system package manager or buid yourself for tools like cmake. reply orra 9 hours agorootparentprevNah, pip is still brittle here because it uses one package resolution context to install all your global tools. So if there is a dependency clash you are out of luck. So that's why pipx was required, or now, UV. reply mixmastamyk 1 hour agorootparentNot happened in the last fifteen years, never used pipx. See my other replies. reply lmm 10 hours agorootparentprev> It works fine until you upgrade to a new version of python, in which case you install the package again. Or you install a second global tool that depends on an incompatible version of a library. reply mixmastamyk 1 hour agorootparentNever happened, and exceedingly unlikely to because your user-wide tools should be few. reply zahlman 15 hours agorootparentprev> pip doesn't resolve dependencies of dependencies. This is simply incorrect. In fact the reason it gets stuck on resolution sometimes is exactly because it resolved transitive dependencies and found that they were mutually incompatible. Here's an example which will also help illustrate the rest of my reply. I make a venv for Python 3.8, and set up a new project with a deliberately poorly-thought-out pyproject.toml: [project] name=\"example\" version=\"0.1.0\" dependencies=[\"pandas==2.0.3\", \"numpy==1.17.3\"] I've specified the oldest version of Numpy that has a manylinux wheel for Python 3.8 and the newest version of Pandas similarly. These are both acceptable for the venv separately, but mutually incompatible on purpose. When I try to `pip install -e .` in the venv, Pip happily explains (granted the first line is a bit strange): ERROR: Cannot install example and example==0.1.0 because these package versions have conflicting dependencies. The conflict is caused by: example 0.1.0 depends on numpy==1.17.3 pandas 2.0.3 depends on numpy>=1.20.3; python_versionpip will only respect version pinning for dependencies you explicitly specify. So for example, say I am using pandas and I pin it to version X. If a dependency of pandas (say, numpy) isn't pinned as well, the underlying version of numpy can still change when I reinstall dependencies. Well, sure; Pip can't respect a version pin that doesn't exist anywhere in your project. If the specific version of Pandas you want says that it's okay with a range of Numpy versions, then of course Pip has freedom to choose one of those versions. If that matters, you explicitly specify it. Other programs like uv can't fix this. They can only choose different resolution strategies, such as \"don't update the transitive dependency if the environment already contains a compatible version\", versus \"try to use the most recent versions of everything that meet the specified compatibility requirements\". > To get around this with pip you would need an additional tool like pip-tools, which allows you to pin all dependencies, explicit and nested, to a lock file for true reproducibility. No, you just use Pip's options to determine what's already in the environment (`pip list`, `pip freeze` etc.) and pin everything that needs pinning (whether with a Pip requirements file or with `pyproject.toml`). Nothing prevents you from listing your transitive dependencies in e.g. the [project.dependencies] of your pyproject.toml, and if you pin them, Pip will take that constraint into consideration. Lock files are for when you need to care about alternate package sources, checking hashes etc.; or for when you want an explicit representation of your dependency graph in metadata for the sake of other tooling. > This assumes the Python version you need is available from your package manager's repo. This won't work if you want a Python version either newer or older than what is available. I have built versions 3.5 through 3.13 inclusive from source and have them installed in /opt and the binaries symlinked in /usr/local/bin. It's not difficult at all. > True, but it's not best practice to do that because while the tool gets installed globally, it is not necessarily linked to a specific python version, and so it's extremely brittle. What brittleness are you talking about? There's no reason why the tool needs to run in the same environment as the code it's operating on. You can install it in its own virtual environment, too. Since tools generally are applications, I use Pipx for this (which really just wraps a bit of environment management around Pip). It works great; for example I always have the standard build-frontend `build` (as `pyproject-build`) and the uploader `twine` available. They run from a guaranteed-compatible Python. And they would if they were installed for the system Python, too. (I just, you know, don't want to do that because the system Python is the system package manager's responsibility.) The separate environment don't matter because the tool's code and the operated-on project's code don't even need to run at the same time, let alone in the same process. In fact, it would make no sense to be running the code while actively trying to build or upload it. > And it gets even more complex if you need different tools that have different Python version requirements. No, you just let each tool have the virtual environment it requires. And you can update them in-place in those environments, too. reply IanCal 9 hours agorootparent> This is simply incorrect. In fact the reason it gets stuck on resolution sometimes is exactly because it resolved transitive dependencies and found that they were mutually incompatible. The confusion might be that this used to be a problem with pip. It looks like this changed around 2020, but before then pip would happily install broken versions. Looking it up, this change of resolution happened in a minor release. reply zahlman 8 hours agorootparentYou have it exactly, except that Pip 20.3 isn't a \"minor release\" - since mid-2018, Pip has used quarterly calver, so that's just \"the last release made in 2020\". (I think there was some attempt at resolving package versions before that, it just didn't work adequately.) reply jshen 14 hours agorootparentprev> Well, sure; Pip can't respect a version pin that doesn't exist anywhere in your project. If the specific version of Pandas you want says that it's okay with a range of Numpy versions, then of course Pip has freedom to choose one of those versions. If that matters, you explicitly specify it Nearly every other language solves this better than this. What your suggesting breaks down on large projects. reply zahlman 13 hours agorootparent>Nearly every other language solves this better than this. \"Nearly every other language\" determines the exact version of a library to use for you, when multiple versions would work, without you providing any input with which to make the decision? If you mean \"I have had a more pleasant UX with the equivalent tasks in several other programming languages\", that's justifiable and common, but not at all the same. >What your suggesting breaks down on large projects. Pinned transitive dependencies are the only meaningful data in a lockfile, unless you have to explicitly protect against supply chain attacks (i.e. use a private package source and/or verify hashes). reply baq 10 hours agorootparentIMHO the clear separation between lockfile and deps in other package managers was a direct consequence of people being confused about what requirements.txt should be. It can be both and could be for ages (pip freeze) but the defaults were not conductive to clear separation. If we started with lockfile.txt and dependencies.txt, the world may have looked different. Alas. reply zahlman 1 hour agorootparentThe thing is, the distinction is purely semantic - Pip doesn't care. If you tell it all the exact versions of everything to install, it will still try to \"solve\" that - i.e., it will verify that what you've specified is mutually compatible, and check whether you left any dependencies out. reply jshen 4 hours agorootparentprevWhat's your process for ensuring all members of a large team are using the same versions of libraries in a non trivial python codebase? reply zahlman 1 hour agorootparentIf all you need to do is ensure everyone's on the same versions of the libraries - if you aren't concerned with your supply chain, and you can accept that members of your team are on different platforms and thus getting different wheels for the same version, and you don't have platform-specific dependency requirements - then pinned transitive dependencies are all the metadata you need. pyproject.toml isn't generally intended for this, unless what you're developing is purely an application that shouldn't ever be depended on by anyone else or sharing an environment with anything but its own dependencies. But it would work. The requirements.txt approach also works. If you do have platform-specific dependency requirements, then you can't actually use the same versions of libraries, by definition. But you can e.g. specify those requirements abstractly, see what the installer produces on your platform, and produce a concrete requirement-set for others on platforms sufficiently similar to yours. (I don't know offhand if any build backends out there will translate abstract dependencies from an sdist into concrete ones in a platform-specific wheel. Might be a nice feature for application devs.) Of course there are people and organizations that have use cases for \"real\" lockfiles that list provenance and file hashes, and record metadata about the dependency graph, or whatever. But that's about more than just keeping a team in sync. reply achileas 17 hours agorootparentprevIt’s like a whole post of all the things you’re not supposed to do with Python, nice. reply EasyMark 19 hours agorootparentprevmost developers I know do not use the system version of python. We use an older version at work so that we can maximize what will work for customers and don't try to stay on the bleeding edge. I imagine others do want newer versions for features, hence people find products like UV useful reply diath 16 hours agorootparentprevThat assumes that you are using a specific version of a specific Linux distribution that happens to ship specific versions of Python that you are currently targeting. That's a big assumption. uv solves this. reply Fnoord 18 hours agorootparentprev(I've just learned about uv, and it looks like I have to pick it up since it performs very well.) I just use pipx. Install guides suggest it, and it is only one character different from pip. With Nix, it is very easy to run multiple versions of same software. The path will always be the same, meaning you can depend on versions. This is nice glue for pipx. My pet peeve with Python and Vim is all these different package managers. Every once in a while a new one is out and I don't know if it will gain momentum. For example, I use Plug now in Vim but notice documentation often refers to different alternatives these days. With Python it is pip, poetry, pip search no longer working, pipx, and now uv (I probably forgot some things). reply zahlman 15 hours agorootparentPipx is a tool for users to install finished applications. It isn't intended for installing libraries for further development, and you have to hack around it to make that work. (This does gain you a little bit over using Pip directly.) I just keep separate compiled-from-source versions of Python in a known, logical place; I can trivially create venvs from those directly and have Pip install into them, and pass `--python` to `pipx install`. >With Python it is pip, poetry, pip search no longer working, pipx, and now uv (I probably forgot some things). Of this list, only Poetry and Uv are package managers. Pip is by design, only an installer, and Pipx only adds a bit of environment management to that. A proper package manager also helps you keep track of what you've installed, and either produces some sort of external lock file and/or maintains dependency listings in `pyproject.toml`. But both Poetry and Uv go further beyond that as well, aiming to help with the rest of the development workflow (such as building your package for upload to PyPI). If you like Pipx, you might be interested in some tips in my recent blog post (https://zahlman.github.io/posts/2025/01/07/python-packaging-...). In particular, if you do need to install libraries, you can expose Pipx's internal copy of Pip for arbitrary use instead of just for updating the venvs that Pipx created. reply Gazoche 3 hours agorootparentprevYeah, venv is really the best way to manage Python environments. In my experience other tools like Conda often create more headaches than they solve. Sure, venv doesn't manage Python versions, but it's not that difficult to install the version you need system-wide and point your env to it. Multiple Python versions can coexist in your system without overriding the default one. On Ubuntu, the deadsnakes PPA is pretty useful if you need an old Python version that's not in the official repos. In the rare case where you need better isolation (like if you have one fussy package that depends on specific system libs, looking at you tensorflow), Docker containers are the next best option. reply graemep 9 hours agorootparentprevI also tend to use the OS package manager to install other binary dependencies. Pip does the rest perfectly well. reply kiddico 20 hours agorootparentprevSometimes I feel like my up vote doesn't adequately express my gratitude. I appreciate how thorough this was. reply stavros 19 hours agorootparentprevOh wow, it actually can handle the Python executable? I didn't know that, that's great! Although it's in the article as well, it didn't click until you said it, thanks! reply meitham 14 hours agorootparentI would avoid using this feature! It downloads a compiled portable python binary from some random github project not from PSF. That very same github project recommends against using their binary as the compilation flags is set for portability against performance. See https://gregoryszorc.com/docs/python-build-standalone/main/ reply rat87 0 minutes agorootparentIts not from some random github project its from a trusted member of open source community. Same as other libraries you use and install. It was used by rye before rye and uv sort of merged and is used by pipx and hatch and mise (and bazel rules_python) https://x.com/charliermarsh/status/1864042688279908459 My understanding is that the problem is that psf doesnt publish portable python binaries (I dont think they even publish any binaries for linux). Luckily theres some work being done on a pep for similar functionality from an official source but that will likely take several years. Gregory has praised the attempt and made suggestions based on his experience. https://discuss.python.org/t/pep-711-pybi-a-standard-format-... Apparently he had less spare time for open source and since astral had been helping with a lot of the maitinence work on the project he happily transfered over ownership to themin December https://gregoryszorc.com/blog/2024/12/03/transferring-python... https://astral.sh/blog/python-build-standalone mkl 6 hours agorootparentprevhttps://github.com/astral-sh/python-build-standalone is by the same people as uv, so it's hardly random. The releases there include ones with profile-guided optimisation and link time optimisation [1], which are used by default for some platforms and Python versions (and work seems underway to make them usable for all [2]). I don't see any recommendation against using their binaries or mention of optimising for portability at the cost of performance on the page you link or the pages linked from it that I've looked at. [1] https://github.com/astral-sh/uv/blob/main/crates/uv-python/d... (search for pgo) [2] https://github.com/astral-sh/uv/issues/8015 reply meitham 2 hours agorootparentThis must have moved recently! I looked at this around end of December and it was hosted on https://github.com/indygreg/python-build-standalone/releases which had nothing to do with UV. If you read through the docs now it still references indygreg and still shows this https://github.com/indygreg/python-build-standalone so I guess the move has not completed it, but yes it's a positive change to see UV taking ownership of the builds. reply zahlman 15 hours agorootparentprevI still don't understand why people want separate tooling to \"handle the Python executable\". All you need to do is have one base installation of each version you want, and then make your venv by running the standard library venv for that Python (e.g. `python3.x -m venv .venv`). reply nicoburns 13 hours agorootparentHaving to manually install python versions and create venvs is pretty painful compared to say the Rust tooling where you install rustup once, and then it will automatically choose the correct Rust version for each project based on what that project has configured. UV seems like it provides a lot of that convenience for python. reply stavros 15 hours agorootparentprev> All you need to do is have one base installation of each version you want Because of this ^ reply zahlman 13 hours agorootparentBut any tool you use for the task would do that anyway (or set them up temporarily and throw them away). Python on Windows has a standard Windows-friendly installer, and compiling from source on Linux is the standard few calls to `./configure` and `make` that you'd have with anything else; it runs quite smoothly and you only have to do it once. reply _ZeD_ 13 hours agorootparentI need to tell you a secret... I'm a long-life Linux user (since mandrake!) Also, I don't have a c compiler installed. reply zahlman 13 hours agorootparentReally? I was told Mint was supposed to be the kiddie-pool version of Linux, but it gave me GCC and a bunch of common dependencies anyway. (By my understanding, `pyenv install` will expect to be able to run a compiler to build a downloaded Python source tarball. Uv uses prebuilt versions from https://github.com/astral-sh/python-build-standalone ; there is work being done in the Python community on a standard for packaging such builds, similarly to wheels, so that you can just use that instead of compiling it yourself. But Python comes out of an old culture where users expect to do that sort of thing.) reply chupasaurus 6 hours agorootparentIn Debian build-essential package is only recommended dependency of pip. Pyenv obviously wouldn't work without it. reply gtaylor 15 hours agorootparentprevI'm glad to let uv handle that for me. It does a pretty good job at it! reply PaulHoule 21 hours agorootparentprevpip's resolving algorithm is not sound. If your Python projects are really simple it seems to work but as your projects get more complex the failure rate creeps up over time. You might pip install something and have it fail and then go back to zero and restart and have it work but at some point that will fail. conda has a correct resolving algorithm but the packages are out of date and add about as many quality problems as they fix. I worked at a place where the engineering manager was absolutely exasperated with the problems we were having with building and deploying AI/ML software in Python. I had figured out pretty much all the problems after about nine months and had developed a 'wheelhouse' procedure for building our system reliably, but it was too late. Not long after I sketched out a system that was a lot like uv but it was written in Python and thus had problems with maintaining its own stable Python enivronment (e.g. poetry seems to trash itself every six months or so.) Writing uv in Rust was genius because it eliminates that problem of the system having a stable surface to stand on instead of pipping itself into oblivion, never mind that it is much faster than my system would have been. (My system had the extra feature that it used http range requests to extract the metadata from wheel files before pypi started letting you download the metadata directly.) I didn't go forward with developing it because I argued with a lot of people who, like you, thought it was \"the perfect being the enemy of the good\" when it was really \"the incorrect being the enemy of the correct.\" I'd worked on plenty of projects where I was right about the technology and wrong about the politics and I am so happy that uv has saved the Python community from itself. reply MadnessASAP 21 hours agorootparentMay I introduce you to our lord and saviour, Nix and it's most holy child nixpkgs! With only a small tithing of your sanity and ability to Interop with any other dependency management you can free yourself of all dependency woes forever! [] For various broad* definitions of forever. [*] Like, really, really broad** [**] Maybe a week if you're lucky reply p_l 20 hours agorootparentExcept python builders in nixpkgs are really brain damaged because of the writers ways they inject search path which for example breaks if you try to execute a separate python interpreter assuming same library environment... reply MadnessASAP 20 hours agorootparentWithin the holy church of Nix the sect of Python is troubled one, it can however be tamed into use via vast tomes of scripture. Sadly these times can only be written by those you have truly given their mind and body over to the almighty Nix. reply p_l 19 hours agorootparentIt's not as bad as Common Lisp support which stinks to high heavens of someone not learning the lessons of the Common-Lisp-Controller fiasco reply MadnessASAP 17 hours agorootparentLisp is of the old gods, only the most brave of Nix brethren dare tread upon their parenthesised ways. reply chpatrick 20 hours agorootparentprevNix is really the best experience I've had with Python package management but only if all the dependencies are already in nixpkgs. If you want to quickly try something off github it's usually a pain in the ass. reply coldtea 18 hours agorootparentprev>May I introduce you to our lord and saviour, Nix and it's most holy child nixpkgs! In this case, instead of working with Python, you change how you manage everything! reply benatkin 21 hours agorootparentprevThe Nix of Python, conda, was already mentioned. > add about as many quality problems as they fix reply MadnessASAP 20 hours agorootparentI used to have 1 problem, then I used Nix to fix it, now I have 'Error: infinite recursion' problems. reply zahlman 14 hours agorootparentprev> You might `pip install` something and have it fail and then go back to zero and restart and have it work but at some point that will fail. Can you give a concrete example, starting from a fresh venv, that causes a failure that shouldn't happen? > but it was written in Python and thus had problems with maintaining its own stable Python enivronment All it has to do is create an environment for itself upon installation which is compatible with its own code, and be written with the capability of installing into other environments (which basically just requires knowing what version of Python it uses and the appropriate paths - the platform and ABI can be assumed to match the tool, because it's running on the same machine). This is fundamentally what uv is doing, implicitly, by not needing a Python environment to run. But it's also what the tool I'm developing, Paper, is going to do explicitly. What's more, you can simulate it just fine with Pip. Of course, that doesn't solve the issues you had with Pip, but it demonstrates that \"maintaining its own stable Python environment\" is just not a problem. >Writing uv in Rust was genius because it eliminates that problem of the system having a stable surface to stand on instead of pipping itself into oblivion, never mind that it is much faster than my system would have been. From what I can tell, the speed mainly comes from algorithmic issues, caching etc. Pip is just slow above and beyond anything Python forces on it. An example. On my system, creating a new venv from scratch with Pip included (which loads Pip from within its own vendored wheel, which then runs in order to bootstrap itself into the venv) takes just over 3 seconds. Making a new venv without Pip, then asking a separate copy of Pip to install an already downloaded Pip wheel would be about 1.7 seconds. But making that venv and using the actual internal installation logic of Pip (which has been extracted by Pip developer Pradyun Gedam as https://github.com/pypa/installer ) would take about 0.25 seconds. (There's no command-line API for this; in my test environment I just put the `installer` code side by side with a driver script, which is copied from my development work on Paper.) It presumably could be faster still. I honestly have no idea what Pip is doing the rest of that time. It only needs to unzip an archive and move some files around and perform trivial edits to others. > (My system had the extra feature that it used http range requests to extract the metadata from wheel files before pypi started letting you download the metadata directly.) Pip has had this feature for a long time (and it's still there - I think to support legacy projects without wheels, because I think the JSON API won't be able to provide the data since PyPI doesn't build the source packages). It's why the PyPI server supports range requests in the first place. > I'd worked on plenty of projects where I was right about the technology and wrong about the politics and I am so happy that uv has saved the Python community from itself. The community's politics are indeed awful. But Rust (or any other language outside of Python) is not needed to solve the problem. reply zahlman 1 hour agorootparentIt occurs to me later: `installer` isn't compiling the .py files to .pyc, which probably accounts for the time difference. This can normally be done on demand (or suppressed entirely) but Pip wants to do it up front. Bleh. \"Installing\" from already-unpacked files would still be much faster. reply morkalork 21 hours agorootparentprevUgh, I hate writing this but that's where docker and microservices comes to the rescue. It's a pain in the butt and inefficient to run but if you don't care about the overhead (and if you do care, why are you still using Python?), it works. reply PaulHoule 20 hours agorootparentMy experience was that docker was a tool data scientists would use to speedrun the process of finding broken Pythons. For instance we'd inexplicably find a Python had Hungarian as the default charset, etc. The formula was - Docker - Discipline = Chaos Docker - Discipline = Chaos Docker + Discipline = Order but - Docker + Discipline = Order If you can write a Dockerfile to install something you can write a bash script. Circa 2006 I was running web servers on both Linux and Windows with hundreds of web sites on them with various databases, etc. It really was simple then as \"configure a filesystem path\" and \"configure a database connection\" and I had scripts that could create a site in 30 seconds or so. Sure today you might have five or six different databases for a site but it's not that different in my mind. Having way too many different versions of things installed is a vice, not a virtue. reply macNchz 16 hours agorootparent> If you can write a Dockerfile to install something you can write a bash script. Docker is great for making sure that magic bash script that brings the system up actually works again on someone else’s computer or after a big upgrade on your dev machine or whatever. So many custom build scripts I’ve run into over the years have some kind of unstated dependency on the initial system they were written on, or explicit dependencies on something tricky to install, and as such are really annoying to diagnose later on, especially if they make significant system changes. Docker is strictly better than a folder full of bash scripts and a Readme.txt. I would have loved having it when I had to operate servers like that with tons of websites running on them. So much nicer to be able to manage dependency upgrades per-site rather than server-wide, invariably causing something to quietly break on one of 200 sites. reply baq 9 hours agorootparentUnspoken libc dependencies are my favorite. Granted, you need to wait a few years after launching the project to feel that pain, but once you’re there, the experience is… unforgettable. Second best are OpenSSL dependencies. I sincerely hope I won’t have to deal with that again. reply TeMPOraL 6 hours agorootparentprev> If you can write a Dockerfile to install something you can write a bash script. The trick isn't installing things, it's uninstalling them. Docker container is isolated in ways your bash script equivalent is not - particularly when first developing it, when you're bound to make an occasional mistake. reply p_l 19 hours agorootparentprevUnfortunately sometimes you get to host things not written by you, or which exist for a long time and thus there's a lot of history involved that prevents nice and tidy. My first production use of kubernetes started out because we put in the entirety of what we had to migrate to new hosting into spreadsheet, with columns for various parts of stack used by the websites, and figured we would go insane trying to pack it up - or we would lose the contract because we would be as expensive as the last company. Could we package it nicely without docker? Yes, but the effort to package it in docker was smaller than packaging it in a way where it wouldn't conflict on a single host, because the simple script becomes way harder when you need to handle multiple versions of the same package, something that most distro do not support at all (these days I think we could have done it with NixOS, but that's a different kettle of deranged fishes) And then the complexity of managing the stack was quickly made easier by turning each site into separate artifact (docker container) handled by k8s manifests (especially when it came to dealing with about 1000 domains across those apps). So, theoretically discipline is enough, practical world is much dirtier though. reply coldtea 18 hours agorootparentprev>For instance we'd inexplicably find a Python had Hungarian as the default charset, etc. Sounds quite explicable: Docker image created by Hungarian devs perhaps? reply PaulHoule 15 hours agorootparentMy understanding is that UTF-8 is the world's charset and that reasonable Hungarians would use that (e.g. I sure don't use us-ascii or iso-latin-1 if I can at all help it. I mean my \"better half\" reads 中文 so I don't have to and having it all in UTF-8 makes it easy) The other mystery is how the data sci's found it. reply chupasaurus 6 hours agorootparentIIRC there was some widely used image with many derivatives that redefined locale (the one in Docker Library used POSIX since forever). reply kussenverboten 14 hours agorootparentprevDocker will make it work, but is a heavy solution as it will happily take up GB of your disk. uv is a more efficient and elegant option. reply coldtea 18 hours agorootparentprev>and if you do care, why are you still using Python? Because I get other advantages of it. Giving in to overhead on one layer, doesn't mean I'm willing to give it up everywhere. reply fulafel 14 hours agorootparentprevYes, another sound reason to use microservices. /s reply ppierald 17 hours agorootparentprevRespectively, yes. The ability to create venvs so fast, that it becomes a silent operation that the end user never thinks about anymore. The dependency management and installation is lightning quick. It deals with all of the python versioning and I think a killer feature is the ability to inline dependencies in your Python source code, then use: uv tool runYour script code would like: #!/usr/bin/env -S uv run --script # /// script # requires-python = \">=3.12\" # dependencies = [ # \"...\", # \"...\" # ] # /// Then uv will make a new venv, install the dependencies, and execute the script faster than you think. The first run is a bit slower due to downloads and etc, but the second and subsequent runs are a bunch of internal symlink shuffling. It is really interesting. You should at least take a look at a YT or something. I think you will be impressed. Good luck! reply zahlman 14 hours agorootparent>Respectively, yes. The ability to create venvs so fast, that it becomes a silent operation that the end user never thinks about anymore. I might just blow your mind here: $ time python -m venv with-pip real 0m3.248s user 0m3.016s sys 0m0.219s $ time python -m venv --without-pip without-pip real 0m0.054s user 0m0.046s sys 0m0.009s The thing that actually takes time is installing Pip into the venv. I already have local demonstrations that this installation can be an order of magnitude faster in native Python. But it's also completely unnecessary to do that: $ source without-pip/bin/activate (without-pip) $ ~/.local/bin/pip --python `which python` install package-installation-test Collecting package-installation-test Using cached package_installation_test-1.0.0-py3-none-any.whl.metadata (3.1 kB) Using cached package_installation_test-1.0.0-py3-none-any.whl (3.1 kB) Installing collected packages: package-installation-test Successfully installed package-installation-test-1.0.0 I have wrappers for this, of course (and I'm explicitly showing the path to a separate Pip that's already on my path for demonstration purposes). > a killer feature is the ability to inline dependencies in your Python source code, then use: uv tool runYes, Uv implements PEP 723 \"Inline Script Metadata\" (https://peps.python.org/pep-0723/) - originally the idea of Paul Moore from the Pip dev team, whose competing PEP 722 lost out (see https://discuss.python.org/t/_/29905). He's been talking about a feature like this for quite a while, although I can't easily find the older discussion. He seems to consider it out of scope for Pip, but it's also available in Pipx as of version 1.4.2 (https://pipx.pypa.io/stable/CHANGELOG/). > The first run is a bit slower due to downloads and etc, but the second and subsequent runs are a bunch of internal symlink shuffling. Part of why Pip is slow at this is because it insists on checking PyPI for newer versions even if it has something cached, and because its internal cache is designed to simulate an Internet connection and go through all the usual metadata parsing etc. instead of just storing the wheels directly. But it's also just slow at actually installing packages when it already has the wheel. In principle, nothing prevents a Python program from doing caching sensibly and from shuffling symlinks around. reply matsemann 12 hours agorootparentIt's not the \"runtime\" that's slow for me with pip, but all the steps needed. My biggest gripe with python is you need to basically be an expert in different tools to get a random project running. Uv solves this. Just uv run the script and it works. I don't care if pip technically can do something. The fact that I explicitly have to mess around with venvs and the stuff is already enough mental overhead that I disregard it. I'm a python programmer at my job, and I've hated the tooling for years. Uv is the first time I actually like working with python. reply zahlman 8 hours agorootparentNone of GP is about what Pip can technically do. It's about what a better tool still written in Python could do. The problems you're describing, or seeing solved with uv, don't seem to be about a problem with the design of virtual environments. (Uv still uses them.) They're about not having the paradigm of making a venv transiently, as part of the code invocation; or they're about not having a built-in automation of a common sequence of steps. But you can do that just as well with a couple lines of Bash. I'm not writing any of this to praise the standard tooling. I'm doing it because the criticisms I see most commonly are inaccurate. In particular, I'm doing it to push back against the idea that a non-Python language is required to make functional Python tooling. There isn't a good conceptual reason for that. reply regularfry 7 hours agorootparentIt may not be required, but it has the virtue of existing. Now that it does, is it a problem that it's not written in Python? Especially given that they've chosen to take on managing the interpreter as well: being in a compiled language does mean that it doesn't have the bootstrap problem of needing an already functional Python installation that they need to avoid breaking. reply matsemann 7 hours agorootparentprevWhy does it matter if it's written in python or not? I want the best tooling, don't care how it's made. reply zahlman 1 hour agorootparentYou are free to evaluate tooling by your own standards. But it commonly comes across that people think it can't be written in Python if it's to have XYZ features, and by and large they're wrong, and I'm trying to point that out. In particular, people commonly seem to think that e.g. Pip needs to be in the same environment to work, and that's just not true. There's a system in place that defaults to copying Pip into every environment so that you can `python -m pip`, but this is wasteful and unnecessary. (Pip is designed to run under the install environment's Python, but this is a hacky implementation detail. It really just needs to know the destination paths and the target Python version.) It also happens that I care about disk footprint quite a bit more than most people. Maybe because I still remember the computers I grew up with. reply cpburns2009 5 hours agorootparentprevI'm fairly minimalist when it comes to tooling: venv, pip, and pip-tools. I've started to use uv recently because it resolves packages significantly faster than pip/pip-tools. It will generate a \"requirements.txt\" with 30 packages in a few seconds rather than a minute or two. reply amluto 22 hours agorootparentprevIf you switch to uv, you’ll have fewer excuses to take coffee breaks while waiting for pip to do its thing. :) reply mosselman 21 hours agorootparentprevI am not a python developer, but sometimes I use python projects. This puts me in a position where I need to get stuff working while knowing almost nothing about how python package management works. Also I don’t recognise errors and I don’t know which python versions generally work well with what. I’ve had it happen so often with pip that I’d have something setup just fine. Let’s say some stable diffusion ui. Then some other month I want to experiment with something like airbyte. Can’t get it working at all. Then some days later I think, let’s generate an image. Only to find out that with pip installing all sorts of stuff for airbyte, I’ve messed up my stable diffusion install somehow. Uv clicked right away for me and I don’t have any of these issues. Was I using pip and asdf incorrectly before? Probably. Was it worth learning how to do it properly in the previous way? Nope. So uv is really great for me. reply hyeonwho4 20 hours agorootparentThis is not just a pip problem. I had the problem with anaconda a few years ago where upgrading the built in editor (spyder?) pulled versions of packages which broke my ML code, or made dependencies impossible to reconsile. It was a mess, wasting hours of time. Since then I use one pip venv for each project and just never update dependencies. reply zahlman 13 hours agorootparentSpyder isn't built-in; IDLE comes with Python (unless you get it via Debian, at least), but is not separately upgradable (as the underlying `idlelib` is part of the standard library). If upgrading Spyder broke your environment, that's presumably because you were using the same environment that Spyder itself was in. (Spyder is also implemented in Python, as the name suggests.) However, IDEs for Python also like to try to do environment management for you (which may conflict with other tools you want to use specifically for the purpose). That's one of the reasons I just edit my code in Vim. If updating dependencies breaks your code, it's ultimately the fault of the dependency (and their maintainers will in turn blame you for not paying attention to their deprecation warnings). reply matsemann 12 hours agorootparentYou're all over the thread defending the standard python tools, which is fine, it works for you. But the amount of times you've had to write that something is natively supported already or people is just using it wrong speaks volumes about why people prefer uv: it just works without having to learn loads of stuff. reply loxias 20 hours agorootparentprevMy life got a lot easier since I adopted the habit of making a shell script, using buildah and podman, that wrapped every python, rust, or golang project I wanted to dabble with. It's so simple! Create a image with the dependencies, then `podman run` it. reply markerz 21 hours agorootparentprevYeah, it unifies the whole env experience with the package installation experience. No more forgetting to activate virtualenv first. No more pip installing into the wrong virtual env or accidentally borrowing from the system packages. It’s way easier to specify which version of python to use. Everything is version controlled including python version and variant like cpython, puppy, etc. it’s also REALLY REALLY fast. reply __mharrison__ 14 hours agorootparentprevI was in your boat too. Been using Python since 2000 and pretty satisfied with venv and pip. However, the speed alone is reason enough to switch. Try it once and you will be sold. reply benreesman 21 hours agorootparentprevPerformance and correctness mostly. reply mplewis 22 hours agorootparentprevPip only has requirements.txt and doesn't have lockfiles, so you can't guarantee that the bugs you're seeing on your system are the same as the bugs on your production system. reply aidos 21 hours agorootparentI’ve always worked around that by having a requirements.base.txt and a requirements.txt for the locked versions. Obviously pip doesn’t do that for you but it’s not hard to manage yourself. Having said that, I’m going to give uv a shot because I hear so many good things about it. reply selcuka 19 hours agorootparentWith pip the best practice is to have a requirements.txt with direct requirements (strictly or loosely pinned), and a separate constraints.txt file [1] with strictly pinned versions of all direct- and sub-dependencies (basically the output of `pip freeze`). The latter works like a lock file. [1] https://pip.pypa.io/en/stable/user_guide/#constraints-files reply zahlman 13 hours agorootparentFor direct requirements you're better off using the `pyproject.toml` for direct dependencies (and you can plausibly use it to pin everything if you're developing an application). It's project metadata that you'll need anyway for building your project, and the \"editable wheel\" hack allows Pip to use that information to set up an environment for your code (via `pip install -e .`). reply remram 18 hours agorootparentprevThis works until you need to upgrade something, pip might upgrade to a broken set of dependencies. Or if you run on a different OS and the dependencies are different there (because of env markers), your requirements file won't capture that. There are a lot of gotchas that pip can't fix. reply zahlman 12 hours agorootparent> pip might upgrade to a broken set of dependencies. I'm only aware of examples where it's the fault of the packages - i.e. they specify dependency version ranges that don't actually work for them (or stop working for them when a new version of the dependency is released). No tool can do anything about that on the user's end. > Or if you run on a different OS and the dependencies are different there (because of env markers), your requirements file won't capture that. There are a lot of gotchas that pip can't fix. The requirements.txt format is literally just command-line arguments to Pip, which means you can in fact specific the env markers you need there. They're part of the https://peps.python.org/pep-0508/ syntax which you can use on the Pip command line. Demo: $ pip install 'numpy;python_version There are a lot of gotchas that pip can't fix. There are a lot of serious problems with Pip - I just don't think these are among them. reply remram 4 hours agorootparentYou can specify markers in the requirements file you write, not in the frozen requirements from 'pip freeze'. Because it's just a list of what's installed on your machine. reply zahlman 1 hour agorootparentRunning 'pip freeze' creates a plain text file. You can edit it to contain anything that would have been in \"the requirements file you write\". \"Your requirements file\" may or may not capture what it needs to, depending on how you created it. But Pip supports it. (And so does the `pyproject.toml` specification.) reply mikepurvis 21 hours agorootparentprevI’m grouchy because I finally got religion on poetry a few years ago, but the hype on uv is good enough that I’ll have to give it a shot. reply kstrauser 21 hours agorootparentI freaking love Poetry. It was a huge breath of fresh air after years of pip and a short detour with Pipenv. If uv stopped existing I’d go back to Poetry. But having tasted the sweet nectar of uv goodness, I’m onboard the bandwagon. reply remram 18 hours agorootparentprevWith the new major release of Poetry that just came out I also feel like it might be a good time to switch to Uv rather than adapt to this new version: https://python-poetry.org/blog/announcing-poetry-2.0.0/ reply polski-g 19 hours agorootparentprev\"pip freeze\" generates a lockfile. reply orf 19 hours agorootparentNo, that generates a list of currently installed packages. That’s very much not a lock file, even if it is possible to abuse it as such. reply zahlman 12 hours agorootparentA list of currently installed packages in the current environment, with their exact versions. This is only the actually needed packages, with their transitive dependencies, unless you've left something behind from earlier in development. If you're keeping abstract dependencies up to date in `pyproject.toml` (which you need to do anyway to build and release the project), you can straightforwardly re-create the environment from that list and freeze whatever solution you get (after testing). reply orf 9 hours agorootparentDoesn’t account for differences in platforms or Python versions, and doesn’t contain resolved dependency hashes. So it’s a “lockfile” in the strictest, most useless definition: only works on the exact same Python version, on my machine, assuming no dependencies have published new packages. reply zahlman 8 hours agorootparentLook, I'm not trying to sell this as a full solution - I'm just trying to establish that a lot of people really don't need a full solution. >only works on the exact same Python version It works on any Python version that all of the dependencies work on. But also it can be worked around with environment markers, if you really can support multiple Python versions but need a different set of dependencies for each. In practical cases you don't need anything like a full (python-version x dependency) version matrix. For example, many projects want to use `tomllib` from the standard library in Python 3.11, but don't want to drop support for earlier Python because everything else still works fine with the same dependency packages for all currently supported Python versions. So they follow the steps in the tomli README (https://github.com/hukkin/tomli?tab=readme-ov-file#building-...). >on my machine (Elsewhere in the thread, people were trying to sell me on lock files for library development, to use specifically on my machine while releasing code that doesn't pin the dependencies.) If my code works with a given version of a dependency on my machine, with the wheel pre-built for my machine, there is no good reason why my code wouldn't work on your machine with the same version of the dependency, with the analogous wheel - assuming it exists in the first place. It was built from the same codebase. (If not, you're stuck building from source, or may be completely out of luck. A lockfile can't fix that; you can't specify a build artifact that doesn't exist.) This is also only relevant for projects that include non-Python code that requires a build step, of course. >assuming no dependencies have published new packages. PyPI doesn't allow you to replace the package for the same version. That's why there are packages up there with `.post0` etc. suffixes on their version numbers. But yes, there are users who require this sort of thing, which is why PEP 751 is in the works. reply orf 6 hours agorootparentSo many misunderstandings here :/ I can’t muster the energy to correct them past these two obvious ones > It works on any Python version that all of the dependencies work on No, it doesn’t. It’s not a lockfile: it’s a snapshot of the dependencies you have installed. The dependencies you have installed depend on the Python version and your OS. The obvious case would be requiring a Linux-only dependency on… Linux, or a package only required on PythonPyPI doesn't allow you to replace the package for the same version Yes and no. You can continue to upload new wheels (or a sdist) long after a package version is initially released. reply zahlman 1 hour agorootparent>So many misunderstandings here :/ I've spent most of the last two years making myself an expert on the topic of Python packaging. You can see this through the rest of the thread. >No, it doesn’t. It’s not a lockfile: it’s a snapshot of the dependencies you have installed. Yes, it does. It's a snapshot of the dependencies that you have installed. For each of those dependencies, there is some set of Python versions it supports. Collectively, the packages will work on the intersection of those sets of Python versions. (Because, for those Python versions, it will be possible to obtain working copies of each dependency at the specified version number.) Which is what I said. > The dependencies you have installed depend on the Python version and your OS. The obvious case would be requiring a Linux-only dependency on… Linux, or a package only required on Python You can continue to upload new wheels (or a sdist) long after a package version is initially released. Sure, but that doesn't harm compatibility. In fact, I would be doing it specifically to improve compatibility. It wouldn't change what Pip chooses for your system, unless it's a better match for your system than previously available. reply orf 1 hour agorootparentHoly hell dude, you don’t need to write a novel for every reply. It’s not a lockfile because it’s a snapshot of what you have installed. End of. It doesn’t handle environment markers nor is it reproducible. Given any non-trivial set of dependencies and/or more than 1 platform, it will lead to confusing issues. Those confusing issues are the reason for lock files to exist, and the reason they are not just “the output of pip freeze”. But you know this, given your two years of extensive expert study. Which I see very little evidence of. reply zahlman 58 minutes agorootparent>It’s not a lockfile because it’s a snapshot of what you have installed. I didn't say it was. I said that it solves the problems that many people mistakenly think they need a lockfile for. (To be clear: did you notice that I am not the person who originally said \"'pip freeze' generates a lockfile.\"?) >It doesn’t handle environment markers nor is it reproducible. You can write environment markers in it (of course you won't get them from `pip freeze`) and Pip will respect them. And there are plenty of cases where no environment markers are applicable anyway. It's perfectly reproducible insofar as you get the exact specified version of every dependency, including transitive dependencies. >Given any non-trivial set of dependencies and/or more than 1 platform, it will lead to confusing issues. Given more than 1 platform, with differences that actually matter (i.e. not pure-Python dependencies), you cannot use a lockfile, unless you specify to build everything from source. Because otherwise a lockfile would specify wheels as exact files with their hashes that were pre-built for one platform and will not work on the others. Anyway, feel free to show a minimal reproducible example of the confusion you have in mind. reply orf 34 minutes agorootparent> Given more than 1 platform, with differences that actually matter (i.e. not pure-Python dependencies), you cannot use a lockfile, unless you specify to build everything from source. Because otherwise a lockfile would specify wheels as exact files with their hashes that were pre-built for one platform and will not work on the others. What is more likely: 1. Using a lockfile means you cannot use wheels and have to build from source 2. You don’t know what you’re talking about (When deciding, keep in mind that every single lockfile consuming and producing tool works fine with wheels) zo1 21 hours agorootparentprevThe requirements.txt file is the lockfile. Anyways, this whole obsession with locked deps or \"lockfiles\" is such an anti-pattern, I have no idea why we went there as an industry. Probably as a result of some of the newer stuff that is classified as \"hipster-tech\" such as docker and javascript. reply n144q 20 hours agorootparentJust because you don't understand it, it's ok to call it an \"anti-pattern\"? Reproducibility is important in many contexts, especially CI, which is why in Node.js world you literally do \"npm ci\" that installs exact versions for you. If you haven't found it necessary, it's because you haven't run into situations where not doing this causes trouble, like a lot of trouble. reply driggs 20 hours agorootparentJust because someone has a different perspective than you doesn't mean they don't \"understand\". Lockfiles are an anti-pattern if you're developing a library rather than an application, because you can't push your transitive requirements onto the users of your library. reply Uvix 20 hours agorootparentIf you're developing a library, and you have a requirement for what's normally a transitive dependency, it should be specified as a top-level dependency. reply driggs 19 hours agorootparentThe point is that if I'm writing a library and I specify `requests == 1.2.3`, then what are you going to do in your application if you need both my library and `requests == 1.2.4`? This is why libraries should not use lockfiles, they should be written to safely use as wide a range of dependencies' versions as possible. It's the developers of an application who should use a lockfile to lock transitive dependencies. reply phinnaeus 17 hours agorootparentThe lock file is for developers of the library, not consumers. Consumers just use the library’s dependency specification and then resolve their own dependency closure and then generate a lock file for that. If you, as a library developer, want to test against multiple versions of your dependencies, there are other tools for that. It doesn’t make lock files a bad idea in general. reply zahlman 12 hours agorootparentAs another library developer, of course I want to test against multiple versions. Or more accurately, I don't want to prevent my users from using different versions prematurely. My default expectation is that my code will work with a wide range of those versions, and if it doesn't I'll know - because I like to pay attention to other libraries' deprecations, just as I'd hope for my users to pay attention to mine. Lockfiles aren't helpful to me here because the entire point is not to be dependent upon specific versions. I actively want to go through the cycle of updating my development environment on a whim, finding that everything breaks, doing the research etc. - because that's how I find out what my version requirements actually are, so that I can properly record them in my own project metadata. And if it turns out that my requirements are narrow, that's a cue to rethink how I use the dependency, so that I can broaden them. If I had a working environment and didn't want to risk breaking it right at the moment, I could just not upgrade it. If my requirements were complex enough to motivate explicitly testing against a matrix of dependency versions, using one of those \"other tools\", I'd do that instead. But neither way do I see any real gain, as a library developer, from a lock file. reply imtringued 11 hours agorootparent>If I had a working environment and didn't want to risk breaking it right at the moment, I could just not upgrade it. The point of a lockfile is to only upgrade when you want to upgrade. I hope you understand that. reply zahlman 9 hours agorootparentWhy do I need a special file in order to not do something? reply orf 18 hours agorootparentprevThat’s not the perspective that OP was sharing, though. reply remram 18 hours agorootparentprevYou literally phrased it as \"I have no idea why\". You can't be upset if someone feels you don't understand why. reply Pannoniae 16 hours agorootparent\"I have no idea why\" the industry went there. One can understand a technology or a design pattern yet think it's completely idiotic. (low-hanging fruit: JavaScript, containers, etc.) reply wiseowise 20 hours agorootparentprevI’m pretty sure it was a sarcasm. reply 2wrist 21 hours agorootparentprevAlso you can set the python version for that project. It will download whatever version you need and just use it. reply whimsicalism 21 hours agorootparentprevin my view, depending on your workflow you might have been missing out on pyenv in the past but not really if you feel comfortable self-managing your venvs. now though, yes unequivocally you are missing out. reply rat87 16 hours agorootparentprevPip is sort of broken before because it encourages confusion between requirements and lock files. In other languages with package managers you generally specify your requirements with ranges and get a lock file with exact versions of those and any transitive dependencies letting you easily recreate a known working environment. The only way to do that in pip is to make a *new* venue install then pip freeze. I think pip tools package is supposed to help but it's a separate tool (one which I've also includes). Also putting stuff in pyproject.toml feels more solid then requirements files (and allows options to be set on requirements (like installing only one package that's only on your company's private python package index mirror while installing the others from the global python package index) and allows dev dependencies and other optional features dependency groups without multiple requirements files and having to update locks on those files. It also automatically creates venvs if you delete them. And it automatically updates packages when you run something with uv run file.py (useful when somebody may have updated the requirements in git). It also lets you install self contained (installed in a virtualenv and linked to ~/.local/bin which is added to your path)python tools (replacing pipx). It installs self contained python builds letting you more easily pick python version and specify it in a .python-version file for your project (replacing pyenv and usually much nicer because pyenv compiles them locally) Uv also makes it easier to explore and say start a ipython shell with 2 libraries uv run --with ipython --with colorful --with https ipython It caches downloads. Of course the http itself isn't faster but they're exploring things to speed that part up and since it's written in rust local stuff (like deleting and recreating a venv with cached packages) tends to be blazing fast reply mardifoufs 21 hours agorootparentprevWell, for one you can't actually package or add a local requirement (for example , a vendored package) to the usual pip requirements.txt (or with pyproject.toml, or any other standard way) afaik. I saw a discourse reply that cited some sort of possible security issue but that was basically it and that means that the only way to get that functionality is to not use pip. It's really not a lot of major stuff, just a lot of little paper cuts that makes it a lot easier to just use something else once your project gets to a certain size. reply BeefWellington 21 hours agorootparentSure you can. It's in their example for how to use requirements.txt: https://pip.pypa.io/en/stable/reference/requirements-file-fo... Maybe there's some concrete example you have in mind though? reply mardifoufs 19 hours agorootparentI don't think so, though maybe I didn't explain myself correctly. You can link to a relative package wheel I think, but not to a package repo. So if you have a repo, with your main package in ./src, and you vendor or need a package from another subfolder (let's say ./vendored/freetype) , you can't actually do it in a way that won't break the moment you share your package. You can't put ./vendored/freetype in your requirements.txt, it just fails. That means you either need to use pypi or do an extremely messy hack that involves adding the vendored package as a sub package to your main source, and then do some importlib black magic to make sure that everything uses said package. https://github.com/pypa/pip/issues/6658 https://discuss.python.org/t/what-is-the-correct-interpretat... reply BeefWellington 16 hours agorootparentIn this scenario, reading between the lines, the vendor is not providing a public / published package but does provide the source as like a tarball? I have yet to run into that particular case where the vendor didn't supply their own repo in favour of just providing the source directly. However I do use what are essentially vendor-supplied packages (distant teams in the org) and in those cases I just point at their GitLab/GitHub repo directly. Even for some components within my own team we do it this way. reply mardifoufs 14 hours agorootparentIt's more for either monorepos or in my case, to fix packages that have bugs but that I can't fix upstream. So for me, in my specific case, the freetype-py repo has a rather big issue with Unicode paths (it will crash the app if the path is in Unicode). There's a PR but it hasn't and probably won't get merged for dubious reasons. The easy choice, the one that actually is the most viable, is to pull the repo with the patch applied, temporarily add it to my ./vendored folder and just ideally change the requirements.txt with no further changes (or need to create a new pypi package). But it's basically impossible since I just can't use relative paths like that. Again it's rather niche but that's just one of the many problems I keep encountering. packaging anything with CUDA is still far worse, for example. reply zahlman 12 hours agorootparent>The easy choice, the one that actually is the most viable, is to pull the repo with the patch applied, temporarily add it to my ./vendored folder and just ideally change the requirements.txt with no further changes (or need to create a new pypi package). But it's basically impossible since I just can't use relative paths like that. You can use the repo's setup to build a wheel, then tell pip to install from that wheel directly (in requirements.txt, give the actual path/name of the wheel file instead of an abstract dependency name). You need a build frontend for this - `pip wheel` will work and that's more or less why it exists; but it's not really what Pip is designed for overall - https://build.pypa.io/en/stable/ is the vanilla offering. reply o11c 21 hours agorootparentprevMuch of the Python ecosystem blatantly violates semantic versioning. Most new tooling is designed to work around the bugs introduced by this. reply sgarland 20 hours agorootparentTo be fair, Python itself doesn’t follow SemVer. Not in a “they break things they shouldn’t,” but in a “they never claim to be using SemVer.” reply zahlman 12 hours agorootparentprevRelevant: https://iscinumpy.dev/post/bound-version-constraints/ Semver is hard; you never know what will break at least one of your users (see Hyrum's law), but on the other hand, clear backwards-compatibility breaks will often not affect a large fraction of users - if they preemptively declare that they won't support your next version, they may prevent Pip from finding a set of versions that would actually work just fine. reply coldtea 18 hours agorootparentprevCool story bro. I've used pip, pyenv, poetry, all are broken in one way or another, and have blind spots they don't serve. If your needs are simple (not mixing Python versions, simple dependencies, not packaging, etc) you can do it with pip, or even with tarballs and make install. reply remus 21 hours agorootparentprevPip doesn't resolve dependencies for you. On small projects that can be ok, but if you're working on something medium to large, or you're working on it with other people you can quickly get yourself into a sticky situation where your environment isn't easily reproducible. Using uv means your project will have well defined dependencies. reply mianos 21 hours agorootparenthttps://pip.pypa.io/en/stable/topics/dependency-resolution/ reply remus 21 hours agorootparentMy bad, see PaulHoule's comment for what I was getting at. reply mosselman 21 hours agorootparentprevOh wow it doesn’t? What DOES it do then? As I commented here just now I never got pip. This explains it. reply zo1 21 hours agorootparentThe guy doesn't know what he's talking about as pip certainly has dependency resolution. Rather get your python or tech info from a non-flame-war infested thread full of anti-pip and anti-python folk. reply benatkin 20 hours agoparentprevThis: > I haven't felt like it's a minor improvement on what I'm using means that this: > I'd love if we standardized on it as a community as the de facto default …probably shouldn’t happen. The default and de facto standard should be something that doesn’t get put on a pedestal but stays out of the way. It would be like replacing the python repl with the current version of ipython. I’d say the same thing, that it isn’t a minor improvement. While I almost always use ipython now, I’m glad it’s a separate thing. reply lolinder 17 hours agorootparent> The default and de facto standard should be something that doesn’t get put on a pedestal but stays out of the way. The problem is that in the python ecosystem there really isn't a default de facto standard yet at all. It's supposed to be pip, but enough people dislike pip that it's hard as a newcomer to know if it's actually the standard or not. The nice thing about putting something like this on a pedestal is that maybe it could actually become a standard, even if the standard should be simple and get out of the way. Better to have a standard that's a bit over the top than no standard at all. reply benatkin 17 hours agorootparentIt feels even more standard than it used to, with python -m pip and python -m venv making it so it can be used with a virtalenv even if only python or python3 is in your path. reply zahlman 12 hours agorootparentJust for the record, `venv` has been in since Python 3.3, although it didn't bootstrap Pip into the environment until 3.4 and wasn't the officially blessed way of doing things until 3.5 (which was still over 9 years ago). Pip isn't part of the standard library; the standard library `ensurepip` (called from `venv`) includes a vendored wheel for Pip (https://github.com/python/cpython/tree/main/Lib/ensurepip/_b...), and imports Pip from within that wheel to bootstrap it. (Wheels are zip files, and Python natively knows how to import code from a zip file, so this just involves some `sys.path` manipulation. The overall process is a bit complex, but it's all in https://github.com/python/cpython/blob/main/Lib/ensurepip/__... .) This is why you get prompted to upgrade Pip all the time in new virtual environments (unless you preempt that with `--upgrade-deps`). The core development team still wants to keep packaging at arms length. This also allows Pip to be developed, released and versioned independently. reply lolinder 17 hours agorootparentprevOh, it's certainly more standard than it used to be, and maybe it's on the way to being fully standard. But it definitely hasn't arrived in the spot that npm, cargo, hex, bundler, and similar have in their respective ecosystems. Npm is a pretty good example of what pip should be. Npm has had to compete with other package managers for a long time but has remained the standard simply because it actually has all the basic features that people expect out of a package manager. So other package managers can spin up using the npm registry providing slightly better experiences in certain ways, but npm covers the basics. Pip really does not even cover the basics, hence the perpetual search for a better default. reply zahlman 12 hours agorootparentPip intentionally and by design does not cover package management. It covers package installation - which is more complex for Python than for other languages because of the expectation of being able to (try to) build code in other languages, at install time. reply comex 13 hours agorootparentprevAs it happens, the Python REPL was just replaced a few months ago! …Not with IPython. But with an implementation written in Python instead of C, originating from the PyPy project, that supports fancier features like multi-line editing and syntax highlighting. See PEP 762. I was apprehensive when I heard about it, but then I had the chance to use it and it was a very nice experience. reply benatkin 12 hours agorootparentOoh, nice. I think recently the times I've used the latest version of python it's been with ipython, so I didn't notice. Going to check it out! It might be easier to make a custom repl now. reply greazy 19 hours agorootparentprev> The default and de facto standard should be something that doesn’t get put on a pedestal but stays out of the way. This to me is unachievable. Perfection is impossible. On the the way there if the community and developers coalesced around a single tool then maybe we can start heading down the road to perfectionism. reply benatkin 19 hours agorootparentI mean, stays out of the way for simple uses. When I first learned Python, typing python and seeing >>> and having it evaluate what I typed as if it appeared in a file was a good experience. Now that I use python a lot, ipython is more out of the way to me than the built-in python repl is, because it lets me focus on what I'm working on, than limitations of a tool. reply mrbonner 20 hours agoparentprev+1 uv now also supports system installation of python with the --default --preview flags. This probably allows me to replace mise (rtx) and go uv full time for python development. With other languages, I go back to mise. reply tehjoker 22 hours agoparentprevWhat is the deal with uv's ownership policy? I heard it might be VC backed. To my mind, that means killing pip and finding some kind of subscription revenue source which makes me uneasy. The only way to justify VC money is a plot to take over the ecosystem and then profit off of a dominant position. (e.g. the Uber model) I've heard a little bit about UV's technical achievements, which are impressive, but technical progress isn't the only metric. reply feznyng 18 hours agorootparentIt’s dual MIT and Apache licensed. Worst case, if there’s a rug pull, fork it. reply tehjoker 15 hours agorootparentIs that the entire story? If so the VCs are pretty dumb. If they kill pip, that means the people who were maintaining it disperse and forking it won't restore the ecosystem that was there before. reply zahlman 16 hours agoparentprev(I will likely base a blog post in my packaging series off this comment later.) What people seem to miss about Pip is that it's by design, not a package manager. It's a package installer, only. Of course it doesn't handle the environment setup for you; it's not intended for that. And of course it doesn't keep track of what you've installed, or make lock files, or update your `pyproject.toml`, or... What it does do is offer a hideously complex set of options for installing everything under the sun, from everywhere under the sun. (And that complexity has led to long-standing, seemingly unfixable issues, and there are a lot of design decisions made that I think are questionable at best.) Ideas like \"welllll I use poetry but pyenv works or you could use conda too\" are incoherent. They're for different purposes and different users, with varying bits of overlap. The reason people are unsatisfied is because any given tool might be missing one of the specific things they want, unless it's really all-in-one like Uv seems like it intends to be eventually. But once you have a truly all-in-one tool, you notice how little of it you're using, and how big it is, and how useless it feels to have to put the tool name at the start of every command, and all the specific little things you don't like about its implementation of whatever individual parts. Not to mention the feeling of \"vendor lock-in\". Never mind that I didn't pay money for it; I still don't want to feel stuck with, say, your build back-end just because I'm using your lock-file updater. In short, I don't want a \"package manager\". I want a solid foundation (better than Pip) that handles installing (not managing) applications and packages, into either a specified virtual environment or a new one created (not managed, except to make it easy to determine the location, so other tools can manage it) for the purpose. In other words, something that fully covers the needs of users (making it possible to run the code), while providing only the bare minimum on top of that for developers - so that other developer tools can cooperate with that. And then I want specialized tools for all the individual things developers need to do. The specialized tools I want for my own work all exist, and the tools others want mostly exist too. Twine uploads stuff to PyPI; `build` is a fine build front-end; Setuptools would do everything I need on the back-end (despite my annoyances with it). I don't need a lockfile-driven workflow and don't readily think in those terms. I use pytest from the command line for testing and I don't want a \"package manager\" nor \"workflow tool\" to wrap that for me. If I needed any wrapping there I could do a few lines of shell script myself. If anything, the problem with these tools is doing too much, rather than too little. The base I want doesn't exist yet, so I've started making it myself. Pipx is a big step in the right direction, but it has some arbitrary limitations (I discuss these and some workarounds in my recent blog post https://zahlman.github.io/posts/2025/01/07/python-packaging-... ) and it's built on Pip so it inherits those faults and is that much bigger. Uv is even bigger still for the compiled binary, and I would only be using the installation parts. reply botanical76 11 hours agorootparentDo you feel that Npm, mix, cargo went the wrong way, doing too much? It seems like their respective communities _love_ the standard tooling and all that it does. Or is Python fundamentally different? reply zahlman 9 hours agorootparentPython is fundamentally used in different ways, and in particular is much more often used in conjunction with code in other languages that has to be compiled separately and specially interfaced with. It also has a longer history, and a userbase that's interested in very different ways of using Python than the \"development ecosystem\" model where you explicitly create a project with the aim of adding it to the same package store where you got your dependencies from. Different users have wildly different needs, and tons of the things workflow tools like uv/Poetry/PDM/Hatch/Flit do are completely irrelevant to lots of them. Tons of users don't want to \"manage dependencies\"; they want to have a single environment containing every package that any of their projects ever uses (and a shoulder to cry on if a version conflict ever arises from that). Tons of users don't want to make a \"project\" and they especially don't want to set up their Python code as a separate, reusable thing, isolated from the data they're working on with it right now. Tons of users think they know better than some silly \"Pip\" tool about exactly where each of their precious .py files ought to go on your hard drive. Tons of developers want their program to look and feel like a stand-alone, independent application, that puts itself in C:\\Program Files and doesn't expect users to know what Python is. People more imaginative than I could probably continue this train of thought for quite a bit. For many of the individual tasks, there are small Unix-philosophy tools that work great. Why is typing `poetry publish` better than typing `twine upload`? (And it would be just `twine`, except that there's a `register` command that PyPI doesn't need in the first place, and a `check` command that's only to make sure that other tools did their job properly - and PyPI will do server-side checks anyway.) Why is typing `poetry run ...` better than activating the venv with the script it provided itself, and then doing the `...` part normally? An all-in-one tool works when people agree on the scope of \"all\", and you can therefore offer just one of them and not get complaints. reply imtringued 11 hours agorootparentprevVirtualenv should have never existed in the first place. So you claiming that UV or whatever tool is doing too much, sounds to me like you're arguing based on \"traditionalist\" or \"conservative\" reasons rather than doing any technical thinking here. Node.js's replacement for virtualenv is literally just a folder named \"node_modules\". Meanwhile python has an entire tool with strange ideosyncracies that you have to pay attention to otherwise pip does the wrong thing by default. It is as if python is pretending to be a special snowflake where installing libraries into a folder is this super hyper mega overcomplicated thing that necessitates a whole dedicated tool just to manage, when in reality in other programming languages nobody is really thinking about that the fact that the libraries end up in their build folders. It just works. So again you're pretending that this is such a big deal that it needs a whole other tool, when the problem in question is so trivial that another tool is adding mental overhead with regard to the microscopic problem at hand. reply dragonwriter 32 minutes agorootparent> Node.js's replacement for virtualenv is literally just a folder named \"node_modules\". Node_modules doesn't support an isolated node interpreter distinct from what may be installed elsewhere on the machine. Third party tools are available that do that for node, but node_modules alone addresses a subset of the issues that venvs solve. OTOH, its arguably undesirable that there isn't a convenient way in Python to do just what node_modules does without the extra stuff that venvs do, because there are a lot of use cases where that kind of solution would be sufficient and lower overhead. reply zahlman 9 hours agorootparentprevUv doing things I'm not interested in, has absolutely nothing to do with the design of virtualenvs. But virtualenvs are easy enough to work with; they absolutely don't \"necessitate a whole dedicated tool just to manage\" unless you could the `activate` script that they come with. But also, Uv uses them anyway. Because that's the standard. And if the Python standard were to use a folder like node_modules, Uv would follow suit, and so would I. And Uv would still be doing package management and workflow tasks that I'm completely uninterested in, and presenting an \"every command is prefixed with the tool suite name* UI that I don't like. There was a proposal for Python to use a folder much like node_modules: see https://discuss.python.org/t/pep-582-python-local-packages-d... . It went through years of discussion and many problems with the idea were uncovered. One big issue is that installing a Python package can install more than just the importable code; the other stuff has to be put somewhere sensible. >So again you're pretending that this is such a big deal that it needs a whole other tool I make no such claim. The tool I want is not for managing virtual environments. It's for replacing Pip, and offering a modicum of convenience on top of that. Any time you install a package, no matter whether you use Pip or uv or anything else, you have to choose (even if implicitly) where it will be installed. Might as well offer the option of setting up a new location, as long as we have a system where setup is necessary. reply dragonwriter 30 minutes agorootparent> But virtualenvs are easy enough to work with; they absolutely don't \"necessitate a whole dedicated tool just to manage\" unless you could the `activate` script that they come with. venv is a dedicated tool. virtualenv is a different dedicated tool. reply riwsky 21 hours agoprevHeck, you can get even cleaner than that by using uv’s support for PEP 723’s inline script dependencies: # /// script # requires-python = \">=3.12\" # dependencies = [ # \"pandas\", # ] # /// h/t https://simonwillison.net/2024/Dec/19/one-shot-python-tools/ reply shlomo_z 17 hours agoparentIs it possible for my IDE (vscode) to support this? Currently my IDE screams at me for using unknown packages and I have no type hinting, intellisense, etc. reply Zizizizz 8 hours agorootparentWith your python plugin you should be able choose .venv/bin/python as your interpreter after you've run `uv sync` and everything should resolve reply randomlurking 2 hours agorootparentSo it doesnt work wirk adhoc venvs, does it? Might still valuable for simply running scripts, but I’m not sure venv-less works for dev reply aeurielesn 20 hours agoparentprevI don't understand how things like this get approved into PEPs. reply Karupan 20 hours agorootparentSeems like a great way to write self documenting code which can be optionally used by your python runtime. reply zanie 20 hours agorootparentprevAs in, you think this shouldn't be possible or you think it should be written differently? reply epistasis 20 hours agorootparentprevThe PEP page is really good at explaining the status of the proposal, a summary of the discussion to date, and then links to the actual detailed discussion (in Discourse) about it: https://peps.python.org/pep-0723/ reply noitpmeder 19 hours agorootparentI see this was accepted (I think?); is the implementation available in a released python version? I don't see an \"as of\" version on the pep page, nor do lite google searches reveal any official python docs of the feature. reply jonesetc 18 hours agorootparentIt's not a python the language feature, it's for packaging. So no language version is relevant. It's just there for any tool that wants to use it. uv, an IDE, or anything else that manages virtual environments would be the ones who implement it independent of python versions. reply epistasis 15 hours agorootparentprevThis is a specification for Python packaging, which is tooling separate from Python releases (for better or worse, IMHO worse but the BDFL disagrees). There's a box below the Table of Contents of the PEP that points here: https://packaging.python.org/en/latest/specifications/inline... reply rtpg 13 hours agorootparentprevIt's helpful as a way to publish minimal reproductions of bugs and issues in bug reports (compared to \"please clone my repo\" which has so many layers of friction involved). I would want distributed projects to do things properly, but as a way to shorthand a lot of futzing about? It's excellent reply misiek08 19 hours agorootparentprevAnd people were laughing at PHP comments configuring framework, right? reply throwup238 19 hours agorootparentPython was always the late born twin brother of PHP with better hair and teeth, but the same eyes that peered straight into the depths of the abyss. reply mkl 6 hours agorootparentPython was first released in 1991, and PHP was first released in 1995. reply franktankbank 17 hours agorootparentprevWhy come types? reply throwup238 16 hours agorootparentThat’s when each language reached sexual maturity but one went on to get a girlfriend and the other discovered internet porn. reply 8n4vidtmkvmk 13 hours agorootparentI don't know which is which in this story. reply blibble 16 hours agorootparentprevwith that expected use case of uv script run command it effectively makes those comments executable python's wheels are falling off at an ever faster and faster rate reply Spivak 13 hours agorootparentBecause of a feature that solves the problem of one off scripts being difficult the moment you need a 3rd party library? A more Pythonic way of doing this might be __pyproject__ bit that has the tiiiiny snag of needing to execute the file to figure out its deps. I would have loved if __name__ == \"pyproject\" but while neat and tidy it is admittedly super confusing for beginners, has a \"react hooks\" style gotcha where to can't use any deps in that block, and you can't use top level imports. The comment was really the easiest way. reply linsomniac 20 hours agorootparentprevI don't think this IS a PEP, I believe it is simply something the uv tool supports and as far as Python is concerned it is just a comment. reply mkl 20 hours agorootparenthttps://peps.python.org/pep-0723/ reply linsomniac 5 hours agorootparentThank you for the pointer, I had searched for it but couldn't find it. (edit: Glad I spent the downvotes to get educated :-) reply zanie 20 hours agorootparentprevNo, this is a language standard now (see PEP 723) reply epistasis 21 hours agoparentprevAnd for the Jupyter setting, check out Trevor Manz's juv: https://github.com/manzt/juv reply __MatrixMan__ 16 hours agoparentprevSo it's like a shebang for dependencies. Cool. reply bityard 17 hours agoprevI usually stay away far FAR from shiny new tools but I've been experimenting with uv and I really like it. I'm a bit bummed that it's not written in Python but other than that, it does what it says on the tin. I never liked pyenv because I really don't see the point/benefit building every new version of Python you want to use. There's a reason I don't run Gentoo or Arch anymore. I'm very happy that uv grabs pre-compiled binaries and just uses those. So far I have used it to replace poetry (which is great btw) in one of my projects. It was pretty straightforward, but the project was also fairly trivial/typical. I can't fully replace pipx with it because 'uv tool' currently assumes every Python package only has one executable. Lots of things I work with have multiple, such as Ansible and Jupyterlab. There's a bug open about it and the workarounds are not terrible, but it'd be nice if they are able to fix that soon. reply meitham 13 hours agoparentuv is great, but downloading and installing base python interpreter is not a good feature as it doesn’t fetch that from PSF but from a project on GitHub, that very same project says this is compiled for portability over performance, see https://gregoryszorc.com/docs/python-build-standalone/main/ reply zahlman 12 hours agorootparent>that very same project says this is compiled for portability over performance Realistically, the options on Linux are the uv way, the pyenv way (download and compile on demand, making sure users have compile-time dependencies installed as part of installing your tool), and letting users download and compile it themself (which is actually very easy for Python, at least on my distro). Compiling Python is not especially fast (around a full minute on my 4-core, 10-year-old machine), although I've experienced much worse in my lifetime. Maybe you can get alternate python versions directly from your distro or a PPA, but not in a way that a cross-distro tool can feasibly automate. On Windows the only realistic option is the official installer. reply globular-toast 10 hours agorootparentYes, which is why it's silly to do it. Developers (not \"users\"!) need to learn how to install Python on their system. I honestly don't know how someone can call themselves a Python developer if they can't even install the interpreter! reply zahlman 9 hours agorootparentThe Python community has the attitude that everyone needs to be welcomed; I mostly agree, and I don't see the point in fussing about what people want to call themselves. Everyone starts somewhere, and people try really hard to help (one random example from 2022: https://discuss.python.org/t/python-appears-in-cmd/15858) But overall, computer literacy is really on the decline these days. Nowadays before you can teach programming in any traditional sense, you may have to teach the concept of a file system, then a command line... reply bennofs 6 hours agorootparentprevI for one enjoy the convenience of automatically installing python versions. Yes I know how to do it manually. Yes it is possible to install multiple versions. But that does not mean I want to do it every time, just to test how things behave in different python versions. For that, it's also okay if it does not install the most performant version of the interpreter. reply zahlman 1 hour agorootparent>Yes it is possible to install multiple versions. But that does not mean I want to do it every time, just to test how things behave in different python versions You only have to do it once per version with this approach. Then you can create venvs from that base, and it's basically instantaneous if you do it `--without-pip`. reply globular-toast 5 hours agorootparentprevSure. We've had system package managers for decades. I install a major version once a year and it gets automatically upgraded to the latest patch version by my system package manager, just like everything else. reply bityard 2 hours agorootparentprevBut PSF doesn't distribute binary builds, so what's the alternative? reply meitham 2 hours agorootparentit does of course here https://www.python.org/downloads/ or are you referring to linux? reply brainzap 7 hours agorootparentprevthat it does it automatically is weird reply meitham 2 hours agorootparentI've only noticed after my corporate firewall stopped me! reply stevage 21 hours agoprevAs a NodeJS developer it's still kind of shocking to me that Python still hasn't resolved this mess. Node isn't perfect, and dealing with different versions of Node is annoying, but at least there's none of this \"worry about modifying global environment\" stuff. reply mdaniel 21 hours agoparentCaveat: I'm a node outsider, only forced to interact with it But there are a shocking number of install instructions that offer $(npm i -g) and if one is using Homebrew or nvm or a similar \"user writable\" node distribution, it won't prompt for sudo password and will cheerfully mangle the \"origin\" node_modules So, it's the same story as with python: yes, but only if the user is disciplined Now ruby drives me fucking bananas because it doesn't seem to have either concept: virtualenvs nor ./ruby_modules reply MrJohz 20 hours agorootparentIt's worth noting that Node allows two packages to have the same dependency at different versions, which means that `npm i -g` is typically a lot safer than a global `pip install`, because each package will essentially create its own dependency tree, isolated from other packages. In practice, NPM has a deduplication process that makes this more complicated, and so you can run into issues (although I believe other package managers can handle this better), but I rarely run into issues with this. That said, I agree that `npm i -g` is a poor system package manager, and you should typically be using Homebrew or whatever package manager makes the most sense on your system. That said, `npx` is a good alternative if you just want to run a command quickly to try it out or something like that. reply zahlman 11 hours agorootparent>It's worth noting that Node allows two packages to have the same dependency at different versions Yes. It does this because JavaScript enables it - the default import syntax uses a file path. Python's default import syntax uses symbolic names. That allows you to do fun things like split a package across the filesystem, import from a zip file, and write custom importers, but doesn't offer a clean way to specify the version you want to import. So Pip doesn't try to install multiple versions either (which saves it the hassle of trying to namespace them). You could set up a system to make it work, but it'd be difficult and incredibly ugly. Some other language ecosystems don't have this problem because the import is resolved at compile time instead. reply fny 19 hours agorootparentprevBecause you don’t need virtualenvs or ruby_modules. You can have however many versions of the same gem installed it’s simply referenced by a gemfile, so for Ruby version X you are guaranteed one copy of gem version Y and no duplicate",
    "originSummary": [
      "Uv offers a unique feature that simplifies ad-hoc scripting in Python by streamlining dependency and environment management. - Users can execute a single command, `uv run --python 3.12 --with pandas python`, to run scripts without altering the global environment. - This ease of use and memorability makes uv an attractive option for Python package management."
    ],
    "commentSummary": [
      "Uv is a package manager that simplifies managing Python dependencies and versions, offering advantages over traditional tools like pip, pyenv, and poetry. - It features inline script dependencies, rapid environment creation, and pre-compiled Python binaries, contributing to its popularity. - Despite concerns about its venture capital backing and reliance on non-Python Software Foundation (PSF) binaries, users value its speed and user-friendliness for Python project management."
    ],
    "points": 454,
    "commentCount": 381,
    "retryCount": 0,
    "time": 1736712409
  },
  {
    "id": 42677587,
    "title": "I deleted my social media accounts",
    "originLink": "https://asylumsquare.com/backstage/2025-01-12/why-i-deleted-my-social-media-accounts",
    "originBody": "Games Tiny Thor Tombs Of Myra Pirate's Gold Abbie's Farm Turrican Anthology Bloo Kid Backstage ← Why I deleted my social media accounts (and why you should too)2025-01-12 Photo by Prateek Katyal In case you missed it: Mark Zuckerberg recently announced that Meta is ditching its fact-checkers (check out the video here). As if that wasn’t enough, he casually mentioned that Meta is teaming up with Trump to fight EU regulations affecting their platforms. The video itself is so absurd it feels like it could’ve been written by J. Michael Straczynski for Babylon 5 — President Clark and Nightwatch vibes all the way. But nope, it’s not sci-fi. It’s our horrifying reality. Meanwhile, Over on Planet Elon... And then there’s Musk. Oh boy. These days, he’s practically glued to Trump. Recently, he hosted a live chat on Twitter with Alice Weidel, the co-leader of Germany’s AfD, a party flagged by the “Verfassungsschutz” as a far-right extremist group. The content? Let’s just say it made me want to yeet my phone into the nearest ocean. How anyone can take that level of garbage seriously is beyond me. But hey, bubbles are cozy, right? (EDIT: Just one example: Alice Weidel claimed that Nazi leader Adolf Hitler was not \"right-wing,\" but a communist instead.) Musk, Zuckerberg, and Trump — what a trio. Honestly, this could be the perfect setup for a dystopian sci-fi thriller. Except, spoiler alert: no happy ending here. Profit First, Morality... Somewhere in the Basement Social media platforms have turned lowering moral standards into an Olympic sport. And boy, are they winning. Sure, companies need to make money. That’s fair. But when they use every psychological trick in the book to keep you doom-scrolling for hours, it’s time to ask some questions. Add to that the constant stream of scammy ads, and you start wondering: “Do these people have even a shred of decency left?” Teens and Social Media: A Toxic Cocktail As a dad of two daughters, I see firsthand what these platforms are doing to young people. They communicate almost entirely through social media, which makes parenting (and schooling) a special kind of nightmare. Not that adults are any better, mind you. Studies have shown that excessive social media use can lead to anxiety, stress, and more. But do platform owners care? Of course not (Yes, they might say otherwise, but the reality is that it’s just PR). The earlier and deeper they integrate into our lives, the harder it becomes to pull away. Bye-Bye Social Media So, I quit. Twitter, TikTok, Facebook — all gone (EDIT: LinkedIn as well). It wasn’t an easy choice, especially since I used these platforms to promote my games. Losing 16,000 Twitter followers? Ouch. And I got a ton of good feedback by posting content to TikTok and Facebook. It wasn’t all bad: I discovered cool stuff, met great people. But at some point, enough is enough. And then thoughts came to my mind: What happens if I need to reach out to someone I haven’t spoken to in a decade? Or what if my next trailer could’ve gone viral? Oh, and where the hell do I even post about this blog post now? The Torture of Deleting Accounts It wasn’t just an emotional hurdle. Platforms like Facebook make the process a Kafkaesque nightmare. Hours spent wrestling with broken verification codes and other roadblocks. Coincidence? Doubt it. Their motto seems to be: “Make it hard enough, and maybe they’ll just stay.”And let’s not forget all the other accounts you signed up for using Facebook or Twitter. You know, the ones you now need to rescue before you accidentally lock yourself out of important stuff. And Then...? Once the accounts were finally gone, I realized just how much of a grip these platforms had on me. The number of times I reflexively typed \"t\" or \"f\" into my browser bar (which autocompletes to twitter.com or facebook.com) was honestly terrifying. Waiting for assets to build? Hit Twitter. Software update running? Quick Facebook check. It’s like opening a bag of chips you swore you’d save for later, only to find it empty five minutes later. Except here, instead of chips, you’re losing hours of productivity. I knew it was a problem before, but only now do I see just how bad it was. Final Thoughts Social media isn’t all bad. But when people like Zuckerberg decide to throw their moral compass overboard, it’s time to take a hard look at what we’re supporting. So, here’s my suggestion: think about whether you really want to stick around on these platforms. And if you decide to quit, stock up on snacks and patience — you’re gonna need both. What’s Next? Honestly? No idea. Some friends recommended Bluesky, but I’m holding off for now. Maybe I’ll go old-school and write more blog posts. Like back in the early 2000s, when you actually had to think before sharing your thoughts with the world. Sounds quaint, doesn’t it? Links: Discussion on Hacker News NewsletterIf you want to receive emails from us when we have any big announcements or updates about the development progress on our projects, feel free to sign up for our Newsletter. About us Legacy Games Jobs Contact Privacy © Copyright 2024 by Asylum Square Interactive GmbH. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=42677587",
    "commentBody": "I deleted my social media accounts (asylumsquare.com)381 points by joemanaco 20 hours agohidepastfavorite448 comments nindalf 19 hours agoThis advice to quit social media is always a hit on HN. When I was 10 years younger I read the same thing on HN, was thoroughly convinced and quit social media. I even followed the advice of trying to stay in touch by email. Sure. Turns out that a lot of people I knew posted huge life updates that I completely missed out on. I asked them why they didn’t tell me and they were confused. They said the posted it on social media. I can’t speak for everyone, but I know a lack of social media meant that I have lost touch with old acquaintances completely. I have a few close friends and that’s it. Maybe that’s an ok tradeoff to make, but it’s worth knowing that before getting into it. reply hypeatei 19 hours agoparent> Turns out that a lot of people I knew posted huge life updates that I completely missed out on This doesn't really seem that important if your only method of knowing this was a post blasted to hundreds (or thousands) of people. Or, to put it another way: if you mattered, you would've gotten a direct message or call from them. I'd argue that social media has normalized keeping up with people who aren't supposed to be part of your life forever. But, we should take a step back and realize that not everything should or will last forever. If you cross paths again then you can catch up, but having life updates constantly? No thanks. reply slg 15 hours agorootparent>if you mattered, you would've gotten a direct message or call from them. That ignores the asymmetry of a lot of life events. For example, if a parent died, I'm not going to call everyone in my life to tell them, I would have more important stuff on my mind. I might post it on social media and then the onus is on other people to reach out to me. And if someone doesn't reach out, it will hurt the relationship a little even if I'm not conscience of it because when I think of people who were there for me during a tough time, the friend who never knew my parent died wouldn't come to mind. reply anxoo 14 hours agorootparentalso in the old days, your friend bob would have told cory, \"hey, did you hear alice's dad died? we should all go out for drinks\". but we live in the bowling alone era, where we're increasingly isolated. quitting social media is not, on its own, going to fix your social life. and being on social media can make you more connected, or more miserable. the responsibility is yours reply jahsome 13 hours agorootparentI'm a firm believer being loosely connected to so many people isn't the fix many seem to think it is. I find shallow connections, which is about all social media can support IMO, are worthless at best and detrimental at worst. YMMV, but my quality of life increased in ways I can't even begin to describe by severing all the dozens or perhaps hundreds of shallow connections social media was encouraging me to cling to. With the saved time and energy, I've been able to cultivate far fewer-- but much deeper and more (mutually) fulfilling-- connections with those who are _actually_ important. reply kelnos 10 hours agorootparentCouldn't agree more. I haven't deleted my Facebook account, but I no longer sign into it (I kept it because of event invitations, but at this point no one I know uses it for that anymore either). I have a little over 1,000 \"friends\" there. Back when I scrolled my feed multiple times per day, I read so many things about so many people who I hadn't interacted with outside of Facebook posts for years and years and years. I read so many things about so many people who I didn't even interact with on Facebook, let alone outside of it. I don't miss any of that. Those connections were beyond shallow, and weren't adding anything positive or useful to my life. reply JavierFlores09 11 hours agorootparentprevThis kind of comment always makes me wonder, are the people doing this doing well financially to afford cutting off all those \"loose\" connections with people like that? Because I couldn't imagine just destroying these relationships for no reason when I myself have benefited vastly from keeping them alive, even if barely communicating at all with these people. I think this advice is generally harmful to networking as someone grows, which is vital in today's society reply jahsome 47 minutes agorootparentI wouldn't agree it is \"vital,\" but that definitely depends on perspective and one's goals, as well as the baseline level of privilege one enjoys. If someone's goal is to achieve CEO and/or the top 1%, certainly every single connection could hold extricable value. I'm perfectly fine hovering somewhere in the middle, even knowing I have the capability to achieve much more. My future is uncertain; I probably won't retire when I would have liked. I've accepted that, and choose to live in the present rather than focusing on the future. I know at least I won't die miserable tomorrow. I don't deny I could have done better financially by maintaining the status quo. Now that I think of it, I'm doing worse financially than when I was using facebook & twitter. I had more money, and my career was progressing at a much higher rate, but I was inconsolable. Without the money, and without the accompanying social media-imposed drag, I see the world more clearly. My relationships are stronger with my wife, kids, and close friends. I am much happier. reply kelnos 10 hours agorootparentprevI don't think this discussion is about professional networking. It's about personal and social connections. If quitting Facebook makes you un-/under-employed then I think you're Doing Life Wrong. GP mentions \"severing\" those connections, but I think that's even too strong a phrasing. There wasn't really anything there in the first place, so there wasn't anything to sever. Simply not reading someone else's social media posts anymore, when you didn't really interact with them outside Facebook (or for some people even inside Facebook) isn't really severing anything. reply GoblinSlayer 1 hour agorootparentprevI had only financial losses from these loose connections. Nobody will shove profit down your throat, but there are many greedy people that will try to extract profit from you. I basically work as a bank for them, muh connections, lol. reply oblio 7 hours agorootparentprev1. LinkedIn. 2. Keep the other accounts, just in case. 3. How exactly are remote connections helping? In the Western world, for example, people you haven't interacted with for months and months in real life for sure won't help you financially. For jobs stuff like LinkedIn is probably better, plus regular chats on 1 instant messenger. You don't need Instagram to keep up with them. reply wink 9 hours agorootparentprevI'm not sure I agree, but I'm not disagreeing on principle. You make it sound as if something was lost, maybe recently. In the grand scheme of things I'm not that old (41) but I don't even remember how that would have worked out, because I wasn't old enough to have people's parents die before social media, at least in my social circles. Yes, of course you'd hear about grandparents and such from your immediate friends but that's usually a handful and people would maybe not be shaken as much. I agree with you that social media doesn't have to mean \"blasting it to hundreds or thousands of followers\", but it's a thing where I actually liked Facebook. Not only techies, and getting enough updates from people who are not your closest friends that you have things to talk about (as in reference) when you met again (or talked synchronously, or privately). reply lodovic 9 hours agorootparentprevIn my circle, very few people maintain a social media presence. I cannot remember posting anything on social media myself - except maybe a job update on LinkedIn, and some light anonymous trolling on X. I don't have Facebook or Instagram accounts and so I never visit those sites anymore (as they require an account to read). Spending a lot of time posting on social media is seen as unintelligent, attention whoring, and a waste of time. reply oblio 7 hours agorootparent> In my circle, very few people maintain a social media presence. You are not characteristic for the population at large (neither am I, don't feel sad :-) ). reply watwut 8 hours agorootparentprev> but we live in the bowling alone era, where we're increasingly isolated What I see over years is that, especially in developers online groups, any usual and normal way of socializing is stigmatized. I remember reading comments about how lazy people who socialize with friends are and how we are better if we code every evening. I remember people being proud about spending christmas coding supposedly being superior to the rest of the family that is socializing. Now we are proud if we remove ourselves from social media. It is always the same - however other people socialize is wrong, they are stupid and lazy. We remove ourselves, because it is superior to not participate. Eventually those places die out or change, but we do not like the new places either. And in each iteration, we expect other people to do work of keeping and managing relationships while feeling superior over not doing that. reply short_sells_poo 8 hours agorootparentI don't think the parent poster was arguing to exclude themselves from social life or do coding instead of talking with people. They merely argued that it's better to have fewer but meaningful and deep connections with people you genuinely care about (and they care about you), rather than having a 1000 meaningless connections with people who are basically strangers on facebook. The role social media plays is in encouraging large numbers of superficial relationships, rather than a small handful of deep ones. It stands to reason: I don't need facebook to keep in touch with a dozen close family and friends. I can do that perfectly well in person, or over phone calls/messages. What the various social media apps did was kill the close circle of friends in favor of having 1000s of followers and turn everyone into a one-way broadcaster. reply oblio 7 hours agorootparentprev> What I see over years is that, especially in developers online groups, any usual and normal way of socializing is stigmatized. Developers are not typical of regular people. They're, basically by design, outliers. reply kelnos 10 hours agorootparentprevMy mother died when I was in college, before social media was a thing. I told a few closer friends about it, and asked them to spread the news and to tell others that I didn't really want to talk about it. I was missing a few weeks of the semester because of it, and knew that people would ask me where I'd been once I was back, and knew I wouldn't have the emotional bandwidth to tell everyone the story over and over and over, and accept their condolences gracefully. It makes me really sad if it's true that people assume that when they post big, difficult stuff like that on social media, anyone who doesn't see it doesn't care about them. Even for people who are active on social media, the feed and post promotion algorithms make it fairly likely that a decent chunk of people who really should see that post might not see it. reply austin-cheney 10 hours agorootparentprev> I would have more important stuff on my mind. I might post it on social media and then the onus is on other people to reach out to me. That seems so bizarre. Just 20+ years ago this sort of sympathy seeking broadcasting action was associated with mental health illness, like Munchausen Biproxy. Yes, back in the day if tragedy happened people would take deliberate effort to call each other. reply nkrisc 9 hours agorootparentI’m not on social media but people have been posting obituaries publicly in newspapers and such for centuries. reply oblio 7 hours agorootparentIt's very country specific. I'm from Romania and I think there were obituaries in newspapers, but I'm having a hard time thinking of people I know that did it. reply TheSpiceIsLife 8 hours agorootparentprevScale matters. You read the obituaries in your local paper, “oh, so and so has passed away”, you don’t know them particularly well, might or might not go to the funeral. Posting it to social media, then thinking if whoever doesn’t contact you to… what? “Sorry for your loss”? “My condolences” … hurts your relationship with that person? Call me old fashioned, but… Is it narcissistic in here, or is it just me? reply PawgerZ 2 hours agorootparent> Posting it to social media, then thinking if whoever doesn’t contact you to… what? “Sorry for your loss”? “My condolences” … hurts your relationship with that person? That's not what anyone said, you're out here fighting ghosts. > And if someone doesn't reach out, it will hurt the relationship a little even if I'm not conscience of it because when I think of people who were there for me during a tough time, the friend who never knew my parent died wouldn't come to mind. reply GoblinSlayer 2 hours agorootparentIt's implied, \"And if someone doesn't reach out to say “Sorry for your loss”, it will hurt the relationship a little\". reply Karrot_Kream 9 hours agorootparentprevRight but it's not 20+ years ago. 20+ years ago when my family visited relatives abroad, our relatives would get to the airport and often have to wait for our delayed flight because they had no way of knowing and half the day would be lost. If your flight arrived early then you just waited. That was normal. Now we update each other over a web messenger, arrive at our destination, hop onto the free WiFi, then wait until our relatives greet us. Technology changes the world around us. reply graemep 8 hours agorootparent> 20+ years ago when my family visited relatives abroad, our relatives would get to the airport and often have to wait for our delayed flight because they had no way of knowing Apart from phoning the airline or airport and checking whether the flight was on time. We used to do that all the time 30+ years ago. 20 years ago you could check on websites IIRC. reply TheSpiceIsLife 8 hours agorootparentBack in my day, we had to walk fifty miles in the snow, up hill both ways, and we couldn’t afford shoes, just to phone the airline. Back when men were real men, women were real women, and small furry creatures from Alpha Centauri were real small furry creatures from Alpha Centauri. reply graemep 6 hours agorootparentand you tell young people of today that and they just won't believe you. reply TheSpiceIsLife 8 hours agorootparentprevWhat has that got to do with social media? Instant messaging and group chat, I’d argue, are distinct services / protocols / products vis-à-vis social media. Strained analogies are weird. I like to call them sieved analogies, the other definition of strained. I strained your analogy and threw out the dross. reply Mashimo 9 hours agorootparentprev20+ years ago you would have put it in the local news paper. reply manuelmoreale 9 hours agorootparentAnd that is still a thing where I live here in Italy. reply 0xEF 9 hours agorootparentprevMost of us would not have even done that, though yes, the option was there, but that sort of thing was much more popular 40+ years ago. There was another discussion where this came up on HN recently, but people get quite emotionally defensive when you start scrutinizing their reasons for staying on social media, so it is hard to have an honest conversation about it without a bunch of hyperbolic takes. In my experience, it was designed to be addictive, partly by using our own behavior against us and partly by vindicating the desire for attention. The idea that we need to be sharing every aspect of our personal narrative with the world is problematic, as it turns out, but we are so steeped in it that's there's no chance of purifying those waters, again. To your point, yes, there was some aspect of this back in the day, what with obituaries in newspapers being out there to both acknowledge that a person lived, but also put out the call to any old acquaintances to come say goodbye, but it was a laughable effort by today's standards of maximum self-aggrandizing and competitive social engagement. We have to ask ourselves if that is a socially and mentally healthy position to be in, which is an admittedly scary question. reply Mashimo 8 hours agorootparent> but it was a laughable effort by today's standards of maximum self-aggrandizing and competitive social engagement. What does this mean? > The idea that we need to be sharing every aspect of our personal narrative with the world is problematic I know about one or two people who does this. And it's far away from an obituary. I'm not quite sure I get what you a saying. I just meant in my upbringing it was quite normal to share publicly when someone died. And they still do it today. reply 0xEF 6 hours agorootparent> What does this mean? Apologies if my wording was too vague. I am using 'Self-aggrandizing' to mean a high exhibition of self-importance, or to put it another way, advertising one's self in a way that makes minor events or details seem bigger than they are. I am using 'competitive social engagement' as an alternative phrase to \"Keeping up with the Joneses\" which illustrates comparing yourself to your neighbors in terms of status, wealth, moral fiber, etc. The invention of Social Media propelled us into extreme versions of these two very-human aspects of our psychology, which I believe to be both dangerous and ill-fated. My intention was not to attack in any way, I just thought your reference to obituaries was an interesting link to our past prior to social media that was worth exploring and comparing. In a way, we can think of our Facebook profile as an extended obituary since that data is all accessible after we die. In fact, I am experiencing this on Instagram, having just lost a friend on New Year's Day and sitting down to peruse his old Instagram posts for the happy memories therein. Your comment just got me thinking, so I decided to expound on it. added: I should maybe clarify that I'm of an age that remembers what the world was like before Social Media and the Internet as we know it today. The differences when I compare those two halves of my life tend to be alarmingly drastic, which is something that warrants examination, to me, since many HN readers might be a bit too young to remember, so from their perspective, Social Media habits are likely more normalized. reply Mashimo 5 hours agorootparentAh, yeah no problem :D I also had no social media in my upbringing, a bit of ICQ via dial up though. Got an Facebook account and smartphone way later compared to my peers. reply khafra 9 hours agorootparentprev> Just 20+ years ago this sort of sympathy seeking broadcasting action was associated with mental health illness, like Munchausen Biproxy. Do you have a reference for the claim that the diagnostic criteria for Munchausen By Proxy (or Factitious Disorder Imposed on Another) once included broadcast-type notices when a family member dies? The DSM-IV would have been in effect 20 years ago, and while version 5 doesn't have that in its warning signs, I guess it could have changed from the previous version? reply watwut 8 hours agorootparentprev20 years ago, death announcements were expected and normal. They appeared in places people were expected to see - including local newspapers. You would also see death announcement being read in churche, posted in buildings etc. 20 years ago people met in person more often and you learned this stuff via gossip and word of mouth. Not being told to you personally, but being told to a whole group of people. The aggressiveness of your response is absurd. No, it was not seen as a mental health illness at all. When you expect personal one to one call, it is equivalent of removing yourself from other social structures in the past. You can do it, but your relationships will weaken and eventually die out. Just like it happened in the past. reply slg 9 hours agorootparentprev> That seems so bizarre. We got a real pot, meet kettle situation here. It is absolutely wild to suggest that doing something standard like arranging for an obituary in the local newspaper would be viewed as a sign of mental illness. reply vFunct 14 hours agorootparentprevIndeed and in the olden times a lot of these life events would have been announced in the local newspaper. But these days, I don’t even know where to even buy a newspaper, let alone make sure everyone is reading it and keeping up with local news. So social media it is, which sucks because they’re extremely edited and filtered out by the algorithm. reply kelnos 10 hours agorootparentFor the people who you care about, you can contact them directly and set up a time to meet to catch up. Or catch up over text or email. Or start a messaging group with mutual friends and keep each other up to date that way. My feeling is that if you only get updates about someone's life via their blasts on social media, you're not really friends. So why do you need to hear about all that stuff? reply nradov 14 hours agorootparentprevOn Facebook at least, the algorithm is heavily tuned to prioritize major life events: births, deaths, graduations, marriages, etc. Occasionally those posts get filtered out but usually they do get prioritized near the top of your feed. reply flakeoil 8 hours agorootparentprevIt must be quite common sense to actively contact the people you know were friends or family to your parents. Not necessarily by phone unless you also know them well, but by email or text or whatever contact details your parents have in their contact book. I very much would think your parents would expect that of their children. >I'm not going to call everyone in my life to tell them It's particularly the people in your parents life you should inform, not necessarily the people in your life. Don't forget that your social media network is not the same as your parent's social media network (if at all they use it). reply spacechild1 9 hours agorootparentprev> I might post it on social media and then the onus is on other people to reach out to me. Nobody can expect that everyone is on social media, let alone a specific platform. You typically tell your family and some close friends and they will spread the word. reply herbst 9 hours agorootparentprevIf someone literally thinks it's going to hurt our relationship that I am not following their facebook nonsense I am totally happy to not have them as friend anymore reply Arch-TK 8 hours agorootparentprevWhen my father died, the last thing on my mind was trying to tell as many people as possible. I didn't (and still don't) have any social media accounts so that was out of the question but I didn't tell almost anyone for a long time until it came up in conversation. reply skeeter2020 14 hours agorootparentprev>> For example, if a parent died... and yet people died quite often before social media; what did we do then? If the realtionship is built upon the foundation of social media, it's actually not that strong, absent social media. We'll be fine. reply s1artibartfast 12 hours agorootparentyou would find out at church or any number of the 3rd places you shared. Yes, that may have been better, but that doesn't mean deleting social media automatically sends you back in time. Doubly so if all of your friends are still on social media and using it as the primary form of communication. Imagine deleting your email and telephone in 1999 and saying \"if they were really my friend, they would drive/fly to my house and talk to me\". reply labster 11 hours agorootparentIn 1999 we had obits and mail, just like in 1899. Of course now all of the newspapers are gone (what’s black and white and dead all over?), so notifying the local community is much harder than 25 years ago. Also some people back then would brag about not having a TV, the same way vegans still do today. reply latexr 8 hours agorootparent> Also some people back then would brag about not having a TV, the same way vegans still do today. This is the toupée fallacy mixed in with something else I haven’t yet put a name on. Most vegans don’t brag about being vegan, just like most TVless people don’t brag about not having a TV. Some people are assholes and brag about anything, and some of those do the things you mentioned. It’s orders of magnitude more common to see people complaining about vegans (or, for an HN example, Apple users) than the actual bragging. It’s a meme, not the reality. reply labster 8 hours agorootparentCool idea, but it’s based on my own experience in life, from a girlfriend and various people in college. And even a newspaper article I read literally this morning. Vegan folks who I knew and talked to every day. That said I could have used airplane pilots for the same example (also based on personal experience). reply latexr 6 hours agorootparentRight, that’s the toupée fallacy. https://rationalwiki.org/wiki/Toupee_fallacy You only know about the people who let you know. You have no idea how many vegans or airplane pilots you encounter regularly who never tell you. A small sample is driving the reputation of the whole. For people with whom you talk every day, it’s no surprise that you know. It’s bound to come up but I doubt it happened on your first conversation with everyone. If it did, you were hanging out with a weird group. If they knew each other, it’s normal that they’d talk about a shared interest. Just like people who hang out on HN would be likely to discuss tech when meeting in person. I have no doubt you found your share of asshole vegans, just like there are assholes who make it a point to make everyone know they eat meat. Though it is important to distinguish a true asshole from someone simply sharing an experience. Saying “no, thanks, I’m vegan” when offered a bite of a meat sandwich is not bragging, it’s context. Unfortunately, too many people take it to be a judgement when it most often is not. https://www.youtube.com/watch?v=ExEHuNrC8yU reply shafyy 10 hours agorootparentprevGod damn vegans and their non-TVs, what assholes! Edit: Jokes aside, I'm vegan and I don't own a TV. Coincidence? Haha reply Intermernet 11 hours agorootparentprevI didn't know not having a TV was a vegan thing... reply labster 10 hours agorootparentSee sibling comment lol reply Kerrick 14 hours agorootparentprevObituaries were published in newspapers. The news spread to local strangers, not just friends of friends of the deceased. reply jhbadger 12 hours agorootparentThey still are. The issue is not many people read newspapers (whether paper or online) these days. reply standardUser 19 hours agorootparentprev> This doesn't really seem that important if your only method of knowing this was a post The landscape of human relationships is deep and broad an varied, and if making bold assumptions about what other people should value is your starting point, you're liable to miss a lot of potential connections. reply ozim 18 hours agorootparentYou write like somehow there would be something to miss out on by not valuing keeping up with people who are far away and most likely have no place in our lives.(by far away I mean you don’t actually get to talk or meet with them or even chat by messanger or so, even if they could live in the same city - I have friends who live far away but we actually meet at least once a year and chat once a week we are far in distance but not far in contact) I would argue that there is much to miss on by wasting time looking up Jenny from primary school when you have your kids, friends and family who you meet day to day. There is actually an option to run into mental health issues that we know social media is causing. reply standardUser 9 hours agorootparent> You write like somehow there would be something to miss out on by not valuing keeping up with people who are far away... Yes, absolutely. The paths our lives take can lead us to have more in common with someone we knew in the past then when we first knew them. And there's a lot of value in having a history with someone, compared to getting to know someone new from scratch. Maintaining loose contact takes virtually no effort but can lead to meaningful interactions down the road. reply johannes1234321 7 hours agorootparentYes, I have a few very good remote friends which I meet only rarely, but when it's one of the best kind of things. However most of my \"Facebook friends\" were shallow faint contacts, where paths may have been close for a while but went apart as each went on with their lives. No more scrolling through which bar they visited, how their kids are doing, or which TV show they were watching didn't take anything from my life, while it encouraged me to reach out more actively to people I really care about, as I didn't \"rely\" on passive information anymore, assuming to hear about \"relevant\" events, but became interested in them and shared things which wouldn't make \"public\" social media. reply hypeatei 18 hours agorootparentprevMaybe I could've worded that better, but I was just providing perspective on the obsessive nature that we have on social media now. IMO, it's not \"normal\" to keep up with acquaintances and people from past times. They're no longer part of your life and you need to let go. If others find the life updates useful and beneficial to them, then so be it. I don't care either way. reply zapzupnz 14 hours agorootparent> IMO, it's not \"normal\" to keep up with acquaintances and people from past times. Fully recognising that you said \"IMO\", I'll say that keeping up with acquaintances and people from the past is normal in my culture. Social media helps to make that more direct and easier to manage than the gossipy grapevine of yore. What's normal depends on your culture and context, of course, and I suspect that's not true in yours — but it is in mine, so ditching something like Facebook is just out of the question for me and many people whose cultures place a heavy emphasis on those connections between people. The middle ground for me has been to check Facebook less and less, accelerated by the algorithm delivering me fewer life updates and more slop reposted from reddit. reply skeeter2020 14 hours agorootparentif the goal is easier to manage and more direct, I'd argue it's not that important. Is your culture 20 years old? What did they do before? There are lots of things in the world where the work required IS the value. Think of a hand written note from your CEO; is it still valuable if it was their assistant and a picture of the signature? \"keeping in touch\" is not inheriently valuable; it's the effort required that makes it so. reply watwut 8 hours agorootparentPeople before deliberately kept contact with acquitances over time and I recall older people regretting not keeping this or that contact. reply harvodex 14 hours agorootparentprevI agree with you but I think we are kind of the oddballs at this point. It does seem quite normal now to keep up with people you haven't seen in 10 years in person and will never see again. Maybe even people you would go out of your way to make sure you don't see in person but you can give them a thumbs up when they post a picture of their lunch. I have no idea why anyone does this but it would be hard for me to say that not having any social media like us is \"normal\". reply exitb 18 hours agorootparentprevDue to some unknown circumstances this might not be true for this person, but it’s certainly true for a lot of people. Social media used in that context is effectively automating human relationships. It used to take effort to have a handful of friends, now you can have hundreds. Somewhere along the way though, friendship turned from active effort to passive status. reply prmoustache 9 hours agorootparentAre these really friends though? Or just some people you met and appreciated in the past? reply watwut 8 hours agorootparentPeople you met and appreciated in the past evolve into friends and friends evolve into people you met and appreciated in the past. Each person can change \"the status\" multiple times, depending on circumstances. However, if you decide that weaker relationships dont matter, they will never grow into friendships. They will die out. And to large extend that is what is happening with \"loneliness epidemics\". We dont care to keep relationships and see it as negative. Then we dont have relationships and act all shocked. reply prmoustache 7 hours agorootparent> People you met and appreciated in the past evolve into friends and friends evolve into people you met and appreciated in the past. Each person can change \"the status\" multiple times, depending on circumstances. Agree with that. > However, if you decide that weaker relationships dont matter, they will never grow into friendships. They will die out. I don't think putting thumbs up on social media posts count as \"growing into friendship\". > And to large extend that is what is happening with \"loneliness epidemics\". I am not even sure a _loneliness epidemics_ exists but if that is true it is mostly self induced and artificial relationship pretense on social medias do not help. Quite the contrary. If you get out of social medias you actually realize your only chance to make relationships is by going outside and meet people that are close to you. And this is how you build relationships that matters and prevent loneliness. > We dont care to keep relationships and see it as negative. Then we dont have relationships and act all shocked. I am an expatriate and moved countries several times. I have lost touch with a lot of my old friends as well as a huge part of my larger family because I don't use facebook and instagram. That doesn't mean I don't have relationships. I made new relationships locally, and am keeping in touch with people who are not in the same country but that are as eager as I am to travel once in a while to see me. OTOH last few years I have called a number of friends who are living abroad or several hours of train/plane/driving away from me at least once a year. Some gave unsolicited apologies and promises that next time they will be the one calling, or that they have plan to visit my area. They never called back, nor visited me and I didn't prioritized them enough to try to visit them either. This year I didn't even try to call them. I just moved them from the _friends_ mental drawer to the _acquaintance_ mental drawer. This is very likely what they passively did 2 years ago already while I was still actively trying to stay in touch. If for some reason I travel close enough to their last known place, I may try to contact them but it is very likely that I may never see most of them. But I don't need to follow what they are posting on social medias nor publish stuff I am living and pretend that I or they care because really we do not, or not enough for it to matter. reply aembleton 5 hours agorootparent> I don't think putting thumbs up on social media posts count as \"growing into friendship\". Unless it was for an invitation to a board game evening and dinner at a friend's house. That would help to grow the friendship. reply watwut 6 hours agorootparentprev> I don't think putting thumbs up on social media posts count as \"growing into friendship\". The interactions I have seen on social media did not consisted from thumbs up only. > If you get out of social medias you actually realize your only chance to make relationships is by going outside and meet people that are close to you. What actually happen to most people is that they stop showing up in meetups organized through social media (majority of them) and over time loose those relationships. From what I have seen, removing yourself from social media does not create new relationships for most people. You do not build relationships by NOT being somewhere. reply prmoustache 2 hours agorootparent> The interactions I have seen on social media did not consisted from thumbs up only. Not necessarily but in my experience unless those people meet on a semi regular basis (as long as 2 years), or have a special bond (family) this usually slides toward superficiality. > What actually happen to most people is that they stop showing up in meetups organized through social media (majority of them) and over time loose those relationships. From what I have seen, removing yourself from social media does not create new relationships for most people. People don't only meet other people through meetups organized in social medias. I usually get invitations to events through calls and messages from friends, coworkers and ex-coworkers and meet other people there where we exchange phone numbers. I meet people on the road while cycling, some through their dance/yoga/crossfit/crochet class, etc. Several of my good friends I met over they years was by seeing them every day in my train commute and ending up talking to. I've met some random people in a bar and ending up sharing tapas with them and going home with their numbers. reply Barrin92 18 hours agorootparentprev>you're liable to miss a lot of potential connections. are you really? If you only notice that it's Bob's birthday because you get a FB reminder and the only form of communication is a post on their timeline once a year that's not a connection, that's like talking to your neighbor about the weather out of courtesy because it's awkward to say nothing at all. The reason a lot of people miss out on life nowadays is not because they have too few connections but because they waste their time on fake ones. Life's short, instead of trying to warm up some high school friendship that's going nowhere, focus everything you have on the few people around you that matter. Cutting connections is as valuable a skill as making them, and an increasingly lost art. reply calvinmorrison 14 hours agorootparentFree for 8 years-ish. Yeah. its hard to look people up. Oh im in this town, yeah wonder what happened to xyz, no chance of finding them or shooting them a message. FB connections are so low key and keeping people around makes them easier to find and stay in touch with, IMO. But there's also lots of upsides. I guess I dont know one way or the other. reply MrOrelliOReilly 11 hours agorootparentprevYes, agreed. For me, quitting social media went hand in hand with a recognition that I maintained superficial contact with a large number of old friends. My relationship with these people was already “illusory”, or at least unsatisfying. Now my relationships are the product of active work, which I find more valuable, even if it means maintaining contact with a smaller group of close contacts (outside my day to day relationships). It doesn’t mean my relationship with old friends and family has died… we just have a lot more to catch-up about when we talk to each other! reply Whatsappsuks 11 hours agorootparentprevAgreed. I also went through it and have found no difficulty with throwing away Facebook, Twitter, etc and sticking to only direct or group messaging. Some people HAVE gone through the \"but I said in X group chat\" like above, but it was all unimportant life events that they were happy to fill me in on there instead. All major things people told me directly. Just because I quit social media didn't mean I wasn't aware of the death of my dog from a world away within 2 minutes of it happening. reply veunes 11 hours agorootparentprevBut it’s also nice to know what’s going on in people’s lives without needing a deep connection... reply kelnos 10 hours agorootparentBut why, though? If all you have is a shallow social-media connection with someone, why is it nice to know what's going on in their lives? We have a finite amount of time and energy to maintain connections with people. Even shallow connections eat into that. I'd rather spend that time and energy on deeper connections. And while it's customary to say \"but sure, I guess other people have different views on this, so to each their own\", I... well, I honestly believe it's unhealthy to obsessively try to maintain all these sorts of shallow connections. I think this is a part of why I read about how so many people are lonely these days and have trouble forming friendships and keeping them going. reply Karrot_Kream 8 hours agorootparentBecause sometimes you rekindle relationships that have drifted apart but you still stayed somewhat tethered to thanks to social media. I rekindled a friendship with an old friend when I realized he was visiting the same foreign country as I was. Funny enough his wife is a mutual college friend of ours whom he had lost touch with but only met again after reconnecting on social media. I also reconnected with her through my friend. reply pmarreck 17 hours agorootparentprev> a call from them um... will someone else tell him/her, or should I? reply prmoustache 9 hours agorootparentMy partner (who's family is living 10000km away on another side of an ocean) learned her estranged father died a few days ago and that he had been terminally ill for months. Apparently someone from that part of the family had posted it on facebook but she didn't notice it as she do not visit it every day. reply mvdtnz 12 hours agorootparentprevGood thing we have you, hypeatai, telling us who (and who not) we're \"supposed\" to stay in touch with. reply johnnyanmac 19 hours agoparentprev>Turns out that a lot of people I knew posted huge life updates that I completely missed out on. TBH I have no idea where or if my friend post stuff on social media anymore. I know maybe 1 person that posted updates often on Facebook, and that was pre-pandemic. Some post more business stuff on twitter. But overall I just kind of accept that sometimes I'll meet up with someone after a few years and realize \"oh yeah, they're married now, took a trip to Japan for 6 months, and is getting some local attention from their band they made a few years ago\" Of course, the first thing men will say after that meeting is simply \"I've been fine, can't complain. How about you?\". Maybe they'll mention their new job, but the rest will come after some 15-30 minutes of observation and chatting about the newest media. >but I know a lack of social media meant that I have lost touch with old acquaintances completely. I have a few close friends and that’s it. likewise, but I'm not sure if social media would have saved that for me. It's definitely a cultural issue, especially with men. reply godelski 12 hours agorootparent> Of course, the first thing men will say after that meeting is simply \"I've been fine, can't complain. How about you?\". Some of this is natural (though I don't believe healthy) but I think some of this is due to social media where people expect others to be aware of all their major events. Ironically I find this aspect of social media fairly dehumanizing. It disincentivizes direct communication. Why tell someone about things they already know? Getting the first account always coveys so much more than a facebook post. Sometimes I think we've forgotten how to talk to one another and read all the communication besides that in text. Text is at such a higher compression rate and it certainly isn't lossless. No matter how many emojis, memes, or images you include. reply Sebb767 9 hours agorootparent> Some of this is natural (though I don't believe healthy) but I think some of this is due to social media where people expect others to be aware of all their major events. I sometimes do this despite not posting any personal stuff on social media. The reasoning here is pretty simple, I usually don't have a full list of all the stuff that happened in the last few years in mind. When meeting someone I see more often, it's quite easy to think about the last week/month and start with the noteworthy events; whereas, when meeting someone after a few years, not only do I need to think about what happened, but also which of those events might interest the person in question and what level of detail is appropriate reply bluGill 18 hours agorootparentprevProblem is facebook decides what you want to see unless you go to the feed which they make hard. Even then the vast majoritiy of what you see is garbage they share instead of life updates that you want. I wish there was a better way but life updates still a posted there only. Facebook is the only one that has a concept of this is a for my friends only. reply johnnyanmac 15 hours agorootparentYeah, I couldn't put it into words on why \"Facebook got worse over the years\". But that was definitely one of the keys shifts (outside of my friends leaving). I was getting less updates from people I know and more \"news that will make you angry\" kind of stuff. I probably really \"left\" around 2017. But 2020 is when I finally got around to freezing the account. reply bluGill 4 hours agorootparentRealistically everything my several hundred friends have to share with me takes maybe 5 minutes a day to go through (close friends of course would call about things that are too personal for facebook so this is about things more distance friends would care about) . There is a lot more money in the handful of people who are scrolling through, finding, and sharing various memes and news that will make you angry (though this is fun if you - like most people - have friends on both the right and the left doing this and so you can see the bias each side is taking) than there is in friends sharing their life which is not lived on facebook (unless your a physically disabled and so you can only live vicariously through others) reply kelnos 10 hours agorootparentprev> But overall I just kind of accept that sometimes I'll meet up with someone after a few years and realize \"oh yeah, they're married now, took a trip to Japan for 6 months, and is getting some local attention from their band they made a few years ago\" This was a big thing I realized, too. For some people in my life I do genuinely want to know about those sorts of things as they are happening, and for those people I'm in frequent contact with them through text, group chat, real-life meetings, etc. But for everyone else, it is completely fine if I hear about those big life updates months or years after they happen, on the less-frequent occasions when we get together and catch up. Some relationships are different, and that's fine. reply stiray 10 hours agoparentprevI never had any social media account. Never. When Facebook was still in diapers I have predicted what will happen (while what really happened was far worse) and distance myself from it and warned all the others who, normally, didn't listen. Dont have FB, twitter, reddit, linkedin, tiktok, not even google account... none of that crap. I am successfully avoiding getting my name anywhere on the internet, I am not posting my photos, videos,... I have 7 friends I meet regularly, I have friends where our life separated years back and we meet once or twice per year and I have phone with 473 phone numbers of various contacts, from former colleges to dishwasher repair technician, etc. And guess what, people call me, sms me (oh yes, it works so much better than having 20 various clients installed for different groups of people) or send me email if something important has happened and I am actually physically invited to birthdays, \"i got son\" celebrations, notified about death (luckily only one, former schoolmate). When we meet, in person (i hate long phone calls), we have a quality chat as I dont know anything about their ingrown hair on the tip of their toe and they don't know anything about me changing job or having knot on hair in my beard. For anyone else, I dont care. I dont disillusion myself how I have 473 good friends on some stupid online platform who need to share every intimate detail with me. I cant even handle so many people. So maybe those tradeoffs are not really the real tradeoffs but rather self deception, how much you matter to the people and to how many people. I can count them using my fingers. Which is perfectly fine. reply abc123abc123 8 hours agorootparentAmen! Same situation, same preferences, same values, same result. Only difference is I have about 70 phone nrs on my dumbphone, I don't own a smart phone, and my vices are this site, usenet and mastodon. reply aaarrm 18 hours agoparentprevI think it's perfectly fine to learn about huge life updates from people the next time you actually speak with them. That seems normal. Seeing people's updates on a wall isn't truly keeping up with friends. Keeping up and staying in touch requires consistent deliberate effort from both parties, via phone calls, messaging, and seeing each other in person. If you're not doing that with someone, then yeah, learning about life updates when you actually chat and catch up just makes sense to me. reply jjulius 18 hours agorootparentPlus it's a lot more personal and meaningful when you can discuss the changes directly rather than on an impersonal \"public\" forum. reply kelnos 10 hours agorootparentRight, and when I meet up with a friend in person to catch up (whether it's a close friend who I see weekly or a less-close friend who I see once or twice a year), we both give each other those life updates in a personally-tailored manner that perfectly fits the nature of our relationship with each other. That's how I want my interactions with people to be. reply AOsborn 18 hours agorootparentprevNo, I disagree. This is about lifestyle ergonomics and your community. Like it or not, social media has significantly reshaped the world. Issues aside, it has brought people together and made communication significantly easier than in the past. There is a reason 1/3 of the world is on Facebook. So, my point is that if you're choosing to be difficult, that is fine but you need to accept the burden falls on you. This is similar to adopting a vegan diet - your body your choice, but don't be intentionally difficult at dinner parties. Personal example here: I've cut down social media significantly, in my case all notifications are off even if the apps are installed. So you're not bombarded and can engage on a cadence that makes sense to you. That said, I need to dedicate time to checking up on extended family, friends etc - as otherwise you do miss announcements and major events. reply johnfn 14 hours agorootparentI don’t understand how you’re being “difficult” by not keeping up to date on the Facebook updates of your friends. I will of course update all my close friends 1:1 on any life changes, and I expect they will do the same to me. For everyone else, there’s nothing “difficult” about asking for a life update the next time you see them. If anything, it shows interest and is a kind thing to do. reply hedvig23 11 hours agorootparentI might guess my comment here in a \"meta sense\" is looked down upon here (for good reason) but that comment you responded to rings a certain way and along with other dialog here and the issue at hand (world scale industry of eyeballs and diversion) i have to politely guess the thought of astroturfing that came to me might be fair. reply kelnos 10 hours agorootparentprev> Like it or not, social media has significantly reshaped the world. Certainly! I don't think that fact is in dispute. But we can definitely debate the quality of relationships that have resulted from that reshaping, and make our own personal determinations as to whether social media has been a net positive or negative in our lives. The problem is that, for some people, it really has had a negative impact on their lives, but they don't or can't see it. reply jjulius 19 hours agoparentprevThis is just said from my perspective and I understand that others might not share it - Fine with me. They're acquaintances. Nobody has 200+ \"friends\", we have a handful of them. Is it nice to know that someone I hung out with a handful of times twenty years ago but otherwise don't really know and haven't said a word to in a decade made a big life change? Sure, I guess, but for the most part it has absolutely no bearing or impact on my day-to-day life nor the lives of those most important to me, and that's where I'm putting my energy. reply EVa5I7bHFq9mnYK 11 hours agorootparentI was applying for a job once and recruiter told me I need to have at least 100 Facebook friends to be hired. Played Mafia for a couple of days and got 400 \"friends\". Was hired and HR presented me at the company meeting as a social media star) reply jader201 11 hours agorootparentThis sounds made up. I’m not saying it is, just that it’s so bizarre, it’s literally unbelievable. Like something The Onion would write. reply EVa5I7bHFq9mnYK 11 hours agorootparentThat's true and happened in 2011. Social media was AI of the day. reply kelnos 10 hours agorootparentprevIf I'd been talking to that recruiter I would have politely declined to continue the interview process. Unless the job itself was directly related to interacting with people on Facebook, the number of Facebook \"friends\" you have has nothing to do with your ability to do the job. reply jader201 14 hours agoparentprev> I know a lack of social media meant that I have lost touch with old acquaintances completely I think that’s a feature, not a bug. Most of the life updates people post on social media are the best of the best, which is what triggers so much fomo and trying to measure up. That’s why social media makes most people feel worse about their own lives. (Not to mention all of the other garbage these platforms try to push on you that you didn’t even ask for.) If these people are really important to us, then we’d find other means of staying in touch: text them, call them, invite them over (if that’s feasible). And if enough people get off social media, everyone else might also realize they need to make an effort to stay in touch with others, instead of the lazy post of glamour shots for the purpose of internet likes and feeding the dopamine addiction. reply nradov 14 hours agorootparentI don't know about other people but social media doesn't make me feel worse about my life. There's no fomo when my 2nd cousin in another state posts about having a baby. She's important to me, and I don't expect her to waste time emailing me directly when the rest of the extended family is all on social media. reply tonyedgecombe 11 hours agorootparentIt didn't make me feel worse but it did bring out the worst in me. It's too easy to slip into bragging about your life and how well it's going. reply Karrot_Kream 8 hours agorootparentPretty mugh all my friends stopped doing that after their mid 20s. Eventually everyone figures out that social media status games give little. reply nradov 2 hours agorootparentprevWhy? reply jader201 14 hours agorootparentprevSure, events/posts like that are the main upside to social media. Unfortunately, that makes up a tiny fraction of most feeds. reply WhyOhWhyQ 14 hours agoparentprevI quit social media 12 years ago and it's been an amazing boost to my personal psychology and productivity. My life is 10x better without it. I've forgotten many acquaintances and gained many more, and forgotten them again. Life is like that, but the core group of people is there, and I'm happy with that. reply __MatrixMan__ 14 hours agoparentprevI did the same thing. Now all I have is github, stackoverflow, and HN. I end up missing out on all sorts of things that I'd like to have been along for. I'm not about to go back, I think that being at the business end of somebody's propaganda machine was even worse for me, but it's still a significant sacrifice. Which is why I don't think the way forward is for everybody to leave social media. It's just not going to happen en masse, that's asking too much. We need to build media which can't be owned. If we ask people to sacrifice something, it should be an extra few cents on their electric bill and yesteryear's phone plugged in somewhere and hosting their share of it. I've only been exploring it for a few days now, but nostr seems promising for this kind of thing. The content is awful, just coin bro stuff, but as something to plug into and build apps for... seems legit. reply kelnos 10 hours agorootparentFor me (been off social media since 2019 or so), the solution has been smaller, targeted groups of friends, as well as making one-on-one effort. I have quite a few group chats with no more than a dozen people in each one, with many that have only 3 or 4 people. And I make a point to message people one-on-one to keep in touch, and set up time to meet in person for people who are local to where I live. For people who aren't local, we make a point to meet up in some city somewhere once a year or so, depending on the closeness of the friendships in the group. It requires more work than scrolling a Facebook feed and commenting on people's posts, but it's orders of magnitude more rewarding. And I don't miss the other hundreds of people on Facebook who I don't hear about at all now. reply 2024user 19 hours agoparentprevLosing touch with old acquaintances is just part of getting older. fwiw, my experience is that I stayed on social media (although I don't post anything, I just keep the account) and still missed huge life updates. I reckon about 80-90% of my FB friends don't post to FB or Insta anymore. They just don't post anywhere. reply Over2Chars 18 hours agorootparentI'm sure the new FB AI will generate synthetic life updates that will seem just as convincing. reply maigret 7 hours agorootparentOne more reason to leave reply coffeefirst 19 hours agoparentprevI quit on and off and came to the opposite conclusion. The acquaintances I never heard from, we weren’t really in touch in any way, seeing their posts had just tricked me into thinking we were. And that’s okay. It means 5 years later when we cross paths for real there’s lots to catch up on. reply itbeho 14 hours agoparentprevYou could just call the people most important in your life and speak and hear their updates. It would mean more to them than a comment on FB or whatever other social they are on. reply suzzer99 19 hours agoparentprevYes, everyone uses social media differently and gets different things out of it. I've got my Facebook feed so well-curated that it rarely causes me distress. And like you, I like keeping up with old acquaintances, seeing their kids' milestones, etc. I get real enjoyment out of that. Instagram I post pics when I travel and otherwise ignore it. Twitter OTOH is probably a net negative for me. I still keep it around to follow sports pundits during games, and I usually only follow my sports list. But I do check in on my main feed during major events, and then inevitably end up doomscrolling. For example, the LA fires hashtags are so far beyond toxic - nothing but engagement farming, malicious misinfo, political nonsense, etc. Amidst all that crap, maybe 1 in 10 tweets has good info, but I have to destroy my psyche to find it. reply shaky-carrousel 12 hours agoparentprevIf your acquaintances didn't take the time to update you directly, then maybe either the updates or the acquaintances themselves aren't really relevant for you. And that's ok. reply ozim 19 hours agoparentprevI cannot say much as I don’t know the people. Turns out that a lot of people I knew posted huge life updates that I completely missed out on. I asked them why they didn’t tell me and they were confused. They said the posted it on social media My impression is „how can one be so self centered” to imagine everyone HAS to know about their big event if they were not part of it and were not invited directly. Is that person Kardashian family or something ;). Even if it was a wedding and they posted photos. I wouldn’t remember a week later - if it is a person I see once in 5 years face to face and I was not invited. There are many big life events of such people. reply ofcourseyoudo 13 hours agoparentprevThinking you can blast something on social media and your friends and family will see it is an old mentality. Even non super tech-savvy people know now what the algorithms are, and they know that everyone regularly misses updates from everyone else. And that light connection to people through social media wasn't a thing that created \"close friends\" anyway. It add to those weak connections that do have value but I doubt many people create intimate friend relationships solely through social media. reply ncr100 12 hours agorootparentNice. Maybe part of the seeming increasing anxiety of society is that we don't have friends in the correct sense and instead we have \"friends\" in the \"just another user ID attached to a database query for your user ID\" sense. reply tomlockwood 12 hours agorootparentprevYeah! Facebook is too busy showing me content farm AI slop to show me my cousin's baby photos. reply pmontra 8 hours agoparentprevI recently learned that the daughter of a friend of mine got married and had two children. That happened in the last 5 years, when I actually didn't hear from that friend of mine. Given that we didn't feel the need to send messages to each other for 5 years, are we really friends or only acquaintances? In the latter case it's OK not to be informed about what is going on. My social media are WhatsApp and Telegram. I get in touch there with people I care about and I don't get streams of useless information like I would if I'd be on FB, X, Instagram or TikTok. I do look for videos on YouTube when I want to learn something for which watching is better that reading. reply DougN7 19 hours agoparentprevI just deleted my FB account yesterday. Believe or not, your experience makes me feel OK about it because even with FB, I’ve drifted apart from all but a few close friends. That makes me think it’s the norm and social media doesn’t do nearly as much to keep us connected as it would like us to believe. reply noman-land 18 hours agorootparentUnfriending specific people in a huge cull of otherwise nice and well meaning people you no longer care about but are chained to by inertia is torturous. Much less psychologically burdensome to unfriend everyone by nuking the account and start over with a clean slate somewhere less corporate and shitty. reply Semaphor 13 hours agoparentprev> Turns out that a lot of people I knew posted huge life updates that I completely missed out on. I wish I would still see those. While I have an account, I rarely use FB nowadays, because the algorithm thinks I’ll be more interested in stuff I don’t care about. So when I go to FB I tend to close the tab again a few seconds later… reply aembleton 5 hours agorootparentI found that this plugin really helped: https://www.fbpurity.com/ reply brightball 13 hours agorootparentprevI just started aggressively unfollowing and hiding stuff I really didn’t want to see. I used to hate the idea of doing that, but I realized if not doing it was making me want to cut off everybody then I was probably better off just filtering aggressively. Now my feed is very pleasant. Family updates, sports news, friends vacation pictures and jokes. reply Semaphor 12 hours agorootparentI only follow stuff I want to see. But facebook shows me a ton of other things. Reels every few posts are usually sexily dressed Asian women, or more normally dressed White women, with some kind of clickbait text overlaid. I think I maybe clicked on a reel once or twice ever. Then my feed is full of suggested content. Which I also don’t want to see. From metal bands I don’t care about and festivals I don’t wanna go to, to offensive content. And finally: Ordering. Non-chronological ordering makes no sense, because everything is random (probably not, somehow maximizes engagement for users very different from me, I guess). So I can’t even scroll for the stuff I want to see. reply pickledoyster 7 hours agorootparent>And finally: Ordering. Non-chronological ordering makes no sense, because everything is random (probably not, somehow maximizes engagement for users very different from me, I guess). So I can’t even scroll for the stuff I want to see. Does https://www.facebook.com/?sk=h_chr not work anymore? It should surface the chronological feed without reels and other recommended slop. reply Semaphor 6 hours agorootparentI love you! This does exactly what I want! It makes FB usable again! reply brightball 4 hours agorootparentprevThe Reels, I'm with you on. It does seem to keep showing you more of what you click on though, so I hit a good trend of funny clips from Modern Family, How I Met Your Mother, Friends and random interesting nature videos and for the most part those dominate now. But no matter what you click on it seems like they are really determined to keep throwing in the various ladies clips. reply noman-land 18 hours agoparentprevIf neither you nor they bothered reaching out, did either of you actually care? It might be a good time to reevaluate the nature of your relationships and start maintaining the ones you actively (instead of passively) care about outside of corporate shopping mall websites. reply monssooon 10 hours agoparentprevI also quit social media . I did not have this experience. I had no problem following what went on with friends and acquaintances?! I don't know why you had this experience. I'm sorry you felt like that. but for me the info that is important always gets to me. And I enjoy emailing and using the phone and meeting people for social events in stead. And when I miss some post on face I always hear it from someone else.... reply austin-cheney 10 hours agorootparentIt’s a matter of perspective and trade offs. If you are a person who experiences anxiety from missing things (FOMO) you are much more likely to notice this. I know since deleting my social media accounts I have missed out on a lot, only because my wife keeps up with stuff, but I cannot say how much I have missed because I really don’t care. The other side to this is how aggressively a person is willing to take deliberate action to maintain personal relationships in the absence of social media automation. If you are that social butterfly who really owns that aspect of life you are likely to never miss any aspect of social media. Most people aren’t that good at taking dedicated action regularly reach out to people outside their most immediate circle though. reply bee_rider 19 hours agoparentprevPeople drift apart over time sometimes. I’m on Facebook still (TBH it is hard to see much by my friends, between all the algorithmic stuff). Despite being on there, there are some folks I’ve just kind of… lost contact with. Maybe have a text chain for your friends or something? The folks I really expect to know things about… they’d tell me while we were interacting. reply kristianc 19 hours agorootparent> People drift apart over time sometimes. It's admittedly a little easier to drift apart though when you deliberately delete your access to the place where they post all the shit that's happening in their lives... reply bee_rider 18 hours agorootparentThat seems intuitive, but most of the people who I’ve stayed in touch with aren’t really active on Facebook anymore or don’t even have accounts. I wouldn’t be surprised if social media following people provided something like disincentive to actually stay in touch for real. No need to perform the checks that maintain the relationship if the info is all posted right there. But that’s also a guess. I suspect neither of us have any data to back up our guesses. reply parsimo2010 19 hours agoparentprevThe messages about going cold turkey are popular, but you do miss out on a lot. I deleted all my social media in 2015, and didn’t mind too much, but years later when I met my wife (and there was more pressure to be social) I made accounts again so people could message me and I’ve been able to hold back from spending all my time doomscrolling. I think the social part of social media can be good for us, and we have to figure out a way to avoid the toxicity. I’d like to see more posts about how to bend the algorithm to show you less toxicity- at least on Instagram I’ve managed to use the “not interested/relevant” button enough and turned on content filtering that it mostly shows me wholesome content. I don’t know if everyone realizes that if you hate-watch a video or hate-read a post then the algorithm sees that as engagement and will show you more. You have to nope yourself out of the dark corners as fast as you realize where you are. reply aembleton 5 hours agorootparentFB Purity plugin helps. It removes all the suggested posts, advert and other junk. Unfortunately, it only works on desktop: https://www.fbpurity.com/ I can scroll through Facebook now on my laptop, and it means I stop doing so on my phone. I have the phone app just to post updates or to quickly check the location of an event or something. reply paganel 10 hours agorootparentprev> algorithm to show you less toxicity- at least on Instagram In the case of IG what worked for me, without me even trying to be explicit about it, was to like and watch lots of photos/reels involving dogs and dog-ownership and right now my IG feed is 90% full with dog-related posts, and that’s the way I like it. Maybe it works the same way if one were to adopt similar strategies for other subjects of interest, such as cats, owning cars etc., the thing is that there’s almost no political/societal info on my feed anymore. reply renegade-otter 7 hours agoparentprevMaybe that was possible with Facebook from 2009. Right now, to have any friend updates, you first you need to scroll through a firehose of bots, AI schlock, and ads. Then risk getting pulled into it and wasting valuable hours of your life. The only winning move is not to play. I just got together with two friends in RL. One I have not seen for 10 years. There were a lot of missed news we all had to catch up on. This is how it's always been, and it's completely normal. Even the olden Facebook way of being so plugged in into your friends' lives was very unhealthy. If you HAVE to know something, life will find a way of letting you know. reply oysterville 19 hours agoparentprevHeck, I missed huge announcements when I was on social media because social media thought that the stuff they had to show me was more important. reply KronisLV 9 hours agoparentprev> I can’t speak for everyone, but I know a lack of social media meant that I have lost touch with old acquaintances completely. I have a few close friends and that’s it. I feel like that's the downside of social media in general, like the network effect - since most people are on social media, that's the place where people will post life updates, as opposed to talking to others about that stuff directly as much. Maybe there could be a healthy way to use social media: to catch up with the people in your social circle, maybe look at a few cute pictures of animals or memes, but don't obsessively doomscroll or compare yourself to the highlights of others' lives. reply noufalibrahim 8 hours agorootparentI think some way of batching information from the people/sources that you're interested in (and then perhaps running an LLM over it to surface the most important information) which is then emailed or texted to one might be a solution. reply mastazi 19 hours agoparentprevIn general I agree with you that there are some tradeoffs to make. IME it's still worth it. For example, my mental wellness has improved immensely. Also, I tend to use my time in more purposeful ways instead of wasting it doomscrolling. Regarding life events: I quit all social media about 5 years ago[1]. People I care about know about that, and if they want to tell me about life events they do it with other means. Those who don't, they weren't really friends, just acquaintances. I am OK with that. [1] with the exception of Linkedin, which I hate and never use, but I have been asked by people in my company to keep a profile for PR-related reasons. reply Refusing23 11 hours agoparentprevyep i deleted fb 10ish years ago. and since then every family event that had been planned, was done on fb (just like before) and i find out about it by a text from my sister. the trick is to not give a shit. Coz they don't. reply richrichardsson 9 hours agoparentprev> Turns out that a lot of people I knew posted huge life updates that I completely missed out on. I had the same thing happen, but both they and I were Facebook users, it's just the algorithm decided I don't need to see posts from my friends and it's better that I see adverts (I can live with that, I don't pay to use the platform after all) and hundreds of random pages/groups that I have zero interest in following. This 2nd \"feature\" is slowly driving me towards the point where the FOMO of no longer passively interacting with my friends may longer keep me on there. reply ZYbCRq22HbJ2y7 14 hours agoparentprevIf someone relies on broadcast notifications to communicate, whether it be by snail mail, SMS, email, megaphone, or otherwise, maybe it is not really worth hearing? To me, it seems like if someone has so many friends or is so busy that they need to manage their life using this strategy, you probably aren't going to have much of a connection anyway. reply supriyo-biswas 14 hours agorootparentLet us just say that not all friendships, even the ones that start out strong, end up having the same depth to them because of the loss of shared context (e.g. moving out of the same city for jobs, new responsibilities caused by marriage etc.) In such cases, there's still some reason for the two people involved to at least have a general idea of what's going on in other people's lives, and even reach out should there be something significant, such as a birth of a child or a loss in their families, etc. Without the broadcast aspect, once communication has ceased for some amount of time, it is very difficult to restart it, at any level. As an introvert, I still find broadcasts weird, because there's that tingling notion that people wouldn't care anyway; and was one of the reasons I ceased to be on social media many years ago. However, I understand why some people choose to do things differently. (There are similar anecdotes throughout this thread, I'd encourage you to read it for perspectives on this matter.) reply larodi 11 hours agoparentprevDropped FB for HN in 2017... and eventually I find myself now again on X for some #genaury stuff which is basically nowhere else to find. Happily most reasonable tech stuff lists on HN, some interesting stuff on MR, both being more-or-less a social network (of sorts) in a 90s disguise. Conventional media can be ok for casual reading/scrolling, but feels increasingly out-of-touch. Interestingly these days cnn, bbc, dw, en, and aj list different headlines, which is not what it was 15 y.ago. Still I'd strongly advise against all push media, and in particular Meta's products which pose a very high-risk of (screen) addiction thanks to hundreds of hidden retention mechanisms. reply joostdecock 11 hours agorootparent> some interesting stuff on MR I'm drawing a blank for MR. What does it stand for? reply lm28469 10 hours agoparentprevIf they're close enough they'll tell you irl, if they don't tell you they're not that close and it really doesn't matter in the grand scheme of things Even with close friends who live far away, I prefer catching up once a year around a beer and some food than get a week by week journal of their lives on social media, it makes you feel like you're connected but you really aren't reply kelnos 10 hours agoparentprev> Maybe that’s an ok tradeoff to make, but it’s worth knowing that before getting into it. I think that's the key point. I realized that ultimately I didn't actually care about those huge life updates if they concerned people who I'm not in somewhat-regular contact with. Like, if my Facebook friend John Smith (let's say he's an old high-school friend I haven't seen since high school) posts about his marriage, or new job, or new child, and I don't actually chat with John anymore and don't know anything about his life outside of what I read on Facebook, why do I even care to know this stuff at all? And it turns out the answer is that... I don't! And there's nothing wrong with that. It's not rude or mean; some people are the closest of friends, and some people barely even warrant the \"acquaintance\" tag -- and everything in between -- and there's nothing wrong with any of that. And yes, I've missed social media posts about big-life stuff from closer friends who I do care about, but that's fine! I chat with those people via some avenue (email, text, messaging group, real-life, whatever) often enough that I still get those big-life updates, and usually it's in a more personalized manner, that gets me details that are tailored to the level of closeness of our friendship. For people who I'm not super close with, but still maintain a relationship with, maybe I get that update about their life 6 months later, when we are next in contact. That's also fine! If we were closer friends, we'd chat more often, and I'd hear about it earlier. But we're not, and I don't, and there's nothing wrong with that. > a lack of social media meant that I have lost touch with old acquaintances completely. I have a few close friends and that’s it. That's more your choice than anything else. You always have the option to text or email someone directly to say hello and see how they're doing, or to set up a time to meet in person to catch up. Even if they're perhaps not the one-on-one type of friends, you can start a group chat with that person and other mutual friends who might enjoy keeping in touch that way. There are so so so many options for communication these days that it's almost overwhelming! But it certainly need not be a binary between \"social media firehose of every person I've ever met\" and \"I only hear about the lives of few people\". Is it important to you to be in touch with those old acquaintances? If so, reach out to them! If not, then it sounds like quitting social media was fine for you. reply jmspring 14 hours agoparentprevIt’s funny, a few groups I belong to solely use FB. One group is for the preservation of weatern history and a friend digitizes and uploads thousands of pictures (if not 10s of rhoughsands) yearly. The only actual digital copies are on FB. It bugs me that he won’t archive elsewhere. The reason is fb is a commons and an additional backup would be a magnitude more work. I’ve offered to buy hard drives. reply coldpepper 13 hours agorootparentWhy not upload to archive.org? reply nradov 14 hours agorootparentprevYou can download the pictures and archive them yourself. reply jmspring 2 hours agorootparentThere are thousands of pictures across hundreds of albums. So it’ll need to be scraped. Something might exist, no time to write such. reply benjaminwootton 13 hours agoparentprevI do think this trade off is real. I came off most of social media for a few years and was the happiest I’ve been in a long time. It is however a bit isolating. I stayed in touch with friends, but lots of acquaintances slipped away without an easy way to keep in touch. reply incoming1211 19 hours agoparentprevI think if people want to 'quit' social media, then just use it to keep up to date with friends/family. You follow ONLY friends/family, and limit consumption to only that, don't consume content outside of that circle. These full fledged 'quit' posts are nothing more than an attempt at a political statement that falls on deaf ears. reply __MatrixMan__ 14 hours agorootparentDoes that really work? I haven't used one of these for a while but if I recall they're quite keen to take content that your friends/family have engaged with and ram it down your throat in hopes that you will too. It seems like you'd need all of your friends/family to do the same thing. reply macagain 19 hours agoparentprevI totally understand you. What find is that when I need to get into touch with old acquaintances an call or email seems to do just fine. It is a bit more inconvenient. Another reason to not use big social media is that I would rather not have my network to be exploited by some big corp for who knows what they do with that info. reply herbst 9 hours agoparentprevWhat are you even talking about when you meet them? Are you reiterating each others Facebook posts together? Not having seen a friend for a longer time and talking about all the things that happened is the one thing that friendship is about IMO. reply veunes 11 hours agoparentprevI guess it comes down to weighing the value of those connections against the downsides of staying on the platforms reply huijzer 12 hours agoparentprev> I have a few close friends and that’s it. Sounds perfect. I rather have a few close friends than two dozen semi-friends. reply JALTU 15 hours agoparentprevMe too! It's okay, you can't do everything and people \"should\" appreciate others who don't do social. reply bugtodiffer 10 hours agoparentprev> I have a few close friends and that’s it. That's enough :) reply jacooper 4 hours agoparentprevI agree completely, I did the same thing and now I've been going back gradually. Staying in contact passively makes starting conversations much easier, commenting on their stories, them reacting to a big event, etc. It keeps people in contact, because nobody reaches out of the blue now. Losing such a network of people is costly, socially and from an opportunity perspective. Still trying to not click anything not related to people I follow, the algorithms on meta apps are just insane. reply Fnoord 19 hours agoparentprevI've quit social media (only use Signal and SMS/telephone/email). My wife functions as my secretary in this. I get the perks without the BS. Win-win. Only thing to remember is SMS/telephone/email aren't secure. reply Dharmakirti 8 hours agoparentprevTouché. Such radical takes are always a hit on HN because they are essentially playing to the gallery. Leaving social media is futile if you don't take efforts to maintain contacts with your friends and families in other ways. reply pino82 9 hours agoparentprevAlso good to consider for that tradeoff: Those people are completely fine to ignore you without some Zuck accounts. Bring that together with your idea about friendship before you run behind them. Maybe it's fine for you. Maybe your conclusion is that it's not worth the thing. It's not a new topic. For me, iit was around 15 years ago. I never had FB or WA. Not even for a day. And that brought a lot of friendships to an end. Most of my friendships in fact. And that was sad!! But well, no other way would even be an option, admittedly! It's sad, but it was the best I could do. reply motohagiography 18 hours agoparentprevalso eschewed social media. it's a different way of relating. normal people now react the way minor celebrities used to react when I'd meet them and not know anything about them, either insulted or very relieved. I think it has made me a better friend in some ways, as I'm a respite from the narratives they sustain, but to others, also a kind of legacy friend who may be an attachment to an old life, and who isn't part of their present. there's an aspect where watching their social media would be to participate in the change in their lives, and separating from it (perhaps selfishly) preserves things that might be left behind. but on the other hand, I'm interested in relating in one way too. social media profiles are strange because they say, \"see, I am all these things now!\" and in not seeing them, it declines to recognize those, like an old uncle you're always going to be a kid to because that's how you always were. I have more old friends than most, and I often think about whether there is an essential self we see in each other, like a character that all these stories happen around where we can peer across them to one another, protagonist to protagonist, as companions in the real. or are the relationships artifacts of the stories, and when they change, we do? it's prob a mix, but I don't think those essential(ist) aspects of friendship survive being mediated by the churn of updates and the curation of a public persona. anyway, being outside social media is a very different way to relate and not everything survives. reply worthless-trash 9 hours agoparentprevIf they don't talk to you personally, you have overestimated your value in the relationship. reply llm_nerd 18 hours agoparentprevLots of other replies already so apologies for adding to the clutter, but this sort of message always appears and it feels super dated. Like, 2003-era sentiment about the Facebook heyday. Facebook is like a ghost town now from the \"social\" and family perspective. I imagine some circles might be strong on it, but from every time this comes up it's clear that the vast majority of normal people have largely abandoned it. They didn't delete their accounts, but updates are incredibly infrequent. The vast majority of Facebook activity seems to be people who don't really know each other in various conspiracy-oriented or political groups, sports arguments, etc. As to huge life events you missed out on, even in 2003 if you only knew about something because of a Facebook post, you aren't very close. And the old acquaintances thing grows super old super quick. Everyone joined a bunch of graduating class groups, connected with old coworkers, and then... eh, turns out there was a reason we all lost contact. In 2025 people use social media overwhelmingly to interact with strangers, not friends or family. Largely to argue and get angry and try to convince and coerce and convert. I mean, HN fits the bill in a microcosm. Social media is a cancer on society. It has made everything much, much worse. It lets the ill-informed and unintelligent find each other and pump each other up. It monetizes and profits off of the absolute worst human traits. If Meta collapsed into a blackhole, Xitter disappeared, and so on, the world would be a much better place. reply scarface_74 13 hours agorootparent> Facebook is like a ghost town now from the \"social\" and family perspective. I looked at my Facebook profile with 400 friends and they are mostly ads, memes and inspirational sayings. It’s really useless. I have four SMS groups of friends/family I care about. My wife gets more value out of it than I do because she is part of a few groups that she cares about reply TheCapeGreek 14 hours agorootparentprevYou can easily replace FB with Instagram in this context. Nobody I know personally posts very much on Facebook, but they do post their updates on Instagram stories. Facebook's last hook on me is groups - small town community groups especially. If you live in an area with its own group, there's a high likelihood that it's going to be on Facebook. I don't really have the time to campaign to non tech-savvy retiring gen Xers and their parents that I don't want to use Facebook to know what's happening in my area, find services, etc. reply yakshaving_jgt 15 hours agorootparentprevFacebook didn’t exist in 2003. reply llm_nerd 6 hours agorootparentI was being facetious that Facemash was started in 2003, for the Harvard homies. The point being simply that Facebook as a social connections property hasn't been a thing for years. reply fakedang 12 hours agoparentprevThis is my experience too. While I do maintain some accounts, I don't check them much anymore except for updating my statuses and life events, and even then I haven't done those in a while now. The advice to \"quit social media\" , \"get a FairPhone\", \" get an FTP account and mount it with curlftps... \" is often tossed around HN a lot, but real life flies in the diametrically opposite direction. While I'm not largely affected by it, I still feel a twinge of disappointment not finding out when an old friend has had a major life event. reply ToucanLoucan 19 hours agoparentprevBecause the reasons to quit social media aren't that it isn't useful and that, absent the market conditions it exists within that denotes it's ability to continue existing, it isn't a good product. People love the stuff, it's why it's been the primary use case of the internet, arguably since it's inception depending on whether you consider early stuff like BBSes and news groups/email newsletters to be social media. We had early prototype social media functionality online before we had commerce. The problem is that these platforms aren't satisfied merely providing a third place within which we can find and build communities, speak with and learn from others with similar interests, and otherwise, be human. Instead we each become a hamster locked in our own little cage, and the principle reason we're there is to sit on our wheel and run, and while we run we're shown a handful of things from people we actually want to hear from and see, and interspersed with those few things are a ton advertisements for products we don't want and aren't interested in, a few we might be, AI generated nonsense that prompts us to engage with the platform to bump metrics up, the dipshit of the day who's said something infuriating that makes us click into the comments and make sure they're getting dunked on (and possibly join!) appropriately that the social media site dug up from obscurity and is now parading to the entire world, and of course, the same posts again. Genuinely, the way people talk about going back in time to kill baby Hitler, if I had a time machine, I would spend the rest of my days sabotaging whatever countless number of people invented or would invent the Curated Fucking Timeline, on however many platforms it was invented, by however many data scientists. I would argue it is the single most destructive thing Silicon Valley has ever turned out. reply devvvvvvv 19 hours agoparentprevIt's how humans lived for all of history before the Internet. Seems healthier to me. If you're not close enough to someone for them to want to share updates with you specifically, or to see them and catch up, why do you need to know every update on their life? Tbf I'm in a family group WhatsApp chat, which I guess fulfills the \"life updates\" part for my family. But no public social media, don't see the need reply tialaramex 18 hours agoparentprevI would distinguish somewhere my friends post stuff for \"friends\" from social media. Lets take my friend Em as our example. If the typical message from Em says \"Where are you? What time did we say we'll meet\" that's a messenger app, that's definitely not Social Media. It might be a fucking SMS, but if it's a WhatsApp or a Signal. it's all the same for this purpose and that's definitely not Social Media. If the typical message from Em says \"They don't know about my trees\" and involves an in-joke reference to a movie that six people saw with her in 2008, that's maybe some sort of \"social\" experience but it's clearly not public. We have a Slack like this, created under pandemic conditions and named \"Cabin Fever Mitigation\". If the typical message says \"Aw! Piggy\" and has a picture of a guinea pig, that is now shading into Social Media. Probably some of the people \"following\" this feed don't know who she is but they like guinea pigs, or they like her art, or something similar. And yes obviously if the typical message is a reply to Elon Musk then it's social media and it can fuck off. But hopefully your friends aren't making crucial life updates as a public address to any watching fascists ? reply wruza 19 hours agoprevI'm just not reading any of it - not interested. SM addiction is so 2015. I have technical accounts to be able to search for something (e.g. while training loras) or to watch without annoying popups when someone links me to it. This dramatic deletion is overreaction, solve the underlying problem instead. Rather than scrolling instagram and tiktok, visit /news and /newest, and then /ask, /show. If nothing interesting there, refresh the /newest until there is. You can be first in upvoting or commenting on it, and can get a good bump to your score if you say something that sounds smart before it hits the frontpage. Then you can re-read the quality content you produced and count how much is left to the round number, like it's only 40 to 9700, only 340 to 10000, etc. Much healthier than just scrolling endlessly and sharing memes. reply insane_dreamer 18 hours agoparent> SM addiction is so 2015 it's more prevalent today than it was then, so no. > solve the underlying problem instead that would be to get rid of FB, X etc. altogether; but since we can't do that, we can do the things that we have control over, i.e., our own accounts reply fsflover 3 hours agorootparentWe can move to Mastodon and attract some friends with good posts. reply abhayhegde 4 hours agoparentprevWhile I agree that HN usually gathers much interesting content, I don't understand why getting more karma on HN matters anyway. Chasing points anywhere isn't healthy by the way. Say something interesting because it is interesting and sparks a conversation and not for the sake of saying something. reply wruza 1 hour agorootparentThis is our nature. People who aren't affected by \"KPI\" are the minority. That's why forums must be very careful with choosing a set of published indicators, because people will \"play\" these, often unreflectively. HN is well-established so its score system is likely balanced by other means. Or maybe they'd like to change it, but changes like that may literally boil the forum and turn it upside down. reply fsflover 4 hours agorootparentprevThe OP is surely joking. Less than 10k karma since 2015 isn't a serious karma race ;) reply wruza 1 hour agorootparentIn general I tend to balance my karma around 0 on most sites, treating it as a sort of a currency. But some forums ask nicely to not do that, so I don't, cause I'm a relatively nice guy, and if they ask instead of commanding it, then why not. As a result, the score keeps piling up for decades. Idk what to do with it, maybe I should start a quest after 10k. reply walthamstow 11 hours agoparentprevVery funny, glad I read to the end reply Karrot_Kream 8 hours agoparentprevWait what do you mean? HN isn't social media, it's a breath of fresh air! I'm only here to talk to the folks in my life I care about like … uh oops. reply bflesch 20 hours agoprevI wouldn't delete social media accounts because they might become available to register for malicious actors who can then impersonate you. Keep the accounts, just don't use them any more. reply atrettel 19 hours agoparentThere isn't anything unique about your account on most social media platforms. This isn't a \"plant your flag\" situation like when trying to prevent identity theft. You don't need to register your account before a bad actor does. Sure, I created an online account with the IRS, credit bureaus, etc. before somebody else could. That's important because they are tied to unique identifiers like your SSN, etc. But somebody could just create a social media account impersonating you even if you already have an account on that social network. There isn't anything enforcing the uniqueness. reply matthewdgreen 19 hours agorootparentMy Twitter account has 140K+ followers and impersonators keep making copies that they use for cryptocurrency scams. So that's why I'm personally a little sensitive to deleting it, even if I've mostly committed to leaving that hellhole. reply angoragoats 19 hours agorootparentWhat does keeping the account actually do to prevent scamming? They’re going to scam regardless. reply esskay 19 hours agorootparentCounter point - why is it an issue to wipe the account of its content and update the bio to simply say the owner is no longer on social media and any other accounts you come across are not them. Removing your account completely from Twitter makes it immediately available for anyone else to take, and for larger accounts you can bet theres a whole host of automated monitoring going on, ready to nab it and use it for easy profit. Keeping the account doesn't have to mean you're 'giving away' any info. Hell delete it and instantly recrate it if thats the worry. reply angoragoats 18 hours agorootparent> Removing your account completely from Twitter makes it immediately available for anyone else to take, Do you have a source for this? The only thing i can find is a random tweet from Elmo in 2023. I deleted my twitter account in the 2022-ish timeframe, and the handle I had (created in 2007) was my first initial + last name, which I would think would be claimed by now. It's not, so I'm thinking that deleted account handles can't be reused. reply paularmstrong 17 hours agorootparentThey can be taken immediately. Source me, former Twitter employee pre 2021 reply angoragoats 16 hours agorootparentIt must have changed between when you worked there and now, because I just checked and I can't sign up with my old handle (despite it returning a \"this account doesn't exist\" error when attempting to view it). reply throwaway290 13 hours agorootparentAs of 2023 the model was to allow taking a handle even if it already exists but did not post for a while: https://www.businessinsider.com/elon-musk-x-twitter-inactive... So it seems unlikely they would keep deleted handles forever. I bet they become part of this marketplace program a la \"premium domains\". reply angoragoats 5 hours agorootparentI can’t find any evidence that the plan outlined in that article was actually launched. The owner of Twitter says a lot of stuff, but most of it is made up. reply johnnyanmac 19 hours agorootparentprevThat's pretty much the only upside to that blue checkmark these days. Making anyone able to buy one was a huge mistake, but they will at least do the minium check to see if someone else with that name already has a checkmark. reply matthewdgreen 16 hours agorootparentI was given a blue checkmark by pre-Musk Twitter because of the cryptocurrency scams. It was taken away in the early days of Musk Twitter when verification meant “anyone with $8.” Ironically, it was forced back against my will and without my paying for it, because Musk was embarrassed that larger accounts didn’t have checks. Obviously it didn’t serve any useful anti-impersonation purposes at that point, but I got free “Grok” I guess? reply davidclark 13 hours agorootparentprevIs this a thing? Why would it be? Look at my username - how many people with that name exist in the world? Only one of us can have a blue check on Twitter? Which one? reply angoragoats 18 hours agorootparentprevWill they? I'd actually be surprised if there are many people that, upon receiving a suspicious message from someone who claims to be Joe Schmoe, will actually go and check to see if a different account from Joe Schmoe with a blue check. I think it's much more likely that they're either going to recognize it as a scam right away, or they won't and they'll fall for it. In either of those cases, it doesn't help for the blue-checkmark-holder to keep their account. reply johnnyanmac 15 hours agorootparentThere will always be someone falling for scams. No amount of safeguards will protect them if they do zero due diligence and the scammer is persistent enough. The checkmark isn't an end-all-be-all, but it's another small step someone can use to verify without too much hassle. also, I just noticed \"they\" is ambiguous here. I meant \"the twitter staff giving checkmarks\". At least I hope they do some basic check before handing out a checkmark to an obvious impersonator. reply chenmike 19 hours agorootparentprevI’m pretty sure GP is saying if you already had an account and you delete it, it’s trivially easy for someone to register with your old handle and impersonate you Of course people can always impersonate you but the goal here is to prevent them from impersonating you with a social handle people knew you had. reply jjulius 19 hours agorootparentIf I recall correctly, the handle y",
    "originSummary": [
      "The author deleted their social media accounts due to concerns about platforms like Meta and Twitter prioritizing profit over ethical considerations.",
      "Highlighted issues include Mark Zuckerberg's decision to drop fact-checkers and collaborate with Trump, and Elon Musk's controversial actions, contributing to a toxic environment.",
      "The author emphasizes the negative impact of social media on mental health, particularly for teenagers, and is exploring alternatives like blogging and new platforms cautiously."
    ],
    "commentSummary": [
      "Deleting social media accounts can result in missing life updates from acquaintances, highlighting the platform's role in maintaining connections.",
      "While some believe important relationships will persist through direct communication, social media often facilitates shallow connections that may be lost when quitting.",
      "The choice to leave social media is subjective, influenced by personal priorities and the value placed on different types of relationships."
    ],
    "points": 381,
    "commentCount": 448,
    "retryCount": 0,
    "time": 1736720795
  },
  {
    "id": 42677835,
    "title": "Right to root access",
    "originLink": "https://medhir.com/blog/right-to-root-access",
    "originBody": "right to root access January 12, 2025 I believe consumers, as a right, should be able to install software of their choosing to any computing device that is owned outright. This should apply regardless of the computer’s form factor. In addition to traditional computing devices like PCs and laptops, this right should apply to devices like mobile phones, “smart home” appliances, and even industrial equipment like tractors. In 2025, we’re ultra-connected via a network of devices we do not have full control over. Much of this has to do with how companies lock their devices’ bootloaders, prevent root access, and prohibit installation of software that is not explicitly sanctioned through approval in their own distribution channels. We should really work on changing that. what's the big deal? A bootloader is the computer program responsible for the process of booting up a computer. Root access refers to the highest level of privileges a user can be granted to a computer system. Having access to the bootloader and root gives you full control of your device. You must have access to these if you want to do things like: inspect the processes a device is running install new operating systems interact with the entire file system The average user does not need access to root, which is why you need to “Run as Administrator” on Windows or use sudo on Linux and MacOS to use these greater privileges. It is easy for a system to be compromised if root access falls into the wrong hands. Despite the risk inherent with a greater set of privileges, this elevated access is useful when a user wants to delve into the lower layers of a computing system and make modifications. Citing this security risk, most smartphones' bootloaders are locked by default, with the ability to unlock varying from easy to impossible. The main issue I see with this being the current paradigm – devices that are locked down at the hardware level, without the option to unlock, are inherently anti-consumer. As a result of the lack of regulation around hardware-level locks, companies are all too willing to sell electronic devices with restrictions on what software is allowed to run. These locks are justified through the lens of safety — essentially, the average consumer is at too high of a cyberattack risk if arbitrary software is allowed to run. This \"security\" justification provides cover for many anti-competitive practices: Requiring third-party software to be vetted by the manufacturer, with the ability to revoke distribution at any time through inconsistent policies. Requiring third-parties to revenue share with the hardware vendor's platform. Voiding a device's warranty if a consumer elects to install alternative software, despite the legal ambiguities of restricting warranties in this manner. Restricting access to certain system APIs to create an advantage against competitors on the hardware vendor's platform. This norm of locking devices and preventing loading of 3rd party software is misrepresented as the only way to keep users secure. But there’s ample evidence to suggest this isn’t true — consumers have already had the ability to install software on desktop operating systems for decades. This double standard on \"security\" becomes abundantly clear when we consider two devices currently on the market: MacBooks and iPads. Both use the same M-series processors, however iPads ship with a locked bootloader. Macs that use M-series chips do not have such locks, which means I can install Linux if I so choose. I can also do work as a developer by compiling programs from source and being able to touch all aspects of my system through sudo. Those who purchase iPads, often at great expense, do not have any such privilege despite having an equally capable device. This has evolved into new heights of maddening with the latest iPad Pro release that launched the M4 chip. One of the most state-of-the-art silicon manufacturing processes of 2024 went into a device that you cannot write code on, a reality forced upon consumers through onerous hardware locks. As an owner of the original iPad Pro, which no longer receives active iPad OS updates, I have no option to load an alternative operating system that could potentially lengthen the life of the device. Some may balk at this being practical. But that's why it's my device and not yours. However impractical, I should be able to modify a device that I own as I see fit. balancing safety with consumer choice Locked down devices have been around long enough that many growing up no longer even know what a filesystem is. While I attribute much of this to the choices made by Apple and the like to hide the filesystem from users, I agree with the premise that consumer devices, such as mobile phones, should be as secure as they can by default. This can even go so far as shipping new devices with locked bootloaders and blocking access to root. But this shouldn’t come at the expense of being able to make an informed choice to unlock these privileges to install any software you want, even if that means adopting a higher level of risk. This is fundamentally about defining what rights you, as the consumer, have when purchasing computing hardware. On balance, I do not believe the security benefit provided by locking devices justifies the many negative impacts to consumers that come through these same restrictions. These impacts include: sustainability Devices that are locked become e-waste once a manufacturer stops supporting them. This keeps happening like clockwork: Spotify’s Car Thing Nest Secure and Dropcam Many fitness devices once Google shuts down the APIs Locked devices also allow companies to restrict who is and is not allowed to repair hardware. This is particularly acute in the case of farming equipment – John Deere, which accounts for over 25% of global marketshare for agricultural equipment, notoriously restricts access to data their equipment generates to only their certified dealers. Restricting this data creates an effective monopoly on repair services, harming both farmers that are limited in repairing their equipment and 3rd party repair shops that cannot properly compete with John Deere dealers. Consumers shouldn't have to be at the whim of manufacturers that have little incentive to maintain hardware longevity. Ensuring devices can be unlocked to inspect / modify any software process would go a long way in removing these artificial monopolies. free speech Disabling the right to load software and forcing consumers through a sanctioned corporate distribution channel makes it easier for nation states to silence speech. Nation states can impose requirements, such as banning apps from distribution, in order to operate in their markets. It remains to be seen if we'll witness a similar dynamic take hold in the United States if TikTok does not go forward with a sale to a US-based entity. Being able to run your own software, regardless of whether the source is considered \"good\" or \"bad\" in the eyes of our governments, makes platforms less brittle against geopolitical conflicts. The main loser of these types of locks is the average person. competition Locking devices and preventing distribution of 3rd-party software restricts competition, as barriers to entry prevent developers from providing the full range of services that consumers want. These restrictions are obvious when looking at Apple's iDevice ecosystem. Apple restricts many APIs in favor their own hardware and service products. Some examples: Restricting apps from using NFC communication to provide an unfair advantage to Apple Pay in the mobile wallet product space. Limiting APIs that allow iPhones and 3rd-party smartwatches to communicate to push iPhone consumers to buy Apple Watches vs other brands. Banning alternative browser engines, as they could offer potentially better native web experiences that could compete with the App Store. (rough) thoughts on legal solutions The de-facto norm of allowing large corporations to dictate how customers can use their own hardware is insidious. It should be considered unreasonable to not have any ability to modify the software for a piece of hardware you own. The main exception to this, I believe, would be for critical systems where compromising operation through software modification presents too high a risk. Examples I'm thinking of include: certain medical devices, such as implants and insulin pumps subsets of electronic control units for cars The standard for such restrictions should be extremely high. The burden should be on a manufacturer to demonstrate what material risks exist that justify a hardware lock. Even in the cases where a device is locked down, there should be some ability to audit the processes the device is running. The large majority of consumer products should be covered by “right to root access” protections. tl;dr If you own a computing device outright, you should be able to make any level of software modification you desire. hardware manufacturers should not be allowed to absolutely restrict distribution of software to their own channels under the guise of safety. In the broader conversation of right to repair regulations, we also need to be thinking about a \"right to root access\" for computing devices.",
    "commentLink": "https://news.ycombinator.com/item?id=42677835",
    "commentBody": "Right to root access (medhir.com)329 points by medhir 20 hours agohidepastfavorite311 comments lapcat 16 hours agoThere are a ton of products on the market that are vastly more dangerous than computers: guns, cars, motorcycles, bicycles, chainsaws, table saws, cigarettes, alcohol, junk food. Yes, consumers do sometimes harm themselves by using these products. That's the price of freedom. I think it's bizarre that we treat computers as the most dangerous products in the world that for some reason demand paternalism, when none of these other products are locked down by the vendor. The reason that computers are locked down by the vendors is not that computers are somehow more dangerous than other things we buy. The reason is simply that it's technically possible to lock down computers, and vendors have found that it's massively, MASSIVELY profitable to do so. It's all about protecting their profits, not protecting us. We know that the crApp Store is full of scams that steal literally millions of dollars from consumers, and we know that the computer vendors violate our privacy by phoning home with \"analytics\" covering everything we do on the devices. This is not intended for our benefit but rather for theirs. reply raxxor 8 hours agoparentIf protection of the casual user was an argument, there would be an easy option to unlock your system, be that phones or desktop computers. But on many systems these options do not exist because the vendor likes people dependent on them. This is why devices like chromebooks or all mobile phones are more or less e-waste in the making. In my opinion it is a waste to use any development capacity for these systems apart from consumer devices offering the next shitty app that hopefully always stays optional. We even have dysfunctional laws that require banking apps to only run on these shitty systems. In my opinion, these errors need a quick correction. Also, the most cases of scam still work as they did before and exfiltrating information, e.g. tracking and \"diagnostic data\" by bad operating systems are an additional security problem. reply throw0101a 6 hours agorootparent> If protection of the casual user was an argument, there would be an easy option to unlock your system, be that phones or desktop computers. Making it easy to unlock could make it easy(er) for scammers to get it unlocked: > I received the same type of call a little later in the day. They were very adamant they were calling from the Bell data centre, on a terrible line and I made them call back three more times while I considered their requests. They wanted to have me download a program that would have given them controI of my laptop. […] * https://forum.bell.ca/t5/Internet/Call-stating-that-an-issue... reply AnthonyMouse 1 hour agorootparent> Making it easy to unlock could make it easy(er) for scammers to get it unlocked Making laptops that weigh two pounds instead of 40 pounds could make it easier for thieves to steal them. Making computers less expensive could increase the number of spammers who can afford one and make it easier to send spam. Making encryption widely available could make it easier for bad actors to communicate. But these things have countervailing benefits, so we do them anyway and then address the problems by a different means. When someone insists on doing it in the way that \"incidentally\" provides them with a commercial advantage, suspect an ulterior motive. reply dns_snek 1 hour agorootparentprevEasy doesn't mean without any warning, it just means that the device is unlockable by design and without OEM's approval. It would be reasonable to: - factory reset the device before unlocking it to protect existing data (like Android phones require) - display warnings, for example \"if someone's asking you to do this, it's probably a scam\" - for the owner to be allowed to permanently disable unlocking, e.g. the commonly cited example of someone setting the device up for their elderly parents reply inetknght 36 minutes agorootparentprev> Making it easy to unlock could make it easy(er) for scammers to get it unlocked Ahh, if only governments would start cracking down on scammers. Alas, scammers are a feature of modern capitalism. You'd not be wrong if you thought modern businesses are built on scamming people. reply e44858 34 minutes agorootparentprevUnlocking should require a physical modification, like soldering a jumper or flipping an internal switch requiring disassembly. That would filter out basically all scam victims. If a scammer can teach a complete novice how to do micro soldering, they've earned their pay. reply blacksmith_tb 18 minutes agorootparentThe Chromebooks that require removing a single internal screw are a fairly civilized example of this approach (might be a little harder to execute in a phone). reply sumtechguy 2 hours agorootparentprev> But on many systems these options do not exist because the vendor likes people dependent on them. Dependent is not exactly the right thing here. Lower support costs probably is. If a vendor gives out root access. If that root access can brick a machine. Then you will get a small percentage of very high touch broken things as returns. Customers like this are in the 'dangerous enough' but not 'good enough to do it correctly' stage of hacking. They will then not claim any responsibility for breaking it. As they are hoping you just fix it for free. I had one customer who would randomly change out stored procedures on our code. Then yell at our tech support for thing not working or being broken. Wasting hundreds of hours of time until we realized what he was doing. Locking him out is very appealing. Instead we sold him and his management on 'we will do the work for you for a fee'. Which was more along the lines of 'you do this again we will fire the customer'. That is but one small thing that can/will happen. reply dns_snek 1 hour agorootparentDamage caused by the customer isn't covered by any warranty anyway, and realistically, how many people would tinker with root access as long as the device worked as intended? I'd be really surprised if the number was more than 1 in 100. And if 1 in 20 brick the device in the process, that's 1 in 2000. According to [1] the average warranty claims rate for consumer electronics is 1 in 100. I doubt the difference in support load would even register on the scale. [1] https://www.warrantyweek.com/archive/ww20240711.html reply sumtechguy 36 minutes agorootparent> Damage caused by the customer isn't covered by any warranty anyway Exactly. We charged the guy for what he did. We gave him 'sa' access to the database and he tried to burn us. I think you may be assuming people act rationally? They do not. Most will but you will always get 'that guy' especially at scale. People will lie about what they have done. Or not even realize what they did goofed things up. In my example the guy was asking us to pay them back for defective software (millions of dollars). Right up until we proved he had broke it on purpose. I later found out he did it on purpose (confirmed by former coworkers of his 'he likes to mess with vendors'). He was not even alone. At least 3 other people tried that trick on us at different companies. Most service requests are 'easy'. Small tweak/reship and off you go. But someone who has really broken something can be as easy as 'ship them a new one' to weeks of trying to figure out why a device has suddenly started acting out of spec. That means at least 1-2 people working on something for a period of time. That costs money. > I'd be really surprised if the number was more than 1 in 100. It is the time you have to put into looking into why did you end up with a defect that is not a defect. The margin on some of these IoT devices is in the couple of bucks range or smaller. You have to dedicate 2 guys for 3 months to figure out what went sideways can eat the entire profit margin of the whole run. I was just saying I can see why a company would withhold the info. I did not say I agree with it. Especially for things that are out of warrantee. I think companies are using it to basically have no support and basically leave what would be a decent customer hanging and hoping they can covert to another sale. There is no 'one reason' there is a list. reply bee_rider 12 minutes agorootparentThat seem extremely frustrating. It does seem like there ought to be a reasonable split between personal software and business stuff. I mean you guys had a big contract, it is some negotiated thing between two peers, it could be reasonable to negotiate root in some subsystems, not in others. In the end you can’t really trust anything a system tells you if somebody has full root of it. It seems like you guys keeping control of the logging would be a reasonable give for them, if they expect support. (But why would you guys have planned around a downright adversarial customer? That guy is weird). Also, doesn’t this seem like… basically some kind of fraud? I wonder if your annoying user expected to be able to add the savings whatever he got back from the support contract to his “value to the company” somehow. For personal customers who are just buying smartphones, we don’t really have giant support contracts to screw around with. reply AnthonyMouse 1 hour agorootparentprevThat only explains why a company wouldn't want to provide free software support for software they didn't write. There are at least two alternatives to that. First, sell hardware the user can replace the software on, or that doesn't even come with software, and then don't provide software support at all. Second, provide software support and bill by the hour, in which case the customer messing up their stuff and calling your support is the opposite of a problem. You can even combine them if you want. Free support for the software that comes with it but if you replace the boot loader then support calls are billed hourly. There is no excuse for not allowing it -- it's leaving money on the table. Unless the reason is that locking the user out of the device has the purpose of monopolizing ancillary markets, which should be an antitrust violation. reply regnull 4 hours agoparentprevBefore we put all the blame on vendors, I submit to you, ladies and gentlemen, this: the public finds this tradeoff (privacy for entertainment) completely acceptable. With all the outrage, privacy-centric solutions are out there and relatively easy to find, how come they don't get more traction? Including among the HN crowd? reply _aavaa_ 2 hours agorootparentThere is nothing inherent to the benefits that these companies tout that require them to lock us out of our own devices. What you are describing is not a tradeoff but a magnificent bribe. They bribe us with measly benefits in order to accept the deal that is incredibly favourable for them. See: https://reallifemag.com/the-magnificent-bribe/ reply lapcat 3 hours agorootparentprev> privacy-centric solutions are out there and relatively easy to find Really? Please name them. Over the past 10 or 15 years, I've never seen anything other than the iPhone/Android or Mac/Windows duopoly for sale in any retail store. I've never seen any advertising for other than those duopolies. The HN crowd may be aware of obscure options, but for the vast majority of consumers, they don't exist. And since we as developers make money catering to the vast majority of consumers, we're kind of stuck with the duopoly too, at least as far as our work is concerned. reply regnull 2 hours agorootparentAnd as for \"why are not selling this in every retail store?\", the answer is the same - because if they were, no one would buy them. I found the situation curious, while everyone complains about it, only very few people are trying to do anything about it. Perhaps the breaking point was not reached yet, and something big has to happen to change people's perspective. reply lapcat 2 hours agorootparent> And as for \"why are not selling this in every retail store?\", the answer is the same - because if they were, no one would buy them. That's purely hypothetical. How could any prove or disprove the assertion? The general point, though, is that consumer awareness is essential for sales. People won't buy things that they don't know about. As an indie developer myself, I'm painfully aware of this. It doesn't matter how great one's product is if nobody knows about it. Advertising is very expensive, so it requires vast capital outlays in order to get your products into the minds of consumers and onto the shelves of stores. The big established brands have a massive advantage, making it difficult for competitors to break into the market. Apple itself leveraged its existing brand, with Mac and iPod, in order to promote iPhone. And Apple's primary competitor is Google, who also was already an established brand via search and Chrome. Remember that back in the day, Microsoft almost destroyed the entire desktop OS market. They almost killed Apple too. Only the Department of Justice put some kind of break on it, and Microsoft let Apple live in order to provide antitrust cover. If MS had for example simply withdrawn its apps from Mac—Office, Internet Explorer (remember that Internet Explorer was originally the default web browser on Mac OS X before Safari!)—Apple likely would have died. reply AnthonyMouse 45 minutes agorootparentIt's not just about familiarity. People are willing to try new things. The actual problems are network effects and vendor lock-in. The hardest part about switching from Facebook isn't installing some other app or anything like that, it's getting everyone else you know to switch from Facebook. The hardest part about switching from Windows isn't installing Linux, it's getting e.g. game developers to target Linux before it has significant consumer market share. That isn't to say that doing these thing is impossible, but it certainly isn't trivial, so anyone wondering why it hasn't happened already can't seriously think the only explanation is that nobody cares. It's like saying nobody cares about high healthcare costs -- of course they care, the question is what do we have to do to fix it? reply immibis 45 minutes agorootparentprevI'm glad that Fairphones are available in stores right next to Xiaomis, but they cost three times the price for half the specifications. It may plausibly be cheaper to buy a Xiaomi phone and then personally sue Xiaomi to get it unlocked than to buy a Fairphone. reply regnull 3 hours agorootparentprevHere you go: https://us.starlabs.systems/ Now, how many of you guys have this? Or anything like this? I bet 95% of the HN crowd happily uses iOS/Android daily. reply lapcat 2 hours agorootparentI've never even heard of that before, and I'm terminally online. Anyway, desktop computers aren't really the main problem here. For example, Apple Macs offer vastly more personal freedom than Apple iPhones. If iPhones behaved like Macs in that respect, then we might not be having this debate. To the extent that Macs have been increasingly locked down over the past 15 years, it's mostly just copying the iPhone, porting the \"features\" over from one platform to the other. reply freedomben 2 hours agorootparentprevI have no data to back this up. So what follows is purely my personal opinion. I think the reason people don't care, is because they don't know. The average person either doesn't know or barely knows That anything deeper than what they see in the user interface is happening on their system. We humans are very much an out of sight out of mind type of creature. If we can't see it, it's hard for us to imagine that it exists. reply Dalewyn 1 hour agorootparentPeople know, Facebook and Google getting crap for all their tracking is evidence enough. The reason people don't care is because digital freedom/privacy is largely irrelevant to most people's lives. You can't convince someone to care about something that doesn't affect their life, they're too busy for that. reply ragnese 3 hours agorootparentprevExactly. Even the people who complain about these things immediately get defensive when you call them out on their uses: \"Well, I can't switch because what about my banking app?\" or \"Well, games don't count as software to me.\" or \"It won't make any difference to the big tech companies if I'm the only one who switches, so why bother?\" reply pseudocomposer 4 hours agorootparentprev“The least bad option in a market oriented against users and designed to maximize profit” is not the same as “completely acceptable.” reply MetaWhirledPeas 3 hours agorootparentI believe GP is referring to things like privacy-centric de-Googled Android phones, which definitely are an option. I would not classify those as \"least bad\" or even bad. GP is correct about Apple products; even among the HN crowd they are likely the most popular devices. I think this is because most readers aren't trying to die on the hill of openness. They're more concerned with software and ubiquity, two areas where Apple is doing very well. You do get many here enthusiastic about open access to your own hardware, but I think we're talking about a Venn diagram; we're not all the same. (I'm an Android user.) reply pseudocomposer 3 hours agorootparentActually, I was disagreeing with the GP specifically about Apple products. I'm an Apple user, but very much because they're the \"least bad\" option. De-Googled Android phones still have awful audio latency (I'm a musician who makes a music app on the side), very limited messaging and notification features, and integrate poorly with desktop OSes. For how I use my devices, open or no, Android simply isn't a viable option. The thing about all this is, Apple's products being well-integrated and well-designed doesn't require them to be locked down the way they are. The EU move to force them to use USB-C/Thunderbolt over Lightning is a perfect example of this. It unilaterally improved things for users, and iPhone 14 vs. 15 sales reflected that pretty clearly. So I'd especially describe Apple as the \"least bad\" rather than \"completely acceptable.\" They're specifically what I had in mind saying that. reply MetaWhirledPeas 2 hours agorootparent> Apple's products being well-integrated and well-designed doesn't require them to be locked down the way they are. That's definitely true, and it's what has made me favor Google over Apple for decades now. Google's deal has been free software for the price of your user data, but I've accepted that deal because Google has never practiced predatory lock-in. Apple makes claims to value your privacy (I wouldn't know) while making predatory lock-in fundamental to everything they do. Denying access to your own device is part of this. The irony is that I loathe the data economy. I think it has gone far beyond what Google ever envisioned (for years it seemed they had yet to discover a way to make money at all). The privacy aspect matters, but I also hate the way it makes companies and their products behave; the way it feels like every click results in an attempt to directly advertise to you. And it's all clumsy and broken. How often are ads even correctly targeted? I feel about conglomerated user data the way I feel about meme coins: it's all built on speculation, hopes, and dreams, and has less to do with people actually buying your product. I can't wait for the bubble to burst and/or for a global ban on the sale and purchase of user data. reply pseudocomposer 1 hour agorootparentI think we're very much in agreement on most of these things, and our \"platform loyalty\" led us to perceive different options as the \"least bad\" - that's totally okay, though! I was an Android user from 2009-2020 because I agreed with you, up until I started working on my own music software, which pushed things the other way for me. For your last sentence, though... user data and its utilities are arguably not a \"bubble.\" And as we've seen with AI training, use of data being illegal doesn't really stop companies from doing it. I think we'll have better actual results from governments forcing Apple to let us run our own software on the hardware we buy, as opposed to governments trying to prevent Google, Meta, et. al. from abusing customer data. A lot of this has to do with the fact that the former is about regulating our rights with hardware, while the latter is about software. Hardware is just easier for governments to regulate. When you try to regulate software, companies will do things like the deliberately-annoying cookie popups we got after GDPR/CCPA, because it's cheap to produce lots of bullshit to experiment with ways around those regulations. reply Ajedi32 3 hours agorootparentprevThis isn't about privacy. Not directly anyway. This is about your right to have control of your own property. You make a fair point though; the case does need to be made as to why this is a market failure and not just consumer choice working as expected. Why _do_ consumers tolerate manufacturers retaining ultimate control of consumer's property after the sale? It certainly doesn't seem to be that important to them. Maybe greater awareness of the issue would help somewhat? reply freedomben 1 hour agorootparent> Why _do_ consumers tolerate manufacturers retaining ultimate control of consumer's property after the sale? Just my opinion from many conversations with normies about this: It's because most of them don't know (the marketing material from these companies certainly doesn't advertise it), and the ones who do know don't care because they wouldn't be able to (technical knowledge) or want to root/unlock and utilize the capabilities. reply Ajedi32 1 hour agorootparent> the ones who do know don't care because they wouldn't be able to (technical knowledge) or want to root/unlock and utilize the capabilities This is a good point. Some of that is perhaps self-perpetuating: Why root if there's nothing you can do with root? And why develop stuff you can do with root if there's nobody who can use it? If there weren't so much active suppression of software freedom by manufacturers maybe the situation would change and the benefits of consumers having full control of their devices would be more apparent. reply lapcat 1 hour agorootparentAnd ironically, it was the jailbreakers who demonstrated to Apple why the company needed to add third-party apps to its platform that originally didn't allow them. reply klabb3 3 hours agorootparentprev> This isn't about privacy. Not directly anyway. Agree fully. Don’t know why you’re being downvoted. I accept the risk or tradeoff of Apple or MS spying on me. It’s not that, but the right to repair, to tinker, to hack. Those things have brought us so much interesting wonderful things. My entire generation (millennial) has superior tech literacy to both those that came before and after (no shade to the older gen - some of you are better than us, but with millennials it’s so much more widespread than eg gen X). Many younger gens never use ”real” computers (only tablet & phone). The gilded age was an anomaly, and is over. > the case does need to be made as to why this is a market failure and not just consumer choice working as expected I swear this consumer choice navel gazing will be the death of innovation. The US is obsessed with this narrative, that the magic market hand will self-correct, without any justification or scrutiny. Yes, consumer choice is necessary, but not sufficient. Just look at the developments in tech over the last decade+. I don’t have the solution but anyone who’s not entirely lost in dogma should be able to see the failures. reply Ajedi32 1 hour agorootparentMarket failures do happen, so I'm not claiming consumer choice is the perfect solution in every case. But consumers aren't stupid either; if this _were_ a mainstream concern the market _would_ self-correct. But it hasn't self-corrected on this issue, because most consumers don't really care that much. So I think you have to carefully consider why that is before you start thinking you know what they want better than than they do and eliminating certain choices by government decree. There are costs to any regulation, and lots of possible unintended consequences. So even though I'm personally a strong advocate for user control and software freedom, I'm wary of acting without strong justification and careful consideration of the underlying reasons behind the status quo. > I accept the risk or tradeoff of Apple or MS spying on me. For what it's worth, I do think this issue has indirect effects on privacy. If you have ultimate control of the software on your device, you can use that control in ways that help protect your privacy. Otherwise you're limited to whatever protections the manufacturer decides to grant you. There are lots of similar positive possible downstream effects of software freedom, which is why I think this is an issue worth serious consideration despite my misgivings. reply userbinator 15 hours agoparentprevNot only profits, but control. Remember the whole CSAM scanning debacle from Apple? reply spacedcowboy 2 hours agorootparentwas that when they said “instead of uploading the images to our servers to do the CSAM scan, we’ll do a quick once over in the privacy of your own phone to see if we can allow-list your photo” ? And then the whole world suddenly went apeshit, so Apple basically shrugged, said “fine, we’ll do it just like everyone else and put your photos in the relatively unprotected server domain to do the scan”. Sucks to be you. Understand that at no point was there an option to not do the scan on upload, like all cloud providers, Apple scans for CSAM on any uploaded photos to stay out of any government grey areas. reply godelski 12 hours agoparentprevIt also significantly hampers progress and the utility of tools themselves. This is hacker news after all. What made the computer great was programs. What made the smart phone great (smart) is applications. It's insane to me that these companies are locking down their most valuable assets. The only way this works is if you're omniscient and can make all the programs users could want yourself. This is impossible considering both individuality and entropy (time). Both in the sense that time marches on and the fact that you don't have time nor infinite resources to accomplish all that. I mean we're talking about companies that didn't think to put a flashlight into a phone but it was one of the first apps developed. You could point to a number of high utility apps, but I'm also sure there's many you all use that you're unlikely to find on most people's phones. We can also look at the maker community. Its flourished for centuries, millennia even. People are always innovating, adapting tools to their unique needs and situations. To some degree this is innately human and I'm not embellishing when I say that closed gardens and closed systems are dehumanizing. It limits us from being us. That person obsessed with cars and makes a sleeper Honda civic, that person that turns trash into art, that person that finds a new use for every day objects. Why would you want to take this away? It even hurts their bottom lines! People freely innovate and you get to reap the rewards. People explore, hack, and educate themselves, dreaming of working on your tech because of the environment you created. By locking down you forgo both short term and long term rewards. I also want to add that we should not let any entity claim to be environmentally friendly or climate conscious that does not create open systems. No matter how much recycling they do. Because it is Reduce, Reuse, Recycle. In that order. You can't reuse if your things turn to garbage and reusing certainly plays a major role in reducing. reply eviks 3 hours agoparentprev> think it's bizarre that we treat computers as the most dangerous products in the world We do not? You don't even need a license to buy /operate a computer unlike with some other examples on your list reply lapcat 2 hours agorootparentBy \"we\" I meant online commenters debating the issue of tech company device lockdown. I didn't mean \"the law\". To the contrary, the submitted article author was proposing that we pass laws giving greater individual consumer rights over their devices. But the big tech companies have been viciously fighting against consumer rights, such as the right to repair. reply eviks 2 hours agorootparentThis is also strange as the commenters don't propose the measures that would correspond to viewing computers as more dangerous than guns (lockdown aren't that), but unlike with the law, I don't have a good simple illustration of that. reply lapcat 2 hours agorootparent> lockdown aren't that Vendor lockdown is that. Defenders of vendor lockdown argue that computer users need to be protected paternalistically from themselves. For some reason we accept that for computers, but nobody would accept refrigerators and ovens that only allow you to eat healthy foods, nobody would accept homebuilders controlling the doors of your house and having to approve anyone who comes in, etc. Why do computers get this special treatment of vendor lockdown, but not any other product? reply eviks 1 hour agorootparentWow, ok, if you think this is on par with the lockdowns that the commenters support for guns (which I've previously proxied as ~ existing restrictions), then I'm not sure what to say > Why do computers get this special treatment of vendor lockdown, but not any other product? Of course they don't, plenty of other products are treated much more seriously by \"us\" (supporting lockdowns that limit your own use without supervision), some of them you've already listed reply lapcat 1 hour agorootparentYou appear to be conflating two different things: legal mandates and vendor lockdown. There are legal mandates regarding the sale and use of certain products. For example, you have to be a minimum age to buy cigarettes and alcohol, stores in some localities can only sell alcohol during certain hours, bars have to close at a certain time, you can't drive drunk, you must wear a seatbelt, you can't exceed the speed limit, etc. But there are no vendor lockdowns in this regard. A cigarette will allow anyone to smoke it, a container of alcohol will allow anyone to drink it, you car still works if you're drunk and don't put on your seatbelt, etc. If your car made you take a breathalyzer test whenever you wanted to drive, or it didn't allow you to exceed the speed limit, that would be vendor lockdown. I discussed the issue in another comment: \"The equivalent would be if you could only use specific brands of replacement chains, blades, tires, or bullets that are approved by the manufacturer, for which the manufacturer gets a cut of the sales of those replacements.\" https://news.ycombinator.com/item?id=42684134 reply PittleyDunkin 2 hours agoparentprev> Yes, consumers do sometimes harm themselves by using these products. That's the price of freedom. \"Freedom\" is also a terrible argument for this. What does it even mean? Freedom from what? Freedom to do what? It's such a meaningless word you're going to lose half your audience just by bringing it up. reply diggan 33 minutes agorootparent\"Freedom - the condition of being free; the power to act or speak or think without externally imposed restraints\" When the context is \"digital devices\", it becomes pretty clear what it means. You should be free to use it however you want, without externally imposed restraints. Locking down the device so much so users cannot run applications they've written themselves without the approval of the company who made it, isn't \"freedom\" as the required approval from the company breaks the \"without externally imposed restraints\" part. reply invalidlogin 6 hours agoparentprevA chainsaw does not introduce an opportunity for thousands of remote criminals to steal money from your bank account. reply Retr0id 4 hours agorootparentIt does introduce an opportunity to lose a limb, though. I think I'd rather have my bank account hacked. reply lapcat 4 hours agorootparent> It does introduce an opportunity to lose a limb, though. I think I'd rather have my bank account hacked. Exactly! reply logicchains 5 hours agorootparentprevBut like a gun or a knife it may give local criminals an opportunity to threaten (or worse) you into giving them money from your wallet. reply kees99 4 hours agorootparentYou are 100% spot-on with the \"local\" thing here. People living in \"bad neighborhoods\" have to spend more energy and money on locks, fences, security cameras, self-policing as to not go out alone after dark, etc. Problem is, Internet (and international phone system, to a lesser degree) makes everything so much closer, that scammers from half-way around the globe are \"local\" for all intents as purposes. Thus, online, every neighborhood is a \"bad neighborhood\". reply lapcat 4 hours agorootparent> Thus, online, every neighborhood is a \"bad neighborhood\". This is like the exaggerated crime coverage on the local news. I've lived in the so-called \"bad internet neighborhood\" for 30 years, and I'm fine. It's not so bad. reply jshen 3 hours agoparentprevOnly computers let bad people drain bank accounts at scale. reply lapcat 3 hours agorootparentPlease cite the statistics on the volume of bank account draining before you claim that it happens \"at scale\". reply spacedcowboy 2 hours agorootparentI mean, the nigerian prince scam is almost a meme these days… reply dTal 20 minutes agorootparentInteresting example. How does denying root to the user mitigate this attack? reply lapcat 2 hours agorootparentprevA meme is not a statistic. Exactly how many people have fallen for the scam, out of all computer users. And how exactly does device vendor lockdown stop this particular scam? reply spacedcowboy 1 hour agorootparent> Exactly how many people have fallen for the scam, out of all computer users Who the fuck knows ? And how is that even remotely a useful question to ask - it's not answerable, those who commit the scam are the only people with the figures, and there's no \"register of fuckers who scam other people\" where they have to tell you how well they do. > how exactly does device vendor lockdown stop this particular scam Premise 1: All (for a suitable definition of \"all\") computer users are clueless when it comes to internet security Premise 2: You are not trying to help any given individual's security, because some of them violate premise #1. You are trying to raise the bar for the clueless hurting themselves. Premise 3: It is not about \"personal freedom\". It is about preventing the clueless (by no fault of their own, this shit is complicated) becoming drones and mules for attacks on others. It is an attempt to increase the greater good at the expense of placing restrictions on what any individual can do on their own phone. Those restrictions can be mitigated mainly by coughing up $100/year, which is a sufficient bar to prevent bad guys from doing it en-masse, but not so high as to prevent the people who want to do stuff from doing it. Stopping people doing stupid stuff because they don't know any better is the goal, and that inevitably gets more and more restrictive as time progresses, because an arms race is instituted between the truly evil arseholes who prey on the clueless, and the manufacturers who don't want their produces seen as vehicles leading the clueless to the slaughter. Personally I don't give a crap. The iPhone is fine for me as-is, I can install my own software on my own phone, and sure it costs $100/year. That's not a big deal IMHO, in terms of outgoings it barely registers above the noise floor. YMMV. reply lapcat 1 hour agorootparent> Who the fuck knows ? And how is that even remotely a useful question to ask - it's not answerable, those who commit the scam are the only people with the figures, and there's no \"register of fuckers who scam other people\" where they have to tell you how well they do. Um, why do crime statistics have to come from the perpetrators rather than from the victims? The victims report the crimes, duh. Anyway, you spent a lot of words avoiding my question, which is how exactly does vendor lockdown stop the Nigerian prince scam? You're arguing that vendor lockdown is supposed to protect consumers, but you can't seem to explain how or how often. reply jmull 4 hours agoparentprev> There are a ton of products on the market that are vastly more dangerous than computers An irrelevant \"whaddabout\" argument. It doesn't change that we need security and privacy for our information handling devices, as well as personal control. The real conversation is about how to best balance these. reply lapcat 3 hours agorootparent> It doesn't change that we need security and privacy for our information handling devices, as well as personal control. The real conversation is about how to best balance these. An irrelevant false dichotomy argument. There's no inherent conflict between security/privacy and personal control. I would argue that a device which has to phone home to the vendor to get approval for everything results in both less privacy and less personal control. reply PittleyDunkin 2 hours agorootparentprev> The real conversation is about how to best balance these. How do you even formulate these values so that they're in conflict in the first place? reply jmull 1 hour agorootparentI guess people are unaware of the various malicious rootkits that have cropped up? If you're serious about this stuff binary thinking is a mistake. It's not a question of whether rooting is possible or impossible. It's a question of under what circumstances it can be done, and under whose control. Also, \"conflict\" is the wrong word here. It's a question of competing concerns not conflicting ones. We probably want root access to be under the end-user's control, but in such a way that minimizes the ability of malicious parties to exploit it. e.g., one way would be to allow anyone to easily install any root they want, but to disallow software from, say, the Apple app store from running on such \"rooted\" devices. While that gives end-users control and would mostly prevent malicious actors from getting things they want, it's probably not what most user's would want. They probably want to run all their regular software along side the root software. Another way would be to allowing people to easily install software as root, and allow software from popular app stores to run on it. That gives users max control, but is pretty easy for malicious actors to exploit too. People aren't going to be too happy with this when some coupon clipping app starts emptying people's bank accounts. These are just examples to give the idea of the range of possibilities. The real answer needs to be a lot more nuanced than this. The point is, pretending there aren't issues doesn't get us anywhere. You might as well have no opinion on this. reply PittleyDunkin 1 hour agorootparentI just don't have this paternalistic instinct to try and protect people from rootkits. Even if I did, this is certainly the wrong way to do so—you need to hold companies accountable for the flaws in their software (for which we have basically no legislation at the moment) or they have no incentive to make the regulations meaningfully protective. Otherwise you just end up with shipping hardware that's still insecure, but checks the right regulatory checkboxes, and still restricts people from using the hardware they bought, and still no way to remediate when something inevitably does slip past the regulatory controls. reply mschuster91 4 hours agorootparentprev> It doesn't change that we need security and privacy for our information handling devices, as well as personal control. I can do online banking on my PC as root user if I so choose, but I cannot do online banking on my phone because my bank's app employs a rooting detector SDK that as of now even Magisk+a host of (questionable) modules can't bypass. reply mardifoufs 1 hour agorootparentprevBringing up whataboutism is even less relevant. Comparisons aren't suddenly bad because of an overused buzzword reply m463 5 hours agoparentprevWhy don't they let you firewall your phone? reply skywhopper 6 hours agoparentprevActually, chainsaws, table saws, cars, motorcycles, and even guns all have safety mechanisms installed by the manufacturers and tampering with them voids the warranty. reply lapcat 3 hours agorootparentNobody is arguing that computers shouldn't have safety mechanisms. But the manufacturers of those devices don't have remote control over what I do with them. There's no equivalent of a \"curated App Store\" (and one that requires a 30% cut to the manufacturer, which is the real point behind it). The equivalent would be if you could only use specific brands of replacement chains, blades, tires, or bullets that are approved by the manufacturer, for which the manufacturer gets a cut of the sales of those replacements. reply sudosysgen 1 hour agorootparentprevTampering with safety mechanisms on your car voids the warranty on the safety mechanism, not on your whole car. Otherwise using third party mechanics would be impossible. reply miki123211 7 hours agoparentprev> There are a ton of products on the market that are vastly more dangerous than computers The thing with chainsaws and motorcycles is that they look and feel dangerous, and people have an intuitive understanding of how to approach those dangers. If you ask a random person on the street about safe motorcycle riding, they'll probably tell you about respecting speed limits, wearing protective gear, only doing it when sober, not pulling stunts / showing off etc. I've never been on a motorcycle, have 0 interest in them, and I know those things. Computers don't work that way. People can't distinguish between a real banking app and a fake banking app that looks real, an update pop-up and a fake \"you need to update Adobe Flash Player\" pop-up on a phishing website etc. I've done plenty of \"helping non-technical people out with computers\" during my middle / secondary school days. That was when people still used Windows a lot, as opposed to doing everything on their phones. Most computers I've seen back then had some app that hijacked your start page, changed your search engine to something strange, would constantly open random websites with \"dpwnload now free wallpapers and ring tones for your mobile now\" etc. You didn't even have to fall for a scam to get something like that, plenty of reputable software came with such \"add-ons\", because that's how you made money back then. I feel like that era of \"total freedom\" has somehow been erased from our minds, and we're looking at things through rose-tinted glasses. I, for one, vastly prefer the world of personalized ads and invasive surveillance over one where I constantly have to be on alert for my default browser being changed to Google Chrome for the hundredth time this year, just because I decided to update Skype. reply lapcat 3 hours agorootparent> If you ask a random person on the street about safe motorcycle riding, they'll probably tell you about respecting speed limits, wearing protective gear, only doing it when sober, not pulling stunts / showing off etc. I've never been on a motorcycle, have 0 interest in them, and I know those things. How did this matter? People may know these things, but they nonetheless ignore speed limits, don't wear helmets, drive drunk, pull stunts, etc. And the motorcycle manufacturer can't stop them. They have the freedom to harm themselves. > Computers don't work that way. People can't distinguish between a real banking app and a fake banking app that looks real Guess what, people can't distinguish between the real and fake apps in the crApp Store either. Let's stop pretending that it's safe, when we've seen over and over that it's not. > That was when people still used Windows a lot, as opposed to doing everything on their phones. People still use Windows a lot. Smartphones have not replaced desktop computers but rather added to desktop computers. Almost every desktop computer owner also has a smartphone I believe that desktop computer sales are as high now as ever; I know that's true for Apple Macs, specifically. > I feel like that era of \"total freedom\" has somehow been erased from our minds, and we're looking at things through rose-tinted glasses. It hasn't been erased. The desktop never left. It's been surpassed in volume by smartphones, of course, but let's not pretend that desktops were somehow made obsolete and removed from the Earth. The people who have enough money buy smartphones and desktops. Many even have a smartphone, a desktop/laptop, and a tablet. The choice is not about security, it's about money and form factor. When I leave home, I put a phone in my pocket. When I'm on the couch, I use a laptop. When I'm reading an ebook, I use a tablet. reply grishka 6 hours agorootparentprev> You didn't even have to fall for a scam to get something like that, plenty of reputable software came with such \"add-ons\", because that's how you made money back then. That's why you never blindly clicked \"next\" in installers. Everyone got one of those IE toolbars accidentally at some point, but it usually only took doing it once to learn the lesson. reply 1vuio0pswjnm7 1 hour agoparentprev\"The reason that computers are locked down by the vendors is not that computers are somehow more dangerous than other things we buy \" It makes sense to allow the _buyer_ to responsibly lock out others. This is common in other products that could be dangerous. But allowing the _seller_ to lock out others, e.g., competitors or the buyer, is a recipe for malfeasance, at the buyer's expense. Interestingly, with computers and pre-installed software, there is no option to lock out the sellers such as Apple or the companies that partner with sellers and pre-install software on the computers, such as Microsoft, Google, etc. \"It's all about protecting their profits, not protecting us.\" It is interesting that the \"protections\" are not optional. It assumed _every_ buyer wants the protections from others _and also from themselves_ enabled by default, and also for protections from so-called \"tech\" companies to be _disabled_ by default. A remarkable coincidence. Perhaps if buyers were given the option to login as single user and change the default protections some (not all) might disable phoning home to Silicon Valley or Redmond. They might block unwanted access to their computers by so-called \"tech\" companies who sell them out as ad targets. The so-called \"tech\" companies and their customers (advertisers) from other peoples' computers might be locked out. Indeed letting buyers lock out whomever they choose might diminish the profits of so-called \"tech\" companies. In the past HN commenters often sidestepped the question of these \"protections\" as self-serving and argued that so-called \"tech\" companies serve the \"majority\" of computer users and in fact these \"protections\" are what computer users want even though these users were never asked or given the choice to opt-out. If that were true then allowing a \"minority\" of users to control the protections themselves, i.e., operate as root, would only affect a minority of profits. reply golergka 13 hours agoparentprevThanks, but no. I'm never buying a device with easy root access for a non technical family member ever again. Freedom is great, and I'm using this freedom to buy something with exactly the capabilities I need. reply alias_neo 9 hours agorootparentSo they'll never use a PC or laptop or anything of that ilk again? To use the same logic, they shouldn't be given anything with a visible screw, or are you going to tell me they _wouldn't_ take a screw driver to an appliance because that would be silly for someone who doesn't know what they're doing in there? reply thinkharderdev 3 hours agorootparentIf there were a multi-billion dollar industry of scammers always trying to trick them into taking the screws out of things so they could steal from them, then no I probably wouldn't buy them anything with visible screws. reply alias_neo 1 hour agorootparentWhich is probably fine, that's not the same as taking away everyone's screwdrivers. The problem is that a line is being drawn in an arbitrary place; if scammers are the worry, don't let them have a phone, or internet or email either, in fact just don't let them talk to any strangers in person or otherwise, but that would be awfully inconvenient for them. Everyone is willing to make a compromise somewhere so long as the compromise isn't something they care about. Some readers probably think the suggestion of taking away their phone or email is absurd to protect them from scammers, and I'd place preventing root-access in the same category; not disabling it by default, I'm ok with that, but preventing it entirely. My opinion is that everything should be secure by default, but when it's something you own, there should be reasonable, measured steps to \"unsecure\" it, whether that's removing a couple of screws, or accepting a disclaimer to gain root access to the device you own. If I don't own it, let's cut the bullshit and tell me I'm merely licensing or renting it, and we'll adjust the price I'm willing to pay accordingly. reply lapcat 2 hours agorootparentprevThis is a strange argument, kind of like, \"We can't defund the police, because look at all of the crime out there!\" If there's so much crime occuring already, then what in the world are the police doing? To an extent, crime can't be eliminated. You can't even eliminate crime by instituting a strict authoritarian regime, because power corrupts, and those in power become criminals themselves. That's why turning big tech companies into paternalistic device authoritarians doesn't work. The big tech companies have become massively corrupt, demanding a 30% cut of everything that happens on your devices, in return for what? Some low paid, low skill reviewer spending a few minutes to approve or reject a third party app submission? That's not security, it's security theater. There were phone scams before there were smartphones. Before there were mobile phones, when everyone had a landline. There's no technical solution for crime and scams, much as tech people want there to be. Education and viligence have always been the only effective resistance. Unfortunately, the big tech companies don't want to do education; to the contrary, they want consumers to be eternally technically ignorant—despite the increasing importance of computers in our lives—because that's more profitable. At least with cars, we have mandatory driver's education. reply drdaeman 12 hours agorootparentprevIt doesn’t have to be easy enough to let through a person who doesn’t understand what they’re doing (aka blindly click through the annoying popups - that’d be bad). And non-owners shouldn’t be able to have access solely based on their physical possession - quite the contrary, owner should have means to fully use hardware security features for their personal benefit, locking their own device as tight as they want (within the device’s technical capabilities). reply xorcist 8 hours agorootparentprevI take it you mean easily unlockable bootloader, not really out-of-the-box root access which no phone have. I have taken the opposite stance on that. Never again will they be left with some Samsung bloatware which hardly receives any Android updates when phones such as Nexus, Nokia and Nothing costs the same and has excellent LineageOS support. Lineage is stable, bloat-free self-updating and requires no maintenance from my side. reply shortrounddev2 12 hours agorootparentprevJust because the device is capable of root access does not mean all users need to be reply bell-cot 9 hours agorootparentprevAnd here is (in effect) a completely legitimate reason for manufacturers to wall off root access. They did not want to sell and support a full-access, general-purpose computer. Nor provide liability coverage for anyone who reprograms their toaster and starts a fire. reply frizlab 10 hours agorootparentprevIt’s impressive how many people downvote this actually über reasonable opinion… reply raxxor 8 hours agorootparentBecause it isn't at all reasonable. There is no argument to not allow root access. You don't have to use it, perhaps most users would be safer with a conventional user account, but it is not reasonable to outright deny full system right to the owner of a device since there are so many disadvantages connected to that. reply DoubleGlazing 6 hours agorootparentMy thinking is that if I have device that doesn't allow me root access, then what I have is more than likely a device designed to keep making money for the company that made it or wrote the software for it. reply frizlab 6 hours agorootparentBut you know you don’t have root access before buying. Why would you buy it if you want root access? reply DoubleGlazing 5 hours agorootparentI'm willing to stand corrected, but I can't think of a single smartphone on the market from a reputable manufacturer that is sold with root access. If I want a smartphone I have to accept that the manufacturer will have the bootloader locked down, I don't have a choice. reply frizlab 5 hours agorootparentI have zero experience in the android world, but a quick search tells me that Xiaomi Devices, Google Pixel Phones, OnePlus Devices, Redmi Note 4, Samsung Devices and MediaTek Devices at least are rootable, with some rules with various degrees of freedom for the procedure (in particular warranty is voided pretty much all the time when device is rooted). reply karteum 4 hours agorootparentGoogle Pixels are the few devices that enable not only to unlock the bootloader but also the ability to flash your own keys and still have secure boot together with full kernel sources availability (which is why Grapheneos only support them as far as I know). As far as I know Mediatek (and vendors that use those chips) are usually not good with regards to GPL Compliance, which means no Lineageos if kernel sources are not available... reply eesmith 9 hours agorootparentprevThat's because the opinion presents a strawman position. From the linked-to page : > I agree with the premise that consumer devices, such as mobile phones, should be as secure as they can by default. This can even go so far as shipping new devices with locked bootloaders and blocking access to root. .. > But this shouldn’t come at the expense of being able to make an informed choice to unlock these privileges to install any software you want, even if that means adopting a higher level of risk. One does not require \"easy root access\" to make that informed choice - complicated root access (within reason, as pulling out the soldering iron might be a step too far) should be enough for tasks like installing a new OS because the company no longer supports the hardware. reply femiagbabiaka 12 hours agoparentprevhalf of those things have computers in them now reply metalman 8 hours agorootparentmy fuel injected chain saw, has a data port, but luckily, my back woods repair shop showed me the computerless,seasonal re-tune procedure that only requires a stop watch, works a charm As to other devices....phones, we need a whole re write of the privacy and publishing laws, to allow each person to regulate themselves. With an ultra basic \"standard\" set up for the masses who do want to be entertained, while having buying \"oportunites\" presented to them. But it has to be consentual, and basics like a phone number, email address, and personal/comercial web space, a non alienable birth right.Ban utopian concepts outright, and get back to bieng the quarlsome and somewhat violent species, that we are. I am starting to wonder, is the root cause of all of the ancient civilisations, lying in there own dust, what we are doing now, and the vast echoing silence from the stars, the same. reply Telemakhos 4 hours agorootparentYou've left out one important player here: it's not just about the manufacturers and the masses yearning for entertainment, but also about the surveillance industry. Phones in particular, but computers in general, are increasingly important for surveilling the population in novel ways that AI opens up. Giving people root access on their tracking equipment would jeopardize its surveillance functions, because people might elect to give themselves privacy. reply gjsman-1000 16 hours agoparentprevThis is a very popular HN opinion; but not a very popular real world opinion. The average customer wants a device that works consistently, every day, that is easy to use, with a collection of 3rd party apps who won’t steal their life savings. Windows failed to deliver this; the average customer never downloads an Exe from a newer publisher without terror. The average consumer is literally dozens of times more likely to trust a new smartphone app than a new desktop app. We can also see this in the console market. Windows exists; old gaming PCs exist; the locked down console market will be with us forever because even Windows can’t deliver a simple experience that reliably works. reply _Algernon_ 9 hours agorootparentSounds a lot like \"We don't need free speech because I have nothing to say\". Just because you don't need or want it, doesn't mean it's not an important right to protect. Considering the influence of computers these days, the right of general purpose computing is probably at least as important as the right to free speech. reply do_not_redeem 16 hours agorootparentprevThe average customer wants a car that doesn't explode because you installed a sketchy spark plug. Does that mean the manufacturers should install locks on the hood of every new car, with the threat of jail time if you pick the lock and look underneath? reply gjsman-1000 16 hours agorootparentA sketchy spark plug does not have the ability to make a car explode, so the analogy is pointless. On that note, even if someone stole your car, at least your car does not have access to your bank account, your passwords, your messages, and even your sexual history. The personal and reputational cost of losing a car is not comparable. Many people would actually probably prefer their car to be stolen than the contents of their phone be public. I think a more accurate comparison would be to an electrician. In Australia, doing your own electrical work is a crime even for the homeowner, because it can cause physical death, and is too likely to be done wrong. Yes, you will possibly go to jail for replacing $2 light switches. I assure you that most people’s phones have things they would prefer physical death over being publicly distributed. reply seszett 13 hours agorootparent> In Australia, doing your own electrical work is a crime even for the homeowner, because it can cause physical death, and is too likely to be done wrong. Yes, you will possibly go to jail for replacing $2 light switches And do you find this reasonable, and a good thing to expand to smartphone use? reply mattclarkdotnet 10 hours agorootparentIt’s bullshit, there are no laws banning you from replacing a light switch in Australia. At worst you might invalidate your home insurance. reply lapcat 16 hours agorootparentprev> On that note, even if someone stole your car, at least your car does not have access to your bank account, your passwords, your messages, and even your sexual history. The personal and reputational cost of losing a car is not comparable. You're conflating vendor lockdown with device encryption. The latter does not require the former. reply eesmith 9 hours agorootparentWhile cars don't have access to your complete sexual history, note that https://foundation.mozilla.org/en/blog/privacy-nightmare-on-... points out: \"The very worst offender is Nissan. The Japanese car manufacturer admits in their privacy policy to collecting a wide range of information, including sexual activity, health diagnosis data, and genetic data — but doesn’t specify how. They say they can share and sell consumers’ “preferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, and aptitudes” to data brokers, law enforcement, and other third parties.\" reply mattclarkdotnet 10 hours agorootparentprevIt’s not a crime to do your own electrical work in Australia, it just invalidates your insurance unless you get the work signed off. The websites saying it “could be illegal” strangely never reference the actual law you’d breach. reply realusername 11 hours agorootparentprev> I think a more accurate comparison would be to an electrician. In Australia, doing your own electrical work is a crime even for the homeowner In this comparison Google and Apple have the role of the government, if you believe that argument, that also implies that you believe they should be broken apart for antitrust reply DoubleGlazing 6 hours agorootparentprevThere's nothing wrong with wanting that, but as the author said those of us who want to opt-out should have the choice to do so. If I buy an iPhone, I should have the option to completely disconnect it from Apple and be able to replace the OS with whatever I want. If I do not have the option to do that do I REALLY own the device? The answer is no bacause what I have is a device that I can only use the way Apple allows. When the phone is obsolete and Apple stops updates then all I can do is send it off for recycling since Apple won't allow me to repurpose it with new software. You are putting a lot of trust in the manufacturers as well. For example, they have the technical capabilities to kill the second hand market in their devices if they simply decided to refuse to allow a new user to login to a device. Sure, you could still sell the hardware, but it wouldn't be much use if the manufacturer stopped it from connecting and autorizing. I know this is an extreme example and no sane manufacturer would implement it, but I think it demonstrates why having to option to disconnect is a good thing. The same applies to all other devices that are locked down, things like smart TVs, IP cameras and appliances. Just look at how many early smart TVs are now dumb because the manufacturer stopped updating the on-board apps. There should be no reason why the owner of such devices should be allowed to do whatever they want with them to try and bring them back to life. reply stavros 16 hours agorootparentprevAnd consumers can have that. That doesn't mean I should be unable to unlock my phone and do whatever I want with it. reply gjsman-1000 16 hours agorootparentThe problem is not the ability to unlock your phone. The problem is that 90% of people unlocking their phones will either be for piracy (against the company’s interests), or against the customer's own interests (stalkerware, data extraction, sale of stolen devices). There is a reason malware is over 50 times as prevalent on Android. reply seszett 12 hours agorootparent> The problem is that 90% of people unlocking their phones will either be for piracy (against the company’s interests), or against the customer's own interests (stalkerware, data extraction, sale of stolen devices). Why would you think that? Many Android phones can be unlocked, so it's not a hypothetical situation. I does not enable software piracy, since piracy doesn't depend on root. I know a few persons would install of sort of shit on their phone, including obvious malware, and they lack the knowledge to root their phones. The data extraction problem happens today on unrooted phones in a \"legal\" way, it's done by your regular friendly companies like TikTok, Google or Meta. Rooting enables limiting this which is likely why they are against it. If you look around on forums that discuss the topic of unlocking/rooting Android phones you will see that there is little discussion of piracy and people seem mostly driven by the will to control their own machine instead. reply saagarjha 15 hours agorootparentprevHaving worked on catching Android malware, I can assure you that Android malware does not proliferate because people can unlock their phones. reply yjftsjthsd-h 15 hours agorootparentprevGiven that the vast majority of Android devices aren't rooted, bootloader unlocked, or even installing apps from outside the store(s) that they ship with, what exactly do you think is the reason for more malware on Android? (Taking the claim at face value) reply _Algernon_ 4 hours agorootparentprev>The problem is that 90% of people unlocking their phones will either be for piracy (against the company’s interests), or against the customer's own interests (stalkerware, data extraction, sale of stolen devices). The first point is irrelevant once I've bought a thing. Once I own a thing it is mine to do with what I want, and the company's interests ought to be irrelevant. As for your second point, that is mitigated by making the process sufficiently annoying (eg. hiding it in the developer menu). reply scarface_74 5 hours agorootparentprevIt came out in the Apple vs Epic trial that 90% of all App Store revenue comes from in app purchase for games - mostly pay to win games. If they all went out of business, nothing of value would be loss. Then you have apps that are free clients for services. There is very little legitimate money being made by mobile from people actually buying apps reply xorcist 8 hours agorootparentprevWhere does the 50x figure come from and what types of malware does it include? It doesn't really match neither my experience or pricing on the grey exploit market. Malware has a wide definition however, and if you include all the spyware included with phones that aren't sold outside China and to a degree also India, you could probably hit that mark. But as they aren't allowed to access Google services or the official Play store, it's also a bit misleading. reply stavros 16 hours agorootparentprevWhy do I give a shit about the company? I bought the phone, it's mine, I should be able to unlock it. If I catch malware, I'm an adult and I'll live with my choices. > There is a reason malware is over 50 times as prevalent on Android. What's the reason for that bogus-sounding statistic? reply alias_neo 9 hours agorootparentLet's say for a second it was accurate (It's probably not), perhaps it's because Android has a far higher market share globally, and it's much cheaper and easier to get started writing apps (or malware) for Android than say iOS. You also don't need to buy a single device from Google to get started. You can take the PC you're at and get started right away, and publish that app (or malware) without spending a penny (though I don't recall whether they still charge that nominal fee to get a developer account). Saying 90% of people root for piracy is hilarious, I rooted every Android device I owned until the last one or two, and I've never pirated anything, why did I root? Mainly for customisation and host-based ad-blocking. I can't understand the thought process of these people who think the things you own should be locked down to protect you. We should stop selling screwdrivers too in case someone's granny tries to open their toaster and electrocutes themselves, after all, a screwdriver is the pre-tech root access to all things electrical and electronic. I suspect those same people who argue in favour of locking these devices down would also say \"don't by silly, my granny wouldn't open her toaster with a screwdriver, because she's not an engineer\". reply stavros 8 hours agorootparentYeah, agreed. This \"I don't want to own my things because I want Big Brother to protect me\" attitude is really frustrating, especially when you can have protection without Google holding all the keys. GrapheneOS isn't less secure than stock Android. reply alias_neo 6 hours agorootparentIt's a kind of madness people only have towards our (technology/IT) industry. I don't know if it's because they don't understand it, and that's scary, so they think it's safer for the big boys to hold the keys, but imagine if people acted the same in other contexts? \"The bank should keep hold of the keys because otherwise I might accidentally lock myself out, or lose my keys, or leave the door unlocked for a bad guy to come in and steal my stuff\". That's fine if you can't trust yourself to look after them, let someone else handle your keys for you, perhaps someone \"trust worthy\" could offer it as a service, but I'll keep my keys in my own pocket thanks. reply raxxor 8 hours agorootparentprevThere would be no piracy on smartphones. That would require desirable applications. Those don't exist because the environment is that shitty. reply nirui 12 hours agorootparentprev> The average customer wants a device that works consistently, every day, that is easy to use And it can only be archived with a fully locked down hardware? Of course not. The modern OS archives system security through permission and isolation, which don't require bootlock etc to work. In fact, it worked well too even after the device is unlocked & rooted. > Windows failed to deliver this; the average customer never downloads an Exe from a newer publisher without terror Windows (and Linux for that matter) is not modern OS. They're classic OS that offers the entire computer as playground for the program running on top of it. That's why Windows can be contaminated with a single malice EXE, but not Android or iOS. OSs are not the same, don't try get the water muddy that way. reply oneshtein 11 hours agorootparentAndroid is Linux. :-/ reply lukevp 11 hours agorootparentAndroid is built on top of Linux. Android the OS has a lot of permissions layers between an app and the bare metal. reply xorcist 5 hours agorootparentStill, those permissions are standard Linux permissions. So the argument that Linux is less secure than Android is a little hard to understand. A little more specificity might help. reply freedomben 1 hour agorootparentThey're definitely not \"standard Linux permissions.\" Yes Android does use many of those (such as standard user IDs, file system permissions, and now SELinux) to implement some of its permissions, but it adds a ton of permissions on top that are not part of Linux. reply throwaway7623 3 hours agorootparentprev> Windows failed to deliver this; the average customer never downloads an Exe from a newer publisher without terror. The average consumer is literally dozens of times more likely to trust a new smartphone app than a new desktop app. Yet that trust is, for the most part, unfounded. There's a ton of malware in app stores - you can assume any app that contains ads is sending data about you to some shady server, for example. You can't even trust the most popular apps not to be malware [0]. [0] https://news.ycombinator.com/item?id=42651115 reply raxxor 8 hours agorootparentprevIf you explain all details about the advantages and disadvantages to them, I am sure they would think differently. There are much more \"hostile\" smartphone apps that exfiltrate your data and sell it to the largest bidder than there are compromised executables these days. Also there are more profitable scams than compromising a PC system outside of industrial espionage. PC in contrast to consoles always were a cost or usage factor. The difficulties of operating a PC isn't significant. It also heavily increases digital competency of the user for computer systems. If you really don't want that, you have other options. reply lapcat 16 hours agorootparentprev> with a collection of 3rd party apps who won’t steal their life savings. This is blatant unempirical scare mongering. How many desktop computer users have had their life savings stolen by 3rd party apps? Citation needed. > The average consumer is literally dozens of times more likely to trust a new smartphone app than a new desktop app. This is a false dichotomy. Almost all desktop computer users have a smartphone too. The people who have enough disposable income buy both smartphones and desktop computers. There's no inherent conflict between the two. > the locked down console market will be with us forever because even Windows can’t deliver a simple experience that reliably works. That's a competely ahistorical interpretation. Originally, the gaming consoles had no third-party games: the games were all written by the vendors. The first third-party game development company was Activision, a group of former Atari programmers who learned that their games were responsible for most of Atari's revenue, but Atari refused to give them a cut, so they left and formed their own company. There was a lawsuit, and it was ultimately settled, allowing Atari to get a cut of Activision while allowing Activision to otherwise continue developing console games. It had nothing to do with \"reliablity\" or \"security\" or any kind of made-up excuse like that. reply gjsman-1000 16 hours agorootparent> This is blatant unempirical scare mongering. How many desktop computer users have had their life savings stolen by 3rd party apps? You’re kidding, right? You seem to have completely forgotten, or put the drunk glasses, on what living in the 2000s was like. Also, I don’t care that you made it, don’t let your survival bias hit you on the way out. > Originally, the gaming consoles had no third-party games: the games were all written by the vendors What a stereotypical HN comment. Cite something that only applied to the 2nd generation of consoles to prove me wrong, even though my point spans almost all console generations. reply lapcat 16 hours agorootparent> You’re kidding, right? You seem to have completely forgotten, or put the drunk glasses, on what living in the 2000s was like. Again, citation needed. I made it through the 2000s just fine, thank you. > What a stereotypical HN comment. Cite something that only applied to the 2nd generation of consoles to prove me wrong, even though my point spans almost all console generations. No, I was explaining the historical origin of the game console business model. Of course the business model continued, as these things usually do, through a combination of monetary incentives and inertia. reply cesarb 4 hours agorootparent> Again, citation needed. I made it through the 2000s just fine, thank you. Playing devil's advocate: banking trojans used to be really common here in Brazil back in the pre-smartphone era of the early 2000s (smartphones already existed, but weren't very commmon; most people who used online banking did it through their home computers). They're the reason why, for a long time, it was hard to use online banking on Linux: banks required (and still require) the use of an invasive \"security plugin\" on the browser (nowadays, there's also a Linux version of that plugin, which IIRC includes a daemon which runs as the root user), which attempts to somehow block and/or detect these banking trojans. reply lapcat 4 hours agorootparent> Playing devil's advocate What does this even mean? Do you stand behind what you say? If so, then just say it without hiding behind the devil. And if you don't stand behind what you say, then why in the world are you saying it? reply dullcrisp 15 hours agorootparentprevOf course. As we all know here, any business that gets started will go on forever regardless of market fit. reply lapcat 15 hours agorootparentThis is a silly criticism. After all, as we all know here (right?), Atari itself fell on hard times. I was talking about the business model, not a specific business. Vendor lockdown and taking a cut of 3rd party software is clearly quite lucrative for vendors, and that's why they do it. There's of course no guarantee of success, but it's obvious why other vendors have emulated that business model. It may be only for historical reasons that desktop computers aren't completely locked down too. It's a lot easier to lock down a new device class, like smartphones, than it is to lock down an existing open device class, without causing consumer outrage and rebellion. reply LocalH 11 hours agorootparentI worry about the long term health of general-purpose computing. It's not going anywhere today, but I fear for future generations that will likely eventually never know the joy of bending a computer entirely to their will, because they'll have never known computing without guardrails. reply tliltocatl 10 hours agorootparentprevThe average customer only exists in marketing people's heads. reply sans_souse 7 hours agorootparentprevYou can stop saying this now, those numbers (of ignorant users) no longer serves the argument. We did our job in informing the majority. reply fargle 15 hours agorootparentprevthat's a, frankly, stupid argument. the conclusion doesn't follow the premise. then don't root your phone or download an .exe. having the ability to do something doesn't mean you are forced to do it. not safe enough for you? fine! make the current status quo comfortable walled-garden-of-illusionary-fake-safety the default. for example, there's no reason windows needs to by default allow unsigned code to run. hell, even make it really annoying to turn off. but the \"safety\" and \"easy to use\" arguments against right-to-repair, digital rights, ownership, etc. is simply nonsense. there is literally ZERO negative safety or usability impact to anyone else's device because i'd like to own mine. it's also an insulting and disingenuous argument to hear anyone on this forum make: our careers and entire segment of the economy would not exist if it were not for open systems. and it's insulting to basically say \"bubba/granny is too dumb to be trusted\" with owning their own device. reply mixdup 16 hours agoparentprevThere's a difference between being able to buy something dangerous and being forced to do so reply homebrewer 9 hours agorootparentWe're heading the opposite way of not being able to buy anything \"dangerous\" thanks to consumers that you're describing. I've been using a Xiaomi phone that stopped receiving updates in 2020, and have since been running LineageOS, which was made possible by the unlocked bootloader. Xiaomi has since changed its policy and it's basically impossible to unlock the bootloader on newer devices. If not for the \"dangerous\" unlocking, I would have to run with dozens of severe vulnerabilities right now, all five years worth of them. A decent phone costs large amounts of money here, the hardware on mine is still very good, and so I would have used it regardless. (Yes, I understand that the firmware does not receive updates, but it's still much better than nothing.) reply lapcat 16 hours agorootparentprevForced? I'm not sure I understand. My guess is that you're assuming, wrongly, that vendor locked devices are \"safe\" and unlocked devices are \"unsafe\". All computers that are connected to the internet are unsafe in some ways. The most dangerous apps on your computer are the vendor's own built-in web browser and messaging app. Also, the vendor-controlled software stores are unsafe cesspools. You will never find a more wretched hive of scum and villainy. Moreover, the vendors deliberately make it impossible for you to protect yourself. For example, iOS makes it difficult or impossible to inspect the file system directly, and you can't install software such as Little Snitch on iOS that stops 3rd party apps—as well as 1st party apps!—from phoning home. In any case, most computers, including Apple computers, have parental controls and the like, so you can lock down your own device to your heart's content if you don't trust yourself, or you don't trust the family member that you're gifting the device. reply mixdup 15 hours agorootparentToday, yes, I can lock down the iPhone I give to my son, but if it can be unlocked to run arbitrary software then he can in theory unlock it. Yes, it is on me to continue to monitor the device to make sure he hasn't done it, but the point stands And the assumption you refer to, there are varying definitions for \"safe\". Is a device with a locked bootloader 100% safe in all use cases and all circumstances? Of course not. But me being able to reasonably trust that someone hasn't put a compromised version of the OS on the device, or, won't be able to put a different firmware on the device to brute force my encrypted contents is a bit of safety in a certain set of circumstances that I want in my device If Apple, or anyone else, were precluded from locking the boot loader yes, I would be forced to buy a device that the FBI or anyone else could in theory poke around on enough to try to get at my data reply lapcat 15 hours agorootparent> Today, yes, I can lock down the iPhone I give to my son, but if it can be unlocked to run arbitrary software then he can in theory unlock it. Yes, it is on me to continue to monitor the device to make sure he hasn't done it, but the point stands You're scared of the wrong thing. The greater danger isn't arbitrary software but rather your son running up massive App Store charges on IAP of exploitative games and other scams. And if you think Apple will refund you, think again. Locking the device to the crApp Store isn't the solution. To the contrary, the solution is to enable parental controls to prevent access to the crApp Store. > But me being able to reasonably trust that someone hasn't put a compromised version of the OS on the device, or, won't be able to put a different firmware on the device to brute force my encrypted contents is a bit of safety in a certain set of circumstances that I want in my device These are possible without vendor lockdown. Devices can be and are designed so that the consumer can lock the device down and prevent modification, etc. Of course you can't constrain yourself, if you have the credentials to unlock the device, but you can constrain everyone else, whether they're children on the one hand or thieves/attackers on the other. reply godelski 12 hours agorootparentprev> but if it can be unlocked to run arbitrary software then he can in theory unlock it. I'm effectively the admin several machines with many users on them. I have root access. I'm not at all concerned that they'll gain root access. Just make yourself admin on your child's phone, I don't see the issue. Apple and Google can even make gaining root access require some technical (but documented) methods. Look at the requirements to gain root on an android phone currently. You should be comfortable going into a terminal and using ADB. I'm not worried about the average user doing this nor even the average smart child. Hell, follow Apple's lead and require a 1hr lockout if you're really concerned about someone getting root on your device. How often will that happen if it requires being connected to a computer for an hour? reply kachapopopow 6 hours agoprevThe reason why this will never happen is simply due to things like DRM. We right now have ENCRYPTED signal going from our computer to our displays, not just computers, but phones too SIMPLY to prevent people from dumping raw data. All of that extra processing done just so you're allowed to for ex: watch netflix with a resolution higher than 720p. Then comically there's Chinese capture cards that you plug your GPU into, use mirroring mode and completely bypass it. DRM is just one example, there's many more motivations such as preventing paid apps / pay for currency games from having these things given for free. This is the primary reason why iOS devices make significantly more money than android as it's near impossible to pirate / hack / crack for an average user. reply cakealert 51 minutes agoparentI suspect DRM will eventually be self defeating. For example, I prefer to torrent content just so that I can get stuff to play using my media player of choice (and the instant seeks) without any hassle. Most normal people probably aren't even aware this is an option. But with cryptocurrencies normalizing it's only a matter of time before a paid piracy service emerges that is both cheaper, simpler and better than Netflix or any other streamer. Some arguably already have. DRM was being broken for years without even a monetary incentive, with one it won't stand a chance. reply beefnugs 1 hour agoparentprevThere is nothing stopping anyone from selling an HSM (hardware security module) that can decode their protected video without fisting the control into the computer itself reply maniacwhat 2 hours agoparentprevPerhaps it imposes some restrictions, like using TPMs, but I don't think it excludes what the author is suggesting, which is the ability to run as root. Case in point: every popular desktop PC let's you run as root, and also watch DRM content. They aren't totally mutually exclusive. reply margana 2 hours agorootparentYou can't play 4K Netflix on Linux, period. Because of DRM. Before you say \"this is just a Netflix issue\" - you can't play 4K Prime Video on Linux either. Nor 4K Disney+. And many other services. Piracy is the only way to watch most 4K streaming content on Linux. You may have the most capable and up-to-date hardware on the market, you still can't. reply tombert 2 hours agorootparentYeah, that realization is what killed my attempt at replacing my Nvidia Shield TV with my home-built SteamOS box. I got everything \"working\" in the most technical sense, but I was limited to 720p on Hulu, and that ended up driving us crazy. I know that the box is capable of streaming 4K video just fine, because I was able to stream my 4K Blu-ray rips from my Jellyfin server just fine, so this limitation is purely artificial. I did do some experimentation with VMs and emulation and whatnot, but I never got anything that worked consistently enough to use full-time, so I bit the bullet and plugged my Nvidia Shield TV back in. reply lrvick 16 hours agoprevI detest Google, but I do think they made the right call with Android devices and Chromebooks. You can unlock either as long as you are willing to totally wipe the device first and start over as a new device under a new security context. This removes the risk of this being abused to compromise the data of stolen devices or evil maid attacks unless a user that knows what they are doing has explicitly opted themselves into that risk. reply yndoendo 16 hours agoparentI contacted the Google through the BBB. Made the statement that lack of ability to install and configure a Kernel level firewall, edit the HOSTS file, and remove unwanted bloat-ware reduces the security of the product. Google agreed their actions do this and said they find the lack of security acceptable. Having a firewall like Little Snitch should be acceptable to know where the phone is communicate, with whom, and how to prevent it. Re-imaging with a rooted image is not acceptable because this also reduces the device's security by prevent OTA updates! Gated community is broken when the end user cannot improve the security of the device above and beyond the lack polices of Google and Apple. For instant there should be no reason my device ever communicates with organizations I do not support such as Facebook or X-Twitter. X-Twitter is often used as command and control service in plain site. It is not just out-wards communicate to monitor but in-wards too. I've used Zone Alarm in the past at an international company to help find the infected servers and computers that where serving up viruses and other malware. *I would argue that the \"Gated Community\" analogy is flawed. A real world gated community still allows for the home owner to improve the security. By installing cameras, security system, and guards. Apple & Google prevent such actions. reply arsome 1 hour agorootparentThere are indeed software firewalls on Android that use the VPN functionality to implement something like this so they don't even require root, I believe Glasswire offers one. reply botanical76 10 hours agoparentprevIt does create an interesting choice, though. For example, certain apps will enforce attestation based on the bootloader status. Even if the user wipes their device and relocks their bootloader with their own keys, this doesn't count as fully secure per the bootloader status. Only Google's keys count. Of course, it is also almost prohibitively difficult to deliver yourself OTA updates after this point. I worry that one day I will have to keep two mobile phones; one for bank apps, which has not been altered from the vendor's security defaults, and one for everything else, that I am actually allowed to modify. At the moment, I just run GrapheneOS and don't bother with any modification. It is not worth the hassle. I've already had my bank account locked out because a Google Store-bought Pixel phone was flagged as \"stolen\", probably due to some attestation measure (they could not tell me why). They recommended that I purchase a new phone. reply JeremyNT 16 minutes agorootparentRight now, although it's possible to use Android with either root or a third party ROM, attestation breaks all sorts of little things. Today this is mostly banking apps, and anything that involves NFC, but this isn't where it's going to end. Attestation requirements are only going to become more prevalent. I predict that in a few years basically all proprietary software for Android will require attestation. So... you may still be able to unlock the device and make it yours, but you'll also be locked out of the ever expanding and ever-more-isolated walled garden. If you can live off of GrapheneOS and F-Droid, that's great, but for a lot of users this won't be a real choice, because you increasingly need proprietary software for access to real things in the physical world (i.e. I needed to install a special app for event tickets recently). reply grishka 6 hours agoparentprevThe problem with bootloader unlocking on modern Android devices is that they have a hypervisor that you don't get to ever unlock but that will snitch on you and make some apps, like some banking ones, refuse to work because the \"integrity\" of your device could not be verified. In other words, because these apps can no longer be certain they are able to hide data from you the device owner. Magisk exists, yes, but it's a flimsy temporary solution. It only works because it's able to lie to Google that your device doesn't support hardware attestation. As soon as Google starts requiring that all devices support hardware attestation, it will stop working. reply flutas 3 hours agorootparentIt's even worse now with the P9 series. They require hardware certification for the Pixel Screenshots app... and for anything that uses Gemini Nano (Call recorder summary, weather, pixel screenshots, etc). reply alex7734 16 hours agoparentprevThis, or even sell \"dev units\" with the bootloader unlocked so that you explicitly have to accept the risk before purchasing the device. The problem though is that rooting by itself is not that useful when a lot of apps use remote attestation to deny you service if you're rooted. We don't just need root access, we need undetectable root access. reply bboygravity 12 hours agorootparentI agree useful rooting should be easier, but it's definitely possible and not super hard to hide rooting. I'm typing this on a rooted phone where all (banking) apps work just fine. All it takes is downloading an app (magisk) and add apps to a list that need to have rooting hidden. reply pbmonster 8 hours agorootparent> it's definitely possible and not super hard to hide rooting. Worth noting that this could change with every update. It's an unstable situation right now, which is undesirable. For that reason, e.g. the GrapheneOS team isn't employing measures to fake compliance at all. They'd really like to get SafetyNet compliance for their operating system (you need that to get Google Pay/Wallet to work), but funamentally can't get it. Right now, they could just fake it, but that's not guaranteed to work reliably, forever (and doing so would probably threaten their official BasicIntegrity compliance). reply Magnusmaster 2 hours agorootparentprevMagisk only works because Google still supports devices that don't support hardware attestation. Very soon you won't be able to fool Play Integrity without hacking the TEE reply cwalv 15 hours agorootparentprev> We don't just need root access, we need undetectable root access. At some point the argument morphs from 'I should be able to do whatever I want with my device' to 'I should be able to access your service/device with whatever I want'. The fact that Google allows this shows that 1. Apple could do it with zero security impact on anyone who doesn't opt in 2. They could keep any service-based profit source intact But they still would never do it. Because it's not only service based profit they want to protect. They want to restrict customers from running competitor's software on their hardware, to ensure they get their cut. reply josephcsible 15 hours agorootparent> At some point the argument morphs from 'I should be able to do whatever I want with my device' to 'I should be able to access your service/device with whatever I want'. I'm not demanding to be able to log in to your service/device and replace IIS with Apache on it. I'm just demanding to be able to access it as a normal user with Firefox instead of Chrome. reply cwalv 15 hours agorootparentI'm not saying you shouldn't be able to access from unlocked devices. I'm just saying it's a different argument. reply stavros 16 hours agoparentprevAgreed, that's a good solution. I can root my phone immediately when I buy it, or I can leave it locked if that's my choice. That's the best of both worlds. reply yjftsjthsd-h 15 hours agorootparentI would argue that the best of both worlds is being able to add your own keys and then relock the bootloader. Which Pixel devices also do:) Not sure about Chromebooks; I kinda think you maybe could reflash the firmware and then put back the write-protect screw? reply solatic 13 hours agoprevThe way to balance security and freedom is with a hardware switch. By default, keep secure boot etc. But if someone opens the case, takes out the battery, and moves a little switch on the board? Start with a fresh, unprotected context. Because it's a hardware switch, it can't be remotely hacked. An adversary who gets the hardware anyway can get control (are we going to pretend otherwise?). So just do the right thing and make it easier for people to take over their own hardware. reply cube2222 5 hours agoparent> are we going to pretend otherwise? aren't we in fact pretending otherwise? Right now I believe that stolen iPhones are effectively bricks (barring state-level actors with unpatched zero-days)? reply dusted 11 hours agoprev> I believe consumers, as a right, should be able to install software of their choosing to any computing device that is owned outright. While I agree, I think even legislation will not fix this, because what is a computing device, and who decides what is and what is not ? I'm sure apple will argue that nothing they sell should be considered computing devices. While the hacker will consider anything they can trick into arbitrary code to be one (is your fridge a computing device?) If we go the legal route, I think the only way is to give the right to flash firmware of _ANYTHING_ that has programmable bits, and that's probably not going to fly either because lots of legislation already dictates users should be prohibited and prevented. reply perlgeek 10 hours agoparent> While I agree, I think even legislation will not fix this, because what is a computing device, and who decides what is and what is not ? If there is legislation, it will contain a definition of what is a computing device and what isn't. It will be imperfect, and the edge cases will be contested in courts. Courts deal with blurry boundaries all the time. That's how it always is with legal matters, and doesn't mean we have to demand that anything with a firmware must be flashable. reply dusted 3 hours agorootparentWhat I mean is that I think this is the fastest way to end the era of widely-available general-purpose-computing devices that we are currently in (and that is currently ending, but at a glacial speed). It's not that hard to imagine a version of the world where computers as we know them do not exist, but are mere appliances (like tablets and smartphones), and if companies feel threatened that they might be forced to open up their computing devices, they will be quick to make them not fall under the definition. Instead of a smartphone, you will get a \"Can telephone and access facebook and instasnap\" device with whatever technical cripplement is needed to make it not a computing device and be exempt from the law. And as the general public and justice system is pretty ignorant with regard to technology, it's going to be pretty resource intensive to convince a judge why every gadget around that suddenly identifies as \"not a computing device\" is in fact on anyway. reply EMIRELADERO 3 hours agorootparentThat's easily solved. Just scope the law to any device that can run code, and have the criteria for control be \"the user must never have less control over code execution as the manufacturer does after the sale\". So, for example, if someone buys a phone from Apple they will get full control of the entire device (SEP/TEE included) because Apple has the ability to exercise post-sale code execution control to that level (they hold the private keys required). reply rad_gruchalski 3 hours agorootparentDoes that apply to those biometric readers issued to me by the government? If not, why not? Can I have a root on my car to disable ISA? Why not, if not? Do you see the problem? reply EMIRELADERO 22 minutes agorootparentI don't really see the problem. I find it perfectly acceptable for people to be able to change every single thing about their cars. If it's an illegal mod you can let the law deal with it. reply Ajedi32 3 hours agoparentprev> I think the only way is to give the right to flash firmware of _ANYTHING_ that has programmable bits This seems reasonable to me. What's wrong with it? reply blueflow 8 hours agoparentprevGerman here - I do believe this legislation already exists - the owner of a thing has full rights of disposal and no other entity is allowed to interfere (except for the state itself). And this is part of the common property rights. afaik the property rights in the US are even stronger. But i wonder, why these rights do not seem to be enforced on computing devices. Either everyone is failing to assert their property rights or i am in the wrong here. Probably the latter. reply reshlo 10 hours agoparentprev> I'm sure apple will argue that nothing they sell should be considered computing devices. “What’s a computer?” reply grishka 8 hours agoprevThe problem is larger than that, it's the IT industry's obsession with denying users the ability to evaluate their own risks and take their own responsibility. You do that all the time every day in most other areas of life, but somehow interacting with technology is different. The manufacturer always knows better. Don't want to have a time component to your biometric authentication because you know your risks? Too bad. Google and Apple know better. Password is required to unlock Touch ID. reply anfilt 8 hours agoprevI have talked about this before. The issue goes further in my opinion and starts to effect property rights themselves. In particular locked down hardware starts to effect the owners right of exclusion. The right of exclusion loosely is the right include or exclude something from/usesing some property. When the hardware is locked down the owner can know longer solely make those decisions. Instead in the instance of like an iDevice Apple makes those choices instead of the owner by only allowed code they have signed or signatures they allow. An other post I have posted regarding this: https://news.ycombinator",
    "originSummary": [
      "Consumers should have the right to install any software on devices they own, but companies often lock bootloaders and restrict root access, limiting user control. - While security is a valid concern, these restrictions are seen as anti-consumer and anti-competitive, contributing to e-waste and limiting repair options. - Advocates suggest that a \"right to root access\" should be included in right to repair discussions, with exceptions for critical systems where risks are too high."
    ],
    "commentSummary": [
      "The debate on root access in devices underscores the conflict between consumer freedom and vendor control, with many advocating for the right to install any software on owned devices. - Critics argue that vendor-imposed restrictions prioritize profit over consumer control and privacy, limiting users' ability to modify their devices. - The discussion extends to broader themes of property rights and the autonomy to manage one's own devices, with differing views on the security benefits of locked devices versus informed consumer choice."
    ],
    "points": 330,
    "commentCount": 311,
    "retryCount": 0,
    "time": 1736722396
  },
  {
    "id": 42678754,
    "title": "Doom (1993) in a PDF",
    "originLink": "https://doompdf.pages.dev/doom.pdf",
    "originBody": "I made a Doom source port that runs within a PDF file.I was inspired by the recent HN post about Tetris in a PDF (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42645218) and I wondered if I could get Doom to run using a similar method.It turns out that old versions of Emscripten can compile C to asm.js code that will happily run inside the limited JS runtime of the PDF engine. I used the doomgeneric (https:&#x2F;&#x2F;github.com&#x2F;ozkl&#x2F;doomgeneric) fork of the original Doom source, as that made writing the IO fairly easy. All I had to do was implement a framebuffer and keyboard inputs.Unlike previous interactive PDF demos, the output for DoomPDF is achieved by creating a text field for each row of pixels in the screen, then setting their contents to various ASCII characters. This gives me a 6 color monochrome display, that can be updated reasonably quickly (80ms per frame).The source code is available at: https:&#x2F;&#x2F;github.com&#x2F;ading2210&#x2F;doompdfNote that this PDF can only run in Chromium-based browsers that use the PDFium engine.",
    "commentLink": "https://news.ycombinator.com/item?id=42678754",
    "commentBody": "Doom (1993) in a PDF (doompdf.pages.dev)311 points by vk6 18 hours agohidepastfavorite60 comments I made a Doom source port that runs within a PDF file. I was inspired by the recent HN post about Tetris in a PDF (https://news.ycombinator.com/item?id=42645218) and I wondered if I could get Doom to run using a similar method. It turns out that old versions of Emscripten can compile C to asm.js code that will happily run inside the limited JS runtime of the PDF engine. I used the doomgeneric (https://github.com/ozkl/doomgeneric) fork of the original Doom source, as that made writing the IO fairly easy. All I had to do was implement a framebuffer and keyboard inputs. Unlike previous interactive PDF demos, the output for DoomPDF is achieved by creating a text field for each row of pixels in the screen, then setting their contents to various ASCII characters. This gives me a 6 color monochrome display, that can be updated reasonably quickly (80ms per frame). The source code is available at: https://github.com/ading2210/doompdf Note that this PDF can only run in Chromium-based browsers that use the PDFium engine. ThomasRinsma 12 hours agoAuthor of \"PDF Tetris\" here. Great work! We had the same idea at the same time, here's my version of PDF Doom: Source: https://github.com/thomasRinsma/pdfdoom Playable here: https://th0mas.nl/downloads/doom.pdf Yours is neater in many ways though! reply OnionBlender 10 hours agoparent\"There was a problem with this document\". Is the problem me, or the document? reply wingi 9 hours agoparentprevThis is just awesome! reply pavo-etc 17 hours agoprev> limited JS runtime of the PDF engine humanity has gone too far reply miki123211 7 hours agoparentSeriously though, is there another format that: 1. Can be easily and freely shared by email / cloud drive, including assets, images and fonts. 2. Supports form filling and saving the form data in the file directly (as opposed to sending it somewhere over HTTP). Basically the electronic equivalent of a paper form that can be filled, send by email and stay filled. 3. Supports (cryptographic) signatures that are again part of the document, and can easily and securely be verified by end users. This is a very important use case in the EU, where electronic signatures are based on cryptography, not \"I pinky swear I'm John Smith\" DocuSign. 4. Has perfect print fidelity. We keep complaining about PDF (and rightly so), but there's truly no other format to replace it. The W3c / Whatwg / whatever could probably come up with one based on web technologies, but they haven't yet. There's Epub which solves a very narrow use case of PDF (electronic book distribution where perfect control over presentation is not required), but nothing that solves the \"business\" use cases. reply kragen 5 hours agorootparentAdding JS to PDF seriously undermines these benefits. If Turing-complete logic can draw arbitrary images on the document, you can no longer have any print fidelity at all, and what you signed cryptographically may have said things you didn't know it said. It may start interfering with #1 if email systems start blocking \"malicious\" PDF features, too. Only benefit #2 survives. I have no idea what the folks at Adobe were thinking when they decided to add this feature that could eventually eliminate most of the benefits of their product. None of this is to say that the Doom implementation is anything less than a very cool hack. reply knome 4 hours agorootparentprobably the same thing that netscape did when adding javascript to the web. \"now we can add some basic client-side validation to these forms\". PDFs can be used as form templates, so having some basic validation is reasonable. reply hardwaresofton 12 hours agoparentprevThat’s the only way we know how to go reply datavirtue 9 hours agorootparentThis. I'm eagerly awaiting the replicators that will explore the cosmos and spread the knowledge of our existence. If we can get them done before we poison ourselves. reply krunck 10 minutes agorootparentAnd they'll be able to run Doom too. reply alganet 6 hours agorootparentprevYou assume that we are the thing being replicated. Nature is crafty. It could be the case that we humans are the replicators, not the main show. reply ieidkeheb 9 hours agorootparentprevYou mean as long as they can run doom, or create a pocket universe that simulates doom. reply quotemstr 15 hours agoparentprev:-) I'll never quite appreciate why people say things like this. Having some kind of embedded scripting is useful for all sorts of things, often form validation. A sufficiently complex validation system becomes Turing complete, so you might as well skip the hassle of a custom language and go right to JavaScript. Once you have JavaScript, input, and some way of updating a graphical pixel grid, you're at Doom-completeness. I think it's a wonderful, not terrible, thing that computation and programmability are so cheap they've become ubiquitous even in the most mundane applications reply llm_trw 13 hours agorootparentWe had that language, it was postscript. Then pdf came along and said: no this is too dangerous the only thing in a document should be layout information not arbitrary code. And here we are two decades later. My hatred of pdf has no end. It killed postscript for dynamic pages and djvu for static pages. reply weinzierl 5 hours agorootparentThis is very misleading thinking. We've came a very long way from PS security-wise and this is a good thing and should be appreciated. The fallacy I see in many comments - either directly or between the lines - is to think that since we can run Doom in PDF, hell's gates must have opened and we can do literally anything, especially anything malicious. This is not the case. PDF is basically comprised of immutable parts and interactive elements that user agents are supposed to render visibly distinctly. Also user agents are not supposed to run any code without explicit user interaction. Contemporary user agents do a good job in both respects. PDFtris and the Doom example are possible because they live in a very small niche of features that enable relatively unobtrusive still interactive form processing. Forms allow code, but do not stick out as much as other interactive elements do and they are relatively flexible. Having found that feature niche is the real genius of PDFtris and related exploits. Still, they need user interaction. There is no way to do anything behind your back in PDF. Another fallacy I see in this and the related threads,is that Adobe Acrobat vulnerabilities are PDF vulnerabilities. Yes, Adobe did a terrible job with Acrobat, but in my opinion not at all with the format and specification of PDF - especially not when it comes to security. reply gorkish 2 hours agorootparentprev> My hatred of pdf has no end. It killed postscript for dynamic pages and djvu for static pages. Interesting to see someone evoke DjVu. With the exception of IW44 wavelet compression, basically everything the DjVu file format supports has a PDF equivalent. I built a tool to convert DjVu to PDF that preserves the image layers and file structure with nearly equivalent compression. My tool did expose some edge cases in the PDF standard which was frustrating. For instance, PDF supports applying a bitonal mask to an image, but it does not specify how to apply it if the two images have different resolution (DPI). It took many years to get Apple to bring their implementation into consistency. reply DiggyJohnson 13 hours agorootparentprevThis is a very concise explanation, thanks for putting it so clearly. It’s not the features or requirements that are the focus of the scorn, per se, but how we got here. I still prefer and use PDF all the time, but between overly dynamic crap and the mainstream tooling, well… “hate” is a reasonable hyperbole. reply llm_trw 11 hours agorootparentHate is too weak a term for what I feel for Adobe. Adobe kept PDF as a proprietary format from 1992 to 2008. You got the reader for free ... on windows, with a single executable. You didn't get an editor and had to pay through the nose for one from Adobe. It wasn't until the late 2010s that it actually became a free-ish standard, if you think that a 3,500 page document is a 'standard'. The only reason why adobe did it is because djvu was eating their lunch, between 2002 and 2008 it was the defacto standard for scanned documents in academia. The documents were easy to edit. The image compression is still better than the native compression on PDF. To add insult to injury after displacing postscript on windows in the name of security, not only did they add a scripting language to PDF, they added one written in two weeks at a time when it was so bad no one used it for anything but pop-ups and with more security vulnerabilities than you could shake a stick at. I suppose we should be happy Adobe didn't put flash in. Oh wait, they did: https://www.reddit.com/r/Adobe/comments/yqisho/flash_content... reply p_ing 14 hours agorootparentprevJS is what made these file types into the Pretty Dangerous Format. Numerous vulnerabilities in Adobe Acrobat surfaced thanks to the embedded JS engine. Updating the Acrobat client across an enterprise used to be quite burdensome. reply quotemstr 14 hours agorootparentThe flip side is that because the industry has converged on just a few embedded scripting systems (JS, Lua, etc.) we can concentrate our security hardening efforts on these few engines and benefit everyone. If PDF, like PostScript, were its own custom thing, it couldn't have been able to benefit from this hardening. In the end, JS was a fine choice. reply lolinder 14 hours agorootparentThe concern isn't that it was JS, the concern is that there's a scripting system inside of PDF at all. Why? What? Form validation is a lousy excuse because forms themselves were a bridge too far for the format. Why do we need to be able to validate them? I knew PDFs could be dangerous, but I didn't realize it was because they're intentionally designed to allow embedded scripts. reply danieldk 12 hours agorootparentI don't think forms are a bridge too far, it was very common that forms were provided as PDF and it is more convenient for the sender and receiver to fill the fields on a computer for readability, etc. before printing. However, forms could be handled by a very simple DSL that would be easy to write a safe interpreter for. reply quotemstr 12 hours agorootparentJavaScript is already a simple language. There's no requirement to have a JIT even. What makes you believe a custom language would be any safer or better in another way? reply luismedel 11 hours agoprevPandora's box has been opened. Next step: embed Bellard's JSLinux (https://bellard.org/jslinux/) and have a fullblown OS with development environment, office suite and all inside a PDF. reply khaledh 16 hours agoprevPortable Doom Format reply takeda 14 hours agoparentAs long as it is in Chrome reply ikari_pl 7 hours agorootparentoh so that why neither version worked for me in any reader reply Narishma 14 hours agoparentprevNot that portable since it only works on a single PDF engine. reply antics 16 hours agoprevOne of my formative experiences as a freshman in CS (I learned to program in college) was accidentally opening a PDF with Emacs and watching as it displayed not weird binary data but a real, rendered PDF. I wondered what else it was doing behind my back that I didn't know about. Sadly, I was not able to run Doom in a PDF, in Emacs. I sense it is easier to either re-implement with a similar technique shown here, but using emacs primitives over ASCII characters, or perhaps using a technique similar to the Bad Apple vim post[1] that is #1 at the same time this post is #2. [1]: https://news.ycombinator.com/item?id=42674116 reply nomilk 16 hours agoprevClick in the area that says 'type here for keyboard controls'. Press z several times to start w, a, s, d to move, e to use, space to shoot. z is enter reply anilakar 14 hours agoprevBoth Doom and Bad Apple in top four articles on the HN front page. This week is off to a good start. reply joelvalleroy 16 hours agoprevCool! Next up, PDF reader that runs in Doom. reply mdaniel 15 hours agoparentThat's kind of cheating given how many RCEs there are in the thing. It'd end up looking like /XObject > /Invoke RCE reply em3rgent0rdr 15 hours agoparentprevPDF readers and Doom all the way down. reply cloudking 15 hours agorootparentWe must go deeper. reply MarekKnapek 16 hours agoprevAs PDF supports DEFLATE compression, it should be possible to shrink the size of the PDF document considerably. reply PierCecco 7 hours agoprevThat's Super Awesome, I know, this is a dumb comment, but, come on!!! reply yoz 11 hours agoprevThis is amazing, but there are even wilder ways to run arbitrary code inside a PDF. How about stringing together several thousand segment commands in JBIG2 (one of the image codecs supported in PDF) to create a programmable virtual machine? https://googleprojectzero.blogspot.com/2021/12/a-deep-dive-i... reply brunorsini 10 hours agoprevNext up: Acrobat in a PDF! reply nmeta 11 hours agoprevam I the only dummy missing an instruction? the game takes off w/o my input, moving and blasting away. surely I'm just OOTL w/ PDF gameplay, which I blame myself for reply jansan 9 hours agoparentKlick into that textbox at the right in the lower part of the page. Then type (WSAD, Z for enter, just as written there) reply RajT88 14 hours agoprevYou monster. reply einpoklum 10 hours agoprevThis is not Doom in a PDF, this is Doom in Chromium which uses a hack with its PDF import engine. reply experian21e 11 hours agoprevWow, I love how doom has become the run it everywhere possible game! reply LorenzoGood 13 hours agoprevBiggest one up in history. reply initramfs 12 hours agoprevDoom, the PDF Movie. In Theatres, Near You reply anthk 8 hours agoprevAlso, if any, this looks why the current industry sucks, putting little and shitty languages everywhere making PDF files very dangerous. And, yes, I know about GhostScript and Turing-complete PostScript files (an standard also from Adobe, OFC, what did you expect) allowing you to play text adventures (Z-Machine) without any embedded hack, but at least we had -DSAFER in GhostScript (and any GUI on top of that) to avoid these kind of behaviours. Inb4 \"this is the true hacker spirit\", I know, yes, this is cool stuff and the true meaning of a hacker, but in the end I'd choose DJVU for a document format. reply Uptrenda 8 hours agoprevSo I find this neat, I can see a potential practical application as being able to demonstrate a piece of engineer work INSIDE a resume when you apply for a job which I think is really creative. But do you all think there are other use-cases for this technology? Like, could you distribute apps using PDFs on highly constrained devices (like iphone possibly, or maybe managed devices e.g. play station, xbox, kiosks?) Just throwing out ideas. Are there other obvious uses for this? I think when I was playing around with adobe reader I saw you could put movies in them, too. I believe that you're able to make customization's to the menu bar. It seems to be fairly flexible for what it is. reply anthk 8 hours agoprevStop restricting yourself to a shareware episode. Use the full legal replacement from https://freedoom.github.io reply vk6 34 minutes agoparentYou actually can use FreeDoom if you want by loading it as a custom IWAD. If you visit the site's landing page (https://doompdf.pages.dev/) you can upload the IWAD file, and then it'll generate a new PDF file (that can even be saved and redistributed). However, I chose the shareware version since the file size is a lot smaller and it's more recognizable to people. reply peterburkimsher 17 hours agoprevIs the WAD file open-source as a PDF attachment now? reply crtasm 15 hours agoparentThere's no WAD in the repo, I assume the linked PDF contains the shareware episode. reply armSixtyFour 15 hours agorootparentThe build wgets the wad from elsewhere. reply crtasm 15 hours agorootparentThanks, yes it is the shareware episode. f0cefca49926d00903cf57551d901abe doom1.wad reply revskill 9 hours agoprevHow can you find the spare time and the focus to finish this ? Why ????? reply remoquete 12 hours agoprevNow, if only I could type IDDQD to print protected PDF files. reply prmoustache 10 hours agoprevNow can we do a pdfbomb with a pdf embedding its pdf renderer recursively loading itself? reply hoc 11 hours agoprevNow: What do other PDFs do while not outputting anything... reply armSixtyFour 15 hours agoprev [–] Now how do I add another WAD file to this. Someone needs to play sigil on this. reply vk6 12 hours agoparent [–] I just added this as a feature. You can launch the game with custom WADs at the site's landing page (https://doompdf.pages.dev/). It'll open a new PDF file as a blob URL (and you can even save and redistribute the PDF it generates). A disclaimer though - I don't have any experience with Doom modding. I don't know if the behavior of this feature is correct. All it does is it loads the PWAD by passing the \"-file\" argument to the game's main function. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A developer has created a Doom source port that operates within a PDF file, inspired by a similar Tetris project. - The project uses an older version of Emscripten to compile C code to asm.js, allowing it to run in the PDF's JavaScript runtime. - The implementation includes a 6-color monochrome display and is compatible only with Chromium-based browsers using the PDFium engine."
    ],
    "commentSummary": [
      "A developer has created a Doom source port that operates within a PDF file, following a similar project that ran Tetris in a PDF. - The project uses Emscripten to compile C to asm.js, allowing it to run in the PDF's limited JavaScript runtime, and employs the doomgeneric fork of the original Doom source. - This innovation has sparked discussions about the potential and security implications of executing code within PDFs, with the source code available on GitHub and functionality limited to Chromium-based browsers using the PDFium engine."
    ],
    "points": 311,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1736729443
  },
  {
    "id": 42682602,
    "title": "Debugging: Indispensable rules for finding even the most elusive problems (2004)",
    "originLink": "https://dwheeler.com/essays/debugging-agans.html",
    "originBody": "David A. Wheeler's Review of Debugging by David J. Agans March 2, 2004 It's not often you find a classic, but I think I've found a new classic for software and computer hardware developers. It's David J. Agan's Debugging: The 9 Indispensable Rules for Finding Even the Most Elusive Software and Hardware Problems (ISBN 0814471684). This book explains the fundamentals of finding and fixing bugs (once a bug has been detected), rather than any particular technology. It's best for developers who are novices or who are only moderately experienced, but even old pros will find helpful reminders of things they know they should do but forget in the rush of the moment. This book will help you fix those inevitable bugs, particularly if you're not a pro at debugging. It's hard to bottle experience; this book does a good job. This is a book I expect to find useful many, many, years from now. The entire book revolves around the \"nine rules.\" After the typical introduction and list of the rules, there's one chapter for each rule. Each of these chapters describes the rule, explains why it's a rule, and includes several \"sub-rules\" that explain how to apply the rule. Most importantly, there are lots of \"war stories\" that are both fun to read and good illustrations of how to put the rule into practice. Since the whole book revolves around the nine rules, it might help to understand the book by skimming the rules and their sub-rules: Understand the system: Read the manual, read everything in depth, know the fundamentals, know the road map, understand your tools, and look up the details. Make it fail: Do it again, start at the beginning, stimulate the failure, don't simulate the failure, find the uncontrolled condition that makes it intermittent, record everything and find the signature of intermittent bugs, don't trust statistics too much, know that \"that\" can happen, and never throw away a debugging tool. Quit thinking and look (get data first, don't just do complicated repairs based on guessing): See the failure, see the details, build instrumentation in, add instrumentation on, don't be afraid to dive in, watch out for Heisenberg, and guess only to focus the search. Divide and conquer: Narrow the search with successive approximation, get the range, determine which side of the bug you're on, use easy-to-spot test patterns, start with the bad, fix the bugs you know about, and fix the noise first. Change one thing at a time: Isolate the key factor, grab the brass bar with both hands (understand what's wrong before fixing), change one test at a time, compare it with a good one, and determine what you changed since the last time it worked. Keep an audit trail: Write down what you did in what order and what happened as a result, understand that any detail could be the important one, correlate events, understand that audit trails for design are also good for testing, and write it down! Check the plug: Question your assumptions, start at the beginning, and test the tool. Get a fresh view: Ask for fresh insights (just explaining the problem to a mannequin may help!), tap expertise, listen to the voice of experience, know that help is all around you, don't be proud, report symptoms (not theories), and realize that you don't have to be sure. If you didn't fix it, it ain't fixed: Check that it's really fixed, check that it's really your fix that fixed it, know that it never just goes away by itself, fix the cause, and fix the process. This list by itself looks dry, but the detailed explanations and war stories make the entire book come alive. Many of the war stories jump deeply into technical details; some might find the details overwhelming, but I found that they were excellent in helping the principles come alive in a practical way. Many war stories were about obsolete technology, but since the principle is the point, that isn't a problem. Not all the war stories are about computing; there's a funny story involving house wiring, for example. But if you don't know anything about computer hardware and software, you won't be able to follow many of the examples. After detailed explanations of the rules, the rest of the book has a single story showing all the rules in action, a set of \"easy exercises for the reader\", tips for help desks, and closing remarks. There are lots of good points here. One that particularly stands out is \"quit thinking and look.\" Too many try to \"fix\" things based on a guess instead of gathering and observing data to prove or disprove a hypothesis. Another principle that stands out is \"if you didn't fix it, it ain't fixed;\" there are several vendors I'd like to give that advice to. The whole \"stimulate the failure, don't simulate the failure\" discussion is not as clearly explained as most of the book, but it's a valid point worth understanding. I particularly appreciated Agans' discussions on intermittent problems (particularly in \"Make it Fail\"). Intermittent problems are usually the hardest to deal with, and the author gives straightforward advice on how to deal with them. One odd thing is that although he mentions Heisenberg, he never mentions the term \"Heisenbug\", a common jargon term in software development (a Heisenbug is a bug that disappears or alters its behavior when one attempts to probe or isolate it). At least a note would've been appropriate. The back cover includes a number of endorsements, including one from somebody named Rob Malda. But don't worry, the book's good anyway :-). It's important to note that this is a book on debugging fundamentals, and different than most other books related to debugging. There are many other books on debugging, such as Richard Stallman et al's Debugging with GDB: The GNU Source-Level Debugger. But these other texts usually concentrate primarily on a specific technology and/or on explaining tool commands, not on timeless debugging principles. A few (like Norman Matloff's Guide to Faster, Less Frustrating Debugging) have a few general suggestions about debugging, but are nothing like Agans' book. There are many books on testing, like Boris Beizer's Software Testing Techniques, but they tend to emphasize how to create tests to detect bugs, and less on how to fix a bug once it's been detected. Of course, once you find a bug, you should add a test for that bug in your regression test suite, but testing (including regression testing) is outside the scope of Agans' book. Agans' book concentrates on the big picture for debugging; these other books are complementary to it. Debugging has an accompanying website at http://www.debuggingrules.com, where you can find various little extras and links to related information. In particular, the website has an amusing poster of the nine rules you can download and print. No book's perfect, so here are my gripes and wishes: The sub-rules are really important for understanding the rules, but there's no \"master list\" in the book or website that shows all the rules and sub-rules on one page. The end of the chapter about a given rule summarizes the sub-rules for that one rule, but it'd sure be easier to have them all in one place. So, print out the list of sub-rules above after you've read the book. The book left me wishing for more detailed suggestions about specific common technology. This is probably unfair, since the author is trying to give timeless advice rather than a \"how to use tool X\" tutorial. But it'd be very useful to give good general advice, specific suggestions, and examples of what approaches to take for common types of tools (like symbolic debuggers, digital logic probes, etc.), specific widely-used tools (like ddd on gdb), and common problems. Even after the specific tools are gone, such advice can help you use later ones. A little of this is hinted at in the \"know your tools\" section, but I'd like to have seen much more of it. Vendors often crow about what their tools can do, but rarely explain their weaknesses or how to apply them in a broader context. There's probably a need for another book that takes the same rules, but broadens them to solving arbitrary problems. Frankly, the rules apply to many situations beyond computing, but the war stories are far too technical for the non-computer person to understand. But as you can tell, I think this is a great book. In some sense, what it says is \"obvious,\" but it's only obvious as all fundamentals are obvious. Many sports teams know the fundamentals, but fail to consistently apply them - and fail because of it. Novices need to learn the fundamentals, and pros need occasional reminders of them; this book is a good way to learn or be reminded of them. Get this book. Slashdot posted an earlier version of this review on February 24, 2004. David A. Wheeler is an expert on developing secure programs and quantitative analysis of open source software / Free Software. He lives in Northern Virginia.",
    "commentLink": "https://news.ycombinator.com/item?id=42682602",
    "commentBody": "Debugging: Indispensable rules for finding even the most elusive problems (2004) (dwheeler.com)302 points by omkar-foss 6 hours agohidepastfavorite139 comments GuB-42 5 hours agoRule 0: Don't panic Really, that's important. You need to think clearly, deadlines and angry customers are a distraction. That's also when having a good manager who can trust you is important, his job is to shield you from all that so that you can devote all of your attention to solving the problem. reply augbog 40 minutes agoparent100% agree. I remember I had an on-call and our pagerduty started going off for a SEV-2 and naturally a lot of managers from teams that are affected are in there sweating bullets because their products/features/metrics are impacted. It can get pretty frustrating having so many people try to be cooks in the kitchen. We had a great manager who literally just moved us to a different call/meeting and he told us \"ignore everything those people are saying; just stay focused and I'll handle them.\" Everyone's respect for our manager really went up from there. reply Cerium 4 hours agoparentprevSlow is smooth and smooth is fast. If you don't have time to do it right, what makes you think there is time to do it twice? reply sitkack 1 hour agorootparent\"We have to do something!\" reply Bootvis 20 minutes agorootparentAnd this is something, so I’m doing it. reply ianmcgowan 2 hours agoparentprevThere's a story in the book - on nuclear submarines there's a brass bar in front of all the dials and knobs, and the engineers are trained to \"grab the bar\" when something goes wrong rather than jumping right to twiddling knobs to see what happens. reply toolslive 1 hour agorootparent\"a good chess player sits on his hands\" (NN). It's good advice as it prevents you from playing an impulsive move. reply stronglikedan 1 hour agorootparentI have to sit on my hands at the dentist to prevent impulse moves. reply throwawayfks 1 hour agorootparentprevI read this book and took this advice to heart. I don't have a brass bar in the office, but when I'm about to push a button that could cause destructive changes, especially in prod, my hands reflexively fly up into the air while I double-check everything. reply tetha 1 hour agorootparentA weird, yet effective recommendation from someone at my last job: If it's a destructive or dangerous action in prod, touch both your elbows first. This forces ou to take the hands away from the keyboard, stop any possible auto-pilot and look what you're doing. reply adamc 4 hours agoparentprevI had a boss who used to say that her job was to be a crap umbrella, so that the engineers under her could focus on their actual jobs. reply airblade 2 hours agorootparentAt first I thought you meant an umbrella that doesn't work very well. reply dazzawazza 3 hours agorootparentprevIdeally it's crap umbrellas all the way down. Everyone should be shielding everyone below them from the crap slithering its way down. reply bilekas 47 minutes agoparentprevThis is very underrated. Also an extension to this is don’t be afraid to break things further to probe. I often see a lot of devs mid level included panicking and thus preventing them to even know where to start. I’ve come to believe that some people just have an inherent intuition and some just need to learn it. reply jimmySixDOF 29 minutes agorootparentYes its sometimes instinct takes over when your on the spot in a pinch but there are institutional things you can do to be prepared in advance that expand your set of options in the moment much like a pre-prepared firedrill playbook you can pull from also there are training courses like Kepner-Tregoe but you are right there are just some people who do better than others when _it's hitting the fan. reply CobrastanJorji 14 minutes agoparentprevI once worked for a team that, when a serious visible incident occurred, a company VP would pace the floor, occasionally yelling, describing how much money we were losing per second (or how much customer trust if that number was too low) or otherwise communicating that we were in a battlefield situation and things were Very Critical. Later I worked for a company with a much bigger and more critical website, and the difference in tone during urgent incidents was amazing. The management made itself available for escalations and took a role in externally communicating what was going on, but besides that they just trusted us to do our jobs. We could even go get a glass of water during the incident without a VP yelling at us. I hadn't realized until that point that being calm adults was an option. reply chrsig 2 hours agoparentprevAlso a pager/phone going off incessantly isn't useful either. manage your alarms or you'll be throwing your phone at a wall. reply ahci8e 2 hours agoparentprevUff, yeah. I used to work with a guy who would immediately turn the panic up to 11 at the first thought of a bug in prod. We would end up with worse architecture after his \"fix\" or he would end up breaking something else. reply hughdbrown 16 minutes agoprevIn my experience, the most pernicious temptation is to take the buggy, non-working code you have now and to try to modify it with \"fixes\" until the code works. In my experience, you often cannot get broken code to become working code because there are too many possible changes to make. In my view, it is much easier to break working code than it is to fix broken code. Suppose you have a complete chain of N Christmas lights and they do not work when turned on. The temptation is to go through all the lights and to substitute in a single working light until you identify the non-working light. But suppose there are multiple non-working lights? You'll never find the error with this approach. Instead, you need to start with the minimal working approach -- possibly just a single light (if your Christmas lights work that way), adding more lights until you hit an error. In fact, the best case is if you have a broken string of lights and a similar but working string of lights! Then you can easily swap a test bulb out of the broken string and into the working chain until you find all the bad bulbs in the broken string. Starting with a minimal working example is the best way to fix a bug I have found. And you will find you resist this because you believe that you are close and it is too time-consuming to start from scratch. In practice, it tends to be a real time-saver, not the opposite. reply nickjj 5 hours agoprevFor #4 (divide and conquer), I've found `git bisect` helps a lot. If you have a known good commit and one of dozens or hundreds of commits after that is bad, this can help you identify the bad commit / code in a few steps. Here's a walk through on using it: https://nickjanetakis.com/blog/using-git-bisect-to-help-find... I jumped into a pretty big unknown code base in a live consulting call and we found the problem pretty quickly using this method. Without that, the scope of where things could be broken was too big given the context (unfamiliar code base, multiple people working on it, only able to chat with 1 developer on the project, etc.). reply jerf 4 hours agoparent\"git bisect\" is why I maintain the discipline that all commits to the \"real\" branch, however you define that term, should all individually build and pass all (known-at-the-time) tests and generally be deployable in the sense that they would \"work\" to the best of your knowledge, even if you do not actually want to deploy that literal release. I use this as my #1 principle, above \"I should be able to see every keystroke ever written\" or \"I want every last 'Fixes.' commit\" that is sometimes advocated for here, because those principles make bisect useless. The thing is, I don't even bisect that often... the discipline necessary to maintain that in your source code heavily overlaps with the disciplines to prevent code regression and bugs in the first place, but when I do finally use it, it can pay for itself in literally one shot once a year, because we get bisect out for the biggest, most mysterious bugs, the ones that I know from experience can involve devs staring at code for potentially weeks, and while I'm yet to have a bisect that points at a one-line commit, I've definitely had it hand us multiple-day's-worth of clue in one shot. If I was maintaining that discipline just for bisect we might quibble with the cost/benefits, but since there's a lot of other reasons to maintain that discipline anyhow, it's a big win for those sorts of disciplines. reply aag 32 minutes agorootparentSometimes you'll find a repo where that isn't true. Fortunately, git bisect has a way to deal with failed builds, etc: three-value logic. The test program that git bisect runs can return an exit value that means that the failure didn't happen, a different value that means that it did, or a third that means that it neither failed nor succeeded. I wrote up an example here: https://speechcode.com/blog/git-bisect reply SoftTalker 1 hour agorootparentprevI do bisecting almost as a last resort. I've used it when all else fails only a few times. Especially as I've never worked on code where it was very easy to just build and deploy a working debug system from a random past commit. Edit to add: I will study old diffs when there is a bug, particularly for bugs that seem correlated with a new release. Asking \"what has changed since this used to work?\" often leads to an obvious cause or at least helps narrow where to look. Also asking the person who made those changes for help looking at the bug can be useful, as the code may be more fresh in their mind than in yours. reply skydhash 3 hours agorootparentprevSame. Every branch apart from the “real” one and release snapshots is transient and WIP. They don’t get merged back unless tests pass. reply forrestthewoods 2 hours agorootparentprev> why I maintain the discipline that all commits to the \"real\" branch, however you define that term, should all individually build and pass all (known-at-the-time) tests and generally be deployable in the sense that they would \"work\" to the best of your knowledge, even if you do not actually want to deploy that literal release You’re spot on. However it’s clearly a missing feature that Git/Mercurial can’t tag diffs as “passes” or “bisectsble”. This is especially annoying when you want to merge a stack of commits and the top passes all tests but the middle does not. It’s a monumental and valueless waste of time to fix the middle of the stack. But it’s required if you want to maintain bisectability. It’s very annoying and wasteful. :( reply michalsustr 1 hour agorootparentThis is why we use squash like here https://docs.gitlab.com/ee/user/project/merge_requests/squas... reply snowfarthing 7 minutes agorootparentAs someone who doesn't like to see history lost via \"rebase\" and \"squashing\" branches, I have had to think through some of these things, since my personal preferences are often trampled on by company policy. I have only been in one place where \"rebase\" is used regularly, and now that I'm a little more familiar with it, I don't mind using it to bring in changes from a parent branch into a working branch, if the working branch hasn't been pushed to origin. It still weirds me out somewhat, and I don't see why a simple merge can't just be the preferred way. - I have, however, seen \"squashing\" regularly (and my current position uses it as well as rebasing) -- and I don't particularly like it, because sometimes I put in notes and trials that get \"lost\" as the task progresses, but nonetheless might be helpful for future work. While it's often standard to delete \"squashed\" branches, I cannot help but think that, for history-minded folks like me, a good compromise would be to \"squash and keep\" -- so that the individual commits don't pollute the parent branch, while the details are kept around for anyone needing to review them. Having said that, I've never been in a position where I felt like I need to \"forcibly\" push for my preferences. I just figure I might as well just \"go with the flow\", even if a tiny bit of me dies every time I squash or rebase something, or delete a branch upon merging! reply forrestthewoods 42 minutes agorootparentprevI explicitly don’t want squash. The commits are still worth keeping separate. There’s lots of distinct pieces of work. But sometimes you break something and fix it later. Or you add something new but support different environments/platforms later. reply Izkata 1 hour agorootparentprevIf there's a way to identify those incomplete commits, git bisect does support \"skip\" - a commit that's neither good nor bad, just ignored. reply smcameron 2 hours agoparentprevBack in the 1990s, while debugging some network configuration issue a wiser older colleague taught me the more general concept that lies behind git bisect, which is \"compare the broken system to a working system and systematically eliminate differences to find the fault.\" This can apply to things other than software or computer hardware. Back in the 90s my friend and I had identical jet-skis on a trailer we shared. When working on one of them, it was nice to have its twin right there to compare it to. reply rozap 18 minutes agoparentprevBinary search rules. Being systematic about dividing the problem in half, determining which half the issue is in, and then repeating applies to non software problems quite well. I use the strategy all the time while troubleshooting issue with cars, etc. reply tetha 1 hour agoparentprevYou can also use divide and conquer when dealing with a complex system. Like, traffic going from A to B can turn ... complicated with VPNs and such. You kinda have source firewalls, source routing, connectivity of the source to a router, routing on the router, firewalls on the router, various VPN configs that can go wrong, and all of that on the destination side as well. There can easily be 15+ things that can cause the traffic to disappear. That's why our runbook recommends to start troubleshooting by dumping traffic on the VPN nodes. That's a very low-effort, quick step to figure out on which of the six-ish legs of the journey drops traffic - to VPN, through VPN, to destination, back to VPN node, back through VPN, back to source. Then you realize traffic back to VPN node disappears and you can dig into that. And this is a powerful concept to think through in system troubleshooting: Can I understand my system as a number of connected tubes, so that I have a simple, low-effort way to pinpoint one tube to look further into? As another example, for many services, the answer here is to look at the requests on the loadbalancer. This quickly isolates which services are throwing errors blowing up requests, so you can start looking at those. Or, system metrics can help - which services / servers are burning CPU and thus do something, and which aren't? Does that pattern make sense? Sometimes this can tell you what step in a pipeline of steps on different systems fails. reply Icathian 5 hours agoparentprevTacking on my own article about git bisect run. It really is an amazing little tool. https://andrewrepp.com/git_bisect_run reply epolanski 1 hour agoparentprevBisection is also useful when debugging css. When you don't know what is breaking that specific scroll or layout somewhere in the page, you can just remove half the DOM in the dev tools and check if the problem is still there. Rinse and repeat, it's a basic binary search. I am often surprised that leetcode black belts are absolutely unable to apply what they learn in the real world, neither in code nor debugging which always reminds me of what a useless metric to hire engineers it is. reply jvans 4 hours agoparentprevgit bisect is an absolute power feature everybody should be aware of. I use it maybe once or twice a year at most but it's the difference between fixing a bug in an hour vs spending days or weeks spinning your wheels reply ajross 3 hours agoparentprevNot to complain about bisect, which is great. But IMHO it's really important to distinguish the philosophy and mindspace aspect to this book (the \"rules\") from the practical advice (\"tools\"). Someone who thinks about a problem via \"which tool do I want\" (c.f. \"git bisect helps a lot\"[1]) is going to be at a huge disadvantage to someone else coming at the same decisions via \"didn't this used to work?\"[2] The world is filled to the brim with tools. Trying to file away all the tools in your head just leads to madness. Embrace philosophy first. [1] Also things like \"use a time travel debugger\", \"enable logging\", etc... [2] e.g. \"This state is illegal, where did it go wrong?\", \"What command are we trying to process here?\" reply nottorp 2 hours agorootparentJust be careful to not contradict #3 “Quit thinking and look”. reply ajross 1 hour agorootparentTouché reply qwertox 6 hours agoprevMake sure you're editing the correct file on the correct machine. reply ZedZark 5 hours agoparentYep, this is a variation of \"check the plug\" I find myself doing this all the time now I will temporarily add a line to cause a fatal error, to check that it's the right file (and, depending on the situation, also the right line) reply overhead4075 3 hours agorootparentThis is also covered by \"make it fail\" reply shmoogy 4 hours agorootparentprevI'm glad I'm not the only one doing this after I wasted too much time trying to figure out why my docker build was not reflecting the changes ... never again.. reply netcraft 58 minutes agoparentprevthe biggest thing I've always told myself and anyone ive taught: make sure youre running the code you think youre running. reply eddd-ddde 5 hours agoparentprevHow much time I've wasted unknowingly editing generated files, out of version files, forgetting to save, ... only god knows. reply ajuc 4 hours agoparentprevThat's why you make it break differently first. To see your changes have any effect. reply chupasaurus 5 hours agoparentprevPoor Yorick! reply reverendsteveii 4 hours agoparentprevvery first order of business: git stash && git checkout main && git pull reply spacebanana7 5 hours agoparentprevAlso that it's in the correct folder reply n144q 2 hours agoparentprev...and you are building and running the correct clone of a repository reply heikkilevanto 5 hours agoprevSome additional rules: - \"It is your own fault\". Always suspect your code changes before anything else. It can be a compiler bug or even a hardware error, but those are very rare. - \"When you find a bug, go back hunt down its family and friends\". Think where else the same kind of thing could have happened, and check those. - \"Optimize for the user first, the maintenance programmer second, and last if at all for the computer\". reply bsammon 2 minutes agoparentAlternatively, I've found the \"Maybe it's a bug. I'll try an make a test case I can report on the mailing list\" approach useful at times. Usually, in the process of reducing my error-generating code down to a simpler case, I find the bug in my logic. I've been fortunate that heisenbugs have been rare. Once or twice, I have ended up with something to report to the devs. Generally, those were libraries (probably from sourceforge/github) with only a few hundred or less users that did not get a lot of testing. reply physicles 46 minutes agoparentprevThe first one is known in the Pragmatic Programmer as “select isn’t broken.” Summarized at https://blog.codinghorror.com/the-first-rule-of-programming-... reply wormlord 3 hours agoparentprevI always have the mindset of \"its my fault\". My Linux workstation constantly crashing because of the i9-13900k in it was honestly humiliating. Was very relieved when I realized it was the CPU and not some impossible to find code error. reply dehrmann 1 hour agorootparentLinux is like an abusive relationship in that way--it's always your fault. reply ajuc 4 hours agoparentprevIt's healthier to assume your code is wrong than otherwise. But it's best to simply bisect the cause-effect chain a few more times and be sure. reply jwpapi 1 hour agoprevI’m not sure that doesn’t sit well with me. Rule 1 should be: Reproduce with most minimal setup. 99% you’ll already have found the bug. 1% for me was a font that couldn’t do a combination of letters in a row. life ft, just didn’t work and thats why it made mistakes in the PDF. No way I could’ve ever known that if I wouldn’t have reproduced it down to the letter. Just split code in half till you find what’s the exact part that goes wrong. reply physicles 49 minutes agoparentRelated: decrease your iterating time as much as possible. If you can test your fix in 30 seconds vs 5 minutes, you’ll fix it in hours instead of days. reply david_draco 6 hours agoprevStep 10, add the bug as a test to the CI to prevent regressions? Make sure the CI fails before the fix and works after the fix. reply Tade0 5 hours agoparentThe largest purely JavaScript repo I ever worked on (150k LoC) had this rule and it was a life saver, particularly because the project had commits dating back more than five years and since it was a component/library, it had quite few strange hacks for IE. reply seanwilson 5 hours agoparentprevI don't think this is always worth it. Some tests can be time consuming or complex to write, have to be maintained, and we accept that a test suite won't be testing all edge cases anyway. A bug that made it to production can mean that particular bug might happen again, but it could be a silly mistake and no more likely to happen again than 100s of other potential silly mistakes. It depends, and writing tests isn't free. reply n144q 8 minutes agorootparentYou (or other people) will thank yourself in a few months/years when refactoring the code, knowing that they don't need to worry about missing edge cases, because all known edge cases are covered with these non regression tests. reply Ragnarork 4 hours agorootparentprevWriting tests isn't free but writing non-regression tests for bugs that were actually fixed is one of the best test cases to consider writing right away, before the bug is fixed. You'll be reproducing the bug anyway (so already consider how to reproduce). You'll also have the most information about it to make sure the test is well written anyway, after building a mental model around the bug. Writing tests isn't free, I agree, but in this case a good chunk of the cost of writing them will have already been paid in a way. reply seanwilson 2 hours agorootparent> Writing tests isn't free, I agree, but in this case a good chunk of the cost of writing them will have already been paid in a way. Some examples that come to mind are bugs to do with UI interactions, visuals/styling, external online APIs/services, gaming/simulation stuff, and asynchronous/thread code, where it can be a substantial effort to write tests for, vs fixing the bug that might just be a typo. This is really different compared to if you're testing some pure functions that only need a few inputs. It depends on what domain you're working in, but I find people very rarely mention how much work certain kinds of test can be to write, especially if there aren't similar tests written already and you have to do a ton of setup like mocking, factories, and UI automation. reply Ragnarork 1 hour agorootparentDefinitely agree with you on the fact that there are tests which are complicated to write and will take effort. But I think all other things considered my impression still holds, and that I should maybe rather say they're easier to write in a way, though not necessarily easy. reply jerf 4 hours agorootparentprevFor people who aren't getting the value of unit tests, this is my intro to the idea. You had to do some sort of testing on your code. At its core, the concept of unit testing is just, what if instead of throwing away that code, you kept it? To the extent that other concerns get in the way of the concept, like the general difficulty of testing that GUIs do what they are supposed to do, I don't blame the concept of unit testing; I blame the techs that make the testing hard. reply n144q 4 minutes agorootparentI can't count how many times when other people ask me \"how can I use this API?\", I just send a test case to them. Best example you can give to someone that is never out of sync. reply Ragnarork 3 hours agorootparentprevI also think that this is a great way to emphasis their value. If anything I'd only keep those if it's hard to write them, if people push back against it (and I myself don't like them sometimes, e.g. when the goal is just to push up the coverage metric but without actually testing much, which only add test code to maintain but no real testing value...). Like any topic there's no universal truth and lots of ways to waste time and effort, but this specifically is extremely practical and useful in a very explicit manner: just fix it once and catch it the next time before production. Massively reduce the chance one thing has to be fixed twice or more. reply soco 5 hours agoparentprevWhat do you do with the years old bug fixes? How fast can one run the CI after a long while of accumulating tests? Do they still make sense to be kept in the long run? reply simmonmt 5 hours agorootparentThis is a great problem to have, if (IME) rare. Step 1 Understand the System helps you figure out when tests can be eliminated as no longer relevant and/or which tests can be merged. reply hsbauauvhabzb 5 hours agorootparentprevI think for some types of bugs a CI test would be valuable if the developer believes regressions may occur, for other bugs they would be useless. reply hobs 5 hours agorootparentprevWhy would you want to stop knowing that your old bug fixes still worked in the context of your system? Saying \"oh its been good for awhile now\" has nothing to do with breaking it in the future. reply jerf 4 hours agorootparentprevI'm not particularly passionate about arguing the exact details of \"unit\" versus \"integration\" testing, let alone breaking down the granularity beyond that as some do, but I am passionate that they need to be fast, and this is why. By that, I mean, it is a perfectly viable use of engineering time to make changes that deliberately make running the tests faster. A lot of slow tests are slow because nobody has even tried to speed them up. They just wrote something that worked, probably literally years ago, that does something horrible like fully build a docker container and fully initialize a complicated database and fully do an install of the system and starts processes for everything and liberally uses \"sleep\"-based concurrency control and so on and so forth, which was fine when you were doing that 5 times but becomes a problem when you're trying to run it hundreds of times, and that's a problem, because we really ought to be running it hundreds of thousands or millions of times. I would love to work on a project where we had so many well-optimized automated tests that despite their speed they were still a problem for building. I'm sure there's a few out there, but I doubt it's many. reply nonrandomstring 5 hours agoparentprevYes, just more generally document it I've lost count of how many things i've fixed only to to see; 1) It recurs because a deeper \"cause\" of the bug reactivated it. 2) Nobody knew I fixed something so everyone continued to operate workarounds as if the bug was still there. I realise these are related and arguably already fall under \"You didn't fix it\". That said a bit of writing-up and root-cause analysis after getting to \"It's fixed!\" seems helpful to others. reply astrobe_ 51 minutes agoprevAlso sometimes: the bug is not in the code, its in the data. A few times I looked for a bug like \"something is not happening when it should\" or \"This is not the expected result\", when the issue was with some config file, database records, or thing sent by a server. For instance, particularly nasty are non-printable characters in text files that you don't see when you open the file. \"simulate the failure\" is sometimes useful, actually. Ask yourself \"how would I implement this behavior\", maybe even do it. Also: never reason on the absence of a specific log line. The logs can be wrong (bugged) too, sometimes. If you printf-debugging a problem around a conditional for instance, log both branches. reply gnufx 1 hour agoprevThen, after successful debugging your job isn't finished. The outline of \"Three Questions About Each Bug You Find\"is: 1. Is this mistake somewhere else also? 2. What next bug is hidden behind this one? 3. What should I do to prevent bugs like this? reply Zolomon 2 hours agoprevI have been bitten more than once thinking that my initial assumption was correct, diving deeper and deeper - only to realize I had to ascend and look outside of the rabbit hole to find the actual issue. > Assumption is the mother of all screwups. reply astrobe_ 45 minutes agoparentI've once heard from an RTS game caster (IIRC it was Day9 about Starcraft) \"Assuming... Is killing you\". reply sitkack 1 hour agoparentprevThis is how I view debugging, aligning my mental model with how the system actually works. Assumptions are bugs in the mental model. The problem is conflating what is knowledge with what is an assumption. reply BWStearns 2 hours agoprev> Check the plug I just spent a whole day trying to figure out what was going on with a radio. Turns out I had tx/rx swapped. When I went to check tx/rx alignment I misread the documentation in the same way as the first. So, I would even add \"try switching things anyways\" to the list. If you have solid (but wrong) reasoning for why you did something then you won't see the error later even if it's right in front of you. reply SoftTalker 1 hour agoparentYes the human brain can really be blind when its a priori assumptions turn out to be wrong. reply ianmcgowan 2 hours agoprevI used to manage a team that supported an online banking platform and gave a copy of this book to each new team member. If nothing else, it helped create a shared vocabulary. It's useful to get the poster and make sure everyone knows the rules. https://debuggingrules.com/download-the-poster/ reply fn-mote 5 hours agoprevThe article is a 2024 \"review\" (really more of a very brief summary) of a 2002 book about debugging. The list is fun for us to look at because it is so familiar. The enticement to read the book is the stories it contains. Plus the hope that it will make our juniors more capable of handling complex situations that require meticulous care... The discussion on the article looks nice but the submitted title breaks the HN rule about numbering (IMO). It's a catchy take on the post anyway. I doubt I would have looked at a more mundane title. reply bananapub 5 hours agoparent> The article is a 2024 \"review\" 2004. reply condour75 5 hours agoprevOne good timesaver: debug in the easiest environment that you can reproduce the bug in. For instance, if it’s an issue with a website on an iPad, first see if you reproduce in chrome using the responsive tools in web developer. If that doesn’t work, see if it reproduces in desktop safari. Then the iPad simulator, and only then the real hardware. Saves a lot of frustration and time, and each step towards the actual hardware eliminates a whole category of bugs. reply __mharrison__ 13 minutes agoprevGo on a walk or take a shower... reply __MatrixMan__ 1 hour agoprev> Check that it's really fixed, check that it's really your fix that fixed it, know that it never just goes away by itself I wish this were true, and maybe it was in 2004, but when you've got noise coming in from the cloud provider and noise coming in from all of your vendors I think it's actually quite likely that you'll see a failure once and never again. I know I've fixed things for people without without asking if they ever noticed it was broken, and I'm sure people are doing that to me also. reply knlb 6 hours agoprevI wrote a fairly similar take on this a few years ago (without having read the original book mentioned here) -- https://explog.in/notes/debugging.html Julia Evans also has a very nice zine on debugging: https://wizardzines.com/zines/debugging-guide/ reply Hackbraten 1 hour agoparentI love Julia Evans’ zine! Bought several copies when it came out, gave some to coworkers and donated one to our office library. reply waynecochran 5 hours agoprevI also think it is worthwhile stepping thru working code with a debugger. The actual control flow reveals what is actually happening and will tell you how to improve the code. It is also a great way to demystify how other's code runs. reply sumtechguy 3 hours agoparentThat is rule #3. quit thinking and look. Use whatever tool you need and look at what is going on. The next few rules (4-6) are what you need to do while you are doing step #3. reply nottorp 2 hours agoparentprevThis is necessary sometimes when you’re simply working on an existing code base. reply nthingtohide 4 hours agoparentprevMake sure through pure logic that you have correctly identified the Root Cause. Don't fix other probable causes. This is very important. reply ajross 5 hours agoparentprevI think that fits nicely under rule 1 (\"Understand the system\"). The rules aren't about tools and methods, they're about core tasks and the reason behind them. reply sandbar 1 hour agoprevTake the time to speed up my iteration cycles has always been incredibly valuable. It can be really painful because its not directly contributing to determining/fixing the bug (which could be exacerbated if there is external pressure), but its always been worth it. Of course, this only applies to instances where it takes ~4+ minutes to run a single 'experiment' (test, startup etc). I find when I do just try to push through with long running tests I'll often forget the exact variable I tweaked during the course of the run. Further, these tweaks can be very nuanced and require you to maintain a lot of the larger system in your head. reply goshx 2 hours agoprev> Quit thinking and look (get data first, don't just do complicated repairs based on guessing) From my experience, this is the single most important part of the process. Once you keep in mind that nothing paranormal ever happens in systems and everything has an explanation, it is your job to find the reason for things, not guess them. I tell my team: just put your brain aside and start following the flow of events checking the data and eventually you will find where things mismatch. reply drivers99 1 hour agoparentThere's a book I love and always talk about called \"Stop Guessing: The 9 Behaviors of Great Problem Solvers\" by Nat Greene. It's coincidental, I guess, that they both have 9 steps. Some of the steps are similar so I think the two books would be complementary, so I'm going to check out \"Debugging\" as well. reply throwawayfks 1 hour agoparentprevI worked at a place once where the process was \"Quit thinking, and have a meeting where everyone speculates about what it might be.\" \"Everyone\" included all the nontechnical staff to whom the computer might as well be magic, and all the engineers who were sitting there guessing and as a consequence not at a keyboard looking. I don't miss working there. reply pbalau 1 hour agoparentprevAcquire a rubber duck. Teach the duck how the system works. reply spawarotti 6 hours agoprevVery good online course on debugging: Software Debugging on Udacity by Andreas Zeller https://www.udacity.com/course/debugging--cs259 reply belter 4 hours agoparentUdacity is owned by Accenture? That is...Surprising. reply omkar-foss 3 hours agoprevFor folks who love to read books, here's an excerpt from the Debugging book's accompanying website (https://debuggingrules.com/): \"Dave was asked as the author of Debugging to create a list of 5 books he would recommend to fans, and came up with this. https://shepherd.com/best-books/to-give-engineers-new-perspe...\" reply burrish 2 hours agoparentthanks for the link reply shahzaibmushtaq 3 hours agoprevI can't comment further on David A. Wheeler's review because his words were from 2004 (He said everything true), and I can't comment on the book either because I haven't read it yet. Thank you for introducing me to this book. One of my favorite rules of debugging is to read the code in plain language. If the words don't make sense somewhere, you have found the problem or part of it. reply apples_oranges 6 hours agoprevA good bug is the most fun thing about software development reply jamesblonde 6 hours agoparentJust let a LLM create even better bugs for you - Erik Meijer https://www.youtube.com/live/SsJqmV3Wtkg?si=MUoiNbWpsunsZ39y... reply waynecochran 6 hours agoparentprevSometimes I am actually happy when there is a obvious bug. It is like solving a murder mystery. reply BobbyTables2 4 hours agorootparentAnd often you’re the culprit too! reply teleforce 5 hours agoprevThe tenth golden rule: 10) Enable frame pointers [1]. [1] The return of the frame pointers: https://news.ycombinator.com/item?id=39731824 reply BlueUmarell 4 hours agoprevPost: \"9 rules of debugging\" Each comment: \"..and this is my 10th rule: \" Total number of rules when reaching the end of the post: 9 + n + n * m, with n being number of users commenting, m being the number of users not posting but still mentally commenting on the other users' comments. reply reverendsteveii 4 hours agoprevReview was good enough to make me snag the entire book. I'm taking a break from algorithmic content for a bit and this will help. Besides, I've got an OOM bug at work and it will be fun to formalize the steps of troubleshooting it. Thanks, OP! reply sumtechguy 3 hours agoparentI recommend this book to all Jr. devs. Many feel very overwhelmed by the process. Putting it into nice interesting stories and how to be methodical is a good lesson for everyone. reply ChrisMarshallNY 5 hours agoprev#7 Check the plug: Question your assumptions, start at the beginning, and test the tool. I have found that 90% of network problems, are bad cables. That's not an exaggeration. Most IT folks I know, throw out ethernet cables immediately. They don't bother testing them. They just toss 'em in the trash, and break a new one out of the package. reply nickcw 4 hours agoparentI prefer to cut the connectors off with savage vengeance before tossing the faulty cable ;-) reply analog31 5 hours agoprevOne I learned on Friday: Check your solder connections under a microscope before hacking the firmware. reply InitialLastName 43 minutes agoparentThe worst is when it works only when your oscilloscope probe is pushing down on the pin. reply nottorp 2 hours agoprevI’d add “a logging module done today will save you a lot of overtime next year”. reply duxup 5 hours agoprevI’m so bad at #1. I know it is the best route, I do know the system (maybe I wrote it) and yet time and again I don’t take the time to read what I should… and I make assumptions in hopes of speeding up the process/ fix, and I cost myself time… reply jgrahamc 2 hours agoprevWasn't Bryan Cantrill writing a book about debugging? I'd love to read that. reply bcantrill 2 hours agoparentI was! (Along with co-author Dave Pacheco.) And I still have the dream that we'll finish it one day: we had written probably a third of it, but then life intervened in various dimensions. And indeed, as part of our preparation to write our book (which we titled The Joy of Debugging), we read Wheeler's Debugging. On the one hand, I think it's great to have anything written about debugging, as it's a subject that has not been treated with the weight that it deserves. But on the other, the \"methodology\" here is really more of a collection of aphorisms; if folks find it helpful, great -- but I came away from Debugging thinking that the canonical book on debugging has yet to be written. Fortunately, my efforts with Dave weren't for naught: as part of testing our own ideas on the subject, I gave a series of presentations from ~2015 to ~2017 that described our thinking. A talk that pulls many of these together is my GOTO Chicago talk in 2017, on debugging production systems.[0] That talk doesn't incorporate all of our thinking, but I think it gets to a lot of it -- and I do think it stands at a bit of contrast to Wheeler's work. [0] https://www.youtube.com/watch?v=30jNsCVLpAE reply PhunkyPhil 5 hours agoprevI would almost change 4 into \"Binary search\". Wheeler gets close to it by suggesting to locate which side of the bug you're on, but often I find myself doing this recursively until I locate it. reply ajuc 4 hours agoparentYeah people say use git bisect but that's another dimension (which change introduced the bug). Bisecting is just as useful when searching for the layer of application which has the bug (including external libraries, OS, hardware, etc.) or data ranges that trigger the bug. There's just no handy tools like git bisect for that. So this amounts to writing down what you tested and removing the possibilities that you excluded with each test. reply urbandw311er 6 hours agoprevRule #10 - it’s probably DNS reply tmountain 5 hours agoparentYears ago, my boss thought he was being clever and set our server’s DNS to the root nameservers. We kept getting sporadic timeouts on requests. That took a while to track down… I think I got a pizza out of the deal. reply berikv 5 hours agoprevPersonally, I’d start with divide and conquer. If you’re working on a relevant code base chances are that you can’t learn all the API spec and documentation because it’s just too much. reply berikv 5 hours agoparentAlso: Fix every bug twice: Both the implementation and the “call site” — if at all possible reply BobbyTables2 4 hours agorootparentYe ol’ “belt and suspenders” approach? reply causal 5 hours agoparentprevCheck the plug should be first reply Tepix 5 hours agoprevMy first rule for debugging debutants: Don't be too embarassed to scatter debug logmessages in the code. It helps. My second rule: Don't forget to remove them when you're done. reply lanstin 1 hour agoparentMy rule for a long time has been anytime I add a print or log, except for the first time I am writing some new cide with tricky logic, which I try not to do, never delete it. Lower it to the lowest possible debug or trace level but if it was useful once it will be useful again, even if only to document the flow thru the code on full debug. The nicest log package I had would always count the number of times a log msg was hit even if the debug level meant nothing happened. The C preprocessor made this easy, haven't been able to get a short way to do this counting efficiently in other languages. reply sitkack 54 minutes agorootparentI really like this. reply mootoday 6 hours agoprevDid anyone say debugging? I've followed https://debugbetter.com/ for a few weeks and the content has been great! reply k3vinw 6 hours agoprevThe unspoken rule is talking to the rubber duck :) reply zarq 6 hours agoparentThat is literally #8 in the list reply dkdbejwi383 6 hours agorootparentRule #10 - read everything twice reply k3vinw 4 hours agorootparentprevHmm. Perhaps, but mannequin is not nearly as whimsical sounding as a rubber duck which inspires you to bounce your ideas off of the inanimate object. reply TheLockranore 5 hours agoprevRule 11: If you haven't solved it and reach this rule, one of your assertions is incorrect. Start over. reply begueradj 5 hours agoprevThis is related to the classic debugging book with the same title. I first discovered it here in HN. reply ChrisArchitect 5 hours agoprev(2004) Title is: David A. Wheeler's Review of Debugging by David J. Agans reply fedeb95 4 hours agoprev [–] rule -1: don't trust the bug issuer reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "David A. Wheeler reviews \"Debugging: The 9 Indispensable Rules for Finding Even the Most Elusive Software and Hardware Problems\" by David J. Agans, highlighting it as a classic resource for developers.",
      "The book emphasizes fundamental debugging principles applicable to both novice and experienced developers, using practical examples and real-life \"war stories\" to illustrate its nine key rules.",
      "Wheeler commends the book for its focus on data gathering and handling intermittent problems, though he notes a desire for more specific tool advice, ultimately recommending it for its timeless debugging insights."
    ],
    "commentSummary": [
      "Debugging is essential for resolving complex issues, emphasizing the importance of staying calm, understanding the system, and verifying assumptions. - Tools like `git bisect` can aid in efficient problem-solving by identifying the specific change that introduced a bug. - It's important to verify fixes and consider writing tests to prevent future regressions, highlighting the need for a systematic approach and sometimes using techniques like rubber duck debugging."
    ],
    "points": 302,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1736770062
  },
  {
    "id": 42679127,
    "title": "How did they make cars fall apart in old movies (2017)",
    "originLink": "https://movies.stackexchange.com/questions/79161/how-did-they-make-cars-fall-apart-in-old-movies",
    "originBody": "Join Movies & TV By clicking “Sign up”, you agree to our terms of service and acknowledge you have read our privacy policy. Sign up with Google OR Email Password Sign up Already have an account? Log in X Skip to main content Stack Exchange Network Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. Visit Stack Exchange Loading… Tour Start here for a quick overview of the site Help Center Detailed answers to any questions you might have Meta Discuss the workings and policies of this site About Us Learn more about Stack Overflow the company, and our products current community Movies & TV help chat Movies & TV Meta your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up Home Questions Tags Users Unanswered Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Try Teams for free Explore Teams Teams Ask questions, find answers and collaborate at work with Stack Overflow for Teams. Explore Teams Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams How did they make cars fall apart in old movies? Ask Question Asked 7 years, 4 months ago Modified 4 years, 4 months ago Viewed 51k times This question shows research effort; it is useful and clear 263 Save this question. Show activity on this post. A staple of old Buster Keaton/Laurel and Hardy films is the car that falls completely apart, either when closing one of the doors or even when driving, like so: How exactly did they pull these stunts off? I could see how it could be done with just two or three joined pieces that you pull a link out of, or something, but in these movies they often collapse into lots and lots of pieces. How was this done? Just for reference, the movie is Buster Keaton's Three Ages (1923), his directorial debut & first feature-length movie. film-techniques effects stunts silent-movie Share Improve this question Follow Follow this question to receive notifications edited Aug 23, 2017 at 7:31 can-ned_food 21811 gold badge22 silver badges88 bronze badges asked Aug 21, 2017 at 8:45 BasBas 2,20122 gold badges1111 silver badges99 bronze badges 3 I suspect that part of it is editing. There appears to be a discontinuity in the shot just before Keaton starts to roll off. I suspect that stagehands came in and jumbled things a bit at that point. – Hot Licks Commented Aug 28, 2017 at 1:42 2 There is no discontinuity, no edit. I've watched this a hundred times, from 2 different sources. It's a straight physical gag, nothing else. – Tetsujin Commented Aug 29, 2017 at 6:29 @Tetsujin - Look where his legs fly up. Sure looks like a cut to me. – Hot Licks Commented Sep 18, 2017 at 0:14 Add a comment1 Answer Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) This answer is useful 343 Save this answer. +650 This answer has been awarded bounties worth 650 reputation by Thunderforge, Charles and Abdelrhman Arnos Show activity on this post. The 'cab' is hinged round the bottom of the box structure, probably released by Keaton at the appropriate moment, and very likely sprung to assist the demolition. The rest is just balanced on top and will fall at the slightest provocation. You only see it move about 6 feet, so it doesn't have to last long. As I can see no evidence of a rope to pull it [and I don't think painting techniques would be good enough to paint it out afterwards] let's assume someone pushed it from a few yards further back then ran out of frame. It has no engine, of course. If you watch it frame by frame, it's easier to see... In the first 2 frames, all is well so far - except the bonnet/hood isn't quite seated properly. Also note Keaton's hand on the bodywork - that may already be loose [or may be part of the release mechanism]. The wheel at camera right isn't quite straight either... By frame 3, that wheel is already falling off, before it even hits the bump... Incidentally, the bonnet/hood has dropped back into its correct position momentarily. After further analysis, the release has been triggered by frame 3. There is a disconnect at the rear, camera left, and I'm now certain the front wheel, camera right, has a sprung steering mechanism to push it the right way. by frame 4, the catch has been released and the bodywork is hinging outwards... The seat is also being hinged backwards and downwards, but we can't see that yet. [Further evidence of the steering being sprung is that the initial impact with the dirt ridge momentarily pushes it back straight. The rear wheel is also off by this point and the axle has dropped, assisting the slew - better detail at the end] Keaton lifts the steering wheel away and the front panel falls forwards - assisting the demolition of the engine bay and grille. One thing to keep track of over the next few frames is that the steering column stays in position relative to the car's chassis, which could be quite a hazard. [My 'sprung steering theory' is that the spring force has now overcome the bump and is back on track. It's hard to tell from this point if it is only the steering, or if the axle itself bends forwards. [From what little I know of cars from this era, that axle ought to be a solid bar, with steering at the ends.] Further evidence now shows the entire axle turns, see detail at the end of the post. Now we can see the seat tilting, preventing Keaton from sliding forwards into the loose debris and containing him within the relative safety of the hinged box. Throwing his legs upwards as the seat drops is a) comedic and b) safer. Now we get to see the steering column that didn't fall away ... Note he seems well aware that the steering column didn't fall away - construction error or design, we may never know. You can see him avoid it over the next few frames, even as the car takes a final lurch. You can also see the clear benefit in these final frames of the seat falling back to 45°, preventing him from sliding forwards. And that's it. He's already stepping out of the car by frame 11. Note his exit route is a) planned and b) padded - the original car door. That is probably a clue as to why the camera right wheel went first, to slew the car in a specific direction. Ten minutes and both he and the car could be ready for another take. Re: the slewing - I've been trying to decide if this frame shows the front axle bending to ensure it goes in that direction. I can't decide. Perhaps the release mechanism also disconnected the steering on that side, or sprung it across, to push it in the correct direction. Last bit of evidence - from another source - the rear camera-left wheel drops first, assisting the slew, even as the front wheel is turning, before it hits the dirt at all... The front axle in its entirety turns too [on this better footage, there's no sign it bends, but it does turn as a whole piece. One final note... The wheels never come off. They're tethered, like a modern Formula 1 car. They fall over but never fully detach from the axle ends. If they weren't, pure chance would say one of them wouldn't land right next to its own axle - especially the rear which we see come loose long before the rest. It would appear that the only totally free-to-fly items are the radiator grille and the two headlamps. Everything else, including the last-to-fall front wing/fender, is held firmly in place, at least by one end. Share Improve this answer Follow Follow this answer to receive notifications edited May 21, 2018 at 18:41 Napoleon Wilson 59.6k6565 gold badges346346 silver badges677677 bronze badges answered Aug 21, 2017 at 10:01 TetsujinTetsujin 53.7k1212 gold badges181181 silver badges173173 bronze badges 1 2 Comments are not for extended discussion; this conversation has been moved to chat. – Ankit Sharma ♦ Commented Dec 7, 2017 at 12:52 Add a commentYou must log in to answer this question. Not the answer you're looking for? Browse other questions tagged film-techniques effects stunts silent-movie . Related 31 How do they make horses fall down in war movies? 5 How did they make Hitler disappear in \"Your Job in Germany\"? 11 How did they make the shimmering colours when Mork calls Orson? 48 How do they make tattoos in movies? 74 Do they really crash super cars in action movies? 5 How do they make bloodshot eyes in movies? 36 How did they make Yogurt so short? 62 Why did cars in 70s and 80s TV and movies slide like they were driving on ice? 1 How do movies make explosions? Hot Network Questions Which door leads out? Does Harvard Medical School give degrees on the basis of donations? How do greenhouse gases absorb so much radiation when they're so rarely found? F# railway style vs lazy seq Withdraw PhD program application? Half-switched duplex outlet always hot after replacement \"He moved with surprising swiftness for someone who had just woken up.\" — Does \"someone\" here mean someone definite rather than someone indefinite? What is the maximum wire thickness that can be crimped into an RJ11 connector? Suspension of Canadian parliament's impact on governing; what if some big emergency happens? What's the reality behind US claims of Chinese influence in Greenland? Is the plane-wave solution to the Maxwell equation an instanton? ברוך ה׳ המברך לעולם ועד: to repeat or not to repeat Why do Newtonian fluids have a single viscosity constant for both shear and normal stresses, while solids have different constants for each? Why do many programming languages use the symbol of two vertical parallel lines `||` to mean \"or\"? Reducing 6V to 3V How could a tropical saltwater lake, turned to freshwater, become salty again? Grouping based on the size of the median What would passenger space and aircraft look like that could carry a multi-ton sapient race? need correct translation from english to latin What is the subject noun in this sentence - what is the 'it' referring to? Why a sine wave? Should the generation method of password-reset-tokens be kept secret? Should I keep all Python libraries only in the virtual environment? Stronger bound on abelianization of 2-transitive group more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. Movies & TV Tour Help Chat Contact Feedback Company Stack Overflow Teams Advertising Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2025 Stack Exchange Inc; user contributions licensed under CC BY-SA . rev 2025.1.10.20996",
    "commentLink": "https://news.ycombinator.com/item?id=42679127",
    "commentBody": "How did they make cars fall apart in old movies (2017) (movies.stackexchange.com)223 points by mgsouth 17 hours agohidepastfavorite84 comments mrb 11 hours agoI immediately assumed this article was about the French movie Le Corniaud (1965) in which a 2CV falls apart in 250 pieces in an accident—this scene specifically: https://www.youtube.com/watch?v=gnLj5Xo4zBc&t=19s It became one of the most iconic scene of French comedy movies. To prepare the scene, the special effects engineer sawed off the car in 250 pieces, reattached every piece with hooks, and secured the hooks with \"explosive bolts\". At the right moment, the actor driving the car pushed a button to trigger the (tiny) explosives which made the car fall apart. Here is a French article about it: https://2cv-legende.com/expo-de-la-2cv-du-film-le-corniaud-a... PS: the French wikipedia article on the movie has a picture of the explosive bolts they used: https://fr.wikipedia.org/wiki/Le_Corniaud#L'accident_de_la_2... reply yowzadave 5 hours agoparentThe one I thought of was the Silver Hornet from Revenge of the Pink Panther: https://youtu.be/0z-FtAMg6Vw?si=zGsEnyt4NKtsMnLb Even though I’ve seen many different versions of this gag, they are all still funny to me. reply dylan604 3 hours agorootparentThis seems very much like an homage to the French film example, just done less well. reply af78 4 hours agoparentprevI'm not sure how to translate this line: « Ah ben maintenant elle va marcher beaucoup moins bien, forcément ! » (Bourvil reportedly improvised it, causing de Funès to start laughing and bow his head to hide it). Google Translate: “Ah well now it’s going to work a lot less well, of course!” Deepl: - It's going to work much less well. - It's going to run much less smoothly. - It's going to run a lot less smoothly. None of these suggestions sounds good to me (in case it isn't clear I'm not a native English speaker). reply colanderman 4 hours agorootparentAll four sound fine to my native ears. \"It's going to run ...\" is most natural when talking about a vehicle. (French if I recall does not distinguish \"working\" from \"running\" for machines generally.) reply af78 3 hours agorootparentThanks. While the primary meaning of 'marcher' is 'to walk', it can be used for machines and vehicles indeed. 'Rouler' is for vehicles only. Interestingly in English the verb 'to run' is used, suggesting higher speed. The expression “to work better” is quite common but I don't remember seeing “to work less well”. And as I was taught that « plus grand » translates to “taller” but « moins grand » to “not as tall as”, I expected something more involved. reply maxerickson 3 hours agorootparentprevSomething like \"Oh well now it will run a lot less well, obviously.\" Seems like the more or less literal translation. \"a lot less well\" is the awkward part, a more natural construction would be a negation \"is not going to run well\" or something like that. reply amelius 8 hours agoparentprevBy the way, I always wondered why we got modern versions of the Mini and the Beetle, but not the 2CV. reply kjellsbells 8 hours agorootparentI guess the answer depends on which aspect of the 2CV is being replicated in the new version. If its \"outrageously small but can still take you and a goose to market\", Citroën have a tiny little electric vehicle, the Ami, today. If its \"something simple enough that a farmer can weld the panels themselves\", I fear those days are long gone, in the same way that the OG Land Rover Defender is no longer a car you can wrench on. The spiritual heir of such cars is probably a toyota hilux(?). Modern safety standards and the presence of complex electronics beneath every surface, to say nothing of the more complex sheet metal shapes, probably stop that idea in its tracks. reply Cthulhu_ 7 hours agorootparentThere's still simple cars being produced but they're aimed at the Chinese and Indian markets, same with motorcycles. Example is (was?) the Tata Nano (https://en.wikipedia.org/wiki/Tata_Nano), at $2500 a very affordable and simple car, mainly aimed at motorcycle/scooter drivers. reply regularfry 3 hours agorootparentprevThe Hilux went the other way: you can apply wrench to nut, but the odds of you needing to do so recede into the distance. reply iglio 6 hours agorootparentprevThere’s the Ineos Grenadier[1] > The Grenadier was designed to be a modern replacement of the original Land Rover Defender, with boxy bodywork, a steel ladder chassis, beam axles with long-travel progressive-rate coil spring suspension (front and rear), and powered by a BMW B58 inline six turbocharged engine. [1] https://en.m.wikipedia.org/wiki/Ineos_Grenadier reply jimnotgym 2 hours agorootparentIt is a lot more complicated than a defender though, isn't it. It has electronics! reply doikor 6 hours agorootparentprevFor Defender there is Ineos Grenadier https://ineosgrenadier.com/ reply eichin 1 hour agorootparentprevhttps://2cev.co.uk/ showed up on ev-youtube last year (but other than the drive train, it's going out of it's way to not actually be modern... the 2cv aesthetic of \"you think a VW Bug is too fancy\" kind of limits the options.) reply potato3732842 7 hours agorootparentprevBecause the 2CV is mostly replaced by the entire crossover and compact SUV market segment. reply andrepd 55 minutes agorootparentprevWell but the modern Mini and Beetle are related to the classics in name only, not in spirit. reply dostick 8 hours agorootparentprevWas one 20 years ago, Citroen c5 or c3 or something. Maybe still is. reply amelius 7 hours agorootparentIt doesn't have the iconic 2CV look ... reply prmoustache 6 hours agorootparentNor do the new mini ever had the original mini look. The Daihatsu Trevis was much closer to the Issigonis Mini look than the new mini ever was. I may be wrong but I don't think the 2cv has a design that can translate as easily to a newer version the same way as the beetle design could without being completely denatured. I think it would be easier to build a modern HY looking van. reply moffkalast 8 hours agoparentprevThat scene would've been a lot more impressive if wasn't edited like Liam Neeson jumping over a fence, haha reply 4gotunameagain 10 hours agoparentprevSuper impressive ! Thanks for sharing. Similar (albeit a bit heavier from the all paperwork) explosive bolts are user for stage separation in launch vehicles (rockets). reply wisty 12 hours agoprevFor those who don't know, Keaton was amazingly dedicated as a comedic stuntman - a silent era Jackie Chan (he was less popular after the silent era, but kept working until his death in the 60s). From Wikipedia: Garry Moore recalled, \"I asked (Keaton) how he did all those falls, and he said, 'I'll show you.' He opened his jacket and he was all bruised. So that's how he did it—it hurt—but you had to care enough not to care.\" This would have been in about 1955, when Keaton (born 1899) was an old man and well past his heyday of really dangerous stunts (he once broke his neck during an early stunt). And he usually had an amazing commitment to film in a lot of other ways. The first time he was shot in a film he took a camera apart to figure out how it worked, because he really cared about every detail (though in the middle of his career this really hurt him, as execs wanted to just trot him up in front of the camera as a high paid celebrity - they didn't want him wasting his valuable time fussing over details, or risk their investment letting him do stunts). reply keiferski 10 hours agoparentVideo of some of his better stunts: https://youtu.be/yOo_ZUVU_O8?si=1OEwZTk-d88ma2Zs And a great Every Frame a Painting film essay on his work: https://youtu.be/UWEjxkkB8Xs?si=n-4ZNr_cMnYVKijs He was truly an innovator that makes today’s “films of people talking to each other” look amateurish. A few months ago the local theatre was playing Sherlock Jr. with a live band, and it was awesome. Try to see it in similar circumstances if possible. reply exhilaration 2 hours agorootparentWow, those stunts are incredible - it's hard to believe he died of old age and not of these super risky stunts. reply acuozzo 4 hours agorootparentprev> A few months ago the local theatre was playing Sherlock Jr. with a live band AFI in Silver Spring? reply keiferski 3 hours agorootparentNope, other side of the world reply exitb 9 hours agoparentprev> He opened his jacket and he was all bruised. So that's how he did it—it hurt—but you had to care enough not to care.\" It reminds me of the glass eating trick by David Blaine, where the trick is to… just eat glass. It makes it quite bittersweet, as after all, those men are trading some of their wellbeing for some of their fame. Not sure how to feel about it. reply db48x 4 hours agorootparentMen sell their bodies all the time. Miners, fishermen, football players, etc. 97% of all workplace fatalities are men. reply thih9 7 hours agorootparentprevI am also trading my short term wellbeing, if only for money - by working in an unappreciative startup; I suppose many others do the same, and even more would like to. My hope is that my long term wellbeing improves as a result. reply exitb 7 hours agorootparentThat's true, although society generally does not applaud sustaining permanent injuries at work as dedication. reply scarface_74 4 hours agorootparenthttps://www.sportico.com/business/media/2025/nfl-owns-73-of-... Usually 90 of the top 100 shows on American TV are football games. It was 72 out of 100 in 2024 because it was an election year. reply thih9 3 hours agorootparentprevI can’t imagine staring at a screen for 8h+ hours a day[1] is not causing some permanent injuries. [1]: Not to mention daily zoom calls with a micromanaging boss and a mandatory video on rule. reply krisoft 6 hours agoparentprev> He opened his jacket and he was all bruised. So that's how he did it—it hurt—but you had to care enough not to care. I don't want performers to risk their safety, health and life for my entertainment. Obviously I cannot stop it, but I can stop watching those who engage in things like this. (And I don't just mean the stunt performer, but the director, the producers, the studio and the franchise.) I have unsubscribed from youtube channels when I felt that they were pushing themselves in dangerous directions. It is not like that alone will stop them, but if I would keep watching I would be complicit in the harm which might befall them. There is the principle attributed to Houdini by Penn Jillette that a performance/trick should not be more dangerous than sitting in one's living room. Especially when it appears dangerous. I don't know about the exact line though. Strictly interpreting the \"not be more dangerous than sitting in one's living room\" definition would disqualify any performance where the performer had to drive (or be chauffeured) to the location of their performance. And that would be a bit ridiculous. reply josefx 4 hours agorootparent> There is the principle attributed to Houdini Houdini died from a rather trivial stunt he performed many times before. A hit to the abdomen before he could flex his muscles most likely ruptured his appendix. Keaton died of lung cancer well past the end of his fame. You can manage the danger of stunts, you can reduce it and prepare for anything that could go wrong. You can never completely avoid it and sometimes a single error is all it takes. reply krisoft 4 hours agorootparent> You can manage the danger of stunts, you can reduce it and prepare for anything that could go wrong. I think that is all I'm asking. Or not even that. Just saying that if they don't, i don't want to watch it. > Houdini died from a rather trivial stunt he performed many times before. The blows which allegedly killed Houdini were not suffered during a performance or stunt. reply gigaflop 3 hours agorootparentprevThere's a youtube channel out there that used to be a sort of nature channel, but seems to have devolved into 'Get stung/bit by painful animal X'. I haven't watched their stuff in ages, but I'm very aware that the original channel host isn't the one getting stung anymore. I have to wonder what it was like from their perspective, watching the view counts go up and up with each successive \"Hurt yourself on camera\" video, and wondering what to do next. reply astura 2 hours agorootparent>There's a youtube channel out there that used to be a sort of nature channel, but seems to have devolved into 'Get stung/bit by painful animal X'. I haven't watched their stuff in ages, but I'm very aware that the original channel host isn't the one getting stung anymore. Brave Wilderness? reply crazygringo 6 hours agorootparentprev> I don't want performers to risk their safety, health and life for my entertainment. I mean, they pretty much all do to some degree. It's not healthy on your body to do eight Broadway shows a week. Or to be constantly switching between all-day and all-night shoots on a TV show. And performing a role of high emotional trauma every day for weeks or months takes its own kind of toll too. Obviously nobody should be at risk of life or of permanent injury, that goes without saying. But getting bruises while doing stunts, that's just what being a stuntperson is. Nobody is forced into it. And this is why there are stuntpeople in the first place -- it's not just for skills. Sometimes the regular actor could do it fine, but there's no time in the schedule for their body to recover afterwards. reply krisoft 5 hours agorootparent> Nobody is forced into it. And i’m not forced to watch it. So all is fair. reply adamc 3 hours agorootparentYour position is similar to why I stopped watched NFL games. I get that players choose to play (for money), but at the end of the day, I am unwilling to contribute to brain damage. reply BiteCode_dev 6 hours agorootparentprevThen you have to stop watching any competition of anything because the winners are always among the ones sacrifying the most. Then stop reading about start up on HN as well. In fact, forget about any extra ordinnary human achivement. reply krisoft 5 hours agorootparent> Then you have to stop watching any competition of anything Done. Easy. > stop reading about start up on HN as well I don’t think there the motivation is to create entertainment though. But i don’t care much about that kind of content either. > forget about any extra ordinnary human achivement I disagree with that. Plenty of extraordinary human achievements were created under circumstances I find acceptable to celebrate and watch. reply daseiner1 3 hours agorootparentLet's not scale mountains, explore the oceans, cross the poles, or go to space. Why be heroic when we can all hold hands and be safe. \"\"\"They have left the regions where it is hard to live; for they need warmth. One still loveth one's neighbour and rubbeth against him; for one needeth warmth.\"\"\" reply krisoft 3 hours agorootparent> Let's not scale mountains, explore the oceans, cross the poles, or go to space. Why be heroic when we can all hold hands and be safe. In terms of exploring the oceans my hero is Admiral Rickover and not Stockton Rush. Different kind of heroism. Not the lack of it. reply adamc 4 hours agoparentprevYou can see some classic Keaton in \"A Funny thing Happened on the Way to the Forum\". He remained great, even as an old man. reply throw4847285 3 hours agorootparentWow, I completely forgot that he played Erronius. Every time I think about the way he says \"stolen in infancy by pirates\" in that gravelly voices of his I have to stifle a laugh. reply draven 8 hours agoparentprevI saw a Jackie Chan interview years ago (20 or so) in which he said Keaton was an inspiration. reply vodou 10 hours agoparentprevHere is the stunt where he broke his neck: https://www.youtube.com/watch?v=yOo_ZUVU_O8&t=187s reply ErigmolCt 12 hours agoparentprevHis dedication was truly next-level reply hilbert42 12 hours agoprevThose reasons seem to make sense but I'd say just as much has to do with Buster Keaton himself, he had nerves of steel. During the filming of the Civil War movie The General there are images of Keaton doing things that even the bravest of stuntmen wouldn't do these days and we'd now rely on film animation and tricks to make the scenes work. For instance, Keaton—who obviously was very fit and agile—is filmed sitting on a cowcatcher of a moving locomotive whilst removing rail ties that were placed on the line to impede the train's progress and then tossing them aside. I read somewhere that Clyde Bruckman the film's director gave instructions to the cameraman \"to keep filming the scene until finished or until Keaton is killed\" or words to that effect. I can't remember whether Bruckman was referring to this scene or another such as when he's running across the locomotive's tender (the comment could equally have applied to many other scenes I reckon). Others who are more knowledgeable could perhaps fill in the details. I like this movie, Keaton was a great performer and his movies are a testament to that. reply ggm 9 hours agoparentMy favourite Keaton movie is the one near his end where he goes across Canada by hand crank car on rail roads. \"The railrodder\" (1965) Kenton died 1966 reply hilbert42 7 hours agorootparentYeah, I came across that one by sheer accident some years back. It was such a surprise. Now you've reminded me of it I'll watch it again. :-) reply mkl 8 hours agoparentprevI found the movie interesting in that they managed to make the Confederates the good guys by simply never showing a Black person on screen or mentioning slavery. There were a few good stunts and it was worth watching as a historical curiosity, but I didn't think it was all that good as a movie. I'm not American, so may have missed some things that would have let me follow the story better. reply throw4847285 3 hours agorootparentI recommend listening to the episode of the Blank Check podcast about The General (and Battling Butler), if you can sanction some buffoonery. It's a mix of a comedy podcast and deep movie analysis, which is not for everybody. For that episode they brought in writer Jamelle Bouie who is both a huge movie buff and a student of American history who brings in some great perspective on the Lost Cause. https://podcasts.apple.com/ca/podcast/battling-butler-the-ge... reply hilbert42 47 minutes agorootparentHave started listening to it, thanks. reply hilbert42 7 hours agorootparentprevI'm not an American either so I've not a patriotic fervor over the outcome of the Civil War to the extent as that most Americans have. That the movie showed the Confederates in better light than the Yankees wasn't appreciated much when it was released. Back then, there were Civil War veterans who were still alive who criticized the film which contributed to its poor ratings. Also, keep in mind the film was based on the story The Great Locomotive Chase, changing it to having the Yankees as the main subject just wouldn't have been feasible. Nevertheless, the film's stature has grown over the years and has developed a bit of a cult status: https://www.oregonencyclopedia.org/articles/the_general_film... https://en.m.wikipedia.org/wiki/The_General_(1926_film) (read 'Legacy') Oh, and I just noticed on the Wiki page there's even an image of Keaton riding the cowcatcher. I'm not a film buff so I'll let those comments/reviews stand on their own merits. reply saalweachter 7 hours agorootparentSome context for non-Americans: the 1920s (when the film was released) was the hey-day of Civil War revisionism; that was when most of the statues of Confederate generals were erected and the narrative of the noble Confederates was written. \"1920s film made Confederates the good guys\" is one of the least surprising things ever. reply hilbert42 7 hours agorootparentThanks for that, that's a perspective of which I was unaware but I've long been aware there was a reasonable level of criticism when the film was released. It's notable from this outsider's perspective that there's still levels of animosity over the War and that statues of Lee get desecrated and or damaged from time to time. reply saalweachter 5 hours agorootparentWell, it wasn't just putting up statues and making movies where the Confederates were the good guys -- the 1920s was also the peak of organized white supremacy like the Ku Klux Klan, when lynching and other mob violence was common. The Tulsa massacre, which involved burning one of the wealthiest black neighborhoods in the United States, was only 5 years before this film came out. You can kind of think of this era as a sort of \"anti-Civil Rights movement\", and it was the same group of people burning houses and lynching and putting up statues and working politically to keep black Americans disenfranchised. And it's still a salient issue today -- disenfranchisement of minorities (closing polls in minority neighborhoods to create multi-hour waits to vote; gerrymandering to concentrate minorities in a small number of Congressional districts; disproportional felony convictions and the accompanying loss of franchise) is an issue in every election. Hell, one of the initial backlashes against public health measures early on in the COVID pandemic was that the early waves primarily affected large cities and the initial mortality rates were higher for blacks than whites, so it was viewed as a problem more for blacks than whites, and therefore, not a problem. The white-washing of Lee and the other Confederate traitors is still part of modern American politics -- it reframes the Civil War from a bunch of rich slave-owners rebelling against the United States to maintain their power and privilege, and getting hundreds of thousands of other people killed for it, to cast these men as victims of a rapacious Federal government meddling where it didn't belong. This narrative that was (and is still, eg, Shelby County v Holder) used to claim the Federal government had no right to improve the lives of minorities over the wishes of the States, is now used to claim the Federal government has no right to mandate minimum wages, or environmental regulations, or educational standards, or a thousand other things, over the wishes of the individual States. So it's still modern politics to cast down Lee and declare that he was not a noble martyr fighting for States Rights against an oppressive Federal government, just a traitor to his oaths who was personally and politically reprehensible. And to point out that States Rights have always just been a political shell game -- Slave States were happy to use the power of the Federal government to override the will of Free States, and force them to extradite escaped slaves back to the Slave States, just like issues like abortion are \"sent back to the States\" until a Federal ban can be passed, at which point it will miraculously no longer be an issue for the States to resolve. It's the old quote -- \"The past is never dead. It's not even past.\" reply scarface_74 4 hours agorootparentThere are eight states that have a Confederate memorial day and two that combine Martin Luther King and Robert E. Lees birthday. reply hilbert42 2 hours agorootparentprev\"It's the old quote -- \"The past is never dead. It's not even past.\" Right, how very true. One of my aunts married a French soldier at the end of WWII and went to live in France. She often told me La Révolution française was far from settled, just scratch the surface anywhere in France and you'll still find much contention. BTW, it was ≈235 years ago. I've been to the US many times, have relatives who live there and have even worked there so I'm somewhat familiar with many of those events you've.mentioned. I suppose I'm still surprised by the intensity and vehemence of the attacks—whether verbal or physical—towards both the black population and the various underclasses/undeprived. That's not say this country I'm in is lily-white by any means—we've had our fair share of atrocities in the past—but present-day vitriol and animosity towards certain peoples certainly isn't as intense as I've seen it in the US. The question is why. Let me give you two instances that come to mind (and I've more) that I think wouldn't be commonplace here (but that's not to say they couldn't happen as sometimes they do). First, I was the only person in a manually-driven elevator and its driver was black and as I was alighting I said to him \"thank you very much sir\" and with a great big smile he said \"and thank you too sir, not many people are so nice and say that to me these days\". I've never forgotten the encounter. The other example is some years back I was traveling around California in a minivan with about a half dozen of my compatriots after having been to a computer conference and we were in Redwood City and had to refuel. At the servics station we were served by a local who asked where we were from and we told him. He then went into a tirade that I'll never forget which I won't repeat in full here to the effect \"you're fucking lucky that down there you don't have any of those… (you can guess the rest), and that was only a small part of his outrageous and vitriolic tirade. It wasn't just his tirade that so surprised me but that he was so open to strangers who he'd never met previously. BTW, that exchange was well after the 1960s civil rights stuff—mid 1980s in fact. Despite me agreeing with your quote, as I said I suppose I've never been fully reconciled to or able to get my head around why the US continues to cycle over these issues with such intensity for so long. One would have thought that after 150+ years things would have settled down much more than they actually have. That said, I accept that discrimination and racism never seem to fully go away no matter where one is, although nowadays in many places it's softer and more nuanced that it once was. My position is pretty straightforward, that is I've found there's a small percentage of bastards in every country and racial group on the planet (certainly in ones where I've been for some length of time to know) but almost without exception most people with whom I've met have been kind and nice to me. I always try to be nice to those who I meet and deal with and again—almost without exception—they reciprocate similarly—no matter who they are and where they come from. That's the rough outline, I'd like to develop that discussion further and make specific comments on the issues and instances you've mentioned. Trouble is, to make my position clear and not be misinterpreted and or misunderstood would take some considerable effort and lots of text not to mention the large amount of time involved—and anyway it'd be too much for a HN post. One thing I've learned online—and HN is no exception (albeit it better than most)—no matter how neutral or impartial one is when discussing these matters at any reasonable depth it's almost impossible not to upset some people, they'll often take great umbrage at the slightest provocation and or at the most innocuous comment for reasons I find unfathomable. Once I was taught formal argument and debating, they've structure and people can (mostly) say what they want without fisticuffs breaking out. Unfortunately, this art of debating propositions in an orderly manner on the web is almost unheard of. It's why I usually steer clear of such topics. reply nejsjsjsbsb 6 hours agorootparentprevThe entire film is embedded on the wiki page. Public domain is cool! reply hilbert42 6 hours agorootparentIf you can, buy a DVD copy from Kino, it's a much higher quality copy than any of the public domain copies available. In fact, the quantity is quite excellent. The reason the DVD copy is in copyright is because it has a new musical soundtrack. That said, the soundtrack is excellent and the music (which includes Civil War tunes) is both appropriate and is well integrated into the visual material. reply db48x 8 hours agorootparentprevIt’s a comedy; the sides don’t matter. It’s a hilarious movie, in fact. reply watersb 9 hours agoprevI'm surprised there's no mention yet of the incredible scene from the 1980 film \"The Blues Brothers\". https://youtu.be/QfN1GRqKXpM?si=-4Mwmipl5sCFtCWN This practical effect took weeks to set up. I can't find documentation specifying any special techniques used to create this version of the car. I recall reading an interview naming the builder who set it up, and how no one on set was allowed to touch it except the actors, John Belushi and Dan Ackroyd. Only one take. Can't find that interview now. reply blululu 6 hours agoparentI remember watching that movie recently and seeing that the cast was almost half stuntmen. The fact that the Chicago police basically gave them free range and unlimited extras also made a lot of things possible. The final chase scene is about 15 minutes of car crashes including the one where the neonazis fly off the bridge and the camera jump cuts to the car dropped from an airplane into Lake Michigan. https://youtu.be/FD9N7v5qGig?si=p-QYJSkkYJIlN3b4&t=110 reply gregoriol 8 hours agoparentprevIt's a very nice scene, but not as good as the 2CV from Le Corniaud. Also looking at it closely, you can see at the camera angle change that the car is not the same (roof shape cut, rear door a bit open, ...), and that it is not standing on its wheels with supports appearing below reply lenerdenator 3 hours agoparentprevtakes off hat reply csours 12 hours agoprev\"The only secret of magic is I'm willing to work harder on it than you think it's worth\" - Penn Jillette https://www.youtube.com/watch?v=trRJ4J15xU8 reply wisty 7 hours agoparentSpending more time and effort than other people are willing to do works in a number of fields. reply vodou 10 hours agoprevI love Buster Keaton. For me he might be the greatest performer ever. I actually watched the video linked in the comments with his greatest stunts and also one short movie together with my kids (5 and 8 years old) just the other day. They laughed their heads off! So if you can hear me, Buster, wherever you are: Your films are holding up a hundred years later. That is quite a feat. reply OuterVale 11 hours agoprevThis made me think of the scene in Chitty Chitty Bang Bang when Lionel Jeffries is captured and forced to convert a car into the titular phantasmagorical fuel-burning oracle. I was wondering just the other day how they achieved that effect. Wonderful little read. Thanks! reply ErigmolCt 12 hours agoprevIt’s hard to believe they could make cars fall apart so perfectly without the tech we have today. reply mkl 8 hours agoparentSeems easier back then; way fewer parts, not held together as well, no roof. reply monkeymeister 14 hours agoprevThis is both engineering and art. Magnificent. reply ErigmolCt 12 hours agoparentThey didn’t just build cars to fall apart... they choreographed it like a performance reply radar1310 9 hours agoprevKinda looks like the Michael Waltrip 1990 crash at Bristol in the NASCAR race.it’s on YT, look it up. reply betimsl 3 hours agoprevThey loosened the screws. reply sirshmooey 13 hours agoprevJust don’t look up how they made the horses fall down. reply Over2Chars 12 hours agoparenttripwires? reply josefritzishere 3 hours agoprevThis is brilliant. Today it'd all be CGI trash. reply DonHopkins 1 hour agoparentOr a CyberTruck dumpster falling apart, bursting into flames, then exploding, naturally. reply sandworm101 8 hours agoprev [–] Much of these tricks comes from how cars used to be constructed. Without any concept of safety cages, they were basically a bunch of very light structures secured atop a heavy metal frame. So long as the actor remains on the seat above the frame, they are in a falling house of cards. Today we build the frames around the people. Pull such a stunt in a modern car and you will be trapped amongst twisted metal rails. Cars were also much simpler to take apart. A few bolts here and there and a couple people could remove an engine. A few more and the roof came off too. Today, it is all spot welded and tight tollerances. Removing any substanial part of a modern car, anything beyond the seats, requires planning and specialized tools. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "In older films, practical effects were used to make cars fall apart, as seen in the 1965 French film \"Le Corniaud,\" where a car was disassembled and reassembled with hooks and explosive bolts for a scene.",
      "These stunts required detailed planning and engineering, facilitated by the simpler construction of cars at the time.",
      "Modern films would likely use CGI (Computer-Generated Imagery) for similar scenes due to the complexity and safety standards of contemporary vehicles."
    ],
    "points": 223,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1736732512
  },
  {
    "id": 42678647,
    "title": "Standard patterns in choice-based games (2015)",
    "originLink": "https://heterogenoustasks.wordpress.com/2015/01/26/standard-patterns-in-choice-based-games/",
    "originBody": "These Heterogenous Tasks Haphazard thoughts on game design and narrative. Skip to content Home About What My Comp Votes Mean My Games ← Year Without Zombies 2: The Dead of Winter Never Alone / Kisima Inŋitchuŋa → Standard Patterns in Choice-Based Games Posted on January 26, 2015 by Sam Kabo Ashwell When I was analysing the structures of CYOA works a few years back, I began to recognise some strong recurring design patterns. I came up with some home-brewed terminology, but didn’t ever lay it out in a nice clear way. This is a non-exhaustive look at some of the more common approaches, somewhat-updated (a lot has changed since then). I should stress that these aren’t discrete categories: while a lot of works will fall very straightforwardly into a single pattern, many will involve elements of multiple patterns. (And yes, I’m aware that you can often simulate one using the mechanics of another. That’s mostly beside the point.) Also, the example diagrams I’m using are smaller and simpler than would be likely in actual works. Time Cave. A heavily-branching sequence. All choices are of roughly equal significance; there is little or no re-merging, and therefore no need for state-tracking. There are many, many endings. Effects: The time cave is the oldest and most obvious CYOA structure. It is often good for narratives about freedom and open possibility, adventures that could go anywhere, flights of fancy. Time caves tend to have relatively short playthroughs, but strongly encourage replay: they are broad rather than long. Even with multiple playthroughs, most players will probably miss a good deal of the content. The time cave’s structure is both organised by chronological progression and detached from it. It’s ungrounded by regularity: possibility is so open that it often becomes fantastic or surreal, with different branches occupying wholly different realities. The player has velocity but little grasp, vast freedom but little ability to comprehend it. Examples: Edward Packard’s earlier work (The Cave of Time, Sugarcane Island), Emily Short’s A dark and stormy entry; Pretty Little Mistakes. Gauntlet. Long rather than broad, gauntlets have a relatively linear central thread, pruned by branches which end in death, backtracking, or quick rejoining. The Gauntlet generally tells one anointed story, which can be adorned with optional content or prematurely ended with failure; if there are multiple endings, they’re likely to derive from a Final Choice. Gauntlets rarely rely on state to any great extent (if they do, they are likely to evolve into a branch-and-bottleneck structure.) Effects: The player is likely to realise that they are on a constrained path, but the presentation of side-branches matters a great deal – do they mean death? incorrect answers? travel back in time? blocked paths, footnotes, or scenic details? Most often, the gauntlet creates an atmosphere of a hazardous, difficult or constrained world. Sometimes this can be punishing or depressing; sometimes it can be darkly comic; sometimes it’s a sign that you’re in a work heavily dependent on reflective or rhetorical choice. Perhaps the easiest structure to author, gauntlets can be conceived of in similar terms to linear stories, and ensure that most players will see most of the important content. There are two major varieties of gauntlet: deadly and friendly. Deadly gauntlets mostly prune the tree with failure; friendly ones mostly do so with short-range rejoining, and look a bit more like simple branch-and-bottleneck structures. Friendly gauntlets have been vastly more common in recent years, making up a high proportion of Twine works. Examples: Zork: The Forces of Krill. Our Boys In Uniform. Branch and Bottleneck. The game branches, but the branches regularly rejoin, usually around events that are common to all versions of the story. To avoid obliterating the effect of past choices, branch-and-bottleneck structures almost always rely on heavy use of state-tracking (if a game doesn’t do this, chances are you are dealing with a gauntlet). Somewhat rarely, the bottlenecks may be invisible – the plot branches and never reaches an explicit rejoining node, but the choices at the end of each branch are the same or similar, creating an exquisite-corpse effect. Effects: Branch-and-bottleneck games tend to be heavily governed by the passage of time, while still allowing the player fairly strong grasp. The branch-and-bottleneck structure is most often used to reflect the growth of the player-character: it allows the player to construct a somewhat-distinctive story and/or personality, while still allowing for a manageable plot. There’s a tendency – not a necessary one, by any means – for playthroughs to be very similar in the early game, then diverge as the effects of earlier choices accumulate. In order for the approach to work, it has to be used in a fairly large piece; you need time to accumulate change before producing results that reflect it. Examples: This is pretty much how Long Live the Queen works, and is the guiding principle of Choice of Games (Dan Fabulich uses the term delayed branching). It’s also a common plot structure in non-IF games that allow significant plot choices. Quest. The quest structure forms distinct branches, though they tend to rejoin to reach a relatively small number of winning endings (often only one). The elements of these branches have a modular structure: small, tightly-grouped clusters of nodes allowing many ways to approach a single situation, with lots of interconnection within each cluster and relatively little outside it. Re-merging is fairly common; backtracking rather less so. Quests generally involve some level of state-tracking, and do poorly when they don’t. The minimal size for a quest is relatively large, and this category includes some of the largest CYOAs. Effects: This mode is well-suited for journeys of exploration, focused on setting; the quest’s structure tends to be organised by geography rather than time. Indeed, most works of this kind involve a journey with a specific purpose in mind. Quests work well for grounded, consistent worlds, but within that context the player-character’s situation is constantly changing. The narrative tends to be fragmentary or episodic, like old-school D&D encounters: little chunks of story which might not have any great significance for the big picture. Examples: The Fighting Fantasy books and their descendants (Lone Wolf, 80 Days). Open Map. Even though quests are structured by geography, time still plays an important part: there’s a built-in direction of travel. But take a CYOA structure, make travel between the major nodes reversible, and you have a static geography, a world in which the player can toodle about indefinitely. Often this is a literal geography and relies on extensive state-tracking both explicit and secret for narrative progress. But it’s not an uncommon mode for things with assumptions grounded in the hypertext-novel idiom – static but non-linear works like Le Reprobateur. Effects: This is often used as an imitation of the default style of parser IF, although some may be parallel derivation from the former’s D&D roots. As with classic map-based parser games, the narrative tends to become slower-paced and less directed; the player has more leisure to explore and grasp the world, but spends less of their time advancing the story. Examples: Duelmaster; Chemistry and Physics. Sorting Hat. The early game branches heavily and rejoins heavily (branch-and-bottleneck is a likely model here), ultimately determining which major branch the player gets assigned to. These major branches are typically quite linear – sometimes they look like gauntlets, but they might be choiceless straight-shots. Sorting Hats almost always rely substantially on state-tracking in the early game, and often bottleneck at the decision point. Effects: The Sorting Hat is a compromise between the breadth of more open formats and the depth of linear ones. Sometimes the nature of the various branches is signaled to the player; this is kind of important, in fact, because the player is pretty likely to notice the linearity of the second half and might assume that all of their choices will ultimately get funneled into that particular thread. The player gets a lot of influence over how the story goes; however, the author may end up effectively having to write several different games. Examples: Katawa Shoujo; Magical Makeover. Floating Modules. A mode only really possible in computer-based works. There is no tree – or, while there may be scattered twigs and branches, there’s no trunk. No central plot, no through-line: modular encounters become available to the player based largely on state, or perhaps randomly. Effects: This is a challenging style to write for, both because it’s difficult to intuitively grasp – writers tend to rebound quickly to a more unified structure – and because few assumptions can ever be made about prior events. Without a large amount of content, the method tends to collapse into a linear system. Because play mechanics are largely about altering stats in order to negotiate a world, there’s a strong incentive to expose those stats to the player; repeated events chosen only to affect a stat (grinding) may be a feature. There are different approaches to floating-module: Emily Short has a tentative categorisation here. Examples: Pure examples of modular design are relatively rare. King of Chicago (hat-tip Sean Barrett) is an early example. StoryNexus and its conceptual relatives (e.g. Bee) inhabit this space, though they generally impose some more linear-progression structure on it. (Alexis Kennedy uses the term quality-based narrative to describe the general approach: ‘pieces of story like mosaic tiles, not pipes or complex machinery.’) Loop and Grow. The game has a central thread of some kind, which loops around, over and over, to the same point: but thanks to state-tracking, each time around new options may be unlocked and others closed off. This is a very general pattern, and can co-exist with many others. Trapped in Time, for instance, is basically a cycle-and-growing Gauntlet; Bee tames its floating-module nature with a year-long loop structure. Effects: Loop and Grow emphasizes the regularity of the world while retaining narrative momentum. A justification is needed for why whole sections of narrative can repeat: the player-character is often following routine activities in a familiar space, engaged in time-travel, or performing tasks at a certain level of abstraction. This regularity often comes at the price of openness: many stories with a strong Loop and Grow structure involve a struggle against confinement or stagnation. An important variation of loop-and-grow structures is spoke and hub: the game has several major branches, but they all originate at and return to a central node or set of nodes. The player may go out along each spoke once, or many times. Examples: Bee, Trapped in Time, Solarium. Share this: Twitter Facebook Like Loading... Related This entry was posted in cyoa, interactive fiction and tagged choice of games, Failbetter, structure. Bookmark the permalink. ← Year Without Zombies 2: The Dead of Winter Never Alone / Kisima Inŋitchuŋa → 96 Responses to Standard Patterns in Choice-Based Games ← Older Comments Pingback: Design Pillars – Part III: Narrative Choice, & Consequence – Forged of Blood Pingback: IF Only: All about SettingRock, Paper, Shotgun Pingback: Choose Your Own Research Adventure: A Resource Guide for the UBC Information Literacy Tutorial – visualibrarian Pingback: Small-Scale Structures in CYOAEmily Short's Interactive Storytelling Pingback: Don't Miss: 6 techniques to make game narratives more dynamic - Arcade Bulletin Pingback: Writing Interactive Fiction on Twine - Nicole Basaraba Pingback: Week 6 – Dramatic Elements in Games – Datt 2300, Game Development I Pingback: IF Only: dressed for the partyRock, Paper, Shotgun Pingback: Update, and Spring Thing previewHannah Powell-Smith Pingback: Mailbag: Teaching Spatial StorytellingEmily Short's Interactive Storytelling Pingback: Standard Patterns in Choice-Based Games Pingback: Choose Your Own AdventureMath Illustrated Pingback: The Ultimate Guide to Video Game Writing and Design (Dille/Platten)Emily Short's Interactive Storytelling Pingback: Friday Grab Bag: Free Zines, Goodies for 5e, and Mad Lib Dungeons – die heart Pingback: Firewatch – Estructuras Narrativas Pingback: Montage, Narrative Deckbuilding and Other Effects in StoryNexusEmily Short's Interactive Storytelling Pingback: Standard Patterns in Choice-Based Games – ShareMeTale Pingback: Choices, Structure, and the Story Will Not Let Me Go - Stephen Granade Pingback: Mid-December History and Games LinksGaming the Past Pingback: Bryonie Bickford’s Histrionic Positronic Habiliments – Colin S Stricklin Pingback: À quoi sert une variable dans un jeu à embranchements ? – Fiction-interactive.fr Pingback: Making Interactive Fiction: The Branch and the Merge - sub-Q Magazine Pingback: Writing for Video Games (Steve Ince)Emily Short's Interactive Storytelling Pingback: Twine Story Clustering – Datalexic Pingback: Introduction aux structures de la fiction interactive, et comment les utiliser – Fiction-interactive.fr Pingback: Taxonomy of Narrative Choices – Vagrant Cursor Pingback: Going Interactive or: How I Learned to Relax and Let the Reader Take Control – The Dream Foundry Pingback: Raid on the Silver City – editing phase – Nick Petrou's Writing Blog Pingback: Mount St. Helens (1980)Renga in Blue Pingback: ID and eLearning Links 7/5/19 - Experiencing eLearning Pingback: Links and Structures from Michael Joyce to Twine – Emily Short's Interactive Storytelling Pingback: House of Danger: CYOA book to tabletop gameThese Heterogenous Tasks Pingback: Introcomp 2019These Heterogenous Tasks Pingback: IntroComp 2019: reviews! – Felicity Drake Pingback: YES/NO Game Prototype – Rabbitoreg (Zsolt Olah) Pingback: Branch and Bottleneck Scenario Structure - Experiencing eLearning Pingback: Storylets: You Want Them – Emily Short's Interactive Storytelling Pingback: What's the difference between a constraint and a state for Upton? - Custom Writings Papers Pingback: Twine 01 – Story Generation using Twine Pingback: Narrative Design [WIP] – Mat Wright Research & Development Journal ← Older Comments Leave a comment Search for: Recent Posts IF Comp 2024: Birding in Pope Lick Park (Eric Lathrop) IF Comp 2022: The Thirty Nine Steps IF Comp 2021: Goat Game IF Comp 2021: The Last Night of Alexisgrad IF Comp 2021: The Mermaids of Ganymede Recent CommentsKathryn Li on IF Comp 2021: Goat GameMilo van Mesdag on IF Comp 2021: The Last Night o…George Parr on Eastshade: Fantasy Without…Narrative Design [WI… on Standard Patterns in Choice-Ba…Michael on House of Danger: CYOA book to… Archives September 2024 October 2022 October 2021 September 2021 March 2020 November 2019 October 2019 September 2019 August 2019 July 2019 May 2019 November 2018 October 2018 July 2018 May 2018 November 2017 October 2017 September 2017 August 2017 July 2017 June 2017 February 2017 January 2017 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 October 2012 April 2012 November 2011 September 2011 August 2011 May 2011 May 2010 October 2009 September 2009 August 2009 July 2009 June 2009 March 2009 February 2009 December 2008 June 2008 April 2008 December 2007 October 2007 September 2005 Categories board games cyoa interactive fiction parser-based review rpg storygames Uncategorized videogames Meta Register Log in Entries feed Comments feed WordPress.com Search for: Recent Posts IF Comp 2024: Birding in Pope Lick Park (Eric Lathrop) IF Comp 2022: The Thirty Nine Steps IF Comp 2021: Goat Game IF Comp 2021: The Last Night of Alexisgrad IF Comp 2021: The Mermaids of Ganymede Recent CommentsKathryn Li on IF Comp 2021: Goat GameMilo van Mesdag on IF Comp 2021: The Last Night o…George Parr on Eastshade: Fantasy Without…Narrative Design [WI… on Standard Patterns in Choice-Ba…Michael on House of Danger: CYOA book to… Archives September 2024 October 2022 October 2021 September 2021 March 2020 November 2019 October 2019 September 2019 August 2019 July 2019 May 2019 November 2018 October 2018 July 2018 May 2018 November 2017 October 2017 September 2017 August 2017 July 2017 June 2017 February 2017 January 2017 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 October 2012 April 2012 November 2011 September 2011 August 2011 May 2011 May 2010 October 2009 September 2009 August 2009 July 2009 June 2009 March 2009 February 2009 December 2008 June 2008 April 2008 December 2007 October 2007 September 2005 Categories board games cyoa interactive fiction parser-based review rpg storygames Uncategorized videogames Meta Register Log in Entries feed Comments feed WordPress.com These Heterogenous Tasks Blog at WordPress.com. Privacy & Cookies: This site uses cookies. By continuing to use this website, you agree to their use. To find out more, including how to control cookies, see here: Cookie Policy %d Design a site like this with WordPress.com Get started",
    "commentLink": "https://news.ycombinator.com/item?id=42678647",
    "commentBody": "Standard patterns in choice-based games (2015) (heterogenoustasks.wordpress.com)217 points by Ariarule 18 hours agohidepastfavorite61 comments dejobaan 16 hours agoThis is great. I've been a game dev for about 30 years, much of which I've spent working with narrative design/writing teams. One thing I've learned to watch out for, especially among junior designers, is what the author labels the \"Time Cave.\" Narrative branching, done well, is fantastic—it gives the player agency and lets them make the story their own (as it were). But when you're creating the story graph, it's easy to get lost in it and lavish care on one path at the exclusion of the others. You can easily end up with one or two long, greatly-detailed paths, and (because dev time is finine, and you need to move on to writing other parts of the game) a pile of other paths that are shorter and less interesting. If the player takes one of the shorter ones, they end up missing out on all your coolest stuff. The tools I would design for the kinds of games I created specifically made it easy to create a main story trunk with side paths (that rejoined the trunk), and more difficult to branch/loop/etc. Of course, that's not the only (or even the best) way to do narrative design—Disco Elysium is a masterwork because it did the branching, merching, loops, jumps, random checks, and so forth, so well! reply spencerflem 14 hours agoparentYour games rule :) reply esperent 12 hours agorootparentWhat game is it? reply spencerflem 10 hours agorootparenthttps://en.wikipedia.org/wiki/Dejobaan_Games Played so much AaaaaAAaaaAAAaaAAAAaAAAAA!!! back in the day, still never 5 starred everything. Holds up IMO. Their other games are cool too reply dejobaan 5 hours agorootparentYou are too kind; thank you! reply chii 12 hours agoparentprevI think it's a mistake to try get a story-focused game to have branching paths, akin to the old choose-your-own-adventure books. Until LLMs can proactively create new stories for the player to enjoy dynamically, i think it's always fraught with peril that the player fails to get the full story (or have to repeatedly play it and choose something else to try). My personal preference would be to have a single, on rails story, where the player don't truly have a choice. It's an interactive movie. Or, pick a sandbox mechanic, and let the player do what they want directly, and compute the consequence (the most common type being the physics system). reply 0xEF 9 hours agorootparentI'm the opposite, apparently. I loved CYOA books as a kid because they could be reread, so I ended up seeking games that boasted multiple endings, including \"bad\" endings. When playing more linear games, I appreciated them for what they were, but there was a disappointment that I could not try different options along the way. I think both have a clear place in gaming, since different gamers obviously look for different things. reply ChicagoDave 10 hours agorootparentprevHumorously, this comment takes a giant swipe at 50 years of CYOA and Interactive Fiction. There are over 14,000 games listed on https://ifdb.org. Perhaps you should play some of them and adjust your perceptions. reply trothamel 3 hours agorootparentAlso node the 52,000 visual novels at https://vndb.org/v . reply arkh 10 hours agorootparentprev> My personal preference would be to have a single, on rails story, where the player don't truly have a choice. It's an interactive movie. I can't bother to play those kind of games. A movie will be able to deliver its stories a lot better than a game. But with choice and branching you get to appropriate the protagonist(s) and some story events can be a lot more impactful then. Lately I played Cyberpunk for which you have some choices in most missions and the endings hit different. If anyone involved in the DLC story is around: kudos to everyone involved in making the \"face in the crowd\" ending. You play some almost super heroic character and due to your choices (which involve betraying and killing a lot of people) you get to survive: alone and back to generic human power level. reply Sander_Marechal 9 hours agorootparent> A movie will be able to deliver its stories a lot better than a game. I don't agree. Something like SOMA would just be a generic sci-fi B-movie but it's an awesome game, even though there's no real choice and is in essence just a walking simulator. reply lmm 11 hours agorootparentprevIf you're not going to have choices matter, why make the story in an interactive medium at all? Branching paths require a lot of compromises, but there are still things you can do much better with handwritten stories than in a sandbox style. reply chii 10 hours agorootparent> why make the story in an interactive medium at all? have you not seen the success of the COD Modern Warfare franchise? Their single player game is essentially an on-rails shooter, with pivotal story points completely scripted (you \"press the buttons\"). There's no choice, there's no branching (of the story). But people like to shoot, like to run around, etc. It feels like they have control, and it feels like the heroics in the story is their contribution. reply mnky9800n 10 hours agorootparentHalo is the same. It is essentially a very long hallway with enemies to take care of before you can move to the next hallway. Also, I recently played through the first Halo again and it was still quite fun. reply lmm 10 hours agorootparentprevI thought we were talking about a story-focused game, which that is not. reply watwut 4 hours agorootparentprevYou skip through the \"pivotal story points\" and ignore them. reply zelos 9 hours agorootparentprevIsn't that dismissing 90% of games? The story can exist purely to give emotional context to the action of the game. reply lmm 9 hours agorootparentI meant in the context of a story-focused game, which is indeed less than 10% of games in general. reply zelos 9 hours agorootparentOh, in that case I agree then: linear story-focused games feel like the developers misunderstood the concept of 'game'. reply spencerflem 9 hours agorootparentThey're good too! See: the 'walking sim' genre ( Done poorly yep, and i argue that most game devs wouldn't do it well enough to justify it. The limited time and resources available means they're almost certainly better off not branching, but make one good main story and polish it. Take a look at the examples you listed - they're all award winning games, from developers with serious experience, grit and determination to make the best game. They spent ages, and lots of resources to do it. Even big studios, with similar or more resources, fail at making such branching good. It's a folly to think that a smaller game developer with more limited resources could make it better. Disco Elysium is almost an exception that proves the rule (or another example is Pathologic). reply Alex-Programs 2 hours agorootparentprevI felt that The Outer Worlds did quite a good job, too. From the devs who built Fallout New Vegas. Funny little game. I find it quite charming in its eccentricities. reply spencerflem 12 hours agorootparentprevimo its not necessary to get the \"whole\" story, or replay it to see every possibility just seing \"my\" story is enough interactive movie type games are great but they're a different experience from choice style games which are also great, (and ofc the article shows that there's many styles within this too, all with a different experience) I don't believe LLMs can recreate the same authored experience that has a point of view. I think they'll be okay at genre work soon enough though, for better or worse. But thats not a type of game I'm personally interested in. reply scotty79 4 hours agorootparentprevI think a good idea is to have a tree-like checkpoint-save system so you can always go back to the state you were in before descending down into a branch and go down another one without replaying everything up to this point. It encourages replays and exploring all content. Papers please had something like this. reply chii 4 hours agorootparentand if i recall correctly, also Detroit: Become Human It also tells you the % of people that managed to reach a certain ending or branch. reply watwut 4 hours agorootparentprevI strongly disagree. I was actively preferring those games and found it fun to try out different endings. > My personal preference would be to have a single, on rails story, where the player don't truly have a choice. It's an interactive movie. I stopped to play those games. Movies are better at being movies then games. reply toast0 2 hours agorootparent> I stopped to play those games. English nit, if you mean you no longer play those kinds of games, as I think you do baaed on context, you should write \"I stopped playing those games.\" reply suddenlybananas 12 hours agorootparentprevI completely disagree, there are plenty of branching games which are extremely good and which would be severely worse if they weren't branching: Disco Elysium and Baldur's Gate 3 come to mind. We are very very far off from an AI being able to come up with compelling stories that are logically coherent. reply chii 10 hours agorootparent> there are plenty of branching games which are extremely good i think you mean there's barely any good ones. The examples you come up with are the exceptions that prove the rule. Look at a game like Dishonered, where the story have _some_ branches, but it's half-assed imho. There's plenty more games where having gone for a branching story made the game more expensive, less deep, and harder to sell as a result. reply tunesmith 14 hours agoprevThis is fun, and holds true in the creative writing group I run. We use a website I programmed that helps us collaborate on writing branching fiction. We have a mapping utility that creates graphs like in the article, except more animated (d3.js, elkjs). As different authors can start their own new stories, one thing I often have to deal with is that they want to design their story to have both long path lengths (multiple chapters before an ending), and also high choice count. Those of you who know something about geometric series know that this causes problems. I often have to tell them they can't have everything they want, which causes minor drama. :) As a result, one of our stories basically shot its \"choice budget\" in the first few chapters, leading to many linear paths in the latter parts of the narratives, which is fun in its own way. Another of our stories has just started playing with the \"gauntlet pattern\" as the article describes. For this one, we decided that all chapters must be in the \"same universe\", just following different characters' perspectives, and are planning for certain \"anchor chapters\" where all characters come together for a meeting. Probably the detective questioning them as a group (it's a murder mystery). All of our stories are supposed to be literary, so usually in third person, sometimes first, never the second-person. So we don't tend to use choices and chapters as directions and rooms; it's all about how the plot moves. We also don't track state; they're designed to be able to be printed as books people can page through. Overall a super-fun project for me and a handful of other writers, it's been a consistent way to spend a few hours of fun each week. reply withinboredom 11 hours agoparentYou can still track small state via the reader. I vaguely remember a choose your own adventure as a kid: If you picked up the key earlier, turn to page XX Otherwise, turn to page YY It was entertaining to a) suddenly realize I had missed an important detail or b) allow me to “escape” if I can’t find the key or just don’t like that part of the book. reply HelloNurse 3 hours agorootparentI usually read text IF following all paths at once, with heuristic combinations of breadth-first and depth-first search in order to maintain the unexplored front small. Not much different from drawing a map in a computer RPG. reply dasfsi 10 hours agorootparentprevOne book I read did a similar thing, but managed to do it spoiler-free. There was a magical crystal, I think, that did some magical things. When you pick it up, the book says \"To use the crystal, look at paragraph (current paragraph + 20)\" and the author actually managed to do that for the most paragraphs from then onwards reply iainmerrick 9 hours agorootparentAn old gamebook series that did lots of this is Steve Jackson's Sorcery! -- I wonder if that could be what you're thinking of? More recently, Jason Shiga has used clever mechanics like this a lot in comic book form, notably in Meanwhile. He's just finished a three-part series aimed at younger readers, Adventuregame Comics. All Shiga's stuff is great, highly recommended. reply tantalor 3 hours agoprevHere's the story map for The Stanley Parable: https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fm... Looks like a \"Time Cave\" reply orthoxerox 6 hours agoprevI don't quite understand what \"floating modules\" are. Is it something akin to sidequests in a CRPG or a \"sandbox VN\"? One interesting (and very complex) approach I've seen in VNs is multiple interleaved paths. Each path looks like a branch and bottleneck, but at certain points a decision taken on one path blocks or forces an outcome on another. You can linearize it into a single \"branch and bottleneck\" with extensive state tracking (this is how it's implemented, after all), but it's far easier to model it as multiple paths. reply numbsafari 5 hours agoprevI loved the Lone Wolf and associated books as a kid. One of my favorite things about them was seeing glimpses of possible story paths as I flipped around the book. When I would get to the end knowing I had missed some enticing possibility in the story line, I would immediately flip back to the beginning and start over. reply kelseyfrog 15 hours agoprevReminds me of Disco Elysium Explorer[1]. Conversations 7, 8, 9, and 10 are great real life examples. 1. http://134.0.119.41 reply flpm 16 hours agoprevThis is very interesting, thanks for posting! Makes me think of the big choice diagrams in Detroit: Become Human. I wonder if there is any literature about this? reply photonthug 15 hours agoparent> I wonder if there is any literature about this? Came here looking for the same.. some kind of map from the game design angle more towards game theory. Fun semi related tangent, I was curious to know authors background, and the About page quotes Borges “garden of forking paths” which jives nicely with tfa. Cataloging rather than inventing is an underrated activity in math sometimes, and we need to do both. Game garden taxonomy! reply the__alchemist 16 hours agoparentprevI'm curious about Alpha Protocol. Probably the #1 game I've played for choices matter. Detroit is interesting, in that it includes some choices made by passing or failing QTEs. They really did the \"You will get emotionally stomped if you screw this up!\" well in that game. I don't know its structure well, as I only played it once. (So experience only one path.) Unless you count time caves like The Stanley Parable! reply eugenekolo 3 hours agoprevThis was actually a really cool analysis. Fun to see how these things can be broken down so cleanly into graphs. reply Over2Chars 13 hours agoprevgauntlet looks like GTA V's story mode pattern. GTA2 with it's competing gangs seem to have a \"state\" tracker in the form of reputation scores with the game, while having an open world map. As your reputation/state changed opportunities would become available/unavailable. I still think GTA 2's system is impressive. reply quotemstr 15 hours agoprevNier Automata is my favorite example of the relatively rare \"Loop and Grow\" pattern. You play through the game three times, with each iteration enriching and elaborating on the story and characters. Brilliant and weird narrative structure. reply twic 2 hours agoparentIsn't Loop and Grow roughly the same as Metroidvania? Or does Metroidvania not require an actual repeating loop, whereas Loop and Grow does? reply codazoda 16 hours agoprevThis is fantastic. Does anyone have any book references that help you do writing in some of these formats. reply livrem 10 hours agoparentThere is this collection of 1980's internal design documents from Flying Buffalo that you can buy: https://www.drivethrurpg.com/en/product/296847/t-t-solo-desi... It is old, but I do not know of any other books really on this topic. I enjoyed reading it anyway. One of the documents in it describes a simple manual algorithm for how to number the sections in a reasonable way (just randomly assigning numbers is not very reasonable, as anyone will learn from reading a book where the author did that). I implemented that in a simple pandoc filter: https://github.com/lifelike/pangamebook reply egglemonsoup 16 hours agoparentprevnot sure if this perfectly addresses your question, but \"Designing Games\" by Tynan Sylvester is a great resource reply andrewstuart 12 hours agoprevNicely timed I’m making a CYOA game right now so very interested learn more. reply ardleon 9 hours agoparentHi, I've just ‘finished’ an app to make interactive text stories/games and I'm looking for people to test my app with real stories, if you're interested I could help you with your story so you can publish it online wherever you want. reply andrewstuart 1 hour agorootparentI’d give it a look my email is in my profile reply ardleon 1 hour agorootparentok, the only thing is that the application is in Spanish, at the time I did not contemplate that it was multilingual, among other things because I made it for me, if you are still interested I will write you and send you a link. reply ninetyninenine 13 hours agoprev [–] Ai can produce a new type of game where choices are dynamic and outcomes are generated by LLM agents. Fiction is an hallucination and LLMs are master hallucinators. Basically LLMs have to be given assets and game components that they can easily compose. reply ben_w 8 hours agoparent> Fiction is an hallucination and LLMs are master hallucinators. They're jacks of all trades, master of none. This has its uses, but they have limits, and for now at least, those limits are under the threshold for that. I have actually tried using them to make a text adventure to help learn German. The result was at the lower end of the quality range I've witnessed from LLM output: a nice first draft, not shippable, missing a core element, missing a lot of content, too simple, the kind of thing where you'd give the output of the LLM as a code challenge to a job candidate to see how they improve it. reply withinboredom 11 hours agoparentprev [–] Having had an LLM tell me a story, my answer would be that this is a dumb idea. LLMs have no concept of realistic cause and effect. reply ninetyninenine 10 hours agorootparent [–] Nah I think not considering this idea at all is the extremely dumb and brain dead opinion. LLMs can tell stories. Realistic causes and effects aren’t even consistent in human stories. A good story isn’t 100 percent dependent on this. The LLM walks the line between hallucinating too much and sometimes not. Either way you can pretty much guarantee that almost all stories made in games now are already mostly written by an LLM. It’s just the writing is edited and curated by a human. reply usrusr 8 hours agorootparentAs a grumpy old symbolic ai hand I do wonder if it was possible to build a (perhaps crude) ontology based simulation with consistency, cause and effect and so on and then use the results of that for prompting an LLM. But as a consumer, I lean far to the side of \"give me a handcrafted tunnel experience with the illusion of choice\" in the divide between consequences yes or no. I don't think I'd actually want this \"simulation behind an LLM facade\". If I'm in the mood for reading (or for listening to voice actors reading to me), I'd rather have it be something more meaningful than just a game state. But to those on the other end of the spectrum, this might actually be the holy grail of game building. reply woolion 8 hours agorootparentprev [–] I've done a short LLM-powered VN, and LLM actions were restricted to local interactions only because of how weak it is at making up the story. It's great at removing the parser-based interactions, but I think that's it. There's a second technical problem that such stories are represented by a form of state-machine and that you would need to recompile it on the fly, making many checks very difficult (you would need to be able to check reachability on the fly, chunk transitions, etc). I think it would take years to get to the level of some of the great IF games with an LLM, and not just a cool PoC. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post by Sam Kabo Ashwell explores recurring design patterns in choice-based games, including Time Cave, Gauntlet, Branch and Bottleneck, and others, which affect gameplay and narrative structure.",
      "These patterns influence player experience and story progression, with examples from interactive fiction illustrating their application.",
      "The post notes that these patterns are flexible and can overlap, rather than being strict categories in game design."
    ],
    "commentSummary": [
      "Choice-based games use narrative branching to enhance player agency, but this can lead to \"Time Caves,\" where some paths are overly detailed, and others are underdeveloped, causing players to miss key content.",
      "Tools can assist in creating a main story trunk with side paths that rejoin, but branching remains complex and resource-intensive, prompting some to advocate for linear stories or sandbox mechanics.",
      "Games like Disco Elysium and Nier Automata are noted for their successful use of branching, and there is discussion on AI's potential to dynamically generate story paths, though current technology may not yet achieve coherent narratives."
    ],
    "points": 217,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1736728522
  },
  {
    "id": 42678584,
    "title": "The Free Movie: Frame-by-frame, handrawn reproduction of \"The Bee Movie\" (2023)",
    "originLink": "https://thefreemovie.buzz/",
    "originBody": "MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF MSCHF INTERACTIVEFree Paint 1.0 The FREE Movie 20 WORKER BEES ONLINE ? Terms Manifesto/FAQ YOUR FRAMES » 0 (anon1738) anon#127 » JUST DREW A FRAME MOVIE PROGRESS » 60000/65244 FRAMES (92%) YOUR NAME anon1738 Draw Erase Undo Preview DONE » NO FRAME SELECTED YOU Frame 0 00:00:00:00 anon PLAY The FREE Movie A hand drawn / crowd pirated / been frame-by-frame recreation of the entire BEE Movie WATCH THE FINISHED MOVIE NEXT UP: DUNE! The FREE Movie is all done, but you can get right back to work on the next crowd-pirated flick: START DRAWING DUNE",
    "commentLink": "https://news.ycombinator.com/item?id=42678584",
    "commentBody": "The Free Movie: Frame-by-frame, handrawn reproduction of \"The Bee Movie\" (2023) (thefreemovie.buzz)217 points by gaws 18 hours agohidepastfavorite56 comments tristanj 15 hours agoShrek Retold https://www.youtube.com/watch?v=pM70TROZQsI is a similar fan-made reproduction, but it's way more polished. The movie is split into roughly 1min sections, with each one done by a different creator. The quality is all over the place, in a hilarious way. One minute it's produced by a professional animator nailing the scenes, then the next minute it's people goofing around in their backyard. reply lloeki 11 hours agoparentI believe the trendsetter was Star Wars Uncut, made of 15s slices: https://www.starwarsuncut.com/ Which I now realise I never watched in full... reply cafeinux 11 hours agorootparentI love the differences between those three movies (The Free Movie, Shrek Retold and Star Wars Uncut). Star Wars really looks like a work of love. Shrek looks like a work of love that tries it's best to not take itself seriously. Free Movie looks like a bizarre troll post. While this is coherent with each movie's tone itself, and perhaps with the time each \"demaster\" was made (I don't know, I didn't check), I feel like this is most coherent with how each movie is viewed by the community: - Star Wars is a beloved movie, with a huge honest fan base. Thus, it was remade with love and attention to detail. - Shrek is a beloved movie, but doesn't take itself seriously in the first place, and is also a meme. Thus it was remade with love, but trying hard to look like it's a troll. - Bee Movie is mainly a meme, isn't AFAIK a beloved movie, and its fan base is mostly made of memers and trolls. Thus it was low-effortly remade at this non-flattering image. reply cf100clunk 1 hour agorootparent> Star Wars is a beloved movie, with a huge honest fan base. Thus, it was remade with love and attention to detail. Agreed. Star Wars Uncut exists because devotees wanted to set the record straight regarding controversial changes in later versions, and their efforts were laudable. Still, the image and sound quality cannot match the latest, greatest studio versions, so watching Uncut versions on the newest home theatre gear means accepting the warts and all. Star Trek: Deep Space Nine's remaster was also a labour of love but with a different purpose (image clarity and upscaling): https://news.ycombinator.com/item?id=19453745 reply Freak_NL 7 hours agorootparentprev> Bee Movie is mainly a meme, isn't AFAIK a beloved movie, […] All you had to do was add one more 'e'. What a senseless waste of a pun… reply johnnyanmac 10 hours agorootparentprevBee movie has more of a cult fanbase at best. It's still Seinfeld, and the jokes aged surprisingly well. But there's only so far you can get with a story of a bee breaking out of his natural programming,followed by a bee-human romance, followed by said bee suing humanity for stealing bee-kind's honey. You can't even truly explain the plot without breaking it down into 3 episodes. It's a surprisingly good benchmark to ask \"is it better than Bee movie\" when measuring other Dreamworks movies against themselves. reply soperj 1 hour agorootparentYou forgot the part where the bees then sit around and do nothing after getting all their honey back. It's basically a diatribe against universal basic income, since without the incentive to slave away for the rest of their lives, their lives become meaningless. reply vhodges 5 hours agoparentprevThere is an episode of Bob's Burgers that did something similar to this. https://bobs-burgers.fandom.com/wiki/Brunchsquatch/Trivia#:~... reply tzs 4 hours agoparentprevSimilar is \"South Park Reanimated: The List\" [1]. From the description: > 6 years, 175 animators, 260 unique scenes, 1 copyright dispute, and a lot of love-- we are proud to finally present the South Park Reanimated Project! We are so grateful for everyone who participated and made this such an amazing recreation. We hope you enjoy! [1] https://www.youtube.com/watch?v=0QXX6AnO0nI reply wk_end 11 hours agoparentprevThere’s an extremely goofy version of RoboCop that’s sort of like this as well. https://m.imdb.com/title/tt3528906/ reply plussed_reader 2 hours agorootparentOur Robocop Remake is a scene by scene remake, made in the same spirit as this FreeBee Movie. Worth a watch with friends present: https://vimeo.com/85903713 reply aceazzameen 5 hours agorootparentprevThe Robocop remake was my first thought. Some of those scenes are hilarious. reply asabla 11 hours agoparentprevSome of the most goofiest scenes reminded me about my self growing up, recording dumb re-creations of movies/tv-shows with friends. I had a blast watching that! thank you for sharing. reply ikesau 3 hours agoprevI drew a few frames in this, but it was a real shame to see how little quality control there was in the final cut. Makes it kind of unwatchable. Progress near the end was really fast, as lots of people had caught on and were racing to get as many frames in as possible. I really don't think this concept has been worn out yet, and someone could make their own version of this with a small review step and then we'd get some fun, actually watchable hand-drawn movies :) reply Reviving1514 15 hours agoprevThis is amazing. I love that this exists. I'm surprised how negative some of the comments are. reply tyleo 6 hours agoparentI can understand the negativity because this only appeals to specific sensibilities. I admit that it’s low brow but it hits my fascination the exact right way. To me there is something special about so many people being organized around something so mundane. reply SV_BubbleTime 13 hours agoparentprevI skipped to a random spot, and lasted about 5 seconds. Two drawings of dicks, 50% scribbles, and a couple frames of just the word PENIS. Yes, this is truly a noteworthy effort. I’m as equally curious as you, just on the other side of the horseshoe. reply polonbike 12 hours agorootparentSame here. If you skip to a random part of the movie, and move frame by frame, it becomes a game of \"spot the penis\". Just one or two can be \"tolerated\", but we're far from that here, so ... My opinion is that it does not really belong in HN reply andybak 6 hours agorootparent> My opinion is that it does not really belong in HN It's a fascinating tech/social experiment. It belongs on HN because it was done and the outcome is what it is. If you actually want to watch the Bee Movie then watch the original. I doubt generating a watchable alternative was behind anyone's motivation in this. reply filcuk 12 hours agorootparentprevBecause of a penis drawing? Are we in preschool? There have been those ever since caveman times. reply robertlagrant 7 hours agorootparentI think the point is we're not in preschool, nor are we cave(wo)men, so we don't need the penis drawings. reply gopher_space 3 hours agorootparentA penis drawing isn't something that's needed, it's something that happens. Feel free to rail against them in a Kanute-like manner, but learning to cock an eyebrow and say \"yeah, that figures\" can be self-preserving. For what it's worth I too prefer the plausible deniability of Georgia O'Keefe's outrageous lies. It's a more sensible chuckle. reply robertlagrant 1 hour agorootparentNo-one's railing against anything. reply shermantanktop 11 hours agorootparentprevDid the caveman Bee Movie have penises in it? Because then you’d have a really good point. reply aceazzameen 4 hours agorootparentprevI'm sure this one had an absurdly short TTP (time to penis). reply astura 7 hours agoparentprev>I'm surprised how negative some of the comments are. It's literally unwatchable - It's 85% scribbles and \"jokes.\" reply eigenblake 14 hours agoprevWe could probably fine-tune a tiny convolutional neutral net image classifier and just hold on the last good frames for longer to cover the frames with clear trolling and nsfw images. reply BriggyDwiggs42 13 hours agoparentI think that would miss the point reply mkagenius 13 hours agorootparentNo, that point was already made. This will be a new point unlike the previous point. reply BriggyDwiggs42 5 hours agorootparentBut i like the previous point. Why would you take it from me? reply I-M-S 1 hour agorootparentIt's not taking away, a new point is by definition adding. Just like The Free Movie added to The Bee Movie. reply binary_slinger 15 hours agoprevWho is going to put each frame into image to image AI model and get out a photorealistic output and recompile? reply cafeinux 11 hours agoparentLots of photorealistic phalluses are going to be generated. reply davidmurdoch 4 hours agorootparentOnly if you're using a model trained on them reply RankingMember 3 hours agorootparentI think it would be against the spirit of the project not to do so reply dang 15 hours agoprevRelated: The Free Movie - https://news.ycombinator.com/item?id=36687399 - July 2023 (100 comments) reply kevingadd 15 hours agoprevFrom a few moments of watching, it seems like over half the crowd-sourced frames are just trolling and not actually a representation of the original film. I guess that kind of adds to the aesthetic, but I'm a little surprised they kept those troll frames... reply doubled112 15 hours agoparentFor at least a few seconds it was an odd game of phallus or frame. reply lukevp 15 hours agoparentprevYeah I agree, there should be a voting system that can kick out bogus frames and have someone remake them, would be really interesting to see what it’d look like if everyone was trying to recreate it. Kind of like rotoscoping I suppose. As it is, there’s just a ton of discontinuity because of the troll frames. reply chem83 15 hours agorootparentI imagine that trolls would eventually band together to try to steamroll the voting system like it happens on r/Place. Maybe. reply nielsole 15 hours agorootparentIf votes are attributable, such voting rings should be easily detectable reply pcblues 13 hours agorootparentprevThis is a fascinating idea. Sort of like a wikipedia, but trying to establish the truth of a single movie rather than everything we know. It would probably end up needing moderators, harassing people for funding, and pay for political purposes as well as its core reason for existing :) reply GrantMoyer 1 hour agoprevI can certainly see why many commenters here find this many dicks hard to swallow, but when I take them wholly with the rest of the work, I find they come together satisfyingly and rawly defile copyright and DRM. reply r-w 11 hours agoprevEpilepsy warning please! :-) reply airstrike 14 hours agoprevI think the word \"crowdsourced\" is conspicuously missing from the title. It would explain a lot. reply cafeinux 13 hours agoparentThey do say it's \"crowd-pirated\". reply pajko 10 hours agoprevThe bunny reenactments are cool too: http://www.angryalien.com/ reply bytematic 15 hours agoprevwould be cool without all the trolling reply smolder 9 hours agoparentBetter in concept than execution, for sure. I feel bad for the people that actually drew their frames competently. reply ternnoburn 3 hours agorootparentThe execution is part of the art of it, like, there's a statement being made about the Internet, anonymity, and humanity here. It may not be what the artist originally set out to create with their art, which I find fascinating. reply tuanx5 15 hours agoprevReminiscent of Shrek Retold reply imdsm 4 hours agoprevI clicked randomly, at 00:37:09 and got a penis reply Deprogrammer9 15 hours agoprevThanks for making me watch that? WTF reply echelon 14 hours agoprevBack when I was first playing around with AI, I built a website where people could upload fine tuned GlowTTS and Tacotron2 text to speech model weights. One of the first things users did was to emit the entire transcript of the Bee Movie: https://youtu.be/0_ToJXHnVFQ?si=qfMNuJXAHW-hWSz5 reply WesolyKubeczek 14 hours agoprevNice idea, but I’m struggling to remember myself ever having consumed so many different pictures of dicks over the course of five minutes. reply renewiltord 15 hours agoprev [–] I watched 5 seconds and it was garbage. Felt like /dev/random reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MSCHF Interactive has launched \"The FREE Movie,\" a collaborative project where users recreate the BEE Movie frame-by-frame, with 92% of frames already completed.",
      "Participants can actively engage by drawing, erasing, and previewing their contributions to the project.",
      "The next ambitious project planned by MSCHF Interactive is a frame-by-frame recreation of the movie DUNE."
    ],
    "commentSummary": [
      "A fan-made, frame-by-frame hand-drawn version of \"The Bee Movie\" has been created, following the trend of collaborative projects like \"Shrek Retold\" and \"Star Wars Uncut.\"",
      "These projects involve multiple creators contributing short segments, leading to a mix of quality and humorous results, reflecting a meme-centric approach.",
      "Opinions on \"The Free Movie\" vary, with some viewers finding the low-quality frames off-putting, while others enjoy the chaotic and crowd-sourced nature of the project."
    ],
    "points": 217,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1736728020
  },
  {
    "id": 42679679,
    "title": "Disco Elysium Explorer",
    "originLink": "http://134.0.119.41",
    "originBody": "Loading... Disco Elysium Explorer Project info This project allow you to visualize and listen dialogues from the Disco Elysium. All rights to dialogues and voice line reserved by studio ZA/UM. Inspired by Disco Reader. Search dialogues Looking for: Told by person: Any person Any person Build conversation Visualize Second language None Chinese Traditional Chinese German Korean Polish Portuguese Russian Spanish None Conversation Id Dialogue Id Legend: - transition nodes - skill nodes - person nodes",
    "commentLink": "https://news.ycombinator.com/item?id=42679679",
    "commentBody": "Disco Elysium Explorer (134.0.119.41)190 points by kelseyfrog 15 hours agohidepastfavorite99 comments SXX 11 hours agoIf someone wonder why conversation graphs look this way in Disco Elysium is because it's build using middleware called Articy:draft. Graphs is how Articy editor designed so it's only natural to visalize it this way. Since my team also made a story driven game using Articy that far less complex than Disco Elysium, but still complex. After we done it I can say for sure that whoever written Disco Elysium is trully insane, in a good way. While Articy is a good tool it's have own set of problems specifically awful performance on huge projects and I can't even imagine how developers of Disco Elysium managed to accompish it. So I pretty sure Disco Elysium isn't just masterpiece of storytelling and writing, but also insane technical achievement of whoever managed to get it all working together using tools they had. Even on a project of much smaller scale getting it all debugged, loop-free and free of tons of logical errors is very hard. Knowing how much variables Disco Elysium have I truly believe developers are all geniuses. PS: It's just everyone always wonder at how cool and complex modern rendering is or performance in game with millions of objects, but very few people understand how hard is to create story-driven game if there is more than 10 variables and not just int skill checks. reply polytely 11 hours agoparentIt's so impressive, similarly I also love the game Pentiment by Obsidian, some incredibly complex branching conversations in there. I recently watched a video where the game director Josh Sawyer (of fallout new vegas fame) walks through some conversations in Obsidians own dialog software. https://youtu.be/u9WFmMY5oyY?si=ruJcmYey8hAiKWDR&t=640 reply Topfi 10 hours agorootparentCan also wholeheartedly endorse Alpha Protocol, also by Obsidian. A must play for anyone who feels RPGs are more than a few skill trees and binary choices, the way AP manages to interweave a massive complex conversations tree with the way players handle the action sequences is frankly second to none. To quote Ben Yahtzee Croshaws review[0] from back then: “For once, this is a game that claims that \"every action has consequences\" and actually means a consequence more significant than a character maybe wearing a different hat. For example, although the hub-based mission system lets you do the operations in any order, during the one I chose to do last, an informant mentioned the previous operations I'd completed in conversation. \"Fuck\", I said, \"this game's just showing off now.\" So I immediately became an aggressive ponce and slammed his head into a desk. After which, there was more security in my next mission because the informant went crying to his big brother or something.” [0] https://youtu.be/K9ZkY-6M5qw?si=IWrCsJqo35p25n23 reply AdmiralAsshat 4 hours agorootparentAlpha Protocol is a very flawed if intriguing game. The player needs to understand that it's more RPG than Metal Gear Solid. If you invest all your points into stealth, you can perform superhuman feats of cloaking by the end of the game. By the same token, however, you can perfectly line up a headshot on your aiming reticle and still miss, because the backend dice-roll said you missed. With that said, I agree, the dialogue choices and \"consequences\" blew away everything else in the genre. In that sense, it felt like it better captured the spirit of the original Deus Ex over even its spiritual successor, Deus Ex: Human Revolution. At the end of the first mission in Alpha Protocol, you confront a terrorist, and can choose to either kill him or let him go. If you let him go, he becomes a contact that you can call upon in later points of the game. He might give you information, assist on a mission, etc. It has a real payoff for keeping him alive. In Deus Ex: Human Revolution, you are also confronted with a terrorist at the end of the first mission, and can decide to kill him or spare him. If you decide to spare him, he shows up in an alley later in the game and...gives you some money. That's it. DE:HR was the far more polished game, overall, but it didn't understand \"consequences\" very well. reply polytely 9 hours agorootparentprevYeah I love Alpha Protocol, did you see the remaster they put out (DRM free!) on GoG last year, for me it was honestly one of the most exciting things that happened in gaming last year. https://youtu.be/UBXbrofwKwM reply Topfi 9 hours agorootparentHonestly the first time I ever bought a game right on release day, cause I knew of the quality and wanted to support both GoG and hopefully in a small way signal to the industry that I want more of this in gaming. reply jfengel 5 hours agorootparentprevCoincidentally, I just got Pentiment, and I've spent the entire weekend on it. I'm not really a gamer, and I'm enjoying the heck out of it. The art is gorgeous. reply czk 4 hours agorootparentEnjoy it! One of my favorite games of the past couple years. Loved it reply jfengel 3 hours agorootparentThank you! It's not much of a \"game\"... which is fine with me, because I'm not much of a gamer. When I'm done I may go try Disco Elysium again. I tried once before, and got myself to an ending very, very quickly -- one that my friend had no idea was even an option. reply polytely 2 hours agorootparentI would also recommend Night in the Woods, it was a big inspiration for the 'gameplay' of Pentiment. mostly just walking through a world, talking to people with some minor minigames thrown in. reply lou1306 5 hours agoparentprevIt is a great achievement in story-driven games, but there are still glaring ways in which you can get softlocked, and hilarious ways of \"winning\" it in less than 1 hour. I am pretty sure one could implement a mechanized, exhaustive checks for this kind of issues (well, at least the game-breaking soft locks) but no-one ever cared so far (: reply pbalau 6 hours agoparentprevNot related, but by googling for Articy:draft I've found there is a new Syberia game. reply geor9e 13 hours agoprevIs something more supposed to show up? Am I supposed to figure out what to do? https://i.postimg.cc/3NWtz6HF/image.png reply camtarn 13 hours agoparentIt's not a great UI, especially on mobile. Start by expanding the Search Dialogs section and type in a search term, then hit Search. Then click on one of the numeric results. Nothing will happen, but if you now expand the Build Conversation section, this will have filled a Conversation ID onto the appropriate box. Or you can input an arbitrary conversation ID into the Build Conversation / Conversation ID box (kelseyfrog posted some good ones). Either way, if you now click Build Graph, you will get a conversation graph on the right. Red nodes are you speaking, or the voices in your head speaking to you. Yellow nodes are other people speaking (your non skill based voices, like Ancient Reptilian Brain, are also yellow). Blue and purple nodes are flow control (setting and reading variables, skill checks, jumping to other nodes). You can zoom into the graph and click the nodes - for red and yellow nodes, this displays the conversation text in the left sidebar, and lets you listen to the audio. The graphs don't show actual skill check difficulties, just the results of them being read, and don't seem to show health/morale damage or items being gained/lost. Quite a remarkable tool, despite the UI. reply gazook89 13 hours agoparentprevIt doesn't seem to work on Firefox at all. Not sure if that is what you are bumping in to, but I had to switch to Chrome. reply maxhasbeenused 10 hours agorootparentI entered some random stuff and a dialogue graph showed up for me in Firefox release 134.0 and some other firefox-based browser. Not sure how it works exactly though. You sure you tried the exact same input for Chrome and Firefox? reply gazook89 40 minutes agorootparentAh, It is the LocalCDN extension that was blocking some stuff. The dropdown menus weren't working at all. Sometimes I forget to check that. Thanks for the prompt to go back and check. reply DanHulton 13 hours agoparentprevSame thing happening here. I think it might not be all-browser friendly. reply ChrisArchitect 12 hours agoprevhttps://en.wikipedia.org/wiki/Disco_Elysium > Disco Elysium is a 2019 role-playing video game developed and published by ZA/UM. reply hiimshort 11 hours agoparentFor those unfamiliar with the game I would highly recommend it if you are interested in CRPG games with excellent writing. There is a lot of text in this game, but with the most recent version of the game most of it is voice acted. Many lines will stick with you later. It's rare to not be taken by something in the game, as expansive as it is. For a more general description of the game: you are a detective, you must solve the case, and your fractured psyche will not let you do it alone. reply PetitPrince 10 hours agorootparentIt's also an atypical videogame in that there's no significant combat mechanics (combat is as rare and exceptional as in real life), and the setting is far from your usual videogame setting (it feels like like alternate history mid-to-late 20th century Eastern block, but it's entirely fictional) and themes (ostensibly a detective game, but leans heavily into political commentary, morality, nostalgia). Despite the bleak setting, it's also super funny at times in both subtle and unsubtle ways. reply k__ 9 hours agorootparentHonestly, I tried it, but I didn't understand the game. reply yoyohello13 2 hours agorootparentThe game is about talking to people, that's pretty much it. If you treat the game like an interactive novel it makes sense. The skills and stuff really don't matter, just pick something and enjoy the ride. reply Etheryte 8 hours agorootparentprevIn my opinion that's part of the game, intentionally so. You're a drunk detective who wakes up from a wicked bender and then has to go figure out how to solve a messed up case. Figuring out how do you even do that, trying and failing, is a great simulation of how being a detective actually is. In other words, figuring out how to play the game is a part of the game. reply MacTea 8 hours agorootparent> figuring out how to play the game is a part of the game. Well said! I'm 5 hours in the game. This is my third time \"trying\" the game. I'm glad I stumbled on some random walkthrough on YouTube and really got to know what's in store for me. reply SketchySeaBeast 4 hours agorootparentprevIt took me about a half dozen tries for it to click with me, but I unfortunately have no idea what it was that caused things to click. I felt like I was banging my head over and over against a wall until I wasn't. It seems like it's one of those high effort, high reward things, but I can't figure out what the effort is. reply voidUpdate 10 hours agorootparentprevWhats the C in CRPG? I know JRPG but not CRPG... reply duohedron 10 hours agorootparentComputer RPG, as opposed to tabletop RPG. So technically JRPG should be JCRPG. reply bakuninsbart 8 hours agorootparentOriginally yes, but I think today it rather means \"complex\". reply failrate 4 hours agorootparentNo, it means \"computer\". reply dlevine 13 hours agoprevI played Disco Elysium when it came out and enjoyed it. In particular, I thought the Inland Empire skill was pretty awesome. I can't imagine what the game would be like without the ability to talk to inanimate objects. reply lacker 13 hours agoparentI took no Inland Empire but I did love Encyclopedia. Guess I'll have to give it another run.... reply Rodeoclash 12 hours agorootparentI loved the way that you'd pass encyclopedia checks in the game and it would give you totally irrelevant information. I think at one point a character tells you she's using a contact mic and the skill check informs you about a boxer called \"Contact Mike\". reply freeone3000 12 hours agorootparentOr in other cases, completely relevant information, like the complete life history and works of Doloros Dei, which is simply unactionable as she has been dead for decades and the church is long-abandoned too. reply lanternfish 11 hours agorootparentprevContact Mike is one of the most important encyclopedia checks in the game, honestly. Its one of the few direct hints you get to HDB's backstory before the literal last scene, which can actually give you actionable insight in a couple scenes IIRC. reply alickz 6 hours agoparentprevi miss my crazy necktie reply red-iron-pine 4 hours agorootparentwho tells you to arrest people and take drugs. some friend that is. reply carterschonwald 6 hours agoprevThe story of how the game assets of the developers were stolen via a malicious investor is kinda crazy reply firtoz 5 hours agoparentIs it this one? https://www.reddit.com/r/OutOfTheLoop/comments/1auprxl/whats... reply yoyohello13 2 hours agoparentprevThe story of this games development would fit perfectly into this game. reply npteljes 8 hours agoprevVery impressive game, one of my all-time favorites. Felt a bit personal because of own struggles with alcohol too. I'm happy that I bought it after they added the voice narration to it, the narrators do a fantastic job bringing the characters alive, and I loved that I could spend in-game points to specialize in insane skills like \"being in tune with the city\". I'm happy that people talk about the game from time to time, I hope it gets to as many folks as possible. reply ireflect 12 hours agoprevLove that this is hosted on a bare IP address. Hack on! reply wayvey 6 hours agoparentSame, there's something mysterious and intriguing about navigating to bare IPs. reply red-iron-pine 4 hours agorootparentsomething that also managed to trip my corporate firewall reply duskwuff 14 hours agoprevAnother, differently formatted explorer for this data: https://fayde.co.uk/ reply koromak 4 hours agoprevAnd then imagine how each separate converstation can affect another. What a tangled mess this would be to work on. reply adamnemecek 15 hours agoprevIf you haven’t played Disco Elysium, please do, there’s a reason people are so obsessed with it. It’s a novel really. reply ahartmetz 11 hours agoparentI don't like anything about the game - writing, art style, characters, UX, voice acting, main storyline... so that opinion exists, too. reply notahacker 8 hours agorootparentIn the middle opinions also exist. I thought the worldbuilding and their spin on conversation mechanics was top tier and liked the art, but I also thought that it was oversold with the \"open world... let you do almost anything\" marketing when the reality is closer to \"you're an alcoholic cop character with a defined backstory, who must solve a mystery with a defined ending by negotiating your way through defined checkpoints, but you can act differently towards the characters you need to pump for information and take on a couple of side quest options if you want\" reply npteljes 8 hours agorootparentOh absolutely. I'm entranced by the game, but I wouldn't emphasize that it's an open world, I think that sets unrealistic expectations. It's rather that it's much deeper than the usual point and click games, more like an interactive novel, and with features that let you put different spins on the story. I think if the expectation is a point and click adventure, then the game can deliver, open world is more like Red Dead Redemption. reply esperent 11 hours agorootparentprevI definitely liked some things about the game. But what I didn't like is that it's incredibly depressing. There's not enough wry humor (even though it's good humor) to cover up the fact that it's a terribly sad story about a man who's alcoholism has destroyed his ability to comprehend reality. I made it about ten hours in, and deciding to stop there felt like opening a window onto a bright sunny day after being stuck in a menial office job. reply egeozcan 11 hours agorootparentprevI could play for 5 hours then gave up because similar reasons. In situations like this I usually think it should be an acquired taste, but in this case, I'm pretty much confident that I'd have never liked it. UX was the biggest pain point for me BTW. reply red-iron-pine 4 hours agorootparentsame. the entire thing could have been done similar to a renpy-style visual novel and generally played out the same. the dialog, trees, and story were the game, everything else was kinda clunky reply afterburner 10 hours agorootparentprevI thought it was a drag having to schlep around town for a point and click game. reply me_me_me 1 hour agorootparentimagine that, in a detective game you are required to poke around and TALK! to people. Crazy idea. reply Freak_NL 10 hours agorootparentprevYou're not alone. From the description it should have been right up my alley — avid reader, including Philip K. Dick, can appreciate weird stuff — but something about the way the protagonist was interacting with other characters sat wrong with me. Perhaps it was the pretending everything was alright whilst suffering from (something induced) amnesia? I'm not sure what exactly irked me. I didn't feel like I wanted to dive into this world. The UX felt underwhelming too. Perhaps I'll look into it again in a few years time, when I finally finish Factorio. reply getwiththeprog 8 hours agorootparentOnce the game started psycoanalysing me I had to put it down. I like reading P. K. Dick, not being a part of his world. reply vandahm 3 hours agorootparentprevI love the game, but I know a lot of people who don't, for a variety of reasons. They were very opinionated about what kind of game they were trying to make, so you'd expect it to resonate strongly with some people and not at all with others. reply pesus 15 hours agoparentprevSome of my all time favorite writing, regardless of medium. reply jakevoytko 14 hours agorootparentI believe that this game is an artistic masterpiece. On the first playthrough when I woke up with amnesia, I simply played a cop with amnesia in all of my responses. It was goofy, I annoyed Kim for fun, I beat the game, etc. On the second playthrough, I chose all of the dialogue options that honestly explained how the character became an alcoholic. It changed the vibe of the whole game to somber, and I was fascinated that the relationship with Kim felt different. It also felt natural to explore all of the lore and sidequests of the game involving the pale in this mood. And then I went on a completionist binge and found some great social commentary in the \"high-net-worth individual\" sections. reply jimmygrapes 13 hours agorootparentAt risk of saying too much... I am playing it again right now and am choosing a path of sobriety and contrition and it's been making me think, pray, and cry every night. I'm working on it. reply pdpi 12 hours agorootparentprevThe writing is an exercise in how closely you can toe the line of turning into purple prose without ever crossing it. It's clearly written by somebody who likes words, for other people who like words. reply Toutouxc 11 hours agorootparentOne of the reasons I played using the \"Psychological\" voiceovers (where the characters are voiced, but not the inner monologue). The skills were to me the purplest element, and the voice acting differed too much from my imagination. reply pdpi 11 hours agorootparentYeah, it's definitely not for everybody, but the narrator/inner monologue VA work is one of my favourite things about the game. reply ajmurmann 13 hours agorootparentprevVery much agreed. When Harry met a certain creature I cried a little. The text and music were so moving. reply pm3003 14 hours agoparentprevI've seen people use it to learn colloquial French. I'll try othetr languages. reply OnionBlender 13 hours agoparentprevI don't know why I haven't played this yet. I absolutely loved Planescape: Torment and Fallout 1 & 2. I already own Disco Elysium. I've just never booted it up yet. reply tapoxi 13 hours agorootparentIt's such an incredible science fantasy universe, probably my favorite piece of fiction from the past decade. The political history, nature of reality and shape of the world, even things about how computers (\"radiocomputers\") work are all fascinating to me. It's a shame what happened to the studio. There deserve to be more stories told in that universe. reply duskwuff 13 hours agorootparent> It's a shame what happened to the studio. And there's an extremely funny winking self-reference in the game: https://fayde.co.uk/dialojue/4600609 For context: ZA/UM was, for a time, called Fortress Occident. reply mariusor 12 hours agorootparentZA/UM is also the name of a physical artefact in the Disco Elysium universe. It gets used in the novel Kurvitz wrote: Sacred and Terrible Air. reply camtarn 13 hours agorootparentprev> ZA/UM was, for a time, called Fortress Occident Hah! I've spent some time reading trivia about the game recently, but I hadn't come across this. That makes a lot of sense. reply armoredkitten 2 hours agorootparentprevI think, even though they are very different games, Disco Elysium has a lot of the same feel as Planescape: Torment. A lot of the same introspection of human nature, philosophy, moral judgment, and wry humour. If you like one, I think you'll like the other. reply braden-lk 14 hours agoprevThis game is right up my alley but it’s so engrossing I always forget to save and lose hours of my progress. reply frontalier 10 hours agoparentit has auto-save?! reply whazor 9 hours agorootparentThough there are dead ends, thus needing to save earlier to undo decisions. reply Etheryte 8 hours agorootparentThere are no dead ends, but it is possible to play yourself into a state that looks like one. Without spoiling anything I would wager you haven't discovered a mechanic that's in the game yet. Figuring out how to play the game is a part of the game. Source: I thought I hit a dead end, but found a different way to approach the problem after a few days of being stuck. reply camtarn 6 hours agorootparentThere is, as far as I can tell, one dead end: if you run out of money near the start of the game and have exhausted your options for getting more, then you'll have to revert to an earlier save. It's not particularly easy to do though, in my opinion. reply tuyiown 10 hours agorootparentprevyes reply harel 8 hours agoprevI really wanted, and tried so hard to like this game. But it just didn't work for me. This visualisation though it very nice and I do wonder what was used to create the graph canvas UI? reply kelseyfrog 15 hours agoprevConversations 7, 8, 9, and 10 are a great place to start. reply lelandfe 12 hours agoparent495 for one of the rarest ones I've found in the game. Needs 9 Perception on Day 1 to do: https://youtu.be/w9aDmCuyU-4?t=10 A preposterously complex dialogue web for a weird throwaway conversation so few would ever come across. Love this game. reply kevingadd 15 hours agoprevHaving played the game before and sincerely appreciated it (plowed through the whole 25 or so hours in 2 days), I had no idea the flowcharts for the conversations were quite so complex. It makes sense though, it's a very dynamic game. reply renegat0x0 11 hours agoprevWhy the IP though? Is that link safe? reply unkeen 11 hours agoparentWhat would make it more secure if it were a domain that gets resolved to an IP address? reply TomasEkeli 10 hours agorootparentgiving it a domain-name and serving with https encryption on it would improve all kinds of security. then again, it feels wonderfully apt that it is on some random ip reply Etheryte 8 hours agorootparentSecurity of what? You're not inputting any data of your own into the site. reply derelicta 9 hours agoprevI, for one, am a proponent of Mazovian socio-economics. reply lz400 10 hours agoprev [–] I wouldn't say I didn't like Disco Elysium, in fact I really enjoyed it in the beginning, when it was more about the case and less about politics. In theory it was really my thing: focus on story, mature, a sense of humour... Planescape: Torment is probably still my favorite game of all time and this looked up that alley. Alas, once the game gets going I found Disco Elysium way too preachy and it completely lost me. Then I saw that it was scripted in part by the guys in el chapo trap house, which I find insufferable and it all made sense. I really really wish they had done away with all the political stuff, it could have been so good. reply miunau 10 hours agoparentYou think Planescape: Torment is non-political? LOL reply PetitPrince 9 hours agorootparentAgreed: Planescape's faction represents ideologies, philosophies and world (... multiverse?)view and societal view that's political by nature. Their label are just not a 1:1 match with our world. And in Disco Elysium (in least in my playthrough and my impression) everyone gets criticized equally at some point, even though the writers are nostalgic communist. But it doesn't feel to me like blatant propaganda. reply Levitz 3 hours agoparentprev>Alas, once the game gets going I found Disco Elysium way too preachy and it completely lost me. Then I saw that it was scripted in part by the guys in el chapo trap house, which I find insufferable and it all made sense. I really really wish they had done away with all the political stuff, it could have been so good. The union workers are depicted as useful idiots for corrupt scumbags at best and parasites at worst, communists as delusional losers, the moralintern as ultimately functional, yet brutally oppressive. The script does very clearly come from a left-wing side of things, but I feel it shows the most when it comes to leftist ideologies which are brutally criticized. Funny enough, one of the most liked characters is the unfettered capitalist,which is depicted as far more humane than the general ideologue in the game. I must assume that you disagree with some or all of this? reply me_me_me 1 hour agorootparentIt feels to me that people who don't like it are allergic to political opinions. They want to exist in world where people have no opinions and disagreements, Disco's world is an amazing melting pot of ideologies, history and clashes of those. reply randomcarbloke 9 hours agoparentprev [–] I agree, if it wasn't so unbearably and unashamedly explicit in its support of one particular ideology I'd have probably really enjoyed it. reply npteljes 8 hours agorootparent [–] What ideology do you think it explicitly supports? I really liked the game, but I thought that it showed character stories, and not particularly an overarching ideology, and now I wonder if I also support something that I don't even know of. reply JansjoFromIkea 6 hours agorootparent [–] I'd say it's fairly explicit that it was written by people with leftist sympathies and I assume that's what they meant. I don't get how anyone could find their portrayal of leftists very flattering; unless your notion of \"leftist\" is Biden/Macron/Starmer, I guess. Not sure how someone could create a world like the one in Disco Elysium without having some kind of political ideology or why anyone would want them to be fixated on moderating themselves to the point of killing any sense of individuality in the work. It'd be like checking out the new Mel Gibson film and being annoyed that it comes across as having a conservative Catholic ideology. reply red-iron-pine 4 hours agorootparent> I don't get how anyone could find their portrayal of leftists very flattering; unless your notion of \"leftist\" is Biden/Macron/Starmer, I guess. they would be considered \"moralists\" on the DE scale of politics reply randomcarbloke 4 hours agorootparentprev [–] It might have an appropriate pessimism but it absolutely endorses fully fledged communism in an uninformed college-bro way (as the other commenter noted, akin to chapo trap house). >It'd be like checking out the new Mel Gibson film and being annoyed that it comes across as having a conservative Catholic ideology Yes, but I know Mel is a catholic, I knew nothing of ZA/UM beforehand, I wonder if all of its spiritual successors will be tainted in the same way, if so I'll know for the future that their games aren't for me. reply yoyohello13 2 hours agorootparentYou can't engage with media that doesn't fit your political ideology? reply JansjoFromIkea 1 hour agorootparentprev [–] Yeah that makes sense, I thought after posting that it isn't a fair comparison when going in blind. Not sure what it says about me or media in general these days, but I seem to never encounter anything now without knowing something about the politics of its creators so I didn't encounter it at all blind. Wouldn't say it endorses it so much as dreams of it (or maybe longs for something like the hope it provided in the past when the ideas were new), I get that that's a very muddy perspective though. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Disco Elysium Explorer Project allows users to visualize and listen to dialogues from the game Disco Elysium, with all rights reserved by the studio ZA/UM. - The project offers features such as searching dialogues, building conversations, and visualizing them, enhancing the interactive experience. - It supports multiple languages, including Chinese, German, and Korean, making it accessible to a broader audience."
    ],
    "commentSummary": [
      "Disco Elysium utilizes Articy:draft, a middleware tool, to construct its complex conversation graphs, showcasing a technical achievement despite performance challenges on large projects.",
      "The game is acclaimed for its storytelling and intricate dialogue systems, drawing comparisons to other narrative-focused games like Pentiment and Alpha Protocol.",
      "Notable for its unique RPG mechanics that emphasize dialogue and story over combat, Disco Elysium garners mixed opinions on its political themes but is recognized as a significant accomplishment in story-driven gaming."
    ],
    "points": 190,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1736737897
  },
  {
    "id": 42676123,
    "title": "If we had the best product engineering organization, what would it look like?",
    "originLink": "https://www.jamesshore.com/v2/blog/2025/the-best-product-engineering-org-in-the-world",
    "originBody": "The Best Product Engineering Org in the World January 10, 2025 This is a transcript of my keynote presentation for the Regional Scrum Gathering Tokyo conference on January 8th, 2025. Introduction People Internal Quality Lovability Visibility Agility Profitability “How are you measuring productivity?” It was September 2023 and my CEO was asking me a question. “How are you measuring productivity?” It was September 2023, my CEO was asking me a question, and my position as Vice President of Engineering was less than three months old. “How are you measuring productivity?” It was September 2023, my CEO was asking me a question, my position was less than three months old, and I didn’t have an answer. So I told the truth. “How am I measuring productivity? I’m not. Software engineering productivity can’t be measured.” It’s true! The question of measuring productivity is a famous one, and the best minds in the industry have concluded it can’t be done. Martin Fowler wrote an article in 2003 titled “Cannot Measure Productivity.” Kent Beck and Gergely Orosz revisited the question 20 years later. Kent Beck concluded, “Measure developer productivity? Not possible.” My favorite discussion of the topic is Robert Austin’s, who wrote Measuring and Managing Performance in Organizations. He says a measurement based approach “generates relatively weak improvements“ and “significant distortion of incentives.” How do I measure productivity? It can’t be done. At least, not without creating a lot of dysfunctional incentives. But this isn’t a talk about measuring productivity. This is a talk about what you do, as VP of Engineering, when somebody asks for the impossible. [turn right] “How are you measuring productivity?” [turn left] “I’m not. It can’t be done.” [turn right] “You’re wrong. I don’t believe you.” I don’t... respond well to that sort of flat dismissal. I said some things that you’re not supposed to say to your CEO. It was September 2023, my position was less than three months old, and it didn’t look like I was going to make it to the end of month four. [beat] Luckily, my CEO’s actually a pretty reasonable person. Our company is fully remote, so he invited me to come to his house next time I was in his city so we could discuss it face-to-face. That gave me a month to cool off and think about what I wanted to say. I had an impossible—or at least, dangerous—request: measure productivity. Given that I couldn’t give my CEO what he wanted without creating dysfunction in engineering, what could I give him? That’s what this talk is really about. The CEO, chief product officer, chief technical officer, and I met a month later. I said, “If we had the best product engineering organization in the world, what would it look like?” I walked them through an exercise right there on the CEO’s dining room table. It had a lot of index cards and sticky notes... of course! In the end, we came up with six categories. Imagine we’re the best product engineering org in the world. What does that look like? For us, it means these six things. People. We’d have the best people in the business, and we’d be the best place for them to work. They’d beg to work for us, and people who left would try to replicate their experience everywhere they went. Internal Quality. Our software would be easy to modify and maintain. We’d have no bugs and no downtime. Lovability. Our customers, users, and internal consumers would love our products. We’d excel at understanding what stakeholders truly need and put our effort where it mattered the most. Visibility. Our internal stakeholders would trust our decisions. Not because we’d be perfect, but because we’d go out of our way to keep them involved and informed. Agility. We’d be entrepreneurial, scrappy, and hungry. We’d actively search out new opportunities and change direction to take advantage of them. Profitability. We’d be the engine of a profitable and growing business. We’d work with our internal stakeholders to ensure our products were ready for the real world of sales, marketing, content, support, partners, accounting, and every other aspect of our business. Are we the best product engineering org in the world today? No. Will we ever be? Probably not. But we don’t need to be. It’s not about literally being the best product engineering org in the world. It’s about constantly striving to improve. These six categories are the ways we want to improve. What does this mean for you? If you did this exercise with your leadership team, you’d probably get different answers. I’m not saying that our categories is right for everyone. But it’s still an interesting thought exercise. We’re an organization that’s steeped in Agile thinking. These six categories may not be exactly what your org would use, but these six—People. Internal Quality. Lovability. Visibility. Agility. Profitability—these are worth investing in. I’m going to talk about what we’re doing in each of these six categories. If you’re a senior manager, some of these techniques might be worth using. If you’re not a senior manager, these are techniques you could potentially take to your managers. Agile only succeeds if the organization really gets behind it. You can share these ideas as an examples of what to do to support your Agile teams. Let’s dig in. People Everybody wants the best people in the business. But our company is relatively small. We can’t compete with the likes of Google, Amazon, Apple... the FAANG companies. They’re looking for the best people, too, and they have way more money than we do. But we can still get the best people in the business. That’s because we define “best” differently than they do. They’re looking for people who went to prestigious schools, who have worked for other FAANG companies, who can solve Leetcode problems in their sleep. We don’t want those people. We’re an inverted organization. That means that tactical decisions are made by the people who are doing the work, not managers. (In theory, anyway, we’re not perfect.) So we’re looking for people who have peer leadership skills, who are great at teamwork, who will take ownership and make decisions on their own. And we’re an XP shop. We use Extreme Programming as our model of how to develop software. As it turns out, XPers love teamwork, peer leadership, and ownership. They also love test-driven development, pairing, continuous integration, and evolutionary design. They tend to be passionate, senior developers. And they’re dying to be part of an XP team again. You see, Extreme Programming is too... extreme... for most companies. Just like real Agile and real Scrum is too extreme for most companies. How many times have you seen Scrum used as an excuse for micromanagement, or a senior leader tell you that you have to be Agile and give them a detailed plan for what your team is going to do over the next year? In other words, there aren’t many companies using XP. There are a lot of great people who wish they could use XP. We have our pick of top-quality candidates. And, as a fully remote company, we have a lot of flexibility in where we hire. I said we’re an XP shop, but that’s not exactly true. The founders were immersed in XP, and XP is where we want to return, but there was a period of time where the company grew quickly and lost that XP culture. We have a bunch of engineers who don’t have the XP mindset. We need to bring them on board, too. This is a matter of changing organizational culture, and organizational culture isn’t easy to change. Our engineering managers are at the forefront of that effort. To help them along, we’ve revised our career ladder. This is the document that describes what you need to do in order to be promoted. The old career ladder emphasized understanding advanced technologies and building complex systems. The new one emphasizes teamwork, peer leadership, ownership, and XP engineering skills such as test-driven development, refactoring, and simple design. QR Code: Career Ladder This is what it looks like. It’s a big spreadsheet which describes each title in our engineering organization, along with the skills required to reach each title. For example, Associate Software Engineers are hired fresh out of university. They’re only expected to have classroom engineering skills. They contribute to the team with the help of other team members. Mid-level software engineers are expected to be able to contribute to the team without explicit guidance. We expect them to have basic communication, leadership, product, implementation, design, and operations skills. Senior software engineers are expected to have the advanced version of those skills; technical leads are expected to mentor and exercise peer leadership; and so forth. As I said, each level defines sets of skills. For example, associate software engineers are expected to be fluent at the skills in classroom engineering, which includes object-oriented programming, following direction as part of a pair, basic debugging skills, and basic function and variable abstraction. Mid-level software engineers are expected to be fluent at basic communication, which includes skills such as working collaboratively with other team members, disagreeing constructively, building on other people’s ideas, and so forth. There’s more details here than I can explain today, but you can use the QR code to find a detailed article, including the documentation we use for the skills. Today, I’d like to highlight a few skills that I think are particularly important. First: communication and teamwork. Before I joined, work was assigned to individual engineers. They would go off and work for a week or two, then come back with a finished result. Now, rather than assigning work to individual engineers, we assign it to teams. (I’ll talk more about how teams are defined later.) We expect those teams to take a valuable increment, go off and work on it together, including collaborating with product management and stakeholders to understand what needs to be done, and to take responsibility for figuring out how to work together as a team. This is a big cultural shift! It’s uncomfortable for some folks. To help change the engineering culture, we’ve defined a lot of skills around communication and teamwork. This is just one example. A new engineer is expected to participate actively in team conversations. Then, as they grow, they’re expected not only to share their perspective, but to actively work to understand other people’s perspectives as well. As engineers grow further, into a senior role on the team, they’re expected to pay attention to who is participating and who isn’t, and make sure there’s room for everybody to speak and be heard. And ultimately, as the team’s most senior engineer, they’re expected to actively work with management to create an environment where people feel safe speaking their mind and expressing disagreement. As a reminder, what I’m trying to do here is to change the engineering culture at my company. One of the ways I think the culture needs to change is to have more team work and less individual work. This ladder of growing expectations and responsibility is one of the ways I’m encouraging those changes. Let me show you another example. If we want to delegate decisions to the people doing the work, and we do, then peer leadership skills are essential. Peer leadership is the ability for everyone on the team to take a leadership role, at appropriate times, according to their skills and the needs of the team, regardless of titles. We have many leadership skills, but one path starts with our most junior engineers, with a skill called “Follow the process.” But this skill isn’t just about following our existing process; it’s also about working with the rest of the team to adjust the process, or make exceptions, when the process isn’t a good fit for the situation. As engineers grow, they start to take on explicit leadership roles. One of those roles is “team steward.” Each team has a “team steward” who’s responsible for defining how the team works together and keeping everyone aligned. This is a role we expect our engineers to take on early in their careers, to start building their leadership muscles. Senior engineers are expected to have a more nuanced and fluid understanding of leadership. They’re supposed to understand that leadership isn’t about who’s “in charge”—who’s been formally identified as a leader—but instead about reacting to what the situation demands and following the lead of the people who know the most about the situation. They’re expected to identify and follow the people who are best suited to lead in any given situation, and build their own ability to do so, regardless of formal leadership roles. Our most senior engineers take this one step further. They work with managers to understand the leadership skills of everyone on the team, the leadership skills that are missing or that need to be developed further, and to help team members grow their peer leadership skills where they’re most needed. Similarly to leadership, if we’re going to delegate decisions to team contributors, then we need them to take ownership of creating great results. One of these paths starts with intrinsic motivation: the idea that our engineers are motivated by the joy of engineering and working with a great team. We don’t want people who have to be constantly monitored by a manager, but who put in their best effort because that’s the kind of person they are. Before they can advance out of a junior role, they have to have the maturity to take on the unpleasant tasks that exist on every team. In English, this is called “scut work”—the tedious, disagreeable chores that have to be done. Scut work isn’t something that only juniors do. It’s something everyone has to do. What we’re looking for is the ability to take it on without being asked. We’re looking for the ability to recognize and take responsibility for things that need to be done, even if they aren’t the most fun. Every engineer participates in the teams’ retrospectives, but to be a senior engineer, they have to do more than participate. They have to take ownership of making improvements. Our senior engineers are constantly identifying and proposing tweaks to improve how teams work together and how they interact with people outside the team. And our most senior engineers take it one step further, identifying impediments to the team’s success that are outside of the team’s control and working with management and other leads to remove them. We also put a lot of emphasis on XP skills, and particularly on simplifying our design. This is one example. Junior engineers are expected to know how to use functions and variables to make code more readable. As they grow into mid-level engineers, they learn how to refactor those functions and variables to improve the abstractions, and they also learn how to create appropriate class abstractions. Senior engineers know how to refactor those class abstractions, and they use that skill to simplify the design of the system. It’s common for an initial design to be overly-complicated, so it’s important for people to be paying attention to how to simplify it over time. This emphasis on simple design is the opposite of what I’ve seen in some companies. In some companies, the more senior you are, the more complicated your designs are expected to be. But we do the opposite. We think complexity is easy, and it’s simplicity that’s hard, so we expect our more senior engineers to produce simpler designs. And finally, our most senior engineers understand how to refactor the system as a whole, and how to prioritize those refactorings according to the risks and costs of change. To recap, our career ladder is a tool for cultural change. We’re using it to move from being an organization that prized individual work, advanced technologies, and complex systems, to one that focuses on teamwork, peer leadership, ownership, and simplicity. We launched the new career ladder in June of last year—about six months ago. It seems to be working. My managers tell me that they’re seeing shifts in behavior, with people volunteering to lead meetings and take on work they didn’t before. We’ve also been able to use the new career ladder as a touchstone for people who are having performance problems. Of course, the career ladder isn’t enough on its own. It helps people know what’s expected of them, but it doesn’t do any good if people don’t know how to perform these skills. To help out, we’re supporting the career ladder changes with an XP coaching team. Every open headcount I’ve gotten since I joined has gone towards hiring very senior XP coaches. These are player coaches who get hands on with the code and lead by example. They work alongside the rest of the engineers, demonstrating as part of the team’s normal work how XP works and why it’s such a great way to work. We’re at a ratio of about one XP coach for every 11 engineers, which isn’t quite enough yet. But it’s enough that we can start developing coaches internally rather than hiring externally. And, of course, as additional positions open up, we’ll be hiring people who already have XP skills, although not necessarily at the same level of seniority. When I look at how other companies approach this problem, the main thing I see is a lack of commitment. They’ll have an “Agile Center of Excellence,” but the ratio will be closer to 1 coach for every 50 or 100 engineers. Those coaches often aren’t engineers, so they can’t lead by example. And even if they could, they’re spread too thin. The best way to learn XP is to be immersed in it, day in, day out, on your real-world work. You need a coach working alongside you as you learn. With ratios of 1 to 50 or worse, there’s just no way for that to happen. As I said, we have a ratio of 1 to 11 XP coaches to engineers, and I would like it to be closer to 1 to 6. Our initial coaching hires are jump-starting that path, and we’re training people internally to get the rest of the way. People are the life blood of any organization, and they’re particularly important in an Agile organization, where so many decisions are made by the team contributors, not managers. If we were the best product engineering org in the world, we’d have the best people in the business, and we’d be the best place for them to work. To get there, we’re defining “best” differently than other companies. We’re looking for teamwork, peer leadership, and ownership. We’re attracting people who love XP and emphasize simple, clean design rather than algorithms and complex solutions. And we’re changing our company culture with a new career ladder and player-coaches who lead by example. Internal Quality I don’t speak Japanese, but I do have a favorite Japanese word: muda. I learned about muda from the Toyota Production System. Muda is my biggest problem, and it’s the biggest problem at many companies I know. Let me give you an example. This graph shows five months of engineering effort on a product. This isn’t real data, for confidentiality reasons, but it’s based on my real-world experiences. On the X axis, we have months of data. On the Y axis, we have the amount of time people spent on various types of work. Over these five months, the example team spent about 35% of their time on deferred maintenance. They had a key technology that they hadn’t kept up to date, and the vendor was dropping support, so they had to put everything else on hold to replace it. They spent about 25% of their time on call and responding to production incidents. They spent about 5% of their time on routine maintenance. They spent about 20% of their time fixing bugs. And only about 15% of their time on doing things that added new value to their business. Let me put it another way: if this fictional organization had spent one million dollars on development during this time, only $150 thousand would have been spent on things their business partners really valued. The other $850 thousand would have been wasted. It was necessary, but not valuable. Muda. And that’s why “muda” is my favorite Japanese word. It’s the thing I need us all to fix. Why is there so much muda? I see three common problems: Complexity Slow feedback loops Deferred maintenance They’re often lumped together as “technical debt” or “legacy code,” but each is its own problem. Let’s take a closer look at each one. Complexity Complexity is the result of having lots of different systems. It’s hard for any one developer to understand how everything works, so they have to work very slowly and carefully, and even then, you still get bugs and production incidents. Our systems don’t have to be that complicated. In the rush to deliver features, people chose complicated technologies that promised fast results. This has been repeated many times. Each technology requires a bunch of expertise, and so it’s become impossible for any one person to be an expert in all of them. It’s become difficult to make those tools do exactly what we want, too, and it’s hard to make them work well together. This is a fundamental mistake I see a lot of companies making. When they’re deciding how to deliver a feature, they focus on how much it will cost to build a feature. They don’t think about how much it will cost to maintain the feature. They choose solutions that are easy to build, but hard to maintain. But the majority of software development costs are maintenance costs, not build costs. Neglecting maintenance costs puts them in a difficult position. In 2025, we can’t talk about development costs without also talking about AI. Don’t get me wrong! AI is a great tool. I used it for the images in this talk, and it allowed me to add character and interest that I otherwise wouldn’t be able to add. But remember this image? Take a closer look at the character in the middle. Did you wonder why he has a coffee cup on his head? That’s because... ...he has an eyeball in his hair. [beat] Or how about this character? He’s hiding a third hand. My point is that these tools are never going to be as good as the people selling them want you to believe. They’ll get better, but they won’t be perfect. The problems with image generation are fairly obvious. The problems with AI in code are more subtle, and they come down to that tradeoff between speed of building and cost of maintenance. You can build code quickly with AI, but getting it all to work together nicely is harder. If you’re using AI to write code, are you considering how you’ll maintain that code? Are you considering how you’re developing the skills of your junior engineers? You can also build features that use AI, such as automatic content generators. It’s pretty easy to do, actually. But fine-tuning those prompts is tricky, and it takes a lot of manual effort to get them just right. Have you considered how you’ll keep those prompts up to date as the AI engines change out from under you? Have you thought about how you’ll find out when those fine-tuned prompts aren’t working like they’re supposed to? Slow Feedback Loops Ultimately, complexity comes from teams that prioritize building over maintaining, and the costs of doing so are devastating. Now let’s look at another source of muda: slow feedback loops. Feedback loops are about how effectively developers can work. After an engineer makes a change, they have to check to see if that change did what they intended. How long does that take? That’s your feedback loop. If it takes less than a second, then they can check every single change. Every line of code, even. This is what test-driven development is all about, and it’s an amazing way to work. Let me show you what it looks like. QR Code: Fast Tests This is the full build for a real production system. The system is on the small side, but it’s over 12 years old, so it’s had the opportunity to accumulate some technical debt. Let’s see how long the build takes. Don’t look away—this won’t take long. [play video] That’s it! Just over eight seconds. For context, most organizations I meet are happy with a build that takes eight minutes, not eight seconds, and most are much, much slower. A big part of the reason this build is so fast—or rather, why most builds are so slow—is the tests. Most teams have slow, brittle tests. This codebase has over 1300 tests, but they only take two and a half seconds. Describing how to achieve these sorts of fast tests is a whole talk of its own, but I have a lot of material on this topic. You can find it at jamesshore.com/s/nullables, or just follow the QR code on the slides. Eight seconds is a nice, fast build, but it’s actually still too slow for a great development experience. In a perfect world, we want engineers to be able to check their work at the speed of thought. If they can check every single line of code they write, as soon as they write it, finding bugs becomes easy: you make a change, run the build, and immediately find out if there was a mistake. There’s no need to debug because you know your mistake is in the line of code you just changed. To get these kinds of results, you need your build to be less than five seconds—preferably less than one second. This isn’t a fantasy! It’s possible to do this with real production code. Let me show you: [play video] That’s less than half a second each time the build runs. There are a few tricks here. First, the build automatically runs when the code is changed. Second, the build is written in a real programming language, and it stays in memory. It’s able to cache a bunch of information, such as the location and age of all the files, the relationships between files, and so forth. So when it detects a change, it doesn’t have to scan the file system again, and it only runs the tests on the code that’s changed. [beat] I have to be honest. Some of our code has these sorts of feedback loops. But for the systems with the most muda, it takes much longer than a second to get feedback. Sometimes it can take tens of minutes just to get the computer into a state where a manual test is even possible. So people don’t check every change. They batch up their work, test it all at once, and then have to go through long, tedious debugging sessions to figure out the cause of each error. Some errors aren’t caught at all. That leads to muda, and that’s why fast feedback loops are important. Deferred Maintenance A third issue that leads to muda is deferred maintenance. Deferred maintenance is really a consequence of the other two problems. If the system was simple, you could upgrade critical dependencies easily. But most companies have a lot of complicated dependencies, and some of their updates require major rearchitectures. Similarly, if the feedback loops were fast, you could make changes quickly and safely. But most companies’ feedback loops are slow, so making changes take a long time. Major rearchitectures plus slow changes means that upgrading dependencies can take weeks or months of effort. Now you have to make tough prioritization decisions. Do we build an important new feature? Or do we upgrade a component for no visible benefit? Business partners often choose to defer the maintenance. I can’t really blame them. But that deferred maintenance compounds, things get even more expensive to upgrade... ...and eventually the bill comes due. What do you do about such a difficult set of problems? Complexity, slow feedback loops, deferred maintenance. These problems are common. Usually you hear people talking about “legacy systems” and “technical debt.” Whatever they call it, the underlying problem is the same: low internal quality. High muda. I’ve seen four approaches to fixing systems with low internal quality: Big-bang rewrite Modular rewrite Change-driven rewrite Improve in place Be careful: The first two approaches are popular... and usually fail. In the “big bang” rewrite, you start up a new team to write a new version of the software. Meanwhile, the old team keeps maintaining the old software: adding features, fixing bugs, and so forth. When the new software is done, the old software will be retired, and the new software will take its place. In this slide, the old system is represented by the spaghetti in the top row, and the new system is represented by the clean, shiny dishes in the bottom row. This sounds nice in theory, but what really happens is the rewrite always takes much longer than expected. Meanwhile, the original software keeps getting bigger and bigger, and the mess gets worse and worse, because people think it’s going to be thrown away. The replacement keeps taking longer than expected. You’re spending twice as much money, because you’re running two teams, but not getting good results for your money. So the replacement is either cancelled or rushed out the door. Customers are unhappy because it doesn’t do everything the old system did, and your engineers are unhappy because, in the rush to get the replacement done, they made a mess. It’s better than the old system is, but not that much better. It’s going to need another rewrite soon. In other words, “big bang” rewrites are dangerous. They should be avoided. Another option, not quite as common, is the modular rewrite. A modular rewrite takes a big existing system and identifies pieces that can be split off and rewritten. Then each piece is rewritten, one at a time. This might be done by a separate team, but sometimes it’s done by the same team. Over time, the whole system is replaced, without the risk of a big-bang rewrite. But, as always, things are harder than expected. You end up chipping away at the easy edges of the system, but the big complicated core stays just as big and complicated as ever. And, as always, other priorities intervene before you can finish. Now, instead of one complicated system, you have multiple complicated systems, all interfacing with each other in confusing ways. If you’re not careful, you end up with a bigger, uglier mess than you started with. Modular rewrites are safer than big-bang rewrites because they work in smaller pieces, but they suffer the same problem: everything is bigger and more complicated than expected, and if you stop part way, you’re left with a mess. The trick to a rewrite is to realize that you can never compete with features. Instead of establishing a rewrite team, continue with a single team. Instead of prioritizing rewrite work, continue to prioritize features and bug fixes. But... whenever you make a change, migrate the code you’re changing to the new system. Don’t describe it as a rewrite; it’s just part of the work to complete the new feature. This way, you don’t have to compete for budget, and you don’t have to justify the cost of the rewrite. You just do it as part of your normal work. This approach is slow. It will take years to complete. But thanks to the Pareto Principle—the 80/20 rule, which says that 80% of the changes to the system will occur in 20% of the code—you don’t have to rewrite everything to see a benefit. You do need to commit to seeing it through, but it’s easier to do so when people aren’t breathing down your neck asking when the rewrite will be done. In this approach, you don’t add a new team. You can still add people, if you want, but you add them to your existing team, and use them to develop new features... migrating code to the new system as you go. But even better than a rewrite is not rewriting at all. Instead, improve your existing system in place. As you work on features and bug fixes, add tests, clean up your automated build, and file off the rough edges. It will never be perfect, but the Pareto Principle will kick in, and the parts of the system you work with most often will be the parts you improve the most. Remember that eight-second build I showed you? That’s a 12-year-old codebase, and it wasn’t always that smooth. Ten years ago, it was kind of a mess. But steady, consistent effort to improve it in place means that, today, it’s a pleasure to work in, and better than it’s ever been. If you have internal quality problems, improve your existing systems in place. That takes specialized skills, so you might need to hire people for those skills. Personally, I hired a bunch of Extreme Programming coaches, and we’re doing a lot of training. Sometimes, you can’t improve in place. If you want to change fundamental technologies, such as the programming language a system uses, or a core framework, you may not be able to improve the existing system. In that case, you can do a change-driven rewrite, and migrate code to a new system as part of your work on the old system. Modular and big-bang rewrites can work, but they’re dangerous. Modular rewrites risk leaving you with a bigger, more complicated mess than before. Big bang rewrites risk leaving you with a half-baked product and angry customers. Although they can work, the risk is high, and I don’t recommend them. If we were the best product engineering org in the world, our software would be easy to modify and maintain, we’d have no bugs, and we’d have no downtime. If only that were true. We’re looking at three things: Simplifying our technology stack. We’re focusing on the cost of maintenance instead of the cost to build. Improving our feedback loops. We’re building systems that allow developers to check their work in less than five seconds, preferably less than a second, and introducing test-driven development. No longer deferring maintenance. When a dependency has a new version, we want to upgrade it immediately. Don’t wait. Don’t allow it to be a prioritization decision. Make it a requirement and stop the line. We’re not there yet, but as our technology stack becomes simpler and our developer experience better, those upgrades will get easier. And for the existing systems, where it’s not as easy as we’d like, we’re avoiding big rewrites. We’re improving our systems in place, where possible, and undertaking a change-driven rewrites where not. Lovability Jeff Patton is here this week! He’s giving tomorrow’s keynote, and from previous experience, I can tell you that he’s not to be missed. Jeff can tell you much more about making software people love than I can. So I’m going to talk about how we make time to put Jeff’s ideas into practice. We just talked about internal quality and reducing muda. The better your software’s internal quality, the faster your teams will go. So investments in internal quality are easy: as much as you can afford. The main challenge is managing cash flows and balancing that investment with forward progress. Lovability is external quality. It’s about building software that our users, buyers, and internal stakeholders love. We’re going to put all our remaining capacity towards external quality. But there are always more ideas than time to build them all. So lovability is also about understanding what stakeholders really need and putting our limited capacity toward what matters most. People have understood this for a long time. Back in the days before Agile, companies would put immense amounts of effort into requirements analysis in order to make sure they were building the right thing. They would analyze the market, define exactly what to build, and then build it. Those of you who weren’t around in the 90’s might think this “waterfall” idea is just a fable—a straw man trotted out by Agile proponents to prove that they’re better than the old way. Surely no one really worked that way! But they did... and a lot of companies still do. They say they’re “Agile,” but when you look at how they plan, what they actually do is... analyze the market, decide exactly what to build, and then build it. The only difference is that they don’t make requirements documents any more. Instead, they make Jiras. They chop up their requirements documents into lots of itty-bitty pieces, and then they move those pieces around a lot. This approach is called “project-based governance.” You create a plan, then you work the plan. If you execute the plan perfectly, coming in on time, on budget, and as specified, you’re going to be successful. At least, that’s the theory. As Ryan Nelson wrote in CIO Magazine in 2006: Projects that were found to meet all of the traditional criteria for success—time, budget, and specifications—may still be failures in the end because they fail to appeal to the intended users or because they ultimately fail to add much value to the business… Similarly, projects considered failures according to traditional IT metrics may wind up being successes because despite cost, time or specification problems, the system is loved by its target audience or provides unexpected value. CIO Magazine, Sep 2006 One of my favorite stories about how this approach fails is the FBI’s “Virtual Case File” system, because there was a US Senate investigation, and we have a lot of details about what happened. It’s unusual in how high-profile it was, but the story is very typical of its time. In June 2001, the FBI launched the Virtual Case File project. 17 months later, they had established “solid requirements.” Seventeen months! They knew exactly what their users needed—or thought they knew—and had a detailed plan. A year after that, the project was delivered, and it didn’t work. The FBI “immediately discovered a number of deficiencies in VCF that made it unusable.” In 2005, it was officially cancelled, and the director of the FBI appeared before a Senate subcommittee to explain how the FBI had managed to waste $104.5 million of taxpayers’ money. The problem wasn’t that the software didn’t function; the problem was that all that detailed planning resulted in the wrong thing. The software didn’t meet the FBI’s needs. Before you say, “that was then... we’re Agile,” look at this list again. How does your company manage projects? Do they define success as delivering on time, on budget, and as specified? Do they ask you to prepare a plan, and then track progress against that plan? [beat] The problem with this approach is that we can’t predict what our customers and users really want. We can only guess. Some of those guesses are right; some are wrong. We have to conduct experiments to find out which ones are really worth pursuing. One of my favorite expressions of this idea is Eric Evans’ “Build, Measure, Learn” loop. We have an idea about what customers might love. We build a simple experiment that lets us test that idea—the smallest, simplest experiment we can think of! It might not even be code. It might just be interviews, surveys, or Figma prototypes. Then we conduct the experiment and see what data comes out of it. We learn from that data and that improves our ideas of what we should build next. This loop is where the idea of “Minimum Viable Product” comes from. But it’s often misunderstood. Minimum Viable Product isn’t the smallest thing we can deliver to customers; it’s the smallest test we can perform to learn what we’re going to deliver to customers. Those tests can be very small. Because the faster we can get through this loop, the more we can learn, and the less time we waste on failed ideas. This leads to product-based governance. Rather than creating a plan and working the plan, we iterate on a series of very small plans. If we learn and change our plans, we can steer our way to success. Success means delighting our stakeholders, and doing so in a way that impacts the business. We aren’t tracking adherence to plan, but rather, return on investment. Adapting plans is one of the key ideas of Agile, of course. Martin Fowler describes the essence of Agile this way: “Agile development is adaptive rather than predictive; people-oriented rather than process-oriented.” So now that Agile’s taken over the world, and just about everybody’s using some flavor of Scrum, we’re all able to adapt our plans, right? I’d like to say that’s true... but we know it’s not, don’t we. There are a lot of companies saying they do Agile, or Scrum, and they’re not either of these things. They predict rather than adapt, and they orient around process rather than people. And if you’re not adaptive, if you’re not people-oriented... you’re not Agile, no matter how many Scrummasters, Sprints, or standups you have. [beat] The build-measure-learn loop and adapting your plans is important at a tactical level. But what about big-picture strategy? At my company, strategy is decided by our Leadership team, which consists of our CEO and heads of Product, Marketing, Sales, Partners, Content, Finance, and so forth. Alignment at this level is critical. Each person has their own view of their world, and their own theories about what’s needed for success. Product wants to improve usability. Marketing and Sales want splashy new features. Finance wants to reduce manual billing. Each person has their own view of the world, and their own theories about what’s needed for success. How do you balance those competing concerns? How do you allocate engineering’s limited capacity? We’re trying something we call “product bets.” A Product Bet is a big-picture hypothesis. For example, our head of sales might say, “A lot of our customers are vampires. We can close a lot more deals if we introduce an AI-powered salesperson that’s available at night.” (I’ll tell you a secret. Our customers aren’t actually vampires. I have to keep the specifics of our situation confidential. They could be werewolves.) Product bets are proposed with a short, one-page summary of the hypothesis. It has a thesis statement: “Increase vampire sales with an AI-powered night salesperson.” A sponsor: the head of sales. The value we expect to gain, which is typically linked to spreadsheet with a financial model. A summary of the reasoning for the value: Vampires spend $80mm on services like ours. Many of them use their human servants to do their shopping, but if we sell to them directly, they’ll be more likely to buy, and we’ll increase our market share among vampires by at least one percent. And finally, a wager, which is the maximum amount of money we’re willing to spend on this bet. Other members of the leadership team have their own priorities and bets, so naturally, they’re going to come up with objections. The head of Content might say, “Shopping is beneath vampires. That’s what they have human servants for.” The head of Marketing might say, “AI isn’t good enough to do sales.” The head of Finance might say, “That’s going to cost a lot more than $400K.” We want these objections. We want an open and honest dialog between members of the Leadership team, picking holes in the bets and finding ways to make them stronger. Ultimately, if a bet is chosen, those objections lead to experiments. “Shopping is beneath vampires? Let’s find out!” Our hypothesis is that vampires will actually feel appreciated by us having vampire-friendly hours. We want to test hypothesis as quickly and cheaply as possible, so we don’t build software; we “build” by temporarily moving some salespeople to a night shift. Then we measure the difference in vampire sales. Let’s say we got five times as many sales. That’s a clear win. Our next objection is that AI isn’t good enough to do sales. Our hypothesis is that no, it’s not going to be as good, but it’s going to be good enough. Again, we want to test that hypothesis with the minimum effort possible, so we still don’t build software. Instead, we build a custom prompt in ChatGPT for the human night shift to use. Some salespeople use ChatGPT and follow the script; others continue as they were. We measure the difference. In this case, let’s say the sales decreased from 5x as many sales to 2x as many. What did we learn? A night shift is a great idea, but AI-driven sales isn’t. We change our plans, and decide to build a permanent human night shift instead of using AI. And now we’ve saved nearly 400 thousand dollars of investment. We spent a little bit of time and money on bonuses for the sales people who participated in the night shift, and some on the custom ChatGPT prompt, but much less than we would have spent on building the full solution. Ultimately, we want our Leadership team to align around strategy. Product bets are our tool for doing so. I have to be honest. Getting adoption on this idea has been very slow, and we’re still rolling it out. So, of all the things I’m presenting today, this one is the most experimental, and the one to be most careful about adopting yourselves. But if it works out, it won’t just be a tool for strategic prioritization. It will also be a tool for Leadership to think critically about their ideas, and a way to generate hypotheses for us to test using the Build-Measure-Learn loop. If we were the best product engineering org in the world, our users, customers, and internal consumers would love our products. But even more, we would understand what our stakeholders need and put our limited capacity where it matters most. First, we need to achieve strategic alignment at the leadership level. We’re starting to use product bets for that. They’re not just for prioritization, though. They’re also a way to think critically about our plans and generate hypotheses that we can test. Second, we need to validate those bets and adapt our plans based on what we learn. We’re using product-based governance that focuses on impact rather than adherence to plan, and we’re using the build-measure-learn loop to test our hypotheses as quickly and cheaply as possible, preferably without building software at all. Visibility Given limited capacity—and there’s always limited capacity—there will be winners and losers in the prioritization game. Some people will be happy about the amount of time they’ve gotten from Engineering, and some will be sad. Even angry. Transparency is vital for building trust with internal stakeholders. Where are we spending our time, and why? We send out regular reports, but that isn’t enough. We also have to talk to people, understand their perspective, share what’s going on and why. And even if you do all that, there will still be people who are unhappy. As we say in the US, you can’t please all the people all the time. I have to admit: I don’t have a lot of good answers about how to build trust. I chose the company I’m in now because I already had their trust. I’d worked with the founders before, 15 years ago. They knew what they were getting, and what I bring was what they wanted. Even so, the founders’ trust didn’t automatically extend to the rest of the Leadership team. A lot had changed in 15 years, and nobody else knew me. In fact, what the founders and I wanted, and what the rest of Leadership wanted, weren’t in alignment. We wanted product-based governance and Build-Measure-Learn. But they wanted predictability. The way they judged trustworthiness was simple: was Engineering doing what they said they would do? In other words, did we ship on time? And the answer was “no.” Engineering was not shipping on time. There are a lot of reasons Engineering wasn’t shipping on time. We didn’t have a good approach to forecasting, to begin with. But even we did, predictability wasn’t something I was planning to bring to the organization. Predictability is the realm of project-based governance. I was planning to introduce product-based governance. Product-based governance can be frustrating to people who want predictability. Because we don’t know what we’ll do here [motions to “Build” step] until we know what happened here [motions to “Learn” step]. We can predict how long it will take us to get through a single loop, but we can’t predict what the next loop will look like. Well, we could, but that would mean we didn’t learn anything, and if we didn’t learn anything, we’re not producing as much value as we could. So, as another classic American saying has it, “the only winning move is not to play.” ...How about a nice game of chess? One of the first things I did after joining the company was to introduce a more rigorous approach to forecasting. This approach requires gathering a lot of data, and while that was happening, I told my teams to stop making predictions to stakeholders. This wasn’t popular with my stakeholders. It’s not that they needed the predictions for any purpose. Predictions were being used as a political weapon. “You promised this would be done!” “Well, we had to set it aside, because the Leadership team decided we should work on this other thing.” “It doesn’t matter—you promised it would be done, and this is more important to me than that other thing they decided on!” In other words, predictions were causing more harm than good. The new forecasting approach is in place now, but we’re still not providing dates. Instead, we’re providing time ranges: “Historically, a valuable increment of this size has taken between two and six weeks.” But we’re not predicting dates, because we don’t know when the work will start. As an example, one of my stakeholders has a project that’s very important to him. It keeps getting delayed by other priorities, and he’s upset about that. So he asks me, “when will it be done?” And I say, “2-6 weeks after it starts.” And he says, “so when will it start?” And I say, “that’s up to the Leadership team to decide. We’re ready to start as soon as they say ‘go.’” He’s still unhappy, but now he’s unhappy with the prioritization process, and putting his effort into influencing prioritization decisions, which I’m going to call a win. QR Code: Agile Fluency Game People really want date predictions. The reason I can get away with not providing them is that the CEO, CTO, and Chief Product Officer are on my side. They understand the value of adaptive planning, and they trust my leadership. It’s the reason I’m working there. I was consultant for 23 years prior to joining this company, and was looking for companies to join for five years prior to choosing this one. I chose them because I knew I would get this level of support. One way the CEO is supporting me is that he invited me to give a presentation about Agile at one of our quarterly Leadership retreats. It’s normal for me to attend these retreats, but for this one, I was given a full four hours out of the schedule to use as I please. I used the time to explain muda and the reasons for our capacity constraints. I talked about product-based governance and many of the things we’ve discussed today. And, most importantly, I had them sit down and play a game. In that game, which is called the Agile Fluency Game, participants experience what it’s like to be part of a team that’s adopting Agile for the first time. There are a lot of different lessons in the game, but one of the biggest is the cost of maintenance. If you aren’t careful about managing your maintenance costs, you’ll go out of business. Before you do, you’ll have several uncomfortable rounds struggling to make progress while all your spare capacity is spent on muda. In other words, exactly the problem we were facing. That opportunity turned things around for me at the company. I wouldn’t say I have everyone’s trust yet. But I do have their respect and understanding. They understand why Engineering isn’t giving them what they want, they understand why we’re focused on adapting our plans, and they respect my ability to improve it... or at least, the founders’ trust in me. I still have a long way to go. In 2025, we’re putting more emphasis on product bets and strategic planning. As part of that work, I’ll be working with the leadership team to create financial models of the cost and value of each of those bets. I’ll be providing forecasts of the capacity available in each of our product collectives, and helping stakeholders understand how their prioritization decisions result in tradeoffs of engineering capacity. I’m hoping that working together in this way will help us further develop the visibility and trust we need. It’s a long, slow process, but without trust, we can’t be successful. If we were the best product engineering organization in the world, our internal stakeholders would trust our decisions. I don’t think we’re there yet. Most of that comes down to unhappiness with our capacity, and with prioritization decisions. To be honest, I’m no political genius. Somebody who’s more clever than me can probably figure out a better way to build the trust we need. For me, though, it started with having champions in the organization who already trusted me; being transparent about our capacity challenges; and showing people why things were operating the way they were with a hands-on experience. But at the same time, I’m staying true to our goals. People always want predictive, not adaptive approaches, and I’m holding firm on staying adaptive. I am providing forecasts, but I’m doing it by extrapolating from historical data that compares estimates to actual results. And even then, I’m only forecasting how long things will take once they’ve started. I’m not providing dates. This is what’s working for me. Your situation is going to be very different, so I’m not suggesting that you follow this approach exactly... or even at all. For example, if your CEO doesn’t support adaptive planning the way mine does, you might need to make predictions. Please adapt these plans for your situation. Agility If we’re going to adapt our plans and follow that build-measure-learn loop, we need the technical ability to do so. There are two aspects to this: tactical and strategic. From a tactical perspective, most engineers don’t know how to design software incrementally. Most software development education still comes from a waterfall perspective, which assumes time spent on analysis and design before coding. If you break up their work into short Sprints, they’re going to create mini waterfalls, where they do a little bit of planning, a little bit of design, a little bit of programming, and a little bit of testing. They won’t have enough time to do the design and test work that they really need to do. They’ll struggle to create a cohesive design, and they’ll struggle with bugs. If you adapt your plans frequently—as you should!—it becomes even harder. Your internal quality will suffer. Muda will rise. To be successful in an adaptive environment, you need to be able to keep your design and code clean at all times. Extreme Programming practices such as evolutionary design, merciless refactoring, and test-driven development allow you to test, code, and design simultaneously, so you always have enough time for design and testing, even if you’re using short Sprints. In other words, before we can have business agility, we need to have technical agility. This is where Extreme Programming shines, and it’s why I’m hiring for XP skills at my company. There’s also a strategic component to supporting business agility. As our business strategy changes, the amount of investment we put into this product or that product changes. Our ability to respond to those changes depends on how we organize our teams. The “classic” way to organize software teams is functionally. A front-end team here, a back-end team there, a database team over there. I think we know the problems this causes by now. In order to deliver any value, we have to coordinate all four teams. It leads to delays and errors. Muda. Agile teams are cross-functional. We create teams that can own an entire portion of a product: product and the front-end and the back-end and the database and operations and security. We could call that “BizDevSecDataKitchenSinkOps.” Well, in theory. In practice, it’s hard to fit that many people into a single team. So we end up having to divide people amongst teams. The most popular book on this subject is called \"Team Topologies.\" Team Topologies provides ways of organizing teams so you can keep them small—seven or so people in each team—while still keeping them autonomous and focused on delivering value. We have stream-aligned teams, which is what we really want, and then we have enabling teams, complicated-subsystem teams, and platform teams as a way of working around the fact that we can’t really have what we want with such small teams. So if we have too many people for one team, we divide them into multiple teams. For example, let’s say we’re at a company with four products: the legacy money-maker, a big bet on the future, a way of expanding into new markets, and a bet on the far future. We can create a stream-aligned team for each product. But now we have a problem. Some of these teams are still way too big. Team Topologies says to split these teams up further. Depending how you design the teams, this can work pretty well. I’ve used this approach many times myself. But over time, I’ve found several problems with the Team Topologies approach. First, everyone is so isolated to their teams, silos form. In my experience, people barely interact across teams, even in the same product. This makes it difficult to succeed at cross-team initiatives, and hard to move people between teams. Second, teams are rigid. When business needs change, adding and removing people from teams is a problem. Because teams are limited in size, new business needs often require you to reorganize your teams, which is a huge disruption. Often, Engineering resists the reorg, leaving their business partners frustrated, because effort isn’t being directed at their highest priorities. Third, specialties such as user experience, security, and operations, are spread too thin. You don’t need a full time person on every team, but having people work part time on each team doesn’t work either. It leads to constant task switching, which makes it difficult for people to focus and wastes a lot of time. And fourth, the teams often aren’t really independent. When you have a legacy codebase, it usually has to be shared across multiple teams, and no team really wants to own it. Quality degrades further as people focus on the code they do own. We need to build in another direction. We need to build up, not out. When people thinking about adding a lot of people to Engineering, they usually think about adding more teams, but that’s just one way to scale: horizontal scaling. You can also make teams bigger. That’s vertical scaling. Vertical scaling allows you to remove the complexities of Team Topologies and... ...return to one value stream per product. My favorite way to do this is an approach called FaST. QR Code: FaST FaST was invented by Quentin Quartel. It stands for “Fluid Scaling Technology.” You can learn more at fastagile.io, and I have several in-depth presentations on my website, which you can find by following this QR code. There’s also a session on FaST at this conference later today! Yoshiki Iida and Takeo Imai will be speaking in room C at 3:15. Here’s how FaST works. First, everybody gathers together in a big room. All the teams I’ve used FaST with have been remote, so we use a videoconference and Miro, a collaborative whiteboarding tool. The meeting starts with announcements, then team leaders from the previous cycle, called stewards, describe what their teams have worked on since the last FaST meeting, two or three days ago. Next, product leaders describe their business priorities. On your whiteboard, you’ll have a set of high-level priorities. My teams work in terms of valuable increments, which are things we can release that bring value to our organization. You can see that some increments are in progress, and some are waiting to be started. When product leads describe their business priorities, they’re describing how these have changed. Usually it’s just a quick, \"no changes.\" But if something has changed, a product lead will explain what has changed and why. Next, team members volunteer to steward a team. Anybody can steward a team, but most teams are stewarded by engineers. The maximum number of stewards is limited to ensure each team has about 3-5 people on it. Each steward describes what their team is going to work on. The stewards are expected to work on something that advances the collective’s business priorities, but they use their own judgment on what that is. Most of the time, it will be feature work, but it can also be things like improving the build or cleaning up noisy logs. Usually, they’re a continuation from the previous cycle. Finally, people self-select onto the teams, based on what they want to work on, what they want to learn, and who they want to work with. Ultimately, they’re expected to do what’s best for the organization. Most of the time, they’ll continue with the same team as the previous cycle. And that’s the FaST meeting. It takes 10-20 minutes, and it’s really all there is to FaST. It’s a way of having a single large group of people work together collectively by dynamically breaking into teams every few days. It’s simple, it’s fast, and it’s effective. FaST completely solves the issues I’ve seen with Team Topologies. I’ve stopped using Team Topologies in favor of just having large product teams. First, when you have lots of small teams, it’s hard to plan work that involves multiple teams. With FaST, you have larger teams, so cross-team initiatives are much less likely. We haven’t had any at my company since we started using FaST over a year ago. Second, Team Topologies has trouble with big business priority changes. That’s not a problem with FaST—the \"F\" stands for \"Fluid,\" and it’s really true. People dynamically adjust to whatever we need. It’s incredibly responsive, too—if there’s an urgent need, we bring it to the next FaST meeting, which happens twice a week. People form a team around it and go! We just have to be careful to manage priorities and minimize work in progress. I’m constantly reminding the product managers that it’s better to finish work than to start it. Third, when you have small teams, specialists tend to get spread across multiple teams, leading to a lot of frustration and task switching. That isn’t an issue with FaST because the collectives are large enough that each one can have a dedicated specialist. They self-select into whatever work needs to be done. And finally, shared code is no longer an issue because because you can combine the teams that share code into a single collective. FaST isn’t perfect, and there are some real challenges with moving to FaST. If you’re interested in trying it, come talk to me about those challenges, or watch my presentations about it. But I haven’t seen anything better for solving the team organization problems that occur at scale in engineering organizations. If we were the best product engineering organization in the world, we would seek out opportunities to change our plans. We would work in small pieces and adjust our strategy based on what we learned. To do that, we not only need the business agility we’ve already discussed, we need technical agility. Specifically: Extreme Programming practices, which allow us to change direction without creating a technical mess. FaST, which allows teams to shift fluidly in response to changing business needs. Profitability Profitability is last on the list for reason. If we take care of our people, if we take care of our internal quality, if we take care of our customers and users, if we take care of our internal stakeholders, and if we are responsive to changes in the market... we will be profitable. Almost. We have to remember that the only way we can take care of our people, our customers, our users, and everyone else... is if we stay in business. It’s not enough to build great software. We also have to build it to be sold, to be cost effective, and to be put into production. There’s a funny paradox about engineering. Great engineering doesn’t seem to be heavily correlated with success. In my career as a consultant, I met a lot of companies that were really struggling from an engineering perspective, but were still very successful from a business perspective. That’s because, no matter how much of a mess they were under the covers, they served the needs of the business. Here are a bunch of departments you might see in a business-to-business product company. Each of them directly contribute to the company’s yearly revenue. Marketing generates leads—people who might want to buy your software—for your Sales department. They’re judged on the number of qualifying leads they create. Partners also generates leads, or even sales, from people who are using complementary software. They’re judged on the revenue partners generate. Sales converts leads into paying customers. They’re judged on the new revenue they generate. Customer Success takes care of your customers. They’re judged on customer retention and upsell rates. So what does product engineering do? [beat] We create new opportunities. Let’s say that the trajectory of your company is to grow its annual revenue by $10mm per year. Our job is to increase that rate of growth, to $12, $15, $20mm per year. Every time we ship a new feature, we should be increasing that rate of growth. Our features should open up new markets, allowing Marketing to generate more leads. We should provide useful APIs, allowing Partners to build new relationships. We should respond to market trends, allowing Sales to convert more leads. And we should fix the problems that get in customers’ way, reducing churn and increasing upsell. Every dollar invested into engineering should be reflected in permanent improvements to the value your company creates. It may not be dollars or yen; it may be helping to cure malaria or fighting climate change. But however you define value, the purpose of product engineering is to change that trajectory for the better. If we were the best product engineering organization in the world, we would build our products to be sold. We would work closely with our internal stakeholders to ensure our products were ready for the real world of sales, marketing, content, support, partners, accounting, and every other aspect of our business. We would plan for observability and operability, for outages and data security. We would build software that changes the trajectory of our business. And we would do it by having the best people in the business; having such high internal quality that changes were easy and bugs were rare; focusing our efforts on the changes that would make the most difference for our users and customers; having the trust of our internal stakeholders; and seeking out new opportunities and adapting our strategy. Are we the best product engineering organization in the world? No. We’re not. But we would like to be. And we’re never going to stop improving. I hope these ideas will help your companies continue to improve as well. Thank you for listening.",
    "commentLink": "https://news.ycombinator.com/item?id=42676123",
    "commentBody": "If we had the best product engineering organization, what would it look like? (jamesshore.com)190 points by kiyanwang 23 hours agohidepastfavorite105 comments abc-1 9 hours agoI always feel like these manager types have drunk deeply from the koolaid for some reason. It’s a lot of words and processes that they usually cargo culted from somewhere else. A lot of it seems to boil down to “don’t be an idiot” and “actually care about your work”. They always have this air of superiority because they’re high up on the org chart. Like CEOs who think they’re the chosen ones, when plenty of people could do it just fine. I laugh hard when people like Zuck say software devs will be replaced by AI, not realizing an AI CEO probably wouldn’t have burnt 30 billion on a terrible metaverse flop. reply wodenokoto 8 hours agoparentWould it have bought instagram and whatsapp? Would it have identified major social media trends in competitors that couldn't be bought and outcompeted them at their own game? Would it have suggested developing your own ad platform or just should banner space per cpm? There is a lot to not like about Meta and Zuckerberg, but saying he's a bad business man is a little silly. Metaverse was a wrong and expensive move, but it was a wrong move they could afford. reply Xcelerate 6 hours agorootparentExactly. I’ve never understood this criticism. A good CEO (in terms of the business) is able to make significant, groundbreaking decisions but is also able to reverse course quickly if those decisions aren’t working out (anyone remember the Facebook phone)? Unless the CEO is psychic, they’re necessarily going to make a lot of bad or wrong decisions. The key is being able to recover quickly and move on to the next thing. A bad CEO makes no big decisions for fear of being wrong. When FB bought Instagram for $1B, there were a lot of talk show hosts riffing on Zuckerberg for making one of the stupidest business decisions of all time. A lot of executives who got to their position by corporate ladder climbing have personalities that would be terrified of that sort of widespread criticism. They would never make the kinds of decisions that might possibly put them in the unenviable position of being made fun of on national television. reply mempko 3 hours agorootparentThe assumption that hierarchical organizations are inevitable blinds us to more effective ways of organizing. When we look at companies today, their feudal-like structure means CEO decisions naturally have outsized impact - but this is a product of the system, not inherent necessity. Take Meta's acquisitions of Instagram and WhatsApp under Zuckerberg. While these proved strategically valuable, framing them as evidence of unique CEO insight misses a crucial point: Many others in the organization likely would have made similar choices given the same position and information. The success stems more from the concentrated decision-making power than from individual brilliance. What's fascinating is how we conflate organizational structure with individual capability. When good outcomes emerge from hierarchical systems, we rush to credit the person at the top rather than examining how the structure itself shapes and amplifies their decisions. This creates a self-reinforcing cycle: hierarchical success is used to justify more hierarchy. But imagine if we distributed decision-making power more broadly, tapping into the collective intelligence and diverse perspectives of entire organizations. Research on collective intelligence and successful worker cooperatives suggests groups often make better decisions than individuals, especially on complex issues. Companies like Valve and Morning Star have demonstrated that flat organizations can be both innovative and profitable. The real opportunity lies in reimagining organizational structures that harness our full human potential - not just that of a select few at the top. By questioning our assumptions about hierarchy, we open ourselves to discovering more dynamic, equitable, and effective ways of working together. reply miki123211 7 hours agorootparentprevIMO metaverse was a bet. It's perfectly fine to take a bet that you're not 100% convinced will pay off (professional poker players and traders understand this on a very deep level), as long as the potential upside is massively larger than the downside. Zuck understood that Meta could take the hit if the metaverse bet didn't pay off, but that they'd be massively better off if it did, and they had their own platform. Apple's blatantly anticompetitive behavior around ATT was a prime example of what happens when your business is reliant on \"platform overlords.\" I'm not fully convinced that the metaverse era is over, though. If they can get Orion costs down and put something of that quality into serial production, I think they still have a chance there. reply Zanfa 6 hours agorootparent> IMO metaverse was a bet. It was FOMO. They had no vision, they had no plan, it was clear that it was only a thing because it was a buzzword at the time. Just like every other company stuffing crypto-adjacent things everywhere. It might have been a bet, but it was obvious that it was a really terrible one. reply rohit89 1 hour agorootparentI do not know where this notion that Meta has given up on the metaverse comes from. Mark continues to talk about AR/VR at every opportunity and Reality Labs continues to invest big on it. The metaverse is a bet but its a 10 year bet that has not been played out yet. reply zqy123007 2 hours agorootparentprevNot really. FB was choked by Apple and trolled by Google for so long, Zuck understood his position without a hardware platform in the furture. XR seems to be a reasonable bet which he already have an edge. If he won, he won big. reply Clubber 3 hours agorootparentprevReminds me of Ballmer buying Nokia or Cook buying Beats (or whatever the company was called.) Cook's bet might have worked out a little better than Nokia. reply wavemode 2 hours agorootparentprevNo. \"Bet\" implies some sort of clear vision or value proposition. Metaverse was a dream. It's okay to dream. It's not okay to burn billions of dollars on dreams with no proof of concept or business plan. Set aside the question of whether or not it's a bad idea - that's just plain bad execution. reply rohit89 59 minutes agorootparentThe Orion is the proof of concept. The metaverse that Mark is thinking of is as ambitious as Musk's Mars plans. And it is something that requires large amounts of capital and time. reply short_sells_poo 7 hours agoparentprevI wholeheartedly agree with your thoughts, with perhaps the exception of the CEO. Or rather, the CEO needs to be someone who will be followed by the rest of the organization. Their decision making could be replaced by an AI, probably more readily than the specialist engineers in fact, but it comes back to humans being tribal. Would the middle management follow an AI overlord? Would the engineers buy into the AIs decisions? The CEO's job is to decide on the future direction of the company, and then convince both the owners and the employees that this is a good direction. The first part is easy to replace with AI, the second part isn't. At least not for now. I guess taken to the limit, the CEO will become replacable the moment that the employees have been replaced with AIs, because that that point there's nobody left to lead really. One could actually argue that this is tantamount to the CEO cutting off the branch they are sitting on. After all, once all the employees are AIs, what's to stop the shareholders from saying: \"Hold it right there Steve/Jeff/Mark/etc, why are we paying you big bucks? You can be replaced with an AI that will make much better decisions, and there are no employees left to lead anyway.\" reply robertlagrant 7 hours agorootparent> I guess taken to the limit, the CEO will become replacable the moment that the employees have been replaced with AIs, because that that point there's nobody left to lead really. This seems very software-centric. You can do this today - e.g. Red Bull famously outsources basically everything but marketing[0], so they already have very few employees. However they do have a lot of suppliers, and that all needs managing. [0] Their marketing is either simple TV ads or incredibly complex stuntwork and extreme sports. reply zug_zug 4 hours agoprevI don't usually love this type of post, but this one is the exception - very valuable. Just the section on how to rewrite alone communicates something incredibly valuable that grumpy-engineers like myself have great trouble getting others to understand. I don't have my mind made-up on XP, I've never worked at a place that actually supported collaboration (often workers spoke different first languages, vastly different experience levels, had minimal social graces, were uncomfortable asking questions), but I think it could exist with great effort and would have a lot of upsides. reply devin 3 hours agoparentIt is likely a bit of nostalgia on my part, but one of my first gigs had an owner that was focused on XP being the way we worked, and given how junior the team was overall, I think it produced excellent results and made for a fun, lively atmosphere. reply Sevii 20 hours agoprevI appreciate that they have a programming philosophy that they want people at the company to adopt. A common problem I see at companies that don't have onboarding is that people join the team with assumptions from previous jobs but you never level set them with the company. So 12 months down the line the new guy wants to change the process and you have to repeat the same discussions about what agile means for the nth time. Amazon does a good job of training new hire on the 'Amazon way'. Amazon does 6 pagers, they do design docs. Amazon does SOA. Amazon does not use relational databases. Everything has an API. Because of the 'Amazon way' and the training they do new team members understand at least some of the context and expectations. Is it the best way? Probably not but no one knows what the best way is anyway. At least they have a way. Saves a lot of effort compared to every new hire relitigating the process and architecture. reply saghm 16 hours agoparent> Amazon does a good job of training new hire on the 'Amazon way'. Amazon does 6 pagers, they do design docs. Amazon does SOA. Amazon does not use relational databases. Everything has an API. Because of the 'Amazon way' and the training they do new team members understand at least some of the context and expectations. As a counterpoint, a huge part of Amazon's culture (or at least, AWS's) in my experience was the emphasis on operations and the fact that they didn't have any separation between SREs/on-call engineers from the people who implement their services, and at least for me as someone who had never been on-call before in any meaningful capacity (due to my previous job working on a libraries rather than services), the training for it was basically non-existent on the two teams I spent time on. The \"training\" I did receive essentially consisted of being put on the rotation once to shadow, where I was able to sort of see what the actual on-call person did but didn't really have any explanation for how to know how to do them other than being told to read the runbooks, which were not really written in a way that was easy to understand for me as someone who was so new to learning all of the internal AWS tooling and ops in general. The next time I came up on the rotation, I was expected to be able to manage on my own, which essentially meant that literally no matter what occurred, I ended up having to escalate because I wasn't knowledgeable enough to fix literally anything within a timeframe that would have been reasonable. reply Sevii 3 hours agorootparentMy team didn't add new members to the oncall rotation for about 6 months to ameliorate this issue. But starting oncall at first is rough and even with months of context on our systems people usually take a few rotations before they really figure it out. We expected new members to have to escalate. reply wesselbindt 5 hours agorootparentprev> they didn't have any separation between SREs/on-call engineers from the people who implement their services I.e., treat DevOps as a way of working, rather than a role meaning something akin to \"Ops person who knows terraform, or k8s, or Ansible etc\". reply xwolfi 13 hours agorootparentprevWhich is the only way to learn tbh, you can receive as much positive reinforcement imaginable, nothing prepares you for a large scale incident like living through one, building the connections you need to solve it, getting the shame of your life, and losing sleep over your failure. reply saghm 4 minutes agorootparentI'm not really sure what you mean by \"positive reinforcement\", but I don't think it's possible to disagree more with this sentiment. \"building the connections you need to solve it, getting the shame of your life, and losing sleep over your failure\" isn't a strategy for teaching for something; it's a coping mechanism for someone trying to brute force their way through something that they weren't adequately trained for. Most people seem to think it's fine for companies to offload the entirety of the burden of learning to individual employees, and maybe I'm an outlier in this regard, but to me, this seems more like a cop out to avoid trying to actually solve the problem at the cost of the employee's emotional health. I'm not surprised that companies default to this, but it's also not surprising that burnout is so common in our industry when this is considered the \"best\" or \"only\" way to do things. reply pyrale 10 hours agorootparentprevNothing teaches you to swim like being thrown in the middle of the atlantic. reply infomaniac 14 hours agoparentprev> Amazon does not use relational databases This is false, at least in my very thin exposure to the company: I interviewed for a team last year which was maintaining EC2 SSH keys using MySQL. reply mnahkies 11 hours agorootparentI watched an interesting talk from the RDS team about how they dogfood RDS the other day https://www.pgevents.ca/events/pgconfdev2024/schedule/sessio... reply chupasaurus 6 hours agorootparentprevIf it was using MyISAM relations are in question. reply 9rx 5 hours agorootparentprevSQL, and therefore MySQL by extension, isn't relational. reply rrr_oh_man 20 hours agoparentprev> Amazon does not use relational databases Huh? reply vamega 17 hours agorootparentRelational databases are not the preferred storage mechanism at Amazon. If a team wants to use an OLTP relational database it’s possible that it will be a decision they will need to defend at any kind of design review. Of course there are relational databases running OLTP workloads, but it’s far away from the norm. There was a program a while ago to shift many RDBMS systems onto something else. reply emmelaich 16 hours agorootparentSo they do joins in code rather than SQL? Wouldn't that risk hiding scaling problems? reply grogenaut 15 hours agorootparentIt can but it's usually more obvious what's happening with code and how to fix it. Amazon wants you to think about the scaling issues while building as they don't want to lose the area under the customer curve on the far right. The theory is that with rdbms you have a magical box that scales vertically until it doesn't. And when it doesn't all you can do is scale back the customers until you fix it with sharding or a re-architecture. Basically you tend to hang yourself with indexes and transactions. Also generally when an RDBMS fails it fails down to like 30% throughput. reply mickael-kerjean 15 hours agorootparentI recently finished a contract at a company who has gone full on dynamo with the idea that if we have slow queries and dynamo is good for Amazon, then it's good for us too. I've ran some explain on the queries causing issues and of course those queries didn't leverage indexes like they thought .... reply redditor98654 7 hours agorootparentHow did you run explain queries on DynamoDB? Or may be you mean something different and I misunderstood you? reply bdavisx 4 hours agorootparentprevThat doesn't make sense, you have to specify the index when you're using Dynamo. reply scarface_74 4 hours agorootparentYou can do scan operations and if they use PartiQL it hides whether you are using indexes. I usually have an explicit DENY for dynamodb:Scan for the IAM role used to access the DDB table reply scarface_74 13 hours agorootparentprevLook up design patterns in DynamoDB. If you know your access patterns and you often do with well defined microservices. You don’t need to do joins. reply yazaddaruvala 11 hours agorootparentprevAmazon Retail has multiple systems to allow you to basically use SQL across databases (eg Datapath). I’m not sure about AWS. reply deskglass 20 hours agorootparentprevFriends say they typically use Dynamo and that using a relational database requires approval from a vp (because of scaling concerns). reply hyperliner 16 hours agorootparentRelated: Amazon kicked out Oracle from the company. Somewhere along the same timeline, the operational recommendation for teams was to not use relational databases. https://www.theregister.com/2019/10/16/amazon_ditches_oracle... reply switch007 10 hours agoprev> We’re an inverted organization. That means that tactical decisions are made by the people who are doing the work, not managers I've worked at various places where this was supposedly the system. Guess who had budgets, hiring powers, went to leadership offsites? Yeah not the ICs. It usually just means the C level will smile and nod while \"listening\" to your feedback instead of ignoring you completely Has anyone worked at a true inverted company where centuries of classical power structures are thrown out the window? I feel it can never be properly implemented unless in eg a cooperative reply Rygian 8 hours agoparentThe \"Valve Handbook for New Employees\" was an interesting read. I still don't know how much is fact and how much is fiction, but I liked some of the ideas there. reply pjm331 5 hours agoparentprevTo be fair - it says “tactical” decisions - not all decisions. reply lubujackson 22 hours agoprevWhat an amazing article on \"de-FAANGing\" the perverse org/incentive structure of most startup/tech places. Would love to see more of this type of leadership in the real world. reply Sevii 20 hours agoparentI like how he says he doesn't need FAANG level people. Then his next paragraph describes working at FAANG. \"We’re an inverted organization. That means that tactical decisions are made by the people who are doing the work, not managers. (In theory, anyway, we’re not perfect.) So we’re looking for people who have peer leadership skills, who are great at teamwork, who will take ownership and make decisions on their own.\" reply numbsafari 12 hours agorootparent> FAANG level people \"FAANG\" isn't a \"level\", it's just a cluster. reply holografix 17 hours agorootparentprevExactly right. People who have “leadership” skills are the ones that pay attention to their own leadership and are manage up more than anything else. They usually repackage people’s work around them into their own, take ownership and defend loudly their territory (project ownership) and methodically build relationships with leadership. Having “leadership skills” and being good a team work are often orthogonal to each other. reply khazhoux 16 hours agorootparent> People who have “leadership” skills are the ones that pay attention to their own leadership and are manage up more than anything else. No, it's actually people who can put together a technical plan and drive its execution with other engineers, or who can clarify complex problems esp. when there are conflicting opinions, or who can see problems before they become disasters and organize the right group of people to take care of it... There are many examples of leadership, which have nothing to do with the sour view of managing-up or taking credit for others' work. reply noirbot 14 hours agorootparentprevI mean, that's the sour way to put it. As someone kinda stuck in that sort of position now, I own a project and have been able to do very little of the work because I'm spending 90% of my time making sure leadership actually makes decisions so I can get the decisions my team needs in order to proceed. I'm spending hours in meetings with other teams to get them to prioritize our dependencies and data access needs. If you're not lucky enough to have management that's exactly technically aligned to your project someone has to be managing up and paying attention to leadership or else expectations will be totally off from reality. reply supriyo-biswas 14 hours agorootparentThis is not a sour way to put it; extreme levels of information hoarding, cookie licking[1] and building and defending fiefdoms were the norm from what I saw at my time at a FAANG. [1] https://devblogs.microsoft.com/oldnewthing/20091201-00/?p=15... reply adastra22 21 hours agoparentprevThe recent book The NVIDIA Way is about that org’s culture that prevent FAANG incentives from creeping in to destroy productivity. reply zeroonetwothree 21 hours agoprevThere’s a weird disconnect because on the one hand I agree you can’t measure productivity and on the other hand we all know that some engineers are vastly more productive than others. So what gives? reply Trasmatta 21 hours agoparentWe all \"know\" that, but there are also some engineers that only give a very strong illusion of being more productive. Maybe engineer #1 is constantly pushing up code. In the time it takes them to merge 15 PRs, engineer #2 opens only 1 - but maybe they thought really deeply about the problem, and their approach actually saves the team hundreds or thousands of hours of future development work vs how engineer #1 would have solved the problem. Part of what makes this so hard to measure is the long tail effects of development decisions. (Incidentally, that's also a source of burnout for me - the constant mental overhead of worrying about the long term implications of what I'm doing, and particularly how they effect other people. It's very challenging.) reply ebiester 3 hours agorootparentThe problem is that the vast majority of code will not have long term implications so long as it reaches a minimum of design, performance, and does its job without bugs. Consistency of patterns is more important than the optimal pattern for most decisions. There are some core areas of the application that are much more important, but they are often the earliest data structures and built before the problem is known. You will not know how your code will change, so make it as consistent as possible with the rest of the system until you know more. reply llm_trw 20 hours agoparentprevThe engineers are only productive because they have the support structure in place. The most productive fpga engineer I ever hired was so hopeless with git that I had to hire a second software engineer to babysit him. After I left both of them got fired and the product they were ahead of schedule on when I left had slipped 2 years behind before it finally got cancelled three years later. reply skeeter2020 16 hours agoparentprevI have an incredibly productive staff developer. Not only does he work a lot, he also produces, and it's very high quality. He also does a relatively poor job of upskilling his teammates, and is a little rough when mentoring. This is not intentional (i.e. he's not a jerk). Overall I don't know if, in the context of a staff developer, he's vastly more productive than say, another dev I have who produces less but levels-up his team better than almost anybody I've ever seen. reply ggregoryarms 15 hours agorootparentMaybe that other dev has a unique ability you should reward. Sound awesome. Focus on that. reply jprete 21 hours agoparentprevThose are two different concepts hiding in similar words. You can't [numerically or precisely] measure productivity, but some engineers are vastly more productive [such that you can easily tell the difference without a formal measurement]. reply fshafique 17 hours agorootparentGut feeling uses all your internal predispositions and biases. reply skeeter2020 16 hours agorootparentyou don't need to rely on gut feeling and risk bias. You can stop looking for the \"productivity metric\" and instead bet onmeasure, then track the change over time. It's the only thing that's ever worked for me. reply noirbot 14 hours agorootparentSure, but which measure you pick is itself a gut feeling and biased. It's the easy way to miss people doing whatever sort of work that you aren't measuring that may be what's letting everyone else excel at what you are measuring. reply Sevii 20 hours agoparentprevYou can measure productivity with correlated metrics. The issue has always been that the metrics which are easy to track don't line up incentives with the actual business goals. A group of 10 people who write 200k loc per year are probably more productive than a group of 10 people who write 10k loc per year. If you took those metrics and then did an investigation of the people in your company writing 10k loc you might find that they are slackers or that they write assembly. The issue is when metrics are used to stack rank teams with no thinking put into it. You can't treat correlated metrics like direct metrics. A logger might be evaluated based on how many trees he cut down in a day. There is no comparable way to pay software engineers piecemeal. Metrics are good, but people want to use them without thinking or taking context into consideration. reply bigs 20 hours agorootparentOr you may find they write higher quality code - less bugs, more performant code, or so on. reply Salgat 19 hours agoparentprevIt's an extremely complex mixture of many factors (which can vary wildly between two different productive engineers), and trying to make that into some magical formula ends up creating a system that can be gamed to superficially appear productive to managers. reply throw5959 21 hours agoparentprevYou can measure productivity by measuring the success, but that's kinda useless for day to day software engineering management. reply ChrisMarshallNY 21 hours agorootparentI tend to go by results, and for me, \"results\" means shipped* code that is used and accepted by end users**, can be maintained and extended***, and doesn't generate trouble tickets. * MVP doesn't count. ** Can include users inside the organization. *** It's OK if it requires senior-level ongoing support. I think expecting it to be maintained by monkeys is a bad idea. reply pinkmuffinere 21 hours agorootparentTo me, \"MVP doesn't count\" feels like a crazy take -- in many roles, the _only_ ask is to produce a series of different MVP's. I guess maybe the definition of \"MVP\" is a bit squishy, and these people-who-ship-MVPs themselves make MVP-MVP's, which shouldn't count as shipped? reply ChrisMarshallNY 20 hours agorootparentI spent most of my career, shipping finished product, which, in many cases, probably could have benefitted from an MVP-like \"tuning phase,\" but we called that \"beta.\" I think MVP generates more useful feedback, but I really don't like thinking of an MVP as \"shipping software.\" I also worked for hardware companies, where shipping stuff had some pretty serious stakes, and learned how to make sure we got it as good as possible, before getting it out the door. I like the idea of evolutionary design, and \"tuning,\" but I think it's a bad idea (for me) to deliberately ship bad software as an end-product. (Also, MVP, by definition, generates lots of trouble tickets. I am allergic to trouble tickets. It's totally a personal thing, but I live by it). reply skeeter2020 16 hours agorootparentprevsaying \"MVP doesn't count\" implies that you throw it away and then right \"the perfect system\" at some point. If you've ever had an MVP land you know that's not how it happens. reply ChrisMarshallNY 7 hours agorootparentI write \"as close to perfect\" as I can get. I know that \"The Perfect is the enemy of the good\" is a popular meme, but I have found that \"The perfect is something to strive for\" has been useful, for me. In fact, my way has been working for me, for decades. I'm quite aware that many folks do it differently, and that's one reason that I try to \"keep it in the I,\" and write about how I do it, and talk about the bar that I set, for myself. Most of the software I write, is free software that Serves a pretty small demographic. It can have a fairly outsize influence on the lives of the people that use my software, and I really care about the end-users of my work, so I tend to set a pretty high personal bar. I'm quite aware that I don't have many of the stressors that beset commercial software houses, so I sincerely don't feel \"snooty.\" In fact, I feel profoundly grateful to be in a position, where I can follow my muse. I really would like it if folks wrote better stuff, but I am also aware of the culture, and how that's next to impossible, these days. reply Trasmatta 21 hours agorootparentprevHow do you define success? If a product bombs, is that because of the engineering or the product design? reply throw5959 21 hours agorootparentI don't think it's possible to answer generally. Track what matters for your business. reply skeeter2020 16 hours agorootparentprevif it's successful, it's because of sales. If it fails, engineering didn't build the right thing / was too slow - it really doesn't matter. reply daz0007 20 hours agoparentpreva weird disconnect... of any true innovation or even reality... such vague objectional blandness... \"They'd beg to work for us\" - what the f8ck.... if they were the best they would not beg anyone how degrading...They would be there for a mission or wanting to improve something about themseves or other parts of the world. There's nothing here apart from Agile coach wanting to get some more work. 1984 was released in 1949, if anyone thinks these words / values really mean what is writen wow. People, Internal Quality, Lovability, Visibility, Agility, Profitability... reply RainyDayTmrw 1 hour agoprevThis pervasive corporate fiction tires me out so much. Everyone says they hire the best candidate, they are leaders in their area, etc. It feels very much like dystopian literature, where everyone knows the thing to be false, but is compelled to say it is true nevertheless. reply gwern 18 hours agoprevOP is an example of how AI-generated images are usually clutter. Not only do the images not add anything meaningful to the text, and arbitrary parts of the images could be deleted or randomized without affecting the reader's understanding, most of them could be randomly shuffled without anyone noticing. (Which makes them worse then clipart/stockart: if an article swapped the 'hacker hoodie' stockart with the 'neural net brain circuit' stockart, some readers would at least briefly be confused.) reply blululu 14 hours agoparentCame here to say exactly this. The ”author” didn’t even bother to use a decent quality image generator. The first image I saw maxed out my AI slop filter and I stopped reading. Made me wonder how much of the article was written by ChatGPT. reply drcwpl 11 hours agorootparentAgree wholeheartedly, it was unfortunate that the author did not use a good image generator, or even correctly prompt Dall-E to get better images, his take on using AI then became rather flimsy. I gave up at this point! reply magic_smoke_ee 9 hours agoprevFirst, it would be worker-owned co-op with very little turnover and intense competition for the few roles that get filled. reply languagehacker 3 hours agoprevThe advice seems reasonably good but I needed to bail on the post because of the cartoons. The anime crossed with precious moments style of illustration is just too creepy and inserts a lot of doubt to me personally on the authoritativeness of the author. reply weinzierl 11 hours agoprevInteresting, but not surprising, that Agility only made place 5, way behind Quality. It will soon be a quarter of a century since the Agile Manifesto has been published. It would be sad if we hadn't progressed since then. reply heeton 10 hours agoparentThe article uses “Internal Quality” which has a specific definition for them. And their definition of agility is not derived from the manifesto or Agile. Indeed, they list Quality immediately followed by customer happiness (“loveability”) which is aligned with XP, the practice supported in the article. The agile manifesto isn’t the only way to deliver good results in software. reply sisve 10 hours agoparentprevI did not understand it as an ordered list? Just a set of 6 items. Did i miss anything? What makes you conclude that this is a priortized list? Profitability is number 6. Who cares about internal quality if you are not making money in the long run reply agos 9 hours agoparentprevif you'll allow some negativity, I wouldn't expect a talk at a \"Scrum Gathering\" to care much about the Agile Manifesto reply mft_ 20 hours agoprevA great post, well worth reading. The principles in the section on 'people' are applicable to any organisation in any industry. I especially liked the simple 'career ladder' example, for a) focussing on mostly on behaviour rather than knowledge, and b) for being simple to use and track progress with. (I've never seen anything like it in any of the large organisations I've worked in to date.) reply mrbluecoat 20 hours agoprev+1 for Extreme Programming. I've been a fan from the beginning when Agile was all the rage and my recommendations for XP were met with blank stares. reply Trasmatta 20 hours agoparentI'm glad that it works for some people, but I did not like the forced pair programming in XP at all. And I found adherents to XP were even more cult like than Agile teams. reply Salgat 19 hours agorootparentDoes XP and pair programming actually require two people to be simultaneously working together at the same time? My understanding is that this includes one person who codes while another person looks at the results and reviews them afterward. The two are still working closely together and exchanging feedback, just at different points in the process in an iterative loop. reply p_l 45 minutes agorootparentNo, the point is that both \"driver\" and \"navigator\" (as some pair programming referred to the roles) are looking at the coffee simultaneously, just one has the keyboard at the time. This is extended to \"mob\" programming where you have whole team of \"navigators\" and one person at keyboard. reply NomDePlum 17 hours agorootparentprevMy understanding is that original meaning was that pair programming requires the pair work together at the same desk and machine. With the ability to share screens/IDEs remotely the need to be at the same desk may have shifted, but working together is intrinsic to pair programming I believe. The original text went into some detail about making the desk work for 2 people, and having screwdrivers available to do so, which for some reason always amused me. reply skeeter2020 16 hours agorootparentprevresist the oxymoron of agile zealot - the first rule is do what works for YOU reply paulcole 15 hours agoprev> Everybody wants the best people in the business. A fundamental mistaken belief. Who wants to pay for the very best people when the 97,000th best person will do? Also how can you decide who the best people are when you can’t even measure their productivity? reply joeldo 19 hours agoprev> There’s more details here than I can explain today, but you can use the QR code to find a detailed article, including the documentation we use for the skills. Why not just provide a clickable link given this is an article on the web? reply jh00ker 19 hours agoparent> This is a transcript of my keynote presentation for the Regional Scrum Gathering Tokyo conference on January 8th, 2025. Because the images are slides from a presentation that the audience could scan. >Thank you for listening. The text of the article appears to be the \"talk-over.\" reply philbo 11 hours agoparentprevThe clickable link is in the right margin just underneath the (first) QR code image: https://www.jamesshore.com/v2/blog/2024/update-on-software-e... reply NoMoreNicksLeft 3 hours agoprev> It was September 2023 and my CEO was asking me a question. > “How are you measuring productivity?” This is sort of like your girlfriend asking you \"how much do you love me\". Except if you answer wrong, it's still more likely that your girlfriend will stay with you than that you'll keep your VP of engineering job. reply gpi 22 hours agoprevThat was a breath of fresh air. Thank you James. reply gijoeyguerra 15 hours agoprev...starts off by saying you can't measure productivity. Then proceeds to explain how to measure productivity. Very sneaky. reply raldi 16 hours agoprevWhat's the clickbait headline refer to? I can't find any mention of the company in a skim of the article. reply dang 16 hours agoparentI've replaced the clickbait title with a more representative sentence from the article. reply theideaofcoffee 19 hours agoprevThis was a really great read, lots of insight and things to think about. But it's also depressing to see how good things could be and how poorly (IME) most orgs are run now. I know I've seen the exact 180-degree opposite of almost everything mentioned here: no team leadership or empowered people, no clear path to the next level for those interested, lack of communication, no emphasis on internal quality, overall pathological product choices (or lack thereof) and on and on. I'd kill to be part of an org that puts this much thought into everything. reply cynicalsecurity 48 minutes agoprevAn article full of buzzwords and a semi-amusing story of how a manager successfully bullshitted a delusional CEO. Okay. reply nine_zeros 19 hours agoprevThis is a very nice post - not because the actual suggestions are good, but it demonstrates what a really technically sound VP looks like. In most large tech companies, VP level people are so detached, delusional, and unskilled in engineering, that they end up undervaluing what engineers really do. They are unable to explain it beyond stack ranking them. As an example, this post talks about how simplicity and maintenance brings value. But my VP literally fired people who did not produce new complex impact. Just goes to show why so many people hate the big tech industry as employees. It is being run by charlatans who abscond from any real leadership. reply LudwigNagasena 17 hours agoprev [–] Saying that you can’t measure productivity is a pseudo-truism and a cop out of doing your job. How do you measure productivity = how do you decide whom to promote; how do you decide whom to fire; how do you decide how to distribute bonuses; etc. If you can’t measure productivity, you can’t do your job as an engineering manager. It’s not a question that should have been asked 3 months into a job. It’s a question that should have been asked during the hiring interview. reply skeeter2020 16 hours agoparentI think I agree with you - definitely about the cop-out part. It's kind of the wrong question; it's not \"how do you measure productivity?\" but you have some hypothesis, \"what do you optimize?\" If you're right, it might help you win (maybe you win or lose regardless) and you get credit. If you keep experimenting, you might get better or worse, adjust and repeat. I'm sick of these (implicitly) absolute measurement questions. I pretty much refuse to look at anything other than the delta. reply n4r9 8 hours agoparentprev [–] So... how do you measure productivity? The engineering managers you refer to probably attempt to measure productivity, but may well fail to correctly identify high and low performers. reply LudwigNagasena 46 minutes agorootparent [–] There are quantitative measures like story points velocity, time to merge, code churn, etc. Those can be simply measured but also simply gamed, so they should be used with caution. There are qualitative measures like code maintainability, satisfaction of stakeholders, communication skills, system design skills, etc. Those have to be assessed using peer feedback, stakeholder feedback, 1-on-1's, activity during calls and meetings and so on. Those are harder to measure, but also harder to game. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The keynote at the Regional Scrum Gathering Tokyo by the VP of Engineering focused on key elements of a successful product engineering organization: People, Internal Quality, Lovability, Visibility, Agility, and Profitability.",
      "Emphasized the importance of attracting top talent, maintaining internal quality by reducing waste, and ensuring product lovability by understanding stakeholder needs.",
      "Highlighted the use of Extreme Programming (XP) and Fluid Scaling Technology (FaST) to enhance agility and adaptability, while aligning products with business goals for profitability."
    ],
    "commentSummary": [
      "The discussion explores the characteristics of an ideal product engineering organization, focusing on management styles, decision-making processes, and productivity. - Opinions vary, with some advocating for distributed decision-making over hierarchical structures, while others emphasize the importance of CEOs in making decisive choices. - The conversation also addresses the challenges of measuring productivity and the influence of organizational culture on innovation and employee satisfaction, referencing companies like Meta and Amazon as examples."
    ],
    "points": 190,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1736710027
  },
  {
    "id": 42682876,
    "title": "Live London Underground / bus maps taken down by TfL trademark complaint",
    "originLink": "https://traintimes.org.uk/map/tube/",
    "originBody": "Live London Underground / bus maps Back in June 2010, I attended Science Hackday in London. The TfL API with open data had just been announced and I thought it’d be fun to build a live tube map based on the available information of arrival times at future stops. (hard at work, look at the tiny computer, tiny phone!). This was the result, a page showing all the trains on the network in approximately real time, using “a bit of maths and magic” as I put it. I liked how occasionally trains would break away and zoom off based upon some miscalculation of where they should be. It proved quite popular, with coverage from the BBC and the Guardian to the Daily Mail. In fact, it proved so popular that they had to take down the API for a few months due to demand (not from me! My site made few calls to generate the data, one per line every couple of minutes). I added a Skyfall version after the film’s release, a schematic map version later on (after I had time to work out all the co-ordinates!), and separately a map that did the same for buses and any London bus route. I have kept this running ever since, through API changes, bus route changes, server moves, and anything else that’s come up. TfL have never minded the site, as far as I am aware. But then on 7th January 2025, I received two emails out of the blue; a vaguely personal one from someone at TfL telling me to remove the schematic Tube map, and my hosting provider received a very impersonal one from the “Trademark Enforcement team”. (That second one says “We informed the registrant of our complaint, but were unable to resolve this issue.” but presumably they can’t mean the first email sent about an hour earlier? This is the first I’ve ever heard from them.) This is of course perfectly within their right so to do, though I would have hoped for a different approach. Sure, I could have made some changes and kept the maps up, although as above they have been fine with it for many years. But I believe it is possible to both “protect” your trademark (or whatever you think this is) and not treat people like this. And rewarding this heavy-handed approach (by continuing to provide a useful addition to their service with no contact bar this) to me feels wrong. The internet isn’t what it was 15 years ago, and I can’t be bothered dealing with large organisations removing any semblance of joy from it. I’m sure they won’t care, but I am just too tired. So sorry, the maps are all gone. Feel free to contact TfL if you found these maps useful, or fun. Or if you work for some bit of TfL upset at what some other bit of TfL has done, do let them know. My traintimes.org.uk is still there. Matthew Somerville PS If you want live bus information and maps, there’s bustimes.org.",
    "commentLink": "https://news.ycombinator.com/item?id=42682876",
    "commentBody": "Live London Underground / bus maps taken down by TfL trademark complaint (traintimes.org.uk)165 points by fanf2 6 hours agohidepastfavorite61 comments madars 2 hours agoThe email TfL sent [1] to traintimes.org.uk ISP looks like a catch-all sent in haste. For example, it doesn't even mention the map. Instead, it invokes trademark registration numbers but these resolve [2] to LONDON UNDERGROUND and UNDERGROUND wordmarks and the roundel, none of them covering the map geometry as far as I can tell. It alleges a violation under Anti-Cybersquatting Consumer Protection Act [3] but the act only applies to domains - and TfL never claims \"traintimes\" to be an infringing domain name (certainly doesn't look so under the marks cited). And, as a sibling comment points out, the act is a U.S. law but the site appears to be hosted in the U.K. If you think you have a case about the map, why not state it explicitly? The cynical answer is that ISPs have incentives not to care so making a case doesn't matter but ... [1] https://traintimes.org.uk/map/tube/email2.txt [2] One can look them up in https://www.tmdn.org/tmview [3] https://www.law.cornell.edu/uscode/text/15/1125 reply mmastrac 1 hour agoprevThis licensing page appears to be a new addition to TfL, which probably suggests some bureaucrat negotiated a deal with a creative agency to license the tube map for pennies on the dollar, resulting in them sending out these notices. https://tfl.gov.uk/info-for/business-and-advertisers/map-lic... Amusingly the notice was sent with references to USA trademark law. I am not sure how it works with regional governments, but copyright information for government-produced works tends to live as a \"crown copyright\" in the UK and former colonies. reply andiareso 4 hours agoprevI don’t see the issue. You were using the TfL schematic map which is very much a form of art. I don’t think it’s unreasonable that they asked you to take that specific map down or continue with a license. To remove the whole site because of that seems petty. It was clearly stated in their api documentation. It’s no different than getting a license or usage rights for hosting an image or video on your site. Just because you are a hobbyist doesn’t mean you don’t have to follow the rules. This is coming from someone who is extremely pro fair-use and right to ownership. reply orra 3 hours agoparent> You were using the TfL schematic map which is very much a form of art The site was taken down by a trademark complaint, not a copyright complaint. reply dcrazy 1 hour agorootparentTrademarks apply to artistic works that identify an entity. See the TFL roundel. reply tuukkah 52 minutes agorootparentYou can use a logo to refer to the entity in question. Is it not fair use if you refer to a subway station using the subway station logo? reply rozab 55 minutes agoparentprevThis is particularly galling because TfL never credited or compensated the designer of the map, Harry Beck, until long after his death. reply darrenf 2 hours agoparentprev> To remove the whole site because of that seems petty. How have they removed the whole site? It literally says \"My traintimes.org.uk is still there.\" at the bottom of the page. Looks like only the maps have been removed. (Edited to add: I'm a long time traintimes.org.uk user who never even realised they had maps on the site, so consequently I am happy the whole site has not been taken down) reply rossng 3 hours agoparentprevThey could have easily offered a free license to use the trademark. This project wasn't harming them in the slightest. Demanding the map's removal and implying that he will have to pay to put it back up shows a lack of empathy. reply crazygringo 2 hours agorootparentNo, trademarks are genuinely important because they allow consumers to distinguish between official things that an organization stands behind, versus hobbyist projects, imitators, etc. But all the creator had to do was to remove logos and possibly change the name so there would be no confusion around whether this was an official project or not. And it seems like the geographic map was fine, only the schematic map would have been an issue because its design is presumably specifically copyrighted and yes you would have to license that just like any other map. The letters he received may have been heavy-handed but there's nothing wrong with the general principle of it. reply samwillis 4 hours agoparentprevIt should be easy for a human at TfL to make an assessment on something like this, see the autistic and technical value, and offer a free but heavily restricted license to the developer. But is suppose many organisations just don't give people the autonomy and authority to do such tings. reply VoidWhisperer 3 hours agorootparentFor that specific map, based on what the email he got sent from TfL said, I don't think they directly have permission to issue that license - their site says people have to go through the partner who produced the schematic art to get a license reply tankenmate 3 hours agorootparentExcept the schematic art is covered by copyright, not trademark. reply tuukkah 59 minutes agorootparentOn their website, TfL says both things: 1. The map is covered by copyright. 2. The only way to get a license is to buy one from their map partner. > We protect the map under copyright and officially license it for brands and businesses to reproduce it. > To use the map in your design, you must have the permission of our map licensing partner, Pindar Creative. This is the only way to officially license the map, no matter how you'd like to use it. Yet, they don't even mention the case you might be a third-party developer providing a non-profit service. https://tfl.gov.uk/info-for/business-and-advertisers/map-lic... > For registered charities and schools, the licence is royalty free, but we still charge an artwork fee of £352 + VAT https://tfl.gov.uk/info-for/business-and-advertisers/using-t... reply tuukkah 3 hours agorootparentprevIf TfL hasn't bought the full rights to their map layouts, the shame is on them. reply polotics 4 hours agoparentprevthere is such a gap between hobbyist developers that do things for fun if and only if it stays fun, and consumers who will qualify as petty the reasonable decisions to pursue some other one of the very many other things-to-do-for-free that could be more fun. don't you think? reply rad_gruchalski 4 hours agorootparentTo own the trademark and defend it means having to proactively find and fight violations. So, nope. How is the trademark holder supposed to know who they are dealing with? Because they said so? Well, in that case I know a Nigerian prince who would like to send you some money… reply jkestner 4 hours agoparentprevEh, if your complaint comes 15 years after the site is launched, and then you wait an hour before following up with a legal notice, I’d be feeling petty too. reply ForHackernews 4 hours agoparentprevThis is like claiming you need to license the Mercator projection. The TFL tube map is almost 100 years old[0] and while we can argue if industrial design is \"art\" the main point of the tube map is utilitarian - to help people navigate the underground. [0] https://www.tandfonline.com/doi/full/10.1080/00087041.2021.1... reply d1sxeyes 2 hours agorootparentSo copyright should only apply to stuff that’s not useful? Whatever the term of copyright should be, there’s no doubt that it was a significant endeavour to create it, and it creatively expresses the topography of London. Your analogy doesn’t work very well I’m afraid. The Mercator projection is 500 years old, and generally speaking, you can only copyright specific works, not processes. If you want to protect a process from being used by others commercially, you need a patent, and generally patents are not as long lived as copyright. reply mmastrac 1 hour agorootparentNo, but it's far more likely that fair use applies to something that is more _useful_ than _creative_, ie: maps and dictionaries. Regardless of how much effort a copyrighted work to produce, most Western countries have a fair-use equivalent to transformative use of a work: https://lawdit.co.uk/readingroom/intellectual-property-law-g.... reply d1sxeyes 2 minutes agorootparentOdd that this site is a “.co.uk” but talks about the US fair use doctrine. The UK does indeed have “fair use” (actually called fair dealing) but this wouldn’t come under it as far as I can tell (IANAL, etc.) saaaaaam 1 hour agorootparentprevThe copyright exceptions in the UK are very limited. There would not be a copyright exception in this situation. reply bloqs 4 hours agoprevThis STINKS of 'new person in the job with no personal connection or background to certain relationships, and crucially, allowances, established by the previous person, wants to test/establish their power in new role and is looking for easy ways to do it' reply shermantanktop 44 minutes agoparentThe Dolores Umbridge Effect reply helsinkiandrew 4 hours agoprevTo be fair they explicitly state don't use their branding in the API documentation. They made £200K in 2024 from licensing - I think about a millionth of a penny per journey https://content.tfl.gov.uk/tfl-advertising-annual-report-202... reply BonoboIO 2 hours agoparent200k? That seems very little to be honest. Who is licensing what from the London Metro? reply lostlogin 2 hours agorootparentI’m m wondering if it’s people putting the tube map on things. There is an application form and guidelines. https://tfl.gov.uk/info-for/business-and-advertisers/contact... reply qeternity 2 hours agorootparentprevIf that is indeed the actual number, it most certainly has deeply negative ROI. There is no way they are running a licensing department for under £200kpa. reply harry_beck 4 hours agoprevIt's a sad irony that TFL didn't even want this version of the map originally, and it was given to them, and maintained for free (I think) by a map enthusiast like the author of this version I think Harry Beck would think history was repeating itself https://youtu.be/cTLCfl01zuE?si=gb-PsswlfW8hSLGW reply soco 5 hours agoprevIn case you ever wondered how can some people be against trademarks and copyrights in general. reply AndrewOMartin 5 hours agoparentItch.io was taken down recently be an over-zealous AI-based service to detect trademark infringement and notify the authority, I wonder if this is a similar case. It'd be nice to know if someone in TFL actually requested this, or if it's a case of fanatical legal enforcement as a service. reply lou1306 4 hours agoparentprevThis is a perfectly legit application of copyright law/brand protection, and I'm no fan of either. The Tube map is copyrighted work that requires a license to be used, even on a free service. The fellow should have stuck to normal OSM overlays, and none of this would have happened. What is controversial about this? reply ForHackernews 4 hours agorootparentIf they want to control who uses the roundel logo, so be it, but I think it's absurd that copyright applies to the tube map itself. In the United States, government works like this would be public domain. reply davidhyde 5 hours agoprevIt’s CRITAAS proliferation. Copyright infringement takedown as a service. It only feels unfair because right now it’s asymmetric. Wait until there is an automatic SAAS to counter balance it and let the bots duel it out whilst laws eventually catch up. reply wkat4242 1 hour agoprevI was wondering what disappeared, I think it's this: https://web.archive.org/web/20241204060614/http://traintimes... Strange enough the owner also seems to have removed the OpenStreetMap-based live train maps. Those can't be copyrighted? reply mikelward 5 hours agoprevI would complain to TfL, but their complaints form is broken > Sorry, something's gone wrong We have a technical problem right now. One of the following options might help you: reply alvis 4 hours agoprevMany years back, it was a thing that TfL actively encouraged developers to use their data, and I was lucky to be a winner of a notional campaign thanks to that. But now, the headwinds apparently have changed. Sad :( reply helsinkiandrew 4 hours agoparent> Many years back, it was a thing that TfL actively encouraged developers to use their data They still do but not the branding: \"Use our data - not our brand\" https://tfl.gov.uk/info-for/open-data-users/design-and-brand... reply tuukkah 3 hours agorootparentHowever, the map layout is data, not branding. If your service has to alter the layout, it's more confusing to the passengers who TfL should be thinking about. reply NoboruWataya 1 hour agorootparentIt has a very unique look and feel, and I'm not sure it is \"just\" data given that the location of stations on the tube map doesn't actually correspond to their geographic locations within London. I do think it is capable of forming part of the TfL brand, though by now it feels quite generic to me. Regardless of whether the map is capable of being protected by IP law (TfL certainly seem to think it is), this just feels stingy and pointless on TfL's part. They are a public service after all, and these maps arguably furthered their public mission. Given how popular the map is I would much prefer they published it under a licence allowing free non-commercial use with attribution (including a statement that the user is not affiliated with TfL). reply mhandley 2 hours agoprevTFL has obviously been aware of his use for 15 years, as the website was widely publicised in 2010. They have not taken any action to defend any trademarks they think he violates in all that time. IANAL, but I would have thought that if he wanted, he probably has a good case to invalidate those trademarks on the grounds that by not defending them for so long, they have become generic. But in the end, it's probably not a good use of his time and money to fight them on this. reply pluc 5 hours agoprevBetween that and CityMapper being part of the Gravy leak, things look pretty bleak for UK third party transit apps. reply gruez 5 hours agoparent>CityMapper being part of the Gravy leak You should take that leak with a huge grain of salt because the alleged list of apps stealing your location contains hundreds of apps that doesn't even contain location permissions. reply d1sxeyes 1 hour agorootparentThis is a bit of an incorrect read on what this leak is. Gravy gathers location data about people from multiple sources but not directly from consumers. Gravy’s customers buy this data. As far as I understand, this is a list of locations and apps used by a person, but without much context. A typical response to this has been something like this: > Grindr has never worked with or provided data to Gravy Analytics. We do not share data with data aggregators or brokers and have not shared geolocation with ad partners for many years. Transparency is at the core of our privacy program, therefore the third parties and service providers we work with are listed here on our website. Note how carefully written this is to imply they don’t share any data at all, but they stop short of saying “we don’t share any data with ad partners”, just geolocation data. But for companies like Gravy, their whole business is about getting data. So it’s not at all implausible (or even unlikely) that this represents an event where a user opened Grindr (conceivably sold to Gravy by one of Grindr’s ad partners following an impression), and the same individual’s location was determined by some other method (for example, IP address geolookup, or bought from a company which IS supplying data to Gravy directly and has location permissions). Take the leak with a grain of salt, sure, but it’s looking reasonably genuine to me. reply kmeisthax 50 minutes agorootparent> We (...) have not shared geolocation data with ad partners for many years Mobile operating systems don't have good (if any) support for opening things in subprocesses with restricted permissions from the rest of the app. So if Grindr loads an ad, that ad runs with Grindr's permissions. Same for any analytics code that ad uses. So if Grindr gets geolocation, even temporarily, so does every ad partner they have, whether they like it or not. And the thing is, ads are a bottomless pit of third-party JavaScript. Nobody trusts nobody in the ad space, so everyone wants their own trackers doing their own client-side data collection. So Grindr doesn't have to know anything about Gravy Analytics, they just have to have an ad partner decide to use them and bam, they're compromised. reply pluc 5 hours agorootparentprevYeah, that's what they all say. Tinder and Spotify were both named specifically and both denied it. I don't trust any of them so I'm assuming they're lying, you do what you want. reply gruez 4 hours agorootparentWhy do you trust an unverified \"leak\" over statements made by multi-billion dollar multinationals? Sure, corporations can lie, but so can such leaks. Extraordinary claims require extraordinary evidence. If the leak is alleging something impossible (ie. stealing location data despite not having location permissions in manifest), then I'd need far more evidence than some csv list. reply blitzar 4 hours agorootparentTrust me, I wouldn’t lie to you for a billion dollars. I might have been caught lying before about these things but this time it’s different. reply nonrandomstring 1 hour agorootparentprev> Why do you trust an unverified \"leak\" over statements made by multi-billion dollar multinationals? Less incentives to lie. Edit: I had a think, and what I picked up on was the idea that sheer concentration of money might stand in as a signal for trust, and so whether somone with more money would naturally be more honest or dishonest than someone with less, is really more of an interesting question. reply imchillyb 3 hours agorootparentprevNot OP, but… I trust a leak from someone with no financial gain from the leak. I do not trust multi nationals worth several million, billion, trillion, to state truth. I expect them to lie until caught by a federal entity and fined. Guess how many times multi nationals lied to the public last year alone. Now you answer: “Why do you put any trust in what statements a corp releases?!” reply gruez 3 hours agorootparent>I trust a leak from someone with no financial gain from the leak. >I do not trust multi nationals worth several million, billion, trillion, to state truth. I expect them to lie until caught by a federal entity and fined. >Guess how many times multi nationals lied to the public last year alone. And what about the leak itself? \"You really think someone would do that, just go on the internet and tell lies?\" Here's an anonymous \"leak\" I found that says whatsapp is backdoored and sends your chats to the CCP: https://pastebin.com/uE4m694M . Are you going to believe it? If asked for comment, Meta is probably going to deny it, but obviously they're going to be lying for the reasons you mentioned. >Now you answer: “Why do you put any trust in what statements a corp releases?!” \"Trust\" isn't binary, it's a spectrum. I don't put much trust in corp releases, but I still trust them far more than an unverified source. Even if you put zero weight on \"statements a corp releases\", you can inspect the AOSP source code yourself and see that it shouldn't be possible for apps to steal your location data when it doesn't have location permissions, and therefore a list claiming that such apps are stealing your location data should be treated with extreme skepticism. reply braiamp 4 hours agorootparentprevIs not whenever they deny it or not, your device can attest to it. Both Google and Apple have no qualms screwing up with third parties in their apps. Also, apps have been datamined up the wazoo, if a company claims not to do something and does it, someone would have already howled about it. reply paul_h 2 hours agoprevI commuted the NJ/USA daily for a while and loved - https://njtranshit.com/ - there's always room for sites that deliver something extra to the official one :) reply krunck 3 hours agoprevOr just host it in a country that doesn't care about UK law. reply NoboruWataya 1 hour agoparentDoesn't really work when you are an individual who lives in the UK. reply PaulRobinson 5 hours agoprevThis feels a bit overzealous, \"just\" somebody following the letter of the job rather than the spirit of it over at TfL. I can see how it sort of happened though. TfL makes money from licensing The roundels and other ephemera are popular. From tourists buying licensed souvenirs, and other transport authorities licensing the signage system in use in their own regions, that is used to invest in the system as a whole (worth noting: TfL is not a private entity, it's non-profit making, everything it makes goes back into investment). Because of that popularity there are a lot of people who try and rip off the TfL brand and trademarks. Lots of tourist souvenir shops might be minded to get their own take on this material, and have some cheaply made and expensively sold to tourists, for example. Another transport authority might skip the millions invested in thinking about how to communicate clearly, and just \"lift\" TfL's thinking. That obviously isn't fair, if you think IP law is able to be fair in any way. So, yeah, there are people whose job it is to protect that revenue and protect the trademarked and copyright material that protect that revenue. But this is a hobbyist having fun. I don't think anybody thought that this was a service provided by TfL, and I can't expect he was making much of a living from it, or depriving TfL of revenue. It's really rather tragically sad that we're now in a World where good intentions on all sides can't really see each other. There is so much utter penny-and-dime theft and copyright infringing shit on the internet that requires constant purging by people who are expected to protect their own trademarks, that the assumption now is everybody is trying to make money off everyone else, and nobody wants to do any actual original work any more. I hope someone at TfL sees the light and comes up with a better license for use of their assets, and sees this for what it is, and reverses the decision. I doubt that'll happen, though. :( reply HackerThemAll 5 hours agoparentI think TfL should focus more on the \"transport\" part of their business. reply zimpenfish 1 hour agorootparent> I think TfL should focus more on the \"transport\" part of their business. I imagine they'd love to if they weren't constantly shafted by Government Du Jour when it comes to funding. reply cynicalsecurity 2 hours agoprevCopyright laws need a reform. reply InsomniacL 4 hours agoprev [–] > But I believe it is possible to both “protect” your trademark (or whatever you think this is) and not treat people like this. They probably do not like the way they are treated by having their artwork/maps/trademarks taken without permission when they might well offer it for free for good causes / hobbyists. I'm guessing there is some copyright notice they require too in the license to protect themselves. I don't know if they do or not, but just pointing out the hypocrisy. Seems the OP would rather keep their site down than ask so i guess we won't know. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In June 2010, a live London Underground map was created using Transport for London's (TfL) open data API, gaining popularity and media attention.",
      "On January 7, 2025, TfL requested the removal of the map due to trademark concerns, leading to its discontinuation despite potential adjustments.",
      "The creator's site, traintimes.org.uk, remains active, and live bus information is available on bustimes.org."
    ],
    "commentSummary": [
      "Transport for London (TfL) removed live London Underground and bus maps from the website traintimes.org.uk following a trademark complaint. - The complaint email from TfL was criticized for its lack of clarity and for referencing U.S. law, despite the website being based in the UK. - The incident underscores ongoing tensions between hobbyist developers and trademark enforcement, with debates on whether TfL should provide free licenses for non-commercial use."
    ],
    "points": 166,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1736772697
  },
  {
    "id": 42677608,
    "title": "Qubes OS: A reasonably secure operating system",
    "originLink": "https://www.qubes-os.org/",
    "originBody": "Toggle navigation Qubes OS Introduction Downloads Documentation News Team Donate Qubes OS A reasonably secure operating system Download & Install Version 4.2.3 What others are saying \"If you're serious about security, @QubesOS is the best OS available today. It's what I use, and free.\" Edward Snowden, whistleblower and privacy advocate \"SecureDrop depends on Qubes OS for best-in-class isolation of sensitive workloads on journalist workstations. Providing journalists with a sane way to handle untrusted content from unknown sources is part of our job, and Qubes gives us the tools we need to do that job well.\" Freedom of the Press Foundation, non-profit dedicated to supporting free speech and public-interest journalism \"When I use Qubes I feel like a god. Software thinks that it's in control, that it can do what it wants? It can't. I'm in control.\" Micah Lee, Director of Information Security at The Intercept, advisor to DDoSecrets \"Qubes OS gives us greater confidence in the security of systems being used to remotely access our servers, mainly because powerful physical and logical privilege separation between workspaces allows our engineers to select appropriate degrees of isolation for different processes.\" Let's Encrypt, non-profit, world's largest certificate authority More endorsements Media & Press \"The world's most secure operating system assumes you've been owned.\" by J.M. Porup \"For those willing to put in the effort, Qubes is more secure than almost any other operating system available today.\" by J. M. P. \"So Rutkowska flipped the game, this time in favor of the defenders.\" by Andy Greenberg More Media & Press News & Announcements XSAs released on 2024-12-17 Posted in Security on 2024-12-18 Fedora 41 templates available Posted in Announcements on 2024-12-07 Qubes Canary 041 Posted in Security on 2024-12-03 XSAs released on 2024-11-12 Posted in Security on 2024-11-29 QSB-106: Information disclosure through uninitialized memory in libxl Posted in Security on 2024-11-12 QSB-105: Missing enforced decorations for stubdomain windows under KDE Posted in Security on 2024-10-17 The NitroPad V56 is Qubes certified! Posted in Announcements on 2024-10-03 More News & Announcements What's Inside of Qubes? Secure Compartmentalization Qubes brings to your personal computer the security of the Xen hypervisor, the same software relied on by many major hosting providers to isolate websites and services from each other. Learn more Operating System Freedom Can't decide which Linux distribution you prefer? Still need that one Windows program for work? With Qubes, you're not limited to just one OS. Learn more Serious Privacy With Whonix integrated into Qubes, using the Internet anonymously over the Tor network is safe and easy. Learn more Research Qubes virtual mini-summit 2021 3mdeb and the Qubes team, August 2021 Qubes Air: Generalizing the Qubes Architecture Joanna Rutkowska, January 2018 Introducing the Next Generation Qubes Core Stack Joanna Rutkowska, October 2017 Introducing the Qubes Admin API Joanna Rutkowska, June 2017 Thoughts on the \"physically secure\" ORWL computer Joanna Rutkowska, September 2016 Security challenges for the Qubes build process Joanna Rutkowska, May 2016 Read More Research Partners Mullvad Freedom of the Press Foundation Invisible Things Lab We would love for you to fund or partner with us Join the Community! Have questions or need a hand? As a free and open-source project, our valued community of users and contributors from around the world are in the best position to help. Before diving in, we encourage you to read about staying safe, our discussion guidelines, and our code of conduct to help keep things positive and on-track. We welcome newcomers and returning users wanting to discuss Qubes and seeking to contribute. Visit the Qubes Forum Search qubes-os.org Go! Introduction What is Qubes OS? Endorsements Getting started Video tours Screenshots FAQ Help and support Security center Architecture Code of conduct Downloads System requirements Certified hardware Compatibility list Installation guide Verifying signatures Supported releases Version scheme Source code Software license Download mirrors Documentation Introduction Choosing hardware Installing and upgrading How-to guides Templates Troubleshooting Security in Qubes Project security Developer docs External docs News Announcements Userbase statistics Research GitHub Qubes forum Twitter Mastodon Reddit Facebook LinkedIn Team Report a bug Report a security Issue Inquiries Core team Emeritus Community contributors Donate How to donate Qubes Partners How to contribute Qubes OS © 2025 The Qubes OS Project and others Website source codeTor onion servicePrivacy policyReport a problemTerms of useSitemap",
    "commentLink": "https://news.ycombinator.com/item?id=42677608",
    "commentBody": "Qubes OS: A reasonably secure operating system (qubes-os.org)162 points by doener 20 hours agohidepastfavorite95 comments irundebian 19 hours agoHave used it for several months as my daily OS and dropped it because of bad graphics performance (only software rendering supported, many frame drops when watching HD videos on YT) and bad battery management. Due to software rendering the overall systems perfmance also dropped. So I cannot recommend it for people with high requirements on graphics and battery duration. Besides that it was an interesting and good experience. I think it would be good to make it possible to deactivate certain security features such as strict graphics isolation so that users can adjust their settings to their risk acceptance level. It would also be interesting to be able to optionally replace Xen with lighter isolation mechanisms, even if the user would compromise on security here too. reply dmm 17 hours agoparent> dropped it because of bad graphics performance (only software rendering supported, many frame drops when watching HD videos on YT) Around Firefox 92 or 93 the new GPU-based renderer ported from Servo was made default and performance under Qubes became much worse. Unfortunately, it seems applications increasingly assume the presence of video acceleration and don't prioritize software rendering. reply josephcsible 17 hours agorootparentIsn't it reasonable for applications to assume that, now that virtually all hardware has it, even super-cheap computers like the Raspberry Pi? reply adastra22 11 hours agorootparentThe issue for Qubes is security. GPUs can be used to subvert basically all the otherwise hardware-enforced security protections. reply creole_wither 58 minutes agorootparentIn a desktop, couldn't you assign a GPU to one video machine and in that scenario would there still be a security problem when there is only one VM using it? reply usr1106 11 hours agorootparentprevI understand GPUs are a security nightmare. If you want to have some understanding of your security, don't use a GPU. reply Narishma 16 hours agorootparentprevThe hardware may be there, but not necessarily the drivers. reply zamadatix 16 hours agorootparentThe drivers are fine for GPU accelerated rendering of the app surfaces, even on the Pi. Hell, the drivers are even there >98% of the time for accelerated decode of the video format itself to boot. Qube's unique choice in software only rendering for user applications is one born out of the isolation goals for security, not what the software/drivers/hardware could do. reply fsflover 57 minutes agorootparent> even on the Pi Only proprietary ones, so not for everyone... reply fulafel 14 hours agorootparentprevYes. Besides Qubes users, a big population of software rendering users is people who have old and/or buggy drivers that are blacklisted by Firefox. reply NegativeK 18 hours agoparentprevGiven the tendency for people to lower their unknowingly compromise their security for the sake of convenience, I can understand why a project wouldn't do that. Knowingly is different and is what you're requesting -- it's when someone is following some Stack Overflow post or some such and doesn't have the training (similarly with the SO commenter, potentially) to know the implications. It kind of feels like a tradeoff between protecting users who are critically in need of something like Qubes or expanding its reach to people who are less at risk and won't use it if it's too inconvenient. reply jwrallie 17 hours agoparentprevI could tolerate no graphic acceleration and battery issues as part of the virtualization overhead, but I had issues with sleep (it would sleep and wake up perfectly only with when plugged in) and other related problems such as Windows VMs crashing when waking up from sleep. I was using it well at home but could not stand it when I travelled around with my laptop. I think Xen is mostly at fault for the issues, but I’m sure using something like KVM would be insecure, or they would have migrated already. reply dmm 17 hours agorootparentDoes sleep and wake work for you with a standard Linux distro? If so a newer kernel might help,like the kernel-latest-qubes-vm package, might help: https://www.qubes-os.org/doc/managing-vm-kernels/#installing... reply jwrallie 15 hours agorootparentYes, it works perfectly. It’s a Thinkpad X260, not exactly new hardware, and even Debian works just fine. reply pgaddict 4 hours agorootparentWeird. Multiple people submitted HCL for X260, and not a single one mentions issues with sleep. https://www.qubes-os.org/hcl/ When I had similar issues in the past, I posted a question either to the mailing list or forum, and people were helpful. reply bobertlo 1 hour agoparentprevYeah, I could not do it without other computers to use, but after a year of keeping a system running it, I find myself mostly using my other systems for specific purposes like a windows machine for gaming (no web browsing ever lol), my macbook air for printing, managing photos, doing stuff with my iOS devices, etc. reply Etherdrake 18 hours agoparentprevQubesOS is best enjoyed with a hefty CPU, lots of SSD space and a multi-screen set-up (in my opinion). Have you tried using Freetube instead of Youtube? In my experience it works a little better. reply jwrallie 17 hours agorootparentThe most annoying issue I had was that even using mpv would lead to audio samples being dropped. I think I fixed it eventually by increasing buffer sizes, but I would expect at least audio should work out of the box. reply fsflover 55 minutes agorootparentYou could try something like this: https://forum.qubes-os.org/t/improve-video-playback-performa... reply orbital-decay 7 hours agorootparentprev>at least audio I imagine audio and other realtime loads having problems the most on a heavily virtualized system like this. reply pgaddict 4 hours agorootparentNot sure what \"mpv\" means in this context, but this reminds me the one actual pet peeve I have with Qubes - video/audio calls just don't work for me. It either doesn't work or the audio quality is really poor. I've tried all kinds of stuff, without much success. I'm using phone/tablet as a fallback, but it's not very convenient. reply dublinben 3 hours agorootparentmpv is a free (as in freedom) media player for the command line. It supports a wide variety of media file formats, audio and video codecs, and subtitle types.[0] https://mpv.io/ reply pgaddict 2 hours agorootparentThanks for the clarification. reply fsflover 1 hour agoparentprev> dropped it because of bad graphics performance (only software rendering supported This is by design, to provide high security, which is the point of Qubes. It's planned to allow GPU for chosen, trusted VMs: https://github.com/QubesOS/qubes-issues/issues/8552 Alternatively, you could perform a GPU passthrough, https://www.qubes-os.org/faq/#can-i-run-applications-like-ga... reply em3rgent0rdr 15 hours agoparentprev> only software rendering supported Isn't this something GPU Virtualization is intended to solve? reply samoit 11 hours agoparentprevI think you do have GPU acceleration in the Dom0 but I do not remember if you can use/install programs on it, it was the \"coordinator\" dom. reply Dalewyn 18 hours agoparentprev>bad graphics performance (only software rendering supported, many frame drops when watching HD videos on YT) It might help if you used a computer with CPU horsepower that actually exists. And in case this sounded facetious, any reasonable CPU from the past 15 years can handle software decoding of high resolution video just fine. This all said however, if you do actually need full use of all hardware resources then being constrained to software is certainly a factor worth considering. reply crest 17 hours agorootparentYou have to do more than just decode the the video stream to display it as smoothly playing video without dropping frames or audio samples or loosing sync. It requires always scheduling the context switches correctly between different virtual machines when using Qubes OS, performing multiple copies across protection domains. Brute force helps a lot, but do you want a ≥5GHz multi-core CPU burning 150W just to watch a single video stream with maximum paranoia settings? reply Dalewyn 15 hours agorootparent>do you want a ≥5GHz multi-core CPU burning 150W just to watch a single video stream with maximum paranoia settings? I mean, yes? We're not talking about bloat here, you're deliberately imposing significant overhead load for a specific purpose. You can't really subsequently complain about performance unless you bring sufficiently powerful hardware to compensate for that overhead. reply psd1 7 hours agorootparentRight, but in a discussion about Qubes, it's germane to explain why you stopped using it reply kllrnohj 13 hours agorootparentprev> any reasonable CPU from the past 15 years can handle software decoding of high resolution video just fine. 4k VP9 from youtube takes my 5950x around 20-25% CPU usage to handle with hardware acceleration disabled. The fastest consumer CPU available 15 years ago could not handle that. Hell, even CPUs from 10 years ago couldn't do that. Add power & thermal limitations of a laptop CPU? Not a chance. And that's just VP9! HEVC or AV1 would really put the hurt on. reply Dalewyn 10 hours agorootparent>4k To be pedantic, OP specified \"HD\" which is 720p. I gave him benefit of the doubt by saying \"high resolution\" in my reply, but I think 4K is unreasonable given the provided context. I'd wager 1080p (\"Full HD\") at most. There's also the question of frame rate, though we can probably safely assume either 29.976 or 59.952 fps since it's Youtube. As an aside, software decoding performance can vary pretty significantly depending on the codec used for both encoding and decoding. Bit of a history lesson, CoreAVC was infamous for being very easy on the CPU compared to other h.264 decoders like ffmpeg. reply irundebian 5 hours agorootparentCorrection: I think I experienced noticeable stutters with Full HD videos not with HD videos. reply pgaddict 4 hours agorootparentI occasionally see stutters too, even with Full HD video. Or more precisely, mplayer complained about slowness and having to drop frames. It often helped to actually give the VM more cores (not just the default 2), but sometimes it was due to some weirdo codec/quality setting, and recoding the video just solved it. Sometimes switching to vlc (from mplayer) helped. Other times it was simply due to the sys-usb vm being overloaded. reply irundebian 5 hours agorootparentprevI'm using an Intel i7-8850H with 6 cores so I think it's powerful enough. It's not that I couldn't watch HD videos but I was experiencing stutters and it left me with the feeling that the CPU is insufficiently utilised. reply Dalewyn 3 hours agorootparentI certainly rescind my insufficient CPU horsepower accusation in that case. I'm not entirely familiar with Qubes's innards, but the overhead it imposes must be substantial. reply pgaddict 15 hours agoprevI'm using Qubes OS as my primary for years - I think I started with the 2.0 release in 2014 (I might have tried/used the 1.0 release, I don't recall.) and I was immediately hooked. I understand the usual story is that the goal is security benefits, and the compartmentalization (or rather the implied inconvenience) is the price for that. But for me the compartmentalization turned out to be a benefit on it's own, and actually convenient. I find it extremely convenient to have multiple isolated / virtual workspaces for different stuff, even if you assume attackers / malice do not exist. Having separate VMs is not the same as having separate folders. I also love the VM templates, which allow me to do all kinds of experiments (e.g. install packages in the app VM, which disappear after restart). Or run VMs with a mix of distros/versions/... Yes, I could do some of that with plain VMs, but Qubes integrates that in a way that I find very convenient. The commands for copying stuff between VMs are muscle memory at this point. Yes, there are limitations, like the lack of GPU acceleration. But movies in 1080p play just fine without it, and I'm not a gamer, so I don't mind much. I can't play with CUDA etc. on these QubesOS machines, and scrolling web pages with large images is laggy, but I find this to be an acceptable price. I went through multiple laptops / workstations over the years, and the situation improved a lot I think. Initially I had to solve quite a few issues with installer, some hardware not working (or requiring setting something special), or poor battery life on the laptops. But after a while that mostly either went away, especially once I switched to laptops with official Linux support (Dell Precision were good, I'm on Thinkpad P1 G7 now). The battery life is pretty decent too (especially once I disabled HT in BIOS). Is it perfect for everyone? No, certainly not. But it sure is great for me, and I hope they keep working on it. reply irundebian 4 hours agoparent> scrolling web pages with large images is laggy Now that I've read this, I can also remember that I was also annoyed by jerks when scrolling web pages. I also found the backup management too complicated. I didn't want to back up entire VMs, just the data within the VMs. In principle, I would have had to start up all VMs for backups and run a backup script for each individual VM. reply pgaddict 2 hours agorootparentI only noticed the jerky scrolling on pages with a lot of images, particularly hires + CSS effects (blur etc.). Everything else feels OK to me (I'm sure it could be smoother, but it's not too bad so I haven't noticed). For backups, I don't them the qubes way, I do \"regular\" backups within VM using rsync/duplicity/... When moving to a new machine I prefer to setup everything from scratch (and then restore the data). And it gives me all the features like incremental backups etc. reply vigilans 12 hours agoparentprevI’m in the same boat. Love the compartmentalization and being able to route VMs to different network backends and the ability to create ephemeral domains for quick tasks. Thank you Joanna, Marek, Andrew, and all the wonderful contributors. I couldn’t live without Qubes. reply tasn 16 hours agoprevI've always wanted to switch to qubes, but it just feels so constraining. It's safer to never leave the house, but I don't want to live in a self imposed prison. On the other hand, the isolation provided by containers and flatpak is more accessible, but with a much larger attack vector. Maybe we need immutable OS + an audit layer on anything that could allow exploits to persist (bashrc and the likes). reply bobertlo 1 hour agoparentI just really appreciate separating things like access credentials from things like web browsers without running multiple accounts. I do all my most and least secure activities on Qubes and use other computers for a lot of the other stuff in between. reply irundebian 4 hours agoparentprevAfter Qubes OS I ended up using Fedora with Wayland, Flatpaks and running applications as different users but this introduced other problems. The security profiles of many \"flatpacked\" applications are quite permissive (see https://flatkill.org/) so that they could be circumvented. Besides that I'm experience some convenience issues when accessing files on my drive. It's especially annoying when using \"flatpacked\" office such as onlyoffice. reply Crontab 18 hours agoprevEven though I never used Qubes OS I used to really enjoy Joanna Rutkowska's passion for it. Other women who's computing enthusiasm I enjoyed was Jessie Frazelle's writing and speaking about running everything in Docker on her laptop and Sacha Chua's love for Emacs. reply Fnoord 18 hours agoparentThere's a lot of awesome females in the infosec community. Check out the podcast Darknet Diaries for a glimpse. Some of the coolest red teaming podcasts were (IMO) with women. In this context, I'd like to mention Dr. Melanie Rieback. She is 'the CEO/Co-founder of Radically Open Security, the world’s first non-profit computer security consultancy company.' Previously in the 00's known for her research in RFID security. Or have a look at hack conferences such as recently 38C3. reply nine_k 17 hours agoparentprevLet me also add the brilliant https://justine.lol reply oswalk 18 hours agoparentprevnext [2 more] [flagged] NathanielK 18 hours agorootparentWhy do you think you need to clarify that? reply mikewarot 4 hours agoprevSince it doesn't look like Genode is going to be ready to be a daily driver for a while, Qubes looks like something ALMOST capabilities based that I could live with. Some day I'll just be able to run stuff without worry.... but it's not going to be any time soon. Can I run old versions of stuff like MS-DOS or Windows 3.1 under it? Or my beloved Windows 2000? Windows 2000 with Office 2000 pro (with the patches to read the new office 2007 formats) would be awesome. I miss outliner mode in Word 2000. reply czk 4 hours agoparentyou can with dosbox/pcem or something similar (dont think modern xen handles 16bit virtualization) -- but you can definitely run Windows 2000 in a VM on Qubes, but the windows tools are not supported on anything under 7 reply zvmaz 19 hours agoprevWith zero-click exploits that we certainly do not know of, Qubes OS offers some peace of mind. reply armSixtyFour 18 hours agoparentUnless there's a zero day in Xen in which case the entire security model falls apart. With all these cloud providers using Xen, I have no doubt that there's already one out there. reply zvmaz 18 hours agorootparentThis is true. But the code base of Xen is significantly smaller than that of a full operating system running bare metal, so the likeliness of a zero-day comprising Qubes is less likely (but possible). reply abtinf 16 hours agorootparentprevIf there is a zero day in Xen, your attackers are probably also going to be having a very, very bad day. reply fsflover 1 hour agorootparentprev> Unless there's a zero day in Xen Most of the time, zero days in Xen do not affect Qubes: https://www.qubes-os.org/security/xsa/#statistics reply DrWhax 19 hours agoprevQubesOS was my main driver for a couple of years, but I have to say that the low battery life compared to only software rendering got pretty annoying after a while. Depending on the hardware, you'll need to possibly disable certain options in the BIOS/UEFI, like for an t490 that I documented: https://groups.google.com/g/qubes-users/c/Z0Kfm53zMxQ/m/IV-A... reply zvmaz 19 hours agoparentWhat to use instead if one cares about privacy and security (what I call personal sovereignty)? It's a non-rhetorical question. reply llm_trw 19 hours agorootparentYou use qubes and eat the loss of battery life. reply barbs 17 hours agorootparentOr maybe Tails? reply nullc 13 hours agorootparentprevThe only serious alternative to Qubes from a security perspective is to use multiple computers. That alternative presumably has better security, but also generally worse usability (particularly if you're going to be mobile! -- two laptops in your bag might be acceptable but comparable isolation would require more than two). reply fsflover 1 hour agorootparent> That alternative presumably has better security Not necessarily: https://www.qubes-os.org/faq/#how-does-qubes-os-compare-to-u... reply udev4096 12 hours agoprevWe wouldn't have to rely on security by hypervisor if linux had proper security measures, sandboxing and access controls OOTB. Qubes is still far from good although it's slowly getting there reply dijit 10 hours agoparentwhat are you talking about? Of course we do? We totally forgotten about mandatory access control systems? AppArmor, SELinux? problem isn’t that they don’t exist; it’s that nobody knows how to use them properly. You can even minimise the kernel attack surface these days with utilities like gVisor. people just understand virtual machines easier. It’s easy to understand the isolation it gives and easier to reduce unnecessary potential attack vectors by having minimal images that don’t contain more than necessary. reply i_love_retros 4 hours agorootparentAm I wrong in thinking that Ubuntu has apparmor configured by default? reply mmh0000 43 minutes agorootparentYes, Ubuntu has AppArmor and Fedora (RHEL, et. al.) have SELinux. The problem is that both systems are quite difficult to use properly. The out-of-the-box configuration is good for a base increase in overall system security against common threats. However, if you want the real isolation benefits that these MAC systems are capable of providing, you'll need a full-time security team with years of training to manage your personal desktop. reply udev4096 9 hours agorootparentprev\"OOTB\" is the term you missed. Obviously, there are kernel features such as seccomp and other LSM but it's not easy to properly configure. Even if you do, it usually comes in your way of getting stuff done. Regarding gvisor, it's solid and I've been intending to use it on k3s reply aborsy 12 hours agoprevIt’s great for compartmentalizing the work, even if security is not important. The UI was surprisingly good when I used it. reply nullc 14 hours agoprevI am a Qubes user for a couple years now, and I wish I'd switched to it years earlier. Basically every criticism you hear is about correct-- principally worse graphics performance and battery life. But the performance issues for me were less bad than I expected, and the seamlessness of its usability was much much higher than I expected. Like copy and paste, moving files between VMs, plugging usb devices into VMs, networking, etc. all pretty much just work. It's pretty impressive if you have any idea of the machinery under the hood needed to make that work. And now I don't feel anywhere near as nervous that whatever vendor program I need to use to configure a device or browser zero day is going to compromise my system. I can read documents from adverse threat actor sources in a netless VM and feel reasonably confident that it can't phone home or steal my data, etc. Obviously it doesn't replace real air gap security, but it's the closest thing you can get to a network of airgapped or firewalled per-application computers which you can fit into a laptop bag. I also like that I can use software that really only works right on fedora/redhat along side software that really only works right on debian. (Or windows, for that matter, but it's not as seamless). I like that I can substantially upgrade my operating system while running--- like I went from fedora40 to 41 just by installing the template, and switching over appvms one at a time. If anything goes wrong it's trivial to roll back, and I can have some app vms that work fine on the new stuff while others are held back if there is a compatibility issue. I like that applications that go nuts and try to use all my memory only screw up the VM that they're in instead of my whole system. It's so nice that when I want to get something working I can spin up a vm and scribble all over it until I get it working. Binary patch my libc, whatever. Then once I've solved it, I can apply the final clean solution to a persistent template. Any random experimentation just goes away when I close the appvm. Need some program just for a single thing? install it in the appvm rather than the template and it naturally is gone later. I can be intentional about changes being either ephemeral or persistent, and never have to worry that the removal of something temporary was incomplete. Of course YMMV, -- if you're someone who is mostly doing text and low performance graphics and can run it on a fast computer then its costs will be small. If you'd find a ten year old computer perfectly usable chances are that qubes on a modern computer won't seem slow or poor battery lifed to you. Particularly if you have other computers for games, 3d gfx, full screen video, etc. If you are someone who has been subjected to targeted hacking attempts the increased peace of mind will be substantial. reply fsflover 1 hour agoparent> Obviously it doesn't replace real air gap security Depending on your use case, Qubes can be even more secure:: https://www.qubes-os.org/faq/#how-does-qubes-os-compare-to-u... reply dang 18 hours agoprevRelated. Others? Converting untrusted PDFs into trusted ones: The Qubes Way (2013) - https://news.ycombinator.com/item?id=42401904 - Dec 2024 (45 comments) Why one would use Qubes OS? (2023) - https://news.ycombinator.com/item?id=42200987 - Nov 2024 (16 comments) Counter argument against QubesOS more secure by being a type 1 hypervisor - https://news.ycombinator.com/item?id=41401318 - Aug 2024 (1 comment) Qubes OS 4.2.2 has been released - https://news.ycombinator.com/item?id=40959109 - July 2024 (5 comments) Working with Qubes OS at the Guardian - https://news.ycombinator.com/item?id=39949882 - April 2024 (74 comments) Qubes OS 4.2.1 has been released - https://news.ycombinator.com/item?id=39833245 - March 2024 (11 comments) A modest update to Qubes OS - https://news.ycombinator.com/item?id=39490264 - Feb 2024 (31 comments) Qubes OS 4.2.0 has been released - https://news.ycombinator.com/item?id=38690597 - Dec 2023 (21 comments) QubesOS – A reasonably secure operating system - https://news.ycombinator.com/item?id=36684946 - July 2023 (135 comments) Qubes OS 4.2-rc1 is available for testing - https://news.ycombinator.com/item?id=36178205 - June 2023 (3 comments) New user guide: How to organize your qubes - https://news.ycombinator.com/item?id=33396604 - Oct 2022 (15 comments) Opsec considerations when using WiFi - https://news.ycombinator.com/item?id=32148920 - July 2022 (2 comments) What Is Qubes OS? - https://news.ycombinator.com/item?id=32036899 - July 2022 (82 comments) Automated OS testing on physical laptops - https://news.ycombinator.com/item?id=31281107 - May 2022 (4 comments) Qubes OS: A reasonably secure operating system - https://news.ycombinator.com/item?id=30776103 - March 2022 (97 comments) Qubes OS 4.1.0 has been released - https://news.ycombinator.com/item?id=30215210 - Feb 2022 (1 comment) Qubes OS or just separate VMs for separating work and private files? - https://news.ycombinator.com/item?id=29537961 - Dec 2021 (6 comments) Qubes OS 4.1-rc1 has been released - https://news.ycombinator.com/item?id=28856957 - Oct 2021 (5 comments) Qubes OS 4.0 has been released - https://news.ycombinator.com/item?id=16699900 - March 2018 (39 comments) Qubes OS: A reasonably secure operating system - https://news.ycombinator.com/item?id=15734416 - Nov 2017 (144 comments) Reasonably Secure Computing in the Decentralized World - https://news.ycombinator.com/item?id=15566563 - Oct 2017 (44 comments) Toward a Reasonably Secure Laptop - https://news.ycombinator.com/item?id=14743238 - July 2017 (100 comments) “Paranoid Mode” Compromise Recovery on Qubes OS - https://news.ycombinator.com/item?id=14218504 - April 2017 (14 comments) Qubes OS Begins Commercialization and Community Funding Efforts - https://news.ycombinator.com/item?id=13069615 - Nov 2016 (24 comments) Qubes OS 3.2 has been released - https://news.ycombinator.com/item?id=12604417 - Sept 2016 (30 comments) Security challenges for the Qubes build process - https://news.ycombinator.com/item?id=11801093 - May 2016 (17 comments) Qubes OS 3.1 has been released - https://news.ycombinator.com/item?id=11260857 - March 2016 (44 comments) Converting untrusted PDFs into trusted ones: The Qubes Way (2013) - https://news.ycombinator.com/item?id=10538888 - Nov 2015 (5 comments) Intel x86 considered harmful – survey of attacks against x86 over last 10 years - https://news.ycombinator.com/item?id=10458318 - Oct 2015 (169 comments) Qubes – Secure Desktop OS Using Security by Compartmentalization - https://news.ycombinator.com/item?id=8428453 - Oct 2014 (49 comments) Introducing Qubes 1.0 (\"a stable and reasonably secure desktop OS\") - https://news.ycombinator.com/item?id=4472403 - Sept 2012 (59 comments) Qubes: an open source OS with strong security for desktop computing - https://news.ycombinator.com/item?id=2645170 - June 2011 (16 comments) Review: Qubes OS Beta 1 — a new and refreshing approach to system security - https://news.ycombinator.com/item?id=2504274 - May 2011 (1 comment) The Linux Security Circus: On GUI isolation - https://news.ycombinator.com/item?id=2477667 - April 2011 (47 comments) Qubes Beta 1 has been released (strong desktop security OS) - https://news.ycombinator.com/item?id=2439096 - April 2011 (3 comments) Qubes Architecture - actual security-oriented OS - https://news.ycombinator.com/item?id=1796384 - Oct 2010 (1 comment) Open source Qubes OS is ultra secure - https://news.ycombinator.com/item?id=1249857 - April 2010 (7 comments) Introducing Qubes OS - https://news.ycombinator.com/item?id=1246990 - April 2010 (20 comments) reply behnamoh 19 hours agoprevA simple screenshot of the OS environment would have been nice. But generally, I don't think people adopt operating systems just by seeing new recommendations on Hacker News or different forums. Most people have settled on macOS and then Linux and then Windows. and within the Linux ecosystem most people just use Ubuntu or Fedora and that's it. I don't see anyone using these other esoteric operating systems as a daily driver. For servers it's a different story. We have OpenBSD and FreeBSD. and of course Linux. But that's about it. Even supercomputers run Linux. creating an operating system in 2025, aside from intellectual curiosity, isn't really pragmatic. reply wongarsu 19 hours agoparentCubesOS is the choice if you need a very high level of security, are willing to accept some workflow changes to achieve that, but still want a modern graphical operating system that runs all your normal software in a unified workspace. Nothing else provides a similar mix of security and usability. The alternatives are either much less secure or have much worse usability. Of course only few people have these kinds of requirements. I'd recommend Qubes OS if you are an investigative journalist or working in offensive or defensive IT security. Everyone else can safely ignore it. Still, even if it's not made for most of us it makes interesting design decisions that are very much of interest to this forum. And a lot of the people it is made for are here too reply liamwire 18 hours agoparentprevQubes is far from esoteric in spaces where security is paramount. Your lack of familiarity with it doesn’t mean it’s obscure. It’s more a tool for a specific subset of people and purposes, rather than an OS meant for wide adoption. reply fsflover 1 hour agorootparentRelated thread: https://forum.qubes-os.org/t/deployments-of-qubes-by-entitie... reply cspeterson 18 hours agoparentprevFWIW, seeing Qubes on HN some weeks ago got me to try it out, and it's been my daily driver since. Good timing since I had holiday vacation to spend time with it before going back to $JOB on the machine. PS Qubes is Linux. The base domain hypervisor is Fedora-based, and while it is possible to run Windows in a \"Qube,\" the docs and tooling clearly concentrate upon Linux (Fedora and Debian) as the primary use case. reply johannes1234321 18 hours agoparentprev> A simple screenshot of the OS environment would have been nice. https://www.qubes-os.org/screenshots/ reply mtreis86 18 hours agoparentprevQubes is a Linux OS. It's like if you took Fedora and installed xen on it and booted up some VMs and the windows for them opened within the base OS instead of in individual system windows. Plus some cool magic with the file system to reduce redundancy and how many times you have to update things. reply accassar 17 hours agorootparentAnd cooler magic to colour code your window borders. Effectively gives you a VM running Firefox and you only see the Firefox window. This will let you run your email in one window, and click on a link to open it in another VM. reply chefandy 18 hours agoparentprevI don’t think qubes is targeted at mainstream users— even mainstream developers— and I don’t think something has to be targeted at the mainstream to be interesting, especially here. There are probably a lot of people here that won’t ever use it that will still find the idea and ethos interesting. reply accassar 17 hours agorootparentQubes is great as a development platform. The simple integration of VM's into a desktop is surprisingly useful and seamless for day to day development, testing and work. I've used it for a number of years. reply schoen 19 hours agoparentprevQubes was created in 2012! It mostly runs Linux applications. reply accassar 17 hours agorootparentRuns Windows pretty well! I find that very convenient. https://www.qubes-os.org/doc/templates/windows/ reply nullc 13 hours agoparentprevIt looks basically identical to fedora with XFCE, plus the window borders being different colors to indicate different VMs and some toolbar icons for VM management. reply fsflover 1 hour agorootparent> It looks basically identical to fedora with XFCE You can install KDE, too: https://forum.qubes-os.org/t/kde-changing-the-way-you-use-qu... reply gjsman-1000 19 hours agoprevI would say, a little more hesitantly, that it deeply depends on what you are doing. When interacting remotely with untrusted services, apps, or documents, Qubes cannot be beaten. However, if I was afraid of my laptop getting attacked with an evil maid attack, I’m sticking with my Mac, Secure Boot, and FileVault; so that my Lock Screen is less likely to be patched against me. If I’m afraid of persistent malware, I want a platform that isn’t necessarily game over if the malware gets sudo privileges once. If I’m afraid of PIN guessing attempts to break in by brute force, I want something like a modern iPhone where the guessing limit is hardware enforced, not a Linux phone where it’s software enforced. Same for if I were in a country with a hostile government. Nothing screams “I’m hiding something and I’m malicious” like using GrapheneOS or Qubes in Russia or China. They might not see your work, but the uncommon choices by itself makes you suspect. An iPhone and Mac over there suggests wealth, and would possibly socially increase your benefit of the doubt due to white collar associations; GrapheneOS and Qubes would shred all benefit of doubt you may have enjoyed. I sometimes think of the Tor incident at a US College. I’m not encouraging this behavior, but a college student sent bomb threats to his university. He was identified, arrested, and convicted because he was the only one using Tor on the university network. A perfect example of how the “more secure” thing used without strategy can shoot yourself in the foot. The point is: If you are reporting on military activity in the Donetsk region, don’t be the only person in the area using Qubes and Tor. Don’t be the only person in the area with a phone pinging GrapheneOS update servers, or a laptop pinging Qubes package repositories. Heck, don’t be the only guy with a phone on the cell network identifying as Android that inexplicably never talks to Google. reply accassar 19 hours agoparenthttps://www.qubes-os.org/doc/anti-evil-maid/ reply sudohackthenews 19 hours agorootparentAEM is a little sketchy though, you need to trust a flash drive to hold it, and make sure that drive doesn’t get overridden by a malicious attacker. Your link goes more into depth about the disadvantages reply transpute 19 hours agorootparentIn some threat models, it's more feasible to protect a portable flash/SSD drive than an entire laptop. In other threat models, laptops/tablets/phones could be physically secured in a safe, or kept under direct physical supervision. reply mrtesthah 18 hours agorootparentprevHow about Heads? https://osresearch.net/ https://forum.qubes-os.org/t/what-makes-heads-so-great-when-... reply mjg59 18 hours agoparentprevIt's probably worth mentioning that secure boot is trivially circumventable given physical access on any Intel Mac (including T2 Macs), so you want at least an M1 to feel safer here reply Fnoord 18 hours agoparentprev> When interacting remotely with untrusted services, apps, or documents, Qubes cannot be beaten. Sums up WWW. But I believe you could use a VM or container and use such. For example, with Whonix (which also works in Qubes!) What I'd like is use such in macOS but alas Jobs & Cook ask premium price for RAM on Macs. With regards to Donetsk example (I like the example). There is a good reason being hidden in plain sight is blending in with masses. It is difficult to get such OPSEC right, and you need to consider different techniques for if one gets burned. reply woctordho 17 hours agoparentprevIn China we have ways to obfuscate those unusual traffic into usual ones like WeChat video calling reply andy_ppp 18 hours agoparentprevHow stupid, if you’re going to send bomb threats do it from someone else’s computer… reply Dalewyn 18 hours agoparentprevThere's an age old saying in Japan that if you want to hide a tree you should do so in a forest. reply patrakov 16 hours agoprev [–] It does not matter in the real world whether the vendor declares it secure. Did it help anyone pass any kind of security audit? In other words, do auditors recognize it as a valid environment for working with potentially malicious documents, or only as a toy? reply adastra22 10 hours agoparent [–] Three points: (1) Qubes is open-source. (2) Qubes is written and maintained by security professionals. (3) Most (all?) security audits are worse than useless. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Qubes OS is a highly secure operating system endorsed by experts, including Edward Snowden, for its strong isolation capabilities, making it suitable for sensitive tasks. - It supports running multiple operating systems and integrates with Whonix to enhance user privacy, appealing to journalists and organizations like the Freedom of the Press Foundation. - The Qubes community is welcoming to newcomers, providing support, resources, and opportunities to contribute or donate."
    ],
    "commentSummary": [
      "Qubes OS is a security-focused operating system that uses virtualization to compartmentalize tasks, enhancing security by isolating applications and tasks. - Users have reported issues with graphics performance and battery life due to its reliance on software rendering, impacting video playback and overall system performance. - While praised for its security features, Qubes OS may not be suitable for users with high graphics or battery demands, though it is recommended for those needing high security, such as journalists or IT security professionals."
    ],
    "points": 162,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1736720910
  },
  {
    "id": 42676529,
    "title": "The Canva outage: another tale of saturation and resilience",
    "originLink": "https://surfingcomplexity.blog/2024/12/21/the-canva-outage-another-tale-of-saturation-and-resilience/",
    "originBody": "The Canva outage: another tale of saturation and resilience Lorin Hochstein incidents, resilience, Uncategorized December 21, 2024December 21, 2024 13 Minutes Today’s public incident writeup comes courtesy of Brendan Humphries, the CTO of Canva. Like so many other incidents that came before, this is another tale of saturation, where the failure mode involves overload. There’s a lot of great detail in Humpries’s write-up, and I recommend you read it directly in addition to this post. What happened at Canva Trigger: deploying a new version of a page The trigger for this incident was Canva deploying a new version of their editor page. It’s notable that there was nothing wrong with this new version. The incident wasn’t triggered by a bug in the code in the new version, or even by some unexpected emergent behavior in the code of this version. No, while the incident was triggered by a deploy, the changes from the previous version are immaterial to this outage. Rather, it was the system behavior that emerged from clients downloading the new version that led to the outage. Specifically, it was clients downloading the new javascript files from the CDN that set the ball in motion. A stale traffic rule Canva uses Cloudflare as their CDN. Being a CDN, Cloudflare has datacenters all over the world., which are interconnected by a private backbone. Now, I’m not a networking person, but my basic understanding of private backbones is that CDNs lease fibre-optic lines from telecom companies and use these leased lines to ensure that they have dedicated network connectivity and bandwidth between their sites. Unfortunately for Canva, there was a previously unknown issue on Cloudflare’s side: Cloudflare Wasn’t using their dedicated fibre-optic lines to route traffic between their Northern Virginia and Singapore datacenters. That traffic was instead, unintentionally, going over the public internet. [A] stale rule in Cloudflare’s traffic management system [that] was sending user IPv6 traffic over public transit between Ashburn and Singapore instead of its default route over the private backbone. Traffic between Northern Virginia (IAD) and Singapore (SIN) was incorrectly routed over the public network The routes that this traffic took suffered from considerable packet loss. For Canva users in Asia, this meant that they experienced massive increases in latency when their web browsers attempted to fetch the javascript static assets from the CDN. A stale rule like this is the kind of issue that the safety researcher James Reason calls a latent pathogen. It’s a problem that remains unnoticed until it emerges as a contributor to an incident. High latency synchronizes the callers Normally, an increase in errors would cause our canary system to abort a deployment. However, in this case, no errors were recorded because requests didn’t complete. As a result, over 270,000+ user requests for the JavaScript file waited on the same cache stream. This created a backlog of requests from users in Southeast Asia. The first client attempts to fetch the new Javascript files from the CDN, but the files aren’t there yet, the CDN must fetch the files from the origin. Because of the added latency, this takes a long time. During this time, other clients connect, and attempt to fetch the javascript from the CDN. But the CDN has not yet been populated with the files from the origin, that transfer is still in progress. As Cloudflare notes in this blog post, when all subsequent clients request access to a file that is in the process of being populated in the cache, they must wait until the file has been cached before they can download the file. Except that Cloudflare has implemented functionality called Concurrent Streaming Acceleration which permits multiple clients to simultaneously download a file that is still in the process of being downloaded from the origin server. The resulting behavior is that the CDN now behaves effectively as a barrier, with all of the clients slowly but simultaneously downloading the assets. With a traditional barrier, the processes who are waiting can proceed once all processes have entered in the barrier. This isn’t quite the same, as the clients who are waiting can all proceed once the CDN completes downloading the asset from the origin. The transfer completes, the herd thunders At 9:07 AM UTC, the asset fetch completed, and all 270,000+ pending requests were completed simultaneously. 20 minutes after Canva deployed the new Javascript assets to the origin server, the clients completed fetching them. The next action the clients take is to call Canva’s API service. With the JavaScript file now accessible, client devices resumed loading the editor, including the previously blocked object panel. The object panel loaded simultaneously across all waiting devices, resulting in a thundering herd of 1.5 million requests per second to the API Gateway — 3x the typical peak load. There’s one more issue that made this situation even worse: a known performance issue in the API gateway that was slated to be fixed. A problematic call pattern to a library reduces service throughput The API Gateways use an event loop model, where code running on event loop threads must not perform any blocking operations. Two common threading models for request-response services are thread-per-request and async. For services that are I/O-bound (i.e., most of the time servicing each request is spent waiting for I/O operations to complete, typically networking operations), the async model has the potential to achieve better throughput. That’s because the concurrency of the thread-per-request model is limited by the number of operating-system threads. The async model services multiple requests per thread, and so it doesn’t suffer from the thread bottleneck. Canva’s API gateway implements the async model using the popular Netty library. One of the drawbacks of the async model is the risk associated with the active thread getting blocked, because this can result in a significant performance penalty. The async model multiplexes multiple requests across an individual thread, and none of those requests can make progress when that thread is blocked. Programmers writing code in a service that uses the async model need to take care to minimize the number of blocking calls. Prior to this incident, we’d made changes to our telemetry library code, inadvertently introducing a performance regression. The change caused certain metrics to be re-registered each time a new value was recorded. This re-registration occurred under a lock within a third-party library. In Canva’s case, the API gateway logic was making calls to a third-party telemetry library. They were calling the library in such a way that it took a lock, which is a blocking call. This reduced the effective throughput that the API gateway could handle. Calls to the library led to excessive thread locking Although the issue had already been identified and a fix had entered our release process the day of the incident, we’d underestimated the impact of the bug and didn’t expedite deploying the fix. This meant it wasn’t deployed before the incident occurred. Ironically, they were aware of this problematic call pattern, and they were planning on deploying a fix the day of the incident(!). As an aside, it’s worth noting the role of telemetry logic behavior in the recent OpenAI incident, and in the locking behavior of tracing library in a complex performance issue that Netflix experienced. Observability giveth reliability, and observability taketh reliability away. Canva is now in a situation where the API gateway is receiving much more traffic than it was provisioned to handle, is also suffering from a performance regression that reduces its ability to handle traffic even more. Now let’s look at how the system behaved under these conditions. The load balancer turns into an overload balancer Because the API Gateway tasks were failing to handle the requests in a timely manner, the load balancers started opening new connections to the already overloaded tasks, further increasing memory pressure. A load balancer sits in front of a service and distributes the incoming requests across the units of compute. Canva runs atop ECS, so the individual units are called tasks, and the group is called a cluster (you can think of these as being equivalent to pods and replicasets in Kubernetes-land). The load balancer will only send requests to a task that is healthy. If a task is unhealthy, then it stops being considered as a candidate target destination for the load balancer. This yields good results if the overall cluster is provisioned to handle the load: the traffic gets redirected away from the unhealthy tasks and onto the healthy ones. Load balancer only sells traffic to the healthy tasks But now consider the scenario where all of the tasks are operating close to capacity. As tasks go unhealthy, the load balancer will redistribute the load to the remaining “healthy” tasks, which increases the likelihood those tasks gets pushed into an unhealthy state. Redirecting traffic to the almost-overloaded healthy nodes will push them over This is a classic example of a positive feedback loop: the more tasks go unhealthy, the more traffic the healthy nodes received, the more likely those tasks will go unhealthy as well. Autoscaling can’t keep pace So, now the system is saturated, and the load balancer is effectively making things worse. Instead of shedding load, it’s concentrating load on the tasks that aren’t overloaded yet. Now, this is the cloud, and the cloud is elastic, and we have a wonderful automation system called the autoscaler that can help us in situations of overload by automating provisioning new capacity. Only, there’s a problem here, and that’s that the autoscaler simply can’t scale up fast enough. And the reason it can’t scale up fast enough is because of another automation system that’s intended to help in times of overload: Linux’s OOM killer. The growth of off-heap memory caused the Linux Out Of Memory Killer to terminate all of the running containers in the first 2 minutes, causing a cascading failure across all API Gateway tasks. This outpaced our autoscaling capability, ultimately leading to all requests to canva.com failing. Operating systems need access to free memory in order to function properly. When all of the memory is consumed by running processes, the operating system runs into trouble. To guard against this, Linux has a feature called the OOM killer which will automatically terminate a process when the operating system is running too low on memory. This frees up memory, enabling the OS to keep functioning. So, you have the autoscaler which is adding new tasks, and the OOM killer which is quickly destroying existing tasks that have become overloaded. It’s notable that Humphries uses the term outpaced. This sort of scenario is a common failure mode in complex system failures, where the system gets into a state where it can’t keep up. This phenomenon is called decompensation. Here’s resilience engineering pioneer David Woods describing decompensation on John Willis’s Profound Podcast: And lag is really saturation in time. That’s what we call decompensation, right? I can’t keep pace, right? Events are moving forward faster. Trouble is building and compounding faster than I, than the team, than the response system can decide on and deploy actions to affect. So I can’t keep pace. – David Woods Adapting the system to bring it back up At this point, the API gateway cluster is completely overwhelmed. From the timeline: 9:07 AM UTC – Network issue resolved, but the backlog of queued requests result in a spike of 1.5 million requests per second to the API gateway. 9:08 AM UTC – API Gateway tasks begin failing due to memory exhaustion, leading to a full collapse. When your system is suffering from overload, there are basically two strategies: increase the capacity reduce the load Wisely, the Canva engineers pursued both strategies in parallel. Max capacity, but it still isn’t enough Montgomery Scott, my nominee for patron saint of resilience engineering We attempted to work around this issue by significantly increasing the desired task count manually. Unfortunately, it didn’t mitigate the issue of tasks being quickly terminated. The engineers tried to increase capacity manually, but even with the manual scaling, the load was too much: the OOM killer was taking the tasks down too quickly for the system to get back to a healthy state. Load shedding, human operator edition The engineers had to improvise a load shedding solution in the moment. The approach they took was to block traffic the CDN layer, using Cloudflare. At 9:29 AM UTC, we added a temporary Cloudflare firewall rule to block all traffic at the CDN. This prevented any traffic reaching the API Gateway, allowing new tasks to start up without being overwhelmed with incoming requests. We later redirected canva.com to our status page to make it clear to users that we were experiencing an incident. It’s worth noting here that while Cloudflare contributed to this incident with the stale rule, the fact that they could dynamically configure Cloudflare firewall rules meant that Cloudflare also contributed to the mitigation of this incident. Ramping the traffic back up Here they turned off all of their traffic to give their system a chance to go back to healthy. But a healthy system under zero load behaves differently from a healthy system under typical load. If you just go back from zero to typical, there’s a risk that you push the system back into an unhealthy state. (One common problem is that autoscaling will have scaled down multiple services due when there’s no load). Once the number of healthy API Gateway tasks stabilized to a level we were comfortable with, we incrementally restored traffic to canva.com. Starting with Australian users under strict rate limits, we gradually increased the traffic flow to ensure stability before scaling further. The Canva engineers had the good judgment to ramp up the traffic incrementally rather than turn it back on all at once. They started restoring at 9:45 AM UTC, and were back to taking full traffic at 10:04 AM. Some general observations All functional requirements met I always like to call out situations where, from a functional point of view, everything was actually working fine. In this case, even though there was a stale rule in the Cloudflare traffic management system, and there was a performance regression in the API gateway, everything was working correctly from a functional perspective: packets were still being routed between Singapore and Northern Virginia, and the API gateway was still returning the proper responses for individual requests before it got overloaded. Rather, these two issues were both performance problems. Performance problems are much harder to spot, and the worst are the ones that you don’t notice until you’re under heavy load. The irony is that, as an organization gets better at catching functional bugs before they hit production, more and more of the production incidents they face will be related to these more difficult-to-detect-early performance issues. Automated systems made the problem worse There were a number of automated systems in play whose behavior made this incident more difficult to deal with. The Concurrent Streaming Acceleration functionality synchronized the requests from the clients. The OOM killer reduced the time it took for a task to be seen as unhealthy by the load balancer, and the load balancer in turn increased the rate at which tasks went unhealthy. None of these systems were designed to handle this sort of situation, so they could not automatically change their behavior. The human operators changed the way the system behaved It was up to the incident responders to adapt the behavior of the system, to change the way it functioned in order to get it back to a healthy state. They were able to leverage an existing resource, Cloudflare’s firewall functionality, to accomplish this. Based on the description of the action items, I suspect they had never used Cloudflare’s firewall to do this type of load shedding before. But it worked! They successfully adapted the system behavior. We’re building a detailed internal runbook to make sure we can granularly reroute, block, and then progressively scale up traffic. We’ll use this runbook to quickly mitigate any similar incidents in the future. This is a classic example of resilience, of acting to reconfigure the behavior of your system when it enters a state that it wasn’t originally designed to handle. As I’ve written about previously, Woods talks about the idea of a competence envelope. The competence envelope is sort of a conceptual space of the types of inputs that your system can handle. Incidents occur when your system is pushed to operate outside of its competence envelope, such as when it gets more load than it is provisioned to handle: The competence envelope is a good way to think about the difference between robustness and resilience. You can think of robustness as describing the competence envelope itself: a more robust system may have a larger competence envelope, it is designed to handle a broader range of problems. However, every system has a finite competence envelope. The difference between a resilient and a brittle system is how that system behaves when it is pushed just outside of its competence envelope. Incidents happen when the system is pushed outside of its competence envelope A resilient system can change the way it behaves when pushed outside of the competence envelope due to an incident in order to extend the competence envelope so that it can handle the incident. That’s why we say it has adaptive capacity. On the other hand, a brittle system is one that cannot adapt effectively when it exceeds its competence envelope. A system can be very robust, but also brittle: it may be able to handle a very wide range of problems, but when it faces a scenario it wasn’t designed to handle, it can fall over hard. The sort of adaptation that resilience demands requires human operators: our automation simply doesn’t have a sophisticated enough model of the world to be able to handle situations like the one that Canva found itself in. In general, action items after an incident focus on expanding the competence envelope: making changes to the system to handle the scenario that just happened. Improving adaptive capacity involves different kind of work than improving system robustness. We need to build in the ability to reconfigure our systems in advance, without knowing exactly what sorts of changes we’ll need to make. The Canva engineers had some powerful operational knobs at their disposal through the Cloudflare firewall configuration. This allowed them to make changes. The more powerful and generic these sorts of dynamic configuration features are, the more room for maneuver we have. Of course, dynamic configuration is also dangerous, and is itself a contributor to incidents. Too often we focus solely on the dangers of such functionality in creating incidents, without seeing its ability to help us reconfigure the system to mitigate incidents. Finally, these sorts of operator interfaces are of no use if the responders aren’t familiar with them. Ultimately, the more your responders know about the system, the better position they’ll be in to implement these adaptations. Changing an unhealthy system is dangerous: no matter how bad things are, you can always accidentally make things worse. The more knowledge about the system you can bring to bear during an incident, the better position you’ll be in to adaptive your system to extend that competence envelope. Share this: Twitter Facebook Like Loading... Published by Lorin Hochstein View all posts by Lorin Hochstein Published December 21, 2024December 21, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=42676529",
    "commentBody": "The Canva outage: another tale of saturation and resilience (surfingcomplexity.blog)157 points by mooreds 22 hours agohidepastfavorite41 comments shaggie76 19 hours agoWe had a similar CDN problem with releasing major Warframe updates: our CDN partner would inadvertently DDoS our origin servers when we launched an update because thousands of cold edges would call home simultaneously when all players players relogged at the same time. One CDN vendor didn't even offer a tiered distribution system so every edge called home at the same time, another vendor did have a tiered distribution system designed to avoid this problem but it was overwhelmed by the absurd number of files we'd serve multiplied by the large user count and so we'd still end up with too much traffic hitting the origin. The interesting thing was that no vendor we evaluated offered a robust preheating solution if they offered one at all. One vendor even went so far as to say that they wouldn't allow it because it would let customers unfairly dominate the shared storage cache at the edge (which sort of felt like airlines overbooking seats on a flight to me). These days we run an army of VMs that fetch all assets from every point of presence we can cover right before launching an update. Another thing we've had to deal with mentioned in the article is overloading back-end nodes; our solution is somewhat ham-fisted but works quite well for us: we cap the connection counts to the back end and return 503s when we saturate. The trick, however, is getting your load-balancer to leave the client connection open when this happens -- by default multiple LBs we've used would slam the connection closed so that when you're serving up 50K 503s a second the firewall would buckle under the runaway connection pool lingering in TIME_WAIT. Good times. reply snackbroken 14 hours agoparentSomething I've been wondering for a while is if BitTorrent or other P2P protocols are ever a consideration for pushing game updates? Naively, it seems like an ideal fit since a large swarm of leechers quickly turns into a large swarm of (partial) seeders mostly chattering amongst themselves. I recall Facebook and Twitter used to internally torrent their updates in the 2010s and BT scales just fine to thousands of peers and tens of GB files at least, but I think I've only ever played one game whose updater was a torrent client so I'm guessing it's a nonstarter for one reason or another. Are game publishers just allergic to it due to the piracy association? Are end-user upload speeds too slow to meaningfully make a difference? Are swarms of ~100k just too large to manage? Edit: Silly me for posting while sleep deprived. It's not the update itself that you're saying is causing thundering herd issues, but the log-ins all being synced up afterwards much like in TFA, duh. My curiosity wrt the apparent lack of P2P game updaters still stands though. reply donavanm 9 hours agorootparentSee my related comment. It was a popular idea around 2005-10. As mentioned Red Swoosh was primarily sold as a “p2p” CDN, was bought up by akamai for a billionty dollars, and promptly disappeared. AWS S3 also implemented a torrent interface early on. AFAIK they keep it alive in name at least, but its effectively deadcode with $0 revenue as far back as Ive ever known. A handful of private companies built p2p themselves, but eventually moved off. As an example p2p is where spotify started in this time range and then moved to a CDN (us) for better consistency and not having to deal with it themselves. The primary business problem is one of visibility and control. The customer UX would be entirely out if your control, and exceedingly variable, based on factors you (the provider) cant even see. At the same time CDNs were pushing down to cents per GB delivered by 2010, and ~1¢/GB by 2015. At a penny per GB distribution for higher throughout, better visibility, and control CDN distribution costs started to not matter compared to other costs and priorities. Oh! Porn delivery companies, theyre an interesting content distribution case. AFAIK commercial CDNs are still way too expensive to meet their business model needs. My recollection is that they all built their own in house CDNs, like GPs “run a bunch of VMs” approach, or used a peers. This was accelerated as all of those companies consolidated ala MindGeek in the 2010s. reply dikei 2 hours agorootparentOne reason for Spotify's move away from p2p was it was absolutely a no-go on mobile platform, which was rapidly becoming dominant at the time. reply UltraSane 8 hours agorootparentprevWindows Update has the option to download signed updates from Microsoft and any other computer that has downloaded it. And it says that 38% (247MB) of all windows update bytes have been downloaded form \"PCs on the internet\" and I have uploaded 340MB to \"PCs on the Internet\" reply tupshin 7 hours agorootparentprevaround 2010, we (Zynga at the time) used torrent to distribute the MafiaWars code/assets to all servers in a couple of data centers. Worked without much challenge. reply masklinn 12 hours agorootparentprevBlizzard used to have p2p support, they removed it around 2015. It’s not hard to think of a bunch of problematic cases which become absolute hell to diagnose because they’re client side. reply AndrewDavis 9 hours agorootparentTheir downloaders for classic games still have the options to enable peer to peer. Though it failed to initialise, but I'm not sure if that's because their tracker is down or because it demands upnp. I recently did this with Diablo 2 and it's expansion. reply gsck 3 hours agoparentprevI have always found it remarkable with how well Warframe handles updates, I've seen other games do the \"Update live now everyone restart!\" and then no one can get in due to thundering herd. But you close Warframe after the red text and the game updates pretty fast, even if its a massive update like 1999 was, and then you are back in the game (Unless you say yes to Optimising download cache, that takes an absolute age for some reason plsfix), definitely a pretty amazing engineering achievement. reply donavanm 15 hours agoparentprevAs someone who worked on a major CDN I have some perspective. > thousands of cold edges would call home simultaneously when all players players relogged at the same time. Our more mature customers (esp console gaming) would enable early background downloads, spaced out over a few hours, the day/hours before 'launch'. Otherwise adhoc/jit is definitely best effort, though we did a few things to help: Conceptually each CDN POP is ~3 logical layers 1) a client-request-terminating 'hot' cache distributed across all nodes in the POP 2) a shared POP cache segmented by content/resource ID 3) a shared origin-request-facing egress layer. Every layer would attempt to perform request coalescing, with 90% efficacy or more. eg, 10 client requests to the same layer 1 node _should_ only generate a single request to the segmented layer 2 cache. The same layer 2 node would we serving multiple requests to the layer 1 nodes, while making a single request back towards the origin. Some exceptional behavior occurred, or was driven by, 'load' and trying to account for 1) head of line blocking 2) tail latencies etc from inequal load distribution. Based on load for an object, or a nodes current total load, we used forward signaling to distribute requests to peers. That is a 'busy' layer 2 node would signal to the layer 1 nodes to use additional/alternate peers. This increased the number of copies of a popular object in the segmented cache, increasing the total throughput available to populate the 'hot' L1 cache nodes _or_ to serve objects that were not consistently popular enough to stay in that distributed L1 cache. And relevant to your example we had similar problems when going back to the origin; In the first case we want to minimize the number of new TCP/TLS connections, which have a large RTT setup penalty, by reusing active & idle 'layer 3' connections to the origin. This, however, introduces hotspots and head of line blocking for those active origin connections. Which, again, based on 'load' would be forward signaled so that additional layer 3 nodes/processes would be used to fetch _additional_ origin content. Normally this all means 1 origin request can serve a few orders of magnitude more concurrent client requests. For very large content, or exceedingly large client numbers, you'd see the CDN 'scale out' on concurrency in an effort to minimize blocking and maximize throughput in the system. > One CDN vendor didn't even offer a tiered distribution system so every edge called home at the same time, another vendor did have a tiered distribution system designed to avoid this problem See above on request coalescing. In the vast vast majority of cases it was effective in reducing the problem by a few orders of magnitude; AFAIK every CDN does/did that. _In addition_ we did have an distributed hierarchal system for caching between edge POPs and origins _but_ it was non-public/invite/managed by us for a long time. The reason being that the _vast_ majority of customers incurred additional latency (& cost to us) without meaningful benefit from this intermediate cache layer. > The interesting thing was that no vendor we evaluated offered a robust preheating solution if they offered one at all. This is interesting to me. AFAIK Akamai Netstorage was sold to solve the origin distribution angle, _and_ drove something like 50% of the revenue from large object distribution customers. For us the customer use case of 'prefetch' was perennial 'top 5' but never one that would drive revenue, and conflicted with other system tenets. > One vendor even went so far as to say that they wouldn't allow it because it would let customers unfairly dominate the shared storage cache at the edge That could have been us. And yes a huge problem is that you're fundamentally asking for control over a shared resource so that you can bias performance to _your content_ at the expense of _all other customers_. Even without intentional 'prefetch' control we had still had some customers with pseudo-degenerate access patterns that might consume 25-50% of the shared cache space in a POP. We did build shared quotas and such but (when I was there) we couldn't see a way to align the pricing & incentives to confidently expose that to customers. It also felt very very bad to tell a customer 'pay us $$$ to care about your bits' when that was our entire job, and what we were doing to the best extent possible already. > we cap the connection counts to the back end and return 503s when we saturate. Depending on the CDN you may be able to use `max-age` or `s-maxage` to implement psuedo backoff from the CDN. For us at least those 'negative hits' would be cached with a short (seconds by default) TTL to act a dampener in failure scenarios. Ensure that your client can handle/recover from the 503 as well, I'd expect the CDN to return those all the way through in the response. reply donavanm 15 hours agorootparent> Otherwise adhoc/jit is definitely best effort, though we did a few things to help I should also give a sense of scale here. Hundreds of GB/s to multi TB/s of throughput for a single customer was pretty normal a decade ago. CDNs, classically, are also biased towards latency & throughput. Once you have millions of client requests per second and pushing that kind of volume its kind of expected/implied that the origin is capable of meeting the demand necessary to maximize that throughput. While cost efficiency maximizing CDNs _were_ a thing they kind of died out with Red Swoosh AFAIK. We repeatedly investigated 'follow the moon' use cases to maximize the diurnal cycle. Outside of a handful of game companies there wasnt any real interest, and the price/revenue wasnt worth investing compared to other priorities. The market wanted better performance, not minimal costs, in the 2000-10s. reply robertlagrant 6 hours agoparentprevI remember I liked the Fastly API because they seemed to offer preheating, but this was a long time ago, and perhaps not sufficient for your needs. reply bolognafairy 18 hours agoparentprevReally one of those “has anyone that built this tried using it for its intended purpose?” things. Not having a carefully considered cache warning solution* is like…if someone built a CDN based on a description someone gave them, instead of actually understanding the problem a CDN sets out to solve. * EDIT: actually, any solution that at least attempts to mitigate a thundering herd. I am at least somewhat empathetic to the “indiscriminately allowing pre-warming destroys the shared cache” viewpoint. But there are still plenty of things that can be done! reply bombcar 16 hours agorootparentThe easiest solution to the pre-warming problem is charge quite a bit for it. Then only those who really need it will pay (or you’ll collect more money to build out the cache). reply Animats 18 hours agoprevThis problem is similar to what electric utilities call \"load takeup\". After a power outage, when power is turned back on, there are many loads that draw more power at startup. The shortest term effects are power supplies recharging their capacitors and incandescent bulbs warming up. That's over within a second. Then it's the motors, which have 2x-3x their running load when starting as they bring their rotating mass up to speed. That extra load lasts for tens of seconds. If power has been off for more than a few minutes, everything in heating and cooling which normally cycles on and off will want to start. That high load lasts for minutes. Bringing up a power grid is thus done by sections, not all at once. reply EvanAnderson 13 hours agoparentIf you're subject to peak load billing it's also a good idea to bring your loads online in sections, too. My family owns a small grocery store. I was taught the process for \"booting-up\" the store after a power outage. It basically amounted to a one-by-one startup of the refrigeration compressors, waiting between each for them to come up to operating pressure and stabilize their current demand. reply ElusiveA 11 hours agoparentprevAn insightful share. You might be interested to know that startup current is called 'inrush current'. For a Direct On Line (DOL) start, (no soft starters or variable speed drives) electrical engineers usually model it as 6x normal full load current. Other electrical devices such as transformers and long overhead power lines also exhibit inrush when they are energised. reply _heimdall 18 hours agoparentprevI live in a somewhat rural area and we got bit hard by this last winter. Our road used to have a handful of houses on it but now has around 85 (a mix of smaller lots around an acre and larger farming parcels). Power infrastructure to our street hasn't been updated recently and it just barely keeps up. We had a few days that didn't get above freezing (very unusual here). Power was out for about 6 hours after a limb fell on a line. The power company was actually pretty quick to fix it, but the power went out 3 more times in pretty quick succession. Apparently a breaker kept blowing as every house regained power and all the various compressors surged on. The solution at the time was for them to jam in a larger breaker. I hope they came back pretty quickly to undo that \"fix\" but we still haven't had any infrastructure updates to increase capacity. reply alvah 17 hours agorootparent\"The solution at the time was for them to jam in a larger breaker\" I've seen some cowboy sh!t in my time but jeez, that's rough. reply cr125rider 16 hours agorootparentThat’s “it can’t keep tripping if I jam in a penny instead” level of engineering from the utility! Wow! reply cudgy 13 hours agorootparentprevGood thing none of your houses burnt down. reply emmanueloga_ 17 hours agoprevThe whole incident report is interesting, but I feel like the most important part of the solution is buried here [0]: * \"We're adding timeouts to prevent user requests from waiting excessively long to retrieve assets.\" When you get to the size of Canva, you can't forget your AbortController and exponential backoff on your Fetch API calls. -- 0: https://www.canva.dev/blog/engineering/canva-incident-report... reply benatkin 15 hours agoprevI happened to prefer the original article: https://www.canva.dev/blog/engineering/canva-incident-report... reply siscia 3 hours agoprevI see few blind spots from the write up. 1. Traffic for a new version was loaded up too quickly. I usually lobby for releasing updates slowly. This alone would have prevented the issue. 1. Tasks cannot fail under load. Load Shedding should be in place exactly for this reason. You don't take more than you can chew. If more arrives you slowly and politely refuse the request. You need to be both, slow and polite, so that the client will slowly retry and you won't incur in the herding issue. 1. The monitoring issue should have triggered (most likely) an increase of latency. That should have been enough to not complete the deployment and rollback carefully. I am sure engineers in canva had their reason, and that the write up does not account for everything. Just some food for thought for other engineers. reply ThinkBeat 3 hours agoprevThis is about penny pinching. If you have created a system that cannot autoscale fast enough, then the triggers for when it does scale up should be much lower. I also think that enormous amounts of headache can be saved by spinning up beefy instances and including scaling it up before scaling out. A big nice beefy instance gets over 50% of whatever metric is used spin up a new one. Make it an even beefer version. Scaling \"just in time\", persumably to lower costs, is much more of a gamble and a lot more complicated. reply ec109685 15 hours agoprevThe incident report said, “the growth of off-heap memory” was a cause for the OOM. Why would have too much traffic caused that to increase specifically? The overhead of a connection in the kernel isn’t that high. To reduce pressure in the future, they could smear the downloading of new assets over time by background fetching. E.g. when canary release of a new canva release starts they probabilistically could download the asset in the background for the existing version, so when they switch, there’s nothing new to download. Features like collapse forwarding and stale-while-revalidate are powerful features for CDN’s, but there are these non-intuitive failure modes that you have to be aware of. Anything that synchronizes huge numbers of requests is dangerous to stability. reply adamc 2 hours agoprevThe distinction between resilience and robustness strikes me as a useful one. Really great article overall. reply tryauuum 16 hours agoprevfuck canva, I remember visiting it from Georgia and being greeted a non-working page and a banner shaming me for the war in Ukraine I know there's probably some US sanctions list somewhere which the company had to adhere to. But experiencing it in Georgia, where streets are covered with Ukrainian flags and people are very open with their opinion on the war is just surreal reply perching_aix 16 hours agoparentthat indeed sounds remarkably puzzling, so much so that i find it a bit hard to believe reply prmoustache 9 hours agorootparentThey are mentionning the country, not the US state. Supposedly Georgia asked to be part of UE since the Ukraine invasion so it somehow implies at the very least empathy towards Ukraine and not support for the war. Having said that and taking into account that IP Geolocation is a fantasy and not something that really work reliably in practice, I would totally understand that some people living in Georgia would be geolocalized in Russia because their ISP is a russian company or is using IPs associated with Russia. I am regularly geolocalized by some websites more that 3000km away from my home. My ISP headquarters and datacenters are in a different country and I guess some of the IP range they use are geolocalized there. reply perching_aix 7 hours agorootparent> They are mentionning the country, not the US state. Yes, I know :) I don't think IP geolocation is so poor that it'd put Georgian residents into Russia. Could be wrong though, of course. reply prmoustache 7 hours agorootparentThen why is it so poor that it sometimes put me in Romania while I am in Spain and closer to Africa than most other european countries but Portugal? reply diggan 6 hours agorootparent> Then why is it so poor that it sometimes it being a company that estimates the location based on publicly available information like \"This ASN belongs to this corporate entity which is registered in this country/related to this association\" and so on. There is no official hashmap with \"IP => Geographical Location\", they're all guesses and estimates. reply stef25 12 hours agoparentprevMaybe your IP was mistakenly seen as being in Russia ? Obviously should never have happened reply tryauuum 10 hours agorootparent\"obviously?\" I've seen Georgia in US embargo list, although it's hard to comprehend what's actually embargoed https://www.bis.gov/ear/title-15/subtitle-b/chapter-vii/subc... reply deathanatos 12 hours agoprevAs the OG post states, CF uses \"Concurrent Streaming Acceleration\" to batch those \"270,000+\" requests into one to the origin. Now, let's grant that the public Internet is not CF's private backbone … but TFA makes it out to be more akin to a mobile connection in a tunnel than the Internet? Like transferring across the planet isn't going to be amazing … but that fails to explain how a download couldn't complete at all over multiple minutes…? reply donavanm 4 hours agoparentThe term of art is normally “request coalescing” or “collapse forwarding”; I believe the later came from the 90s/00s via squid or ocean. Yes, multiple minutes to complete is very believable. Cloudflare reported 60% packet loss over ~100ms distance. Thats going kill window sizes and goodput. I wouldnt be surprised by this pathological case also exposing problems in their concurrent streaming window access between so many clients as well. reply deathanatos 12 minutes agorootparent> Yes, multiple minutes to complete is very believable. Cloudflare reported 60% packet loss over ~100ms distance. Thats going kill window sizes and goodput. You're begging the question: that 60% packet loss is exactly what I'm questioning. That's not normal for public Internet connectivity, so we need something beyond \"oops, we routed the request over the public Internet\" in order to fully explain the outage. Sure, given 66% packet loss, \"multiple minutes to complete is very believable\" and \"Thats going kill window sizes and goodput\" (sic), I agree with those points. But it's the premise — that packet loss on the external link was also absurd — that needs more explaining? (… this is where I wish Canva would have linked that quote to its source. AFAICT, Cloudflare never published that, so IDK if that's a private correspondence, or what.) reply cpatil 14 hours agoprevPerhaps a canary deployment per region might help in such situations? Prime the CDN assets with a smaller set of users. reply faramarz 14 hours agoprevSo what is the suggestion at the end of the post? Did I understand correctly that a sandboxed-replica simulator with the fundamental training would harden the system design? Cool! Can you run the simulator based on the basic but complete input architectural drawing? I’d be curious to know if LLMs are able to go and abstract it all across the public network and come back with an attention for all possible known scenarios. Frankly, you can even serve the scenarios into financial forecast models to serve and move the right levers for appropriate actions. These blind spots are exploits waiting to be discovered. reply jongjong 10 hours agoprev [–] Many outages can be summarized simply as \"Too many clients attempting to perform an action at the same time.\" This is a common situation after a sudden crash or reboot... After recovery, sometimes clients try to reconnect to the servers so quickly that it crashes the servers again and the cycle repeats... Particularly problematic with WebSockets and other stateful connections; hence we use mechanisms like exponential backoff with randomization to spread out the load over time. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Canva outage was caused by a deployment of a new editor page version, leading to a \"thundering herd\" effect with 1.5 million requests per second overwhelming the API Gateway. - A stale traffic rule routed user traffic over the public internet, causing high latency and a backlog, while a known performance issue in the API Gateway worsened the situation. - Canva engineers manually intervened by increasing capacity and using Cloudflare's firewall to block traffic temporarily, highlighting the importance of human intervention in managing system performance issues during crises."
    ],
    "commentSummary": [
      "The Canva outage underscores challenges with Content Delivery Network (CDN) saturation and resilience, common when many users access servers simultaneously, leading to overloads. - Proposed solutions like tiered distribution and preheating often fall short, while P2P (Peer-to-Peer) protocols like BitTorrent face piracy and control concerns, leading companies like Spotify and Blizzard to abandon them. - Effective load management strategies, such as capping connections and using exponential backoff, are crucial for mitigating these issues, highlighting the importance of robust infrastructure and strategic planning for handling high traffic efficiently."
    ],
    "points": 157,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1736713123
  }
]
