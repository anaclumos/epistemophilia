[
  {
    "id": 41903616,
    "title": "Egypt declared malaria-free after 100-year effort",
    "originLink": "https://www.bbc.com/news/articles/cm2yl8pjgn2o",
    "originBody": "Egypt declared malaria-free after 100-year effort PA Malaria is caused by a complex parasite which is spread by mosquito bites Egypt has been certified malaria-free by the World Health Organization (WHO) - an achievement hailed by the UN public health agency as \"truly historic\". “Malaria is as old as Egyptian civilization itself, but the disease that plagued pharaohs now belongs to its history,\" said WHO chief Tedros Adhanom Ghebreyesus. Egyptian authorities launched their first efforts to stamp out the deadly mosquito-borne infectious disease nearly 100 years. Certification is granted when a country proves that the transmission chain is interrupted for at least the previous three consecutive years. Malaria kills at least 600,000 people every year, nearly all of them in Africa. Urgent action needed as malaria resists key drug First West African nation hits malaria-free status In a statement on Sunday, the WHO praised \"the Egyptian government and people\" for their efforts to \"end a disease that has been present in the country since ancient times\". It said Egypt was the third country to be certified in the WHO's Eastern Mediterranean Region, following the United Arab Emirates and Morocco. Globally, 44 countries and one territory have reached this milestone. But the WHO said the certification was only \"the beginning of a new phase\", urging Egypt to be on the alert to preserve its malaria-free status. To get the WHO certification, a country must demonstrate the capacity to prevent the re-establishment of transmission. The UN public health agency said first efforts to limit human-mosquito contact in Egypt began in the 1920s when it banned rice cultivation and agricultural crops near homes. Malaria is caused by a complex parasite which is spread by mosquito bites. Vaccines are now being used in some places - but monitoring the disease and avoiding mosquito bites are the most effective ways to prevent malaria. World Health Organization (WHO) Egypt Mosquitoes Malaria Africa",
    "commentLink": "https://news.ycombinator.com/item?id=41903616",
    "commentBody": "Egypt declared malaria-free after 100-year effort (bbc.com)312 points by thunderbong 6 hours agohidepastfavorite31 comments lysozyme 57 minutes agoIt’s interesting how Egypt’s efforts to monitor and test for malaria contributed to this accomplishment. It underscores how eradicating many infectious diseases will require a deep understanding not only of the disease itself, but also the cycles of transmission and the complex ecology of different hosts. Malaria’s complex lifecycle [1] seems like it would be easy to “break” with different interventions, but we’ve seen historically malaria has been difficult to eradicate. Why is this? 1. https://en.m.wikipedia.org/wiki/Plasmodium#/media/File%3ALif... reply db48x 2 hours agoprevDramatic reenactment: https://www.youtube.com/watch?v=ljmifo4Klss (smallpox instead of malaria; close enough) reply zahlman 1 hour agoparentJai's blogpost has been one of my favourites for a while now. I didn't know there was an animation like this, thank you. reply db48x 53 minutes agorootparentYou’re welcome. reply hcurtiss 3 hours agoprevI presume they used insecticides. Anyone know what they used? reply amluto 2 hours agoparentI don’t, but there are quite a few techniques aside from nasty insecticides: Fish. Many species of fish think that mosquito larvae are delicious and will eat them. Some of these species will also thrive even in small bodies of water with little assistance. Sterile insects. Male mosquitos don’t bite, and females only mate once, so releasing large numbers of sterile males will reduce the population. Wolbachia. There are bacteria that live in mosquitoes, are quite effective at infecting the next generation, will not infect humans, and prevent malaria from living in the mosquito. Bti. There’s a species of bacteria that produces a bunch of toxins that are very specific to mosquito larvae. I have no idea why it evolved to do this, but you can buy “mosquito dunks” and commercial preparations that will effectively kill mosquito larvae in water. They’re apparently entirely nontoxic to basically anything else. I expect that they’re too expensive for country-scale control, but they’re great for a backyard puddle. You can kill mosquito pupae in water by spraying with an oil that makes a surface film for a few days. The pupae suffocate. reply QuercusMax 2 hours agoparentprevI can't actually find any articles that actually describe it other than \"vector control\" and eliminating breeding sites, along with working with neighboring countries. I know some groups are using sterile male mosquitos to prevent breeding, as females can only mate once. reply dotancohen 4 hours agoprevDo other states in the area have malaria? How and when were they resolved? reply Jackim 4 hours agoparentEgypt has been low-risk for malaria for some time. This declaration is from the WHO: WHO declares a nation as a ‘malaria-free’ upon receiving valid proof that the Anopheles mosquito-borne native malaria transmission chain has been broken for at least the previous three years on a national level. A country must also demonstrate the capacity to prevent the re-establishment of transmission. In June 2024, the WHO confirmed that there was no local transmission of malaria in Egypt, with all identified cases being imported from endemic countries. Egypt’s robust surveillance system was instrumental in early case detection, facilitated by collaboration with relevant stakeholders. Neighbouring countries to the south have a high risk for malaria, but Egypt has had significant efforts to eliminate the disease since the '40s. reply ignoramous 4 hours agoparentprevIsrael Jacob Klinger quite famously rid Palestine (mandate) of malaria. In the Galilee and around Lake Kinnereth (Sea of Galilee), malaria had decimated the Jewish settlements, with the incidence rate at 95%+ of the workers in 1919. https://en.wikipedia.org/wiki/Malaria_in_Mandatory_Palestine reply pmontra 3 hours agoparentprevOn the north shore of the Mediterranean sea Italy got malaria free by removing many swamps and flooded lowlands, quinine and eventually by using DDT. It was a very long effort, more that one century long. Details at https://pmc.ncbi.nlm.nih.gov/articles/PMC3340992/ reply dsign 1 hour agorootparentHow dare they? Mosquitoes are a vital part of the ecosystems. And they should restore the swamps too. Now, that DDT perversion, for that alone they deserve a second flood. Sarcasm aside, I love swamps and I hate mosquitoes, other bugs and crocodiles because they don't let me enjoy the swamp. I also don't like cities nor agriculture for the same reason. But I like people and people being happy. Humanism and environmentalism are at odds more often than they are not. reply mmooss 31 minutes agorootparentDid anyone besides you say these things? reply TeMPOraL 50 minutes agorootparentprev> Humanism and environmentalism are at odds more often than they are not. It gets less surprising when people realize that nature is red in tooth and claw, an uncaring shithole we're evolutionary conditioned to find pretty - at least the parts we see. Beautiful meadows and happy animals and careless people are just propaganda - in reality, the people are sick and busy with back-breaking work, and animals are all on the verge of starvation, and that doesn't even touch the microbiological scale. Ecological balance is achieved by means that, when applied to balance between humans, we'd call unending war of attrition. Humanism and environmentalism are at odds because nature doesn't care about humans anymore than it cares about anything else. Brutal death and constant suffering are hallmarks of nature. reply soperj 3 hours agoparentprevThe US and Canada used to have a big malaria problem. Over a thousand people died constructing the Rideau Canal, and majority were from malaria.[0] [0]https://en.wikipedia.org/wiki/Rideau_Canal#Construction_deat... reply killjoywashere 31 minutes agoprevI find it hard to believe this will be durable, but best of luck reply D-Coder 2 hours agoprevGood news items like this in the medical, social, environmental areas is available via a weekly free email from https://fixthenews.com/. (A premium version is also available.) _Some_ of the items from last week (each has a paragraph of details): * India is finally becoming a clean energy superpower * United States designates a massive new marine sanctuary * India officially eliminates trachoma as a public health problem * Global electric vehicle sales soared in September * Global teen pregnancy rates have dropped by one-third since 2000 reply SoftTalker 1 hour agoparentAnother one that was pointed out on HN last week: https://www.newsminimalist.com/ Not strictly \"good news\" but tries to be significant news without clickbait. reply ricardo81 1 hour agoparentprevCool indeed. Traditional media is so desperate with click bait and riddled with ads. And social media of course. reply dr_dshiv 2 hours agoparentprevFormerly futurecrunch. It’s my only paid newsletter. I love it. reply Tepix 3 hours agoprevFantastic news! Now lets continue this trend! reply EGreg 19 minutes agoprevThe first to actually eradicate Malaria anywhere was Jacob Kliger, a Zionist who studied biology and epidemiology in New York City and came to Palestine. His programs involved education, draining swamps where anopheles mosquitoes lived, and it allowed many more people to settle the area - both Arabs and Jews. It showed the world the way to eradicate malaria. Is that right? https://en.wikipedia.org/wiki/Israel_Jacob_Kligler The reason I find this information fascinating and useful is because it provides a big counterpoint to anti-Zionists who claim that all “colonization” is bad for the people already living there. The word “colonization” as used by the Zionist congress in 1897 and until 1920 meant settling the land and farming, it didn’t mean “forcibly overthrowing the current government and creating a state” (since the Ottoman empire was ruling and would hardly allow it — any more than USA would allow Muslims moving to Michigan to overthrow the government of Michigan and create their own state there). Those who wanted to create their own state were called “Territorialists”, they went in search of other sparsely inhabited places like “Uganda” to do that. https://en.wikipedia.org/wiki/Jewish_Territorial_Organizatio... This isn’t to say Revisionist Zionists like Jabotinsky later didn’t want to create a state. Just that the original Zionists did not, they were mainly just small-time Kibbutzniks or Hassidim (who may have come even before Zionists in the 1880s). Even Noam Chomsky, a foremost critic of Israel and USA, who I interviewed a few times on my show, said he himself was a Zionist at that time and wanted to build a multinational country as per the Mandate that Britain had. And he said “until 1942 there was no official commitment among Zionist organizations to a State. And even then it was in the middle of World War II”. https://chomsky.info/20111107/ reply zjp 54 minutes agoprevFuck yeah! On to the rest of the continent. Let's eradicate our oldest, deadliest adversary. reply bufferoverflow 4 hours agoprev [–] Doesn't it take just one tourist with malaria to bring it back? reply tshaddox 4 hours agoparentFTA: > Certification is granted when a country proves that the transmission chain is interrupted for at least the previous three consecutive years. And > To get the WHO certification, a country must demonstrate the capacity to prevent the re-establishment of transmission. reply eacnamn 2 hours agorootparentDo you know whether that capacity is regularly reinvestigated? Because if not you could get the certification, wait a couple of years, and then dismantle all infrastructure while still reaping the, if ephemeral, benefits. reply lenzm 1 hour agorootparentAs if the main benefit of being certified malaria free is the certification instead of actually being malaria free? reply yaseer 4 hours agoparentprevIt's not quite like other infectious diseases (e.g. COVID), in that transmission is dependent on mosquitos as a vector. If they've sufficiently damaged the vector one tourist alone cannot bring it back - the disease vector would also need to come back. reply dotcoma 3 hours agoparentprevI don’t think so. I don’t think malaria is contagious. reply umanwizard 4 hours agoparentprev [–] Malaria isn't contagious reply folli 3 hours agorootparent [–] That's technically correct, but an infectious person can infect another person over a vector (i.e. mosquitos). So you get rid of mosquitos OR rid of malaria. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The World Health Organization (WHO) has declared Egypt malaria-free after nearly a century of eradication efforts, marking a significant public health achievement.",
      "Egypt is the third country in the WHO's Eastern Mediterranean Region to achieve this status, following the United Arab Emirates and Morocco.",
      "The certification requires evidence of interrupted malaria transmission for at least three consecutive years, and WHO emphasizes the importance of ongoing vigilance to sustain this status."
    ],
    "commentSummary": [
      "Egypt has been certified malaria-free by the World Health Organization (WHO) after a century-long effort, emphasizing the significance of understanding disease transmission and ecology.",
      "The achievement involved strategies such as monitoring, testing, and collaboration with neighboring countries, along with techniques like insecticides and biological controls targeting mosquito larvae.",
      "WHO certification necessitates evidence of interrupted malaria transmission for three years and measures to prevent its re-establishment, with Egypt's strong surveillance system playing a crucial role."
    ],
    "points": 313,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1729515068
  },
  {
    "id": 41898723,
    "title": "Microsoft said it lost weeks of security logs for its customers' cloud products",
    "originLink": "https://techcrunch.com/2024/10/17/microsoft-said-it-lost-weeks-of-security-logs-for-its-customers-cloud-products/",
    "originBody": "Microsoft has notified customers that it’s missing more than two weeks of security logs for some of its cloud products, leaving network defenders without critical data for detecting possible intrusions. According to a notification sent to affected customers, Microsoft said that “a bug in one of Microsoft’s internal monitoring agents resulted in a malfunction in some of the agents when uploading log data to our internal logging platform” between September 2 and September 19. The notification said that the logging outage was not caused by a security incident, and “only affected the collection of log events.” Business Insider first reported the loss of log data earlier in October. Details of the notification have not been widely reported. As noted by security researcher Kevin Beaumont, the notifications that Microsoft sent to affected companies are likely accessible only to a handful of users with tenant admin rights. Logging helps to keep track of events within a product, such as information about users signing in and failed attempts, which can help network defenders identify suspected intrusions. Missing logs could make it more difficult to identify unauthorized access to the customers’ networks during that two-week window. The affected products include Microsoft Entra, Sentinel, Defender for Cloud, and Purview, according to the Business Insider report. Affected customers “may have experienced potential gaps in security related logs or events, possibly affecting customers’ ability to analyze data, detect threats, or generate security alerts,” the notification said. Microsoft would not answer specific questions about the logging outage, but a Microsoft executive confirmed to TechCrunch that the incident was caused by an “operational bug within our internal monitoring agent.” “We have mitigated the issue by rolling back a service change. We have communicated to all impacted customers and will provide support as needed,” said John Sheehan, a Microsoft corporate vice president. The logging outage comes a year after Microsoft came under fire from federal investigators for withholding security logs from certain U.S. federal government departments that host their emails on the company’s hardened, government-only cloud; investigators said having access to those logs could have identified a series of China-backed intrusions far sooner. The China-backed intruders, referred to as Storm-0558, broke into Microsoft’s network and stole a digital skeleton key that allowed the hackers unfettered access to U.S. government emails stored in Microsoft’s cloud. According to a government-issued postmortem of the cyberattack, the State Department identified the intrusions because it paid for a higher-tier Microsoft license that granted access to security logs for its cloud products, which many other hacked U.S. government agencies did not have. Following the China-backed hacks, Microsoft said it would start providing logs to its lower-paid cloud accounts from September 2023. Carly Page contributed reporting. Topics China, cybersecurity, Microsoft, Security, us government Most Popular A closer look at the AirPods Pro’s new hearing aid features Brian Heater Throne’s toilet camera takes pictures of your poop Brian Heater Former OpenAI CTO Mira Murati is reportedly fundraising for a new AI startup Anthony Ha DJI sues Department of Defense over listing as a Chinese military company Anthony Ha What we know about the layoffs at Meta Cody Corrall I just spent my first week ever with an EV, the Chevy Equinox — here’s what it was like Matt Rosoff Stripe in talks to acquire Bridge for $1 billion Margaux MacColl Zack Whittaker Security Editor Zack Whittaker is the security editor at TechCrunch. You can send tips securely via Signal and WhatsApp to +1 646-755-8849. He can also be reached by email at zack.whittaker@techcrunch.com. You can also submit files and documents securely via SecureDrop. View Bio Related Hardware A closer look at the AirPods Pro’s new hearing aid features Brian Heater 7 hours ago Hardware Throne’s toilet camera takes pictures of your poop Brian Heater 1 day ago In Brief Former OpenAI CTO Mira Murati is reportedly fundraising for a new AI startup Anthony Ha 2 days ago Latest in Security See More Security 23andMe faces an uncertain future — so does your genetic data Carly Page 2 days ago Security Microsoft said it lost weeks of security logs for its customers’ cloud products Zack Whittaker 4 days ago Security Feds arrest man who allegedly participated in hack of the SEC’s X account, boosting Bitcoin’s price Lorenzo Franceschi-Bicchierai 4 days ago",
    "commentLink": "https://news.ycombinator.com/item?id=41898723",
    "commentBody": "Microsoft said it lost weeks of security logs for its customers' cloud products (techcrunch.com)269 points by alephnerd 21 hours agohidepastfavorite142 comments neya 11 hours agoIf you use Azure in any realistic production environments, then it's on you. Even with $100k in free credits, they couldn't convince me to use it for more than a month. It is expensive, the interface is highly user unfriendly and most important of all, their products don't at all seem reliable for production workloads because of stuff like this. Sorry Microsoft, I think you can do much better. reply prennert 8 hours agoparentWhen you come from other cloud providers, working with Azure has so many dark-orange flags. It feels totally inconsistent and patched together. This makes it hard for me to believe that anybody can properly audit it for security. The most uncomfortable part is their log in. The amount of re-directs and glitches there are insane. Its hard to believe that it works as intended. As an example, for some reason I could not download the BAA because trying to download it lead to a login loop on their trust website, while I was still able to see the Azure console ok in the same browser. When I signed out of my Azure account to try if a fresh login helped, it did not trigger my 2FA at the next login. In my mind, if I actively logged out from a browser window, I withdraw my trust in that device. So not being triggered for 2FA is a massive red flag. (no I still could not download the BAA, nor file a ticket for it, but somehow a colleague could download it ok.) reply chrisandchris 5 hours agorootparent> [...] is their log in. On every first try, I cannot log in into Azure Portal. I chlick \"try again\", it works. And it's like that for months, if not years. IMHO it says a lot of your culture if every first interaction of your customers with your product end with an error - and you simply don't care to fix it. reply paulryanrogers 4 hours agorootparentI wonder if things like this are due to testing only on the vendor's own/preferred browser. In this case Edge? reply rat9988 4 hours agorootparentAlmost 0 chance. reply velcrovan 4 hours agorootparentprevNo offense, but consider that there's a chance it's a problem on your end. I have never had this issue, and no one I know has had this issue. reply deathanatos 4 hours agorootparentEvery login I've ever done into the Azure portal is like the upstream describes: an absurd number of redirections and refreshes that leave you wondering \"is it supposed to work like that?\" I've also encountered strange bugs, like asking to log into tenant A and getting logged into, instead, tenant B. In a loop, effectively locking me out. The exact quirks and bugs seem to come and go, I presume as the code is changed & updated. reply NBJack 3 hours agorootparentprevWill add my anecdotal evidence: I've seen this across the board from Microsoft. I've been a customer for several decades, and it is a bit of a nightmare now. Logins that redirect to odd places. Jolting issues because you changed a seemingly innocuous security setting (i.e. OneNote refuses to sync on specific versions of the app/software if you don't grant them full access). Or just inconveniences, like having to login multiple times across their own sites when I dive into Office settings management. Seemingly forced use of the Microsoft Authenticator app from time to time. Multiple computers, multiple devices. I can usually work around it, but it is a pain. reply lukeschlather 3 hours agorootparentprevI would guess it is a problem with OP's account. Which is to say it is thoroughly a Microsoft problem, and probably one that could be fixed but would require weeks of back-and-forth until someone with direct access to some number of auth databases corrected the issue. I will say, they made a change to the auth system recently that made log-in significantly worse. Now several times a day my session expires or something and I go through a 5-10 second redirect flow which visibly jumps between different login APIs to refresh my log in state. (And of course this happens at the start of the day.) reply chrisandchris 3 hours agorootparentprevNone taken. It is probably my \"fault\" by using Safari (no extensions) and not the all-praised(tm) Edge. I couldn't add a billing profile to my MPN account the other day - endless loading without any indicator of success. It did work in Chrome though, except the \"save\" action which resulted in endless loading too, but still saved everything as expected. reply mrweasel 7 hours agorootparentprev> It feels totally inconsistent and patched together. I believe that multiple article, e.g. on The Register, has mentioned that people who have left the Azure team has routinely complained that the pace was to high, and that everything is pretty much duct taped together. This was years ago, so it may have changed. reply m_mueller 6 hours agorootparentNarrator: It hasn't. reply stogot 6 hours agorootparentprevI read that recently after their security breaches reply stogot 6 hours agorootparentprevI have had similar issues. And I know a fair amount about these systems, and still cant figure what the backend mess looks like that results in these problems. I found a reproducible login bug on Teams and spent a while trying to figure out who to report it to and gave up reply hedora 3 hours agorootparentprevI’ve never used Azure, but my kid plays Minecraft (offline), and got forced into using a Microsoft account to login. From what I can tell, they use it as proving ground for whatever crap they’re going to force on other applications. After getting it to work on a raspberry pi, I decided I wouldn’t use any logged in Microsoft product in a professional setting. Anyway, I’m sure they’ll eventually unify GitHub and LinkedIn login the same way they did with Minecraft. At that point, our industry will implode. reply blitzar 3 hours agorootparentprevWhen you come from bare metal, working with any of the cloud providers feels totally inconsistent and patched together. reply moi2388 4 hours agorootparentprevIt’s not. Their security has known massive issues and security holes, and they consciously do not fix them. Look at the CVEs for azure, msal and Active Directory for some good laughs. Now realise most governments, large companies and education works on this reply BodyCulture 10 hours agoparentprevI was laughing recently when at some place they started to install MS software on all Linux machines to integrate them into Azure. At that point you should just stop and think for a while about it. Didn’t you go for Linux because you wanted to have a reliable system? reply ratg13 9 hours agorootparentThe MS security software (for better or worse), is better than any open-source linux solution, and can follow attackers as they move laterally through the network, instead of linux servers being a big black hole were adversaries can do as they please. All security software from any vendor is going to have issues, and often you just have to go with whatever the company is running for the whole environment and not compromising security because of some jokes from the 90s reply stogot 6 hours agorootparentAll the Linux shops I know not using MS security are doing just fine and probably better given the current headline you’re commenting under reply ratg13 6 hours agorootparentYou seemed to have missed my point entirely. If your organization is running a chosen enterprise security solution, often fragmentation is not better, whatever your reasoning. reply BodyCulture 6 hours agorootparentThis is wrong. What you see as fragments are security boundaries for others. reply BSDobelix 5 hours agorootparentCorrect that's why for example the Root-DNS servers run Linux,FreeBSD and Windows. reply Gud 7 hours agorootparentprevMaybe it looks like a black hole to you - but there are open source operating systems with far better security practices than anything that came out of Redmond. reply ratg13 6 hours agorootparentYes, everything works better in a vaccum. You're not the first person to notice this. The point is, that if your organization has chosen an enterprise security platform, you don't make exceptions because of ideology. reply BodyCulture 6 hours agorootparentThe ideology here is „enterprise security platform“. This is marketing brainwash. reply ratg13 2 hours agorootparentAt the moment I can trace every action of every user on every machine, all from one platform that alerts me if anything abnormal happens. As an administrator of around 10,000 servers and devices, I have never had this ability before. I am sure there are better products out there, but this is what the company purchased, and the visibility it has given us into our organization has been a game changer for us. I apologize for not hating it just because it is Microsoft. reply eitally 2 hours agorootparentArguably, I'm not as concerned about \"every action of every user on every machine\" as I am the exceptions, and the usability issues the aforementioned \"security platform\" causes in terms of end user efficiency are probably not offset by the perceived security gains from your POV. Fwiw, for as much rightful criticism as Google receives for things like killing consumer products and behaving badly with user data, its internal IT runs better than -- in my opinion as an ex-employee -- any other large enterprise in the world. And it's secure. reply hulitu 5 hours agorootparentprev> The point is, that if your organization has chosen an enterprise security platform, you don't make exceptions because of ideology You're right. MS can always blame state actors when something fails. /s reply BSDobelix 9 hours agorootparentprev>and can follow attackers as they move laterally through the network, That i wanna see ;))) reply BodyCulture 6 hours agorootparentprevMostly it’s the other way around: attackers follow MS „security software“ to get deep into your systems. reply hulitu 5 hours agorootparent> Mostly it’s the other way around: attackers follow MS „security software“ to get deep into your systems. Don't tell them. They just forgot about this with the new Win 11 24H2. reply e40 6 hours agorootparentprevPlease give us details, because this seems unbelievable. reply ratg13 6 hours agorootparentIt's just basic EDR .. you have events that are flagged .. so on linux, let's say someone does something like setuid or setgid on a system file. Innocuous but potentially dangerous actions like this get flagged in the system. These events are correlated against other actions that might have happened on the same system or other systems that the user had logged onto prior to this one. Even if it's not the same user, the events are still correlated and alerted upon if suspicous. (both individually and holistically) If users are using microsoft authentication for access, the accounts will be flagged and locked out, generally forcing users to fully authenticate with MFA and forcing a password change. reply simonh 4 hours agorootparentMicrosoft isn't the only company to provide a service like this, and the others are cross platform. reply EricE 3 hours agorootparentOr open source - security onion is amazing! reply gruez 4 hours agorootparentprevCrowdstrike, for instance :^) reply BobaFloutist 3 hours agorootparentHey, an outage is better than a hack...right? reply lkjdsklf 3 hours agorootparentA crashed machine is a secure machine. That’s what grampy used to say reply opwieurposiu 2 hours agorootparentIf you can't boot it, they can't hack it. reply hulitu 5 hours agorootparentprev> If users are using microsoft authentication for access, the accounts will be flagged and locked out, generally forcing users to fully authenticate with MFA and forcing a password change. Last i heard the \"state actors\" had access to AD master credentials. reply hulitu 5 hours agorootparentprev> The MS security software (for better or worse), is better than any open-source linux solution is it able to detect ransomware ? Seeing MS and security in the same sentence makes me suspicious. reply stackskipton 2 hours agorootparentYes. Their security products are not terrible outside the fact many are acquisitions that have been shoehorned poorly into InTune. reply light_hue_1 8 hours agorootparentprevThe joke from the 90s is the fact that people still use MS products and think they aren't compromising security. MS have had disastrous outcome after disastrous outcome with an uncountable amount of security holes. There's been an astronomical toll on the economy from their crappy software with no end in sight. reply mschuster91 6 hours agorootparent> The joke from the 90s is the fact that people still use MS products and think they aren't compromising security. Well, it's not like there are that many alternatives. macOS is out of the price range for public service and most large companies, in addition to a lot of specialist software not being available for macOS. Linux has it even worse regarding application compatibility on desktop - and no, WINE is not an option, because the kind of software used in public services comes with strict stipulations where you can run it, sometimes down to minor versions, and if you violate that, the vendor can and will refuse support. For a lot of FOSS software, there isn't even commercial support available so it gets automatically off the list because companies actually want to pay people so that they have someone to talk to when they get issues. And that's before you hit the cost wall that is employee (re)training. IMHO, it would have been the role of our governments to mandate MS get their shit together first before diving into AI and advertising crap. reply ramses0 5 hours agorootparentprevHowever: Micro$oft deserves _massive_ credit for biting the bullet and systematically improving their security posture post like IE7. *nix started from a better _initial_ posture as it was multi-user, permissioned, and network-aware from the start (vs. corporate MS-DOS => single user => GUI => networked), but MS really doubled down on systematic improvements that Linux is only now going through. See the recent CUPS fiasco, C-code from 1999 running as root, and the \"stuck in the mud\" mentality that Linux has because there isn't the appetite for consistent investment and wholesale overhauls. It has to do with \"activation energy\" and \"local maxima\". Linux feels like it's reached the local maxima, and it's a pretty tall peak to start from, so we can't get over the hump to make a step-change or drop back to a hypothetical \"POSIX 0.5\" so we can pivot to a \"POSIX 2.0\" (eg: take the loss for a decade or so in reduced functionality to end up on a more sane \"other side\" with better security principles and systematic depreciation of inefficient or insecure API-types). There was a LWN article which talked about \"permissions should be managed at the mount level, not the file level\", and honestly that makes so much more sense, but it \"loses\" POSIX, and no one person is willing to \"break linux\" to admit to that mistake. Tons of other examples (eg: file race conditions, unprivileged by default, more protections on /usr than /home, etc) reply nullindividual 3 hours agorootparent> *nix started from a better _initial_ posture as it was multi-user, permissioned, and network-aware from the start (vs. corporate MS-DOS => single user => GUI => networked) Windows NT started as a multi-user, permissioned, and network-aware OS. The team that built NT came from DEC, not the MS-DOS team. Windows Me was the last version of Windows that had any form of DOS underpinnings. reply justinclift 5 hours agorootparentprev> but MS really doubled down on systematic improvements Doesn't seem to have really worked for MS though, as evidenced by their many significant security lapses over the last several years. The US Gov even officially called them out on it a few months ago, specifically singling out MS for their atrocious repeated security fuck ups. reply ramses0 4 hours agorootparentDownvotes accepted, I guess, but there was a step-change improvement. References: https://www.itprotoday.com/attacks-breaches/the-story-behind... https://www.microsoft.com/en-us/security/blog/2022/01/21/cel... ...while they may also (deservedly) be getting flack now, 20 years ago it was orders of magnitude worse. reply blueflow 9 hours agorootparentprev> and can follow attackers as they move laterally through the network ... which does not stop them from disrupting production and stealing your data. Your defenses are at the wrong place. reply ratg13 8 hours agorootparentIt does stop them, actually. It's not perfect, but it does work. reply imglorp 2 hours agoparentprevEven internally: \"Not even LinkedIn is that keen on Microsoft's cloud: Shift to Azure abandoned\" https://www.theregister.com/2023/12/14/linkedin_abandons_mig... reply eitally 2 hours agorootparentTo be comparatively fair, Google doesn't run almost any of it's public products on Google Cloud, either (nor many of the internal apps). reply BSDobelix 9 hours agoparentprev>I think you can do much better. Not to be a troll, but I really think they cannot. The last \"good\" product they made was SQL-Server/Exchange/Windows2000, and that was a long time ago. reply mrweasel 7 hours agorootparentWhile I can think of a few other, dotnet and Visual Studio, I think that you're generally correct. Microsoft, Google and others, have created a culture that are no longer able to produce high quality solutions, because they can't focus on a single vision for their products. Or in some cases the vision does not align with creating good products. SQL Server is a really good example, it's highly focused, it exists outside the current hype bobble, there's no advertising, no subscription, just a database server and it's a really good product. Exchange sucks, because it been pulled in to new subscription based world, and it's going to suffer for it. reply dijit 4 hours agorootparentFamously, visual studio gets worse- not better, with time. https://youtu.be/GC-0tCy4P1U reply marcosdumay 4 hours agorootparentWell, it gets better and worse, with a worsening trend. It's not monotonic, so one can easily point \"hey, VS XX is better than VS YY for some XX > YY\". reply j16sdiz 7 hours agorootparentprevdotnet is a mixed bag of good and bad. VSCode catch on, but i would rather have Atom instead. Exchange have beth broken before migrating to cod reply Tempest1981 1 hour agorootparent> migrating to cod cod? Call of Duty? reply preciousoo 3 hours agorootparentprevThe topic is good software and you mention Visual Studio? reply cookingrobot 31 minutes agorootparentprevI worked on Windows 2000, thanks! But Windows 7 was better. reply renegade-otter 8 hours agorootparentprevJust judging by the deteriorating state of the Windows OS... I know these are different divisions, but it does say something about the culture. Windows has always been a dumpster fire, but when it was built by nerds and not managers, it felt more, uh, tolerable. reply rightbyte 6 hours agorootparent> but when it was built by nerds and not managers, it felt more, uh, tolerable. The 'WIN32_LEAN_AND_MEAN' era. Ye. Way more relatable than todays malware riddled joke of an OS. It is too bad since the Windows 7 foundation seems OK. reply phkahler 7 hours agorootparentprev>> Windows has always been a dumpster fire.. It was always a dumpster fire for security, but it did have a pretty good UI and functionality at say XP-SP3, but now the UX had been thrown on the fire too. reply renegade-otter 7 hours agorootparentI remember enjoying using Windows 2000/XP but I feel like that's my nostalgia talking. I was customizing a new installation for days, messing with registry keys and obscure settings dialogs. It was never that user-friendly to begin with. After having used MacOS for the last few years, I do not miss the hassle. To be fair, not a lot of things were user-friendly back then, and Windows was the standard consumer OS for a good reason. It was solidly OKAY. Using the latest versions of Windows, however, is just infuriating even without any complicated setup. reply Gud 7 hours agorootparentAbsolutely not your nostalgia talking. I’m as OS agnostic as they come and Win2k was the last true great desktop OS. I now use FreeBSD almost exclusively, with miscellaneous VM guests. reply eitally 2 hours agorootparentI grew up with an Apple II, then switched to Windows from 3.11 for Workgroups all the way up to Vista, at which point I switched to desktop Linux (variety of distros, but mostly ended up on Kubuntu in my house and Mint for family). Then it was 8 years of ChromeOS. The past couple of years I've been on MacBooks and, although there are quirks I don't really like, I can't argue with the fact that it mostly \"just works\", which is really the primary requirement of any operating system. reply Gud 1 hour agorootparentStill, I would say peak Win2k was faster, cleaner and more no nonsense than modern MacOS. I use macs as well, they are not at all as snappy as windows 2000 was. reply bbkane 4 hours agorootparentprevI actually REALLY LIKE MacOS, especially workspace/window management when using Rectangles. So much so that I'm trying to recreate it on Linux (I don't want to buy a new Mac when I have a perfectly good gaming desktop to repurpose for dev work). reply Gud 3 hours agorootparentMacOS is pretty good, can’t argue with you there. reply miyuru 7 hours agoparentprev> If you use Azure in any realistic production environments, then it's on you its unfortunately decided by the higher ups, who just follows the hype train. reply mistrial9 5 hours agorootparentI disagree this is \"hype train\" trails that lead to Azure. Management and their legal departments navigate in different ways. reply belter 2 hours agoparentprev\"Azure’s Security Vulnerabilities Are Out of Control\" - https://www.lastweekinaws.com/blog/azures_vulnerabilities_ar... reply rmbyrro 2 hours agoparentprevMicrosoft just opened a new startup vertical: security services for security logs. If those startups use Azure to run their production workloads, the industry will quickly enter an infinite loop and skyrocket to $2 trillion/yr. reply rhaps0dy 5 hours agoparentprevAzure Blob storage is considerably cheaper than S3 or Google, for example. (Not cheaper than Cloudflare, but that one doesn't have a supported FUSE driver). I've been trying hard to find instances where they lost data and could not. Them offering the ~same product but cheaper is good. reply nijave 5 hours agorootparentiirc Blob Storage is tied to a \"storage account\" that has throughput limits that can't easily be changed so it has a performance ceiling reply gruez 4 hours agorootparentprev>Azure Blob storage is considerably cheaper than S3 or Google, for example Really? I did a quick search and azure charges 2.08 cents per GB for \"hot\" storage compared to 2.3 cents for aws. That's not that big of a difference. Am I missing something? reply victor106 3 hours agoparentprevAgree 100% with this. One example is if you have multiple subscriptions and you want to select a particular subscription; the UI is so horrendous that even after using it everyday it’s so confusing. It’s such a simple thing that I am sure MSFT implemented it a million times but they just can’t do it in Azure. It’s the worst of the three cloud providers. The main reason they are second is because they have a sales org that sells well to naive cto’s. reply crmd 4 hours agoparentprevThe UI, retail pricing, and reliability reputation are not primary factors for large enterprise IT infrastructure and cloud decision makers. They look at: 1. Executive Support - can you assure me that MSFT will have my back when (not if) the shit hits the fan? Can I count on Satya or Jason Zander calling my CEO to reassure them if we’re working through a catastrophic issue? Because as an executive my career at this company is over otherwise when that happens. 2. Industry and analyst landscape - Which of my competitors / peers use your technology? I won’t be first in the pool. What does Gartner tell me about your company behind closed doors? 3. Competitive - Do any of your divisions compete directly with any of ours? Because I’ll be fired at the next board meeting if they read in the WSJ that we’re funding an adversary. Cost is negotiable, what is a UI?, and sorry, I don’t care if all of the above is good but Azure isn’t the engineers’ favorite thing. Y’all work for me. reply miah_ 4 hours agorootparentHaving worked for many bosses like you, I think the solution is clear: tech needs more unions and co-ops. reply crmd 1 hour agorootparentI’m 100% pro union and not the guy you’re thinking of. Apologies if that wasn’t clear because of the first person writing in my comment. I’m an engineer on the vendor side that begrudgingly got promoted into CTO role where I was helping get deals done with F100 c-levels. So I know how these people think. I hated it, left enterprise a few years ago and never looked back. reply thewebguyd 3 hours agorootparentprev> 3. Competitive - Do any of your divisions compete directly with any of ours? Because I’ll be fired at the next board meeting if they read in the WSJ that we’re funding an adversary. This is a big point that others in this thread are missing. Amazon is increasingly competing in more and more spaces, and companies are rightly hesitant to get into bed with Amazon when they are a direct competitor. Azure is the only other serious choice, GCP isn't even going to be considered. Silicon Valley might run on AWS but the rest of non-tech company corporate America runs on Azure (or on-prem still). The IT landscape looks a lot different outside of the SF Bay Area SaaS bubble. reply stackskipton 2 hours agorootparentIt’s the reason we are over in Azure. We compete somewhat with Amazon retail and our customers compete 100%. reply rdl 6 hours agoparentprevFor a long time they were the leader in confidential computing and a few other specific things. reply jnsaff2 8 hours agoparentprevportal.azure.com developers are _proud_ to claim that they have the largest SPA in the world[0]. I hated every moment of using it. [0] https://learn.microsoft.com/en-us/shows/visual-studio-visual... reply marcosdumay 3 hours agorootparentOh, if the devops (new tfs) interface redesign is a representative sample, it's easy to make the world's largest SPA when you convert simple form submits into 5 JS-loaded logical pages, with unreliable navigation and complex JS session data that is too large to transfer on a LAN. I imagine they can beat any record with a simple single-table CRUD. reply moi2388 4 hours agorootparentprevAnd if you change some state, better refresh the page because updating the UI or two way data binding isn’t something they haven’t figured out yet at Microsoft apparently reply Citizen_Lame 6 hours agoparentprevThis is wrong take on Microsoft. In their entire existence they couldn't do better and they will never be able to do so. There is no incentive, as long as monopoly money from captures audience keeps rolling in. reply sublimefire 8 hours agoparentprevIt really depends on what type of business you run and who will be building and maintaining the system. Azure gives the business the ability to integrate with other MS systems and has good sales teams who will hold your hand. If you are an ISV then it is not that important to you, instead you need specific SLAs, region support and an easy path for the integration. Overall nobody cares about small teams that count every penny and spend up to XXk a month on infra because they could spin up their openstack cluster at any moment and leave. I agree there is room for improvement but your arguments are weak. The user interface (whoever is using it?) is questionable in AWS and in GCP as well, IMO it is because of the underlying complexity in all clouds. Reliability statement should be backed by the existing SLA, or is it some complaint that MS does not provide four/five 9s for every service? The bit about it being expensive depends on what you compare it with, AWS is notorious as well, every time you need something to build you do not know if that will cost 1k or 10k per month. I am not some sort of Azure fanboi and love AWS but there are things MS is good at as well, however people hate that. reply locusofself 14 hours agoprevThere really was a bug in an application that just about every team runs on their VMs (simplifying here) that pushes application logs to storage. Even my team had to restart processes to get logs going again. It was a \"sev 0\" incident - an oopsie that was not easy to fix without many, many teams taking manual steps to restart agents which normally just hum along in the background. reply justinclift 5 hours agoparent> an oopsie that was not easy to fix Wouldn't it be nice if MS actually did automated testing to a reasonable depth, so stuff like this wouldn't keep happening? The recent ClownStrike global outage showed a lack of testing before deployment (by ClownStrike). This latest MS problem just demonstrates it's happening at the source (of Windows) too. It's not a good look. reply thewebguyd 3 hours agorootparentThis is an industry-wide problem, not exclusive to Microsoft. I feel like everyone has just outsourced QA to users. There's been a drastic decline in software quality at release, particularly in the past year-year and a half. Initially I thought maybe it was just getting difficult to maintain these behemoth platforms that have been around since the 90s but it's infected the gaming industry as well, total green field projects where you can expect the v1.0 release to be almost unusable until 1.1 or 1.2+ reply eitally 2 hours agorootparentI think a lot of it has to do with how little software -- even enterprise software -- is actually written from the ground up. Reliance on both external libraries and modules owned by unrelated internal teams has made a lot of both the programming and debugging almost black box, where effective testing isn't really tractable. reply stogot 6 hours agoparentprevAre you talking about within Microsoft internally or as a customer of theirs? reply ethbr1 16 hours agoprev>> The affected products include Microsoft Entra, Sentinel, Defender for Cloud, and Purview, according to the Business Insider report. Oof. Entra (formerly Azure Active Directory) being impacted is rough. But who needs SSO logs? reply VyseofArcadia 6 hours agoparentJesus Christ, MS. Pick a name and stick to it. Azure Active Directory wasn't great, but the constant rebranding of everything all the time is exhausting. reply marcosdumay 3 hours agorootparentMS drops products faster than Google. But they go through the hassle of creating a completely redundant one, porting all the users, and only then killing the old one. Some times the new one is even an improvement over the one they are killing... I mean... it's not the rule, but it happens here and there. reply quercusa 1 hour agorootparentprevRumors are that people expected AAD to be \"AD, but in the cloud!\" and were sorely disappointed to find out that it wasn't. reply switch007 3 hours agorootparentprevMicrosoft are hilariously bad at naming and I don't think they'll ever improve Blazor Server Blazor Web App Blazor WebAssembly .NET Framework .NET Core .NET EF EF 7 EF Core reply gonzo41 16 hours agoparentprevThere was a time when we looked at the sky and didn't think about asteroids hitting the earth. Ignorance is bliss. -- btw, when they listed Entra, I thought it was Encarta. I momentarily so excited that still existed. reply isodev 13 hours agorootparentThat animation when it was searching... it was so amazing back then, you could \"feel\" the knowledge and facts unfolding in one's computer. reply ethbr1 2 hours agorootparentI'm kind of sad for the people that didn't get to experience the magic of book encyclopedia -> encyclopedia on CD-ROM -> Wikipedia. It was a lot of amazing change to live through! (Cue grade school teachers reminding students that Wikipedia isn't a valid source) reply aitchnyu 11 hours agorootparentprevThe same users then revolted at travel sites returning results too fast, so they had spinners to show they were hard at work compiling results. reply rudasn 15 hours agorootparentprevEncarta 96 reply hulitu 5 hours agoparentprev> But who needs SSO logs? You don't. Without logs, there is no compromise. reply anthk 3 hours agoparentprevThis is a good source of jokes in Spanish, as entra means \"You, come inside/get in\" (as being called to enter in a room/building/space). reply hggigg 11 hours agoparentprevUh, we do. Have compliance obligations. Fortunately our production systems are NOT integrated into Entra. Only the non-customer stuff. reply tjpnz 8 hours agorootparentWell you're still hosed if investigating a back office breach. reply hggigg 6 hours agorootparentHey I want advance knowledge of how hosed everyone is so I can get my resume out first. reply bobnamob 10 hours agorootparentprevI'm definitely reading > But who needs SSO logs? with a heeeefty dose of /s reply hggigg 10 hours agorootparentLacking a hefty dose of coffee and humour here :) reply CoastalCoder 7 hours agorootparent> Lacking a hefty dose of coffee and humour here Look, you can claim to drink coffee, or you can spell \"humor\" with an \"ou\", but not both :) reply SSLy 7 hours agorootparentYou can if you learnt BrE in school… reply stavros 6 hours agorootparentOr if English isn't your first language... reply hggigg 7 hours agorootparentprevHaha :) Sod tea. Nasty stuff. Right to chuck it in the sea! reply ruffrey 2 hours agoprevLet's to forget this long article from just over a month ago, outlining Microsoft's failings and seemingly willful neglect regarding cybersecurity overseas. https://www.lawenforcementtoday.com/bombshell-allegations-th... reply eitally 1 hour agoparentOn the one hand, there are some important nuggets in this report. On the other hand, Schiller doesn't seem like an entirely credible witness and his outreach to look for government oversight seems limited to extreme MAGA Republican lawmakers, which is also telling. That said, I 100% agree that 1) relying on foreign national support staff to support critical USG infrastructure should not be allowed, and 2) all the big tech companies -- including the hyperscalers -- have deals with the PRC via domestic proxy businesses (Tencent, Alicloud, etc) in order to allow them to operate in China. There isn't enough oversight of these contracts, or the terms allowing Chinese hands-on access to the hardware & software stacks. reply kelsey98765431 18 hours agoprevI wonder which intelligence operation this supported... reply h4ck_th3_pl4n3t 12 hours agoparentWe call them APT -1 (Microsoft) reply notimetorelax 12 hours agoparentprevNone, Microsoft’s internal logging infrastructure is from the nineties. reply sofixa 12 hours agorootparentSame as their security infrastructure then. reply marcosdumay 3 hours agorootparentWhoever ran the Solar Winds infrastructure probably put some effort into hardening the MS infrastructure too. reply jaimsam 17 hours agoparentprevnext [9 more] [flagged] LAC-Tech 16 hours agorootparentWhat are you referring to here? reply cptskippy 17 hours agorootparentprevLiving in your mind must be wild... reply zmgsabst 17 hours agorootparentnext [7 more] [flagged] mattigames 16 hours agorootparentNor anyone prosecuted for the disappearance of the security cam footage of his cell during the alleged suicide due a \"technical error\". reply hsbauauvhabzb 15 hours agorootparentThis entire thread smells of multiaccounting reply h4ck_th3_pl4n3t 12 hours agorootparent> This entire thread smells of multiaccounting Indeed so, username \"hsbauauvhabzb\". reply mattigames 15 hours agorootparentprevMultiaccounting? Why don't you ask Dang about that? I'm sure all my IPS are from Colombia since 2018 and all my comments about living here are all plot of a major conspiracy (yeah the irony it's not lost on me), you people are not that good at spoting multiaccounting as you believe yourself to be. While we are in the subject, probably someone should research such thing, I wouldn't be surprised if the results show that correctly spotting multiaccounting it's impossible for most even by techies except maybe for the less sophisticated attempts, like bots that just use text templates or just copy-paste without changing it much. reply hsbauauvhabzb 14 hours agorootparentWeird response, and cool story. reply mattigames 13 hours agorootparentThat's really the best argument you have, \"weird response\" and \"cool story\"? Come on. I do grant that the other user is very sus, \"jaimsam\" doesn't have any comments and -4 karma, but then again you could suspect its a throwaway account of someone else, anyone at all, maybe even yours? Pretty sure Dang could easily check who it belongs to by checking the IP address and other clues, but it may no be in the best moderation interests to disclose as much. This is the fitting opportunity to declare that sometimes I suspect I think a bit too highly of the users of this forum than deserved when they make this kind of poor judgment calls, I don't believe its you alone in that regard. reply azuresucksdeez 9 hours agoprevAzure sucks. Especially the Batch Service, the jobs scheduler is not accurate at all. reply passwordoops 8 hours agoprevAnd MS expects us to trust they can deliver a functional, useful \"AI\" service product? reply lupusreal 5 hours agoparentIt could be functional and useful, but I wouldn't bet on secure. reply outside1234 15 hours agoprevWhy did they admit this publicly? reply tacticus 14 hours agoparentthey got called out for hiding the reporting of it in tooling that can't be accessed by most security teams. reply dathinab 14 hours agorootparentand they have some large contracts to which they are legally obligated to disclose it, maybe why they tried to hide it reply downrightmike 15 hours agoparentprevProbably to have something out there so that when they admit it was a foreign actor who deleted them, it won't seem like big news that it is. That's typical for MSFT and how they handle these things. reply hulitu 13 hours agorootparent> when they admit it was a foreign actor It is always a \"foreign actor\". I bet that all bullshit implemented in Windows in the last years (telemetry, spying, dumbed down UI) was also from a \"foreign actor\". /s reply JohnMakin 3 hours agoprevSome of the worst infrastructure I've ever seen with terrible practices had elaborate mechanisms in place to make this kind of thing effectively impossible, because if it happens it's... pretty damn bad. I'm not sure I'd ever want my business to sit on Azure-managed cloud infra even before this. I'm trying to go through some thought experiments and even imagine how something like this is possible without some kind of full-system catastrophic error and I'm struggling. reply PunchTornado 9 hours agoprevThere were comments here about how msft is more enterprise friendly than google because they don't lose any data. msft is the opposite of reliable. reply hinkley 17 hours agoprev [3 more] [flagged] karlgkk 16 hours agoparent [–] Don’t worry, they did that too a couple of years ago. They also lost their tenancy separation records. reply reddalo 11 hours agorootparent [–] > They also lost their tenancy separation records. What does that mean? Can you find any source about it? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft experienced a bug in its internal monitoring agents, resulting in over two weeks of missing security logs for certain cloud products from September 2 to September 19.",
      "The affected products include Microsoft Entra, Sentinel, Defender for Cloud, and Purview, impacting network defenders' ability to detect intrusions.",
      "Microsoft has resolved the issue and informed affected customers, amidst previous criticism for withholding security logs from U.S. government departments."
    ],
    "commentSummary": [
      "Microsoft experienced a loss of weeks' worth of security logs for its cloud products, sparking concerns about the reliability of Azure, Microsoft's cloud computing service.",
      "Users have criticized Azure for its interface, security, login issues, and inconsistent performance, suggesting that Microsoft's cloud services may be poorly integrated.",
      "The incident raises broader concerns about Microsoft's focus on sales over product quality and its ability to provide secure and reliable cloud services, despite Azure's competitive pricing."
    ],
    "points": 269,
    "commentCount": 142,
    "retryCount": 0,
    "time": 1729460547
  },
  {
    "id": 41898736,
    "title": "Today is Ubuntu's 20th Anniversary",
    "originLink": "https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000003.html",
    "originBody": "Announcing Ubuntu 4.10 \"The Warty Warthog Release\" Mark Shuttleworth mark at hbd.com Wed Oct 20 11:06:23 CDT 2004 Previous message: Announcing Ubuntu 4.10 (Release Candidate) Next message: Warty Live CD Released Messages sorted by: [ date ] [ thread ] [ subject ] [ author ] === Announcing Ubuntu 4.10 \"The Warty Warthog Release\" === The warm-hearted Warthogs of the Warty Team are proud to present the very first release of Ubuntu! Ubuntu is a new Linux distribution that brings together the extraordinary breadth of Debian with a fast and easy install, regular releases (every six months), a tight selection of excellent packages installed by default and a commitment to security updates with 18 months of security and technical support for every release. You get a distribution that is: * absolutely committed to free software, every end-user application on the CD is free software * 100% free of charge, and the Ubuntu team is committed to keeping Ubuntu free of charge * security updates for the distribution at no charge for 18 months for any release * updated to the latest desktop and kernel and infrastructure every six months with a new release * supports x86, amd64 and ppc processors, with additional ports under way If you've heard all about Ubuntu and just want to get the install CD or test the Release Candidate Live CD, you can download it here immediately: http://www.ubuntulinux.org/download/ If you want a shrinkwrapped CD we will gladly ship it to you at no cost. To receive a complimentary copy of the Warty Warthog CD -- or a handful to give to your friends, your school or LUG, register online at: http://shipit.ubuntulinux.org/ For more information, you can turn to any of the following resources: Ubuntu Website: http://www.ubuntulinux.org The website contains some basic background on Ubuntu, an overview of the project, information on how to get it, and some documentation for the software. Ubuntu Wiki: http://wiki.ubuntulinux.org The wiki is a shared web space used by the Ubuntu community to develop new ideas for Ubuntu. Anybody is welcome to edit and add to the wiki. Ubuntu IRC Channel: #ubuntu and on irc.freenode.net The Ubuntu IRC channel is your best place to start for help and discussion about Ubuntu and the Warty Warthog release. We aim to keep the signal-to-noise ratio as high as possible on that channel, and on all community forums. Ubuntu Mailing Lists: Ubuntu mailing lists are the heart of our community. In addition to the announcement list, and lists for users and developers of Ubuntu, there are now Ubuntu mailing lists in German, French, Spanish as well as lists devoted to Ubuntu security, news, translators, and the inevitable lighthearted chitchat list (\"the Sounder\"). To get more information or subscribe, visit: http://lists.ubuntu.com Warty Warthog Features * Simple and fast Installation Ubuntu comes on one single CD, with thousands of extra packages available online. The install is optimised for speed and simplicity. Ubuntu has excellent support for laptops (both x86 based and Powerbook / iBook PPC based), and can also be setup in a minimalist server configuration. * GNOME 2.8 Ubuntu was the first distribution to ship Gnome 2.8, on the day of the 2.8 release. Ubuntu is a great way to try out Gnome 2.8 if you have not already experienced its speed and simplicity. * Firefox 0.9 (with security patches) * First class productivity software Evolution 2.0 and OpenOffice.org 1.1.2 * XFree86 4.3 with improved hardware support We also worked hard to detect as much hardware as possible, simplifying the X install considerably. Warty can be installed in a minimalist mode for servers, or in full desktop mode. It works well on laptops and desktops. Warty is secure by design - a key goal was to ensure that Warty was as safe from attack over the internet as possible after a default install. Thanks to the team of professional and volunteer maintainers who have worked so hard to bring the Warthog to life, and also to our rapidly growing community, who have provided excellent testing and ideas for the future of Ubuntu! \"Ubuntu\" is an ancient African word for \"humanity towards others\", and we think it's a perfect name for an open source community project. In that spirit we invite you to join, to contribute and to share Ubuntu with your own community. Our next release, the Hoary Hedgehog, is due in six months time. You can help to shape it by joining the team and contributing your own expertise. See you at #ubuntu on irc.freenode.net. Previous message: Announcing Ubuntu 4.10 (Release Candidate) Next message: Warty Live CD Released Messages sorted by: [ date ] [ thread ] [ subject ] [ author ] More information about the ubuntu-announce mailing list",
    "commentLink": "https://news.ycombinator.com/item?id=41898736",
    "commentBody": "Today is Ubuntu's 20th Anniversary (ubuntu.com)235 points by aquova 21 hours agohidepastfavorite100 comments neom 20 hours agoI'm sure lots of people have lots of thoughts on them, but personally I'd like to give a shout out to Canonical. At least from my perspective in the early days of DigitalOcean the few interactions I did have with them were super positive, they seemed to really want us to win. I'll always have a soft spot Ubuntu and as far as community stewards go, on average Mark Shuttleworth has been good. Thanks Ubuntu Community! Thanks Mark! Thanks Canonical! To me Ubuntu is what Mandrake never became. reply silisili 19 hours agoparentUbuntu was leaps ahead of Mandrake at least in installation friendliness, which, let's face it - is often the biggest hurdle getting people interested. The installer wasn't bad, but kinda threw you to the wolves wrt partitioning and such. As a novice, I had no idea what this meant. Ubuntu came along and made it easy. A live bootable image to play with and see work, and an installer that just let you click through and let it do the dirty work without me having to know what I was doing. That went a long way, and IIRC was the first of its kind to take this approach. I'd honestly still be using it today if not for snaps. I generally don't like tinkering and optimizing, much preferring to just get something working quickly and out of my way. reply arp242 18 hours agorootparentI don't recall Mandrake being particularly difficult, although it's been a long time and I don't remember much details, but it was my first Linux distro back in the day. From what I recall, the installation was very similar to Ubuntu(?) I guess my memory of things must be wrong. Might be interesting to get a hold of an old Mandrake install CD and try in QEMU. I do remember I had some problems upgrading Mandrake: after the upgrade I just got gibberish on the screen – some X problems I guess, but I didn't have the skill to debug it at the time. I just reinstalled with FreeBSD (which I had tried before Mandrake, but I couldn't get \"xfree86 -configure\" to work – the second time I had learned enough from Mandrake to make that work) and didn't look much at Linux for a long time after that. reply silisili 18 hours agorootparentCame across this. Better than many others, but still a little technical for a newbie IMO. https://youtu.be/GvFelGwZBcc reply arp242 18 hours agorootparentLooking at release dates for Mandrake, FreeBSD, and Ubuntu, I think I must have used Mandrake 9.x, or maybe 8.x. I found a video of that: https://www.youtube.com/watch?v=CBpRuXjHTw8 – in a quick watch-though, it seems roughly similar to what I remember of early Ubuntu (actually, in a quick check it seems that very early Ubuntu only had a text installer?) I think Linux is inherently a bit tricky/hard for a newbie, because unlike Windows or macOS it can't just assume it's going to be the only OS. Installing e.g. Windows isn't necessarily newbie-friendly either – it's just that most people never have to do that. reply mmcnl 11 hours agorootparentprevYou can install Gnome Software with one command if you don't like snaps. That's a negligible amount of tinkering for the average Linux user. reply Moomoomoo309 6 hours agorootparentYou also need to add PPAs to get the non-snap versions of certain applications, like Firefox, since Ubuntu's deb package for it just installs the snap. reply silisili 2 hours agorootparentprevThis wasn't true the last time I messed with it. I used synaptic to install packages, and many would just install the snap version anyways. Has that changed? reply delduca 20 hours agoparentprevI miss Mandrake, after Slackware, it was my first distro! reply neom 19 hours agorootparentI learned linux proper because I ordered a mandrake CD in the mail. I saw a friends Slackware and I presumed it was just a \"cooler windows\" or something, so I ordered Mandrake, installed it, was like uh oh..oooh shit... this isn't windows at all...!!! But I couldn't figure out how to go back to NTFS so I just learned linux instead ha. Doubt I'd be here today if it was not for Mandrake (and basically hand rolling a modem driver). reply linguae 20 hours agoprevI believe Ubuntu has been a positive for the Linux community. While there definitely were distributions before Ubuntu that focused on the user experience (Mandrake Linux and Lindows/Linspire come to mind), there are many people whose first experience with desktop Linux was through Ubuntu. Although I personally prefer FreeBSD for my Unix needs and Debian if I need actual Linux, Ubuntu is the distribution I recommend to those who are coming from Windows or macOS who want to try out desktop Linux. The last few times I used Ubuntu, whether it's on actual hardware or inside a VM, it seems to be reasonably simple to install, has sensible defaults, and supports a wide range of hardware. reply musicale 20 hours agoparentAgreed - Ubuntu is the path of least resistance for installing Linux on your laptop or desktop. But I still appreciate KDE-based Linux environments for their more straightforward, consistent, no-nonsense GUI, which seems to be derived from classic (pre Windows 8) Windows. Another thing that KDE seems to have gotten right is realizing that what makes macOS and Windows useful isn't just the GUI itself but the set of apps that use it and interoperate seamlessly with each other. Ubuntu seems to have more UI churn than I'd like (even though I prefer Mac-style menu bars, etc.) And Wayland (which KDE has also moved to for better or for worse) has never brought me happiness. I understand the motivation for Snaps, but I only want them for app store type apps, not for everything. reply jorvi 20 hours agorootparent> But I still appreciate KDE-based Linux environments for their more straightforward, consistent, no-nonsense GUI, which seems to be derived from classic (pre Windows 8) Windows. Another thing that KDE seems to have gotten right is realizing that what makes macOS and Windows useful isn't just the GUI itself but the set of apps that use it and interoperate seamlessly with each other. I am deeply confused by this passage. KDE takes a much less staunch top-down development approach than Gnome, which means that every KDE application, and sometimes even with the KDE GUI, things are done their own way. It makes for a very disjointed experience when UI/UX patterns don't transfer between applications. Its why I always end up switching back to Gnome, despite deeply disliking the flipside of the Gnome team's attitude. For example, it is beyond me why they haven't integrated Dock-to-Dash, Tiling Assistant and Night Theme Switcher. Especially Dash-to-Dock is so vastly popular that I reckon there's more people running Gnome with rather than without. reply zymhan 17 hours agorootparentThat the GNOME team is notorious for regularly breaking their ABI compatibility between upgrades _is_ the issue. reply HKH2 17 hours agorootparentWhy do they do that? reply butterfly42069 18 hours agorootparentprevWhen was the last time you used KDE? Cause this time around that hasn't been my experience at all, it's gnome that's currently really disjointed with random hamburger menus and whitespace everywhere. Current KDE feels like the most well put together DE I've ever used, and its really efficient once I get my custom keybinds in there. reply jorvi 17 hours agorootparent> When was the last time you used KDE? Half a year ago, thereabouts? And no, Gnome is not inconsistent. I just opened a bunch of applications, and they either have a hamburger menu on the top left or top right, mostly with the same options list and \"About\" at the bottom of the list. There is some slight visual difference between GTK4 applications and GTK3 applications that are yet to have a rewrite, but it is very consistent. Which does comes with the aforementioned problem of the Gnome devs being very \"my way or the highway\". In a strange way KDE reminds me of Windows, where the application devs always seem to be using 3-4 different frameworks, 3-4 different installers, and none of them try to get more than broad consistency between eachother. reply butterfly42069 7 hours agorootparentI did say in my experience, whereas you seem to be speaking in absolutes. Either way I disagree with you. I think we have differing opinions of what good actually is, and gnome just isn't good anymore to me. Best of luck to you though. reply cassepipe 19 hours agorootparentprevI personally started with Mint and that's what I would recommend. It's \"straightforward, consistent, no-nonsense\" and works out of the box. Cinnamon is less configurable (though it is) than KDE maybe but more focused. The fact that is Ubuntu makes troubleshooting so much easier. reply xcv123 19 hours agorootparentprevWe have Kubuntu for that EDIT: for stupid downvoters - Kubuntu is an official Ubuntu variant. It is officially part of the Ubuntu project. https://kubuntu.org reply throwdotnet 16 hours agorootparentI prefer kubntu too, I'm currently on 22.04 LTS or 24.04 LTS depending on the machine. I don't like the fact that you can't surface the menu with standard cua keyboard shortcuts in dolphin, e.g. alt-v for view. As someone previously using windows this is a step backwards for efficiency. reply xcv123 16 hours agorootparentDepends on the individual. I don't use those shortcuts on Windows either. I almost never need the menus. I set the view settings globally, applied to all folders. There are keyboard shortcuts for actions within those menus, like cut/copy/paste. The shortcuts are more configurable than on Windows. F4 opens a terminal in the current directory That's all I use 99.99% of the time reply bachmeier 17 hours agoparentprevTo a large extent, Ubuntu is the reason there is a Linux community. They did a lot of things right - mailing free installation CDs, having a guide for new users, having a distro that made installation easier on more hardware than any other - but the one revolutionary change was to kill RTFM. Prior to Ubuntu, Linux was a tool for social misfits to get revenge on everyone else for getting stuffed in lockers in high school. Eric Raymond and his merry band of followers did way more than Microsoft to slow Linux adoption. Ubuntu put an end to that. reply musicale 20 hours agoparentprevAlso on the down side, Ubuntu seems to have lost the plot a bit by adding things like advertisements for Ubuntu Pro. I would like to submit PRs to remove these ads, but of course they would never be accepted. We've seen where the road of advertisements eventually leads (Windows 11), and it isn't good. Adding extra garbage into CLI sessions is something that I greatly dislike because 1) it adds distracting noise and 2) it can break scripts. Some non-Canonical offenders include GNU parallel and Apple's cc and c++. I don't like how vim includes political messages either, even for causes that I might otherwise support, simply because it is distracting when I want to concentrate on getting work done. Tools should focus on the task at hand and avoid promotional messaging. reply benoau 20 hours agoparentprevIn addition to the direct impact Ubuntu had it also sits at the core of so many other popular distros which de facto standardized setting up and configuring so much consumer Linux software. reply tastysandwich 20 hours agoprevSeems like everyone loves to hate Ubuntu lately. From the Amazon search icon years ago, to Snaps, Mir, \"pro\" updates (which I don't get the backlash about..). But man, I started using this distro 18 years ago? And I still use it today. I can tell you, it's gotten more usable, more stable, and easier to install, without (imo) sacrificing any of what we love about Linux systems. If you hate snaps you can just remove them. It's an OS I can easily recommend to beginners who want to dip their toes in the Linux world. They can install it without any help. And I get that so much is a testament to the software Ubuntu uses getting better. But it brings it all together in such a great way. I used Arch Linux for a few years. But I didn't really like having to check message boards for any breaking changes before updating lest my system become unusable... As a busy professional and dad, I don't see myself switching off of Ubuntu anytime soon. reply bigstrat2003 18 hours agoparent> If you hate snaps you can just remove them. You kinda can't, and that's the point people are angry about. I never personally cared one way or the other about snaps. But it is not at all acceptable that Ubuntu will sometimes install a snap when I explicitly use apt to install it. That was the moment I decided I'm not gonna use Ubuntu any more: they started to override my decisions about what to install on my computer, and that isn't ok. reply kiwijamo 20 hours agoparentprev> If you hate snaps you can just remove them. The last time I tried Ubuntu it would automatically install the snap version if I tried to `apt install` a package. Is this behavior easy to disable? Do they even ship apt packages of stuff they use snap for? reply proactivesvcs 19 hours agorootparentapt \"pinning\" is the process you're looking for. This allows you to prevent reinstallation of snapd and prefer other sources for packages, e.g. Firefox from Mozilla's PPA. reply kiwijamo 19 hours agorootparentSo Ubuntu doesn't provide apt packages so if I want to use apt I have to muck around with PPAs? I don't want to have to add a PPA for every software I install. This is why Debian is better for my use case -- all I have to do is apt install and boom I have the Debian package installed. All this works out of the box on a Debian installation. reply bachmeier 17 hours agorootparentprevSorry, but this is not true. It's claimed you can use pinning, but at least on my installation, it didn't work no matter what documentation I followed. There was literally no way to stop it from silently switching to the Firefox snap. Well, installing Linux Mint fixed it, but that's not really the same thing. reply theamk 18 hours agorootparentprevnope, for major things like Firefox, it's either snaps or third-party repos. reply dax_ 4 hours agoparentprevI'm overall very positive on Ubuntu, but snaps was a big misstep in my opinion. When snaps were rolled out, lots of features were no longer working in apps packaged as snaps, or it was at least confusing to users (like file picker suddenly defaulting to some isolated path). For an operating system that always had a big focus on good user experience, this was really mismanaged and prematurely rolled out. And then they decided to force that bad UX on people by pointing apt packages to snaps suddenly, taking away the users choice to not use snap. The Amazon search lens was also a mistake, but at least it was easy for \"regular\" users to disable it. About Mir: so long as everything works, regular users wouldn't even notice, which is fine. I don't like the fragmentation in the Linux landscape, but oh well. reply zamadatix 20 hours agoparentprevUbuntu was ahead of its time in usability but things like \"They can install it without any help\" aren't particularly unique or compelling claims to make 20 years later. In some ways it has went backwards in usability. As an example, you mention snaps: say a user comes to find they don't like dealing with sandboxed apps being delivered via a separate update and package system... can they really \"just remove them?\". E.g. try removing snapd and Firefox then installing Firefox... it's no longer in the repositories and you're now in the realm of adding custom PPAs just to have a contiguous package system for default apps. By this point users have said \"fuck it\" and moved on. If you've already got something going there's probably not enough reason to bother switching things up. If you're doing it as a new user... why not compare to Debian or another Debian-based rather than something radically different in type and focus as Arch? reply autoexec 18 hours agoparentprevUbuntu fully earned a lot of that hate, but I can't deny that it's done a lot of good. It's had a lot of influence on getting people into linux and on other distros. I've still got some early Ubuntu CDs sitting around somewhere. reply nubinetwork 9 hours agoparentprev> \"pro\" updates (which I don't get the backlash about.) Waving a security update in my face on the premise that I'll pay them more is rather unacceptable. That said, I wish Ubuntu all the best... in the meantime, I'm considering switching to debian for my cloud stuff, hopefully Trixie comes out in decent shape next year... reply m463 19 hours agoparentprev> If you hate snaps you can just remove them. lol. ubuntu makes it really hard for you to regain control of their system. https://askubuntu.com/questions/1035915/how-to-remove-snap-f... https://askubuntu.com/questions/1322292/how-do-i-turn-off-au... https://askubuntu.com/questions/676374/how-to-disable-welcom... https://askubuntu.com/questions/1434512/how-to-get-rid-of-ub... reply jakebasile 19 hours agorootparentIf you don't like Snap, use a distro that doesn't use Snap. If you pick a Snap distro it's just making your life harder for the sake of being difficult. This applies to many things in Linux world in addition to Snap: - systemd/sysvinit - Wayland/X - apt/rpm - Musl/glibc reply pbhjpbhj 18 hours agorootparentYes, but when you just update as you've done many times and then several of the main apps don't work, then it's not a case of 'just pick a non-snap distro' as you already did that. Canonical have been aggressive in changes to snap only, reversing changes that would normally prevent their install. Now they've gone with 'pro', again using the installation system against their users. When you've used the same distro (Kubuntu for me) for many years, the forced choosing of a different distro with the need to migrate, ... it's like you're back with Microsoft again. I'm still unpicking the situation because moving ones family to a new OS is hard work (for me). reply jakebasile 18 hours agorootparentUbuntu was never a non-Snap distro. It was always a distribution provided by Canonical wherein they make decisions for you based on their own criteria. Snap became one of those decisions. I can't help but think of \"Open Source is Not About You\" by Rich Hickey. If Canonical is, as you say, using Ubuntu _against_ users (and not, in what is much more likely, simply making a decision that you disagree with) why would you want to continue using it? They have root on all your systems via apt. Personally, I can't wait to try the all-Snap flavor whenever it's ready. I'm curious if it will work for my usage (gaming in particular). reply unpopularopp 20 hours agoparentprevbtw what happened to the fully snap-based atomic version aka Ubuntu Core Desktop? Last news was from February that it got delayed but no news ever since reply singhrac 20 hours agoprevOnce long ago our family Windows computer had a motherboard failure and I convinced my parents to buy just a replacement motherboard in the right shape (maybe processor as well, now that I think about it), and we tried to fix it. While the data on the disk was recovered, we couldn’t get Windows to recognize this as the “same device” so we had to buy a new license. Instead I installed Ubuntu and casually taught my mom how to use it - I figured it was worth a try before buying a license. She had no problems whatsoever using it for all of her work and barely noticed the change, and it brought new life to a computer that would have almost certainly ended up in a landfill 7 years early. Given the hard work of the Wine/Proton developers (and many, many others) I can only imagine the situation is even better now. reply Happily2020 10 hours agoparentI think many \"average\" computer users spend almost all of their time inside a web browser. I think that apart from MS Office apps (which don't have a good enough alternative, especially for Excel), most of the other apps that people use are already available on linux, either natively, through electron, or as a web app. A beginner friendly distro like Ubuntu (especially the LTS version) can be ideal for a lot of people. Reducing the bloatware overhead that Windows brings, increasing performance and battery life, adding privacy, and reducing the likelihood of malware. reply triyambakam 16 hours agoparentprevAs a teen I installed dd-wrt on our router. Visiting home about ten years later after college I opened up the gateway address to a nice reminder seeing it was still there. My parents had no need or understanding to change it. reply sundarurfriend 20 hours agoprevUbuntu did a lot for the ease-of-access of Linux, and was the gateway for a lot of us. In the mid-2000s, getting professionally packaged CDs, one for Live Ubuntu and one for installation, with clear instructions, and having that load into an easy to use (relatively, for the time) OS installer, made the prospect of trying out Linux so much easier and more appealing, compared to downloading it on a torrent or trying to find a friend to copy it from. That was right away a positive first impression of Linux, and probably played a part in me continuing to try it out despite many kernel panics in the early days, and then eventually moving to Linux entirely. And along the way, I've been able to help out many others get into Linux and have an easier time of it as well. None of this might have happened if not for Ubuntu's attitude of actively reaching out to help new users, and the end products of that. reply wejick 19 hours agoprevI ordered many CDs of Ubuntu from 2005 until they don't send it anymore. For a middle schooler, getting a package that cool from abroad is something to be proud of. Thanks Ubuntu! reply gessha 18 hours agoparentAyy, I did the same in high school in the 2000s. It was most if not all the mail I got back then and it felt special. reply cph123 5 hours agorootparentYes I remember doing the same. I'd probably be about 12 years old, and my dad asked \"What is this Ubuntu thing you got in the mail?\", even now he asks me about Ubuntu. reply downrightmike 17 hours agoparentprevMy first one was the Red package, then the orange reply nikon 16 hours agoprevI just Googled, Warty Warthog was released October 2004. I ordered a free CD of this to my house, as a teen, and installed it — wiping Windows — without a second thought. What followed, after feverishly owning up to breaking the machine (the DSL modem only worked with Windows), was truly the start of my future in technology, computers and software. Ubuntu has stayed constant with me as many, many things have changed over these two decades. reply willmeyers 20 hours agoprevMaybe ten something years ago my dad got a free Ubuntu boot disk somewhere. Curious how he got it because he doesn't work with computers at all lol, but he's a personable person and he meets a lot of people. Knowing I was into computers and programming he gave me the disk. He told me how he loved what \"ubuntu\" meant: \"I am because we are\". Ubuntu was my first distro and I am grateful for that. reply harel 18 hours agoprevThis makes me feel my age... I ordered and got that free CD, and \"tested\" it on my windows XP laptop, with a dual boot, because this is just a test, right? I never booted XP again on that laptop, or any other since. For the past 3-4 years I've been planning to get off Ubuntu and move to Arch, but I keep postponing because everything just works, and I can't risk downtime due to something not working as I'm used to. reply aabhay 18 hours agoparentLucky you that everything just works. I’ve never had a seamless post install experience with Ubuntu. There’s always some weirdness, some display oddity, some wifi quirk, some sleep/hibernate thing, right click on the trackpad, or installing packages is broken. As much as I love and respect Ubuntu, I just can’t imagine using it for personal work. And yes, I also installed the first version via CD in 2005, way back when, on my XP machine. reply harel 17 hours agorootparentexcept \"that\" laptop which was a Toshiba, all the rest of them were Thinkpads. Maybe that's it? My requirements for \"just works\" change over the years, and maybe at times I was a bit lax (I don't really need a fingerprint reader) but yeah everything so far just worked (including that print reader in recent years). I still plan to move to Arch because 20 years is a long time and change is inevitable, but I need some free time first... reply thetyikergg 20 hours agoprevCongrats Ubuntu! Ubuntu was a big factor in me getting into Linux as a kid in India. In an era where the only internet we had was expensive dial-up, Ubuntu shipping CDs all the way here was such a nice act of kindness! I was even able to evangelize people around me into Linux, since I was able to give away CDs. reply StableAlkyne 20 hours agoparentSeconding this, I ordered a free 9.04 live CD as a kid and got started by putting it on my personal computer, which had a broken windows install that I wasn't knowledgeable enough to fix (and couldn't afford to reinstall) The kindness they gave by shipping that CD inspired a lifelong interest in computers in me. reply vegabook 20 hours agoprevNixOS really makes Ubuntu (and all the other distros) feel old though. I mean I _love_ Ubuntu, and I’ve used it faithfully for 12 years, but once you get used to Nix, and granted, it’s tough, but it’s just an absolute revelation in terms of confidence in one’s operating system, freedom to use so much more software, and not be worried about even very advanced configs. I could never go back. reply chaychoong 17 hours agoparentI've always thought that NixOS is a new distro because of the recent hype, but apparently it is older (by about a year) than Ubuntu! reply unpopularopp 20 hours agoprevI still have my official 4.10 Live CD. As a total oblivious person about the Linux world back then that was one of the biggest technology epiphany for me. Such a small thing yet it was just so mind blowing to run a full functioning OS from a disk that works everywhere (almost) in a \"non-destructive\" way. I've felt like a king to have my own OS with my own stuff on the locked down school computers. It was like finally I could do everything and it was thanks to Ubuntu (even though I know since that Ubuntu wasn't the first Live distro at all) reply jakebasile 19 hours agoprevUbuntu has been my favorite distro since I first tried it from one of the CDs ages ago. It was probably 5.04 or 6.06. I've used it off and on since then on desktops and servers. Since the middle of 2022 I started using it in place of Windows for PC gaming and it's been (mostly) a dream come true - I can finally use a reasonable OS to run my games. It's sad to see so many Linux people try to tear Ubuntu/Canonical down over minor technical complaints (snaps are fine, Unity was good) since they've done so much for Desktop Linux in particular. I hope the distro will keep going strong for another 20 years as I have no desire to switch to another. reply jillesvangurp 16 hours agoprevI've used Ubuntu on servers for probably the last twelve or so years. Mostly the reason is that it's the path of the least resistance. Most software that has installation documentation for Linux will document Ubuntu first. Maybe Red Hat and then you are on your own. So, getting stuff running on it tends to be well documented, well supported, etc. For personal use, I've moved away from it. I've had a Manjaro laptop for a few years now for some light gaming on Steam, which works great. I picked an Arch based distro because that's what Steam uses for the Steam Deck. IMHO, Ubuntu should move to rolling releases if they want to stay relevant. It doesn't make sense to run years out of date kernels and software packages these days. Especially if you want to run e.g. games and benefit from driver upgrades. And the bi-annual upgrade cycle just creates a lot of hassle for users. I know lots of Ubuntu users that routinely wipe their laptop because it's just easier than upgrading. Not a thing with Manjaro. I installed it nearly three years ago and it's fully up to date. And there seems to be a steady flow of kernel work that has gradually improved support for the hardware I have. I wouldn't want to miss out on that. Now that Arch has a more usable installer, I might move over to that but I'm not in a hurry. Three years ago installing Arch was a 50 step process that was a bit challenging as it involved fiddling with boot loaders and what not just to get it to boot to a cli with a working network connection (which requires the right kernel modules to be there). Manjaro has a nice live CD that sort of makes that a whole lot easier. That sort of makes it the Ubuntu equivalent for Arch, I guess. reply mattbillenstein 15 hours agoprevI've been using LTS releases for just about everything since 10.04. It's been very stable; the transition through all the init systems wasn't that fun (sysv, upstart, systemd), but it's mostly just always worked. And it's sorta the default for 3rd party software to support first. Congrats to the team. reply major505 16 hours agoprevI remember getting their free cds when I was in college. At the time I was a user of Conectiva Linux with widowmaker Desktop. Conectiva would later fuse with a french company and became Mandriva. After sufering a couple of years in slack, I tried ubuntu for the first time and was my go to distro until I foun Fedora. And them I never really leave Fedora, but I still try new Ubuntu and arch from times to times. I do ernjoy how easy they make everything, and while I understand that people dont like Snaps, they do work well enough for the use I do of a linux desktop. I also run Ubuntu Server on my home server. reply anacrolix 8 hours agoprevCanonical have been going downhill since 2012-2014 From 2004 to 2008 they were game changing. Their hiring process absolutely sucks. Crap hiring processes means you hire people that don't have other options. reply pentagrama 15 hours agoprevInstalled Ubuntu LTS on some family members PCs who use it for web browsing and Libre Office, and they are so happy, they got no issues, and I'm chill because I know they will not get OS ads, auto installed apps, or crap like that. Thanks Canonical. What a solid and user friendly OS. Will have to go to their homes to change Ublock Origin for the lite version since they use Chrome tho. I think the developer should choose to update the V2 with the V3 on the Chrome Store. reply dgfitz 20 hours agoprevApply for a job at canonical and get back to me. reply greyadept 19 hours agoparentI have to agree here. My experience with Canonical's job application process has significantly lowered my opinion of them. reply zahlman 16 hours agoparentprevA few people here seem rather down on this process, but without giving any detail. What in particular is unpleasant about such job applications? reply greyadept 12 hours agorootparentThis discussion is a good place to start: https://news.ycombinator.com/item?id=39750181 reply oaththrowaway 18 hours agoparentprevI interviewed with them maybe 8 years ago and it seemed like a pretty typical interview process. Has that changed in the years since? reply zerr 19 hours agoparentprevOne of the reasons I keep away from Ubuntu - only desperate people might be willing to work at Canonical. reply rldjbpin 10 hours agoprevfair to say it has been worked on much longer than 20 years, but it is a good time to recognize its role in getting people on to the linux ecosystem. despite running off of a cd and memory, a live ubuntu 10 experience felt faster on my p4 old box than the windows xp installed on a scsi drive. despite the challenges with certain hardware support (broadcom wifi on 12.04; nvidia hybrid mode on laptops especially around 14.04 era), it was a relatively safe environment to learn about the linux internals at my own pace. from academia to work, i continue to see it being a default choice for linux environments, and for a good reason. happy birthday! reply phendrenad2 16 hours agoprevUbuntu was a godsend when it came out. Linux \"just worked\" on most hardware out of the box (well, I never did get my wifi driver working, but it was amazing that everything ELSE worked). Since then, Ubuntu's role in the Linux community has changed. I don't think anyone would seriously recommend Ubuntu to a newbie, when there are so many better options. (And that's okay, things are allowed to change). reply Coolbeanstoo 20 hours agoprevUbuntu was my first experience with linux and my first introduction to free software from around 2011, when I was 13. It changed the game for me, I ended up succeeding in my career so far quite significantly from the Ubuntu introduction to computing I do wish the canonical hiring process was less goofy, as I think I'm qualified for roles there but I ended the process early years ago and now I don't seem to be under consideration since reply evanjrowley 19 hours agoprevUbuntu 8.04 was my first daily driver. Got it from an issue of Linux Magazine, back when they used to include live CDs. It kept my laptop from becoming a brick after the Windows Vista installation corrupted itself during a long vacation. Everything just worked! reply atum47 20 hours agoprevMy first Linux distro. I think Ubuntu helped a lot of us, Windows users, to migrate to Linux. reply CarVac 20 hours agoprevI went Ubuntu in 2006 and haven't looked back. Hopefully it'll be around for many more years. reply shriphani 20 hours agoprevI still have all the CDs from the shipit program - such a great project. reply inportb 16 hours agoparentCame here to say the same. The shipit program relieved a major pain point that prevented me from getting into Linux at the time: dial-up networking. It was a clever idea :) reply byhbwshdh 13 hours agoprevI owe Canonical a lot. I don't remember exactly if I had a metered connection when they started sending out CDs, but even if we already upgraded to an unmetered connection, downloading a whole CD would not have been practical. Was one of the first times to try a usable Linux installation reply umvi 16 hours agoprevMy first distro. Not currently using it, but I'll always recommend it for new Linux users as a gateway distro reply cassepipe 19 hours agoprevWe've had our differences but I am glad you exist. You are a net positive in the world. Cumple años feliz. reply julianeon 18 hours agoprevI use Ubuntu myself. Happy birthday to this great distro: you’ve done a lot of good in the world. reply omani 20 hours agoprevhappy birthday ubuntu. a distro I liked in the past and now try to avoid whenever possible. reply dallas 17 hours agoprevA happy user since 5.04! A happy Debian user prior to that. reply meiraleal 20 hours agoprevUbuntu was really revolutionary and helped me bring hundreds or maybe thousands of people to Linux years ago. I wish they would still have the power to push things further and do the same to mobile. reply bitwize 5 hours agoprevI dislike Ubuntu, but I'm glad it exists. It gives more normie-adjacent people an alternative to frickin' Windows. And it gives the \"year of the Linux desktop\" crowd a project to base their efforts on leaving me in peace with my command-line-heavy, systemd-free, GNOME-free distros. reply musicale 20 hours agoprevUbuntu seems to have lost the plot a bit adding things like advertisements for Ubuntu Pro. This kind of nonsense is one of the things that people hate about Windows. Also adding extra garbage into CLI sessions is something that I greatly dislike because 1) it's very irritating and adds unnecessary noise and 2) it can break scripts. Some non-Canonical offenders include GNU parallel and Apple's xcode CLI tools. I don't like how vim includes political messages either, even for causes that I might otherwise support, simply because it is distracting when I want to concentrate on getting work done. reply zahlman 16 hours agoparent>I don't like how vim includes political messages either Aside from the part where you can explicitly `:help uganda`, I can't fathom what you might be referring to. reply sgarland 19 hours agoparentprevGNU parallel asks you to acknowledge its license once, IIRC? Doesn’t seem like that big of an ask. reply theamk 17 hours agorootparentonce per user per machine, which means it's basically unusable in scripts you share ir use in CI. I avoid it for that reason. For simple cases, I use xargs; and for complex, small (stdlib-only) python scripts - they are a bit more verbose but infinitely easier to debug and improve. reply kiwijamo 20 hours agoparentprevAm surprised you're being downvoted. What you've described is in a nutshell why I don't use Ubuntu -- and why I stick with vanilla Debian instead. I left Windows to get away from certain things such as irritating advertisements and the like. reply kwar13 20 hours agoprevFor all the hate it gets, I for one love Ubuntu. I have been an Ubuntu user since 16.04, and it's the distro that taught me the most about Linux. Thank you Canonical. You guys are amazing! reply bearjaws 19 hours agoprevI remember using my schools high speed internet to download 4.04 since all I had was dial up. Went home and put it on my old crappy laptop dual boot, had barely any drive space left since it was like 60gb drive at the time lmao. I am always thankful I got into Linux since it made learning programming so much easier, and my current career involves Linux every day... reply blackeyeblitzar 13 hours agoprevI like Ubuntu but I feel like Linux for the everyday user is still far away. Like on laptops battery life and sleep behavior is still not great. Does Canonical maintain a list of things they want to fix that would make it a true viable OS for users who aren’t enthusiasts? reply rootsudo 16 hours agoprevCan’t believe it. I remember the live cds and install cds that were free to order. Brought life back to an old g3 iMac and it taught me much about grub bootloader to dual boot. Was also an early teen too. 20 years later and I knew I’ll be in tech. I’m 50/50 about that. My first distributor was either mandrake or suse. But this was the one I used the most. The other exciting thing at the time was knoppix. reply westurner 18 hours agoprevUbuntu! SchoolTool also started 20 years ago. Then Open edX, and it looks like e.g. Canvas for LMS these days. To run an Ubuntu container with podman: apt-get install -y podman distrobox podman run --rm --it docker.io/ubuntu:24.04 bash --login podman run --rm --it docker.io/ubuntu:24.10 bash --login GitHub Codespaces are Ubuntu devcontainers (codespaces-linux) Windows Subsystem for Linux (WSL) installs Ubuntu (otherwise there's GitBash and podman-desktop for over there) The jupyter/docker-stacks containers build FROM Ubuntu:24.04 LTS: https://github.com/jupyter/docker-stacks/blob/main/images/do... : docker run -p 10000:8888 quay.io/jupyter/scipy-notebook:2024-10-07 reply type0 20 hours agoprev [–] Remember how it was called \"linux for human beings\". Now, not so much, I tried it recently and it's clear that desktop users aren't their current audience any-more. reply nacs 20 hours agoparentI've been using it exclusively as my main OS for years with no issues. All hardware supported (better than Windows), fast and works out of the box. reply askonomm 20 hours agoparentprev [–] It's my favorite Linux distro precisely because it is very easy to use and install. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ubuntu 4.10 \"The Warty Warthog Release\" marks the debut of a new Linux distribution, offering a combination of Debian's extensive features with a user-friendly installation process and regular updates.",
      "This release supports x86, amd64, and ppc processors, and includes key features such as GNOME 2.8, Firefox 0.9, Evolution 2.0, and OpenOffice.org 1.1.2, along with enhanced hardware support.",
      "Ubuntu emphasizes its commitment to free software, providing the distribution at no cost, with options to download or request a free CD, and encourages community involvement for future releases."
    ],
    "commentSummary": [
      "Ubuntu marks its 20th anniversary, highlighting its significant role in making Linux more accessible through a user-friendly installation process.",
      "Despite some criticisms regarding recent changes like snaps (a software packaging and deployment system) and Ubuntu Pro ads, Ubuntu is still valued for its contribution to popularizing Linux.",
      "The early distribution of free CDs by Ubuntu played a crucial role in allowing users with limited internet access to explore Linux, cementing its status as a recommended choice for new users."
    ],
    "points": 235,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1729460648
  },
  {
    "id": 41903864,
    "title": "Epublifier – scrape pages (books, manuals) for offline reading",
    "originLink": "https://github.com/maoserr/epublifier",
    "originBody": "Epublifier Converts websites into epub. A tool that allows you to extract a list of html pages from a website and compile them into an ePub book to be imported into your eReader of choice. For advanced users who can write javascript, you can add additional parser definition to customize parsing of any site. Check out the wiki for usage. Currently supporting following sites: Novel Update Wuxia World Most sites from awesome-read-the-docs Custom sites with UL/OL elements as table of content, or regex on Link text, or use query selector Custom web app with predefined Title (header) element and Next button (clickable) Installation Firefox: https://addons.mozilla.org/en-US/firefox/addon/epublifier/ Chrome: https://chrome.google.com/webstore/detail/epublifier/eopjnahefjhnhfanplcjpbbdkpbagikk Example Usage Extracting list of pages Tranversing Webapp through next button Extracting other documentation Building Build Environment Windows 10 NPM version 8.1.2 Build Steps Install NPM Run npm install in base directory Run npm run build_ff for Firefox Run npm run build for Chrome CI/CD Acknowledgements jEpub",
    "commentLink": "https://news.ycombinator.com/item?id=41903864",
    "commentBody": "Epublifier – scrape pages (books, manuals) for offline reading (github.com/maoserr)187 points by maoserr 5 hours agohidepastfavorite32 comments 3abiton 12 minutes agoThis is an amazing tool! Long gone are the days when I used to force cache many webpages for offline travels. reply kemayo 4 hours agoprevHaving written my own one of these, the interesting thing about this one is really the UI for iterating on extracting content from an arbitrary site. Having a full GUI for working through the extraction is much more flexible than the norm. reply ffsm8 3 hours agoprevHeh, I'm currently creating something very similar. A web scraper for blogs and mainly web novels etc and ePub parser that persists the data to database along with categories and tags, and a companion PWA for offline reading to track reading progress on various stories and let me keep multiple versions of the same story (web novels and published epub). reply stronglikedan 4 hours agoprevIf this can handle those sites where every section is behind an accordion that must be expanded (and especially where it collapses other sections when you expand one), then this is going to be awesome. reply maoserr 4 hours agoparentWorks on this site: https://docs.ray.io/en/latest/ for me. reply dotancohen 4 hours agoparentprevCan it remove popups for newsletters, or subscription, or logins, or cookies' notifications? Can it read pages that requires signing in? reply maoserr 4 hours agorootparentIt extract the main content using Readability by default (you can configure it with something else). Logins would depend on how you're parsing. It has two modes, it either browses to the page inside the window (for non-refreshing pages), or retrieves it in the background using fetch. reply vivzkestrel 1 hour agoprevGonna love running this on all the documentation heavy websites like AWS VueJS MDN w3schools realpython betterstack reply solarkraft 4 hours agoprevNeat! I once made a simple version of this concept that saves an epub file on the server‘s file system, which is then synced to my e-book reader: https://github.com/solarkraft/webpub The main ingredient is Postlight Parser, which gives a simplified „document“ view for a website. reply Mkengine 3 hours agoprevDoes it support http://fanfiction.net/ ? I never found an easy solution for that one. reply seridescent 1 hour agoparentyou can export epubs from https://fichub.net/ reply pasc1878 2 hours agoparentprevI use a calibre add-in https://www.mobileread.com/forums/showthread.php?t=259221 It sort of works ie some stories just work others just get the first page. reply maoserr 2 hours agoparentprevYou can import a csv of all the chapter links, looks like it's just incremental numbering in the url reply t-3 45 minutes agorootparentThe issue is most likely cloudflare blocking most the best scraping methods. If the site can be pulled down with eg. wget or curl without a bunch of options that you definitely aren't writing by hand, pandoc can just be used to directly make an epub. reply bloopernova 4 hours agoprevE-Reader makers, take note. This is a cool feature that should be built in or at least able to be used with an API to get content onto the Kindle/etc. Or even a \"send to Kindle\" email address that can accept URLs too. reply andai 4 hours agoparentI wonder if this would have a positive or negative effect on profits. On the one hand, they'd be adding a massive amount of free content to a platform where they make money because people pay to consume content. On the other hand, it might actually increase sales simply because I'd spend more time using it, which would presumably result in more book purchases too. (Also Kindle store is already full of $0 public domain stuff, so they already don't seem too bothered by that possibility.) reply joseda-hg 3 hours agorootparentHuh didn't know that, guess I never assummed they would bother with it, I'd think about a published work in kindle like a product page in amazon therefore doesn't make sense to have 0$ items Are they an amazon offer or do third parties take the time to set that up? reply 39896880 2 hours agoparentprevKobo has Pocket integration, is this substantially different? reply bryanrasmussen 3 hours agoparentprevYou have this with the Remarkable sort of - https://remarkable.com/blog/introducing-read-on-remarkable reply anthk 3 hours agoprevI had that, buf for terminal under Unix and for web pages, Gopher and Gemini. Offpunk: https://sr.ht/~lioploum/offpunk/ Instead of Epub, it get catched down into text files (Gopher), Gemini files (Gemini) and HTML+images (Web Pages). You can visit the hier from ~/.cache/offpunk or directly from Offpunk. With the \"tour\" function, forget about doomscrolling. You'll read all the articles in text mode sequentially until you finish down. reply dartharva 5 hours agoprevAwesome! reply B1FF_PSUVM 22 minutes agoparentIt's rather unfair to \"first commenters\", who got the article up from the pile and left a quick recommendation, to get downvoted by latecomers. (dartharva's comment was the only thing here when I first looked from the front page) reply stuxnet79 3 hours agoprevFor those interested in a simple to use command line tool that accomplishes the same I've had success with percollate - https://github.com/danburzo/percollate reply tra3 2 hours agoparentThis looks great!! I've long been looking for something that leverages readability (or similar). Edit: Tried it with Reuters and it looks like percolate requires javascript, etc. Back to using \"Print as PDF\" from the browser. reply Mkengine 3 hours agoparentprevDoes it support http://fanfiction.net/ ? I never found an easy solution for that one. reply mati365 5 hours agoprev [–] Is it legal? reply Tepix 3 hours agoparentIf you can read it on a website, why not on an ebook reader? If you start selling the resulting files, now that would be a copyright violation. German law has a right to create a \"Privatkopie\", i.e. a private copy. I guess this is similar to fair use in US law? reply reaperducer 5 hours agoparentprev [–] Depends on where you live. Where I am, it's perfectly legal. Before cell service was as widespread as it is today, there were programs that would scrape web pages into ePUBs so you could read them later on your Palm Pilot. I used it every day during my commute. And the best part was that they ended. No mind-numbing infinite scroll. When I switched to a \"smart\" phone (SonyEricsson m600c), I really missed it. reply latchkey 3 hours agorootparentDanger Hiptop had a proxy that reformatted websites for their built in browser. Mostly as a way to reduce data transfer amounts. https://medium.com/@chrisdesalvo/the-future-that-everyone-fo... reply thesuitonym 4 hours agorootparentprevI wouldn't want to go back, because having instant access to anything is pretty amazing, but I do miss those days of offline internet. reply richardlblair 4 hours agorootparentFully agree. I recently replaced my doomscrolling with a retro handheld and it really makes me happy. It also pushed me to pick up my ereader again. I spend enough time at a computer than I shouldn't really need a smartphone outside of 'I need to message ___' or 'I need to go ___' reply anthk 3 hours agorootparentprev [–] If you have a GNU/Linux/Mac/BSD machine with Python: https://sr.ht/~lioploum/offpunk/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Epublifier is a tool that converts websites into ePub books, making them accessible for eReaders by extracting HTML pages and compiling them into the ePub format.",
      "It allows advanced users to customize the parsing process using JavaScript and supports popular sites like Novel Update and Wuxia World, as well as custom sites with specific elements.",
      "The tool requires a Windows 10 environment and NPM 8.1.2 for building, and it acknowledges the use of jEpub in its development."
    ],
    "commentSummary": [
      "Epublifier is a tool designed for scraping web pages, such as books and manuals, to enable offline reading, praised for its user-friendly graphical user interface (GUI) that enhances content extraction flexibility.",
      "It is capable of handling complex websites and removing popups, offering modes for both browsing and fetching pages, which sets it apart from typical web scrapers.",
      "While legal concerns vary by location, private copies are generally permissible, and the tool is valued for improving e-reader functionality and offline reading experiences."
    ],
    "points": 187,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1729516688
  },
  {
    "id": 41898076,
    "title": "Create mind maps to learn new things using AI",
    "originLink": "https://github.com/aotakeda/learn-thing",
    "originBody": "Enter a topic and get a learning mind map generated by an LLM with links to learn more about each subtopic.You can use it with local models (through Ollama) or external models.If you have any feedback, please share it! Hope it&#x27;s usefulDemo: https:&#x2F;&#x2F;youtu.be&#x2F;Y-9He-tG3aM",
    "commentLink": "https://news.ycombinator.com/item?id=41898076",
    "commentBody": "[flagged] Create mind maps to learn new things using AI (github.com/aotakeda)169 points by arthurtakeda 23 hours agohidepastfavorite73 comments Enter a topic and get a learning mind map generated by an LLM with links to learn more about each subtopic. You can use it with local models (through Ollama) or external models. If you have any feedback, please share it! Hope it's useful Demo: https://youtu.be/Y-9He-tG3aM simonbarker87 9 hours agoNice example of using AI for something but (like most “mind map” tools) the output isn’t a mind map, it’s a spider diagram. The point of a mind map is to label the line and not the node. This helps the brain form a visual and spacial connection between ideas where the lines act as bridges to the next concept/idea. Not faulting the creator here, looks like a solid implementation of AI making spider diagrams, good job. 30 years of people misusing mind maps and no one reading the Tony Buzan book have brought us to this point though where no one actually knows what mind maps are or why they are so powerful. reply 7734128 9 hours agoparentIf most people are using a term \"wrong\", then they are using it correctly. reply QuantumGood 4 hours agorootparentLooosely speaking. reply cloudhead 6 hours agorootparentprevThank god the world doesn’t actually work this way.. reply lolinder 5 hours agorootparentThis is exactly how the world works. Language does what language wants to, pedants try to fight it, pedants eventually lose. To boldly split an infinitive continues to be correct in spite of it being \"wrong\". reply royaltjames 3 hours agorootparentSpecifically this is how American English works. From composition to pronunciation. It's a beautiful messKhan Academy used to have [...] but for some reason they removed it I'm quite saddened by all the stuff that's been removed from Khan Academy over the years. Most of the non-maths content has been removed. The knowledge graph has been removed, etc. I've stopped donating to them because every time I use it, the experience has gotten worse. reply sersi 7 hours agorootparentDo they ever explain why they removed this content? reply matly 11 hours agoparentprevYou could take a look at Obsidian [1], or similar knowledge management tools. There is certainly a lot of movement in the plugin ecosystem right now, for example the obsidian-canvas-llm-extender [2], which (likely) does what you're asking for. 1: https://obsidian.md/ 2: https://github.com/phasip/obsidian-canvas-llm-extender reply soco 8 hours agorootparentI always found Obsidian and whatever other tools a huge time investment by itself with more effort to use them than actually getting the managed knowledge. reply Tier3r 7 hours agorootparentSame for Anki, Notion, Zettelkatsen etc. Even ignoring the setup cost, it costs as much effort to insert in something as it does to retrieve it. The value prop tends to be low for individuals. reply artur_makly 19 hours agoparentprev@andai check out https://www.perplexity.ai/spaces its _kind of_ what you are describing.. it's UX is unstructured compared to a mind-map or timeline. But we are starting to see the nascent stages of where all this is going. exciting times indeed. reply hm-nah 17 hours agoparentprevSmells like a knowledge graph reply arthurtakeda 18 hours agoparentprevthat's a very interesting use case, could be the long-term vision for the project, thanks for sharing! reply pier25 14 hours agoprevThe point of making mind maps by hand is that they help you memorize and study by synthesizing a topic. If this is done by AI it's pretty much pointless. reply InkCanon 13 hours agoparentOne of the reasons I think LLMs should not be in most parts of education. There is a huge body of research about the importance of deliberate practice. High quality learning needs effort. A lot of these tools perform the ostensible ritual of learning - note taking, mind mapping, etc - and make them frictionless, thus rendering them worthless. reply crucialfelix 12 hours agorootparentExactly. If anything we should build LLM powered apps that ask questions, test understanding and create that friction necessary to build deep comprehension and memory. reply elric 11 hours agoparentprevAgreed. I wouldn't be opposed to a (local) LLM inside my Obsidian vault; but not for writing notes, but rather for discovering connections between notes. That's something that would actually help me. Generating mind maps like this? Not so much. reply knighthack 7 hours agoparentprevIt's not quite pointless. A prepared outline/mind-map gives you a roadmap for learning beforehand; it can function like a good index. It's better to memorize domain knowledge upon paths that are clearly understood (thus you are memorizing well-worn, acceptable paths), rather than synthesizing your understanding of a topic as you go along. The second is prone to mistakes and mistaken understanding, unless you're a subject matter expert or charting new domains of knowledge. reply Tier3r 7 hours agorootparentThat is IMO already done by domain experts in the form of textbooks, courses, university classes etc. And with formal, tested relationships between various islands of knowledge. I'm not too sure how much I would trust an LLM to do this reply pier25 2 hours agorootparentprevMaking the index is part of the learning. reply Tomte 5 hours agorootparentprevThat’s an advance organizer (again, Joseph Novak). But it needs to be created by an expert, not by an LLM. reply simonbarker87 9 hours agoparentprevCame here to say this. Also, most mind map implementations are actually spider diagrams which (while useful) are not mind maps, the point of a mind map is the label the line and not the node. reply andai 9 hours agorootparentHuh, I've been doing it wrong my whole life. reply simonbarker87 8 hours agorootparentUse Your Head by Tony Buzan is totally worth a read and is how I learned mind mapping 22 years ago. I can still remember the rough layout and shape of many of the mind maps I made for GCSE and A-level revision. I’m 100% certain that reading that book and sticking to his methodologies added one or two letter grades to my exams at that age. reply ainiriand 10 hours agoprevThat is really cool. I am very much into learning with the support of AI. Last month I started to learn Rust and I used this prompt to help me: 'I am an experienced software engineer who wants to learn Rust, design a learning path for 3 months, with daily goals. Try to have a good balance of learning and working exercises using available resources like Rustlings or Learn rust by example. Output this path in markdown and add space to gather my learnings in a note at the end of each week.' Maybe with some tweaks can be useful to someone else! reply aquariusDue 10 hours agoparentI'd also recommend the wonderful 100 Exercises to Learn Rust authored by Luca Palmieri who also wrote Zero To Production in Rust. I'm halfway through the exercises and I find them to complement Rustlings and The Book perfectly. So yeah, 100 Exercises to Learn Rust is what finally made traits (especially From and Into), impls and trait bounds finally click for me and I can't recommend it enough. reply ainiriand 10 hours agorootparentI did that! https://github.com/sh4ka/100-exercises-to-learn-rust reply chamomeal 7 hours agorootparentprevHow do they compare to “the book” (I forget the exact title). I tried to get through it a couple times, but I have no background in C and the lifetimes part confused the hell outta me. Combined with all the other new things, my interest just sorta fizzled. Love those enums, though!! reply airstrike 22 hours agoprevI'd say the README should have a pic of the results otherwise I have to install it and run it to see if I want to install it and run it Also why not host it online and let users bring their own keys? reply arthurtakeda 22 hours agoparentjust updated the readme with the video: https://www.youtube.com/watch?v=Y-9He-tG3aM I considered that but if I were the user I'd be wary of adding my own keys to a random person's website haha, but now that you mentioned that, since the code it's open-source I guess it's fine, thanks for the feedback! reply airstrike 21 hours agorootparentThanks for that! You can use something like gifski to turn that video into a gif so that you can embed it into the README. Here's an example from the gifski repo: https://github.com/ImageOptim/gifski You can use the CLI version but they also have executables with a dead simple GUI if you're so inclined. I have only ever used the GUI and it's perfect on a Mac (just drag and drop your video into it). Not sure if it's the exact same on Windows but I imagine it's amazing there too reply arthurtakeda 20 hours agorootparentNice! Will replace the screenshot with a gif, if that doesn’t work for me I guess ffmpeg may be able do that too, thanks! reply airstrike 15 hours agorootparentFYI your current demo is 25MB. Not sure if you used gifski but usually it's super helpful for making gifs smaller reply nosioptar 18 hours agorootparentprevFfmpeg can output a gif. The only difficult part might be figuring out which options you need to get the quality you want. reply cj 21 hours agorootparentprevThat’s cool! It would be great if you could easily expand each subtopic into further sub-subtopics. Was there anything particularly interesting about how you built it or the prompts needed to get decent results? reply arthurtakeda 21 hours agorootparentI noticed that, at least with the models I tested (gpt 3.5, 4o and llama 3.1 8b), to get a response with just the JSON and then have it follow the exact structure so it correctly renders the topic and subtopics was the hardest part. Ended up having to prompt I think twice (at the beginning and the end) so it finally followed the exact JSON structure. reply airstrike 21 hours agorootparentYou can use structured outputs to tell ChatGPT-4o to create specific JSON matching a schema: https://platform.openai.com/docs/guides/structured-outputs/i... It's a bit annoying because the schema has some limitations but it works with enough elbow grease reply arthurtakeda 20 hours agorootparentinteresting, didn’t know about that feature, thanks for sharing! reply graypegg 2 hours agoprevNeat! Though I feel like this needs to be \"integrated\" into something else to achieve full potential. I think a lot of people end up falling down wikipedia rabbit holes (me included!) and something where my browser can provide the same sort of \"topic quicklinks\" that wikipedia articles have [0] but for anywhere on the web? I'd love that! [0] https://en.wikipedia.org/wiki/Very_high_frequency#Electromag... (deep linked to the \"Electromagnetic Spectrum\" topic quick links, but you might have to click the accordion control to unfold it) reply andai 4 hours agoprevWhy the heck did this get flagged? Almost all the comments are positive. reply airstrike 1 hour agoparentThe worst part is we can't even vouch for it? reply InkCanon 2 hours agoparentprevWould also like to know this. reply sourcepluck 7 hours agoprevI wonder has the author of this project seen the discussions here on tacit knowledge (which a lot of comments here seem to almost be touching off) https://news.ycombinator.com/item?id=29531947 reply dr_dshiv 22 hours agoprevWould be great to have a video of it working so I can see what it does before installing. Thanks! Also, I’m generally interested in UIUX variations around LLMs. Hoping to see a round up of examples like this, at some point. reply arthurtakeda 22 hours agoparentjust uploaded a demo on youtube: https://www.youtube.com/watch?v=Y-9He-tG3aM thanks for checking out! reply wonger_ 18 hours agoparentprevThis reminds me of https://tree-of-knowledge.org/, posted a few months ago on HN. The branching/exploratory/canvas approach is better UX than a chat box. reply anon012012 6 hours agorootparent(TOK dev here.) Thanks for the linking. I'm sorry I could not keep it glorious (that is free and login-free). I got spammed by bots and I don't have time to discriminate so I made it BYOK and kept it at that. My idea deserves a better execution than this and so I don't mind if other people go at it. All the best. BTW, related to learning AI software, I'm working on something else... Calling it \"InfoRush AI\" for now, it looks like that: https://imgur.com/0tncarJ This is Python, BYOK, FOSS. Multicolumn synchronous browsing... orchestrated by the tree of knowledge. Going beyond and beyond. I'll post it on HN soon, (when it's done). reply null0pointer 17 hours agoprevI like this a lot. Great for autodidacts like myself. Often when entering a new topic I’m faced with many unknown unknowns. I don’t know _what_ I should be learning. So having an LLM effectively lay out a course of study would be very helpful. reply arthurtakeda 17 hours agoparentglad you liked it! hope it’s useful reply afro88 22 hours agoprevHow do you generate / validate the links to learn more? If they're generated by the LLM there's a really high chance they are hallucinated and won't work. reply downboots 19 hours agoparentHuman intervention would be necessary for any kind of reliable knowledge curation if that were the goal . https://en.wikipedia.org/wiki/World_Brain reply arthurtakeda 22 hours agoparentprevto be quite honest, I don't, just manually tested with different topics and got working links almost every time but agree, that can definitely happen reply afro88 22 hours agorootparentYou could have a second automatic step that searches the web for the title of the link and validates it or gets the correct one. Cool project! reply fao_ 9 hours agorootparentBut with the amount of AI generated content on the web, in a year or two (or maybe less) surely this method will fail dramatically? reply arthurtakeda 22 hours agorootparentprevgood idea, I'll add that to the improvements list, thanks! reply adfm 14 hours agoprevSimpleMind plays well with LLMs and OPML. https://simplemind.eu/blog/mapping-your-thoughts-with-chatgp... reply artur_makly 20 hours agoprevnice work! hmm...perhaps there could be some compounded synergies with my https://VisualFlows.io // also made with ReactFLoW. i will DM you.. reply pryelluw 19 hours agoparentCould you add a screenshot based demo or example to the main page? reply artur_makly 19 hours agorootparenti'm guessing you were referring to the OP's app? he just added the video demo. but if not, our main page IS the app ;-) reply hmottestad 22 hours agoprevDo you have any examples to look at? All I can see in the readme is a page with a search field and no mind maps. reply arthurtakeda 22 hours agoparentjust uploaded a demo on youtube: https://www.youtube.com/watch?v=Y-9He-tG3aM thanks for checking out! reply SuperHeavy256 17 hours agoprev [–] I love the idea but, I have no Open AI API credits reply airstrike 1 hour agoparent\"OpenAI API\" has sort of become a standard API for any models through other tools which expose identical end points regardless of the underlying model. You can use ollama for that reply arthurtakeda 17 hours agoparentprev [–] If you setup Ollama and download a local model, all you have to do is follow the readme instructions, let me know if you need any help! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A new AI tool allows users to create mind maps by entering a topic, generating a learning map with links for each subtopic.",
      "The tool can be used with both local and external models, and feedback from users is encouraged.",
      "While praised for its learning potential, some users suggest improvements, such as better link validation and integration with other platforms, noting that AI-generated maps may differ from traditional mind maps."
    ],
    "points": 169,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1729454479
  },
  {
    "id": 41899873,
    "title": "A step toward fully 3D-printed active electronics",
    "originLink": "https://news.mit.edu/2024/mit-team-takes-major-step-toward-fully-3d-printed-active-electronics-1015",
    "originBody": "By fabricating semiconductor-free logic gates, which can be used to perform computation, researchers hope to streamline the manufacture of electronics. Adam ZeweMIT News Publication Date: October 15, 2024 Press Inquiries Caption: The devices are made from thin, 3D-printed traces of the copper-doped polymer. They contain intersecting conductive regions that enable the researchers to regulate the resistance by controlling the voltage fed into the switch. Credits: Image: Courtesy of the researchers Active electronics — components that can control electrical signals — usually contain semiconductor devices that receive, store, and process information. These components, which must be made in a clean room, require advanced fabrication technology that is not widely available outside a few specialized manufacturing centers. During the Covid-19 pandemic, the lack of widespread semiconductor fabrication facilities was one cause of a worldwide electronics shortage, which drove up costs for consumers and had implications in everything from economic growth to national defense. The ability to 3D print an entire, active electronic device without the need for semiconductors could bring electronics fabrication to businesses, labs, and homes across the globe. While this idea is still far off, MIT researchers have taken an important step in that direction by demonstrating fully 3D-printed resettable fuses, which are key components of active electronics that usually require semiconductors. The researchers’ semiconductor-free devices, which they produced using standard 3D printing hardware and an inexpensive, biodegradable material, can perform the same switching functions as the semiconductor-based transistors used for processing operations in active electronics. Although still far from achieving the performance of semiconductor transistors, the 3D-printed devices could be used for basic control operations like regulating the speed of an electric motor. “This technology has real legs. While we cannot compete with silicon as a semiconductor, our idea is not to necessarily replace what is existing, but to push 3D printing technology into uncharted territory. In a nutshell, this is really about democratizing technology. This could allow anyone to create smart hardware far from traditional manufacturing centers,” says Luis Fernando Velásquez-García, a principal research scientist in MIT’s Microsystems Technology Laboratories (MTL) and senior author of a paper describing the devices, which appears in Virtual and Physical Prototyping. He is joined on the paper by lead author Jorge Cañada, an electrical engineering and computer science graduate student. An unexpected project Semiconductors, including silicon, are materials with electrical properties that can be tailored by adding certain impurities. A silicon device can have conductive and insulating regions, depending on how it is engineered. These properties make silicon ideal for producing transistors, which are a basic building block of modern electronics. However, the researchers didn’t set out to 3D-print semiconductor-free devices that could behave like silicon-based transistors. This project grew out of another in which they were fabricating magnetic coils using extrusion printing, a process where the printer melts filament and squirts material through a nozzle, fabricating an object layer-by-layer. They saw an interesting phenomenon in the material they were using, a polymer filament doped with copper nanoparticles. If they passed a large amount of electric current into the material, it would exhibit a huge spike in resistance but would return to its original level shortly after the current flow stopped. This property enables engineers to make transistors that can operate as switches, something that is typically only associated with silicon and other semiconductors. Transistors, which switch on and off to process binary data, are used to form logic gates which perform computation. “We saw that this was something that could help take 3D printing hardware to the next level. It offers a clear way to provide some degree of ‘smart’ to an electronic device,” Velásquez-García says. The researchers tried to replicate the same phenomenon with other 3D printing filaments, testing polymers doped with carbon, carbon nanotubes, and graphene. In the end, they could not find another printable material that could function as a resettable fuse. They hypothesize that the copper particles in the material spread out when it is heated by the electric current, which causes a spike in resistance that comes back down when the material cools and the copper particles move closer together. They also think the polymer base of the material changes from crystalline to amorphous when heated, then returns to crystalline when cooled down — a phenomenon known as the polymeric positive temperature coefficient. “For now, that is our best explanation, but that is not the full answer because that doesn’t explain why it only happened in this combination of materials. We need to do more research, but there is no doubt that this phenomenon is real,” he says. 3D-printing active electronics The team leveraged the phenomenon to print switches in a single step that could be used to form semiconductor-free logic gates. The devices are made from thin, 3D-printed traces of the copper-doped polymer. They contain intersecting conductive regions that enable the researchers to regulate the resistance by controlling the voltage fed into the switch. While the devices did not perform as well as silicon-based transistors, they could be used for simpler control and processing functions, such as turning a motor on and off. Their experiments showed that, even after 4,000 cycles of switching, the devices showed no signs of deterioration. But there are limits to how small the researchers can make the switches, based on the physics of extrusion printing and the properties of the material. They could print devices that were a few hundred microns, but transistors in state-of-the-art electronics are only few nanometers in diameter. “The reality is that there are many engineering situations that don’t require the best chips. At the end of the day, all you care about is whether your device can do the task. This technology is able to satisfy a constraint like that,” he says. However, unlike semiconductor fabrication, their technique uses a biodegradable material and the process uses less energy and produces less waste. The polymer filament could also be doped with other materials, like magnetic microparticles that could enable additional functionalities. In the future, the researchers want to use this technology to print fully functional electronics. They are striving to fabricate a working magnetic motor using only extrusion 3D printing. They also want to finetune the process so they could build more complex circuits and see how far they can push the performance of these devices. “This paper demonstrates that active electronic devices can be made using extruded polymeric conductive materials. This technology enables electronics to be built into 3D printed structures. An intriguing application is on-demand 3D printing of mechatronics on board spacecraft,” says Roger Howe, the William E. Ayer Professor of Engineering, Emeritus, at Stanford University, who was not involved with this work. This work is funded, in part, by Empiriko Corporation. Share this news article on: X Facebook LinkedIn Reddit Print Paper Paper: “Semiconductor-free, monolithically 3D-printed logic gates and resettable fuses” Related Links Luis Fernando Velásquez-García Microsystems Technology Laboratories Department of Electrical Engineering and Computer Science School of Engineering MIT Schwarzman College of Computing Related Topics Research 3-D printing Additive manufacturing Carbon materials Nanoscience and nanotechnology Electronics Materials science and engineering Microsystems Technology Laboratories Electrical engineering and computer science (EECS) School of Engineering MIT Schwarzman College of Computing Related Articles MIT engineers 3D print the electromagnets at the heart of many electronics Researchers 3D print components for a portable mass spectrometer Scientists 3D print self-heating microfluidic devices Researchers 3D print a miniature vacuum pump Previous item Next item",
    "commentLink": "https://news.ycombinator.com/item?id=41899873",
    "commentBody": "A step toward fully 3D-printed active electronics (news.mit.edu)168 points by gmays 17 hours agohidepastfavorite69 comments jayyhu 17 hours agoReading the article, it looks like so far they only have a working resettable fuse (a passive device), and only hypothesize that a transistor was possible with the copper-infused PLA filament. So no actual working active electronics. And from the paper linked in the article[1], it seems the actual breakthrough is the discovery that copper-infused PLA filament exhibits a PTC-effect, which is noteworthy, but definitely not \"3D-Printed Active Electronics\" newsworthy. [1] https://www.tandfonline.com/doi/full/10.1080/17452759.2024.2... reply jayyhu 10 hours agoparentI want to clarify that they actually did build a transistor-like device, and not just hypothesize about it. I missed section 3.2 when I initially skimmed the paper, which demonstrates and shows the results of a working “transistor”. Unfortunately I can’t edit my original post, so apologies for causing any confusion. reply netrap 4 hours agorootparentThis image shows logic gates they made: https://www.tandfonline.com/cms/asset/5043deae-e79e-45fc-bb7... reply IanCal 12 hours agoparentprevHang on, can you explain why this is passive and not active? > Harnessing the described phenomenon, we created the first semiconductor-free active electronic devices fully 3D printed via material extrusion. We demonstrate this breakthrough through the implementation of monolithically 3D-printed logic gates. reply magicalhippo 11 hours agorootparentThey've created a Polymeric Positive Temperature Coefficient (PPTC) device. As it heats up the resistance gets very high very abruptly. While it is non-linear, diodes are also considered passive devices[2], as active is taken to mean electrical control of current flow. In this case one could induce current control through thermal means, ie an adjacent heating element, and if you potted that in a box I guess you could argue the box is an active device. But not the PPTC itself. [1]: https://m.littelfuse.com/~/media/electronics/technical_paper... [2]: https://wiki.analog.com/university/courses/electronics/text/... reply adrian_b 8 hours agorootparentUnlike a device with positive temperature coefficient, the NTC thermistors (negative temperature coefficient) can be used by themselves as active devices that provide a negative resistance, which can be used to make amplifiers and oscillators, exactly like with any other diodes with negative resistance, e.g. tunnel diodes, IMPATT diodes, Gunn diodes, Shockley diodes, diacs and so on. Nevertheless, I do not think that anyone has ever made amplifiers or oscillators with thermistors, because unlike the diodes where the negative resistance has electrical causes, the inertia of the heat transfer in thermistors makes the achievable upper limit for the amplified frequencies very low, typically under 1 Hz. A device with positive temperature coefficient could be used as an amplifier or as a switch (like a relay) only together with a separate heater, as you say. reply notjulianjaynes 3 hours agorootparentI have seen old organs which used solid state VCOs that also had an incandescent lightbulb near the circuit boards to help maintain a stable temperature, and had thought they must use a thermistor although I seem to be mistaken as I can't find much information about that. I did find this however: https://northcoastsynthesis.com/news/temperature-compensatio... reply amelius 9 hours agorootparentprev> active is taken to mean electrical control of current flow Is a transformer an active device? Asking because current in one loop can control current in the other loop. From there, are two copper wires an active device? reply adrian_b 8 hours agorootparentThe current in one transformer loop does not control the current in the other loop. The power from one loop is transferred into the other, there is no control. The same for two copper wires. \"Control\" means that you can determine the value of the power in some circuit by consuming less power to do this. If you have to use the same power, not less, then you are the provider of power, not someone in control, i.e. this is the difference between bosses and the workers commanded by them. The bosses do not lift heavy parcels themselves, they order to some worker to do that. A device that apparently looks like a transformer but it is an active device is the magnetic amplifier. There are 2 differences from a transformer, the magnetic core is saturable during normal operation (any magnetic core is saturable at a high enough magnetic field, but when that happens in a transformer this means that the transformer has failed, which leads to overcurrents that would destroy the equipment unless a protection is triggered), and the second difference is that the control coil has a very high number of turns, so that a very small current can saturate the magnetic core. In a magnetic amplifier, the output coil is inserted in an AC circuit where the power must be controlled. When the core is not saturated, the impedance of the coil is high and the output AC current is low. When the core is saturated, the impedance of the coil is low and the output AC current is high. Whether the magnetic core is saturated or not is controlled with a very small current and power on the control coil, which makes this an active device. Magnetic amplifiers have been heavily used during WWII, especially by the Germans, who had improved them, and they continued to be used for a few decades after the end of WWII, when USA had captured the German technology, because of their very high reliability, until the transistor amplifiers have become reliable enough. reply amelius 5 hours agorootparent> The current in one transformer loop does not control the current in the other loop. You are right about the power, but the current in one loop __does__ control the current in the other loop. reply adrian_b 4 hours agorootparentYou use \"control\" in the wide sense of \"dependency\", i.e. if two quantities are constrained by an equation, you say that one quantity controls the other, only because their values are not independent (which means that fixing the value of anyone of the two quantities also determines the value of the other quantity). According to your usage, the voltage on a resistor is controlled by its current, because the voltage is proportional with the current (by the resistance of the resistor), and also the current is controlled by the voltage, because the current is proportional with the voltage (by the conductance of the resistor), exactly like in a transformer the input and output currents and voltages are bound by proportionality relationships. It is true that this meaning of \"control\" is encountered in speech, but in engineering and physics \"control\" has a precise meaning, more restricted that how you use it. In the engineering use of \"control\", it is always possible to distinguish which is the controller and which is the controlled in a control relationship. When \"control\" is used like you use it, the \"control\" relationship is bidirectional and you cannot say which is the controller and which is the controlled, e.g. between the primary loop and the secondary loop of the transformer, or between the current and the voltage through a resistor. For \"control\" in the engineering sense, unidirectionality is an essential property. Real control devices have some internal feedbacks that make them not completely unidirectional, but this is considered a defect and serious efforts are done to improve the unidirectionality of the control devices. A device with total feedback like a transformer cannot be used to implement any of the known control methods, i.e. you cannot make amplifiers or oscillators or logic gates with it. reply amelius 3 hours agorootparentBut when electrical power is used to drive a simple DC motor, then that power \"controls\" the speed of that motor. When the power is removed and the motor keeps turning (by e.g. a flywheel) then the power is delivered back to the input. So in that example there is bidirectionality, where you still \"control\" the speed of the motor. reply adrian_b 20 minutes agorootparentAs I have said, some people, including you, are using the word \"control\" in this wider sense, where it is synonymous with \"dependency\". Nevertheless, using \"control\" with this meaning in any engineering text would be a mistake, because there \"control\" must be used in its strict sense, to avoid confusions. In any system there are many dependency relationships, corresponding to all the equations that are applicable to that system, but much fewer control relationships. The control relationships are quite important for the understanding of the system, so they must be identified clearly in a distinct way from other dependencies. Etymologically, the right sense of \"control\" is the strict sense, because it has never been applied to a bidirectional relationship like that between the quantities connected by an equation, but it referred to a unidirectional relationship, between a dominant party, the controller, and a subordinate entity, the controlled, whose accounts were checked by the controller. reply qwery 8 hours agorootparentprevBut transformers don't do that. The electricity you put in one winding \"comes out\" on an/the other, transformed -- there isn't one current controlling another, there's just one current[0]. [0] very loosely speaking, also I am not a doctor reply IanCal 8 hours agorootparentprev> as active is taken to mean electrical control of current flow. Does the building of logic gates controlling a motor not show electrical control of current flow? reply bee_rider 12 hours agoparentprevWell, it is just “a step.” Whether or not it is newsworthy… eh, I mean, what is MIT News? A campus newspaper? I’m pretty sure we had articles on particularly big games of capture the flag in mine. reply notjulianjaynes 2 hours agorootparentWhat seems cool about this to me is that they seem to have done it with a plain old FDM printer and copper impregnated PLA. The devices are fairly large (mm scale) so presumably anyone with a $200 ender and the correct filament could print these. I am able to find copper PLA for sale too, although I'm not positive it is what was used in the experiment, and it's kind of pricey (~ $100/kg). reply jayyhu 11 hours agorootparentprevIt looks like the editors have amended the title of their article since this was initially posted. The original title was just “3D-printed Active Electronics” reply westurner 3 hours agoparentprevFrom https://news.ycombinator.com/item?id=40759133 : > In addition to nanolithography and nanoassembly, there is 3d printing with graphene. And conductive aerogels, and carbon nanotube production at scale From https://news.ycombinator.com/item?id=41210021 : > There's already conductive graphene 3d printing filament (and far less conductive graphene). Looks like 0.8ohm*cm may be the least resistive graphene filament available: https://www.google.com/search?q=graphene+3d+printer+filament... > Are there yet CNT or Twisted SWCNT Twisted Single-Walled Carbon Nanotube substitutes for copper wiring? Aren't there carbon nanotube superconducting cables? Instead of copper, there are plastic waveguides reply hatsunearu 13 hours agoparentprevusually even these academia hype pieces have some grain of utility but this one was so incomprehensibly bad that i was genuinely confused if i'm reading it incorrectly. what the hell? reply atoav 11 hours agoparentprevNot to be that guy, but this is a typical situation as it occurs a thousand times per week: 1. Scientists make minor progress as part of a multi-year effort, release a paper, paper features overly optimistic outlook to get future funding 2. Institute marketing department both hypes it up and dumbs it down a little 3. Popular science press picks it up and both hypes it up and dumbs it down a little more 4. Scientific literate readers read it and complain TL;DR: Nothing new under the sun reply greenavocado 13 hours agoparentprev> So no actual working active electronics. Oh so this is another scam like the MIT Food Computer. At this point I assume everything coming out of MIT is a scam until independently validated by disinterested third parties reply frognumber 8 hours agorootparentThis shouldn't be a downvote or flag. It's a serious problem, especially at elite institutions, and especially at MIT and Stanford. It's also not out-of-line with what credible sources observe: https://blogs.bmj.com/bmj/2021/07/05/time-to-assume-that-hea... I'm affiliated with MIT, and have been for the vast majority of my life, including at points in fairly senior roles. If you shut people out pointing problems, it will never get better. There's an incredible urge to defend elite academic institutions, but it's not in the interest of those institutions. Remember your civics class (patriots criticize their government institutions). The only way I see this fixed involves a period where MIT is viewed like a used car salesman in the public eye for at least enough years to cause enough pain to lead to reform. The endowment is big enough it'll do fine in the end. If it keeps sliding to fraud, it won't. reply LASR 11 hours agoprevI am a fan of 3D printing. And I think you can probably get some circuit traces 3D printed for some niche applications. But active electronics? That's a huge stretch. But more importantly, the economics just doesn't make sense. Components already cost fractions of a cent. Small-run PCB prototyping is likewould massively simplify iteration I seriously doubt it. It’s far easier and more effective (and economical) to have a bunch of jelly bean components around in stock. You’re going to have a hard time 3D printing anything that can be solderable (either the 3D printer needs to work at high temperatures for DIY, or you need exotic solder that melts at low temperatures). If you have the need to fabricate quick PCBs for prototyping, you’ll be better served by a cheap CNC machine and some copper foil blanks. The only real promise I see is that you might, in the very long future, be able to print custom multi-purpose devices, that integrate the characteristics of non critical electronics with mechanical elements, i.e. integrating NTCs on cases or fan supports,.. reply shultays 3 hours agorootparentprevI think it is valid to point of feasibility of something. For fast prototyping there are already breadboards or PCBs that you can just solder wires on, so it doesn't really help with tracing PCB lines. Printing transistors or other things that are good and compact enough to use, even for prototyping, seems to be indeed a stretch reply seanthemon 11 hours agoparentprevin places where you have a 3d printer but you don't have an active shipping line that can easily reach you. You can easily prototype things or build electronics. I also don't see this as the final result, printers could be purpose built for this to speed up production and make size smaller reply dsv3099i 2 hours agorootparentI suppose, but the 3D printer requires consumable inputs. So without active shipping that printer is going to have a very limited lifetime. There’s always a corner case, like having to 3D print on Mars or something, but thats a niche of a niche. reply reader9274 13 hours agoprevThis is like posting \"Landing on Mars\" and all you did was catch a reusable rocket. reply dools 13 hours agoparentI think the \"step towards being an inter planetary species\" as a result of catching a re-usable rocket might have merit in that it makes construction of things in outer space easier (although that's probably a charitable interpretation of the statement). My take on the Spacex is Mars habitation project is that Musk will put a bunch of edgelords on Mars, and then not really be able to follow up with adequate supply lines and the operation will be offline for a hundred years or so while the climate settles down. The people who live on Mars will then have been there alone for a century and in the 2100s we will send a follow up mission with hilarious consequences. reply latexr 10 hours agorootparentHe won’t put anyone living on Mars. And even if he did, they wouldn’t last that long. https://defector.com/neither-elon-musk-nor-anybody-else-will... https://www.acityonmars.com/ reply EnigmaFlare 9 hours agorootparentThat first article is just nonsense. The south pole can't support life? It's been supporting humans for half a century. Can't protect against radiation? Live underground like Hamas did. Have to wait 9 months for food? We do that on Earth too - if you're hungry, call a farmer and ask him to plant some food then come back in a year or two when it's harvested. We solve that problem by pipelining it, just as you would on Mars. reply latexr 4 hours agorootparentThe South Pole hasn’t been “supporting humans”, we forced ourselves in there and survive despite the conditions in an environment that is harsh but even so considerably more hospitable than Mars. All your other solutions are hand-wavey. Sure, let’s “just live underground” as if that’s just as easy as pitching a tent. And who on Earth is surviving nine months without any food? You’re talking as if Mars is just turning right on Albuquerque. reply peepeepoopoo87 12 hours agorootparentprevThunderf00t, is that you? reply poulpy123 9 hours agorootparentYou don't need to be thunderfoot to understand that pretending you will start to colonize mars in the next years lies somewhere between daydreaming and scamming. Although I like when thunderfoot compare the archives of what musk said with what actually happened reply peepeepoopoo87 7 hours agorootparentThe punchline is that everyone here thought being called \"thunderf00t\" was a compliment, even though I meant it as an example of someone who is consistently proven wrong at every turn for casting shade on Musk's tech ambitions. It seems HN's original techno-optimist hacker ethos is dead in the grave. reply dools 7 hours agorootparentOr is the REAL punchline the fact that Musk has optimised his entire empire to tap into the hacker ethos/ideals as the world's biggest pump n dump scheme? He seems to just do things that have the biggest wow factor because growth stocks need to keep growing otherwise there is no point in owning them. reply dools 12 hours agorootparentprevHaha I had never heard of that dude, but I like the look of his content thanks! BTW you can tell I'm not Thunderf00t because he says that \"the taxpayer\" paid $3 billion. I would never use \"taxpayer funding\" language, I would only ever call it public money (because Treasury creates money when it spends). reply exe34 10 hours agorootparentby that logic tax should be zero? reply baq 9 hours agorootparenttaxes are how you don't have inflation. the government can also destroy money about as easily as it creates it, too. it isn't a politically (and usually economically) desirable thing to do. when it's done, it's usually via replacing the whole currency wholesale (e.g. brazilian real). reply exe34 3 hours agorootparent> taxes are how you don't have inflation. or you know, don't print trillions to bail out failed banks. reply dools 10 hours agorootparentprevNot really, if there was no tax there would be no money. The most succinct way that I have found to express the relationship between taxation and spending is that spending at the federal level is constrained by aggregate spending this year, not tax receipts last year. reply photochemsyn 5 hours agorootparentprevMusk hasn't to my knowledge financed any architectural design projects for a long-term livable Mars habitat that can sustain itself without constant inputs from Earth. I suspect this is because the most casual analysis reveals incredible difficulties - the structures would have to be buried under a few meters of regolith to avoid constant radiation burn, and the ration of human living space to plant growing space (for food) would have to be about 1:6 I'd guess. The amount of material required to build such a structure for 100 humans? Let alone maintenance, etc. If realistic plans were actually presented no doubt everyone would start laughing, which is why we haven't seen any mock-ups, VR models, etc. reply dotnet00 3 hours agorootparentThey're working on getting the issue of transport sorted out first because the entire architecture is shaped by the constraints and requirements of your transport system. The amount of mass you can land, the energy needed for ISRU and so on. HN just has forgotten its hacker roots and instead gets off to unconstructively sitting back and criticizing with shallow gotchas. reply cladopa 11 hours agoparentprev\"all you did was making rockets 10x cheaper\", so you have plans for making them 100x to 1000x? That has nothing to do with Mars! reply resonious 11 hours agoparentprevIt's a step towards landing on Mars. They aren't even claiming to have landed. reply reader9274 2 hours agorootparentMusk claimed we would be on Mars in 2022 so... https://www.theguardian.com/technology/2016/sep/27/elon-musk... reply mikewarot 13 hours agoprevLong, long ago, Tunnel Diodes were going to usher in an era of ultra-fast computing because their negative resistance region allowed for current gain in the simple 2 pin device. It didn't work out for most of it, but does show that you can do logic without transistors. Think of these as incredibly slow negative resistance devices. Computing with them might be possible, barely. But sometimes that's all you need. reply AyyEye 13 hours agoparentThey arent dead! Diy 8ghz (sampling) oscilloscope with tunneling diodes https://hackaday.io/project/167292-8-ghz-sampling-oscillosco... reply lmpdev 15 hours agoprevI thought resettable fuses were already polymer based? PPTCs are just plastic and metal with no semiconductors afaik Is this actually an MIT article? reply curtisf 14 hours agoparentFrom the paper, > This is, to the best of our knowledge, the first report of fully 3D-printed resettable fuses. I think unique the contribution is that the entire circuit -- active and passive -- can be made with this one single material. Normally, you need to use many different materials and chemical baths to make a circuit with active components, but using this metal-polymer mix, you can _just_ deposit the metal and you are finished. reply sebstefan 9 hours agoprevOh my god. We might get real life redstone. reply gtzi 10 hours agoprevAlso you may want to take a look at https://www.botfactory.co reply elif 5 hours agoprevHmmm not sure I see the advantages of 3d printing vs lithography. I mean, yes, technically, this approach could advance to catch up with lithography. In practice? Yes we are working toward smaller feature size in additive, but we are nowhere close to micro let alone nano scale. If the advantage is \"hobbyists can do it\" then I would say \"hobbyists can eaglecad too\" This seems like a detour or speedbump on our existing path toward atom level logic production. reply wkat4242 9 hours agoprevInteresting idea but millimeter-scale logic doesn't really have much practical use in this day and age :) but it's s nice proof of concept. reply jayd16 3 hours agoparentEven placing traces inside a print and then slotting in IC boards would be useful. Even at mm scale it changes the types of prints that can be done. reply Taniwha 9 hours agoparentprevAh - but it's not just mm scale - it's 3d mm scale - sure it still needs to be smaller but if you can print into volumes rather than just on 2d planes things get interesting reply wkat4242 6 hours agorootparentYeah, but a GPU would still be the scale of the pentagon :P reply sambeau 9 hours agoprevIt's refreshing to see this labeled as a \"step toward\". reply torginus 12 hours agoprevWhile mentally stimulating, this sounds practically not very useful. They're using a copper-doped polymer for printing, which probably has way worse properties anything we make PCB traces out of. And the 3D part is gimmicky. We have built electronic systems of monstrous complexity just with planar printers. Wake me up when someone build a system that can reliably make PCBs at home, with placing components, and doesn't cost an arm and a leg, and is cheap and easy to run. reply DoctorOetker 9 hours agoparentIt would be nice to pattern diodes and semiconductors on PCB without components as follows: etch circuit layout of a copper layer, mask the traces so they don't oxidize, then heat the PCB to have unmasked copper turn into Cu2O (cuprous oxide, a semiconductor). Anyone seriously attempting this should make sure they understand solid state physics, and at a minimum understand diffusion length of charge carriers and the different type of contacts: Ohmic, Schottky ( for example https://lampz.tugraz.at/~hadley/psd/lectures20/contacts.pdf ) Performance will be horrible, but in some situations constructing and inspecting the device oneself can be paramount ( bootstrapping a secure computational platform, implementing formal verifier associated to a cryptocurrency, ... ) reply matthewfelgate 4 hours agoprevThis is fascinating. I did thing a while back if it was possible to 3D print primitive electronics. reply peter_d_sherman 13 hours agoprev>\"They saw an interesting phenomenon in the material they were using, a polymer filament doped with copper nanoparticles. If they passed a large amount of electric current into the material, it would exhibit a huge spike in resistance but would return to its original level shortly after the current flow stopped.\" This is interesting -- large amounts of current being associated with increased resistance... I have never seen or read about something like that with respect to other electronic components or systems. It would be an interesting experiment to see if this effect could be simulated, and if so, under what conditions, in non-nanoparticle standard regular-sized electrical components... I'm guessing (but not knowing!) that you'd you'd need a very high amount of current (like something from a car battery), but at a very low voltage, like maybe 0.1 or 0.01 volts (or less), and maybe like a very thin long wire made of some mostly-conducitive material, and then maybe something at some scale or low voltage if the experimenter was lucky... Anyway, large current associated with increased resistance... I've never heard of that one before, except I suppose if the current heats the electrical path so much that it destroys it... which would be different for different materials, voltages, cross-section of conductors, temperatures, etc., etc. I'd assume that wouldn't happen at the nanoscale and/or in a switching semiconductor... but perhaps I might be wrong on some level... reply tlb 12 hours agoparentPTC (positive temperature coefficient) thermistors do that. https://en.wikipedia.org/wiki/Thermistor. They’re used as auto-resetting fuses in many devices. reply torginus 12 hours agoparentprevlarge amount of electric current into the material, it would exhibit a huge spike in resistance considering the low melting point of these 3d printing plastics, they probably melted the wire. reply elictronic 13 hours agoparentprevLightbulbs? High amount of current increases resistance. reply graycat 16 hours agoprev [3 more] [flagged] wrsh07 16 hours agoparent [–] Did you comment on the wrong post? Or did the link get swapped out on you? reply graycat 14 hours agorootparent [–] Thanks. Don't know what happened: Intended to comment on \"HN Update – Hourly News Broadcast of Top HN Stories (hnup.date)\" at https://news.ycombinator.com/item?id=41893524 but apparently somehow posted to \"3D-Printed Active Electronics (news.mit.edu)\" at https://news.ycombinator.com/item?id=41893524 Just deleted the post at \"3D-Printed Active Electronics (news.mit.edu)\" and submitted the post to \"HN Update – Hourly News Broadcast of Top HN Stories (hnup.date)\" Thanks. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MIT researchers have created semiconductor-free logic gates using 3D-printed copper-doped polymer, potentially simplifying electronics manufacturing.",
      "This innovation could democratize electronics production by enabling smart hardware creation outside traditional manufacturing hubs, despite not yet matching semiconductor performance.",
      "The project builds on work with magnetic coils and utilizes a unique resistance property of the copper-doped polymer, with future aims to print fully functional electronics and explore more functionalities."
    ],
    "commentSummary": [
      "MIT researchers have advanced 3D printing technology by creating a resettable fuse and a transistor-like device using copper-infused PLA (Polylactic Acid) filament.",
      "The innovation is significant due to the use of a single material, but it does not yet achieve fully functional active electronics.",
      "The discovery of the filament's Positive Temperature Coefficient (PTC) effect is important, though the technology remains in its early stages and is not yet ready for broad application."
    ],
    "points": 168,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1729473434
  },
  {
    "id": 41898603,
    "title": "Skeptical of rewriting JavaScript tools in \"faster\" languages",
    "originLink": "https://nolanlawson.com/2024/10/20/why-im-skeptical-of-rewriting-javascript-tools-in-faster-languages/",
    "originBody": "Read the Tea Leaves Software and other dark arts, by Nolan Lawson Home Apps Code Talks About « The greatness and limitations of the js-framework-benchmark 20 Oct Why I’m skeptical of rewriting JavaScript tools in “faster” languages Posted October 20, 2024 by Nolan Lawson in performance, Web. Tagged: javascript. 7 Comments I’ve written a lot of JavaScript. I like JavaScript. And more importantly, I’ve built up a set of skills in understanding, optimizing, and debugging JavaScript that I’m reluctant to give up on. So maybe it’s natural that I get a worried pit in my stomach over the current mania to rewrite every Node.js tool in a “faster” language like Rust, Zig, Go, etc. Don’t get me wrong – these languages are cool! (I’ve got a copy of the Rust book on my desk right now, and I even contributed a bit to Servo for fun.) But ultimately, I’ve invested a ton of my career in learning the ins and outs of JavaScript, and it’s by far the language I’m most comfortable with. So I acknowledge my bias (and perhaps over-investment in one skill set). But the more I think about it, the more I feel that my skepticism is also justified by some real objective concerns, which I’d like to cover in this post. Performance One reason for my skepticism is that I just don’t think we’ve exhausted all the possibilities of making JavaScript tools faster. Marvin Hagemeister has done an excellent job of demonstrating this, by showing how much low-hanging fruit there is in ESLint, Tailwind, etc. In the browser world, JavaScript has proven itself to be “fast enough” for most workloads. Sure, WebAssembly exists, but I think it’s fair to say that it’s mostly used for niche, CPU-intensive tasks rather than for building a whole website. So why are JavaScript-based CLI tools rushing to throw JavaScript away? The big rewrite I think the perf gap comes from a few different things. First, there’s the aforementioned low-hanging fruit – for a long time, the JavaScript tooling ecosystem has been focused on building something that works, not something fast. Now we’ve reached a saturation point where the API surface is mostly settled, and everyone just wants “the same thing, but faster.” Hence the explosion of new tools that are nearly drop-in replacements for existing ones: Rolldown for Rollup, Oxlint for ESLint, Biome for Prettier, etc. However, these tools aren’t necessarily faster because they’re using a faster language. They could just be faster because 1) they’re being written with performance in mind, and 2) the API surface is already settled, so the authors don’t have to spend development time tinkering with the overall design. Heck, you don’t even need to write tests! Just use the existing test suite from the previous tool. In my career, I’ve often seen a rewrite from A to B resulting in a speed boost, followed by the triumphant claim that B is faster than A. However, as Ryan Carniato points out, a rewrite is often faster just because it’s a rewrite – you know more the second time around, you’re paying more attention to perf, etc. Bytecode and JIT The second class of performance gaps comes from the things browsers give us for free, and that we rarely think about: the bytecode cache and JIT (Just-In-Time compiler). When you load a website for the second or third time, if the JavaScript is cached correctly, then the browser doesn’t need to parse and compile the source code into bytecode anymore. It just loads the bytecode directly off disk. This is the bytecode cache in action. Furthermore, if a function is “hot” (frequently executed), it will be further optimized into machine code. This is the JIT in action. In the world of Node.js scripts, we don’t get the benefits of the bytecode cache at all. Every time you run a Node script, the entire script has to be parsed and compiled from scratch. This is a big reason for the reported perf wins between JavaScript and non-JavaScript tooling. Thanks to the inimitable Joyee Cheung, though, Node is now getting a compile cache. You can set an environment variable and immediately get faster Node.js script loads: export NODE_COMPILE_CACHE=~/.cache/nodejs-compile-cache I’ve set this in my ~/.bashrc on all my dev machines. I hope it makes it into the default Node settings someday. As for JIT, this is another thing that (sadly) most Node scripts can’t really benefit from. You have to run a function before it becomes “hot,” so on the server side, it’s more likely to kick in for long-running servers than for one-off scripts. And the JIT can make a big difference! In Pinafore, I considered replacing the JavaScript-based blurhash library with a Rust (Wasm) version, before realizing that the performance difference was erased by the time we got to the fifth iteration. That’s the power of the JIT. Maybe eventually a tool like Porffor could be used to do an AOT (Ahead-Of-Time) compilation of Node scripts. In the meantime, though, JIT is still a case where native languages have an edge on JavaScript. I should also acknowledge: there is a perf hit from using Wasm versus pure-native tools. So this could be another reason native tools are taking the CLI world by storm, but not necessarily the browser frontend. Contributions and debuggability I hinted at it earlier, but this is the main source of my skepticism toward the “rewrite it all in native” movement. JavaScript is, in my opinion, a working-class language. It’s very forgiving of types (this is one reason I’m not a huge TypeScript fan), it’s easy to pick up (compared to something like Rust), and since it’s supported by browsers, there is a huge pool of people who are conversant with it. For years, we’ve had both library authors and library consumers in the JavaScript ecosystem largely using JavaScript. I think we take for granted what this enables. For one: the path to contribution is much smoother. To quote Matteo Collina: Most developers ignore the fact that they have the skills to debug/fix/modify their dependencies. They are not maintained by unknown demigods but by fellow developers. This breaks down if JavaScript library authors are using languages that are different (and more difficult!) than JavaScript. They may as well be demigods! For another thing: it’s straightforward to modify JavaScript dependencies locally. I’ve often tweaked something in my local node_modules folder when I’m trying to track down a bug or work on a feature in a library I depend on. Whereas if it’s written in a native language, I’d need to check out the source code and compile it myself – a big barrier to entry. (To be fair, this has already gotten a bit tricky thanks to the widespread use of TypeScript. But TypeScript is not too far from the source JavaScript, so you’d be amazed how far you can get by clicking “pretty print” in the DevTools. Thankfully most Node libraries are also not minified.) Of course, this also leads us back to debuggability. If I want to debug a JavaScript library, I can simply use the browser’s DevTools or a Node.js debugger that I’m already familiar with. I can set breakpoints, inspect variables, and reason about the code as I would for my own code. This isn’t impossible with Wasm, but it requires a different skill set. Conclusion I think it’s great that there’s a new generation of tooling for the JavaScript ecosystem. I’m excited to see where projects like Oxc and VoidZero end up. The existing incumbents are indeed exceedingly slow and would probably benefit from the competition. (I get especially peeved by the typical eslint + prettier + tsc + rollup lint+build cycle.) That said, I don’t think that JavaScript is inherently slow, or that we’ve exhausted all the possibilities for improving it. Sometimes I look at truly perf-focused JavaScript, such as the recent improvements to the Chromium DevTools using mind-blowing techniques like using Uint8Arrays as bit vectors, and I feel that we’ve barely scratched the surface. (If you really want an inferiority complex, see other commits from Seth Brenith. They are wild.) I also think that, as a community, we have not really grappled with what the world would look like if we relegate JavaScript tooling to an elite priesthood of Rust and Zig developers. I can imagine the average JavaScript developer feeling completely hopeless every time there’s a bug in one of their build tools. Rather than empowering the next generation of web developers to achieve more, we might be training them for a career of learned helplessness. Imagine what it will feel like for the average junior developer to face a segfault rather than a familiar JavaScript Error. At this point, I’m a senior in my career, so of course I have little excuse to cling to my JavaScript security-blanket. It’s part of my job to dig down a few layers deeper and understand how every part of the stack works. However, I can’t help but feel like we are embarking down an unknown path with unintended consequences, when there is another path that is less fraught and could get us nearly the same results. The current freight train shows no signs of slowing down, though, so I guess we’ll find out when we get there. Related 7 responses to this post. Posted by akbar akkah on October 20, 2024 at 2:54 PM Great blog! Great read! Reply Posted by HN user on October 20, 2024 at 3:09 PM Comments on HN: https://news.ycombinator.com/item?id=41898603 Reply Posted by David on October 20, 2024 at 5:49 PM I have a tool that I am writing that I would like to write in JavaScript – but will probably end up writing in rust for a reason you haven’t included here: most developer machines have 8 or 16 cores and Js can only use one of them at a time … so this allows for a 10x speedup for tasks that are well parallelizable … Reply Posted by Nolan Lawson on October 20, 2024 at 6:18 PM Good point! Node does have native support for Workers now, but it’s hard to share memory between them. “Embarrassingly parallel” tasks definitely make a lot of sense to do in Rust. Reply Posted by tracker1 on October 20, 2024 at 11:39 PM I’m genuinely very mixed on this. I learned to program around when JS started and grew up with it as my main language through the years. I still consider it my strongest language by a wide margin.. I was thrilled when Node took off and JS rolling was finally getting built with JS. 6to4 (Babel) was a blessing and the advances to the browser engineer has been amazing. I absolutely love a lot of what’s come out of it all and wouldn’t go back to 90s style dev at all. That said, waiting seconds every commit for limiting and formatting sucks. The first time I used Biome (then Rome) I was sold. It was blink of an eye fast. White there’s something getting about being able to work on your tooling and many do. Not everyone needs to. Having a few developer gods among men work on better tools isn’t a bad thing. Relatively few write JS engines. And the aren’t a ton of C compilers in wide use. It’s definitely time for a few winners to rise with the lessons learned. I’m all for it. I think the real reason for these faster tools is much the same as having an nvme drive is now essential for web development. There’s so many touch points that all those milliseconds just add up to too much real time. Reply Posted by t1oracle on October 21, 2024 at 4:12 AM I use JavaScript because that’s the only thing that Web Browsers understand. If I could use Rust instead I would. Unfortunately, that just isn’t practical yet. Reply Posted by Fred on October 21, 2024 at 6:15 AM And then there’s the new Deno runtime which is supposed to make things faster. Reply Leave a comment This site uses Akismet to reduce spam. Learn how your comment data is processed. Recent Posts Why I’m skeptical of rewriting JavaScript tools in “faster” languages The greatness and limitations of the js-framework-benchmark Web components are okay Improving rendering performance with CSS content-visibility The continuing tragedy of emoji on the web About Me Hi, I'm Nolan. I'm a web developer living in Seattle and working for Salesforce. Opinions expressed in this blog are mine and frequently wrong. Archives October 2024 (2) September 2024 (3) August 2024 (1) July 2024 (1) March 2024 (1) January 2024 (1) December 2023 (4) August 2023 (2) January 2023 (2) December 2022 (1) November 2022 (2) October 2022 (2) June 2022 (4) May 2022 (3) April 2022 (1) February 2022 (1) January 2022 (1) December 2021 (3) September 2021 (1) August 2021 (6) February 2021 (2) January 2021 (2) December 2020 (1) July 2020 (1) June 2020 (1) May 2020 (2) February 2020 (1) December 2019 (1) November 2019 (1) September 2019 (1) August 2019 (2) June 2019 (4) May 2019 (3) February 2019 (2) January 2019 (1) November 2018 (1) September 2018 (5) August 2018 (1) May 2018 (1) April 2018 (1) March 2018 (1) January 2018 (1) December 2017 (1) November 2017 (2) October 2017 (1) August 2017 (1) May 2017 (1) March 2017 (1) January 2017 (1) October 2016 (1) August 2016 (1) June 2016 (1) April 2016 (1) February 2016 (2) December 2015 (1) October 2015 (1) September 2015 (1) July 2015 (1) June 2015 (2) October 2014 (1) September 2014 (1) April 2014 (1) March 2014 (1) December 2013 (2) November 2013 (3) August 2013 (1) May 2013 (3) January 2013 (1) December 2012 (1) November 2012 (1) October 2012 (1) September 2012 (3) June 2012 (2) March 2012 (3) February 2012 (1) January 2012 (1) November 2011 (1) August 2011 (1) July 2011 (1) June 2011 (3) May 2011 (2) April 2011 (4) March 2011 (1) Tags accessibility alogcat android android market apple app tracker benchmarking blobs boost bootstrap browsers bug reports catlog chord reader code contacts continuous integration copyright couch apps couchdb couchdroid developers development emoji grails html5 indexeddb information retrieval japanese name converter javascript jenkins keepscore listview logcat logviewer lucene nginx nlp node nodejs npm offline-first open source passwords performance pinafore pokedroid pouchdb pouchdroid query expansion relatedness calculator relatedness coefficient s3 safari satire sectioned listview security semver shadow dom social media socket.io software development solr spas supersaiyanscrollview synonyms twitter ui design ultimate crossword w3c webapp webapps web platform web sockets websql Links Mastodon GitHub npm Blog at WordPress.com.",
    "commentLink": "https://news.ycombinator.com/item?id=41898603",
    "commentBody": "Skeptical of rewriting JavaScript tools in \"faster\" languages (nolanlawson.com)147 points by todsacerdoti 21 hours agohidepastfavorite265 comments munificent 4 hours ago> I don’t think that JavaScript is inherently slow It is. Brilliant engineers have spent decades making it faster than you might expect, subject to many caveats, and after the JIT has had plenty of time to warm up, and if you're careful to write your code in such a way that it doesn't fall off the JITs optimization paths, etc. Meanwhile, any typical statically typed language with a rudimentary ahead of time compiler will generally be faster than a JS VM will ever approach. And you don't have to wait for the JIT to warm up. There are a lot of good things about dynamically typed languages, but if you're writing a large program that must startup quickly and where performance is critical, I think the right answer is a sound typed language. reply __s 2 hours agoparentI spent years tuning a JS game engine to play nice with JIT for best performance. Then rewrote engine in Rust/WASM over a few weekends (turns out JIT friendly code is straightforward to port to statically typed language) & things are an order of magnitude faster now with the benefits of static type checking & no spooky jit perf to optimize for Just because JS can be fast doesn't mean it's a pleasure to write fast JS reply gagaq 3 hours agoparentprev> I think the right answer is a sound typed language. What do you mean by a \"sound typed language\". Go and Java have unsound type systems, and run circles around JS and Dart. Considering your involvement with Dart, I find contradictory information [1]. [1] - https://github.com/dart-lang/language/issues/1461 reply munificent 25 minutes agorootparent> What do you mean by a \"sound typed language\". I mean that if the type checker concludes than an expression or variable has type T, then no execution of the program will ever lead to a value not of type T being observed in that variable or expression. In most languages today, this property it enforced with a combination of static and runtime checks. Mostly the former, but things like checked casts, runtime array covariance checks, etc. are common. That in turn means that a compiler can safely rely on the type system to generate more efficient code. Java intended to have a sound type system, but a hole or two have been found (which are fortunately caught at runtime by the VM). Go's type system is sound as far as I know. Dart's type system is sound and we certainly rely on that fact in the compiler. There is no contradictory information as far as I know, but many people seem to falsely believe that soundness requires zero runtime checks, which isn't the case. reply causal 3 hours agoparentprevIt mostly depends on the application. If you're doing complex transforms over hundreds of GBs of data- yeah, use a workhorse for that. But the vast majority of slow JS I've encountered was slow because of an insane dependency tree or wildly inefficient call stacks. Faster languages cannot fix polynomial or above complexity issues. reply chubot 3 hours agorootparentYeah but the application we're talking about here is JavaScript tools, or more generally \"language processors\" / AST-based workloads These are very different than your average JavaScript program And that's exactly where it starts to be the case that JavaScript semantics are the issue Take it from Lars Bak and Emery Berger (based on their actions, not just opinions): https://lobste.rs/s/ytjc8x/why_i_m_skeptical_rewriting_javas... :) reply steve_adams_86 10 hours agoprevI’ve written quite a bit of tooling in JS, and I genuinely enjoy the language, but I feel like Rust and Go are a godsend for these types of tools. I will sometimes prototype with TypeScript, but if something requires massive concurrency and parallelism, it’s unlikely I’ll stick with it. I wonder if the author would feel differently if they spent more time writing in more languages on tooling like this. My life got a lot easier when I stopped trying to write TypeScript everywhere and leveraged other languages for their strengths where it made sense. I really wanted to stick to one language I felt most capable with, but seeing how much easier it could be made me change my mind in an instant. The desire for stronger duck typing is confusing to me, but to each their own. I find Rust allows me to feel far, far more confident in tooling specifically because of its type system. I love that about it. I wish Go’s was a bit more sane, but there are tons of people who disagree with me. reply bluGill 4 hours agoparent> The desire for stronger duck typing is confusing to me, but to each their own I really like duck typing when I'm working on small programs - under 10,000 lines of code. Don't make me worry about stupid details like that, you know what I mean so just do the $%^#@ thing I want and get out of my way. When I work with large programs (more than 50k lines of code - I work with some programs with more than 10 million lines and I know of several other projects that are much larger - and there is reason to believe many other large programs exist where those who work on them are not allowed to talk about them) I'm glad for the discipline that strong typing forces on me. You quickly reach a point in code where types save you from far more problems than their annoyance costs. reply nobodyandproud 1 hour agorootparentA language that goes from prototype-quality (duck typing, dynamic, and interpreted) to strict static compile checks would be nice. I can’t think of any in the mainstream, however. reply Spivak 25 minutes agorootparentIsn't that TypeScript or more generically \"type hints\" in other languages? If all you care about is that your program is valid for some gradually increasing subset of your code at compile time then these work great. reply Joker_vD 4 hours agorootparentprev> you know what I mean just do the $%^#@ thing I want Yeah, it's just that about 10k LoC, as I've also noticed, you don't actually know what you yourself mean! It's probably because such amount of code is almost never written in one sitting, so you end up forgetting that e.g. you've switched, for this particular fields, from a stack of strings to just a single string (you manage the stacking elsewhere) and now your foo[-1] gives you hilarious results. reply romwell 3 hours agorootparentTypes in the end are contacts you make with yourself, enforced by the compiler. A weak type system gives you the freedom to trick yourself. I don't feel it's a feature. reply yesiamyourdad 4 hours agoparentprevI just skimmed the article but the author had a statement about JS being \"working class\" in that it didn't enforce types and that he dislikes TS for that reason. Rust is completely anathema to that attitude, you have to make a LOT of decisions up front. People who don't see the value in a compiler are never going to like working in Rust. The author is completely satisfied with optimizing hacks in the toolchain. reply Vinnl 4 hours agoparentprev\"Skeptical\" doesn't mean completely against their usage. From the author in the comment section: > “Embarrassingly parallel” tasks definitely make a lot of sense to do in Rust. reply steve_adams_86 2 hours agorootparentI noticed that, and I was left kind of confused. My experience might be limited, but operating on thousands of files and performing multiple reads or writes on them over and over seems exactly like the type of thing a lot of JS tooling does, so I’m not really sure where the author decides that JS is no longer the right fit. I don’t see why that doesn’t dispel skepticism right off of the bat. reply romwell 3 hours agoparentprev>The desire for stronger duck typing That's what C++ templates always have been, and got way, way tighter with concepts circa C++23. Rust's traits are also strong duck typing if you squint a little. The idea in both cases is simple: write the algorithm first, figure outwhat can go into it later — which allows you to write the code as if all the parts have the types you need. But then, have the compiler examine the ducks before the program runs, and if something doesn't quack, the compiler will. reply cies 8 hours agoparentprevI also though rewrites of JS projects were only in part motivated by perf gain. Sure they were the most advertised benefits of the rewritten tools, as that communicates a benefit for the users of those tools. > I find Rust allows me to feel far, far more confident in tooling specifically because of its type system. Usually the JS projects become really hard to work on the growing up. Good JS needs a lot of discipline on the team of devs working on it: it get messy easily and refactoring becomes very hard. Type systems help with that. TypeScript helps, but only so much... Going with a languages that both has a sound type system (like Rust) and allows lots of perf improvements (like Rust) becomes an attractive option. reply roca 8 hours agoprevI think the importance of parallelism has been overlooked by the OP and most commenters here. Even laptops these days have at least 8 cores; a good scalable parallel implementation of a tool will crush the performance of any single-threaded implementation. JS is not a great language for writing scalable parallel code. Rust is. Not only do Rust and its ecosystem (e.g. Rayon) make a lot of parallel idioms easy, Rust's thread safety guarantees let you write shared-memory code without creating a maintenance nightmare. In JS you can't write such code at all. So yes, you can do clever tricks with ArrayBuffers, and the JS VMs will do incredibly clever optimizations for you, but as long as your code is running on one core you cannot be competitive. (Unless your problem is inherently serial, but very few \"tool\"-type problems are.) reply nine_k 5 hours agoparentOTOH in production settings you can run 6-8 copies of your Node app to utilize the 8 physical CPU cores without (much) contention; the JS engine runs additional threads for GC and other housekeeping. Or you can use multiple \"web workers\" which are VM copies in disguise. The async nature of the JS runtime may allows for a lot of extra parallelism (for waiting on I/O completion) on one core if your load is I/O-bound, as it is the case for most web backends. Th same does not hold for frontend use, as it's for one user, and latency trumps throughput in the perception of being fast. You need great single-thread performance, and an ability to offload stuff to parallel threads where possible, to keep the overall latency low. That's basically the approach of game engines. reply chrismorgan 4 hours agorootparentWhen talking about tooling: around five years ago, I introduced parallelism to a Node.js-based build system, in a section that was embarrassingly parallel. It was really painful to implement, made it noticeably harder to maintain, and my vague memory is that on an 4-core/8-thread machine I only got something like a 3–4× speedup. I think workers are mature enough now that it wouldn’t be quite so bad, but it would still be fairly painful. In Rust, I’d have added Rayon as a dependency to my Cargo.toml, inserted `use rayon::prelude::;` (or a more specific import, if I preferred) into my file, changed one `.iter()` to `.par_iter()`, and voilà, it’d have compiled (all the types would have satisfied Send) and given probably at least a 6–7× speedup. Seriously, when you get to talking about a lot of performance tricks and such (I’m thinking things like the bit maps referred to at the end), even when they’re possible* in JavaScript, they’re frequently—I suspect even normally—way easier to implement in Rust. reply akamaka 1 hour agorootparentAs someone who hasn’t used Rust, I found your comment very helpful for understand the advantages compared to how JS does parallelism. reply nine_k 1 hour agorootparentprevThis is very correct for new development. I only mean that utilizing extra CPU cores in JS is a bit easier for an API server, with tons of identical parallel requests running, and where the question is usually in RPS and tail latency, than for single-task use cases like parallelized builds. reply Vinnl 4 hours agoparentprevThey acknowledge it in the comments: > “Embarrassingly parallel” tasks definitely make a lot of sense to do in Rust. reply LordHeini 6 hours agoprevAs someone who as written a lot of Go and trained a couple of people in it. The \"more difficult\" in this quote makes me somewhat angry. `This breaks down if JavaScript library authors are using languages that are different (and more difficult!) than JavaScript.` JS is absolutely not easy! It is not class oriented but uses funky prototypes, it has classes slapped on PHP-Style. Types are bonkers, so someone bolted on TypeScript. It has a dual wield footgun in the form of null/undefined, a repeat of the billion dollar mistake but twice! The whole Javascript tooling and ecosystem is a giant mess with no fix in sight (hence all the rewrites). The whole JavaScript ecosystem is ludicrously complicated with lots of opinions on everything. Tooling is especially bad because you need a VM to run stuff (so lots of rewrites). This is why Java never got much traction in that space too. Go for example is way easier to learn than Javascript. Here i mean to a level of proficiency which goes beyond making some buttons blink or load a bit of stuff from some database. Tooling just works. There is no thought to spend on how to format stuff or which tool to use to run things. And even somewhat difficult (and in my opinion useless) features like classes are a absent. Want to do concurrency? Just to `go func whatever()`. Want it to communicate across threads? Use a channel it makes stuff go from A -> B. Try this in JS you have to know concepts like Promises, WebWorkers and a VM which is not really multithreaded to begin with. reply WD-42 4 hours agoparentAgree that JS is not easier these days. It only seems easier because we all already know it. reply ForHackernews 2 hours agoparentprevThis author already has JS Stockholm syndrome and at least acknowledges that up front. reply thomasvogelaar 12 hours agoprevI don't buy the argument that a lot of the performance jumps from rewrites comes from developers writing more optimised code. I've worked on multiple rewrites of existing systems in both JS and PHP to Go and those projects were usually re-written strictly 1:1 (bugs becoming features and all that). It was pretty typical to see an 8-10x performance improvement by just switching language. reply xandrius 11 hours agoparentExactly the same experience. For a smallish batch processing script I had written in node, I just fed it to chatgpt and got the golang version. It went from being unusable with over 100K records to handling 1M on exactly the same machine. And only then I started adding things like channels, parallelism, and smart things. reply thomasvogelaar 11 hours agorootparentNow that there's no perfect parity to maintain we've started optimising the Go versions as well. Multiple 2x performance improvements once we started doing this, on top of the original performance improvements. This translates to insane cost savings when you're working at scale. reply Joker_vD 7 hours agoparentprevYeah, it reminds me about all those old \"Haskell can be faster than C!\" posts that used to be very popular. Sure, some exquisite, finely-crafted Haskell code can be faster than a plain, dumb, straightforward, boring C code. But if you compare plain, dumb, straightforward, boring Haskell code with plain, dumb, straightforward, boring C code, the latter will be faster pretty much always. reply nineteen999 7 minutes agorootparentPlus it will be readable by a much larger percentage of working programmers. reply austin-cheney 8 hours agoprevThe sentiment of the article is 90% right. In all fairness there are opportunities for making tools faster by writing them in faster languages, but these tend to be extreme scenarios like whether you really need to send 10 million WebSocket messages in the fastest burst possible. Aside from arithmetic operations JavaScript is now as fast as Java and only 2-4x slower than C++ for several years now and it compiles almost instantly. Really though, my entire career has taught me to never ever talk about performance with other developers... especially JavaScript developers or other developers working on the web. Everybody seems to want performance but only within the most narrow confines of their comfort zone, otherwise cowardice is the giant in the room and everything goes off the rails. The bottom line is that if you want to go faster then you need to step outside your comfort zone, and most developers are hostile to such. For example if you want to drive faster than 20 miles per hour you have to be willing to take some risks. You can easily drive 120 miles per hour, but even the mere mention of increased speed sends most people into anxiety apocalypse chaos. The reactions about performance from other developers tend to be so absolutely over the top extreme that I switched careers. I just got tired of all the crying from such extremely insecure people who claim to want something when they clearly want something entirely different. You cannot reasonably claim to want to go faster and simultaneously expect an adult to hold your hand the entire way through it. reply dartos 7 hours agoparentBut build tools in js land are quite slow. Especially when you start throwing behemoths like Nx in the mix. Look at the performance gains in build tool land (esbuild specifically) and you’ll see the performance gains with native languages. For most webservers and UIs it’s plenty fast though. reply WorldMaker 1 hour agorootparentMuch of esbuild's performance gains are in throwing out a lot of cruft. It definitely benefits from the \"fresh rewrite can avoid the cruft of an organic project\", including specifically benefiting a lot from ESM as the winning end goal format, and the hindsight of Webpack eventually a massive organic ecosystem of plugins towards a core set of \"best practices\" over a lot of versions and ecosystem churn. esbuild versus webpack performance is never a fair fight. Most of the other behemoths are still \"just\" webpack configurations plus bundles of plugins. It will take a while for the build tools in that model to settle down/slim down. (esbuild versus Typescript for \"Typescript is the only build tool\" workflows is a much more interesting fight. esbuild doesn't do type checking only type stripping so it is also not a fair fight, and you really most often want both, but \"type strip-only\" modes in Typescript are iterating to compete with esbuild in fun ways, so it is also good for the ecosystem to see the fight happening.) I appreciate esbuild, but I also appreciate esbuild had so much of the benefit of a lot of hindsight and not developing in the open as an ecosystem of plugins like webpack did but rather baking in the known best practices as one core tool. reply dartos 58 minutes agorootparent> Much of esbuild's performance gains are in throwing out a lot of cruft. I don’t think there’s a great way to be sure of this. Parcel 2 (my personal favorite), for example, doesn’t include, by default, much of the cruft from mid-2010s JavaScript, but esbuild is still faster. Theoretically, being able to use multiple cores would bring speed improvements to a lot of the tree manipulation tasks involved in building js projects. > esbuild versus webpack performance is never a fair fight. Yeah webpack is just the worst. Bloated from day 1 reply chubot 3 hours agoparentprevIt's maybe 90% right in general, but it's 10% right (90% wrong) for the workload of language processors in particular. Lots of tiny objects are terrible workloads for JavaScript and Python. Related comments: https://news.ycombinator.com/item?id=35045520 Direct comparison I did between Python and C++ semantics - Oil's Parser is 160x to 200x Faster Than It Was 2 Years Ago - https://www.oilshell.org/blog/2020/01/parser-benchmarks.html This is the same realistic program in both Python and C++ -- no amount of \"optimizing Python\" is going to get you C++ speed. --- FWIW I agree with you about the debates -- many people can't seem to hold 2 ideas in their head at once. Like that C++ unordered_map is atrociously slow, but C++ is a great language for writing hash tables. And that Python was faster than Go for hash table based workloads when Go first came out, but also Python is slow for AST workloads. Performance is extremely multi-dimensional, and nuanced, but especially with programming languages people often want to summarize/compress that info in inaccurate ways. reply ludovicianul 4 hours agoparentprevJava is very fast: https://github.com/gunnarmorling/1brc reply austin-cheney 4 hours agorootparentSo... about performance. Performance is the difference between two or more measures. When not compared against something everything is itself fast. Out of a list of 1 item that one item will always be the 100% fastest item in the list. So, what's really important is not whether something is fast, but whether that thing is faster than something else and by how much. I suspect Java is fast. JavaScript is also fast. They are both fast. Without comparing measures the only significant distinction between the two is the time to compile. In that case Java is slow, or at least just substantially slower than JavaScript. Fortunately there are comparative benchmarks: The Programming Benchmark Games. It is not always the best, but it is certainly better than naught. https://benchmarksgame-team.pages.debian.net/benchmarksgame/... reply wormlord 4 hours agoprev> For another thing: it’s straightforward to modify JavaScript dependencies locally. I’ve often tweaked something in my local node_modules folder when I’m trying to track down a bug or work on a feature in a library I depend on. Whereas if it’s written in a native language, I’d need to check out the source code and compile it myself – a big barrier to entry. Anecdotally I have had to do this in js a few times. I have never had to do this in Rust. Probably because Rust projects are likely to ship with fewer bugs. Also Rust is harder to pick up but what are you going to do, use the most accessible tool to solve every problem, regardless of its' efficacy? I am not a Rust expert by any means, but just reading the Rust book and doing a couple projects made me a better programmer in my daily driver languages (js and Python). I think speed is less important here than correctness. Every time you ship a buggy library you are wasting the time of every single end user. The correctness alone probably saves more time in total than any performance gains. reply benesch 4 hours agoparent> Anecdotally I have had to do this in js a few times. I have never had to do this in Rust. Probably because Rust projects are likely to ship with fewer bugs. Still anecdotal, but I have worked on a large Rust codebase (Materialize) for six years, worked professionally in JavaScript before that, and I definitely wouldn’t say that Rust projects have fewer bugs than JavaScript projects. Rust projects have plenty of bugs. Just not memory safety bugs—but then you don’t have those in JavaScript either. And with the advent of TypeScript, many JS projects now have all the correctness benefits of using a language with a powerful type system. We’ve forked dozens of Rust libraries over the years to fix bugs and add missing features. And I’m know individual Materialize developers have had to patch log lines into our dependencies while debugging locally many a time—no record of that makes it into the commit log, though. reply wormlord 2 hours agorootparentIt could be that I just haven't written enough Rust to encounter this issue. Thanks for the insight! reply eviks 12 hours agoprev> One reason for my skepticism is that I just don’t think we’ve exhausted all the possibilities of making JavaScript tools faster. Exhausting is very exhausting, so at a fraction of that effort you could build on better foundations reply sksxihve 20 hours agoprev> Whereas if it’s written in a native language, I’d need to check out the source code and compile it myself – a big barrier to entry. Is it though? Rust/Zig/Go programs are pretty much all incredibly easy to checkout and compile, it's one of the big selling points of those languages. And at the end of the day how often are javascript developers fixing the tooling they use even when it's written in javascript? I've always felt learning new languages give me not only new tools to use but shapes the way I think about solving problems. reply strken 13 hours agoparentIt's definitely more of a pain to figure out rustup than to use the Node.js environment that's already installed. As noted in the article, you can quite literally go edit the source of your NPM packages without downloading or compiling a single thing. Minor speedbumps like installing Rust don't stop me now, and probably don't stop you either, but they might have at the start of my career. You have to think about the marginal developers here: how many people are able to debug the simple thing who would be unable or unwilling to do it for the complicated thing? As you note, it's already quite rare to fix up one's tooling, so we can't afford to lose too many potential contributors. I like learning new languages too, but not to the extent that I'd choose to debug my toolchain in Zig while under time pressure. This is something I've actually done before, most notably for FontCustom, which was a lovably janky Ruby tool for generating font icons popular about a decade ago. reply kstrauser 12 hours agorootparentThat’s not objectively true at all. I learned to use Rust long before I ever touched a Node setup, and the first time I wanted to run a JS app it took me a lot longer to figure out how to do it than it did to type `cargo run foo`. Neither is easier than the other. Whichever one you already know will be easier for you, and that’s it. reply strken 12 hours agorootparentSorry, I think we might be talking across each other. I am saying from the perspective of someone who is already using a full Node.js environment, adding Rust must necessarily increase complexity. I am taking this perspective because in the article we're talking about, the examples are exclusively JavaScript tooling like Rollup, Prettier, and ESLint, where the only people using those tools are JavaScript developers who are already running node. I have absolutely no interest in getting into a pissing match about whose language and ecosystem is better, and I in fact agree that the Rust tooling is less complicated than JS to start with. Nevertheless, the article is not about choosing either JS or Rust, it's about rewriting tools for working with JS in Rust, which necessarily makes you learn Rust on top of JS if you want to modify them. reply ndriscoll 3 hours agorootparentThat's true of any tools, and most tools that a developer uses (e.g. `grep`) are written in C, C++, or more recently Rust. Or if they want to understand the details of how some part of JS actually works, they'll need to delve into C++. So that requirement already exists. reply M4v3R 13 hours agoparentprevI agree with you on the easy part, but it’s definitely not as fast. In JS you get instant hot code reload and even without that the interpreter starts up pretty fast. In comparison Rust takes a while to recompile even with simple changes, and if you have more changes in many files (eg. switching between branches) then it’s really slow. reply sksxihve 4 hours agorootparentI didn't say compiling was fast, though compiling go is pretty fast. I also don't think anyone is arguing that tools need to be written in AoT languages, if you or anyone want to use js and js tools go for it. I think having more choices is a good thing, and sometimes rewriting something from scratch will result in a cleaner/better version. The community at large is going to decide which tooling becomes the standard way to do it, so the author should make an argument on why the js tooling is better instead of weak statements like the one I quoted. reply mtndew4brkfst 5 hours agorootparentprevThis matters, a lot, for contributing to the tool authored in Rust. But for merely installing and benefiting from it, compile-time is largely a one-time cost that pays returns quite soon. reply timeon 20 hours agoparentprevI wonder if author is aware that Node.js is not written in JavaScript. reply jbreckmckye 9 hours agorootparentProbably: he is a contributor to Servo reply claytongulick 3 hours agorootparentprevIt depends. A lot of it absolutely is, I've been through a ton of that source. Low level stuff is mostly c++ to talk to v8 or do system calls, talk to libuv, etc... but even that stuff has a bunch of js to wrap and abstract and provide a clean DX. reply agentultra 6 hours agoprev> a rewrite is often faster just because it’s a rewrite – you know more the second time around I think people often overlook this factor when doing rewrites and making big claims about the results. Chances are if you’d done the rewrite in the same language you’d get similar results. I don’t know if it’d be possible to empirically prove that. I’ve only seen it happen a few times. reply ipnon 6 hours agoparentIt’s like saying you’ll rewrite Romeo and Juliet because you already know they die at the end. It’s a little more nuanced than that! reply agentultra 5 hours agorootparentYes, I agree it is nuanced. I have a particular stereotypical programmer in mind. The one that rewrites their entire program in X, because it's fast. Not because they understand the data dependencies and run-time performance characteristics of their program. Typically these folks misattribute the performance gains they experience in such projects to the language itself rather than the tacit knowledge they have of the original program. reply acureau 1 hour agorootparentAt the end of the day switching from an interpreted language to a natively compiled language will result in a faster program. Of course there are performance gains to be had refactoring with a deeper understanding of the problem. That might be enough in many cases, but if the primary goal is speed the language cannot be ruled out. reply williamstein 17 hours agoprevI recently discovered Rspack, which is a compatible rewrite of Webpack in Rust by a guy at ByteDance. It is genuinely 5x-10x faster across the board on my large/complicated project. I've been using Webpack for 8 years, and I was absolutely blown away to be able to easily swap Webpack out for something so similar (written in Rust) and get such a massive performance improvement. This has made my life so much better. reply okaleniuk 3 hours agoprevWe used to rewrite Python code in C++ \"for performance\". We stopped when the equivalent rewritten version appeared 3 times slower than the original. The very notion of \"fast and slow languages\" is nonsense. A language is just an interface for a compiler, translator, or interpreter of some sort. A language is only steer wheel and pedals, not the whole car, so the whole arguments which one is faster is stupid. In our case, AOT compilation backfired. We used (contractually had to) support older architectures and our Eigen built with meager SSE2 support couldn't possibly outrun Numpy built with AVX-512. So we stopped rewriting. And then Numba (built on the same LLVM as clang) came up. And then not one but several AOT Python compilers. And now JIT compiler is in the standard Python. reply jart 3 hours agoparentAnd Numpy can't come anywhere close to the performance of my C++ code. Last time I benchmarked my CPU matrix multiplication algorithm, it went 27x faster than Numpy. Mostly because Numpy only used a single core. But this was a machine with eight cores. So my code went at least 3x faster. Moral of the story: C++ isn't something you can just foray into whenever Python is slow. It's the most complicated language there is, and the language itself is really just the tip of the iceberg of what we mean when we talk about C++. Do not underestimate the amount of devotion it takes get results out of C++ that are better than what high level libraries like Numpy can already provide you. https://justine.lol/c.jpg reply klodolph 4 hours agoprevSure, but ESBuild is here, it works, and the subjective speed improvement is just fucking massive. I’m sure you could get something with similar performance in JS. I’ve messed around with JS daemons, so you don’t care about startup time for programs like tsc and whatnot. The problem is that it’s just a pain in the ass to get any of this to work, whereas ESBuild is just fast. Maybe these problems with JS will get solved at some point, because we haven’t exhausted all of the possibilities for making JS faster (like the author says). However, when you write the tools in Rust or Go or whatever, you get a fast tool without trying very hard. reply bionhoward 5 hours agoprevOne part I took issue with is “elite priesthood of Rust and Zig developers” .. I love Rust and hope everyone working on interpreted / high level / “easy” languages knows Rust is accessible and doable for most developers, especially in sync land. You can benefit from 1000x (!) speed ups just rewriting sync Python in sync Rust, in my measured experience, because the compiler helps exponentially more the more abstract your code is, and Rust can absolutely do high level systems. The main blocker is when you’re missing some library because it doesn’t exist in Rust, but that’s almost always a big opportunity for open source innovation reply cxr 4 hours agoparent> exponentially more No. The word means something. It's bad enough when it gets misused colloquially e.g. by folks on Twitter and clueless podcasters trying to spice up their talking points, but in a thread like this one, it has no place getting dropped into the discussion except if talking about something that actually fits an exponential curve. reply VeejayRampay 5 hours agoparentprevmost of the python code that would benefit from goign faster is already written to use libraries written in C anyway, I doubt you would get x1000 out of them like that (I'd be happy to be proven wrong if you have examples though) reply from-nibly 20 hours agoprev> It’s very forgiving of types I lost you here. JavaScript doesn't work around type issues, no language really can. It just pushes the type issues to a later time. reply nickserv 9 hours agoparentIndeed, and later on in the same sentence: > this is one reason I’m not a huge TypeScript fan Sorry but TS is the only thing making the JS ecosystem palatable. So many bugs caught thanks to typing... reply greybox 8 hours agoprevI kind see the points he´s making, however I think there's something subtle here that's worth talking about: > Rather than empowering the next generation of web developers to achieve more, we might be training them for a career of learned helplessness. Imagine what it will feel like for the average junior developer to face a segfault rather than a familiar JavaScript Error. I feel this slightly misses the point. We should be making sure that the next generation of Software Engineers have a solid grounding in programing machines that aren't just google's V8 Javascript Engine, so that they are empowered to do more, and make better software. We should be pushing people to be more than just Chrome developers. Also, while I understand what the author is getting at, referring to lower level developers as demigods is a little unhelpful. As someone who switched careers from high-level languages to a C++ engineer, I can attest to the fact that this stuff is learnable if you are willing to put the time and effort in to learning it. It's not magic knowledge. It just takes time to learn. reply rob74 6 hours agoparentIn my experience with Node, these \"familiar JavaScript errors\" were extremely cryptic and had little to do with the actual issue most of the time. And talking about segfaults is pure FUD - it's not as if a build tool written in another language will throw a segfault at you if your JS code is broken. And if it does, you should file a bug report - same as you would for Node (maybe some JS developers have the knowledge to hunt for bugs in JS-written build tools, but I doubt even those have a particular desire to actually do it). reply prewett 4 hours agorootparentSegfaults are the easy bugs! You get a handy stack trace right where the problem is, all the variables intact, it's great! It's the ones where you mess up a data structure, or overwrite an array that kill you, with the symptom occurring long after the problem was caused. Much like React, in fact, where problems in React usage don't get reported until the event loop, long after your function creating your component has finished executing. So maybe those developers will be right at home after all. reply bryanrasmussen 21 hours agoprevthe big issue here is the debuggability by having all your dependencies in the same language, and it's not even like these rewrites will all be in the same performant language for you to learn, so essentially if you are using a wasm compiled dependency you are not likely to be able to go into that dependency's code and figure out where the library author has messed up or what you have misunderstood from the documentation. reply Kinrany 21 hours agoparentThe solution to that is dependencies that work reply bawolff 21 hours agorootparentDoes the dependency that always works and has no bugs also come with a free rainbow and unicorn? reply bryanrasmussen 21 hours agorootparentI sure hope so - otherwise how would I know the dependency I installed was going to always work? reply worthless-trash 11 hours agorootparentAsk the unicorn. reply bryanrasmussen 11 hours agorootparentexactly, which is why I was hoping I would get one? reply a_wild_dandan 20 hours agoprevI’m continually surprised at JavaScript’s speed. Seeing JS sometimes nipping at the heels of C/rust/etc in performance benchmarks blows me away. V8 is such an incredible piece of engineering. In my work, it’s hard to justify using something other than JS/TS — incredible type system, fast, unified code base for server/mobile/web/desktop, world’s biggest package ecosystem for anything you need, biggest hiring pool from being the best known language, etc. It’s just such a joy to work with, ime. Full-stack JS has been such a superpower for me, especially on smaller teams. The dissonance between how the silent majority feels about JS (see, e.g the SO yearly survey), vs the animus it receives on platforms like HN is sad. So here’s my attempt at bringing a little positivity and appreciation to the comments haha. reply ivanjermakov 11 hours agoparentI want more people to understand how much people are working on JS performance. Google, Apple, Microsoft, Mozilla - all have incentive to make it as fast as possible. Too bad JS is not the best candidate for many optimizations. I wonder if we'll get to the point of having a compiled version of JS that allows more static optimizations to be done. WebAssembly might occupy that niche if it gets nice standardized runtime. reply andrewstuart 8 hours agorootparentYou could try the quickJS compiler. https://bellard.org/quickjs/quickjs.html#qjsc-compiler-1 reply jmull 4 hours agoprevDon't bother telling me you're rewriting something for performance if you haven't profiled the existing solution and optimized based on that. reply zcmack 3 hours agoprevi share some of this sentiment as well and i think a lot of my hesitance is that these solutions seem born of the popularity of rust. we have had c and c++ for as long as javascript has been a full-stack workhorse. is it just the barrier of entry / novelty of rust has prompted longtime js devs to make the leap into building tooling? along with it, it seems the \"new framework every week\" jab at javascript can be applied to the build system as well. in any case, i welcome the speed improvements and this certainly does not preclude me from using these new tools where i'm able. reply afavour 5 hours agoprevI have sympathy with this viewpoint... but only to a certain limit. I've optimized JS codebases before now and ended up reaching for typed arrays, arraybuffers and even workers to get things running in parallel and yeah, it's possible. But I'd much rather just be doing in Rust or a similar language. And now that WASM is a realistic possibility I can. reply Jcampuzano2 5 hours agoparentThe interesting part is that often you don't even need to parallelize when using a language like Go or Rust to see a speedup when compared to JS. You could start by writing everything super simple and straightforward, completely sync and already see a multiplier on the speed just due to the language being that much faster. reply showsomerespect 4 hours agoprevRunning a blank .js file in node took 66 milliseconds. An optimized binary I wrote in rust takes 2 milliseconds to execute. So, I think there's a cap there on how fast JavaScript tools can be reply WorldMaker 1 hour agoparentThis is addressed in the article. Node doesn't cache an JS compilation data by default. There's an environment flag to turn that on, and the startup time drastically reduces with it on (on the second+ run of the file). Also both Deno and Bun have more optimized startup times in general by default, some of that startup time is just Node, not a reflection of the language itself. reply noname120 20 hours agoprev> I just don’t think we’ve exhausted all the possibilities of making JavaScript tools faster Rewriting in more performant languages spares you from the pain of optimization. These tools written in Rust are somehow 100× as fast despite not being optimized at all. JavaScript is so slow that you have to optimize stuff, with Rust (and other performant languages) you don't even need to because performance just doesn't bubble up as a problem at all, letting you focus on building the actual tool. reply dan-robertson 20 hours agoparentI think there’s a lot of bias in the samples one tends to see: - you’re less likely to hear about a failed rewrite - rewrites often gain from having a much better understanding of the problem/requirements than the existing solution which was likely developed more incrementally - if you know you will care about performance a lot, you hopefully will think about how to architect things in a way that is capable of achieving good performance. (Non-cpu example: if you are gluing streams of data with processing steps together, you may not think much about buffering; if you know you will care about throughput, you will probably have to think about batching and maybe also some kind of fan-out->map->fan-in; if you know you will care about latency you will probably think about each extra hop or batch-building step) - hopefully people do a bit of napkin math to decide if rewriting something to be faster will achieve the goals, and so you only see the rewrites that people thought would be beneficial (because eg you’re touching a lot of memory so a better memory layout could help) I feel like you’re much more likely to see ‘we found some JavaScript that was too useful for its own good, figured out how to rewrite it with better algorithms/data structures, concurrency, and sims instructions, which we used rust to get’ than ‘our service receives one request, sends 10 requests to 5 different services, collects the results and responds; we rewrote it in rust but the performance is the same because it turns out most of what our service did was waiting’. reply winwang 20 hours agorootparentOnly semi-relevant, but there's also the fact that lower level languages can auto-optimize more deeply -- but that's also more my intuition (would love to get learnt if I'm wrong). For example, I'd expect that Rust (or rustc I guess) can auto-vectorize more than Node/Deno/etc. reply WorldMaker 1 hour agorootparentAhead of Time, perhaps. (Of course the benefit of AOT is that you can take all the time in the world and only slow down the developer cycle without impacting users. In theory you can always build a slower AOT compiler with more optimizations, even/especially for higher level languages like JS. You can almost always trade off more build time and built executable size for more runtime optimizations. High level languages can almost always use Profiler Guided Optimization to do most things low level languages use low level data type optimization paths for.) A benefit to a good JIT, though, is that you can converge to such optimizations over time based on practical usage information. You trade off less optimized startup paths for Profiler Guided Optimization on the live running application, in real time based on real data structures. JS has some incredible JITs very well optimized for browser tab life-cycles. They can eventually optimize things at a low level far further than you might expect. The eventually of a JIT is of course the rough trade-off, but this also is well optimized for a lot of the browser tab life-cycle: you generally have an interesting balance of short-lived tabs where performance isn't critical and download size is worth minimizing, versus tabs that are short-lived but you return to often and can cache compiled output so each new visit is slightly faster than the last, versus a few long-lived tabs where performance matters and they generally have plenty of time to run and optimize. This is why Node/Deno/et al excel in long-running server applications/services (including `--watch` modes) and \"one-off\"/\"single run\" build tools can be a bit of a worst case, they may not give the JIT enough time or warning to fully optimize things, especially when they start with no previous compilation cache every time. (The article points out that this is something you can turn on now in Node.) reply strken 12 hours agoparentprevI really don't think the problem is JavaScript in all these cases. I've seen codebases using webpack where the JS was being run through babel twice in a row, because webpack is a complicated nuisance and nobody on the team had gotten around to fixing it. You can't blame that on V8 or node being slow. reply WorldMaker 52 minutes agorootparentIt's also fascinating how many developers got so burnt on IE11- compatibility issues and feel a need to use Babel as a comfort blanket still. Babel does so very little now with reasonable, up-to-date Browserlist defaults (but still takes a lot of time to do so very little), but the number of developers unwilling to drop Babel from their build pipelines is still to me so surprisingly high. Babel was a great tool for what it did in the \"IE11 is still an allowed browser\" era, but you most probably don't really need it today. reply lenkite 11 hours agoparentprevJavascript should introduce integers and structs and it will have 10-100x the performance it has today without spending another $100 billion on VM optimization. reply jvanderbot 20 hours agoparentprevSomeday soon I hope webasm gets another decent compiled language targeted for JS speedups. Something interoperable with JS. For analogies, look no further than ASM in the early days and the motivations that brought us C, but with the lessons learned as well. Rust is fine for this, except for interoperability. reply metadat 20 hours agorootparentIt looks like Rust can interop with JS via WebASM? https://stackoverflow.com/questions/65000209/how-to-call-rus... reply jvanderbot 20 hours agorootparentOf course that's why we're discussing it. I'm referring to stronger interop like how you can embed ASM in C, and therefore CPP. Like a JS/TS that can have compiled blocks specified in the same language, preferably inline? I'm reaching here. reply peutetre 4 hours agorootparentBut why use JavaScript at all if you can just compile it all to WebAssembly? reply ignoramous 20 hours agoparentprevBeing a statically-typed compiled language has its perks (especially when doing systems programming). Regardless, JS runtimes can and will push forward (like JVM / ART did), given there's healthy competition for both v8 & Node. reply noname120 20 hours agorootparentJavaScript, Python, Lua, I don't see any dynamic language with good performances. Do you have examples? reply EasyMark 15 hours agorootparentJavascript is screamingly fast compared to the vast majority of other dynamic languages (scripting type, not something like Objective C). This is with the V8 engine of course. I’m not sure where you’re getting that it’s slow? reply metadat 20 hours agorootparentprev\"Good\" compared to what? All the mentioned languages keep getting more performant year-over-year, but in the medium future scripting languages are unlikely to reach the performance levels of C, Rust or other low-level languages. Wouldn't it be amazing though? Maybe some combination of JIT and runtime static analysis could do it. Personally, I never assign different types to the same variable unless it's part of a union (e.g. stringHTMLObjectnull, in JS). It would probably require getting rid of `eval' though, which I am fine with. On average, eval() tends to be naughty and those needs could be better met in other ways than blindly executing a string. reply dkersten 20 hours agorootparentprevLua with LuaJIT has pretty good performance. With that said, I spent today writing in C++, so I do agree with the overall sentiment. reply jart 3 hours agorootparentThen normal Lua by itself would probably be fastest for you. Nothing makes me happier than writing native extensions for Lua. Its C API is such a pleasure to work with. With Python and JavaScript, writing native extensions is the worst most dismal toil imaginable, but with Lua it's like eating dessert. So if you know both Lua and C++ really well, then you're going to be a meat eating super programmer who builds things that go really fast. reply BoingBoomTschak 19 hours agorootparentprevCommon Lisp (SBCL)? reply bitwize 13 hours agorootparentDon't forget Scheme (Gambit, Chez, Racket). reply BoingBoomTschak 12 hours agorootparentThat's true, though I'd argue these are not as dynamic. reply evanjrowley 20 hours agoparentprev>JavaScript is so slow that you have to optimize stuff This raises the question, is JavaScript more prone to premature optimization? reply noname120 20 hours agorootparentWell, can we really call it premature optimization if it's needed? reply dpritchett 20 hours agorootparentReminds me of using Ruby ten years ago and having to contend with folks who wanted to default to using the string literal style over another because it was known to be more performant at scale. That awkward stuff surfaces earlier with some languages than with others. reply robinsonrc 16 hours agorootparentprevI guess if folks write JS with the idea that optimisations are needed in mind, then the chances of premature optimisations may go up along with those of the required ones reply __mharrison__ 4 hours agoprevThis is a huge boon for Python. I would blame numpy for Python's popularity today. Writing coffee as fast as c or fortran in Python is awesome (and keeps me employed). reply cincinnatus 3 hours agoprevAs I have (admittedly snarkily) put it many times \"every line of JS is tech debt\" reply peutetre 6 hours agoprevWith WebAssembly getting better both on the client side and the server side, when WebAssembly achieves better performance than JavaScript, and when with WebAssembly you have the opportunity to use almost any other language, why would you use JavaScript? reply bastawhiz 6 hours agoparentOn the server, you wouldn't choose wasm, you'd choose a language that compiles to wasm. Which is really just saying \"choose another language\": there's no point in compiling (e.g.) Rust to wasm just to run it with Node on the server. reply peutetre 6 hours agorootparentYou'd choose wasm on the server if you're using a framework that supports it. For example: https://blog.nginx.org/blog/server-side-webassembly-nginx-un... https://github.com/WebAssembly/wasi-http Write in any language, compile to WebAssembly, have it run on the server no matter what the server's CPU architecture, achieve better performance with high compatibility. reply ndriscoll 3 hours agorootparentA quick skim suggests that this framework re-inititalizes the sandbox on each request (so there's no shared context across requests). That's not going to achieve better performance. reply peutetre 42 minutes agorootparentPhew! Thank goodness you skimmed it! For a minute there I thought the NGINX developers knew something you didn't. reply bastawhiz 6 hours agorootparentprevWhy in the world would you compile to wasm and make your code slower instead of just compiling to native code? To your example: you don't need nginx to talk to wasm, it already talks to your code running on a local socket. The only reason for wasm is portability. If you can't compile your code for the server you're going to be running it on, then the original argument of choosing wasm over JavaScript is already moot. reply peutetre 6 hours agorootparent> instead of just compiling to native code? Because the managed service you signed up to doesn't offer that to you. They like WebAssembly modules because it's a sandboxed runtime. reply bastawhiz 5 hours agorootparentIt's an awfully silly choice to use a managed service that only offers JavaScript and wasm if you don't want to use JavaScript and you care enough about performance that you're willing to accept the fairly meager benefits of wasm over JS. The real reason you'd choose wasm in this case is \"I don't want to use JavaScript and I'm otherwise forced to use this service\". reply peutetre 5 hours agorootparentOr the price is right. Come on down! Use WebAssembly. You'll love it. reply jart 3 hours agorootparentUse Plinko https://github.com/jart/cosmopolitan/tree/master/tool/plinko Back in the 1980s it was my greatest ambition to go on The Price Is Right and play Plinko. However all I could accomplish was making this cursed programming language instead. You'll love it. reply thiht 1 hour agoprevHonestly if you’ve used esbuild (you have if you use Vite) once, you cannot legitimately be skeptical of rewriting JS tools in faster languages. The difference is so huge it’s not even funny. reply metadat 20 hours agoprev> I should also acknowledge: there is a perf hit from using Wasm versus pure-native tools. So this could be another reason native tools are taking the CLI world by storm, but not necessarily the browser frontend. I didn't know about this before, I wonder how much overhead? The reason I am reluctant to rely on JS tools for anything CLI is because of Node.js instability due to version sensitivity and impossible-to-fix-without-reinstalling-the-os low level LibC errors. Compared to go, rust, or python, the odds that any given CLI.js program will run across my (small) fleet of machines is very low, by factor or 10x or more compared to alternatives. Some boxes I don't want to reinstall from scratch every 4 years, they're not public facing and life is too short. reply csomar 8 hours agoparentwasm itself is a bit slower than code compiled for a particular CPU. However, there is significant overhead when it comes to the browser. This is because, now, wasm has to use JavaScript to talk to the browser. The performance gain/loss will depend on the type of operations you are doing. There are plans, however, for wasm to have direct access. reply lenkite 11 hours agoparentprevDeno is fixing this with their standard library and JSR. reply Decabytes 4 hours agoprevHonestly I think the future of languages is strong and sound type inference. Writing Dart, F#, Swift, Crystal, and even modern C# has shown me to varying degrees how good a language can be at type inference, and what the tradeoffs are. I much prefer it to the gradually typed approach, but I've found that library authors in gradually typed languages usually type the entire library and as a developer I always appreciate it. reply qianli_cs 20 hours agoprevI'm not sure if JavaScript supports it, but some Python libraries allow you to choose whether to install a more optimized binary version or the pure Python implementation. For example, if you install psycopg you'll get a pure Python implementation which is easy to debug and hack. But you can also install psycopg[binary] to obtain a faster, compiled version of the library. https://www.psycopg.org/psycopg3/docs/basic/install.html reply jampekka 20 hours agoparentThat typically means two totally different implementations, and pure Python versions are often unusably slow, so it doesn't help much to hack that. reply cyberax 20 hours agoprev> For another thing: it’s straightforward to modify JavaScript dependencies locally. I’ve often tweaked something in my local node_modules folder when I’m trying to track down a bug or work on a feature in a library I depend on. Whereas if it’s written in a native language, I’d need to check out the source code and compile it myself – a big barrier to entry. Yeah, JavaScript is sloppy, but you can always monkey-patch it by modifying tool-controlled files. Great idea. Not. JS is just not a good language. The JIT and the web of packages made it slightly more usable, but it's still Not Good. There's no real way to do real parallel processing, async/await are hellish to debug, etc. It's unavoidable in browsers, but we _can_ avoid using it for tools. Look at Python, a native PIP replacement improved build times for HomeAssistant by an order of magnitude: https://developers.home-assistant.io/blog/2024/04/03/build-i... reply eviks 12 hours agoparent> Whereas if it’s written in a native language, I’d need to check out the source code and compile it myself – a big barrier to entry. Or you could use the source code already downloaded by a package manager and do similar tweaks locally with the build manager picking them up and compiling for you reply GianFabien 11 hours agoprevSeems to me that the article and many of the comments conflate JS with Node. Personally I abhor Node and work with both Bun and Deno. In both cases avoiding the Node compatible bits. reply leerob 17 hours agoprevIf you want to steel man _for_ writing in Rust or Zig or Go, previous discussion here: https://news.ycombinator.com/item?id=35043720 reply musicale 19 hours agoprevJavaScript is a terrific language: more ubiquitous than BASIC ever was; nearly as easy to learn and use as Python; syntax that is close to Java/C/C++. And it only uses 10x the CPU and memory of C or C++. reply simultsop 13 hours agoprevWhile the skeptical's stall in decision making, new comers who do not think twice about rewriting. Rewrite software on a smaller budget in experimental attempts. Then management decides to chose the cheaper option available, because lean startup is promising. Then in next 10 years the new comers become the new skepticals. I don't know if there's a case that this cycle is not repeated, at any time frame. reply gwerbret 13 hours agoparentI've read your comment several times and still don't know what it means. Can you clarify? reply strken 13 hours agorootparentI've rewritten it for clarity: \"While the skeptics stall in decision making, newcomers will not think twice about rewriting software on a smaller budget as an experiment. Then management will decide to choose the cheapest option available, because the lean startup was more competitive. Then in the next 10 years, the newcomers become the new skeptics. In my experience this cycle is always repeated, albeit across different time frames.\" I made the difficult choice to rewrite it in English again, even though French might have been more performant. reply simultsop 11 hours agorootparentThank you so much, you taught me how to express this \"albeit across different time frames\". It is difficult to use any language to simplify complex narrations. reply simultsop 11 hours agorootparentprevImagine developers as the choosing (management) part and if you are involved with webdev in last decade. Compare webpack and vite. As pricing parameter consider the time to bundle (faster=cheaper). So, what webpack did not do by not rewriting itself, vite came in offering. reply actualwitch 5 hours agoprevI was recently watching a talk about uv by one of the developers. One thing he said has really stayed with me after the talk - having tooling that works near-instantaneously unlocks whole new category of experiences. Inspired by that I gave Bun a try to see how modern front-end may look like without all the cruft, and it's just insane how much complexity you can shave off that way. For example, I would never imagine just bundling your whole project in a server handler. But it just works and if not for the lack of client auto refresh when files change (backend rebuilds just fine), I wouldn't even be able to notice the difference between that and dev server. reply bawolff 21 hours agoprevYeah, i agree. I think there is a time where rewriting in a faster language is useful (just like how handcrafted assembly is still a thing), but most of the time you are very far away from the point where that is neccesary. I also think there is an element of, \"rewrite in rust\" is just easy to say, where changing data structures or whatever requires analysis of the problem at hand. reply MrHamburger 20 hours agoparentIt is analogues discussion to C vs Rust. Sure Rust is memory safe, but whole ecosystem I am using today is C based. Compiler, SDK, drivers, RTOS, ... Nobody sane is going to rewrite it for the sake of rewriting it into a different language. reply bawolff 12 hours agorootparentI would disagree with that comparison. Rust really does provide an improvement in memory safety that is hard to achieve by other means. That's not to say you should always rewrite in rust, there are plenty of situations where that doesn't make sense. However its not analgous to the performance situation in my opinion. reply paulddraper 20 hours agoprevThe biggest reason to be skeptical is that these tools are not open to extension in the same way that JavaScript is. Webpack has an enormous community of third-party plugins, it would be very hard to do something similar with e.g. Go or Zig. reply klabb3 20 hours agoparentRight, because tooling is standardized in eg Go. There’s no custom build pipeline, transpilation hell, or experimental language features that are selectively enabled randomly. I’m not even against JS, like at all, and I think the majority of perf issues can be resolved. However, JS tooling is the prime example of where things get truly nightmarish from a software development perspective. Webpack being a perfect example of this horror. reply postalrat 20 hours agorootparentIt's also a breeding ground where the best ideas often end becoming a sort of standard not only for javascript devs but for other langauges as well. reply FridgeSeal 19 hours agorootparentWell curious as to what some of these ideas might be. NPM has done a pretty great job of showing everyone else what to avoid doing. The mere mention of “web pack” sends most of the FE devs I’ve met into borderline trauma flashbacks. There’s seemingly half a dozen package managements tools, some of which also seem to be compilers? There’s also bundlers, but again some of these seem integrated. Half of the frameworks seem to ship their own tools? reply paulddraper 17 hours agorootparentprevYou can ship a 20MB Go program and no one blinks. Go programs start at 20MB. The Go AWS Terraform provider is something like 300MB. A massive amount of the complexity/difficulty in webdev build tools space has to with optimizing delivery sizes on the web platform. Node.js tooling is straightforward comparatively. reply paulddraper 1 hour agorootparentTo be clear: Server-side Node.js tooling is relatively simple. It the web tooling (Webpack, etc) that is complicated. reply tightbookkeeper 20 hours agoparentprevThis is funny to me. Go and zig are built with the Unix shell in mind - the most extensible and modular system around. The webpack ecosystem on the other hand is it’s own OS. reply dpritchett 20 hours agorootparentMaybe for some the appeal of JS is in (hopefully) never having to learn Unix? I’ve heard several folks say that about Kubernetes, but in my experience the *nix core always resurfaces the second things get weird. reply tightbookkeeper 20 hours agorootparentThat certainly can be a benefit. But as we see here it also limits your thinking to that ecosystem. reply speedypypy 9 hours agoprevI find PyPy gives me enough speedup for my Python scripts to leave it. reply kristianp 15 hours agoprevCan you now write client and server js code without installing npm? I guess you'd be reinventing a lot of wheels that the packages provide. reply al2o3cr 17 hours agoprevIMO the biggest win when Phoenix switched to esbuild wasn't about _speed_ exactly, it was about not having to install&debug things like node-gyp just to get basic asset bundling going. reply anyfoo 21 hours agoprev> I’ve written a lot of JavaScript. I like JavaScript. And more importantly, I’ve built up a set of skills in understanding, optimizing, and debugging JavaScript that I’m reluctant to give up on. It's not that hard to do the same for a less terrible language. Choose something markedly different, i.e. a low level language like rust, and you will learn a lot in the process. More so because now you can see and understand the programming world from two different vantage points. Plus, it never hurts to understand what's going on on a lower level, without an interpreter and eco-system abstracting things away so much. This can then feed back into your skills and understanding of JS. reply captnObvious 20 hours agoparentI think we’re reading too far into the authors impostor syndrome. He’s making contributions in Rust already. His opinion isn’t invalid just because he has a bias, he opens by acknowledging his bias. reply FridgeSeal 21 hours agoparentprevI swear some JS devs will go out of their way to avoid learning anything else, whilst simultaneously and breathlessly espousing that we rewrite everything else in JS. reply tylerchilds 20 hours agorootparenti swear some non js devs will go to extreme lengths to demonstrate solutions that will never run on another machine instead of writing js reply anyfoo 20 hours agorootparentWhy would they never run on another machine? It's not that hard to write portable code, and done very often. Nowadays for example, you rarely ever think about whether you're on arm or x86. If you write non-portal code, there might be an important reason (like writing OS components, which you won't do in JS). reply tylerchilds 20 hours agorootparentalmost every time code doesn’t run on my machine, the root cause is a political disagreement with a c-compiler author three layers below my actual problem. javascript doesn’t have a compiler is my main point. reply FridgeSeal 19 hours agorootparentBit rich to complain about that when all the major browsers have just as significant differences, and that’s before we bring node into the equation, let alone talking about a good 30% of websites I visit with any quantity of JS in them are either perpetually broken in some way, or so jank as to be effectively broken. reply tylerchilds 15 hours agorootparenttotally agreed about all of the above and i take credit for none of that code i write plaintext at uris, progressively enhance that to hypertext using a port with a deno service, a runtime that unifies browser js with non browser js. that hypertext can optionally load javascript and at no point was a compiler required aside from the versioned browser i can ask my customers to inspect or a version of deno we get on freebsd using pkg install. node is not javascript would be my biggest point if i had to conclude why i responded. microsoft failed at killing the web with internet explorer and only switched to google’s engine after securing node’s package manager overtly through github and covertly through typescript. microsoft is not javascript is my final point after circling back to my original point of microsoft is also one of the aforementioned reasoned c-compilers are politically fought over instead of things that just work. reply oneweekwonder 20 hours agorootparentprev> Any application that can be written in JavaScript, will eventually be written in JavaScript. - Jeff Atwood (2007) reply peutetre 4 hours agorootparentThe game is changing with WebAssembly though. Large JavaScript applications are replacing JavaScript bits with wasm: https://web.dev/case-studies/google-sheets-wasmgc Any application that is written in JavaScript will have more and more of it replaced with WebAssembly. reply mardifoufs 20 hours agorootparentprevIt's usually the opposite. And the post is specifically about making JavaScript tools, why would you not expect them to be written in JS? I guess not making tools for say, c# devs in c# would also be bad? reply rbower 18 hours agorootparent> It's usually the opposite. And the post is specifically about making JavaScript tools, why would you not expect them to be written in JS? Take a look at rollup, vite, etc. These tools are essentially replacing webpack, which is written in JS. Modern Rollup (^4) uses SWC (Rust-based bundler), and vite is currently using a mix of esbuild (Go) and Rollup. I think they're switching to SWC in v6 though. The point here is that for certain operations JS is not nearly as fast as lower-level languages like the aforementioned. Bundling is one of those performance-critical areas where every second counts. That said, as a TypeScript developer I agree with the sentiment that JS tools should be written in JS, but this isn't a hard and fast rule. Sometimes performance matters more. I think the reasonable approach is to prefer JS – or TS, same difference – for writing JS tools. If that doesn't work, reach for something with more performance like Rust, Go, or C++. So far I've only had to do the latter for 2 use cases, one of which is hardware profiling. reply FridgeSeal 19 hours agorootparentprevPresumably because, apart from Python (see Ruff, uv, etc) most languages aren’t running into such major issues with their own “self hosted” tooling that it’s worthwhile to rewrite several of them in a completely different language. reply mardifoufs 19 hours agorootparentYes I agree! And JavaScript also isn't really at that point yet. Python is really in a class of its own here... sadly enough. Though I don't see an issue with tools for JS built without JS. It's just that I don't think that it's a bad thing for a JavaScript dev to want the ecosystem around JavaScript to be written in JS. JS is orders of magnitudes faster than python in any case. reply atmavatar 13 hours agorootparentprevIt's funny you mention C# since VS Code is a perfect example of JS devs rewriting existing tools in JS. reply jbreckmckye 9 hours agorootparentprevOP is a Servo contributor reply TacticalCoder 20 hours agorootparentprev> I swear some JS devs will go out of their way to avoid learning anything else, whilst simultaneously and breathlessly espousing that we rewrite everything else in JS. The JStockholm syndrome. reply jauntywundrkind 18 hours agoparentprev> It's not that hard to do the same for a less terrible language. I miss that brief era when coding culture had a moment of trying to be nice, of not crudely shooting out mouths off at each other's stuff crudely. JS, particularly with typescript, is a pretty fine language. There's a lot of bad developers and many bad organizations not doing their part to enable & tend to their codebases, but any very popular language will likely have that problem & it's not the languages fault. It's a weakness & a strength that JS is so flexible, can be so many different things to different people. Even though the language is so so much the same as it was a decade & even two ago, how we use it gone through multiple cycles of diversification & consolidation. Like perl, it is a post-modern language; adaptable & changing, not prescriptive. http://www.wall.org/~larry/pm.html If you do have negative words to say, at least have the courage & ownership to say something distinct & specific, with some arguments about what it is you are feeling. reply anyfoo 17 hours agorootparentI’d normally agree with you, but JS is more or less designed to be terrible. It was hacked together by Brendan Eich in literally 10 days, who originally wanted to do something more Scheme-like. It was a quick and dirty hack that got stretched way beyond what it was even meant for. It then literally had decades of ECMAscript committee effort to shape it into something more useable. I could repeat the numerous criticisms, but there’s enough funny videos about it that make a much better job pointing out its shortcomings and, sometimes, downright craziness of it. > but any very popular language will likely have that problem & it's not the languages fault. No, sorry, just no. I get where you are coming from, but in the case of JavaScript, its history and idiosyncrasies alone set it apart from many (most?) other languages. Perl for example was made with love and with purpose, I don’t think it’s comparable. reply sswatson 17 hours agorootparentJS wasn’t created in 10 days. It was prototyped in 10 days, and the prototype contained very little of the stuff people complain about. Hillel Wayne posted about this recently: https://www.linkedin.com/posts/hillel-wayne_pet-peeve-people... reply anyfoo 17 hours agorootparentOkay, I stand corrected. So this prototype didn’t ship, or did it ship and evolve? Brendan Eich himself calls JS a “rush job” and with many warts though, having had to add aspects that in retrospect he wouldn’t have. This snippet from your link is consistent with that: Also, most of JavaScript's modern flaws do *not* come from the prototyping phase. The prototype didn't have implicit type conversion (`\"1\" == 1`), which was added due to user feedback. And it didn't have `null`, which was added to 1.0 for better Java Interop. Like many people, I find JS super frustrating to use. reply throwitaway1123 15 hours agorootparentThis anecdote about the double equality operator might have originated from Eich's chat with Lex Fridman where he states (at about 5 minutes and 26 seconds) that during the original 10 day sprint JavaScript didn't support loose equality between numbers and strings: https://www.youtube.com/watch?v=S0ZWtsYyX8E&t=326s The type system was weakened after the 10 day prototyping phase when he was pressured by user feedback to allow implicit conversions for comparisons between numbers and serialized values from a database. So it wasn't because he was rushing, it was because he caved to some early user feedback. reply jauntywundrkind 15 hours agorootparentprevThe implicit type conversion is good for a very funny conference video (\"wat\") but man, it's just so overplayed as a weakness especially versus how much real world impact it has on anyone. And with TypeScript or linting, many of the strange comparison/conversion issues go away. I struggle to find any substantial arguments against the js language, in spite of a lot of strong & vocal disdainful attitudes against it. reply consteval 1 hour agorootparent> I struggle to find any substantial arguments against the js language The biggest problem with JavaScript is that it's an extremely footgunny language. IMO, of the C++ variety, but probably worse. 1. The type system is unsound and complicated. Often times things \"work\" but silently do something unexpected. The implicit type conversion thing is just one example, but I know you've seen \"NaN\" on a page or \"Object object\" on a page. Things can pass through and produce zero errors, but give weird results. 2. JS has two NULLs - null and undefined. The error checking around these is fragile and inherently more complex than what you'd find in even C++. 3. JS has an awful standard library. This is footgunny because then basic functionality needs to be reimplemented, so now basic container types have bugs. 4. JS has no sane error handling. Exceptions are half-baked and barely used, which sounds good until you remember you can't reliably do errors-as-values because JS has no sane type system. So it's mostly the wild wild west of error handling. 5. The APIs for interacting with the DOM are verbose and footgunny. Again things can look as though they work but they won't quite. We develop tools like JSX to get around this, but that means we take all the downsides of that too. 6. Typescript is not a savior. Typescript has an okay-ish type system but it's overly complex. Languages with nominal typing like C# are often safer (no ducks slipping through), but they're also easier to work with. You don't need to do type Olympics for most languages that are statically typed, but you do in TS. This also doesn't address the problem of libraries not properly supporting typescript (footgun), so you often mix highly typed code with free-for-all code, and that's asking for trouble. And it will become trouble, because TS has no runtime constraints. reply anyfoo 12 hours agorootparentprevThe implicit coercion and its weird behavior is absolutely a major footgun, not just fodder for the “wat” video. It’s something that can get you into serious trouble quite easily if left unchecked, for example by just looking at a list wrong. For someone to say that it has never caused them surprising pain in plain JavaScript is probably disingenuous. This is something that most other languages plainly don’t have as a problem, at least not as baffling. Other things worth mentioning are the unusual scoping (by default at least), prototypes, “undefined”, and its role versus \"null\"... the list goes on. I give TypeScript a lot of credit for cleaning up at least some of that mess, maybe more. But TypeScript is effectively another language on top of JS, not everyone in the ecosystem has the luxury of only dealing with it, and across all layers and components. Is my knowledge about JavaScript outdated and obsolete? Certainly. Is the above stuff deprecated and turned off by default now? Probably. I left web development more than 10 years ago and never looked back. I’m a bit of a programming language geek, so I’ve used quite a few languages productively, and looked at many more. But not many serious programming languages have left quite the impression that JavaScript and PHP have. In the meantime, I have always remembered that one conversation I had with someone who was an ECMAscript committee member at that time: They were working really hard to shape this language into something that makes sense and compiles well. Maybe against its will. EDIT: Dear god, I completely forgot about JavaScript’s non-integerness, and its choice of using IEEE 754 as its basic Number type. Is that still a thing? reply jampekka 20 hours agoparentprevHopefully the lot includes that writing stuff in low level languages isn't worth the pain most of the time. reply winwang 20 hours agorootparentCurious what you mean by \"most\" (I'm agnostic/unlearned on the statistics tbh). I \"feel\" like it doesn't happen too often when it's not either already low-level or the supposed extra performance is likely worth it. Like, I can't imagine most people using Javascript would want to rewrite in Rust without some decent reason. reply jongjong 21 hours agoprevI also love JavaScript. It's true, it has some really bad parts but you can avoid them. If I could design the perfect language for myself, it would have the syntax of JavaScript and the portability of JavaScript but it would use Python's strong duck typing approach. reply anyfoo 20 hours agoparentWhat have static type systems ever done to you, that you avoid them so much? reply dbrueck 20 hours agorootparentNot the OP, but the appeal of languages like JS has a lot to do with developer productivity. I write gobs of JS and Python code and the finished programs and libraries can be strongly and statically typed end-to-end. I just don't want to be forced to do it in cases when it doesn't really make a difference, and I don't want to waste time on it when I'm still figuring out the design. My hope is one of the Next Big Things in programming languages is the widespread adoption of incremental typing systems. So during the early stages of dev you get the productivity benefits of dynamic and loose/duck typing as much as you want, and then as the code matures - as the design firms up - you begin layering in the type information on different parts of the program (and hopefully the toolset gives you a jump start by suggesting a lot of this type info for you, or maybe you specify it only in places where the type info can't be deduced). Then those parts of the program (and hopefully eventually the entire program) are strongly and statically typed, and you get all of the associated goodies. reply bryanrasmussen 20 hours agorootparentprevmost static type systems are verbose, probably due to linguistic verbosity, so one obvious thing that static type systems have probably done to a lot of people is given them pain from typing so much. reply anyfoo 20 hours agorootparentI don't feel it's so much typing. Especially for the clarity and, most importantly, safety and correctness I get back. I'd rather type 3 1/2 seconds more than debug a dumb type issue for half an hour. It gets really old to get something like \"NoneType does not have blah\" in a deeply nested, complicated data structure in python, but obviously only at runtime and only in that hard to hit corner case, when all you did is forget to wrap something in the right number of square brackets in some other part of the code. I haven't fully given up on python, but I only deal with it using mypy, which adds static typing, anymore. reply jwells89 20 hours agorootparentA bit of extra verbosity as added by static typing can also be immensely helpful for trawling through and/or learning an unfamiliar codebase, especially in the absence of an IDE or debugging environment (e.g. browsing code on GitHub or in a filemanager). For instance, take function definitions. By just adding types to the function's arguments, you're potentially saving the reader a ton of time and mental overhead since they don't have to chase down the right the chain of function calls to figure out what it is exactly (or is supposed to be) that's getting passed in. reply ggregoire 13 hours agorootparentprevNot sure what languages you are thinking to with \"most static type systems\", but in languages like TypeScript or Rust (and I guess modern Java/C#, haven't touched those in a while), most of the types are inferred by the system such as you don't need to write them. You type your functions arguments (and return values in Rust) and that's about it. reply bryanrasmussen 8 hours agorootparentOk I was thinking Java / C# which I haven't touched in a while either and they were verbose, Typescript may be able to infer types but every place I've used it we write just everything out, and as such there is quite a lot of extra declaring of things that could be inferred, that may be cultural, but it seems pretty ingrained culture. reply singpolyma3 20 hours agoparentprevJavaScript isn't really all that portable? Heck just making it run on the different JS engines and runtimes is a big pain sometimes reply o11c 20 hours agorootparentI can't even figure out how to write typescript that conditionally uses browser-only or node-only libraries depending on which environment it's in. My current best guess is to write 2 completely independent typescript projects that happen to point to the same source files? Let me cross-compile a C++ project any day ... reply Jean-Papoulos 12 hours agoprevThis guy is not competent to talk about what he's talking about. >\"JavaScript is, in my opinion, a working-class language. It’s very forgiving of types (this is one reason I’m not a huge TypeScript fan).\" Being \"forgiving of types\" is not a good thing. There's a reason most \"type-less\" languages have added type hints and the like (Python, Typescript, etc) and it's because the job of a programming language is to make it easier for me to tell the CPU what to do. Not having types is detrimental to that. reply Doxin 12 hours agoparent> There's a reason most \"type-less\" languages have added type hints and the like (Python, Typescript, etc) I would like to clarify that even without typing python is a LOT less \"forgiving of types\" than javascript. It has none of the \"One plus object is NaN\" shenanigans you run into with javascript. reply tugberkk 11 hours agorootparentsure. one is strongly typed and the other weakly typed. reply makapuf 12 hours agorootparentprevTypes are guidelines and strictly useful and a good thing. That said, one can wonder why languages like basic, python, scheme or php (dynamic, implicit types) have grown popular. Maybe for bad reasons but there IS an added value for implicit types. C++ (maybe even C !) has grown the auto keyword and other typed language have type inference. Which is not the same as \"typeless\" (it always is typed) but it defeats one of the \"double check\" security of types. And it's sometimes not needed (yes, if I initialize it with \"abc\" it may be a string) reply graemep 10 hours agoparentprevPython is not \"type-less\" it is strongly typed. It will raise a TypeError if you do something like 1 + \"1\". reply wesselbindt 9 hours agoparentprevIt's not forgiving of types at all. Reality is not forgiving of type errors. The only thing JavaScript does is move the moment where you find out reality is not forgiving of type errors to when your code is running in prod rather than at compile time, and makes them more implicit. That doesn't make it a bad thing per se to be forgiving of type errors. For example, if you really like fixing errors in production rather than before pushing them to production, this faux forgiveness is precisely what you should be looking for. It's all up to personal preference. Personally, I prefer knowing early on if there's problems with my code, and having a higher degree of certainty regarding it working or not. All of this is under the assumption that whatever you're writing has some degree of complexity to it (an assumption which is satisfied very quickly). Five line python glue scripts don't necessarily benefit from static typing. reply mkl 8 hours agoparentprev> the job of a programming language is to make it easier for me to tell the CPU what to do. Not having types is detrimental to that. JavaScript and Python have types, and Python has always been strongly typed (type hints have not changed that). Neither TypeScript or Python use type hints at runtime to help tell the CPU what to do. What type hints in these languages do is make it easier for you to describe more specifics of what your code does to your tooling, your future self, and other programmers. reply gagaq 10 hours agoparentprevPlease, preferring dynamic typing is not a sign of \"incompetence\". Stop this nonsense. Also, I won't question your competence because you called Python and JavaScript \"type-less\". The type-less languages (other than assembly) that were ever used were BCPL and B (predecessors of C). reply camgunz 11 hours agoparentprevThere are plenty of good things written in languages with weaker type systems than TypeScript (Linux, your browser, HN). Using C/C++ or a dynamic language doesn't immediately make you incompetent. reply thefroh 12 hours agoparentprevwhile I'm a fan of TypeScript and using type hints in Python from an autocomplete and linting perspective, I am curious... ... has either language leveraged these to better tell the CPU what to do? presumably for perf. reply yurishimo 11 hours agorootparentPHP does but the types actually mean something. If your types can be stripped out to make the program run, I have a hard time believing that there is any optimization occurring there. reply iforgotmysocks 11 hours agorootparentprevpython ignores type hints reply stabbles 9 hours agoprevWhat makes me skeptical is reading > I just don’t think we’ve exhausted all the possibilities of making JavaScript tools faster and then > Sometimes I look at truly perf-focused JavaScript, such as the recent improvements to the Chromium DevTools using mind-blowing techniques like using Uint8Arrays as bit vectors, and I feel that we’ve barely scratched the surface. Bit vectors are trivial? I think the author is too ignorant about those \"faster languages\". Sure, maybe you can optimize javascript code, but the space of optimizations is only a small subset of what is possible in those other languages (e.g. control over allocations, struct layout, SIMD, ...) reply gagaq 5 hours agoparentIf JS runtimes take type hints into consideration, then they can be much faster. SIMD is not really hard to support, though value types can be hard to retrofit. reply jpalawaga 21 hours agoprevAnyone who has done a programming contest, advent of code, etc knows that the language doesn’t matter so much as your algorithm. Yes, the language can bring a nice speed up, or might give you better control of allocations which can save a lot of time. But in many cases, simply picking the correct algorithm will deliver you most of the performance. As someone who doesn’t JavaScript a lot, I’d definitely prefer a tool written in go and available on brew over something I need to invoke node and its environment for. reply xlii 12 hours agoparent> Anyone who has done a programming contest, advent of code, etc knows that the language doesn’t matter so much as your algorithm. This is one of the biggest falsehoods in the software engineering I know. Language is a collaboration glue and influences way of thinking guiding solution development. As an analogy: you can make a statue from a glass or from ice, but while both can be of the same shape and be equally awed upon, the process and qualities will differ. For the prototypes and throwaways context doesn’t matter - That’s why all short lived contests, golfs and puzzles ignore it. Yet, when software is to be developed not over the week but over the decades and (hopefully) delivered to thousands if not millions of computers it’s the technological context (language, architecture, etc.) that matters the most. reply tyree731 21 hours agoparentprevLots of very smart people have worked very hard on Python tools written in Python, yet the rust rewrites of those tools are so much faster. Sometimes it really is the programming language. reply jampekka 20 hours agorootparentPython is really really slow compared to JS though. reply kukkamario 13 hours agorootparentNode is so slow to start that python script can complete before Javascript even begins to execute. reply jampekka 9 hours agorootparentFor extremely simple scripts maybe. I get around 70 ms difference in startup time. $ time python3 -c \"print('Hello world')\" Hello world real 0m0.017s $ time node -e \"console.log('Hello world')\" Hello world real 0m0.084s reply zeroonetwothree 20 hours agorootparentprevI once worked on a Python system that had 50 machines dedicated to it. We were able to rewrite it in a more performant language such that it easily ran on one machine. This also allowed us to avoid all the issues distributed systems have. So yeah, Python is not great for systems programming reply gaganyaan 20 hours agorootparentprevCPython is (though it's slowly getting better). Pypy is amazingly fast reply charrondev 20 hours agorootparentprevIn the JavaScript world a lot of speed up comes from 3 major things as far as I can tell: - easier concurrency. - the fact that things are actually getting rewritten with the purpose of speeding them up. - a lot of the JS tooling getting speedups deals with heavily with string parsing, tokenizing, generating and manipulation of ASTs. Being able to have shared references to slices of strings, carefully manage when strings are copied, and have strict typing of the AST nodes you enable things to be much faster than JavaScript. reply jvanderbot 20 hours agorootparentprevThis is a very nice counterexample, but it's not actually a counter example without an example. Also, this was a thing before Rust. I've rewritten several things in C or Cpp for python back ends, and most pytbon performance-critical code is already an API to a shared library. You'd be surprised to run OR tools and find Fortran libraries loaded by your python code. reply runesoerensen 20 hours agorootparentRuff is one example https://astral.sh/ruff reply aragilar 10 hours agorootparentBut can I write plugins for it? My understanding it is only implements a subset of the common plugins (and does not do any of the linting that pylint is useful for), so it avoids scanning the filesystem for plugins? reply anyfoo 21 hours agorootparentprevChoosing the right algorithm effectively means optimizing runtime complexity. Then, once runtime complexity is fixed with the right algorithm, you're still left with a lot of constant factors that O-notation deliberately ignores (it's only about growth of the runtime). Sometimes, optimizing those constant factors can be significant, and then the choice of language matters. And even some details about the CPU you are targeting, and overall system architecture. reply o11c 20 hours agorootparentOften languages like Javascript and Python don't allow optimal runtime complexity, because the types baked in to external interfaces fundamentally disallow the desired operation. And these languages are too slow to rewrite the core logic in the language itself. (but of course, the vast majority of the code, even in widely used tools, isn't properly designed for optimization in the first place) I only dabble in javascript, but `tsc` is abominable. reply worik 20 hours agorootparentprev> Lots of very smart people have worked very hard on Python tools written in Python Yes, I agree that is very sad Python is achingly slow. I know the Python people want to address this, I do not understand. Python makes sense as a scripting/job control language, and execution speed does not matter. As an application development language it is diabolical. For a lot of reasons, not just speed reply bsder 19 hours agorootparentprev> Lots of very smart people have worked very hard on Python tools written in Python, yet the rust rewrites of those tools are so much faster. So? Some tool got written and did its job sufficiently well that it became a bottleneck worth optimizing. That's a win. \"Finishing the task\" is, by far, the most difficult thing in programming. And the two biggest contributors to that are 1) simplicity of programming language and 2) convenience of ecosystem. Python and Javascript are so popular because they tick both boxes. reply tyree731 18 hours agorootparentDon’t disagree about finishing the task, but personally I don’t find more performant languages any less productive for the sort of programming I tend to do. reply bsder 18 hours agorootparentCongratulations on being a programming god. This discussion isn't for you. From my point of view, I'm happy if I can convince my juniors to learn a scripting language. Okay? I don't care which one--any one. I'd prefer that they learn one of the portable ones but even PowerShell is fine. I have seen sooooo many junior folks struggle for days to do something that is 10 lines in any scripting language. Those folks who program but don't know a scripting language far outnumber the rest of us. reply fredrikholm 12 hours agorootparent> I have seen sooooo many junior folks struggle for days to do something that is 10 lines in any scripting language. > Those folks who program but don't know a scripting language far outnumber the rest of us. What domain are you in? This sounds like the complete inverse of every company I've ever worked at. Entire products are built on Python, Node ect, and the time after the initial honeymoon phase (if it exists) is spent retrofitting types on top in order to get a handle, any handle, on the complexity that arises without static analysis and compile time errors. At around the same time, services start OOM'ming left and right, parallellism=1 becomes a giant bottleneck, JIT fails in one path bringing the service performance down an order of magnitude every now and then etc... > Congratulations on being a programming god. This discussion isn't for you. On the behalf of mediocre developers everywhere, a lot of us prefer statically typed languages because we are mediocre; I cannot hold thousands of implicit types and heuristics in my head at the same time. Luckily, the type system can. reply Ferret7446 19 hours agoparentprevQuite the opposite, for most cases you don't hit the scale where asymptotic algorithmic performance really makes a big impact (e.g., for many small set comparisons, iterating over a list is faster than a hash set, but only by 10-50% or so), vs switching to a compiled language which instantly gets you 10x to 100x performance basically for free. Or perhaps another way to look at it, if you care enough about performance to choose a particular algorithm, you shouldn't be using a slow language in the first place unless you're forced to due to functional requirements. reply hawski 20 hours agoparentprevIn higher level languages your code may allocate memory or trigger a GC pass or other smartness in unexpected places. This may cause slowdowns you may not have control over or may change from version to version or vendor to vendor. It is often easier to manage in \"faster\" languages. Good algorithm may not be enough. reply zeroonetwothree 20 hours agoparentprevThe types of problems in those contests are meant to highlight algorithms. In the real world you might have a trivial algorithm but a huge input size, where the constant factor matters much more. reply rk06 4 hours agoparentprevI have done so, and one has to be out of their mind to attempt in java instead of c++ in those contests. reply eviks 12 hours agoparentprevAnd anyone who expands the horizon to the real world, instead of focusing on the artificial one of contests, knows that the language matters a great deal reply moomin 20 hours agoparentprevHere’s the thing: languages like C#, Java and Rust all have extensive libraries and packages that implement many common data structures and algorithms well. With all due respect to the incredible work that goes into projects like lodash, JavaScript does not. (Nor does C, for that matter.) reply CrimsonRain 11 hours agoparentprevThat's why almost every thing important in python is in C reply tightbookkeeper 20 hours agoparentprev> knows that the language doesn’t matter so much as your algorithm. I know what you’re referring to but these problems have also taught me a lot about language performance. python and JS array access is just 100x slower than C. Some difficult problems become much harder due to this limitation. reply jampekka 20 hours agorootparentJS array access is a lot faster than Python array access. JS is easily within magnitude of C and can be even about as fast with typed arrays or well JITable code. reply tightbookkeeper 20 hours agorootparent> JS is easily within magnitude of C Typed arrays help a lot, but I’m still doubtful. Maybe it all the processing is restrict to idioms in the asm.js subset? And even then you’re getting bounds checking. reply jampekka 19 hours agorootparentIn benchmarks JS is usually well within magnitude (i.e. under 10x slower). E.g. C++ vs Node.js here: https://benchmarksgame-team.pages.debian.net/benchmarksgame/... Couldn't find C vs JS easily with the new benchmarksgame UI. reply igouy 19 hours agorootparent> Couldn't find C vs JS easily Try: A) Find JS in the box plot charts https://benchmarksgame-team.pages.debian.net/benchmarksgame/... or B) Find JS in the detail https://benchmarksgame-team.pages.debian.",
    "originSummary": [
      "Nolan Lawson questions the trend of rewriting JavaScript tools in languages like Rust, Zig, and Go, suggesting JavaScript is often sufficiently fast.",
      "He points out that performance improvements can be made by optimizing current JavaScript tools, leveraging its bytecode cache and JIT (Just-In-Time) compiler.",
      "Lawson warns that using newer languages might create a divide, limiting accessibility and debuggability, and potentially alienating average developers from contributing."
    ],
    "commentSummary": [
      "The debate centers on whether rewriting JavaScript tools in faster languages like Rust or Go is beneficial, with some arguing JavaScript's speed is sufficient while others see performance gains in using statically typed languages.",
      "Rewriting in languages like Rust or Go can lead to significant performance improvements, as demonstrated by tools such as esbuild.",
      "The discussion also considers the ease of modifying JavaScript dependencies and the familiarity of the language, weighing these against the potential benefits of using faster languages."
    ],
    "points": 147,
    "commentCount": 265,
    "retryCount": 0,
    "time": 1729459502
  }
]
