[
  {
    "id": 41510252,
    "title": "We spent $20 to achieve RCE and accidentally became the admins of .mobi",
    "originLink": "https://labs.watchtowr.com/we-spent-20-to-achieve-rce-and-accidentally-became-the-admins-of-mobi/",
    "originBody": "By — Benjamin Harris — Aliz Hammond — Sep 11, 2024 We Spent $20 To Achieve RCE And Accidentally Became The Admins Of .MOBI Welcome back to another watchTowr Labs blog. Brace yourselves, this is one of our most astounding discoveries. Summary What started out as a bit of fun between colleagues while avoiding the Vegas heat and $20 bottles of water in our Black Hat hotel rooms - has now seemingly become a major incident. We recently performed research that started off \"well-intentioned\" (or as well-intentioned as we ever are) - to make vulnerabilities in WHOIS clients and how they parse responses from WHOIS servers exploitable in the real world (i.e. without needing to MITM etc). As part of our research, we discovered that a few years ago the WHOIS server for the .MOBI TLD migrated from whois.dotmobiregistry.net to whois.nic.mobi – and the dotmobiregistry.net domain had been left to expire seemingly in December 2023. Putting thoughts aside, and actions first, we punched credit card details as quickly as possible into our domain registrar to acquire dotmobiregistry.net - representing much better value than the similarly priced bottle of water that sat next to us. Our view was that as a legacy WHOIS server domain, it was likely only used by old WHOIS tools (such as phpWHOIS, which conveniently has an Remote Code Execution (RCE) CVE from 2015 for the parsing of WHOIS server responses – thus fitting our aim quite nicely). Throwing caution into the wind and following what we internally affectionately refer to as our 'ill-advised sense of adventure' - on Friday 30th August 2024 we deployed a WHOIS server behind the whois.dotmobiregistry.net hostname, just to see if anything would actually speak to it actively. The results have been fairly stunning since - we have identified 135000+ unique systems speaking to us, and as of 4th September 2024 we had 2.5 million queries. A brief analysis of the results showed queries from (but certainly not limited to): Various mail servers for .GOV and .MIL entities using this WHOIS server to presumably query for domains they are receiving email from, Various cyber security tools and companies still using this WHOIS server as authoritative (VirusTotal, URLSCAN, Group-IB as examples) However, significant concern appeared on 1st September 2024 when we realised that numerous Certificate Authorities responsible for issuing TLS/SSL certificates for domains like 'google.mobi' and 'microsoft.mobi', via the 'Domain Email Validation' mechanism for verifying ownership of a domain, were using our WHOIS server to determine the owners of a domain and where verification details should be sent. We PoC'd this with GlobalSign and were able to demonstrate that for 'microsoft.mobi', GlobalSign would parse responses provided by our WHOIS server and present 'whois@watchtowr.com' as an authoritative email address. Effectively, we had inadvertently undermined the CA process for the entire .mobi TLD. As is common knowledge, this is an incredibly important process that underscores the security and integrity of communications that a significant amount of the Internet relies upon. This process has been targeted numerous times before by well-resourced nation-states: Hack Obtains 9 Bogus Certificates for Prominent Websites; Traced to Iran State-sponsored hackers in China compromise certificate authority While this has been interesting to document and research, we are a little exasperated. Something-something-hopefully-an-LLM-will-solve-all-of-these-problems-something-something. As always, we remind everyone - if we could do this, anyone can. Onto the full story... Setting The Scene We're sure you’re familiar with the old adage, ‘it never rains but it pours’. That was definitely the case here, where we set out with the intention of just getting some RCE’s to fling around, and ended up watching the foundation of secure Internet communication crumble before our eyes. Before we get ahead of ourselves, though, let’s start at the beginning, in which we decided to take a quick look at a WHOIS client. The protocol being some 50+ years old, we expected WHOIS clients to be constructed with the same brand of string as an enterprise-grade SSL VPN appliance, and so we took a naive shot and served up some A’s. # python3 -c \"printf( 'Domain Name: ' + 'A' * 3000)\"nc -w1 -l whois Haha, we were right. Funny. This, at first glance, looks like an easily-exploitable crash. We were keen to find more bugs, and keenly started examining some other client implementations - but we were soon interrupted by some vocal killjoys naysayers. They were quick to remind us that, to get to this state in our lab environment, we’d impersonated a WHOIS server, redirecting traffic from the usual server to our test server via iptables. How realistic was this attack scenario, the naysayers asked? We tried to silence the killjoy's naysayers and convince them our attack was plausible - we could find a registrar that allows us to set a Referral WHOIS value, or buy an IP range and control the range ourselves - but they suggested we spend more time doing, and less time playing academia. The reality was that in order for an attacker to carry out an attack against a WHOIS client, they’d need one of the following: A Man-In-The-Middle (MiTM) attack, which requires the ability to hijack WHOIS traffic at the network layer - out of reach for all but the most advanced of APTs, Access to the WHOIS servers themselves, which is plausible but unlikely, or A WHOIS referral to a server they control. These are effectively the preconditions of a nation-state or someone who is very comfortable compromising global TLD WHOIS servers in pursuit of exploiting clients. You would, at this point, be forgiven for thinking that this class of attack - controlling WHOIS server responses to exploit parsing implementations within WHOIS clients - isn’t a tangible threat in the real world. We were left unsatisfied. We had located some shoddy code, but declaring it out of reach sounded like something you might bill a day rate for. Perhaps there was another avenue for attack? Collateral Damage In Pursuit Of RCE The key to turning this theoretical RCE into a tangible reality is rooted in the tangled mess of the WHOIS system. One of the biggest ‘kludges’ in the WHOIS system is the means of locating the authoritative WHOIS server for a given TLD in the first place. Each TLD (the bit at the end of the domain), you see, has a separate WHOIS server, and there’s no real standard to locating them - the only ‘real’ method being examining a textual list published by IANA. This list denotes the hostname of a server for each TLD, which is where WHOIS queries should be directed. As you can imagine, maintainers of WHOIS tooling are reluctant to scrape such a textual list at runtime, and so it has become the norm to simply hardcode server addresses, populating them at development time by referring to IANA’s list manually. Since the WHOIS server addresses change so infrequently, this is usually an acceptable solution. However, it falls down in an ungraceful manner when server addresses change. With a little bit of legwork, we found that the WHOIS server for a particular TLD - .mobi - had been changed some years ago from the old domain whois.dotmobiregistry.net to a new server, at whois.nic.mobi. Of course though, because the Internet is joined together by literal string and hopes/wishes at this stage, somebody had neglected to renew the old domain at dotmobiregistry.net meaning it was up for grabs by anyone with $20 and an ill-advised sense of exploration. We registered the domain, working on the theory that, while most client tooling would be updated to use whois.nic.mobi, most of the Internet population is still surprised when their 2011 SAP deployment gets popped, and thus WHOIS applications in production had a fairly decent chance of still referencing whois.dotmobiregistry.net. Of course, this being the Internet, we got a little more than we bargained for. So What? It's Old We soon realized the threat model for this attack had just changed. Now that we control a WHOIS server, we were in the position to ‘respond’ to traffic sent by anyone who hadn’t updated their client to use the new address (auto updates are bad, turn them off). No longer do we require a Man-In-The-Middle attack, or some exotic WHOIS referral, to exploit a WHOIS client vulnerability - all we need to do is wait for queries to come in, and theoretically respond with whatever we want. The pre-requisites for real-world exploitation now sat within what we deemed ‘rough reality’. Things were beginning to escalate. We had set out to find some simple bugs in WHOIS client tooling, file for some CVEs, get them fixed.. but then we realised that once again we’d probably chewed off more than we intended and things were about to become worse - much worse. Never Update, Auto-Updates And Change Are Bad Unfortunately, there is a lot of Internet infrastructure which depends on the antiquated WHOIS protocol. Starting off slow, we’re now in a position to attack the many websites that run a WHOIS client and echo the results back to the user, injecting XSS or PHP eval payloads. Ethical (and legal) concerns prevent us from doing so, however - and we did not spend $20 to get an XSS. Of course, our original goal was to find and exploit some 0day in WHOIS clients, or some other system that embeds a WHOIS client (such as a spam filter), similar to the trivial memory corruption we found earlier. Our biggest hurdle here - as alluded to above - was the simplicity of the WHOIS protocol itself, which is a simple text-based TCP data stream. With so little complexity, there seemed very little room for developers to make errors. Ha. Prior Art To fully understand and look to leverage our new capability and adjusted threat model, we decided to examine the area’s ‘prior art’ in exploitation, looking at historic attacks on WHOIS clients. We were somewhat surprised that a search for relevant CVE data yielded relatively few results, which we attributed to the area being under-researched - the search return 26 CVE records. Once we discount the irrelevant results, we are left with only three bugs that are triggered by malformed WHOIS responses. This small number - three bugs since 1999 - makes it obvious to us that very little research has been done - likely due to the perception that any real-world exploitation comes with difficult prerequisites, such as control of a TLD WHOIS server. But, there have been some interesting cases - just to give you a taste of where this is going. phpWHOIS (CVE-2015-5243) The first bug that our retrospective found was CVE-2015-5243. This is a monster of a bug, in which the prolific phpWhois library simply executes data obtained from the WHOIS server via the PHP ‘eval’ function, allowing instant RCE from any malicious WHOIS server. The vulnerable code snippet: foreach ($items as $match => $field) { $pos = strpos($val, $match); if ($pos !== false) { if ($field != '') { $var = '$r' . getvarname($field); $itm = trim(substr($val, $pos + strlen($match))); if ($itm != '') eval($var . '=\"' . str_replace('\"', '\\\\\\\\\"', $itm) . '\";'); } if (!$scanall) break; } } What’s going on here? The important item is the juicy eval statement in the middle of the snippet, which is fed data returned from the WHOIS server. While it attempts to escape this data before it evaluates it, it does so imperfectly, only replacing \" with the escaped form, \\\\\\\\\" . Because of this, we can sneak in our own PHP code, which is then executed for us. Netitude’s blogpost lays out all the details, and even provides us with exploitation code - ”;phpinfo();// - is enough to spawn a phpinfo page. We tried this out on an application that uses phpWhois, purely to demonstrate, and it worked swimmingly: https://labs.watchtowr.com/content/images/2024/08/image-5.png Clearly this is a powerful bug - the best part being that phpWhois hardcodes our newly found whois.dotmobiregistry.net in vulnerable versions (it's old, but at a cursory glance no-one appears to have ever updated phpWhois). What other historic artefacts could we find, though? Fail2Ban (CVE-2021-32749) As we continued to examine historic client-side bugs, we came across CVE-2021-32749. This one is again a pretty nasty bug, this time in the ever-popular fail2ban package. It’s a command injection vulnerability, a vulnerability class keenly sought by attackers due to its power and ease of exploitation. As you may know, if you have administered a fail2ban server, the purpose of fail2ban is to monitor failed login attempts, and prevent bruteforce or password-guessing attacks by blocking hosts which repeatedly fail to log in. Being the polished package it is, it also includes the ability to email an administrator when an IP address is banned, and - very helpfully - when it does so, it will enrich the email with information about who owns the banned IP address. This information is gleaned from - yeah, you guessed it! - our friend WHOIS. Unfortunately, for some time, the output of the WHOIS client wasn’t correctly sanitized before being passed to the mail tool, and so a command injection bug was possible. Fortunately - or unfortunately, if you’re an attacker - because fail2ban runs a WHOIS query on the IP address rather than, for example, a domain name specified in the PTR record of an IP address of blocked hosts - this attack is not within reach still based on our newly found capability. For those that control a WHOIS server that is queried for IP addresses, though, exploitation is simple - simply attempt to unsuccessfully authenticate to a server via SSH a few times to trigger a ban, and once fail2ban queries the WHOIS server for information on your IP address - serve a payload wrapped in backticks. Reality check So, the burning question on our minds - can we actually exploit these bugs, right now? Well, at this stage, our view was fairly pessimistic in terms of achieving real-world impact. We saw the following pre-requisites: The WHOIS client must be querying an old authoritative .MOBI WHOIS server and thus by definition, has not been working for quite a while To achieve client-side code execution (i.e. compromise) via a WHOIS client vuln - the only public option available to us was disclosed in 2015 and appears to have been rectified in 2018 - likely due to the perceived lack of real-world exploitation mechanisms. Meh. Our gut feeling remained that most of the Internet and those in the sane world would logically be querying the new .mobi authoritative WHOIS server whois.nic.mobi, rather than the decommissioned dotmobiregistry.net (which we now controlled). “Surely no large organisations would still reference the old domain”, we thought to ourselves. Kill WHOIS With Fire Without skipping a beat and really not considering the consequences, we set up a WHOIS server beneath our new domain at whois.dotmobiregistry.net, and logged incoming requests. We specifically focused on two things: Source IPs (so we can perhaps begin to work out who exactly was querying an outdated server), and, The queried domain (because again, this may give off some clues). We threw together the lglass server to respond to WHOIS requests that found their way to our WHOIS server, and returned: ASCII art (we were relatively refrained here, but it was a priority) Fake WHOIS details indicating watchTowr as the owner for every queried entity. As this was our private server, we included a request for queries to cease (after all, they were unauthorised). A quick test directly to our new WHOIS server showed that all was working as expected, with the following response provided for a query about google.mobi: Nice. Uh….. Well, it’s 2024 - absolutely no one has the ability to exercise patience, including ourselves. So, we began just looking around the Internet for obvious locations that could be sending queries our way. Surely, we thought - surely! - the broken clients using an outdated server address wouldn’t be in anything major, that we use every day? A significant number of domain registrars and WHOIS-function websites domain.com godaddy.com who.is whois.ru smallseo.tools seocheki.net centralops.net name.com webchart.org etc (you get the idea) A screenshot of each WHOIS tool would become repetitive, but you get the idea. urlscan.io - “A sandbox for the web” - used our WHOIS server for .mobi, too. You can see the results by browsing to a page representing any .mobi domain (like this one). VirusTotal, the popular malware-analysis site, was querying us! A tool dedicated to the analysis of hostile code seemed like an opportunity for enjoyment. Sadly, VirusTotal doesn't render our ASCII art properly, but as you can see - VirusTotal is querying our makeshift WHOIS server for this global .TLD and presenting back the results. We were also pleased to see that VirusTotal updated their records of who owns bbc.mobi: For anyone that has ever worked in offensive security, you occasionally get a sinking feeling where you realize something may be a little larger than expected, and you begin to wonder.. “what have we broken?”. (Editors note: Technically, this should be ‘what was broken’, because people were querying our WHOIS server without authorisation and we’re very upset - get off our lawn!). Well, with our WHOIS server clearly working - we figured we’d come back in a few days and see if anything at all reached out to us - giving us us a good excuse to stare at a separate PSIRT response indicating a 2 year lead time to resolve a vulnerability. Being insatiable and generally finding it hard to focus on anything longer than a TikTok video of a dog in a hat, we took a look to see how many unique IPs had queried our new WHOIS server after a few hours: $ sqlite3 whois-log-copy.db \"select source from queries\"|sort|uniq|wc -l 76085 Uh. Yes, that’s correct - this is 76,000+ unique source IP addresses that have sent queries to our WHOIS server in just a couple of hours. We were somewhat dismayed when, after leaving our server running for around two days, the poor little SQLite DB containing the logs ballooned to some 1.3 million queries! Clearly, we’d stumbled into something more major than we’d anticipated. We threw the list of IPs at ZDNS and just sat back, as a relatively feeble way of doing attribution: $ cat whois-src.txt|./zdns PTR > ptr.txt Anyway, the results were curious. $ grep gov ptr.txt |{magic}|sort|uniq .gov-east-1.compute.amazonaws.com.\" .gov.ar.\" .gov.bd.\" .gov.br.\" .gov.il.\" .gov.in.\" .gov.ph.\" .gov\" Great. We’d inadvertently done a thing. Some other highlights of source hosts (not exhaustive, but just to give you some idea of just how bad this trash fire appeared to be): Mail servers! Lots and lots of mail servers. Spam filters will often do WHOIS lookups on sender domains. We saw a bunch of these, ranging from the aptly-named cheapsender.email through to mail.bdcustoms.gov.bd - which appears to be part of the Bangladeshi government's infrastructure. Yikes! Theoretically, we could cause mayhem by serving responses indicating that the sending domain was a known spammer - and even more mayhem-worthy to start fuzzing the WHOIS parsing code to pop RCE on the mail servers themselves. (We didn’t) Leading on from that thought, what other .gov apparatus have we been queried by? Well, we found Brazil in our logs multiple times - for example, antispam.ap.gov.br and master.aneel.gov.br , and Brazil was not alone. We also found .gov addresses belonging to (but again not limited to): Argentina, Pakistan, India, Bangladesh, Indonesia, Bhutan, Philippines, Israel, Ethiopia, Ukraine, USA. Neat. Militaries (.mil) Swedish Armed Forces, for example Universities (.edu) All of them We even saw cyber security companies - hey Group-IB, Detectify! - query our WHOIS server (presumably doing threat intel things for .mobi domains). We saw Censys query us for ‘google.com’ and wondered if we’d get an APT number and a threat intel report shout-out if we’d been actively delivering payloads. Maybe we did? Check your boxen. (We didn't. Or did we?) We’re still trying to determine what software solutions are in play here/configured to query this WHOIS server for .mobi - let us know if you have any ideas. Those who are nefariously minded likely realised what we saw as well - with .gov and other mail servers querying us each time they received an email from a .mobi domain - we could begin to passively determine who may be in communication. This is not ideal. How do we fix this? Well, hold that thought - IT GETS WORSE. Tales of TLS TLS/SSL. Everyone knows it - it’s that friendly little padlock icon in the address bar that assures you that your connection is secure. It’s powered by the concept of certificates - sometimes used for HTTPS, sometimes used for signing your malware. For example, say you’re the owner of watchTowr.mobi. You want to secure communications to your web server by speaking TLS/SSL , so you go off to your favourite Certificate Authority and request a certificate (let’s also pretend you haven’t heard of LetsEncrypt). The Certificate Authority will verify that you own the domain in question - watchTowr.mobi - and will then sign a private certificate, attesting to your identity as the owner of that domain. This is then used by the browser to ensure your communications are secure. Speaking of LetsEncrypt, this thread is interesting - https://community.letsencrypt.org/t/why-doesnt-lets-encrypt-use-whois-information-for-domain-validation/46287). In this thread, forum posters detail why LetsEncrypt doesn’t validate domains via WHOIS. Seems paranoid. Anyway, what does this have to do with WHOIS, and what does it have to do with us?! Well, it turns out that a number of TLS/SSL authorities will verify ownership of a domain by parsing WHOIS data for your domain - say watchTowr.mobi- and pulling out email addresses defined as the ‘administrative contact’. The process is to then send that email address a verification link - once clicked, the Certificate Authority is convinced that you control the domain that you are requesting a TLS/SSL cert for and they will happily mint you a certificate. For example: Perhaps you can see where we’re going with this? sobs If a TLS/SSL certificate authority is using our WHOIS server for .mobi domains, we can likely provide our own email address for this “Email Domain Control Validation” method. Uh-oh. Is this a fringe feature supported only by two-bit, poor-quality certificate authorities? No! Here’s a sample of large TLS/SSL Certificate Authorities/resellers that support WHOIS-based ownership verification: Trustico Comodo SSLS GoGetSSL GlobalSign DigiSign Sectigo Going through the normal order flow, we began cautiously - by generating a CSR (Certificate Signing Request) for the fictitious domain watchTowr.mobi - the logic being that as long as our WHOIS server was queried, whether or not the domain was real was irrelevant because we respond positively to absolutely every request including domains that don’t actually exist. # sudo openssl req -new -key custom.key -out csr.pem You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:SG State or Province Name (full name) [Some-State]:Singapore Locality Name (eg, city) []:Singapore Organization Name (eg, company) [Internet Widgits Pty Ltd]:watchTowr Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []:watchtowr.mobi Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []: We’re not going to walk through each provider - for the purposes of illustration, we’ll use GoGetSSL. Once we upload our watchTowr.mobi CSR to GoGetSSL, it is parsed, and we continue. The indication of these placeholder email addresses indicates that WHOIS was not successful - instead of the email address that our WHOIS server is configured to respond with (whois@watchtowr.com), we’re presented with only @watchtowr.mobi domains. That’s something of a relief. The Certificate Authority has correctly determined that the domain watchTowr.mobi does not exist and thus if WHOIS is working as expected, no email addresses will be returned. We concluded that our newly set up WHOIS server was not being queried by the provider. At least the world isn’t ending. Right? (spoiler: it actually was) We carried on trying a few other providers until a thought occurred. The WHOIS protocol is extremely simple. Essentially it is a string blob returned in various formats depending on the TLD serving it. Each provider implements parsing in their own way. Perhaps, before we write off our theory, we should make sure this verification mechanism is actually working as it is supposed to. So, we began again - choosing microsoft.mobi as a .mobi domain that appeared to follow a fairly typical WHOIS format (when using the current .mobi WHOIS server). The screenshot below shows that the legitimate WHOIS record for microsoft.mobi was correctly parsed at Entrust, as the only email addresses available for validation were at the microsoft.com domain: While the WHOIS record for watchTowr.mobi was not being parsed at all (indicating that Entrust was using the correct WHOIS server, and not ours): Looks good you think? WRONG. We skipped and hopped over to the next provider, GlobalSign. GlobalSign reported that they were unable to parse the WHOIS record of microsoft.mobi: At this point, something clicked in our minds. Perhaps GlobalSign WAS querying our new WHOIS server - but the string returned by our WHOIS server was incompatible with GlobalSign’s parsing? We copied the microsoft.mobi output from the legitimate WHOIS server, made it our own, and loaded it into our own WHOIS server - updated to look like the following: Holding our breath, we then re-triggered GlobalSign with a CSR for microsoft.mobi… We want to be explicitly clear that we stopped at this point and did not issue any rogue TLS/SSL certificates to ourselves. This would undoubtedly create an incident, and require significant amounts of work by many parties to revoke and roll back this action. Success! The GlobalSign TLS/SSL certificate WHOIS domain verification system had queried our WHOIS server, parsed whois@watchTowr.com from the result, and presented it as a valid email address to send a verification email to, allowing us to complete verification and obtain a valid TLS/SSL certificate. This is then blindingly simple: Set up a rogue WHOIS server on our previously authoritative hostname, responding with our own email address as an ‘administrative contact’ Attempt to purchase a TLS/SSL certificate for a .mobi domain we want to target (say, microsoft.mobi) A Certificate Authority will then perform a WHOIS lookup, and email us instead of the real domain owners [theory] We click the link, and.. [theory] … receive an TLS/SSL cert for the target domain! [theory] Now that we have the ability to issue a TLS/SSL cert for a .mobi domain, we can, in theory, do all sorts of horrible things - ranging from intercepting traffic to impersonating the target server. It’s game over for all sorts of threat models at this point. While we are sure some may say we didn’t ‘prove’ we could obtain the certificate, we feel this would’ve been a step too far — so whatever. One Last Thing Please stop emailing us.. Here We Go Again.. We hope you’ve enjoyed (and/or been terrified by) today’s post, in which we took control of a chunk of the Internet’s infrastructure, opened up a big slab of juicy attack surface, and found a neat way of undermining TLS/SSL - the fundamental protocol that allows for secure communication on the web. We want to thank the UK's NCSC and the ShadowServer Foundation for rapidly working with us ahead of the release of this research to ensure that the 'dotmobiregistry.net' domain is suitably handled going forwards, and that a process is put in place to notify affected parties. The dotmobiregistry.net domain, and whois.dotmobiregisry.net hostname, has been pointed to sinkhole systems provided by ShadowServer that now proxy the legitimate WHOIS response for .mobi domains. We released this blog post to initially share our process around making the unexploitable exploitable and highlight the state of legacy infrastructure and increasing problems associated with abandoned domains - but inadvertently, we have shone a spotlight on the continuing trivial loopholes in one of the Internet’s most vital encryption processes and structures - TLS/SSL Certificate Authorities. Our research has demonstrated that trust placed in this process by governments and authorities worldwide should be considered misplaced at this stage, in our opinion. We continue to hold concern around the basic reality - we found this on a whim in a hotel room while escaping the Vegas heat surrounding Black Hat, while well-resourced and focused nation-states look for loopholes like this every day. In our opinion, we are not likely to be the last to find inexcusable flaws in such a crucial process. Although subverting the CA verification process was by far the most devastating of impacts that we uncovered, it was by no means the limit of the opportunity available to us as we also found everything from memory corruptions to command injections. Our ‘honeypot’ WHOIS server gave us some interesting statistics, revealing just how serious the issue is, and a large amount of Internet infrastructure continues to query us instead of the legitimate WHOIS servers. We do not intend to call out any specific organization or maintainer here - the prevalence of this issue and the statistics on hand show that this is not a pure-negligence or competence related issue - but a fundamental flaw in how these processes work together. It’s worth noting that all the above attacks that we were able to orchestrate given our takeover are also possible by any entity that is able to carry out MITM attacks - such as entities that control or can influence transit backbones. It would be very easy for an attacker with such access to fake WHOIS data for any domain, and thus obtain valid TLS/SSL certificates. Of course, there has been an insurmountable level of effort by major players to add transparency to this process over the years, and thus, 'pulling off' a heist of this scale has its operational hurdles. At watchTowr, we passionately believe that continuous security testing is the future and that rapid reaction to emerging threats single-handedly prevents inevitable breaches. With the watchTowr Platform, we deliver this capability to our clients every single day - it is our job to understand how emerging threats, vulnerabilities, and TTPs could impact their organizations, with precision. If you'd like to learn more about the watchTowr Platform, our Attack Surface Management and Continuous Automated Red Teaming solution, please get in touch. Previous post Veeam Backup & Response - RCE With Auth, But Mostly Without Auth (CVE-2024-40711)",
    "commentLink": "https://news.ycombinator.com/item?id=41510252",
    "commentBody": "We spent $20 to achieve RCE and accidentally became the admins of .mobi (watchtowr.com)870 points by notmine1337 7 hours agohidepastfavorite212 comments post-it 4 hours agoObviously there are a lot of errors by a lot of people that led to this, but here's one that would've prevented this specific exploit: > As part of our research, we discovered that a few years ago the WHOIS server for the .MOBI TLD migrated from whois.dotmobiregistry.net to whois.nic.mobi – and the dotmobiregistry.net domain had been left to expire seemingly in December 2023. Never ever ever ever let a domain expire. If you're a business and you're looking to pick up a new domain because it's only $10/year, consider that you're going to be paying $10/year forever, because once you associate that domain with your business, you can never get rid of that association. reply ohashi 7 minutes agoparentThis is the most obvious reason why Verisign is a monopolist and should be regulated like a utility. They make false claims about choice and not being locked in. You buy a domain, you use it, you're locked in forever. And they know it. That's why they fight tooth and nail to protect their monopoly. reply ryanmcbride 3 minutes agoparentprevBut if companies did that then I never would have been able to buy coolchug.com! reply swiftcoder 2 hours agoparentprevEven Google managed to (briefly) fuck that one up. https://money.cnn.com/2016/01/29/technology/google-domain-pu... reply yumraj 37 minutes agoparentprev> If you're a business and you're looking to pick up a new domain because it's only $10/year, consider that you're going to be paying $10/year forever, because once you associate that domain with your business, you can never get rid of that association. Please elaborate... Also, what about personal domains? Does it apply there as well? reply liquidgecka 23 minutes agorootparentMy brother used to own .com and wrote on it a bunch. Eventually he bailed out and let it expire. It turned into a porn site for a few years and now its for sale for like $2k from some predatory reseller. reply MontagFTB 28 minutes agorootparentprevAs per the article, the old domain expired and was picked up by a third party for $20. Said domain was hard-coded into a vast number of networking tools never to be updated again, effectively letting the new domain owner unfettered access into WHOIS internals. reply judge2020 29 minutes agorootparentprevPeople bookmark stuff. Random systems (including ones you don’t own) have hardcoded urls. Best to pay for it forever since it’s so low of a cost and someone taking over your past domain could lead to users getting duped. Personal domains are up to you. reply declan_roberts 4 hours agoparentprevAlways use subdomains. Businesses only ever need a single $10 domain for their entire existence. reply craftkiller 2 hours agorootparentNot true. If you are hosting user content, you want their content on a completely separate domain, not a subdomain. This is why github uses githubusercontent.com. https://github.blog/engineering/githubs-csp-journey/ reply joelanman 1 hour agorootparentinteresting, why is this? reply varun_ch 51 minutes agorootparentI can think of two reasons: 1. it's immediately clear to users that they're seeing content that doesn't belong to your business but instead belongs to your business's users. maybe less relevant for github, but imagine if someone uploaded something phishing-y and it was visible on a page with a url like google.com/uploads/asdf. 2. if a user uploaded something like an html file, you wouldn't want it to be able to run javascript on google.com (because then you can steal cookies and do bad stuff), csp rules exist, but it's a lot easier to sandbox users content entirely like this. reply mananaysiempre 16 minutes agorootparent> if a user uploaded something like an html file, you wouldn't want it to be able to run javascript on google.com (because then you can steal cookies and do bad stuff) Cookies are the only problem here, as far as I know, everything else should be sequestered by origin, which includes the full domain name (and port and protocol). Cookies predate the same-origin policy and so browsers scope them using their best guess at what the topmost single-owner domain name is, using—I kid you not—a compiled-in list[1]. (It’s as terrifying as it sounds.) [1] https://publicsuffix.org/ reply wiradikusuma 25 minutes agorootparentprevI'm wondering, many SaaS offer companyname.mysaas.com. Is that totally secure? reply ganoushoreilly 3 hours agorootparentprevI actually think they need 2, usually need a second domain / setup for failover. Especially if the primary domain is a novelty TLD like.. .IO which showed that things can happen at random to the TLD. If the website down it's fine, but if you have systems calling back to subdomains on that domain, you're out of luck. A good failover will help mitigate / minimize these issues. I'd also keep it on a separate registrar. Domains are really cheap, I try to just pay for 5-10 year blocks (as many as I can), when I can just to reduce the issues. reply shafoshaf 3 hours agorootparentprevAnd a second for when your main domain gets banned for spam for innocuous reasons. reply playingalong 2 hours agorootparentprevI think it's a sane practice to keep the marketing landing page on a separate domain than the product in case of SaaS. reply declan_roberts 2 hours agorootparentWhy? I always get frustrated when I end up in some parallel universe of a website (like support or marketing) and I can't easily click back to the main site. reply stagalooo 2 hours agorootparentprevCould you elaborate on why? The companies I have worked for have pretty much all used domain.com for marketing and app.domain.com for the actual application. What's wrong with this approach? reply darkr 38 minutes agorootparentIf there’s any scope for a user to inject JavaScript, then potentially this gives a vector of attack against other internal things (e.g admin.domain.com, operations.domain.com etc) reply CountVonGuetzli 21 minutes agorootparentAlso, if for example the SaaS you’re running sends a lot of system emails that really shouldn’t end up in spam filters, you can’t afford to let things like marketing campaigns negatively influence your domain’s spam score. Easier and safer to have separate domains. reply lovasoa 33 minutes agoprev> $ sqlite3 whois-log-copy.db \"select source from queries\"|sort|uniq|wc -l Oh cool they saved the logs in a database ! Wait... |sort|uniq|wc -l ?? But why ? reply SSLy 19 minutes agoparentbeats up re-re-remembering how to do it in sql reply radicality 1 minute agorootparentAnd probably because for quick things like that you’re already working in a “pipeline”, where you first want to see some of the results so you output with SQLite, and then add more to the pipeline. Similarly, I often do ‘cat filegrep abc’ instead of just grep, might be probably out of habit. reply forgotpwd16 1 hour agoprevVery cool work. >The dotmobiregistry.net domain, and whois.dotmobiregisry.net hostname, has been pointed to sinkhole systems provided by ShadowServer that now proxy the legitimate WHOIS response for .mobi domains. If those domains were meant to be deprecated should be better to return a 404. Keeping them active and working like normal reduces the insensitive to switch to the legitimate domain. reply epc 26 minutes agoparentWhois doesn't support HTTP status codes, but the shadowserver sinkhole responds with: Domain not found. >>> Please update your code or tell your system administrator to use whois.nic.mobi, the authoritative WHOIS server for this domain. You would, at this point, be forgiven for thinking that this class of attack - controlling WHOIS server responses to exploit parsing implementations within WHOIS clients - isn’t a tangible threat in the real world. Let's flip that on its head - are we expected to trust every single WHOIS server in the world to always be authentic and safe? Especially from the point of view of a CA trying to validate TLS, I would not want to find out that `whois somethingarbitrary.ru` leaves me open to an RCE by a Russian server! reply sundarurfriend 3 hours agoprevThe article puts the blame on > Never Update, Auto-Updates And Change Are Bad as the source of the problem a couple of times. This is pretty common take from security professionals, and I wish they'd also call out the other side of the equation: organizations bundling their \"feature\" (i.e. enshittification) updates and security updates together. \"Always keep your programs updated\" is just not feasible advice anymore given that upgrades as just as likely to be downgrades these days. If that were to be realistic advice, we need more pressure on companies to separate out security-related updates and allow people to get updates only on that channel. reply dt3ft 3 hours agoprevI wish I had the time they have… reply giancarlostoro 6 hours agoprevI still remember when websites would redirect you on your phone to their .mobi website, completely screwing up the original intent. They didn't show you the mobile version of whatever Google let you towards, they just lazily redirected you to the .mobi homepage. I bet they asked a non-dev to do those redirects, that one IT neckbeard who shoved a redirect into an Apache2 config file and moved on with life. :) But seriously, it was the most frustrating thing about the mobile web. Is this TLD even worth a damn in 2024? reply Tepix 6 hours agoprevWow! Highly entertaining and scary at the same time. Sometimes ijust wish i was clueless about all those open barn doors. reply mannyv 5 hours agoprev\"He who seeks finds.\" - old proverb. reply asimpleusecase 6 hours agoprevWonderful article! Well done chaps. reply adolph 2 hours agoprevConjecture: control over tlds should be determined by capture the flag. Whenever an organization running a registry achieves a level of incompetence whereby its tld is captured, the tld becomes owned by the attacker. Sure there are problems with this conjecture, like what if the attacker is just as incompetent (it just gets captured again), or \"bad actor\" etc. A concept similar to capture the flag might provide for evolving better approaches toward security than the traditional legal and financial methods of organizational capture the flag. reply nusl 6 hours agoprevPretty horrible negligence on the part of .mobi to leave a domain like this to expire. reply wbl 4 hours agoprevIs this in the bugzilla/MDSP yet? reply captn3m0 4 hours agoparenthttps://bugzilla.mozilla.org/show_bug.cgi?id=1917896 reply peterpost2 4 hours agoprevThat is so neat. Good job guys! reply pimlottc 7 hours agoprevAs a reminder, RCE = remote code execution (it’s not defined in the article). https://www.cloudflare.com/learning/security/what-is-remote-... reply worthless-trash 6 hours agoparentThese days people use \"RCE\" for local code execution. reply acdha 5 hours agorootparentI would clarify that as running code somewhere you don’t already control. The classic approach would be a malformed request letting them run code on someone else’s server, but this other pull-based approach also qualifies since it’s running code on a stranger’s computer. reply devvvvvvv 6 hours agoprevEntertaining and informative read. Main takeaways for me from an end user POV: - Be inherently less trustworthy of more unique TLDs where this kind of takeover seems more likely due to less care being taken during any switchover. - Don't use any \"TLS/SSL Certificate Authorities/resellers that support WHOIS-based ownership verification.\" reply DexesTTP 6 hours agoparentNone of these are true for the MitM threat model that caused this whole investigation: - If someone manages to MitM the communication between e.g. Digicert and the .com WHOIS server, then they can get a signed certificate from Digicert for the domain they want - Whether you yourself used LE, Digicert or another provider doesn't have an impact, the attacker can still create such a certificate. This is pretty worrying since as an end user you control none of these things. reply devvvvvvv 6 hours agorootparentThank you for clarifying. That is indeed much more worrying. If we were able to guarantee NO certificate authorities used WHOIS, this vector would be cut off right? And is there not a way to, as a website visitor, tell who the certificate is from and reject/distrust ones from certain providers, e.g. Digicert? Edit: not sure if there's an extension for this, but seems to have been done before at browser level by Chrome: https://developers.google.com/search/blog/2018/04/distrust-o... reply tetha 5 hours agorootparentCAA records may help, depending on how the attacker uses the certificate. A CAA record allows you to instruct the browser that all certs for \"*.tetha.example\" should be signed by Lets Encrypt. Then - in theory - your browser could throw an alert if it encounters a DigiCert cert for \"fun.tetha.example\". However, this depends strongly on how the attacker uses the cert. If they hijack your DNS to ensure \"fun.tetha.example\" goes to a record they control, they can also drop or modify the CAA record. And sure, you could try to prevent that with long TTLs for the CAA record, but then the admin part of my head wonders: But what if you have to change cert providers really quickly? That could end up a mess. reply tialaramex 2 hours agorootparentCAA records are not addressed to end users, or to browsers or whatever - they are addressed to the Certificate Authority, hence their name. The CAA record essentially says \"I, the owner of this DNS name, hereby instruct you, the Certificate Authorities to only issue certificates for this name if they obey these rules\" It is valid, and perhaps even a good idea in some circumstances, to set the CAA record for a name you control to deny all issuance, and only update it to allow your preferred CA for a few minutes once a month while actively seeking new certificates for any which are close to expiring, then put it back to deny-all once the certificates were issued. Using CAA allows Meta, for example, to insist only Digicert may issue for their famous domain name. Meta has a side deal with Digicert, which says when they get an order for whatever.facebook.com they call Meta's IT security regardless of whether the automation says that's all good and it can proceed, because (under the terms of that deal) Meta is specifically paying for this extra step so that there aren't any security \"mistakes\". In fact Meta used to have the side deal but not the CAA record, and one day a contractor - not realising they're supposed to seek permission from above - just asked Let's Encrypt for a cert for this test site they were building and of course Let's Encrypt isn't subject to Digicert's agreement with Meta so they issued based on the contractor's control over this test site. Cue red faces for the appropriate people at Meta. When they were done being angry and confused they added the CAA record. [Edited: Fix a place where I wrote Facebook but meant Meta] reply donatj 5 hours agoprevI have written PHP for a living for the last 20 years and that eval just pains me to no end eval($var . '=\"' . str_replace('\"', '\\\\\\\\\"', $itm) . '\";'); Why? Dear god why. Please stop. PHP provides a built in escaper for this purpose eval($var . '=' . var_export($itm, true) . ';'); But even then you don't need eval here! ${$var} = $itm; Is all you really needed... but really just use an array(map) if you want dynamic keys... don't use dynamically defined variables... reply dustywusty 1 hour agoparentCouldn't agree more with this. In general, if you're writing eval you've already committed to doing something the wrong way. reply xp84 3 hours agoparentprevI mean no disrespect to you, but this sort of thing is exactly the sort of mess I’ve come to expect in any randomly-selected bit of PHP code found in the wild. It’s not that PHP somehow makes people write terrible code, I think it’s just the fact that it’s been out for so long and so many people have taken a crack at learning it. Plus, it seems that a lot of ingrained habits began back when PHP didn’t have many of its newer features and they just carried on, echoing through stack overflow posts forever. reply hinkley 2 hours agorootparentOn a new job I stuck my foot in it because I argued something like this with a PHP fan who was adamant I was wrong. Mind you this was more than ten years ago when PHP was fixing exploits left and right. This dust up resolved itself within 24 hours though, as I came in the next morning to find he was too busy to work on something else because he was having to patch the PHP forum software he administered because it had been hacked overnight. I did not gloat but I had trouble keeping my face entirely neutral. Now I can’t read PHP for shit but I tried to read the patch notes that closed the hole. As near as I could tell, the exact same anti pattern appeared in several other places in the code. I can’t touch PHP. I never could before and that cemented it. reply podunkPDX 1 hour agorootparentPHP: an attack surface with a side effect of hosting blogs. reply dartos 3 hours agorootparentprevJavaScript land fares little better. IMO it’s because php and js are so easy to pick up for new programmers. They are very forgiving, and that leads to… well… the way that php and js is… reply btown 3 hours agorootparentThe saving grace of JS is that the ecosystem had a reset when React came out; there's plenty of horrifying JQuery code littering the StackOverflow (and Experts Exchange!) landscape, but by the time React came around, Backbone and other projects had already started to shift the ecosystem away from \"you're writing a script\" to \"you're writing an application,\" so someone searching \"how do I do X react\" was already a huge step up in best practices for new learners. I don't think PHP and its largest frameworks ever had a similar singular branding reset. reply MajimasEyepatch 1 hour agorootparentThe other thing making JavaScript a little better in practice is that it very rarely was used on the back end until Node.js came along, and by then, we were fully in the AJAX world, where people were making AJAX requests using JavaScript in the browser to APIs on the back end. You were almost never directly querying a database with JavaScript, whereas SQL injection seems to be one of the most common issues with a lot of older PHP code written by inexperienced devs. Obviously SQL injection can and does happen in any language, but in WordPress-land, when your website designer who happens to be the owner's nephew writes garbage, they can cause a lot of damage. You probably would not give that person access to a Java back end. reply snerbles 2 hours agorootparentprevLaravel, maybe. But not as much as React, or the other myriad JS frontend frameworks. (to include the ones that appeared in the time I spent typing this post) reply sjm-lbm 15 minutes agorootparentI'd argue that PHP7 is the closest thing PHP has had to a quality revolution. It fixed a zillion things, got rid of some footguns like legacy mysql, and in general behaved a lot more rationally. If you were doing things right, by that point you were already using Laravel or Symphony or something, so the change didn't seem as revolutionary as it was, but that was the moment a lot of dumb string concatenated query code (for example) no longer worked out of the box. reply numb7rs 2 hours agorootparentprevI've heard it said that one of the reasons Fortran has a reputation for bad code is this combination: lots of people who haven't had any education in best practices; and it's really easy in Fortran to write bad code. reply hinkley 2 hours agorootparentWhich is why that “you can write Fortran in any language” is such an epithet. reply tracker1 1 hour agorootparentMost horrific code I've ever seen was a VB6 project written by a mainframe programmer... I didn't even know VB6 could do some of the things he did... and wish I never did. Not to mention variables like a, b, c, d .. aa, ab... reply MajimasEyepatch 1 hour agorootparentprevCode written by scientists is a sight to behold. reply craigmoliver 53 minutes agorootparentand they think cause they're scientists they can just do it because they're scientists and stuff. Very pragmatic to be sure...but horrifying. reply jimkoen 3 hours agorootparentprevI'm sorry, I haven't encountered bare eval in years. Do you have an example? And even then it's actually not that easy to get RCE going with that. reply donatj 3 hours agorootparentSomething like half of of reported JavaScript vulnerabilities are \"prototype pollution\" because It's very common practice to write to object keys blindly, using objects as a dictionary, without considering the implications. It's a very similar exploit. reply baq 2 hours agorootparentarguably worse, since no eval is needed... reply johnisgood 2 hours agorootparentprevYeah, same with the use of \"filter_input_array\", \"htmlspecialchars\", or how you should use PDO and prepare your statements with parameterized queries to prevent SQL injection, etc. reply hinkley 2 hours agorootparentprevAt least the node community is mostly allergic to using eval(). The main use I know of goes away with workers. reply larsnystrom 1 hour agorootparentprevI mean, in this case the developer really went out of their way to write bad code. TBH it kind of looks like they wanted to introduce an RCE vulnerability, since variable variable assignment is well-known even to novice PHP developers (who would also be the only ones using that feature), and \"eval is bad\" is just as well known. A developer who has the aptitude to write a whois client, but knows neither of those things? It just seems very unlikely. reply amelius 2 hours agorootparentprevReplace PHP by C or C++ in your comment, and then read it again. reply zoover2020 3 hours agoparentprevThis is why PHP is mostly banned at bigCo reply dr_kretyn 2 hours agorootparentPretty sure there's plenty of PHP at Amazon and Facebook (just with slightly different names) reply jonhohle 2 hours agorootparentThere is no PHP at Amazon (at least not 2009-2016). It was evaluated before my time there and Perl Mason was chosen instead to replace C++. A bunch if that’s still appears to exist (many paths that start with gp/) but a lot was being rebuilt in various internal Java frameworks. I know AWS had some rails apps that were being migrated to Java a decade ago, but I don’t think I ever encountered PHP (and I came in as a programmer primarily writing PHP). reply dr_kretyn 46 minutes agorootparentOk, my \"pretty sure\" turns out to be \"not sure at all\". Thank you for the refresher! I was thinking about Mason and somehow conflated Perl with PHP. I left Amazon 2020. Had various collaborations with ecommerce (mainly around fulfillment) and there was plenty of Mason around. reply joeframbach 2 hours agorootparentprevI can *assure* you that php is expressly prohibited for use at Amazon. reply johnisgood 1 hour agorootparentReally? How come? What is the history with regarding to that? What are their reasoning? Does it apply to PHP >=8? reply NovemberWhiskey 3 hours agorootparentprevTo paraphrase: you can write PHP in any language. PHP is a negative bias for bigCo mostly because of the folkloric history of bad security practices by some PHP software developers. reply jeremyjh 3 hours agorootparentBy “folkloric history”, don’t you actually mean just “history”? reply playingalong 2 hours agorootparentI guess they mean the stigma that arose based on the reality in the past. So kind of both. reply hinkley 1 hour agorootparentThey fucked themselves and the rest of us moved on. You can become a good person late in life and still be lonely because all your bridges are burned to the ground. reply hinkley 2 hours agorootparentprev> folkloric I think the word you’re looking for is “epic” or “legendary” reply dartos 3 hours agorootparentprevIsn’t Facebook one of the biggest? reply packetslave 2 hours agorootparentHack is not PHP (any longer) reply phplovesong 3 hours agorootparentprevPretty much. PHP for a banking software? For anything money related? Goomg to have a bad time. reply smashed 3 hours agorootparentMagento, OpenCart or WooCommerce are money related. All terrible but also very popular. But I guess they work, somehow. What would you use to build and self-host an ecommerce site quickly and that is not a SaaS? reply smsm42 3 hours agorootparentprevYou're saying all big companies ban whole language ecosystem because somebody on the internet used one function in that language in knowingly unsafe manner contrary to all established practices and warnings in the documentation? This is beyond laughable. reply EwanToo 2 hours agorootparentLaughable, but accurate. Google for example does exactly this. reply iscoelho 7 hours agoprevGreat write-up - the tip of the iceberg on how fragile TLS/SSL is. Let's add a few: 1. WHOIS isn't encrypted or signed, but is somehow suitable for verification (?) 2. DNS CAA records aren't protected by DNSSEC, as absence of a DNS record isn't sign-able (correction: NSEC is an optional DNSSEC extension) 3. DNS root & TLD servers are poorly protected against BGP hijacks (adding that DNSSEC is optional for CAs to verify) 4. Email, used for verification in this post, is also poorly protected against BGP hijacks. I'm amazed we've lasted this long. It must be because if anyone abuses these issues, someone might wake up and care enough to fix them (: reply candiddevmike 6 hours agoparentOur industry needs to finish what it starts. Between IPv6, DNSSEC, SMTP TLS, SCTP/QUIC, etc all of these bedrock technologies feel like they're permanently stuck in a half completed implementation/migration. Like someone at your work had all these great ideas, started implementing them, then quit when they realized it would be too difficult to complete. reply colmmacc 5 hours agorootparentIf you look at say 3G -> 4G -> 5G or Wifi, you see industry bodies of manufacturers, network providers, and middle vendors who both standardize and coordinate deployment schedules; at least at the high level of multi-year timelines. This is also backed by national and international RF spectrum regulators who want to ensure that there is the most efficient use of their scarce airwaves. Industry players who lag too much tend to lose business quite quickly. Then if you look at the internet, there is a very uncoordinated collection of manufacturers, network providers, and standardization is driven in a more open manner that is good for transparency but is also prone to complexifying log-jams and hecklers vetos. Where we see success, like the promotion of TLS improvements, it's largely because a small number of knowledgable players - browsers in the case of TLS - agree to enforce improvements on the entire eco-system. That in turn is driven by simple self-interest. Google, Apple, and Microsoft all have strong incentives to ensure that TLS remains secure; their ads and services revenue depend upon it. But technologies like DNSSEC, IPv6, QUIC all face a much harder road. To be effective they need a long chain of players to support the feature, and many of those players have active disincentives. If a home users internet seems to work just fine, why be the manufacturer that is first to support say DNSSEC validation and deal with all of the increased support cases when it breaks, or device returns when consumers perceive that it broke something? (and it will). reply dgoldstein0 2 hours agorootparentIPv6 deployment is extra hard because we need almost every network in the world to get on board. Dnssec shouldn't be as bad, but for dns resolvers and software that build them in. I think it's a bit worse than TLS adoption in part just because of DNS allowing recursive resolution and in part DNS being applicable to a bit more than TLS was. But the big thing seems to be that there isn't a central authority like web browsers who can entirely force the issue. ... Maybe OS vendors could do it? Quic is an end to end protocol so should be deployable without every network operator buying in. That said, we probably do need a reduction in udp blocking in some places. But otherwise, how can quic deployment be harder than TLS deployment? I think there just hasn't been incentive to force it everywhere. reply MichaelZuo 1 hour agorootparentPlus IPv6 has significant downsides (more complex, harder to understand, more obscure failure modes, etc…), so the actual cost of moving is the transition cost + total downside costs + extra fears of unknown unknowns biting you in the future. reply GTP 3 hours agorootparentprevAFAIK, in the case of IPv6 it's not even that: there's still the open drama of the peering agreement between Cogent and Hurricane Electrics. reply jimt1234 2 hours agorootparentprevIn my 25+ years in this industry, there's one thing I've learned: starting something isn't all that difficult, however, shutting something down is nearly impossible. For example, brilliant people put a lot of time end effort into IPv6. But that time and effort is nothing compared to what it's gonna take to completely shut down IPv4. And I've dealt with this throughout my entire career: \"We can't shut down that Apache v1.3 server because a single client used it once 6 years ago!\" reply dogleash 5 hours agorootparentprev> Our industry needs to finish what it starts. \"Our industry\" is a pile of snakes that abhor the idea of collaboration on common technologies they don't get to extract rents from. ofc things are they way they are. reply iscoelho 5 hours agorootparentLet's not fool ourselves by saying we're purely profit driven. Our industry argues about code style (: reply z3phyr 4 hours agorootparentOur industry does not argue about code style. There were a few distinct subcultures which were appropriated by the industry who used to argue about code style, lisp-1 vs lisp-2, vim vs emacs, amiga vs apple, single pass vs multi pass compilers, Masters of Deception vs Legion of Doom and the list goes on, depending on the subculture. The industry is profit driven. reply iscoelho 4 hours agorootparentDo you use tabs or spaces? Just joking, but: The point is that our industry has a lot of opinionated individuals that tend to disagree on fundamentals, implementations, designs, etc., for good reasons! That's why we have thousands of frameworks, hundreds of databases, hundreds of programming languages, etc. Not everything our industry does is profit driven, or even rational. reply Joker_vD 2 hours agorootparentFWIW, all my toy languages consider U+0009 HORIZONTAL TABULATION in a source file to be an invalid character, like any other control character except for U+000A LINE FEED (and also U+000D CARRIAGE RETURN but only when immediately before a LINE FEED). reply hinkley 1 hour agorootparentI’d be a python programmer now if they had done this. It’s such an egregiously ridiculous foot gun that I can’t stand it. reply wwweston 3 hours agorootparentprev> > Our industry argues about code style (: > Our industry does not argue about code style. QED reply brookst 2 hours agorootparentOur industry does not argue about arguing about code style. reply wwweston 2 hours agorootparentOur industry doesn't always make Raymond Carver title references, but when it does, what we talk about when we talk about Raymond Carver title references usually is an oblique way of bringing up the thin and ultimately porous line between metadiscourse and discourse. reply svieira 2 hours agorootparentprevI'm pretty sure this is QEF. reply doubled112 5 hours agorootparentprevDoesn't every place have a collection of ideas that are half implemented? I know I often choose between finishing somebody else's project or proving we don't need it and decommissioning it. I'm convinced it's just human nature to work on something while it is interesting and move on. What is the motivation to actually finish? Why would the the technologies that should hold up the Internet itself be any different? reply kevindamm 3 hours agorootparentWhile that's true, it dismisses the large body of work that has been completed. The technologies GP comment mentions are complete in the sense that they work, but the deployment is only partial. Herding cats on a global scale, in most cases. It also ignores the side effect benefit that completing the interesting part -- other efforts benefit from the lessons learned by that disrupted effort, even if the deployment fails because it turns out nobody wanted it. And sometimes it's just a matter of time and getting enough large stakeholders excited or at least convinced the cost of migration is worth it. All that said, even the sense of completing or finishing a thing only really happens in small and limited-scope things, and in that sense it's very much human nature, yeah. You can see this in creative works, too. It's rarely \"finished\" but at some point it's called done. reply hinkley 1 hour agorootparentprevI was weeks away from turning off someone’s giant pile of spaghetti code and replacing it with about fifty lines of code when I got laid off. I bet they never finished it, since the perpetrators are half the remaining team. reply ozfive 1 hour agorootparentprevOr got fired/laid off and the project languished? reply trhway 1 hour agorootparentprevIPv6 instead of being branded as a new implementation should probably have been presented as an extension of IPv4, like some previously reserved IPv4 address would mean that it is really IPv6 with the value in the previously reserved fields, etc. That would be a kludge, harder to implement, yet much easier for the wide Internet to embrace. Like it is easier to feed oatmeal to a toddler by presenting it as some magic food :) reply immibis 52 minutes agorootparentIt would have exactly the same deployment problems, but waste more bytes in every packet header. Proposals like this have been considered and rejected. How is checking if, say, the source address is 255.255.255.255 to trigger special processing, any easier than checking if the version number is 6? If you're thinking about passing IPv6 packets through an IPv4 section of the network, that can already be achieved easily with tunneling. Note that ISPs already do, and always have done, transparent tunneling to pass IPv6 packets through IPv4-only sections of their network, and vice versa, at no cost to you. Edit: And if you want to put the addresses of translation gateways into the IPv4 source and destination fields, that is literally just tunneling. reply mschuster91 6 hours agorootparentprev> Like someone at your work had all these great ideas, started implementing them, then quit when they realized it would be too difficult to complete. The problem is, in many of these fields actual real-world politics come into play - you got governments not wanting to lose the capability to do DNS censorship or other forms of sabotage, you got piss poor countries barely managing to keep the faintest of lights on, you got ISPs with systems that have grown over literal decades where any kind of major breaking change would require investments into rearchitecture larger than the company is worth, you got government regulations mandating stuff like all communications of staff be logged (e.g. banking/finance) which is made drastically more complex if TLS cannot be intercepted or where interceptor solutions must be certified making updates to them about as slow as molasses... reply iscoelho 6 hours agorootparentConsidering we have 3 major tech companies (Microsoft/Apple/Google) controlling 90+% of user devices and browsers, I believe this is more solvable than we'd like to admit. reply idunnoman1222 3 hours agorootparentThose companies have nothing to do with my ISP router or modem reply mschuster91 5 hours agorootparentprevBrowsers are just one tiny piece of the fossilization issue. We got countless vendors of networking gear, we got clouds (just how many AWS, Azure and GCP services are capable of running IPv6 only, or how many of these clouds can actually run IPv6 dual-stack in production grade?), we got even more vendors of interception middlebox gear (from reverse proxies and load balancers, SSL breaker proxies over virus scanners for web and mail to captive portal boxes for public wifi networks), we got a shitload of phone telco gear of which probably a lot has long since expired maintenance and is barely chugging along. reply nativeit 4 hours agorootparentOk. You added OEMs to the list, but then just named the same three dominant players as clouds. Last I checked, every device on the planet supports IPv6, if not those other protocols. Everything from the cheapest home WiFi router, to every Layer 3 switch sold in the last 20-years. I think this is a 20-year old argument, and it’s largely irrelevant in 2024. reply mschuster91 4 hours agorootparent> I think this is a 20-year old argument, and it’s largely irrelevant in 2024. It's not irrelevant - AWS lacks support for example in EKS or in ELB target groups, where it's actually vital [1]. GCE also lacks IPv6 for some services and you gotta pay extra [2]. Azure doesn't support IPv6-only at all, a fair few services don't support IPv6 [3]. The state of IPv6 is bloody ridiculous. [1] https://docs.aws.amazon.com/vpc/latest/userguide/aws-ipv6-su... [2] https://cloud.google.com/vpc/docs/ipv6-support?hl=de [3] https://learn.microsoft.com/en-us/azure/virtual-network/ip-s... reply gavindean90 3 hours agorootparentprevPlenty doesn’t support IPv6. reply iscoelho 6 hours agorootparentprevobligatory https://xkcd.com/927/ Honestly: we're in this situation because we keep trying to band-aid solutions onto ancient protocols that were never designed to be secure. (I'm talking about you DNS.) Given xkcd's wisdom though, I'm not sure if this is easily solvable. reply Dylan16807 5 hours agorootparentCan we all agree to not link that comic when nobody is suggesting a new standard, or when the list of existing standards is zero to two long? It's not obligatory to link it just because the word \"standard\" showed up. I think that covers everything in that list. For example, trying to go from IPv4 to IPv6 is a totally different kind of problem from the one in the comic. reply iscoelho 4 hours agorootparentThe point is that, ironically, new standards may have been a better option. Bolting on extensions to existing protocols not designed to be secure, while improving the situation, has been so far unable to address all of the security concerns leaving major gaps. It's just a fact. reply sulandor 6 hours agorootparentprevdns should not have to be secure, it should be regulated as a public utility with 3rd-party quality control and all the whistles. only then can it be trustworthy, fast and free/accessible reply IgorPartola 5 hours agorootparentThere is nothing fundamentally preventing us from securing DNS. It is not the most complicated protocol believe it or not and is extensible enough for us to secure it. Moreover a different name lookup protocol would look very similar to DNS. If you don’t quite understand what DNS does and how it works the idea of making it a government protected public service may appeal to you but that isn’t actually how it works. It’s only slightly hyperbolic to say that you want XML to be a public utility. On the other hand things like SMTP truly are ancient. They were designed to do things that just aren’t a thing today. reply iscoelho 5 hours agorootparentprevIf my DNS can be MITM'd, and is thus insecure, it is not trustworthy. reply 8organicbits 5 hours agorootparentThis sort of all-or-nothing thinking isn't helpful. DNS points you to a server, TLS certificates help you trust that you've arrived at the right place. It's not perfect, but we build very trustworthy systems on this foundation. reply quesera 5 hours agorootparentBut DNS is all-or-nothing. If you can't trust DNS, you can't trust TLS or anything downstream of it. Even banks are not bothering with EV certificates any more, since browsers removed the indicator (for probably-good reasons). DV certificate issuance depends on trustworthy DNS. Internet security is \"good enough\" for consumers, most of the time. That's \"adequately trustworthy\", but it's not \"very trustworthy\". reply 8organicbits 4 hours agorootparentBank websites like chase.com and hsbc.com and web services like google.com, amazon.com, and amazonaws.com intentionally avoid DNSSEC. I wouldn't consider those sites less than \"very trustworthy\" but my point is that \"adequately trustworthy\" is the goal. All-or-nothing thinking isn't how we build and secure systems. reply quesera 4 hours agorootparentI am definitely not arguing in favor of DNSSEC. However, I don't think it's reasonable to call DNS, as a system, \"very trustworthy\". \"Well-secured\" by active effort, and consequently \"adequately trustworthy\" for consumer ecommerce, sure. But DNS is a systemic weak link in the chain of trust, and must be treated with extra caution for \"actually secure\" systems. (E.g., for TLS and where possible, the standard way to remove the trust dependency on DNS is certificate pinning. This is common practice, because DNS is systemically not trustworthy!) reply 8organicbits 3 hours agorootparentIs certificate pinning common? On the web we used to have HPKP, but that's obsolete and I didn't think it was replaced. I know pinning is common in mobile apps, but I've generally heard that's more to prevent end-user tampering than any actual distrust of the CAs/DNS. I think you're \"well-secured\" comment is saying the same thing I am, with some disagreement about \"adequate\" vs \"very\". I don't spend any time worrying that my API calls to AWS or online banking transactions are insecure due to lack of DNSSEC, so the DNS+CA system feels \"very\" trustworthy to me, even outside ecommerce. The difference between \"very\" and \"adequate\" is sort of a moot point anyway: you're not getting extra points for superfluous security controls. There's lots of other things I worry about, though, because attackers are actually focusing their efforts there. reply quesera 1 hour agorootparentI agree that the semantics of \"adequate\" and \"very\" are moot. As always, it ultimately depends on your threat profile, real or imagined. Re: certificate pinning, it's common practice in the financial industry at least. It mitigates a few risks, of which I'd rate DNS compromise as more likely than a rogue CA or a persistent BGP hijack. reply goodpoint 5 hours agorootparentprevStandards evolve for good reasons. That's just a comic. reply iscoelho 5 hours agorootparentThe comic is about re-inventing the wheel. What you propose \"standards evolving\" would be the opposite in spirit (and is what has happened with DNSSEC, RPKI, etc) reply 8organicbits 6 hours agoparentprev> 2. DNS CAA records aren't protected by DNSSEC, as absence of a DNS record isn't sign-able. NSEC does this. > An NSEC record can be used to say: “there are no subdomains between subdomains X and subdomain Y. reply iscoelho 6 hours agorootparentYou're correct - noting that Lets Encrypt supports DNSSEC/NSEC fully. Unfortunately though, the entire PKI ecosystem is tainted if other CAs do not share the same security posture. reply 8organicbits 5 hours agorootparentTainted seems a little strong, but I think you're right, there's nothing in the CAB Baseline Requirements [1] that requires DNSSEC use by CAs. I wouldn't push for DNSSEC to be required, though, as it's been so sparsely adopted. Any security benefit would be marginal. Second level domain usage has been decreasing (both in percentage and absolute number) since min-2023 [2]. We need to look past DNSSEC. [1] https://cabforum.org/working-groups/server/baseline-requirem... [2] https://www.verisign.com/en_US/company-information/verisign-... reply iscoelho 5 hours agorootparentI agree that DNSSEC is not the answer and has not lived up to expectations whatsoever, but what else is there to verify ownership of a domain? Email- broken. WHOIS- broken. Let's convince all registrars to implement a new standard? ouch. reply 8organicbits 5 hours agorootparentI'm a fan of the existing standards for DNS (§3.2.2.4.7) and IP address (§3.2.2.4.8) verification. These use multiple network perspectives as a way of reducing risk of network-level attacks. Paired with certificate transparency (and monitoring services). It's not perfect, but that isn't the goal. reply iscoelho 4 hours agorootparentBGP hijacks unfortunately completely destroy that. RPKI is still extremely immature (despite what companies say) and it is still trivial to BGP hijack if you know what you're doing. If you are able to announce a more specific prefix (highly likely unless the target has a strong security competency and their own network), you will receive 100% of the traffic. At that point, it doesn't matter how many vantage points you verify from: all traffic goes to your hijack. It only takes a few seconds for you to verify a certificate, and then you can drop your BGP hijack and pretend nothing happened. Thankfully there are initiatives to detect and alert BGP hijacks, but again, if your organization does not have a strong security competency, you have no knowledge to prevent nor even know about these attacks. reply account42 5 hours agoparentprev> 1. WHOIS isn't encrypted or signed, but is somehow suitable for verification (?) HTTP-based ACME verification also uses unencrypted port-80 HTTP. Similar for DNS-based verification. reply Arch-TK 3 hours agorootparentIf it used HTTPS you would have a bootstrapping problem. reply bootsmann 3 hours agorootparentprev> HTTP-based ACME verification also uses unencrypted port-80 HTTP I mean, they need to bootstrap the verification somehow no? You cannot upgrade the first time you request a challenge. reply iscoelho 4 hours agorootparentprev100% - another for the BGP hijack! reply michaelt 1 hour agorootparentThe current CAB Forum Baseline Requirements call for \"Multi-Perspective Issuance Corroboration\" [1] i.e. make sure the DNS or HTTP challenge looks the same from several different data centres in different countries. By the end of 2026, CAs will validate from 5 different data centres. This should make getting a cert via BGP hijack very difficult. [1] https://github.com/cabforum/servercert/blob/main/docs/BR.md#... reply graemep 2 hours agoparentprevIts used for verification because its cheap, not because its good. Why would you expect anyone to care enough to fix it. If we really wanted verification we would still be manually verifying the owners of domains. Highly effective but expensive. reply jrochkind1 6 hours agoparentprev> It must be because if anyone abuses these issues, someone might wake up and care enough to fix them If anyone knows they are being abused, anyway. I conclude that someone may be abusing them, but those doing so try to keep it unknown that they have done so, to preserve their access to the vulnerability. reply iscoelho 5 hours agorootparentCertificate Transparency exists to catch abuse like this. [1] Additionally, Google has pinned their certificates in Chrome and will alert via Certificate Transparency if unexpected certificates are found. [2] It is unlikely this has been abused without anyone noticing. With that said, it definitely can be, there is a window of time before it is noticed to cause damage, and there would be fallout and a \"call to action\" afterwards as a result. If only someone said something. [1] https://certificate.transparency.dev [2] https://github.com/chromium/chromium/blob/master/net/http/tr... reply hinkley 1 hour agorootparentprevIt’s like the crime numbers. If you’re good enough at embezzling nobody knows you embezzled. So what’s the real crime numbers? Nobody knows. And anyone who has an informed guess isn’t saying. A big company might discover millions are missing years after the fact and back date reports. But nobody is ever going to record those office supplies. reply ChrisMarshallNY 6 hours agorootparentprevDidn't Jon Postel do something like this, once? It was long ago, and I don't remember the details, but I do remember a lot of people having shit hemorrhages. reply NovemberWhiskey 5 hours agoparentprevNone of these relate to TLS/SSL - that's the wrong level of abstraction: they relate to fragility of the roots of trust on which the registration authorities for Internet PKI depend. reply iscoelho 5 hours agorootparentAs long as TLS/SSL depends on Internet PKI as it is, it is flawed. I guess there's always Private PKI, but that's if you're not interested in the internet (^: reply NovemberWhiskey 4 hours agorootparentI would say that TLS/SSL doesn't depend on Internet PKI - browsers (etc) depend on Internet PKI in combination with TLS/SSL. reply detourdog 6 hours agoparentprevFor reasons not important hear I purchase my SSL certificates and barely have any legitimating business documents. If Dunn & Bradstreet calls I hang up... It took me 3 years of getting SSL certs from the same company through a convoluted process before I tried a different company. My domain has been with the same registrar since private citizens could register DNS names. That relationship meant nothing when trying to prove that I'm me and I own the domain name. I went back to the original company because I could verify myself through their process. My only point is that human relationships is the best form of verifying integrity. I think this provides everyone the opportunity to gain trust and the ability to prejudge people based on association alone. reply lobsterthief 6 hours agorootparentHuman relationships also open you up to social engineering attacks. Unless they’re face-to-face, in person, with someone who remembers what you actually look like. Which is rare these days. reply detourdog 6 hours agorootparentThat is my point. We need to put value on the face to face relationships and extend trust outward from our personal relationships. This sort of trust is only as strong as it's weakest link but each individual can choose how far to extend their own trust. reply GTP 3 hours agorootparentThis is what the Web of Trust does but, > This sort of trust is only as strong as it's weakest link but each individual can choose how far to extend their own trust. is exactly why I prefer PKI to the WoT. If you try to extend the WoT to the whole Internet, you will eventually end up having to trust multiple people you never met with them properly managing their keys and correctly verifying the identity of other people. Identity verification is in particular an issue: how do you verify the identity of someone you don't know? How many of us know how to spot a fake ID card? Additionally, some of them will be people participating in the Web of Trust just because they heard that encryption is cool, but without really knowing what they are doing. In the end, I prefer CAs. Sure, they're not perfect and there have been serious security incidents in the past. But at least they give me some confidence that they employ people with a Cyber Security background, not some random person that just read the PGP documentation (or similar). PS: there's still some merit to your comment. I think that the WoT (but I don't know for sure) was based on the 7 degrees of separation theory. So, in theory, you would only have to certify the identity of people you already know, and be able to reach someone you don't know through a relatively short chain of people where each hop knows very well the next hop. But in practice, PGP ended up needing key signing parties, where people that never met before were signing each other's key. Maybe a reboot of the WoT with something more user friendly than PGP could have a chance, but I have some doubts. reply detourdog 1 minute agorootparentI’m fine with PKIs presumably in America the department of education could act as a CA. dopylitty 6 hours agorootparentprevThis is such a good point. We rely way too much on technical solutions. A better approach is to have hyperlocal offices where you can go to do business. Is this less “efficient”? Yes but when the proceeds of efficiency go to shareholders anyway it doesn’t really matter. reply detourdog 4 hours agorootparentIt is only efficient based on particular metrics. Change the metrics and the efficiency changes. reply mrguyorama 5 hours agorootparentprev>Is this less “efficient”? Yes but when the proceeds of efficiency go to shareholders anyway it doesn’t really matter. I agree with this but that means you need to regulate it. Even banks nowadays are purposely understaffing themselves and closing early because \"what the heck are you going to do about it? Go to a different bank? They're closed at 4pm too!\" reply detourdog 4 hours agorootparentThe regulation needs to be focused on the validity of the identity chain mechanism but not on individuals. Multiple human interactions as well as institutional relationships could be leveraged depending on needs. The earliest banking was done with letters of introduction. That is why banking families had early international success. They had a familial trust and verification system. reply sebstefan 6 hours agoprev>The first bug that our retrospective found was CVE-2015-5243. This is a monster of a bug, in which the prolific phpWhois library simply executes data obtained from the WHOIS server via the PHP ‘eval’ function, allowing instant RCE from any malicious WHOIS server. I don't want to live on this planet anymore reply coldpie 5 hours agoparentAs has been demonstrated many, many (many, many (many many many many many...)) times: there is no such thing as computer security. If you have data on a computer that is connected to the Internet, you should consider that data semi-public. If you put data on someone else's computer, you should consider that data fully public. Our computer security analogies are modeled around securing a home from burglars, but the actual threat model is the ocean surging 30 feet onto our beachfront community. The ocean will find the holes, no matter how small. We are not prepared for this. reply callalex 43 minutes agorootparentDo you use a bank account? Or do you still trade using only the shells you can carry in your arms? Perhaps networked computers are secure enough to be useful after all. reply ruthmarx 2 hours agorootparentprev> As has been demonstrated many, many (many, many (many many many many many...)) times: there is no such thing as computer security. Of course there is, and things are only getting more secure. Just because a lot of insecurity exists doesn't mean computer security isn't possible. reply coldpie 1 hour agorootparentIt's a matter of opinion, but no, I disagree. People are building new software all the time. It all has bugs. It will always have bugs. The only way to build secure software is to increase its cost by a factor of 100 or more (think medical and aviation software). No one is going to accept that. Computer security is impossible at the prices we can afford. That doesn't mean we can't use computers, but it does mean we need to assess the threats appropriately. I don't think most people do. reply GTP 2 hours agorootparentprev> Our computer security analogies are modeled around securing a home from burglars Well, no home is burglar-proof either. Just like with computer security, we define , often just implicitly, a threat model and then we decide which kind of security measures we use to protect our homes. But a determined burglar could still find a way in. And here we get to a classic security consideration: if the effort required to break your security is greater than the benefit obtained from doing so, you're adequately protected from most threats. reply coldpie 1 hour agorootparentI agree, my point is we need to be using the correct threat model when thinking about those risks. You might feel comfortable storing your unreplaceable valuables in a house that is reasonably secure against burglars, even if it's not perfectly secure. But you'd feel otherwise about an oceanfront property regularly facing 30 foot storm surges. I'm saying the latter is the correct frame of mind to be in when thinking about whether to put data onto an Internet-connected computer. It's no huge loss if the sea takes all the cat photos off my phone. But if you're a hospital or civil services admin hooking up your operation to the Internet, you gotta be prepared for it all to go out to sea one day, because it will. Is that worth the gains? reply myself248 41 minutes agorootparentprevAnd I think there's some cognitive problem that prevents people from understanding that \"the effort required to break your security\" has been rapidly trending towards zero. This makes the equation effectively useless. (Possibly even negative, when people go out and deliberately install apps that, by backdoor or by design, hoover up their data, etc. And when the mainstream OSes are disincentivized to prevent this because it's their business model too.) There was a time, not very long ago, when I could just tcpdump my cable-modem interface and know what every single packet was. The occasional scan or probe stuck out like a sore thumb. Today I'd be drinking from such a firehose of scans I don't even have words for it. It's not even beachfront property, we live in a damn submarine. reply ffsm8 4 hours agorootparentprevby this logic, every picture you'll ever take with your phone would be considered semi-public as phones are Internet connected. While I wouldn't have too much of an issue with that, I'm pretty sure I'm a minority with that reply coldpie 4 hours agorootparent> every picture you'll ever take with your phone would be considered semi-public as phones are Internet connected Correct. https://en.wikipedia.org/wiki/2014_celebrity_nude_photo_leak https://www.cybersecurity-insiders.com/glitch-makes-data-fro... https://arstechnica.com/gadgets/2023/09/apple-patches-clickl... reply detourdog 6 hours agoparentprevAlways look on the bright side of Life. The non-sensicalness of it is just a phase. Remember the Tower of Babel didn't stop humanity. Here is a link that was posted a few days ago regarding how great things are compared to 200 years ago. Ice cream has only become a common experience in the last 200 years.. https://ourworldindata.org/a-history-of-global-living-condit... reply larsnystrom 6 hours agoparentprevThe fact they're using `eval()` to execute variable assignment... They could've just used the WTF-feature in PHP with double dollar signs. $$var = $itm; would've been equivalent to their eval statement, but with less code and no RCE. reply hypeatei 6 hours agorootparentThe fact PHP is used for any critical web infrastructure is concerning. I used PHP professionally years ago and don't think it's that awful but certainly not something I'd consider for important systems. reply XCSme 5 hours agorootparentWouldn't \"eval\" in any language result in RCE? Isn't that the point of eval, to execute the given string command? reply account42 4 hours agorootparentFully compiled languages don't even have an eval at all. reply sebstefan 4 hours agorootparentNot with that attitude Start shipping the compiler with your code for infrastructure-agnostic RCEs reply adolph 2 hours agorootparentWhen you turn pro you call it security software and add it to the kernel. reply lambda 4 hours agorootparentprevNo, but they have system or the like, which is effectively the same, just being evaluated by the shell. https://man7.org/linux/man-pages/man3/system.3.html reply mdaniel 2 hours agorootparentAnd thanks to the magic of \"shoving strings from the Internet into a command line\", poof, RCE! It bit GitLab twice reply xrisk 54 minutes agorootparentWhat incident are you referring to? reply cryptonector 1 hour agorootparentprevYou can build an eval for a compiled language, absolutely. You can embed an interpreter, for example, or build one using closures. There's entire books on this, like LiSP in Small Pieces. reply dpcx 6 hours agorootparentprevI'm curious about some specifics of why you wouldn't use PHP for _critical_ web infrastructure? reply sebstefan 6 hours agorootparenthttps://duckduckgo.com/?q=hash+site:reddit.com/r/lolphp https://duckduckgo.com/?q=crypt+site:reddit.com/r/lolphp >crc32($str) and hash(\"crc32\",$str) use different algorithms .. >Password_verify() always returns true with some hash >md5('240610708') == md5('QNKCDZO') >crypt() on failure: returnstrcmp() will return 0 on error, can be used to bypass authentication > crc32 produces a negative signed int on 32bit machines but positive on 64bit mahines >5.3.7 Fails unit test, released anyway The takeaway from these titles is not the problems themselves but the pattern of failure and the issue of trusting the tool itself. Other than that if you've used php enough yourself you will absolutely find frustration in the standard library If you're looking for something more exhaustive there's the certified hood classic \"PHP: A fractal of bad design\" article as well that goes through ~~300+~~ 269 problems the language had and/or still has. https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/ Though most of it has been fixed since 2012, there's only so much you can do before the good programmers in your community (and job market) just leave the language. What's left is what's left. reply rty32 5 hours agorootparentPeople keep saying \"oh it's php 5.3 and before that are bad, things are much better now\", but ... reply KMnO4 6 hours agorootparentprevAny language can be insecure. There’s nothing inherently bad about PHP, other than it’s the lowest-hanging fruit of CGI languages and has some less-than-ideal design decisions. reply sebstefan 5 hours agorootparentDon't just swipe the \"less-than-ideal design decisions\" under the rug reply krageon 5 hours agorootparentprevIt's very easy to make PHP safe, certainly now that we've passed the 7 mark and we have internal ASTs. Even when using eval, it's beyond trivial to not make gross mistakes. reply bell-cot 6 hours agorootparentprevModern PHP is about as solid as comparable languages. It's two biggest problems are: Lingering bad reputation, from the bad old days Minimal barrier to entry - which both makes it a go-to for people who should not be writing production code in any language, and encourages many higher-skill folks to look down on it reply JW_00000 2 hours agoparentprevHave you ever witnessed a house being built? Everywhere is the same :) At least in our industry these issues are generally not life-threatening. reply brynb 6 hours agoparentprevthat seems like a bigger lift than just deciding to help fix the bug “be the change” or some such reply fanf2 5 hours agoprevThis is a fantastic exploit and I am appalled that CAs are still trying to use whois for this kind of thing. I expected the rise of the whois privacy services and privacy legislation would have made whois mostly useless for CAs years ago. > This is the approach taken by whois on Debian. Years ago I did some hacking on FreeBSD’s whois client, and its approach is to have as little built-in hardcoded knowledge as possible, and instead follow whois referrals. These are only de-facto semi-standard, i.e. they aren’t part of the protocol spec, but most whois servers provide referrals that are fairly easy to parse, and the number of exceptions and workarounds is easier to manage than a huge hardcoded list. FreeBSD’s whois starts from IANA’s whois server, which is one of the more helpful ones, and it basically solves the problem of finding TLD whois servers. Most of the pain comes from dealing with whois for IP addresses, because some of the RIRs are bad at referrals. There are some issues with weird behaviour from some TLD whois servers, but that’s relatively minor in comparison. reply vool 6 hours agoprevTLDR > While this has been interesting to document and research, we are a little exasperated. Something-something-hopefully-an-LLM-will-solve-all-of-these-problems-something-something. reply rixthefox 6 hours agoprev [–] > We recently performed research that started off \"well-intentioned\" (or as well-intentioned as we ever are) - to make vulnerabilities in WHOIS clients and how they parse responses from WHOIS servers exploitable in the real world (i.e. without needing to MITM etc). R̶i̶g̶h̶t̶ o̶f̶f̶ t̶h̶e̶ b̶a̶t̶, S̶T̶O̶P̶. I̶ d̶o̶n̶'t̶ c̶a̶r̶e̶ w̶h̶o̶ y̶o̶u̶ a̶r̶e̶ o̶r̶ h̶o̶w̶ \"w̶e̶l̶l̶-̶i̶n̶t̶e̶n̶t̶i̶o̶n̶e̶d̶\" s̶o̶m̶e̶o̶n̶e̶ i̶s̶. I̶n̶t̶e̶n̶t̶i̶o̶n̶a̶l̶l̶y̶ s̶p̶r̶i̶n̶k̶l̶i̶n̶g̶ i̶n̶ v̶u̶l̶n̶e̶r̶a̶b̶l̶e̶ c̶o̶d̶e̶, K̶N̶O̶W̶I̶N̶G̶L̶Y̶ a̶n̶d̶ W̶I̶L̶L̶I̶N̶G̶L̶Y̶ t̶o̶ \"a̶t̶ s̶o̶m̶e̶ p̶o̶i̶n̶t̶ a̶c̶h̶i̶e̶v̶e̶ R̶C̶E̶\" i̶s̶ b̶e̶h̶a̶v̶i̶o̶r̶ t̶h̶a̶t̶ I̶ c̶a̶n̶ n̶e̶i̶t̶h̶e̶r̶ c̶o̶n̶d̶o̶n̶e̶ n̶o̶r̶ s̶u̶p̶p̶o̶r̶t̶. I̶ t̶h̶o̶u̶g̶h̶t̶ t̶h̶i̶s̶ k̶i̶n̶d̶ o̶f̶ r̶o̶g̶u̶e̶ c̶o̶n̶t̶r̶i̶b̶u̶t̶i̶o̶n̶s̶ t̶o̶ p̶r̶o̶j̶e̶c̶t̶s̶ h̶a̶d̶ a̶ g̶r̶e̶a̶t̶ e̶x̶a̶m̶p̶l̶e̶ w̶i̶t̶h̶ t̶h̶e̶ U̶n̶i̶v̶e̶r̶s̶i̶t̶y̶ o̶f̶ M̶i̶n̶n̶e̶s̶o̶t̶a̶ o̶f̶ w̶h̶a̶t̶ n̶o̶t̶ t̶o̶ d̶o̶ w̶h̶e̶n̶ t̶h̶e̶y̶ g̶o̶t̶ a̶l̶l̶ t̶h̶e̶i̶r̶ c̶o̶n̶t̶r̶i̶b̶u̶t̶i̶o̶n̶s̶ r̶e̶v̶o̶k̶e̶d̶ a̶n̶d̶ f̶o̶r̶c̶e̶ r̶e̶v̶i̶e̶w̶e̶d̶ o̶n̶ t̶h̶e̶ L̶i̶n̶u̶x̶ k̶e̶r̶n̶e̶l̶. EDIT: This is not what the group has done upon further scrutiny of the article. It's just their very first sentence makes it sound like they were intentionally introducing vulnerabilities in existing codebases to achieve a result. I definitely can see that it should have been worded a bit better to make the reader aware that they had not contributed bad code but were finding existing vulnerabilities in software which is much better than where I went initially. reply rmnoon 5 hours agoparentMake sure you read the article since it doesn't look like they're doing that at all. The sentence you cited is pretty tricky to parse so your reaction is understandable. reply projektfu 5 hours agoparentprevI think you misinterpreted the sentence. They don't need to change the WHOIS client, it's already broken, exploitable, and surviving because the servers are nice to it. They needed to become the authoritative server (according to the client). They can do that with off-the-shelf code (or netcat) and don't need to mess with any supply chains. This is the problem with allowing a critical domain to expire and fall into evil hands when software you don't control would need to be updated to not use it. reply rixthefox 5 hours agorootparentYes, getting through the article I was happy to see that wasn't the case and was just vulnerabilities that had existed in those programs. Definitely they could have worded that better to make it not sound like they had been intentionally contributing bad code to projects. I'll update my original post to reflect that. reply josephg 5 hours agoparentprevI hear you. And I mostly agree. I’ve refused a couple genuine sounding offers lately to take over maintaining a couple packages I haven’t had time to update. But also, we really need our software supply chains to be resilient. That means building a better cultural immune system toward malicious contributors than “please don’t”. Because the bad guys won’t respect our stern, disapproving looks. reply SSLy 3 hours agoparentprevyou'd rather have blackhats do it and sell it to asian APT's? reply drekipus 5 hours agoparentprev [–] You're right. They should have just done it and told no one. We need to focus on the important things: not telling anyone, and not trying to break anything. It's important to just not have any knowledge on this stuff at all reply rixthefox 5 hours agorootparent [–] That was not my intention at all. My concern is groups who do that kind of red team testing on open source projects without first seeking approval from the maintainers risk unintentionally poisoning a lot more machines than they might initially expect. While I don't expect this kind of research to go away, I would rather it be done in a way that does not allow malicious contributions to somehow find their way into mission critical systems. It's one thing if you're trying to make sure that maintainers are actually reviewing code that is submitted to them and fully understanding \"bad code\" from good but a lot of open source projects are volunteer effort and maybe we should be shifting focus to how maintainers should be discouraged from accepting pull requests where they are not 100% confident in the code that has been submitted. Not every maintainer is going to be perfect but it's definitely not an easy problem to solve overnight by a simple change of policy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers exploited vulnerabilities in WHOIS clients and discovered the old .MOBI TLD WHOIS server domain was available for purchase, leading to unexpected control over the domain.",
      "Their server received 2.5 million queries from various entities, including government and military, cybersecurity companies, and Certificate Authorities (CAs), exposing significant security risks.",
      "The incident highlights vulnerabilities in the WHOIS system and CA verification process, emphasizing the need for continuous security testing and vigilance against outdated internet infrastructure."
    ],
    "commentSummary": [
      "Researchers spent $20 to gain Remote Code Execution (RCE) and accidentally became admins of the .mobi TLD due to an expired domain.",
      "The incident underscores the importance of never letting a domain expire and suggests that Verisign's monopoly on domains should be regulated.",
      "The article emphasizes the fragility of TLS/SSL and the critical need to maintain domain ownership to prevent security breaches."
    ],
    "points": 870,
    "commentCount": 212,
    "retryCount": 0,
    "time": 1726053552
  },
  {
    "id": 41510103,
    "title": "Why is Pave legal?",
    "originLink": "https://news.ycombinator.com/item?id=41510103",
    "originBody": "If you haven&#x27;t heard of it, Pave is a YC-backed startup that helps startups with compensation. I can&#x27;t actually access the system so I&#x27;m speaking from hearsay and what&#x27;s information on public parts of their website. The way I understand it works is that you connect Pave to your HR and Payroll systems, they take the data about who you employ and how much you pay them, combine it with all their other companies, and give companies a collective breakdown of compensation ranges.My question is, isn&#x27;t this specifically anti-competitive wage fixing? This seems exactly like RealPage but for employee compensation. As far as I know, colluding on wages like this is illegal. Is there something about the company that I&#x27;m missing?",
    "commentLink": "https://news.ycombinator.com/item?id=41510103",
    "commentBody": "Why is Pave legal?731 points by nowyoudont 8 hours agohidepastfavorite297 comments If you haven't heard of it, Pave is a YC-backed startup that helps startups with compensation. I can't actually access the system so I'm speaking from hearsay and what's information on public parts of their website. The way I understand it works is that you connect Pave to your HR and Payroll systems, they take the data about who you employ and how much you pay them, combine it with all their other companies, and give companies a collective breakdown of compensation ranges. My question is, isn't this specifically anti-competitive wage fixing? This seems exactly like RealPage but for employee compensation. As far as I know, colluding on wages like this is illegal. Is there something about the company that I'm missing? atrettel 3 hours agoI have never heard of Pave before, but this just sounds like yet another copy of Equifax's \"The Work Number\" [1]. Basically, HR at many companies gives your salary and employment history data to Equifax, who then sells access to the information to certain parties with supposed need to access it, including potential and current employers and creditors. This report is likely one of the most invasive consumer files out there for many people. I cannot comment on the legality of this kind of data sharing, but as I and others have pointed out, it has existed for a while. I do agree that it is concerning. You can freeze your Equifax The Work Number report at least, just like other credit reports. [1] https://theworknumber.com/ reply wing-_-nuts 3 hours agoparentI downloaded a personal report from the work number website and found to my horror that my employer was reporting every. single. paystub. gross and net, to equifax. That felt like a huge breach of privacy. Given that equifax had already proven incompetent at keeping my data secure, I immediately sent HR a request to stop sending my supposedly 'confidential' pay info. They politely told me to kick rocks, so I went on TWN's website and froze that report so no one would be able to request it, and it will be a cold day in hell before I thaw it. reply iav 2 hours agorootparentI am an investor in equifax. Let me clear up a misconception on where the data comes from. Half the data comes from large enterprise customers, who “sell” the data in exchange for Equifax doing I-9 verification for free. The other half comes from 39 payroll companies. Every single payroll company except for Rippling and Gusto sell paystub data to Euifax. (Rippling will start next year). Those are exclusive revenue share deals. You cannot be a competitive payroll provider without the revenue share from Equifax. So before you blame your employer, they might not be selling it directly and even if they opted out, your payroll company will sell it anyway. reply dsr_ 2 hours agorootparentYou make an excellent argument here for tight regulation of the industry. reply Kon-Peki 1 hour agorootparent… and the usage? Most highly-paid people have no idea how much privilege this affords them. You wonder why so many businesses are nice to you? It’s because they’ve already looked you up and know you’ve got a high income and are a millionaire. Write a personal check for your next automobile? Sure thing, you can drive it off the lot a few minutes later. They won’t even bother cashing the check for a week or two. Try doing something like that as an hourly worker, even if you’ve got the money in the bank. reply gperkins978 1 hour agorootparentThis is also why certain homes get hit in high-end burglary crews. There are multiple crews hitting those who purchase precious metals with physical delivery (like gold American Eagle coins). It is not all positive. Considering how few victims even bother to report such crimes, it is terrifying. From what I understand from my cousin, a career criminal, there are entire theft rings working off of databases such as these. He knew mostly of car-related theft rings, but I hear about safe-cracking burglaries quite often, usually stealing Rolex watches or precious metals. reply llamaimperative 3 minutes agorootparentprevNow this is some remarkable gymnastics. Are you also an investor in Equifax or have some other financial interest in similar services? If not, I'm very curious to hear how you tied yourself into this knot. wing-_-nuts 1 hour agorootparentprevNo thank you. I value my privacy and my negotiating ability more than I value a 'service' I didn't even ask for. reply Kon-Peki 1 hour agorootparentNobody asked for it. But it’s part of the world we live in. And we’re all walking around oblivious to the advantages and disadvantages it gives us. reply dsr_ 1 hour agorootparentprevThis is the view from a bubble I am not familiar with, and really don't care about. reply Guvante 1 hour agorootparentprevThe finance companies are nice enough that it doesn't really matter. Bought mine with cash but realized it was Sunday and I didn't have a way to get a cashier's check from a savings account. They offered to put the down payment on a credit card and finance. Paid it off once I had access to the account. Ended up being a wash, the points were worth a little more than the percentage charge. reply oblio 50 minutes agorootparentprev> Write a personal check for your next automobile? Personal check? What year is this?!? :-) reply ghaff 22 minutes agorootparentI did this 2 years ago. I write personal checks all the time (although many are actually \"written\" by my bank in the US). reply duderific 40 minutes agorootparentprevI did this for my recent automobile purchase. It's very convenient from my perspective to simply write a check and hand it to them. reply 369548684892826 10 minutes agorootparentBut why not just pay by card, that must be even easier? tourmalinetaco 1 hour agorootparentprevThat sounds like the most unappealing exchange imaginable. Yes, let me lose both bargaining power with new jobs while simultaneously painting a target on my back, all in exchange for companies being more willing to take my money. reply fragmede 1 hour agorootparentprevPersonal check? You can buy a car in full with a credit card if you pass the vibe check. reply MichaelZuo 1 hour agorootparentA personal check is much less secure because it’s not linked up to the network anti fraud systems. reply Arelius 2 hours agorootparentprevDo you have a sense of why, according to you Gusto will remain the only company that doesn't sell payroll data to Equifax? reply dawnerd 14 minutes agorootparentSeems like something gusto can turn into a marketing point. Surely there’s a desire for a privacy respecting payroll/hr platform. reply jnwatson 1 hour agorootparentprevGusto is still pre-revenue? reply SoftTalker 3 hours agorootparentprevDon't ever work in the public sector then. Your salary is public record, open to anyone who is curious enough to look. reply hypeatei 3 hours agorootparentI think that's widely understood and part of the job description of being a public servant. What's not widely understood is HR secretly selling your data while working at a private company. reply hunter2_ 2 hours agorootparent> your data Is it yours though? The employer could probably argue that it's theirs. Devil's advocate: I think it's widely understood that entities can be transparent with their data if they choose, other than NDA scenarios. reply mapt 53 minutes agorootparentIn a market-first values system, where we rely on the labor market to largely self-regulate given the promises that free market idealogues & corporate actors made us, colluding on wages like this should lead to scorched-earth retribution from the FTC. Not \"Oh hey there, you're not allowed to do that, stop that\", but \"We are diluting your stock by a quarter and distributing it to your workers\" type shit. reply karaterobot 31 minutes agorootparentprevDoes my personal health information belong to my doctor? Not according to HIPAA, at least not in a way that gives the doctor control over selling it. While my pay is currently not protected by similar regulation, it seems like the kind of protection regulation similar to HIPAA could defensibly target. reply GTP 2 hours agorootparentprevWell, if we're discussing whose data it is the information about how much I pay you, even from a devi's advocate perspective, you can't do better than arguing that this data pertains to both of us. So we should share the property of that data somehow. I don't see how you could argue that that data would be solely the employer's data. reply hunter2_ 2 hours agorootparentIf I administer a survey, collect responses, and put them into a spreadsheet, is the data in that spreadsheet not mine despite the fact that it consists of things that other people told me? I can't share it without the permission of those surveyed, assuming I didn't promise not to? reply wavemode 45 minutes agorootparentA key distinction is that people need employment. People don't need to fill out surveys. That's why there are many things companies aren't allowed to require of their employees, that they are allowed to require of other parties. So while, in many jurisdictions, it's fine for companies to sell data collected on their employees, and it could be argued that those employees consented to this data sharing by working there, one could also easily argue for an employee protection law that prevents companies from requiring their employees to consent to this. reply BobbyJo 46 minutes agorootparentprevThese are answered questions. If you are talking about the raw data, you have to get the respondents to agree that their answers become your property (either implicitly or explicitly, there are rules around both), or no, you do not own the data in the spreadsheet. reply GTP 1 hour agorootparentprevIt really depends on which kind of data you're collecting. If you're collecting health related data that is linkable to the people it pertains to, the GDPR would prevent you from sharing that data with third parties without one of the admissible legal basis, the most common of which is the consent of the people whose data you collected. In the case of health data, maybe even USA laws would prevent you from sharing it. Edit: it is now some time since I studied the GDPR, so I'm actually unsure if, for healt-related data, it can be used any legal basis other than consent. The reason being that health, together with a few other categories, has special protections. reply willcipriano 1 hour agorootparentprevTechnically sure, but the sort of people who live that way don't get invited out anywhere. reply Retric 2 hours agorootparentprevMost companies request people not share pay information. Information asymmetry is a huge deal in negotiations. reply dcrazy 2 hours agorootparentSuch a “requirement” is illegal. https://www.nlrb.gov/about-nlrb/rights-we-protect/your-right... reply skyyler 2 hours agorootparentAnd pot was illegal in the 70s. reply jgalt212 1 hour agorootparentIt still is. reply ghaff 20 minutes agorootparentAt the US federal level. It may or may not be illegal at the state level--and is regulated in any case. mcherm 1 hour agorootparentprevBut in the US, federal labor law makes it illegal for employers to prevent employees from sharing pay information (at least for employees who are entitled to unionize). reply vkou 1 hour agorootparentprevThey can request it, but can't stop you if you do. You can also request them to do likewise, with similar recourse. A request is nothing without teeth behind it. reply klingoff 1 hour agorootparentTeeth like employment at-will? reply vkou 30 minutes agorootparentIt's generally quite unlikely that sharing your salary is going to result in getting bitten by that. You'd need to do labour organization (or be completely surrounded by rats and snitches and other vermin at your workplace, who already have an axe to grind) to actually get blowback for this stuff. Most of the taboo around it is cultural, because people here attach their self-worth to their paycheck. You could also always do it anonymously or pseudonymously. You'd have almost no chances of retaliation in that case. reply klingoff 24 minutes agorootparenthttps://news.ycombinator.com/item?id=32216332 reply vkou 2 minutes agorootparentThe singular of anecdote is not data. Getting hit by lightning is also unlikely, but it happens to thousands of people every year. That doesn't mean I'll be flying a kite in a thunderstorm, but it also doesn't mean that you should lock yourself in a bunker the moment the sky turns grey. Most of the taboo around this is cultural, not retributive. abeppu 1 hour agorootparentprev... should companies be nervous about this also though? Is the decision for their payroll info to be visible to unknown buyers an intentional, well-considered one? Is this effectively leaking potentially strategically important info? Like, I haven't seen this happen, but could a recruiting team buy the compensation data on staff at a competing firm, identify those that look like a good deal, and poach them starting with a \"we'll offer you k% more than your current employer\"? Could market analysts use this data to notice when a company starts firing more people, or starts giving fewer/smaller raises? What if the next time your company showed up in a Gartner or Forrester report, it came along with a caveat \"however given decreased investment in staff, their pace of product development or quality of client services may be at risk.\" reply samus 1 hour agorootparentprevThe employer requires this data to do payroll correctly. Apart from that, it sound only be used for expressly authorized purposes. But maybe that's a european GDPR-influenced way of seeing this issue. reply tptacek 48 minutes agorootparentprevWhy would that be a widely understood part of the job description? Almost every American teacher, firefighter, planner, street engineer, health inspector, police officer, train conductor, bus driver, along with the managers, office administrative staff, janitors, and groundskeepers that support those activities are public sector employees. What do they have in common that would suggest they deserve less privacy than you do? Most of these jobs are not special or meaningfully \"public\". They're just normal jobs for firms that happen to be public bodies. I don't think it's at all obvious that people are knowingly and deliberately making these tradeoffs by working there. reply Barrin92 5 minutes agorootparent>What do they have in common that would suggest they deserve less privacy than you do? That they receive their salary from the tax payer, the public is their employer. They're executive organs of the state, police and firefighters, unlike private workers, also don't get to choose what laws they enforce or what fires they put out. If you're a civil servant you obviously forego most of the rights of private sector workers in exchange for usually lifetime employment and set pay rates. wing-_-nuts 2 hours agorootparentprevYeah I used to work for the navy. Pay was standardized under the GS pay schedule and anybody could have looked that up. I was fine with that. In the private sector, your comp is determined by a negotiation undermined by an asymmetric information disparity. HR at a hiring company has way more information around market comp as it is without having your exact current comp when they make an offer. What I find particularly egregious about this is that management at this company had admonished me that my comp was 'confidential' and that I shouldn't discuss it, while simultaneously selling it to equifax. reply samus 1 hour agorootparentThere are countries (Sweden IIRC) where the salary record is public, probably to eliminate this information asymmetry. reply worstspotgain 21 minutes agorootparentFinland has National Jealousy Day: https://www.weforum.org/agenda/2018/11/finland-has-just-publ... reply SoftTalker 2 hours agorootparentprevSome jobs fall under this \"public sector\" transparency but work much more like a private employer when it comes to salary negotiation. For example a state university recruits staff and negotiates compensation much like a private employer (no equity options of course) but your salary will be public if you are hired. reply gperkins978 1 hour agorootparentprevPublic servants do not make enough money to be useful targets. The meaningful threat comes from large compensation tied to other asset information (tying an online person to that income, not difficult). You can buy lists of these already tied up and ready to download for your scheming pleasure. From English Rolex robbers to Florida kidnappers, they all enjoy the data. I do not think it can be stopped, but the days when a wealthy person could safely live in a suburb and have the kids imagine that they are middle class is long gone. It is terrifying. The best thing for a wealthy discrete person to do is move to Singapore or Australia, or somewhere with a sufficiently low crime rate to feel comfortable, or get quality security, which sucks. reply fragmede 47 minutes agorootparentThe security minded can move to a gated community, which are all over the place and have existed for a very long time, and don't require moving to Singapore or Australia to live in one. reply zdp7 1 hour agorootparentprevThe information available via the public record is not as detailed (typically annual salary)and not definitively tied to any person. The Work Number is tied to your SSN and is much more detailed than the public record (each paycheck and a breakdown of different compensation). reply gperkins978 1 hour agorootparentIn the US, most municipalities will publish each employee's compensation every year. You can literally look them up by name. reply admissionsguy 3 hours agorootparentprevor live in Sweden (where your earnings as well as your address and property, car or pet ownership are public record) reply f1shy 2 hours agorootparentI’ve read about that. Is ipen to ANYbody? Is there a link? Thanks reply uriah 1 hour agorootparentprevMany if not most companies outsource employment verification to The Work Number. When you get a new job, a frozen report will complicate your background check. They don't give out salary info in employment checks though. AFAIK they require your explicit permission except for government agencies who use it to verify your eligibility for benefits. I would be surprised if they are not selling aggregate salary data though reply wing-_-nuts 1 hour agorootparentIf they want my info, they can ask me. I would rather them not have this info before an offer is made. reply uriah 1 hour agorootparentThat's normally how it goes. At least, I've always had the background check happen after an offer is signed. It's usually a separate company and they just report back whether your job titles/employment dates match your resume reply ghaff 14 minutes agorootparentI don't know how common it actually is. I've always provided references and probably OKd a background check but post-school my few jobs have always been through people I knew and there was really no reason to run a check except fr pro-forma reasons. reply idbehold 49 minutes agoparentprevThe freeze is mostly ineffective for when you actually want it to work. From what I remember (even for the credit freezes) is that if you provide written consent to, say, a background check, then that overrides your freeze. So if you're applying for a job (basically the major instance where you'd want your salary information private) they're going to ask for your consent to do a background check and bingo they'll know how much money you make. IMO this type of information should be illegal to sell or request. reply jnwatson 1 hour agoparentprevI froze the report, and I also told my employer not to report anything to Equifax (which luckily my employer allows). This made getting approved for a mortgage more difficult. These days, loan officers just expect to be able to hit a button and get all your info. We're losing the privacy battle. reply Panini_Jones 3 hours agoparentprevAs a datapoint for how I've seen this used in the real world, I've spoken to startups who will defer to Pave regarding how much they'll offer to pay. The startup I spoke to said 'We pay you the 85th percentile for your YOE and role based on Pave data'. reply nsxwolf 2 hours agorootparentThen I want 99th. reply abhisharma2 2 hours agoparentprevIn case folks want to quickly know how to start a freeze, heres the info from the website: To communicate a freeze request, send an email to the address below requesting a Freeze Placement Form: TWNFreeze@equifax.com reply no_wizard 1 hour agoparentprevThese services feel not dissimilar to the Realpagr case that is ongoing now with rent price fixing. How does this ultimately not end up having a depressing impact on salaries? reply immibis 26 minutes agorootparentIt does depress salaries, which is the point. reply blackeyeblitzar 1 hour agoparentprevPave is a company that has been snapping up other existing companies that performed this kind of aggregation of compensation data. Basically companies look at this benchmarking data to figure out what they should pay for different jobs and levels. Just some extent companies genuinely need this kind of data to figure out what to do. But I also think it breaks supply and demand. Companies are not discovering price of labor but just using each other’s signals to decide what to pay collectively https://www.pave.com/blog-posts/announcing-paves-series-c-an... reply yonran 3 hours agoprevThe FTC Guidelines for Collaborations Among Competitors https://www.ftc.gov/sites/default/files/attachments/dealings... says it’s illegal to share “competitively sensitive variables”, not just any data. Some forms of data sharing such as industry averages may not be illegal, but more detailed data such as numbers of applicants or price elasticity that enable the companies to act together as if they were a monopoly probably are. RealPage crossed the line by sharing an optimization algorithm and encouraging collective action. I’m not sure what Pave does. reply pwillia7 3 hours agoparent> Under the Sherman Act, agreements among competitors to fix prices or wages, rig bids, or allocate customers, workers, or markets, are criminal violations. Other agreements such as exclusive contracts that reduce competition may also violate the Sherman Antitrust Act and are subject to civil enforcement.Dec 20, 2023 A good US.A. could probably argue this meets that bar. Didn't they just do something similar with the rent fixing from that similar SaaS product? reply ecshafer 39 minutes agorootparentOp mentioned realpage, which is the company that does rent fixing as a service (RFAAS) reply uoaei 2 hours agoparentprevHow sure are you that the incentives in place don't encourage inching toward this behavior as they begin to establish themselves? reply billjings 3 hours agoprevAs described, it is a fair ways away from what RealPage is doing. Specifically: * RealPage sells raising rents, not just market info. * RealPage pressures clients into taking their higher rents. * RealPage also pressure clients to refuse to rent at lower rates for their own narrow economic interest - in other words, they actively seek to circumvent competitive pressure to keep rents high. (edit: to clarify, I mean they discourage lowering rent to attract a renter) Pave does sound like it gives businesses a leg up over employees in wage negotiations, but until it e.g. starts promising clients that they will be able to pay lower salaries, the critical element of coordination won't be in the mix. Pave gives you the data, but you can still choose to pay above market to attract talent. reply __loam 2 hours agoparentWhat's the point of getting this data if it's not to pay less money? What is the value add? reply dmattia 2 hours agorootparentIt's almost certainly for the companies to pay less money, but with a more generous reading, I think it could be argued that that doesn't necessarily have to come out of employee salaries. That data could be used to: - Set reasonable ranges to find the right candidates they are looking for faster and minimize hiring friction - Standardize payment levels in a way that reduces legal liability in certain states like Colorado/California. Or the most generous reading of \"reduces legal liability\" would be \"promoting fairness\". - Reduce the time spent by HR/other teams of negotiating or setting salaries, as they can simply target some target like \"we want to pay more than 60% of companies like us\" - For budgeting/forecasting with new hires, this allows companies to have more confidence in their estimates as they plan hiring. - Some companies now offer calculators even before you're hired with what your salary/compensation might look like, such as https://posthog.com/handbook/people/compensation But yes, overall I do believe that most companies also expect a general reduction in salaries when they use these tools. reply kstrauser 1 hour agorootparentprevI routinely get emails like \"we'd like to hire you as our CTO, and because we just got a bunch of VC money, we're prepared to offer you a generous comp package of up to $90,000 salary plus .05% equity! Must be onsite in San Francisco.\" If they were aware of market rates, they could avoid making potential candidates laugh at them. reply danielmarkbruce 2 hours agorootparentprevTo pay more money. reply Retric 2 hours agorootparentThe limit on pay is the amount of money they can budget to the position not what other people are paying. reply danielmarkbruce 2 hours agorootparentAnd how do they arrive at the budgeted number? Lots of companies want to ensure they are paying a sufficiently high number to get sufficiently capable employees in a competitive market. While many (including me) find things like Pave gross, it's not a one way street, they can push wages up. reply Retric 2 hours agorootparentYou’re thinking of the actual budget for a position not what a company could in theory budget. A small businesses owner who pays themselves whatever is left over after expenses doesn’t care about what other companies pay, the company only has so much money. Apple could increase salaries up to the point where they make zero profit, but the goal is profit maximization not salary maximization. It’s fundamentally the attempt to limit salaries that causes companies to look at the overall market. reply danielmarkbruce 40 minutes agorootparentSmall business owners aren't the target market and are likely to not use such a product. Hiring well is hard - it's not super obvious if you aren't paying enough or your company isn't desirable or what else is the cause of not seeing good candidates. While in theory you could solve that by wildly overpaying, in practice you have to be able to justify your decision to higher ups in most cases, and pointing to a tool that shows what you really need to pay to get good people can be very helpful. I still find it gross, but, there are practical situations where it will drive salaries higher. reply ars 2 hours agoparentprevYah, that's the main difference: RealPage pressured landlords (i.e. tracked then) if they did not raise rents based on its recommendations. If they had limited themselves to simply reporting the numbers, and letting landlords make their own decisions they would probably be legal. reply bjornsing 16 minutes agoprev> My question is, isn't this specifically anti-competitive wage fixing? This seems exactly like RealPage but for employee compensation. As far as I know, colluding on wages like this is illegal. As long as Pave just helps employers look backward in time so to speak I’m not sure I’d call it collusion. But if they enable some kind of coordination between future potential employers, then yes, maybe it is. In the RealPage case the coordination aspect consisted of providing a recommended rent for the property if I understand it correctly. I guess the equivalent for Pave would be if they gave a recommendation on what compensation to offer. reply ghaff 11 minutes agoparentCompanies compare pricing all the time even if it doesn't involve smoke-filled rooms with execs doing tit-for-tat. You don't think your local grocery store knows what the other local chain is charging (or what they're paying their employees)? reply a123b456c 1 hour agoprevNot a lawyer, but this document seems highly relevant. DOJ Withdraws “Safety Zones” for Information Sharing and Other Collaborations https://www.crowell.com/en/insights/client-alerts/doj-withdr... reply jasode 3 hours agoprevCompetitors making similar price adjustments or salary adjustments based on seeing each other's public announcements or sharing data is legal: https://en.wikipedia.org/wiki/Tacit_collusion That's why it's \"legal tacit collusion\" when one leading law firm announces salary increases and other law firms immediately match it: https://www.reuters.com/legal/legalindustry/large-law-firms-... That type of salary matching has been happening for decades. What's illegal is competitors making agreements with each other to set wages -- via secret emails, etc. reply impossiblefork 5 minutes agoparentYes, but the information here is not public, since it's being sold as a service. If it were public, employees and job seekers would also have the information. reply nowyoudont 29 minutes agoparentprevHuh, this tacit collusion being legal thing is mind boggling. The law firm example seems imperfect though. Publicly announcing that you’re raising salaries isn’t really the same as internally sharing that data and choosing to set the same salary based on that. reply w10-1 56 minutes agoprevI hear the complaint but it's a bit of a trap to just seek protection. Yes, the law on point is permissive. That goes with the evolution of law. But assuming for the moment that we want not just avoid injury to ourselves but to create the world now and to come, what are we called to do? - What exactly do you, or employees, want in this situation? - What would Pave do if they wanted to take the high ground? Could that be a business differentiator? - What law could you write and enforce, to protect what interest, without also damaging other interests that are socially beneficial? I think the organizational evolution towards having loose laws with tightening enforcement, or tight laws with lax enforcement, give way too much latitude to policing/enforcement and create a corrupting political franchise of affected stakeholders taxed with managing regulators. My hope would be that internet-scalable transactions have similarly scalable regulatory solutions: dead-simple to detect and assess, finely-tuned to the balance of interests, and so patent as to be indisputable. Then people can get stuff done without dealing with the shadows and forces of ambiguity. Is something like that possible here? Could Pave be a champion of it? reply rossdavidh 32 minutes agoprevSo, I'm sure the format and medium is different, but companies HR depts have been sharing notes and checking on what the market compensation is (for any given job and experience level) since forever. IANAL, but this is not a new thing. I've even known co-workers who said \"I deserve more $$, I'm underpaid for my experience and job\" and supervisor repeats this to HR, then HR checks on it, and a few days later says \"true, here's a raise\". This was in the 90's, btw. Not a new thing. reply msy 7 hours agoprevCompanies like Radford have been doing this for decades and are used by pretty much everyone, Pave is just a more efficient version of the same game. reply levi-turner 5 hours agoparentBeyond salary, there is a whole industry of data brokers who get transactional data from individual participants in an industry vertical (CPG, Health Insurance, Salary, etc), aggregate it with their competitors and present it back to those participants as benchmarks. Management Consulting likewise is a way to launder getting strategic insight into your competitors from a third-party. reply grayfaced 4 hours agoprevHate it in concept (I can't speak to Pave specifically). Metrics are useful if the manager doesn't make hard targets, the data is good and the model is good. How do you model compensation packages? how does 20 days leave compare to unlimited leave? Are two companies with unlimited leave equivalent? When the managers have targets, they're incentivized to massage the data in their favor. So even if you're doing it right, that makes every other companies data suspect. This also sounds like it will reinforce some negative behaviors. If the data shows that other companies are paying women 80% of their male peers, shouldn't this recommend I also pay women 80%? But I doubt the output will spell it out that way, will I even know it's influencing me this way? reply tensor 3 hours agoparentNo, you don't set salary bands based on race or sex. That sounds like it would be illegal. The way that bias creeps in is not from data gathering and setting salary ranges, it's from managers bias when they choose from the ranges for candidates. Setting company wide salary bands actually HELPS fairness in pay by providing objective ways employees can argue that they are underpaid if that's the case. reply t-writescode 3 hours agorootparent> it's from managers bias when they choose from the ranges for candidates. It also comes in from how / if they choose to promote. If they take too long or if they just don't put you up for promotion / reject you, overall earning potential is weakened. Do this enough times and it becomes a substantial reduction in life-long earnings, life-long title, respect, etc. reply hi-v-rocknroll 14 minutes agoprevThey already expect people to work for next to nothing simply because they're \"startups\". Instead, maybe they should focus on hiring decent people AND paying them industry-fair wages. reply gaivota 15 minutes agoprevI've been hired to a startup that uses Pave - they mentioned that they paid at the 75% percentile, and my offer ended up being more competitive than others I was getting. I think that while obviously some companies can use this data to pay less, the more likely outcome is that they pay MORE. You forget that companies are competing with each other for talent, they are not in the business of colluding together. So if I can see that other companies are paying X, I am more inclined to pay X + Y reply HeyLaughingBoy 2 hours agoprevThe overwhelming question I have when reading these responses is \"don't you guys read salary survey data when you're looking for a new job, or at annual review time?\" reply amelius 2 hours agoparentBut then we're not sharing other people's personal information. reply blast 31 minutes agorootparentAre they actually sharing personal information? From the OP it sounds like they're sharing aggregate data. (I've never heard of Pave.) reply Leherenn 2 hours agorootparentprevIsn't levels.fyi or similar just the employee side of the coin? reply nowyoudont 26 minutes agoparentprevBut the salary data that’s available online to me as an employee is imperfect and extremely limited. This would be like if every employee of a major company sent their exact salary and demographic information to levels.fyi, which would never happen because it’s an insane sacrifice of privacy reply elawler24 7 hours agoprevI got access to Pave through one of my investors. Seeing the data made us set salaries and contractor rates higher, not lower. It’s like salary banding at big companies. It’s a framework for how much other people are paid at the same level, not a contract. HR will make it seem like a rule, but if you do spectacular work - you can always negotiate. Collusion requires an agreement between rivals that negatively disrupts market equilibrium. Is this company not actually making the market more efficient and transparent? That said - an efficient market is good for the collective, not necessarily the rogue / outlier individual. reply bjourne 4 hours agoparent> Collusion requires an agreement between rivals that negatively disrupts market equilibrium. Is this company not actually making the market more efficient and transparent? That said - an efficient market is good for the collective, not necessarily the rogue / outlier individual. I'm sure you practice what you preach and tell all your employees what their coworkers earn? reply ativzzz 4 hours agorootparentI'm sure he means transparency in the market where employers are competing with each other, not the market where individual employees are choosing employers With a few exceptions, I've found that companies that talk about transparency are transparent with everything but employee salary reply lantry 6 hours agoparentprevThe market might be more transparent for the people who have access to pave, but for those who dont, the information asymmetry becomes worse. reply vundercind 4 hours agorootparentDing ding ding. Improving information for only the already-more-powerful side of such an asymmetric relationship doesn’t help the weaker side. reply HeyLaughingBoy 3 hours agoparentprev> HR will make it seem like a rule, but if you do spectacular work - you can always negotiate. Every single developer should take this to heart. The phrase I once used at the end of an annual review was, \"you can't give me a review like that but a raise like this!\" Yes, my manager had to get permission to give me the % increase I wanted, but it was to his benefit to do it since he wanted me to stay. reply mcntsh 7 hours agoprevWage fixing is when multiple companies agree to set wages at a certain amount. Sharing compensation data across companies doesn't necessarily mean wage fixing. Company A can use the compensation data from Company B to try and compete better for talent. Not saying thats what it will be used for, but it's technically not wage fixing. reply the_mitsuhiko 7 hours agoparent> Wage fixing is when multiple companies agree to set wages at a certain amount. I am not an expert on wage fixing laws in the US, but I came across a class action on wage fixing a few days ago (Ron Brown et al v JBS USA Food Company et al) where part of what was aledged was the illegal exchange of salary data via surveys [1]. > The Red Meat Industry Compensation Survey conducted by WMS on behalf of the Defendant Processors violated the Safe Harbor Guidelines in at least three ways. First, the Defendant Processors, not WMS, collectively managed and controlled the annual Red Meat Industry Compensation Surveys. Second, those Surveys often contained information about the Defendant Processors’ future compensation plans and practices. Third, Defendant Processors had extensive discussions about the Survey results, including at in-person meetings, during which they disclosed their respective compensation rates, practices, and plans [1]: https://www.classaction.org/media/brown-et-al-v-jbs-usa-food... reply nowyoudont 7 hours agoparentprevI'm asking this as someone with 0 legal knowledge: doesn't the context matter? If every company takes this data and is like \"we want to pay at the 95th percentile\" (which is what they all do), that seems like wage fixing even if they're not all agreeing to it together. reply aikinai 7 hours agorootparentIf they’re all shooting for the 95th percentile and have up-to-date data then you certainly won’t have fixing; rather you’ll get insanely rapid wage inflation! reply michaelt 7 hours agorootparentThere's also a more cynical explanation. It's possible the purpose of wage benchmarking companies is to allow bosses to say they pay the 95th percentile - which is useful to be able to say, when someone at an all-hands Q&A asks about raises and bonuses. Then the benchmarking company simply has to define 'comparable roles' broadly enough to give the customer the result they want. reply itronitron 4 hours agorootparentprevNot necessarily, if everyone's wages (except 5%) were set at minimum wage then the 95th percentile would be the minimum wage. reply wccrawford 7 hours agorootparentprevYeah, it seems more like they'd all shoot for 45th percentile and say \"We pay competitive wages\" instead, slowly driving the wages down. reply throwway120385 4 hours agorootparentThat's what my employer does. The head of our HR team got in front of the entire company and said that they aim for 50th percentile for everyone in every pay band. It instantly made me want to job hop, tempered only by the million things I have going on in my personal life that have a better expected value than a 5 or 10k pay bump. reply hobs 4 hours agorootparentThat's... hilarious. We all know they are thinking that but to say it loudly and proudly to the employees is a self own on a level that makes me Cheshire Cat. reply tensor 3 hours agorootparentprevI think some basic math knowledge would help more, if every company paid at the 95th percentile then it wouldn't be the 95th percentile, it would in fact be the average. But no, these distributions are not flat like that, there is a large spread and \"by definition\" of the 95th percentile only a few companies pay at that rate. reply dctoedt 7 hours agorootparentprev> If every company takes this data and is like \"we want to pay at the 95th percentile\" It's thought by some that this is how CEO compensation has gone up so much: Corporate boards of directors have compensation committees, which are fed survey data about comp ranges; a comp committee will say, \"We want our CEO's comp to be in the top quartile\" — which, as time goes on, leads to an inexorable upward ratchet effect. reply mcntsh 7 hours agorootparentprevIf you opened up a business selling water bottles, you'd probably check what price water sells at across brands, then decide in which segment to price it. \"I want to sell my water at the upper end and market it as a gourmet brand\" reply drw85 4 hours agorootparentBut in this case you're not selling, you're buying. reply vishnugupta 7 hours agorootparentprevThat’s how pricing works in a market? In fact if every company did pay at 95th percentile then I’d say it’s a good outcome. There’s a 5 percentile slack which is not too bad? reply Suppafly 3 hours agoparentprev>Company A can use the compensation data from Company B to try and compete better for talent. My company has done this in the past sorta indirectly, we were losing a lot of people to competitors and data like this is how they justified paying a bunch of us better so we wouldn't leave. I agree that it could be used to fix wages, but companies will always have to pay their best talent more if they want to retain them, whether that means paying them above what the data says or if it means inventing new job titles for them to progress into. reply neilv 51 minutes agoparentprev> Company A can use the compensation data from Company B to try and compete better for talent. Company A could make offers and negotiate with prospective hires based on the value they can get out of the hire. Rather than secretly leverage surveillance capitalism against the prospective hire, to base their offer on what the person is currently making (and, hey, if lots of employers do that by convention, you pretty much have collusion). reply chipgap98 7 hours agoparentprevStochastic wage fixing is still wage fixing reply sameoldtune 7 hours agorootparentYour honor, the algorithm made me do it! reply toomuchtodo 4 hours agorootparenthttps://www.ftc.gov/business-guidance/blog/2024/03/price-fix... reply jskrablin 7 hours agorootparentprevChatGPT did it! reply econcon 7 hours agorootparentprevunless you collude with other companies, doesn't seem like it is. reply willseth 4 hours agorootparentThat is the entire basis for the RealPage lawsuit. The point is that if the effect on pricing is indistinguishable from price fixing, it doesn't matter if the act of colluding is abstracted into and laundered through a 3rd party with an algorithmic system responsible for setting prices. reply ambicapter 3 hours agorootparentHonestly, I think it does still matter. The basis for the RealPage lawsuit seems to be that people inside and outside the company glibly considered it price-fixing, and said it out loud to each other. The didn't really seem to make the case that it was \"algorithmic price-fixing\" (Disclaimer: not a lawyer). You can only argue in court about existing laws, so until algorithmic price-fixing is written in the law books (or settled case law) you're gonna have a tough time bringing that up to a judge. reply willseth 3 hours agorootparentIt seems you don't understand the lawsuit. Most of the claims are based on the actual mechanics of how algorithmic price fixing does violate existing laws. reply theGnuMe 7 hours agorootparentprevunless disclosed to employees and applicants this seems like de facto colluding. I always ask myself, as to the legality or ethics, would this survive review by a jury of my peers... reply fastball 4 hours agorootparentYou think it is more ethical for wage data to be kept secret? Why? Rarely is requiring secrecy the more ethical option. reply cryptonym 4 hours agorootparentWhat would be the least ethical would be keeping it secret to one party and having the other party sharing data... Oh wait! reply HeyLaughingBoy 3 hours agorootparentGlassdoor exists. reply simion314 2 hours agorootparent>Glassdoor exists. you are not allowed to tell anyone how much you make so you might be in trouble if your found out but the companies share this info without your consent. From my POV make it all transparent. reply HeyLaughingBoy 46 minutes agorootparentI have had a career spanning over 30 years at this point. I've worked in businesses with 6 employees and F500 corporations. No one has ever told me that I can't tell anyone else how much I make. reply theGnuMe 1 hour agorootparentprevI never said that. reply dsotirovski 7 hours agoprevGratitude for the expose. I was(and assume most here were as well) completely unaware of Pave. I will, and hopefully other potentially impacted, look up if I am affected by this and if so - share experiences/info here. reply jmull 3 hours agoprevSharing compensation ranges may not be enough to qualify as wage fixing. It seems like there would have to be an agreement or at least a collective incentive to lower wages. While companies could use the data to ensure their offers are always below average (pushing wages down), they could also use the data to ensure their offers are above average, pushing wages up. Think about it: prices are transparent in most markets and that doesn't seem to generally lead to anti-competitive prices. In fact, it seems to encourage companies to compete. reply atomicnumber3 3 hours agoparent\"at least a collective incentive to lower wages.\" ??? In accordance with HN guidelines I am going to resist giving a snarky answer and will instead clearly articulate: Companies clearly have a strong collective incentive to lower wages. \"While companies could use the data to ensure their offers are always below average (pushing wages down), they could also use the data to ensure their offers are above average, pushing wages up.\" They may do both. Collude to keep average wages low, so that when they offer outliers \"above-average\" it is still cheaper to cross that many standard deviations. \"Think about it: prices are transparent in most markets and that doesn't seem to generally lead to anti-competitive prices. In fact, it seems to encourage companies to compete.\" The problem imo is less about transparency and more about information asymmetry. It's asymmetrical that companies have access to literally what every other company is offering, while the employee is sitting there trying to guess based on glassdoor (which is utter shit information) and levels.fyi (marginally less shit?). reply evtothedev 2 hours agoprevFWIW, I believe that Pave works to raise engineering salaries. Every company wants to say, \"We pay above the 50% mark\", thereby steadily raising it over time. reply mydogcanpurr 1 hour agoparentFWIW, I believe RealPage works to lower rent. Every landlord wants to say, “We charge below the 50% mark”, thereby steadily lowering it over time. reply mcculley 1 hour agoprevI found that salary data for some of my employees was being leaked and very accurately reported by a now defunct company called Paysa. We determined that the employees who had their data leaked had recently applied for mortgages and auto loans. Your bank could be selling your data. reply hiyer 4 hours agoprev> helps startups with compensation It's not only startups - I know decades-old, listed firms that use it too. reply numlocked 8 hours agoprevI believe wage-fixing would require companies to agree to…fix wages. Having knowledge of average compensation is not inherently problematic. Firms are still able to decide to pay more or less than the prevailing wage. reply october8140 8 hours agoparentRealPage also gave recommendations. Giving a salary range seems like the same thing. reply cscurmudgeon 4 hours agorootparentBy this logic, employees who share their salary publicly also contribute to wage fixing. reply Nevermark 3 hours agorootparentIf it’s public there is no information asymmetry. There is no fixing. reply cscurmudgeon 2 hours agorootparentHow? Information symmetry doesn't prevent fixing. E.g., rents are all public information. If it is public, won't employers still have access to the salary ranges for free? The very thing Pave is giving them at a cost? reply InsideOutSanta 7 hours agoparentprev\"I believe wage-fixing would require companies to agree to…fix wages\" They don't need to officially twirl their mustaches and laugh evilly while telling each other how they're definitely fixing wages. They just need to share data on wages with other companies in the same or a similar business with the intent of decreasing wages. That is already illegal, because they're colluding with competitors to keep wages low. reply cryptonym 4 hours agoparentprevThe whole point of getting such data is to ... fix wages. reply beeboobaa3 7 hours agoparentprevDo you actually believe they will be doing this? reply dboreham 3 hours agoprevThis already existed for decades. It's called \"Radford\". reply october8140 8 hours agoprevIt’s new so the government hasn’t caught on yet. They probably also need a larger market share to be considered fixing. reply kasey_junk 7 hours agoparentADP has a product called compensation benchmarking which is very similar to how this is described. They’ve had that product (with various names) for years. reply jcomis 4 hours agorootparentExperian also sells a similar product reply tensor 3 hours agoparentprevNo, this is not new. There are many many many companies selling compensation metrics. This is also not salary fixing. These companies typically do not offer recommendations. It's up to each individual company to decide how to structure their salary bands and how they want to stack up to the market. reply Moto7451 3 hours agorootparentI actually run a product in this space in Europe as part of my portfolio. To echo your recommendation point; How we do it is pretty much the industry works. We give the low, mid, and high points in our data set based on what variables you input. We get the data from salary surveys and government data sets. It’s all very boring and above board. If companies choose to talk to each other to suppress salaries, they’re not using our tooling to do it. There are also firms that will do all this work for you especially if you lack enough people in your own offices for an internal benchmark. If you’re building a CAD tool you can tell them (paraphrasing) to pretend you’re just like Autodesk and ask how much you should pay a UI designer. reply eximius 1 hour agoprevHm, sounds like the recent rent fixing collusion. Which was ruled illegal. reply napolux 4 hours agoprevI had the same concern where I work. They told me data are aggregated anonymously, so no risks, in any case it's useful to compare yourself with others when your salary is below average, so you can ask for a raise (which I will do next salary-review cycle) :) reply altdataseller 3 hours agoparent\"data are aggregated anonymously\".. that's what almost every data broker with a security breach also said at one point in the past. reply eightysixfour 3 hours agoparentprevRealpage does the same thing with rental market data but they are clearly at risk. reply bsilvereagle 3 hours agoprevYou may also be interested in the Work Number product: https://news.ycombinator.com/item?id=29834753 reply nowyoudont 4 hours agoprevI guess my issue with all the “it’s just info” arguments is this. Employers inherently have an information advantage in salary negotiations. A tool like Pave drastically increases that imbalance. How am I ever going to realistically negotiate salary vs a company that has this level of information (even during performance reviews)? And frankly something that worries me is, what level of data are they getting? If it’s tied to your HR system, does it get anonymized performance reviews? If every company can perfectly profile me and place me in an expected salary, I as the employee give up all my power. That’s strictly bad for me reply tensor 3 hours agoparentYour salary negotiation point speaks more to a call for open salary data, which many people have been arguing for. You're missing a lot with your second point though. If a company has excellent salary data and can put in you a band, then it also means that you have better grounds to argue for raises when you gain experience, or argue if you are underpaid, or even find jobs at companies who intentionally pay a higher percentile to market as a way to attract better talent. In contrast, if we all operate 100% blind with no data, as many here seem to want, it would lead to all sorts of unfair wage situations with people doing equivalent jobs earning vastly different amounts. This sort of environment is biased towards more aggressive people who have strong social skills when it comes to negotiation. In fact, you see exactly this when companies choose not to buy data like this to set their bands. reply nowyoudont 22 minutes agorootparentI super agree that fully open salary data would be amazing. On the second point, I would argue that you have very little ability to determine when you’ve gained enough experience as an employee to argue for a raise. Whereas an employer with access to Pave has a _ton_ of ability to determine whether you have or have not. Yours is based entirely on personal experience and feel, plus maybe talking to a few coworkers. Theirs is based on aggregated data from thousands of employees reply braden-lk 7 hours agoprev“I’m not punching you in the face, I’m just putting my fist on a movement vector that happens to intersect your face.” I hope these guys get their shit rocked in court. I’m tired of a world run by cartels. reply jmkni 4 hours agoparent\"Alright, pie, I'm just gonna do this...and if you get eaten, it's your own fault!\" reply zelphirkalt 7 hours agoprevWell, I wouldn't mind getting my wage fixed, if that works both ways, down _and up_, because then I would be guaranteed to never earn less than average for my skill and experience. Assuming, that those things of course factor into the averaging. Person X with experience Y in position Z. However, something tells me, that there is a tendency towards the downwards direction and none towards wage increase. (Background: In Germany not so many companies pay competitive wages for their software engineers, especially not, once you worked for some years and are no longer a bloody junior. So I calculate it would result in a wage increase for me, since everyone says I am underpaid for my experience.) reply felurx 7 hours agoparentGenuine question: What makes you believe that an employer would decide to pay you more when they notice that they're paying you less than other companies would? Why wouldn't they just think \"Oh, neat, we got such a bargain!\" reply zelphirkalt 5 hours agorootparentThat's the reason I wrote, that I wouldn't mind, if (and only if) the upwards direction also happens and why I wrote, that \"something tells me\" that that wouldn't be the case ; ) reply gosub100 4 hours agorootparentprevIf they cannot fill positions or are shedding talent. reply smabie 51 minutes agoparentprevIf you're truly underpaid for your experience and location then you should be able to get a new higher paying job easily. And if you can't easily get one well maybe you aren't underpaid. reply AlchemistCamp 2 hours agoparentprevWhat stops senior devs in Germany from remote contracting for foreign companies that pay better? reply nla 3 hours agoprevThis is exactly what Pixar/Disney and Ed Catmull got sued for. Apple, Google and FB as well. reply SoftTalker 3 hours agoparentDidn't they agree not to hire each other's employees? Which would serve to suppress salaries, even without sharing what anyone was getting paid. reply corry 4 hours agoprev1) Salary surveys for local or national startup scenes have been a staple for decades. Here in Waterloo (Canada), there was a dominant local survey that all tech companies participated in annually with results being shared. Then, as you get bigger, you come across larger versions of the same thing. 2) VCs are often the vector by which this all happens. They ask their portfolio companies to pull together the info for their employees, presumably submit it into the companies aggregating everything, and then the startup gets a copy of the recent data. 3) Even done the old way (Excel), the data was incredibly detailed. You can slice and dice by startup stage (series A vs series B vs seed), employee count, region, sector, etc to determine if you're paying market rates or not. This is particularly useful for growing startups, where the founders have no idea what to pay, say, a VP Marketing at their pre-revenue mobile gaming startup in Helsinki. 4) Obviously whether or not this is bad for employees themselves is debatable, but I think people are missing the point that these surveys are ALWAYS skewed UPWARDS due to the much higher volume of data from the large tech companies (because they have far more employees and tend to be offering significantly higher comp). So in practice, the impact is likely to RAISE wages at earlier stage startups who are competing for the same talent as later stage tech. reply corry 4 hours agoparentAn interesting artifact of the information disparity between the startup executives (who have access to this benchmark data) and the employees (who don't) is that the employee is often wildly off the mark in their expectations, either too low or too high. There are so many variables at play too that it turns into a negotiation like everything else. Say an employee wants a big raise because they are having their first child are heading towards a higher cost base at home. Say the employer simply says \"the salary data shows that your current pay is at the market average\". Well, is the employee truly \"average\"? Perhaps they're a high performer. Does the average number take into account not only the company dynamics (stage, domain, funding, revenue level, etc)? Not perfectly. But then on the other side, just because an employee wants a higher salary due to a higher cost structure at home doesn't mean they are automatically entitled to it, right? Then on the startup side again, the manager is looking at the data and thinking about all the time and cost of replacing this person and realizing it's likely more than the cost of just granting the raise. Then the HR person comes in and says the salary grid -- whose whole purpose is to provide in theory tight constraints on these conversations -- rules this all out, there's no budget or wiggle room. When the Manager suggests the grid hasn't been updated in a few years, HR takes it personally and tells the person to take it up with the CEO. So then the CEO gets involved. She knows the employee has a unique view on the technology and market direction and considers them Tier 1 can't-lose-them. She knows the grid is out of date. She looks at the data and thinks that she can justify to herself and the financial plan that it'll be OK to do it, with fingers crossed that this doesn't happen across the board because then their runway will shorten considerably. So the raise happens. My point is just that the salary information asymmetry is just one relatively minor aspect to this whole negotiation and in the end I'm not sure it advantages the company all that much. reply darby_nine 3 hours agoprevThere's a lot of technically legal anti-competitive behavior. We need our legislators to get off their asses and legislate. reply estebandalelr 7 hours agoprevMy guess is that they would argue that most of the data they use is public, just go on LinkedIn and look at 10 job listings to have a range. reply the_mitsuhiko 7 hours agoparentPave's data is _incredibly_ revealing. First of all it covers historical data for every single employee, secondly it includes stock as well. It also relates the compensation to performance. reply gwervc 7 hours agoparentprevJob listings is very different kind of data than compensation actually paid to employees. reply RayVR 4 hours agoprevThe fundamental difference between something like Pave or Radford is that, AFAIK, they are simply providing companies with information. I am not a lawyer, but I believe the fundamental issue with RealPage is that by entering into a service agreement with them, you agree not to violate their price “recommendation” and so you are centralizing actual pricing power into a single entity. I believe RealPage has some way to negotiate out of this standard deal but it’s not common. Companies likely insist on staying within certain pay bands for a whole host of legal and HR reasons but they aren’t getting a specific salary from Pave that they are contractually obligated to use in their offer. reply p0w3n3d 4 hours agoparentI can recite at least five situations where \"simply providing X with information\" is considered a crime. One of them is company A providing planned price to company B in order to agree on it, and it is called price fixing. Other are insider trading, pump and dump, etc. reply swampthing 3 hours agoprevI have to say I am pretty surprised to see the negative sentiments toward Pave and salary benchmarking data in general here. Why is the assumption that salaries would always be lower given this data? It seems just as likely to inform companies that what they had in mind is below market or that they are under-compensating someone in light of market changes. reply vasco 3 hours agoparentA company doesn't need to know they are under compensating for a role from a third party. They'll find out by the quality and number of applicants pretty fast. I've seen this in practice directly when we couldn't hire for certain roles, raised the range, filled the spots. On the other hand, to know that other companies are paying lower and still able to deliver roughly the same work is harder unless you know how much they are paying. reply swampthing 2 hours agorootparentI have to disagree. If you're not getting quality applicants, how do you know if that's because of your salary range, the default applicant pool, or something idiosyncratic to your company? If you're a new startup founder, you don't always have a good sense of what the default applicant pool should look like. You might have a sense of what quality looks like but how would you know without recruiting experience what the mix of quality to non-quality applicants is supposed to be? There are many reasons why you might not be getting the number of quality applicants you want, and compensation is just one of them. Salary benchmarking data helps eliminate that as a possible cause. reply vasco 6 minutes agorootparentBecause when we raised the range it fixed the problem. reply dan_quixote 2 hours agoparentprevI think it's a pretty safe assumption that a company paying to gain information that gives them an advantage in negotiation isn't going to freely give up that advantage. reply swampthing 1 hour agorootparentYou're assuming that the only reason to pay for Pave is to get a negotiation advantage. My point is that there are other reasons, for example, to make sure that you're not below market. reply MathMonkeyMan 3 hours agoparentprevI've never run a company and probably have a chip on my shoulder, but I also think that it's a reasonable assumption that most employers want to pay as little as possible. reply swampthing 2 hours agorootparentI guess my point is that without some sort of sense of the market, whether through Pave or something else, the motivation to pay as little as possible may lead some employers to have lower salary ranges than they would otherwise. reply stray 3 hours agoparentprevObviously, this sort of information would always be used for good. reply swampthing 2 hours agorootparentBecause information has to be used for good or bad and never both? reply rocqua 7 hours agoprevIf this were illegal, so should the KornFerry Hay system be, and that system has existed for decades. reply jiripospisil 7 hours agoprevHuh, looks like they've pivoted (or is that a different Pave?). > Pave: We turn your Google Analytics data in actionable insights + reports with our data science AI algorithm. https://www.ycombinator.com/companies/pave reply giladvdn 7 hours agoparenthttps://www.ycombinator.com/companies/pave-2 reply grues-dinner 7 hours agorootparentThat proud phrase \"HR Tech\" in that link, gives me the heebie-jeebies. I'd say \"if you work in a company like this you're a bad person\", but sociopaths, sorry, Wharton graduates won't give a shit anyway. reply carlmr 7 hours agorootparentIt seems like the term Wharton graduates is now a wart on graduates' resumes. reply FooBarBizBazz 7 hours agorootparentprevThere's an unfortunate lack of fear. reply that_guy_iain 7 hours agoprevBecause it's not wage fixing? To be competitive you need to be paying more than others not the same as them. This is why FAANG got themselves into a fix of paying 500k for people they would have paid 200k for a year or so beforehand. reply flerchin 3 hours agoprevIt shouldn't be, and neither should The Work Number. reply paxys 7 hours agoprevCollecting and sharing data is in itself not illegal. RealPage was also using the data to algorithmically generate a rent number and encouraging landlords to automatically use that number with no room for negotiation. It literally branded itself as a service that would prevent landlords from bidding against each other. That part is what pushed it into collusion. And regardless of whether it is legal or not, the problem has to go beyond a handful of small startups for the DoJ to get involved. RealPage is used in 80%+ of multifamily rental buildings in the US. What is Pave's market share? How many employees are affected by their practices? reply master_crab 7 hours agoparentNo. DoJ is suing RealPage because they used non-public, sensitive data to set rental prices in a way that reduces competition. Literally what these guys are doing. Complaint: https://www.justice.gov/opa/media/1364976/dl?inline=&utm_med... more readable Press release for DoJ on RealPage: https://www.justice.gov/usao-mdnc/pr/justice-department-sues... reply failuser 4 hours agoprevUber and AirBnb are essentially illegal taxi and illegal hotel services. Remember taxi medallions? Remember zoning laws? Being illegal is not a showstopper for a startup because they are under a radar, being illegal is not a problem for a large business because they have enough power to not get prosecuted. reply teraflop 3 hours agoparentAnybody else remember that time YC funded an international smuggling operation? https://www.ycombinator.com/companies/backpack https://news.ycombinator.com/item?id=8199286 reply darby_nine 3 hours agorootparentHell you could write off all of web3 with that sentence reply timdiggerm 1 hour agorootparentYou say that like writing them off would be a bad thing reply naikrovek 3 hours agorootparentprevbecause web3 was a smuggling/laundering operation and those are illegal. basically, all cryptocurrency activity is immediately a suspicious activity. reply bitcoin_anon 1 hour agorootparentReminder that until recently, most cannabis patients relied on smugglers to treat their illnesses. reply tourmalinetaco 59 minutes agorootparentReminder that a minority of cases does not justify smuggling for recreational use. reply 8organicbits 2 hours agorootparentprevTheir website is \"backpack bang\"? What a strange name, the last thing I want is for my backpack to \"bang\" when moving unknown goods across international borders! reply calgoo 3 hours agorootparentprevLooks like they are still active! reply oldgregg 2 hours agorootparentprevAnyone else remember when HackerNews had an adventurous libertarian ethos before the school marms infested the place with irrelevant and low vibrational commentary? reply consteval 1 hour agorootparent> adventurous libertarian ethos Libertarian is when smuggle drugs and oppress the working class through secret surveillance When people say that libertarians are just right-wingers kidding themselves, I think they mean this kind of stuff. I don't think it's in a \"libertarian ethos\" to do wildly unethical and immoral things on a large scale, with the intention of exploiting people for profit. If that actually is libertarian ethos, then it sucks. reply Geee 5 minutes agorootparentI think the critique was directed towards the attitude of being overtly scared of doing something illegal or breaking rules (which does not equal being unethical!). Backbag is simply a way to transport stuff in a backbag, which isn't illegal. reply gperkins978 1 hour agorootparentprevNo, it is simply an idea to live and let live. Sane people escape California or New York where one must be insanely wealthy to live a decent life. In most of the US, you can do your thing and no one will bother you. Most hellishly expensive places would become affordable and fun if they implemented Texas-style zoning. Now, obnoxious white people in Silicon Valley would be upset that multifamily housing had allowed displeasing minorities in, but man would that make life better for everyone. I lived in East Asia, and it is really nice when cities are not too expensive for regular people to live in. Furthermore, I have no sympathy for rich @holes who complain about losing their expensive view. Freedom helps all, but especially the poor. The leftists have tricked people in California and NYC into thinking the system fails the poor, when in reality it is their stupid regulations that made these places expensive. reply hughesjj 25 minutes agorootparentYeah, this is why all the homeless people in Cali and New York move to Texas and Mississippi. It's all those darn regulations hurting poor people. Not the rich people who are always whining about regulation and taxes. No, the poor people. Btw anyone else seen that bear rummaging around? Any idea how to get rid of it? reply romwell 1 hour agorootparentprev>Anyone else remember when HackerNews had an adventurous libertarian ethos before the school marms infested the place with irrelevant and low vibrational commentary? Boy will you not like the \"high vibrational\" commentary people would have for that adventurous™ libertarianism©* of yours. The \"medium vibrational\" ones merely wish its pursuers behind bars, the more energetic ones are discussing optimal guillotine blade shape profiles. * * * * * * * Question to you. Would a startup that maintains a public database of names, addresses, and approximate locations of people with net worth over $1B be libertariously adventurous enough by your standards? You know, like Page, but crowdsourced, and with wage workers as the users (not as product). Purely opt-in. Give it a higher-energy vibe name, like, say, 'rage (as in \"average\" - for the average people). Anyone who sees Elon Musk could anonymously report his location to 'rage, giving wage workers an option to avoid providing services to him - just like Pave gives employers an option to avoid getting services from undesirable workers. Are you a pastry seller who'd rather call in sick the day Peter Thiel or his buddy JD Vance are in town again? Get 'rage, and avoid the awkward interaction. Anyone who gets a wind of someone fitting the wealth profile will have an option to anonymously contribute this data to 'rage's Wealth Accumulator Registry (Rage WAR™). They may be breaking their NDA's while doing so, but that won't be 'rage's problem, of course. 'rage will not be in the business of policing individual actions and limiting users' personal freedoms. The user identity will be e2e encrypted, guaranteeing anonymity. It will be impossible to prove that someone has a 'rage account against their will, or find out they have one. The app, however, will also allow users to confirm that they have a 'rage account if they choose to do so. This way, wage workers who are concerned about their peers could ask them to privately confirm their account status and contribution karma to avoid sharing a workplace with a scab. Registration will require entering your own personal wealth data into 'rage WAR™. While tax returns can be faked, someone uploading a copy of their W-2 paystub will practically ensure that one does not fit the target wealth profile for the B-status. Those could be faked too, of course - and one could see large employers not wanting to collaborate with 'rage for whatever reasons. That's exactly where startups like Pave come into play to verify the correctness of the data. 'rage and Pave would not only complement each other in the financial data ecosystem, they would form a natural symbiosis, giving wage workers incentives to ask their employers to use Pave. As for Pave, 'rage would merely be one of its clients, consuming W-2 data just like everyone else. I hope you will find this proposal sufficiently adventurous and relevant; and I would love to hear your thoughts on this matter. reply Havoc 4 hours agoparentprevEric Schmidt echoed similar sentiment in his recent interview. Basically do it, if startup fails then it doesn’t matter. If it succeeds then lawyers can sort it out. reply arrosenberg 3 hours agorootparentHe would know. https://www.npr.org/sections/thetwo-way/2014/04/24/306592297... reply SoftTalker 3 hours agorootparentprevAlso in these \"gig\" type companies, the people who are actually breaking the laws are the workers, e.g. the drivers or the homeowners in the case of Uber and AirBnB. The startup is the enabler, yes, but they will try to throw their workers under the bus before they take responsibility themselves. They don't own the cars, they don't own the properties, and they are most likely in a far-away jurisdiction. reply lisnake 2 hours agorootparentprevDidn't work for Elizabeth Holmes reply rodiger 2 hours agorootparentThere's a difference between \"doing an illegal thing as a product\" and \"lying to investors about your product\" reply papercrane 2 hours agorootparentThis is true, although it's also true that many startups lie to, or mislead, investors about the state of their products. If things work out, then the investors don't care, and if they don't its usually at scale and messy enough the government isn't going to prosecute. reply santiagobasulto 3 hours agorootparentprevWhich makes total sense for consumers as well. If the startup succeeds is because consumers are finding value in it. Uber is the best example. Uber is ilegal only in countries with deep corruption where taxi unions can make legislators ignore their constituents. Uber (and any other car sharing app) is the best solution for me as a consumer compared to the traditional old school taxi service. reply consteval 1 hour agorootparentUber is objectively worse for every single party involved. Driver makes less, customer pays more, Uber has to coordinate a huge system. Uber \"won\" because they cheated. They operated at a loss for almost 15 years, on the welfare of investors. Guess what, mom and pop running a taxi can't live on a negative wage. reply cellis 1 hour agorootparent> Uber is objectively worse for every single party involved. Wrong on at least one count. I've never been refused service while black from Uber. The taxi industry was brought out of the dark ages of discrimination by Uber et al. Taxis (around the world) have tried to rip me off almost half the time I've used them, with no accountability. reply Havoc 3 hours agorootparentprev> Which makes total sense for consumers as well. Kinda. Often this casual law breaking isn’t entirely victimless even if it benefits both consumer and the startup. I think Schmidt was talking about using content to train models. So artists getting short end of stick. Or Airbnb causing locals getting prices out or whatever. There is certainly some dodgy protectionism happening of the sort you describe but there are also externalities borne by society for this break laws startup style. reply warkdarrior 2 hours agorootparentAs a user of GenAI, I get to create and save drawings in the style of any artist I like, without having to pay the artist $$$$. This is important to me because I like certain styles, but do not care for an original drawing nor have the money to pay for such. And the externalities introduced here are not borne by all of society, but only by a small number of people (How many important artists are there? 10,000? 100,000?). Just like horse-and-buggy drivers were affected by automobiles, while the vast majority of people benefited from automobiles. reply talldatethrow 2 hours agorootparentprevTotally. I propose we start a brothelBnB next door to your home. Home owner wins, customer wins, worker wins, startup wins! Score! Market has spoken! I also recommend HoboSleepinCar Driveway as a service next to your home. The consumer has spoken! reply yonran 3 hours agoparentprevIronically for a question about antitrust price fixing you just named two incumbent government-sanctioned cartels (zoning and taxi medallions) that restrict supply and keep prices high. They would be illegal if private companies made them. reply jmward01 3 hours agorootparent> They would be illegal if private companies made them. A lot of things governments do would be illegal if private companies did them. Are you arguing that governments shouldn't have special abilities that companies can't have? Should every road be owned by a company? Should the police report to Amazon instead of the local municipality where you may actually have a say in how they are run? We give governments additional powers because they, at least nominally, answer to citizens and society. Companies have no such responsibility. reply yonran 2 hours agorootparentI’m saying that government regulations that fix prices should be scrutinized and repealed if they reduce opportunity for ordinary people. Such as zoning codes that price out the poor. reply keerthiko 1 hour agorootparentI believe the argument here is that the way to do that isn't by establishing a private business that flaunts and undermines those government regulations, but by changing the policies through government process. Obviously that's easier said than done, and SV has a track record of \"ask forgiveness not permission\" as a successful tactic for effecting policy change. But many times it results in indefinite undermining of government which leads to selective enforcement and cartels, which is worthy of criticism (of both government and VC-powered undermining of government). reply ascagnel_ 1 hour agorootparent> I believe the argument here is that the way to do that isn't by establishing a private business that flaunts and undermines those government regulations, but by changing the policies through government process. And to make those who interfaced with the prior system in good faith whole again; eg: drivers who bought taxi medallions for six figures USD, only to have the value of the medallion plummet with the arrive of \"rideshare\" services. reply yonran 46 minutes agorootparentTo make beneficiaries whole is perhaps the worst reason to keep a monopolistic system. In the case of taxi medallions in San Francisco, they are technically still owned by the city and the medallion should never have had any private value to begin with; Mayor Gavin Newsom should have leased them to the drivers instead of creating a $250,000 transfer program to give windfalls to retirees. In the case of zoning, ideally we would tax much of the land rent to reduce the incentive to exclude and increase the incentive to create capital. Rents from a government-created monopoly should not be anyone’s ticket to retirement. reply digging 3 hours agorootparentprev> They would be illegal if private companies made them. Yes, that's kind of the main difference between government functions and private companies. Are you saying the very idea of zoning strikes you as a problem? Or are you trying to call out the bad implementations which strangle urban prosperity in the US? reply yonran 3 hours agorootparent> Yes, that's kind of the main difference between government functions and private companies Perhaps that should change. Or at least it’s a reason to scrutinize and repeal laws that are used for price fixing. > Are you saying the very idea of zoning strikes you as a problem? Or are you trying to call out the bad implementations which strangle urban prosperity in the US? Zoning Rules! by William Fischel gives good a history of zoning. Zoning was originally for segregation within the city but to the question of prices, no it was not inherently problematic. It was not until the 1970s that zoning was used for growth control to make entire cities unaffordable. reply BeFlatXIII 3 hours agorootparentprevUnless it's pollution-based zoning, I agree that the idea of zoning is a problem. reply swatcoder 3 hours agorootparentprevThat's not ironic. Governments and private companies are not the same kind of entities. They have different roles, different roots of legitimacy, different forms of accountability, different operational objectives, and carry different expectations. reply yonran 2 hours agorootparentIt’s ironic that in response to a question about price fixing, failuser brought up other companies that were formed to circumvent government price fixing, and in his examples the governments doing the price fixing were supposedly the good guys! In the case of Uber, they successfully broke up the taxi cartel since the state PUC ruled that ride hail is a separate category. In the case of Airbnb, according to their founding story they were created to help economize on space because rents were high in San Francisco due to zoning. Although they made a useful service, they did not succeed in reducing rents because the underlying zoning is still the constraint that keeps rents high. reply ein0p 1 hour agorootparentprevZoning, ok, but yellow cabs are now often cheaper than Uber. Last time I took a ride from the airport the difference was not small, like 50%. reply asdasdsddd 1 hour agorootparentcompete or die reply ein0p 25 minutes agorootparentThe problem with that is thanks to many years of below cost pricing Uber has become synonymous with taxi now, and most people don’t even realize (or care) that taxis are often cheaper. reply fwip 3 hours agorootparentprevMany things a government does would be illegal if private companies did them. For example, prison, the draft, and taxes. The government is allowed to do it because we (as a society) believe it's better for the government to do these things than private individuals or companies. reply yonran 3 hours agorootparentCan you give examples of the topic at hand, price fixing, that are justified? There are a handful of progressive forms of price fixing (e.g. minimum wage laws), but many others should be added to the Niskanen Center’s list of bad regulations in the Captured Economy. reply __loam 2 hours agorootparentUtilities that trend towards natural monopolies due to high barriers to entry like water and electricity infrastructure are often run by the government or heavily regulated because pricing would be extortionate if the market were allowed to set prices. reply yonran 1 hour agorootparentFair enough, utility regulations fix prices except in the opposite direction. Without zoning, landowners could not act as a cartel since that would violate antitrust laws, whereas without utility regulation, a natural monopoly could set prices as high as the market will bear. reply keerthiko 1 hour agorootparentprevYep, basic human rights are priceless, and by capitalist mechanics, their pricing will always converge at \"how much can we get away with in the current economy?\" Government oversight is the only way we currently have to manage this somewhat. As an example in support of this, healthcare is barely price-regulated and hardly run by the government in America, and is thus extortionate. reply yonran 1 hour agorootparent> As an example in support of this, healthcare is barely price-regulated and hardly run by the government in America, and is thus extortionate. They are supply-regulated by governments. According to Niskanen Center, the high cost of health care is due to the American Medical Association limiting new accredited medical schools and certificate-of-need laws limiting new hospitals. https://www.niskanencenter.org/faster_fairer/liberating_the_... reply rrrix1 3 hours agorootparentprevYou have heard of the Prison Indistrial Complex right? Our Prisons have been For-Profit for a long time now. Totally legal, government sanctioned privatized penitentiaries. reply insane_dreamer 1 hour agoparentprev> essentially illegal taxi and illegal hotel services I don't know about Uber, but Airbnb deflects this by saying they are not in fact a hotel service. Rather hosts are hotel services, and Airbnb is simply a discovery platform matching buyers and sellers. It's up to an individual host to make sure they are complying with local laws including whether their city or district allows an individual to rent out their house or a room in it without a hotel license (this varies from city to city). In this way Airbnb (fairly or unfairly) pushes the burden and liability onto the hosts. I believe this is also a huge reason why Uber doesn't want to classify drivers as employees because then it is the taxi service, whereas it could argue that the drivers are each operating their own taxi service and Uber is just a discovery and payment platform. reply Manuel_D 1 hour agoparentprevTaxi medallions were (and AFAIK still are) required to respond to people hailing a taxi from the street. It is not required to book a ride via phone or internet. Uber and Lyft drivers never needed taxi medallions. reply whimsicalism 2 hours agoparentprevuber: mostly not technically because the key thing is that they are not soliciting rides from the street, was my understanding. airbnb had a much more legally tenuous start reply shortrounddev2 2 hours agoprevSharing data about wages, I think, is not wage-fixing. Agreeing not to go higher than a particular wage is. What got RealPage into trouble was the fact that, in order to use the system, you HAD to use their algorithm for selecting rental prices. You entered into a legal agreement not to compete on the prices that RealPage provided. reply ipv6ipv4 3 hours agoprevLegal or not, Pave is unethical. reply blackeyeblitzar 1 hour agoprevYep all of these compensation intermediary are basically supporting illegal collusion reply Eumenes 4 hours agoprevCarta has a similar product, and Radford is a popular one as well. These things have been around for decades. Nothing new. reply louthy 5 hours agoprevIn the UK and EU it would be outright illegal on GDPR grounds (unless you consented to it, which would be unlikely without coercion — also illegal) reply Tknl 1 hour agoparentIn the EU there are definitely companies providing aggregated salary band norms, in fact utilizing them is nearly required by upcoming EU directives as salaries must be justified by HR. In the Netherlands I know Bureau Baarda is gathering and selling data. reply KoftaBob 2 hours agoprevFrom a US perspective, if Pave existed in a market where salaries weren't transparent, I would say yes, it's veering into being an anti-competitive/wage-fixing tool. However, more and more states every year are introducing laws that require salary ranges on job listings. What that means is that Pave is basically just organizing the data that's already becoming public for job applicants and employers alike. reply znpy 2 hours agoprevThe \"funny\" thing really is the fact that most company forbid you from discussing your wages with your coworkers, but this seems like an automated way for companies to discuss the salaries they pay with other companies. reply toolslive 1 hour agoparent> most companies forbid you from discussing your wages .... Yes, but this interdiction might not be legal. Companies typically add such clauses to achieve a chilling effect. (Ie, you abide because you fear running a risk if you don't). Consult your legal representative. reply cess11 2 hours agoprevI'm not sure about the legal aspects in your jurisdiction but in many others it is common for unions to aggregate this information and there are usually companies that do this or hook into the books and compare what different companies are paying for services and material they use that then sell back information about whether they could cut costs. reply greenthrow 2 hours agoprevIANAL so i don't know the actual legality, but it certainly seems like it should be. Definitely immoral. reply skywhopper 3 hours agoprevSounds very illegal. reply josefritzishere 6 hours agoprevThis sounds very obviously illegal. Where is the local DA on this? reply hungie 4 hours agoprevIt's almost certainly illegal, but it benefits the capital class, so it's going to get a pass for a long time. That said, it's exactly the same thing that landlords were doing with that pricing software and the government is coming after them, so maybe pave will get hit with a big lawsuit sometime soon. Here's hoping this sort of thing gets regulated down into the earth. reply d883kd8 4 hours agoprevVERY similar to the situation in the rental market where large landlords are being investigated for using software to enable collusion and price fixing. https://www.politico.com/news/2024/07/12/justice-department-... reply exabrial 7 hours agoprevWow first we got our rents fixed, now we're getting our wages fixed. Awesome reply benoau 7 hours agoparentUsing a rent estimate would be a brilliant way to double-check if you are overpaying salaries. /s reply theGnuMe 7 hours agoprevYeah this is borderline. It probably won't fly in California. In the past big tech companies (like Symantec) used to require you to submit your last W2 or tax return for a job offer. Credit card companies also sell your salary information etc... reply lulznews 1 hour agoprevAnything that keeps the code monkeys in line is legal. reply Fokamul 7 hours agoprevLol try this here in EU. reply cryptonym 4 hours agoparentIt exists, many companies are using wage benchmarks to fix salary in EU. Companies assume they don't need your approval to collect data on salary range for your position as aggregates are not directly pointing at you. reply sanj 7 hours agoprevIt smells awfully close to: https://www.justice.gov/opa/pr/justice-department-sues-realp... reply blast 4 hours agoparentThe OP specifically mentions that. reply mhx1138 6 hours agoparentprevSoon you need to waive your class-action rights when applying for a job. reply carterschonwald 7 hours agoparentprevReal pages is such a gross business reply vishnugupta 7 hours agoparentprevThis was the exact thing that popped into my mind when I read the poster’s description. reply zulban 7 hours agoparentprevReading that, seems like RealPage could protect itself from similar problems if they simply avoid using the same sales rhetoric, and don't do explicit recommendations. \"You are paying way more for this position than others... hmmmm.\" Surely they are aware of the similarities and are strategizing. reply tazu 7 hours agoprev [–] > Pave is a YC-backed startup This thread is getting removed from the front-page in 3... 2... reply dang 4 hours agoparentI understand why people assume we do that, but actually we do the opposite—that is, we moderate less when YC or a YC startup is part of a story. There is plenty of past explanation at https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu.... Note: we still moderate such threads. We just do it less than we otherwise would. reply hysan 7 hours agoparentprevI thought this was a joke comment but I went back after reading the comments and it’s now on the second page already. reply tazu 6 hours agorootparentIt was at #3 for me and then suddenly demoted to the second page. It's pretty common for these touchy-YC threads. reply dang 4 hours agorootparentIt's less common than it would be for comparable threads on other topics. See https://news.ycombinator.com/item?id=41511529 for more explanation. reply maeil 6 hours agorootparentprevPage 3 now. reply mitchbob 6 hours agorootparent211 points in an hour, and now page 4. reply PawgerZ 5 hours agorootparent#10 on the front page now reply benterix 7 hours agoparentprev [–] I really, really hope they don't. reply hoseja 7 hours agorootparent [–] Aaaaand it's gone. reply benterix 6 hours agorootparent [–] This makes me sad. @dang, can you comment on that? I appreciate your integrity. reply dang 4 hours agorootparent [–] We didn't see it or demote it—it set off the flamewar detector. I've turned that off now, in keeping with the principle described here: https://news.ycombinator.com/item?id=41511529. p.s. @dang is a no-op - I only saw this thread because I was doing our standard review of the flamewar detector. If you want guaranteed* message delivery, hn@ycombinator.com is the only way. * Well, mostly guaranteed. I assume there are a few that fail to get noticed in the spam bin, though we check that pretty carefully. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pave, a YC-backed startup, assists other startups with compensation by integrating with HR and Payroll systems to aggregate data and provide compensation range breakdowns.",
      "There are concerns about whether this practice could be considered anti-competitive wage fixing, similar to the case with RealPage, raising questions about the legality of colluding on wages.",
      "The legality and ethical implications of Pave's business model are under scrutiny, as colluding on wages is generally illegal."
    ],
    "commentSummary": [
      "Pave, a YC-backed startup, aggregates data from HR and Payroll systems to provide compensation ranges, raising questions about potential anti-competitive practices.",
      "Critics compare Pave's service to RealPage's rent pricing issues, while others argue that sharing compensation data isn't illegal without explicit wage-fixing agreements.",
      "Similar services, such as Equifax's \"The Work Number\" and Radford, have been around for years, but concerns about privacy and wage suppression persist."
    ],
    "points": 731,
    "commentCount": 297,
    "retryCount": 0,
    "time": 1726051855
  },
  {
    "id": 41505009,
    "title": "Another police raid in Germany",
    "originLink": "https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533",
    "originBody": "Artikel_5_e.V 3d Hello to all list-reading entities! On Aug 16th 2024 German police considered it once again appropriate to raid the home&office at the registered address of our organization. The first raid was 2017. There are obviously still people working in German law enforcement today, who think that harassing a node-operator NGO would somehow lead to the de-anonymization of individual tor users. At least that is what they claim in the paperwork. As with the first time, the raid team was fortunately a bit better educated and acted significantly more reasonable than the non-technical people applying for the raid and the judge signing it off. So again, no hardware was seized. Apart from one burnt (middle relay) node and some billing paperwork for the exit node this was all about, the team left the house after one and a half hours with mostly empty hands. We intend to legally challenge the search warrant to make sure that this does not happen again (but this is not what this email here is about). These were once again one and a half hours with armed police in a personal living room, threatening to de-facto kill a livelihood and software business (carrying a truckload hardware away) of an non-profit board member to force cooperation. As a consequence, I am personally no longer willing to provide my personal address&office-space as registered address for our non-profit/NGO as long as we risk more raids by running exit nodes. That is a risk I am just no longer willing to take anymore. Artikel 5 e.V. is now calling for a general assembly on Sep 21st 2024. We are looking for new board members (who take over and organize a new registered address and keep running exits) or discuss ALL alternative options. These options include \"just stop running exits\" or even the most drastic step of liquidating the entire organization and distribution of the remaining budget to other German organizations (that would have to qualify under our non-profit by-laws). Assembly time&location details can be found at https://artikel5ev.de/ 857 We intend to provide a stream primarily for our members but also for interested parties who can not attend in person. The event/stream will however be in German only. Details for the stream will be made available on the website on short notice before the event starts. If you plan to attend in person, an upfront email would be appreciated, so we can pick the right room to set things up. Thank you! Regards, Gero Kühn for Artikel 5 e.V. ··· 1 22.1k views 1 link",
    "commentLink": "https://news.ycombinator.com/item?id=41505009",
    "commentBody": "Another police raid in Germany (torproject.org)485 points by costco 22 hours agohidepastfavorite287 comments edm0nd 22 hours agoPart of the reason I sadly stopped running any exit nodes was law enforcement harassment. I ran a few exits for about about ~5 years. In those 5 years, my hosting provider (DigitalOcean) received 3 subpoenas for my account information. The first two were random. The 1st one was someone sent a bomb threat email to a university. The 2nd one was someone sending a phishing email. The last and final subpoena was the most serious one. Some nation-state hackers from Qatar had ended up using my exit IP to break into some email accounts belonging to people they were interested in and spied upon them and stole some info. Thankfully both the Tor Project and the EFF were able to help me pro-bono. The EFF lawyer that was assigned to me helped me fight this subpoena but ultimately we had to turn over my account information to the DOJ + I had to give an affidavit stating that I was simply just an operator and nothing on the server in question would be useful to their investigation (by design). The stress of having to deal with law enforcement, lawyers, and having to entertain the possibility of having my home raided over something so silly ultimately led to me finally shutting down my exits. Even though I had all of my exits using a reduced exit policy and I would blacklist known malicious IPs and c2/malware infra from being able to use it, I was still a target. I feel law enforcement realizes this is a big weakness they can target since a lot of Tor exit operators are individuals with not a lot of resources to fight them. They can use the legal system to scare operators into shutting down. I one day hope to resume running exits as I find it rewarding to be able to help people from around the world in a small way. reply zepearl 20 hours agoparentIs something like this unexpected? I personally never ever thought so (which is the reason why I never ever even considered running a TOR exit node). As much as I can respect the idealism about privacy and liberty etc..., I could not ignore the fact that any \"really!!!\" bad actor could use the same infrastructure to avoid investigation/prosecution, therefore I did not want to provide indirectly any help. > I feel law enforcement realizes this is a big weakness they can target since a lot of Tor exit operators are individuals with not a lot of resources to fight them. They can use the legal system to scare operators into shutting down. On one hand I admit that that might be the case, on the other hand even government organizations/departments/agencies can be \"local\" and scattered (e.g. similar IT departments for each \"canton\" in Switzerland) and not have huge amounts of resources/knowledge to track/identify perpetrators of all ongoing (sophisticated?) IT crimes => somebody somewhere might see the same IP involved in a lot of \"bad\" stuff not realizing it's just a TOR node. I hate the current general trend pushing a position of an either absolute \"yes/no\" for any theme, including this one (of encryption for privacy/etc vs. crime). In my opinion it's obvious that the current situation of solutions is in general bad: too much pressure on services that provide privacy because it's too easy for crime to misuse them :o( reply Sebb767 20 hours agorootparent> As much as I can respect the idealism about privacy and liberty etc..., I could not ignore the fact that any \"really!!!\" bad actor could use the same infrastructure to avoid investigation/prosecution, therefore I did not want to provide indirectly any help. Well, what would be considered a \"really!!!\" bad actor for some might be a hero for others. Just as an example, depending on which side of the Israel/Palestine conflict you are on, either side using your node for military intelligence might be an use worth fighting for or terrible abuse. In the end, this really comes down to whether you value freedom or state protection more; either of which can be abused by rogue actors or a malicious state, respectively. There is no win-win-solution, unfortunately. reply Peteragain 12 hours agorootparentDuring The Troubles bombs were sent via the Royal Mail. Nobody blamed the post office. Indeed any infrastructure is a tool of terrorism as we rely on it (I am not going to make a list for obvious reasons). I think the reason we tolerate this problem with infrastructure is that the benefits outweigh the risk. The question is whether or not the same applies to free speech - you're right there is no win-win solution, but it still might be worth it. reply blitzar 10 hours agorootparentHowever if you start \"Peters no questions asked hand delivery service, shipping direct from Ireland to London so reliably you can set a timer by it\" - and you deliver 3 bombs to politicians you might find yourself being asked a few questions. reply petertodd 9 hours agorootparentAt the time that's exactly what the Royal Mail was. Requiring identification to send packages is a much more recent development. Society just accepted that bad actors could do this and solved the root problem instead. reply watwut 5 hours agorootparent> Society just accepted that bad actors could do this and solved the root problem instead. You ... do not read much about history, I guess from this. reply _ph_ 4 hours agorootparentprevThere are quite a bit of differences here. The mail services transport physical goods, and the whole path can be tracked. Every letter or parcel is registered by the postal office where it was submitted to for transport. And usually there is quite some physical evidence with everything you do mail. reply lolinder 5 hours agorootparentprev> I think the reason we tolerate this problem with infrastructure is that the benefits outweigh the risk. The thing is, we absolutely don't tolerate this with infrastructure. We have entire systems in place to make sure that we can find people who use our infrastructure to kill people. The USPS has its own entire law enforcement branch whose sole job is to track down people who misuse the mail. I'm sure there are processes in the UK for the same. With our infrastructure there's some non-zero amount of abuse that we acknowledge we won't be able to prevent in order to make everything work without infinite enforcement cost, but we don't just close our eyes to the abuse and not even try to do anything about it at all. The difference between the post office and Tor is that Tor is very specifically designed to make tracking a sender of a bomb threat impossible. State-run postal services at least try to have an audit trail for what they send. reply atemerev 2 hours agorootparentWell, many (if not most) exit nodes are ran by three-letter agencies, so at least there is some infrastructure in place. reply tzs 17 hours agorootparentprev> Well, what would be considered a \"really!!!\" bad actor for some might be a hero for others. Just as an example, depending on which side of the Israel/Palestine conflict you are on, either side using your node for military intelligence might be an use worth fighting for or terrible abuse. Stepping back though neither side in that conflict needs Tor. They both have numerous supporters in other countries where that support is legal. They can send and receive information through trusted outside supporters including some outside governments. They just need secure communication channels to a few representatives among those supporters rather than something is general as Tor. reply lolinder 19 hours agorootparentprev> In the end, this really comes down to whether you value freedom or state protection more If we're talking about the decision to actually run an exit node, I disagree with this breakdown of the ethics. I can value freedom more than state protection in the abstract while at the same time not feeling that helping support freedom in Russia and China and Iran is worth the cost of simultaneously helping to shield perpetrators of violence closer to home. In most people's ethical frameworks choosing not to run a Tor node does not make me culpable for the actions of a state suppressing its people, but choosing to run one does make me at least somewhat complicit in shielding the perp of a bomb threat. reply saikia81 8 hours agorootparenthow is this different from running a postal service? would you be against that? reply lolinder 6 hours agorootparentThe USPS has an embedded law enforcement agency [0] whose full time job is to track down people who are using the postal service to commit crimes. Tor is very specifically designed to make an equivalent impossible. [0] https://en.m.wikipedia.org/wiki/United_States_Postal_Inspect... reply cm2012 19 hours agorootparentprevThere's enough truly bad actors out there, not everything is shades of gray. Cartels, North Korea, ISIS, etc. reply blitzar 10 hours agorootparentAre we the baddies? https://www.youtube.com/watch?v=ToKcmnrE5oY reply hsbauauvhabzb 18 hours agorootparentprev‘Truly bad’ still relies on the perspective of the participant though. Parents point is that ‘bad’ is a matter of perspective, and that right or wrong, at lease some cartel/nk/isis operatives believe their actions are justified for some greater good, Palestine/Israel opinions and belief are obviously a more easy to understand perspective, but the point still stands. reply eptcyka 12 hours agorootparentNK operatives feel incredibly lucky they get to not starve. Unless they got to where they are at due to nepotism. reply snapcaster 6 hours agorootparentYou don't know that, you've never been there or probably spoken to a north korean. Not saying you're wrong (i can admit i have no idea), but i'm annoyed you're swallowing narratives from warlords who have been known to lie to start wars as if it's assumed default true reply hsbauauvhabzb 11 hours agorootparentprevI have no idea about nk politics, but if the media continually pumps out ‘the west is the reason we’re starving, join the military today!’ then they might feel lucky to both be fed, and to be serving their country. reply watwut 8 hours agorootparentprevHitler thought he is a good guy. Stalin thought he is good guy. Everyone thinks he is a good guy from the own perspective. reply lolinder 19 hours agorootparentprevYes. And running a Tor exit node means helping these people in addition to any in the morally gray area that you personally consider evil. If you look at that and still come to the conclusion that the people you're helping are worth the cost of also helping commit atrocities, that's a decision you can make. But an occasional subpoena related to a bomb threat or similar is a good and necessary reminder of what it is that you chose to do. reply _heimdall 16 hours agorootparentThis is the crux of every argument against free speech, no? There is a fundamental trade off we have always had to make between safety and freedom. If you believe that privacy online is a freedom worth having, or if you believe one should be able to say whatever they want, you have to accept the bad with the good. As soon as you start gating access by judging a person by what they're trying to do privately online, or what they're trying to say, you've thrown out that freedom and made it a privledge. There's not even anything wrong with that if that's the world you would prefer to live in. Its important to know that's the tradeoff you're making though, and be prepared to accept the consequences if you one day find yourself running into new leadership that believes what you want to do online, or what you say, isn't worthy of the privilege. reply dt3ft 13 hours agorootparentWasn’t the raid done in a democratic land? There is no gestapo in Germany in 2024, is there? Privacy is what terrorists love too. There needs to be a balance. Even guns need permits and psychological evaluation. reply notarget137 13 hours agorootparentThe goverment has just revoked your speech license. Please upstain from public talking to more than three people. reply newaccount74 4 hours agorootparentI really don't get how bomb threats can be considered \"speech\". Like, there is no benefit to society from allowing people to make bomb threats. reply raxxorraxor 3 hours agorootparentBe more precise in your thinking. This is not about bomb threats, this is about punishing people that provide a line of communication. It is not a new concept that defendants of freedom of speech often have to protect scoundrels too. The argument doesn't change, it always has the same pattern and principle. And yes, it is advisable to err on the side of freedom, there is enough literature here to expand on that point. Additionally the agencies that would demand these information are prone to break the law itself. So this isn't even a discussion about doing something just or not. This is purely a discussion about how much power you want the executive to have. Or in case of Germany, the often misdirected and overworked judicative branch. reply newaccount74 3 hours agorootparentHere's a different take: Criminals and fraudsters will abuse pretty much every technology they can get their hands on. As a consequence, every service operator needs to do their part to prevent fraud and abuse. If you offer a service anonymously and indiscriminately, your service will be overrun by crooks, and you'll end up serving criminals. The fact that your service could be used to defend free speech does not absolve you from your duty of monitoring the use of your service. If you realise your service is used for exchanging illegal content and bomb threats, it's your duty to do something against that, or stop providing the service. reply raverbashing 23 minutes agorootparentprev> that provide a line of communication. Except that line of communication puts your address on whatever it is sent by who-knows-who A \"perfect proxy\" does that by design pretty much. Law enforcement can't know what was on that address until an inquiry Freedoms and laws apart, that's the problem here hcfman 1 hour agorootparentprevThe government are against free speech if you are criticising illegal things they are doing. reply raxxorraxor 8 hours agorootparentprevGermany certainly needs more liberty instead of raiding the home that called an official a penis. reply lolinder 16 hours agorootparentprev> Its important to know that's the tradeoff you're making though Exactly. This is all I'm saying. I don't have enough knowledge of Tor to make an argument that it does more harm than good or vice versa. But I do know that a lot of people on here are just as ignorant as I am but are quick to assume that Tor must be inherently good because it protects privacy. As I said, if you look frankly at the risks and decide that the benefits are still worth it, that's a decision I'm comfortable with you making. But that requires looking very frankly at the risks, which most seem reluctant to do in favor of high-minded abstract discussions of the merits of freedom and privacy. This subthread spawned from someone who helped facilitate a bomb threat through an exit node they were running, and that kind of concrete harm needs to be mentioned in any discussion of the merits of Tor. reply marcus_holmes 12 hours agorootparentAnd someone else pointed out that the IRA used to send bombs through the mail. Yet we are not debating shutting down the Royal Mail because of that (and rightly so). There are governments out there who kill people who criticise them, usually journalists. We need those people to continue their work. We do not want a world in which all communication is government-approved. reply lolinder 6 hours agorootparent> the IRA used to send bombs through the mail. Yet we are not debating shutting down the Royal Mail because of that (and rightly so). As I said elsewhere, at least in the US there's an entire law enforcement agency whose sole job is tracking down people who use the postal service to commit crimes and hurt other people. I'm sure there's an equivalent process in the UK. Tor is specifically designed to make that impossible. There's really no comparison. > There are governments out there who kill people who criticise them, usually journalists. We need those people to continue their work. We do not want a world in which all communication is government-approved. I agree, and it may well be that on the balance we come to the conclusion that Tor is worth it. All I'm asking is that we stop looking at the harms as an abstraction and the benefits as concrete. OP facilitated a bomb threat but seems to have thought primarily about how unfair it was that law enforcement subpoenaed them rather than the complexity of the moral choice they made and its consequences. reply _rm 11 hours agorootparentprevThis trade off concept is a popular belief but completely fictitious and dishonest. The state is not fundamentally better than the people as a whole. They just have more focused resources. More resources to brainwash their subjects about how their power is always such a great and wonderful thing and is only ever used for good, and definitely better than people exercising power themselves. Oh and also much more resources to gas people to death in camps, starve them to death, blow them to bits (but always for completely good and justified reasons of course). Complete crock of shit, it is. reply ethbr1 18 hours agorootparentprevI'm as much of a supporter of encryption as anyone, but I also accept that true effective encryption enables some pretty horrible things. One of those \"better look your meat in the eyes, before you murder and eat it\" idealism-meets-realism moments. On the whole, though, I think even with perfect encryption the remaining physical traces of illegality are sufficient for law enforcement purposes (granted: if more difficult). reply lolinder 17 hours agorootparentI don't think the analogies to encryption are fair because a Tor exit node is far more active in shielding criminals than the inventor of a new cryptography scheme is. The inventor merely puts out an idea that can be used for good or bad. The exit node operator is actively paying on an ongoing basis to shuttle CSAM and bomb threats. The exit node operator is also shuttling other content, so it's not wholly evil and on the balance someone might decide it's still worth it, but it's still a much less obvious ethical call than simply designing a piece of tech. reply BlarfMcFlarf 10 hours agorootparentSomeone has to pay for distribution, maintenance, and integrations of the encryption on an ongoing basis. If it was legal to write encryption but illegal to distribute it, what would be the difference from a ban? Both tor and cryptography require an ongoing effort to provide their service. reply lolinder 5 hours agorootparentI see a pretty strong difference between hosting the latest build of gpg and actually running a server that moves the bytes that cause the harm. You may not, but I do. reply roenxi 17 hours agorootparentprevYou're naming things that are in the grey zone though. For example I can find polls [0] suggesting that North Korea is one of the least popular countries, but not strikingly different in absolute terms than someone like Russia or the USA. Internationally speaking they aren't unusually bad actors. The problem with a \"no shades of grey\" stance is that in any large organised group there are going to be some good points and reasonable ideologies for why they have banded together to do what they do. They may be mistaken on important points, and it certainly may be necessary to put all empathy aside and try to ruthlessly crush them regardless of any good points they have - but in practice that approach almost always leads to terrible results compared to negotiating to emphasise the good and suppress the bad. Take ISIS - the reason we have groups like ISIS running around is generally because of a no-shades-of-grey approach taken to deal with their precursors. The US policy in the Middle East typically destabilises things (although they are hardly alone in doing that). [0] https://en.wikipedia.org/wiki/Foreign_relations_of_North_Kor... - \"Results of the 2017 BBC World Service poll. Views of North Korean Influence by country\" reply nkrisc 20 hours agorootparentprevYou have to ask yourself if the good is worth the harm. reply AnthonyMouse 18 hours agorootparentBut the math on that looks like this. The \"really bad\" people have no conscience. No qualms about compromising the device of some innocent victim and then using that as their \"exit node\" if Tor wasn't available. So if Tor doesn't exist, that's what they do, and that's worse. Because not only do the bad guys still get to be anonymous, now the owner of the compromised system takes the blame. Which is more likely to be someone less able than you to articulate what happened, and who has to claim they were hacked with perhaps scant evidence rather than being able to point to their IP address on the public list of Tor exit nodes. They also might not be in a country with due process. So what you're doing there isn't helping the bad guys, it's saving some of their innocent victims from being unjustly punished. Meanwhile the \"good guys\" who use Tor do have a conscience, so they wouldn't do that to an innocent third party, and then without Tor they have nothing. So you'd be helping them too. reply qsdf38100 12 hours agorootparentWe shouldn’t have keys then. Really bad actors are going to force your door anyway. Let’s at least save the doors. Come on, Tor main use is child pornography and drugs. If you think you’re helping oppressed journalists, it’s 99% false. You’re mostly enabling all sorts of criminal activities, from benign to major. Hosting a tor exit nod doesn’t make you a hero, quite the opposite actually. reply AnthonyMouse 11 hours agorootparent> We shouldn’t have keys then. Really bad actors are going to force your door anyway. Let’s at least save the doors. Locks aren't for the really bad people, who are in fact going to break down the door. They prevent crimes of convenience. But Tor is the lock, and the crimes of convenience would be e.g. mass surveillance of the population, in the event that ordinary people don't have it. So it's not clear what you're arguing here. That everyone should use Tor? > Tor main use is child pornography and drugs. If you think you’re helping oppressed journalists, it’s 99% false. Start here: https://news.ycombinator.com/item?id=41507790 Add to this, the illegal stuff isn't accessed via exit nodes, which link into the ordinary internet. Those things use hidden services, which are internal to the network and don't use exit nodes. But let's even explore the premise. Suppose a lot of the traffic is people trading in illegal materials. Well, that's not really a big problem; people do that stuff via several other existing channels and the societal cost of each instance of someone buying pot over the internet isn't very high. Whereas the societal benefit of one single whistleblower is massive. These things can change the lives of millions of people. So even if it's 99% contraband, the remaining 1% is ten million times as valuable. reply Ferret7446 9 hours agorootparentprevI would use that argument if I were an oppressive government that was troubled by journalists using Tor to expose me. It's only 1% right? Think of the children. Quoth Fidel Castro: ¿Armas para qué? (What do you need guns for?) Guess what he did after he took the people's guns reply II2II 19 hours agorootparentprev> depending on which side of the Israel/Palestine conflict you are on Here's the thing: I am not on either side of that conflict, or likely any other conflict you could use as an example. There are atrocities committed by both sides. There are victims on both sides. You could argue over who committed the worse atrocities or over who is the biggest victim until your face turns blue, it isn't going to end the cycle of violence as long as there are people facilitating that violence. And no, I am not naive. I know there are people out there who care nothing about causes beyond their own self interest and who care nothing about their victims. I realize that these people are impossible to combat without the innocent coming in harms way. Yet the moment we fail to be ashamed of the harm we cause in the name of the cause, the moment we fail to acknowledge who is being harmed in the name of the cause, is the moment we become no better than them. reply Ferret7446 9 hours agorootparentHere's a better example then. Publishing the truth or publishing opinions about political leaders is illegal in some jurisdictions. Would you be unwilling to provide help to these \"bad actors\"? Lots of horrible dictators have used rhetoric like yours to rationalize/facilitate their actions. The fact of the matter is, there really is no absolute objective moral compass; and yes, that includes \"we should just stop facilitating violence\" because you absolutely can be enabling others to take advantage of that to cause more harm. You have to pick a stance and live with the harm that comes out of it (yes, whichever stance you pick, will cause harm). reply szundi 17 hours agorootparentprevOh just because you are not affected yet, you might be in the future, most probably if no one is there to help against people with obscene power and they start to easily win reply nox101 11 hours agorootparentprevI don't think that dicotomy is quite right. bad actors can take away my freedoms (for example if they steal my bank account I'm no longer financially free as I'd have no money) I don't know the correct balance. maybe it's just an impossible problem. I just don't think the two sides are freedom vs state protection. reply totetsu 17 hours agorootparentprev>In the end, this really comes down to whether you value freedom or state protection more; either of which can be abused by rogue actors or a malicious state, respectively. There is no win-win-solution, unfortunately. I want to argue for freedom, on the grounds that most people know whats best for themselves better than others, so on balance there should be more people using that freedom for good, but then most people are busy, and not as motivated or knowledgable of how to use that freedom as the malicious actors are.. so is that even freedom in the end? reply zepearl 19 hours agorootparentprev> In the end, this really comes down to whether you value freedom or state protection more... This is again a forced binary \"and/or\"-decision, without anything inbetween. It doesn't have to be like that - both can coexist, if both terms are not extreme. (disclosure: my post is not related in any way to Israel nor Palestine and I'm personally not linked in/directly to anything related to Israel nor Palestine and this post is not related to the current conflict) reply User23 18 hours agorootparentprev> Just as an example, depending on which side of the Israel/Palestine conflict you are on, either side using your node for military intelligence might be an use worth fighting for or terrible abuse. The problem is when you choose to involve yourself in nation-state conflicts they’re just not going to care about your protestations of neutrality and freedom. They’re just going to see you aiding their enemy. reply _ph_ 4 hours agorootparentprevAnd it doesn't need to be a \"really bad actor\". I have been spammed by someone for years who clearly used a script to target an online service of mine. Always connecting from TOR, so banning an IP or a range wouldn't block that person. This shows how easily TOR can be abused, even for small misdeeds. reply AnthonyMouse 18 hours agorootparentprev> even government organizations/departments/agencies can be \"local\" and scattered (e.g. similar IT departments for each \"canton\" in Switzerland) and not have huge amounts of resources/knowledge to track/identify perpetrators of all ongoing (sophisticated?) IT crimes => somebody somewhere might see the same IP involved in a lot of \"bad\" stuff not realizing it's just a TOR node. Decentralization is not an excuse for negligence. Anyone working in cybercrimes should be aware that Tor exists and of what it is. The list of exit nodes is public. Harassing the operators can only be one of malice or incompetence and neither alternative is excusable. reply raxxorraxor 9 hours agorootparentprevIdealism around privacy and liberty are quite important, otherwise you end up with a worse country and there is a reason for laws to usually grant people these rights. The law failed here and it is a typical problem for Germany, that historically and still today has problems with liberties in general. FUD doesn't mean we should do away with liberty. To say otherwise is naive idealism that requires infallible human actors in security related agencies. That is impossible. reply john_the_writer 20 hours agorootparentprevAgreed.. this \" I could not ignore the fact that any \"really!!!\" bad actor could use the same infrastructure to avoid investigation/prosecution,\" could be dependant on what you personally see as bad actor. Would being gay count? In some countries it's a death sentence, so using TOR is how they avoid being thrown off a roof or stoned. Talking about anything LGB is a crime. What about someone who wants to read 1984.. Would you be okay with them committing that crime? reply zepearl 19 hours agorootparent> I hate the current general trend pushing a position of an either absolute \"yes/no\" for any theme... reply paperplatter 19 hours agorootparentprevYes being gay is illegal in some countries, but those governments don't have the ability to raid a German citizen's home for it. reply AnthonyMouse 18 hours agorootparentThe people who do live in those countries could, however, be using an exit node in Germany. It isn't the exit node operator who chooses who uses it. reply raverbashing 12 hours agorootparentprev> I hate the current general trend pushing a position of an either absolute \"yes/no\" for any theme, including this one (of encryption for privacy/etc vs. crime). Exactly Making an analogy, I feel these people are kinda the European ideological equivalents of the \"sovereign citizens\" in the US (though sure, they're usually more informed) In one way, deeply concerned about very legitimate worries of free speech and privacy. In another way, very naive about what happens in the real world or how legal process works Expectations: \"We're helping people fight dictators!11\" Reality: 80% malicious usage, 10% \"just a prank bro\", 5% people with legitimate uses and then the rest reply qsdf38100 12 hours agorootparentAgreed, except, what is especially European about this? reply raverbashing 11 hours agorootparentThe idealism and rose-tinted/\"self righteous\" view of the world. \"Wir schaffen das\" reply oefrha 18 hours agoparentprevI’m surprised DO allows Tor exit nodes. No wonder their IP reputation is trash the time I tried to set up my mail server there. https://docs.digitalocean.com/products/droplets/details/poli...: > We do not specifically disallow Tor exit nodes, but as the account holder, you are responsible for all the traffic going through your Droplet (including traffic that an exit node may generate), and we do prohibit some of the traffic types that may go through a typical Tor exit node. > If you are unable to stop prohibited traffic like torrents, spam, SSH probes, botnets, and DDoS attacks, running a Tor exit node may lead to us suspending or terminating your account. We send you an email in the event of a violation of our Terms of Service, and you must address these issues as soon as possible. Running Tor exit node without abuse? How is that possible? Since they didn’t shut you down after three abuses serious enough to get law enforcement involved, I guess they don’t really give a shit about abuse after all. reply ErikBjare 4 hours agorootparentRestrictive exit policies reply Hizonner 22 hours agoparentprevI actually think that Tor should deemphasize exit nodes and trying to provide access to the clearnet, in favor of better hidden services. Nearly every major site ends up either totally blocking anything that comes from a Tor relay, or applying massive numbers of weird CAPTCHAs and restrictions, so it's getting to be basically unusable anyway. reply costco 17 hours agorootparentThe new Cloudflare captcha has changed this and it's a lot better now. There's no more Recaptcha hell. I read the Ben Collier book about Tor recently and in his interviews he found that some Tor contributors actually feel the opposite, because they feel the negative attention that the \"dark web\" mythology brought on has been bad for Tor. According to the book the archetypal Tor user is someone in a censorship heavy country like Iran visiting facebook.com or nytimes.com, so they don't get much out of hidden services. reply saagarjha 14 hours agorootparentI don't even use Tor (this is literally stock Safari) and Cloudflare will not let me through as of last week or so. reply genpfault 21 hours agorootparentprev> I actually think that Tor should deemphasize exit nodes and trying to provide access to the clearnet, in favor of better hidden services. Isn't that I2P[1]? [1]: https://en.wikipedia.org/wiki/I2P reply beefnugs 20 hours agorootparentprevThere really is a fundamental difference between : secure end to end messages of willing participants. VS arbitrary anything-illegal from someone else's public ip. reply amy-petrik-214 17 hours agorootparentprevThis gets back to AnthonyMouse's argument (above) that (1) TOR exit node operators are buffers to protect people from being hacked. A hacker would more easily use TOR than need the effort to runa scan for vulnerable routers, root one, and hop between various routers. Which implies (2) if TOR had no exit nodes and/or clearnet service blocked TOR ranges, hackers will just resort to hacking routers / other systems / botnets to make their own proxy. Now the block doesn't work, someone(s) got hacked, TOR is gone. Basically TOR as a \"containment\" system. Seems to me that would be preferable for law enforcement, particularly because some state actors (https://www.infosecinstitute.com/resources/general-security/...) are putting great effort into unmasking TOR, making it a great honeypot. Ironic that Germany prosecuted a German exit node when they were the same ones investing heavily in unmasking it! reply beaglesss 22 hours agoparentprevWouldn't the true exit node be the ISP as you are one clear node behind them? How many ISP execs get raided by SWAT teams? reply edm0nd 21 hours agorootparentYes the IP was just a DO vps I setup to be a Tor exit. That's why they requested my personal account information, billing info, IPs that I logged into DO with, all of that. If not interrupted by me getting the help of the amazing EFF lawyers, the next step after getting my personal information, could have been to raid my home and seize all my electronics. I work from home and would have been greatly disrupted and not been able to work without my computers and etc. Then I'd have to wait months/years to be found innocent and then get all of my electronics back + spend thousands on lawyers. During all of this, the EFF lawyers straight up told me to prepare my home as if it were to be raided and encrypt all my devices. Thankfully it did not come to that. reply q3k 5 hours agorootparentprevIn jurisdictions whose ISP laws I'm familiar with, ISPs have a special protection granted: they don't get raided because they're seen as an infrastructure provider, but only as long as they can point to a customer responsible for some given traffic when served a court order. reply tzs 18 hours agoparentprevI was going to run an exit node when I first learned about Tor, but realized that the cool positive use cases I was imagining it would help with could be effectively done in other ways. In some cases those other ways might not be as easy, but there would be enough resources available to the people involved to get the job done. It seemed likely that it would be the horrible use cases it would benefit the most. Balancing an increase in the efficiency of doing good things that could already by done other ways against greatly benefiting horrible use cases made it so that I could not morally justify it. reply batch12 20 hours agoparentprevSituations like this are the main reason I shuttered the torwhois.com service. The barely zero gain wasn't worth the risk, sadly. reply shadowgovt 21 hours agoparentprevBut flipping the script: bomb threats and Qatar conducting international espionage aren't silly things as far as the government is concerned, and if we intentionally interpose ourselves in the comms channel in a way that the attack trace stops at us, we should be expecting follow-up from a human being tasked with enforcing the law, right? reply edm0nd 21 hours agorootparentI suppose my issue stems from my perception of the seemingly lack of serious investigation on their law enforcement side. If you had visited any of my exit nodes via port 80 or 443, I had a lander on them stating that it was a Tor exit node and to please contact me if you wanted your IP to be blacklisted from it. I also stated that there was no useful information contained on this server (by design) that would be helpful for any evidence gathering or investigations. Seriously, all they had to do was plug my IP into a browser or do a simple scan of it but I suppose that's asking too much from LE lol. Additionally, Tor exit nodes are public and all they had to do was look into my IP more than 5 seconds after finding it in logs somewhere and firing off a warrant or subpoena for it. The first two were straight up vague templated fishing expeditions. The 3rd subpoena actually came straight from the DOJ and was a lot more detailed and serious. They should know what Tor is and know that any Tor server contains ZERO info that would be able to assist them in whatever they are attempting to investigate. Sure, I do think such situations require follow-up but as soon as they are informed it's a Tor ip, they should know to drop any pursuit of getting evidence from it. They do not, they continue to go after you via legal means. Even though I had the EFFs help, this entire process still took months. It's pretty stressful to be in a situation where its lil ole me VS the entire United States government who has unlimited resources, time, and money to go after you. I am extremely blessed to have had the EFF lawyers at my defense and will forever be a life long supporter and donor to them. They really do fight for our digital rights and can help defend you in a digital equivalent of a David versus Goliath situation. reply cortesoft 21 hours agorootparentThe end goal is probably to get you to do what you did, which is shut down the exit node. If they make it painful to run a Tor exit node, they make Tor harder to use. reply lolinder 21 hours agorootparentExactly. Which is not as obviously an unethical approach as some here would think—if you are standing between law enforcement and a bomb threat, \"I'm intentionally ignorant of the activities of the people that I'm shielding\" is a morally dubious place to stand. The law allows law enforcement to subpoena records related to an investigation like this, and I honestly think it's fair to force Tor exit node operators to handle those subpoenas every time, even if the answer is always the same. To have some sort of automated process in place to deflect blame allows an exit node operator to ignore the real damage their work can do. They may still decide that the good that they're doing outweighs the bad, but forcing them to see the negative consequences of shielding anyone who wants a shield has value. reply courseofaction 20 hours agorootparentIs that the horseman we're giving up our rights for today? reply lolinder 20 hours agorootparentYour right to knowingly run a service that is used by people to kill other people while never having to interact with the consequences of that decision? I'm not suggesting people shouldn't be able to run a Tor exit node. I'm suggesting that people who run Tor exit nodes should occasionally have to a deal with a subpoena that says \"your exit node was used by a criminal to hurt people in ${these ways} and we require any information you have to help apprehend the attacker.\" I don't want to deprive anyone of the right to make a moral decision, but I do want them to feel the weight of the full import of that decision. reply AnthonyMouse 18 hours agorootparent> Your right to knowingly run a service that is used by people to kill other people while never having to interact with the consequences of that decision? Can you name a product or service for which this is not the case? Militaries use general purpose software to design weapons. Murderers use vehicles and transit systems. We don't expect the government to harass the makers of cutlery because they provided a product used in a mugging. reply lolinder 17 hours agorootparentI think that any creator of any tool should be faced on a regular basis with the harm that that tool causes and have to make the call on a regular basis if it's still worth it. reply AnthonyMouse 16 hours agorootparentSo steel workers should get a subpoena they have no effective means to respond to on a regular basis because steel is used to make all manner of weapons and machinery that gets used by bad actors? reply numpad0 11 hours agorootparentAside from this being a bad faith comparison - no way you actually believe that steel rods and bars can't be subject to EAR reply AnthonyMouse 11 hours agorootparentYou can't justify a bad policy with a different bad policy. Trying to control access to a fungible global commodity is pointless. reply numpad0 10 hours agorootparentYou've questioned existence of such \"bad policy\". I pointed out that there are such policies. I neither supported nor opposed them. I won't be surprised if there were something in US criminal code with supreme court precedents that specifically dictate the government harass in timely manners the makers of cutlery used in a mugging. There _are_ always laws. _Everything_ is regulated. Most of those regulations are reasonable. reply lolinder 16 hours agorootparentprevThis is a bad faith comparison and I'm not going to engage with it. reply AnthonyMouse 15 hours agorootparentI'm honestly not sure what distinction you're trying to draw between them. Clearly any ordinary product can be used for nefarious purposes. The distinction some people try to draw is when a higher proportion of a product's users are nefarious, but that doesn't really work either because who uses something can change over time. If you have a society where nobody has window blinds or locks on their doors because it's a rural area and there is no one around to invade your privacy then locks will be disproportionately used by neerdowells \"with something to hide\", and then busybodies will claim that anyone with nothing to hide shouldn't be concealing their private spaces and anyone selling or using any privacy technology should be pressured to stop. Which sustains the status quo through external pressure even if someone does start invading everyone's privacy. And that's what's been happening on the internet. Surveillance is the default, Cloudflare et al block Tor users as a matter of course and that drives normal people from Tor and similar technologies even though they would otherwise benefit from its use. People are told that it's the dark web where there are criminals and they shouldn't use it -- it being Tor Browser, the thing that keeps ad networks from tracking them across the internet. Then after dispersing the normal users who would otherwise benefit from using it, people say that it has a lot of nefarious users to justify the continued harassment of anyone who does. But that's just path dependence, and there are parties interested in leading us down the garden path to mass surveillance. reply Propelloni 10 hours agorootparent> Clearly any ordinary product can be used for nefarious purposes. Right, I could kill a person with a spoon. Still we regulate guns and not spoons, why is that? reply zo1 14 hours agorootparentprevYou are talking into a void following this line of reasoning. There is no logical consistency in the context of a state and all the myriad of terms and concepts in its wake. That's by design and everyone that's brought up under it from a young age is taught to embrace that, as a feature. Your words are foreign invaders and every core of these smart people's beings will fight you with their ridiculously smart and well trained antibodies. Not trying to single out the person you're responding to, but I've seen this play out many times and engaged in it previously to no effect. reply shadowgovt 14 hours agorootparentprevWe, uh, absolutely expect the government to \"harass\" people operating transit systems for any and all information about a criminal using that system. Camera feeds, ticket records... All of that is accessible via warrant. That's probably the most salient example in this context. reply AnthonyMouse 13 hours agorootparentTor exit nodes don't have any information to identify the end user. They don't know who it is, so there is nothing to subpoena or turn over. Subjecting low-resource entities to a known-futile legal process is a form of harassment. reply shadowgovt 4 hours agorootparentIt's not known-futile. A misconfigured Tor node could be storing all sorts of useful traffic data. Besides, there's also the possibility that the exit node operator themselves could be the actor; since the trail stops at them, they're under suspicion. reply aspenmayer 18 hours agorootparentprevIf it is moral for the US government to create Tor, it is moral to use it. Sure, it may be it’s a tragedy of the commons, but there’s no individual moral accountability or responsibility for those running Tor because of things other people do or don’t do on it. That’s outside anyone’s ability to control anyway. reply Nursie 17 hours agorootparent> there’s no individual moral accountability or responsibility Of course there is. If I am deciding whether to dedicate resources, money and time to running a service which - a) Helps dissidents in authoritarian regimes communicate freely and b) Enables bad actors to send threats and/or move CSAM around Then that is absolutely a moral choice I need to make. It's not outside your control, you get to decide whether or not to provide the service. reply BLKNSLVR 19 hours agorootparentprev> I suppose my issue stems from my perception of the seemingly lack of serious investigation on their law enforcement side. That's my experience too from actually having my house raided. I had two kids in bed at the time, and the police didn't even know to expect kids in the house (both kids were over 11 years old, had birth certificates, had lived in that house all their lives and attend local schools and are darn fine students). They didn't know. It's mind boggling to me that they could get a raid warrant without having done even the most basic (below even basic) investigation. My opinion of police investigative competence took a 99% hit as a result. It's a lesson my kids won't forget either. reply hcfman 1 hour agorootparentThe raid no doubt was carried out by the police. They just what they are told to do by an organisation that is higher up. No one will get reasons. Maybe the chief of police. But only a limited amount so he can claim plausible deniability. The dirty people behind all this are in the way they run the investigations. And what way is that ? Well it’s the “organised crime investigations”. The Netherlands pushed the RIEC way of working here to Germany and Belgium. Look it up. Euriec. The whole way of working is to do dirty tricks in an unaccountable way. reply admax88qqq 20 hours agorootparentprevI don’t know. Could you imagine if you were in charge of investigating something like this and you _didnt_ check one of the computers involved just because the guy who owned the computer claimed it doesn’t have anything useful on it? There could be logging bugs in Tor that you were unaware of, or the owner could be using Tor as a cover. It would be negligent _not_ to at least check the device logs for anything useful. reply thecrash 19 hours agorootparentBy that logic why not also seize and do forensics on all the ISP's routers too then, just in case? After all, the ISP could be secretly in on the criminal plot, and how could you know without imaging every hard-drive in the data center? It would be negligent not to. The truth is that police investigations normally are restrained based on the disruption that they cause the public. Police deviate from standard operating procedure when it comes to TOR exit node operators because they want to punish and intimidate them. They want to punish operators because the authorities are frustrated by the effectiveness of these technologies in countering the pervasive surveillance environment which the authorities take for granted. reply lolinder 16 hours agorootparent> Police deviate from standard operating procedure when it comes to TOR exit node operators because they want to punish and intimidate them. Citation needed. ISPs have entire departments dedicated to cooperating with law enforcement. Comcast has a whole portal with its own subdomain specifically for handling requests from law enforcement [0]. Cox has a page detailing exactly how to send them a subpoena [1]. These guys are clearly dealing with subpoenas just like the ones OP is describing all the time. It only seems out of the ordinary this time because it's a random person who decided to play middle-man instead of an enormous corporation with a massive legal department. [0] https://lrc.comcast.com/lea [1] https://www.cox.com/aboutus/policies/law-enforcement-and-sub... reply aspenmayer 18 hours agorootparentprev> By that logic why not also seize and do forensics on all the ISP's routers too then, just in case? After all, the ISP could be secretly in on the criminal plot, and how could you know without imaging every hard-drive in the data center? It would be negligent not to. Implying that they don’t have the capability to do this already and/or alternative means to accomplish the same thing. https://en.wikipedia.org/wiki/Room_641A > Room 641A is a telecommunication interception facility operated by AT&T for the U.S. National Security Agency, as part of its warrantless surveillance program as authorized by the Patriot Act. The facility commenced operations in 2003 and its purpose was publicly revealed by AT&T technician Mark Klein in 2006. reply gamblor956 16 hours agorootparentprevISPs cooperate with law enforcement. Most even have dedicated staff for that. So there's no need to seize their equipment. reply bongodongobob 17 hours agorootparentprevnext [–]\"source IP address\" is useless as evidence of a crime, because, as this server and many, many other proxy services demonstrate, the IP listed as the origin is in no way guaranteed (or even likely) to be the actual origin of the traffic. It doesn't have to be the actual origin for it to be useful—unless the software is specifically designed to avoid traces (i.e., Tor), there are often logs that will lead you to another IP address, which might lead you to another, which might eventually lead you to the source. It would be foolhardy for police investigating a bomb threat to not at least ask, given how many people they do in fact catch this way. > It's like raiding the home of the mail carrier because someone got drugs in the mail. No, in the case of OP it's like subpoenaing the local post office and asking for everything they know about where that package came from. Which is, incidentally, quite common, except that in the US the post office is a government entity that doesn't need to be subpoenaed because it has its own law enforcement agency that should have jurisdiction over the case. reply edm0nd 21 hours agorootparentprevFair enough! reply shadowgovt 21 hours agorootparentprev> Seriously, all they had to do was plug my IP into a browser or do a simple scan of it but I suppose that's asking too much from LE lol. I mean, yes, I'm pretty sure \"just take my word for it\" is asking too much of LE. We can always say \"Come back with a warrant\" but then sometimes they'll come back with a warrant. > They should know what Tor is and know that any Tor server contains ZERO info Unless, of course, one has misconfigured it... Which could be the case. Definitely the kind of thing LEO can figure out on the other side of a seize-and-strip of the hardware. Unfortunately, I think the only way to not be a part of the story here is to not be a part of the story here... Don't proxy anonymous traffic if you don't want law enforcement asking after the anonymous traffic you proxied. Otherwise, expect the responsibility imposed upon a service provider (since you're providing a service). Other ISPs avoid this scrutiny by going out of their way to be helpful to law enforcement. reply zadokshi 14 hours agorootparentThere is no way for police to know if the traffic came through tor, or was initiated by the owner of computer/server. It seems reasonable that the police have the right to investigate. If not, anyone could run a tor node to cover up their own criminal activities. Even if you did have logs suggesting it was tor activity, should we trust someone’s claim that the logs are proof that it was someone else? It would in fact be negligent if the police did not properly investigate the server/computer/house of the device. reply edm0nd 21 hours agorootparentprevYup that's the same conclusion that I've come to for now. I got a family and stuffs now so don't want to bring any stress to them. One day I will resume but in the future :) reply treebeard901 21 hours agorootparentprevThe danger is that the Government could just make all this up to specifically target nodes they do not control. The exit nodes have been known to be the weakest part of the tor design. It has been a logical theory for a while that all exit nodes are visible to the U.S. Govt. This is just one way they can leave a system like Tor up for their uses and also make sure anything domestically is fully visible to them. reply impossiblefork 21 hours agorootparentWhat about timing attacks though, things like governments controlling things coming and going into routers and the internet as a whole? Surely that's worse than the exit nodes? The way I see it, the right approach is some kind of continuous communication where messages end up in fixed slots, where if no message would have gone, there'd have been a randomly generated message. reply gary_0 21 hours agorootparentprevYes, but they should be able to investigate without placing an undue burden on exit node operators (or regular people with a compromised device that was used as a proxy). Unfortunately it's hard not to be cynical and assume that these kinds of overreactions (and worse) are going to continue. But in my opinion, any society where policing is convenient for the police is a horrible place to live. (Is it really such a radical concept that law enforcement should be focused on protecting the innocent, not punishing the guilty?) reply lolinder 21 hours agorootparent> but they should be able to investigate without placing an undue burden on exit node operators Is the burden undue? A Tor exit node operator has made the ethical judgment call that they're doing more good than harm. That might be a reasonable position to take, but I don't think it's unreasonable for us to expect an operator to face up to exactly what it is that they are doing. I'm fully on board with any bomb threats (as just one example) leading to a subpoena on the exit node operator who shielded the threat actor, even if the answer is the same every time. Making the decision that you're doing more good than harm requires you to fully understand the harm that you're justifying, and law enforcement subpoenaing you every single time is one way to make it very clear what it is that you're choosing. reply gary_0 20 hours agorootparentI can think of very few cases where the possibility of your home being raided by heavily armed police officers, and your property seized, is appropriate if it's clear all you're doing is running software. (Side note: I'm surprised how often attitudes on this site are at odds with the \"hacker\" part of \"Hacker News\".) It is fair that running an exit node might be inconvenient, maybe even to the point where consulting a lawyer is advisable, but I think we should draw a hard line at direct threats to an innocent person's liberty, livelihood, and physical safety. That kind of fear is definitely an \"undue burden\". reply lolinder 20 hours agorootparentYes, I can agree that an armed raid or the threat thereof is definitely an undue burden. > it's clear all you're doing is running software. (Side note: I'm surprised how often attitudes on this site are at odds with the \"hacker\" part of \"Hacker News\".) I do not view software as amoral. It's a tool, and like any tool it is an extension of myself. Software that I run is acting on my behalf, and what my software is designed to do is something that I should be held morally accountable for. I'm not sure when the hacker ethos came to mean that \"just running software\" absolved you from having to account for the damage your software causes, but if that's what the hacker ethos is about then yes, you can count me out. reply gary_0 19 hours agorootparentMy point was that running any kind of software should not come with a presumption of guilt. But in the eyes of the establishment, it often does; see: Aaron Swartz, or how pressing F12 might be illegal[0], or many other such cases. A \"hacker\" should not have any sympathy for this kind of draconian knee-jerking. [0] https://techcrunch.com/2021/10/15/f12-isnt-hacking-missouri-... reply lolinder 19 hours agorootparent> should not come with a presumption of guilt Where is the presumption of guilt? A threat of violence was traced to their IP and they were served a subpoena to provide information that might lead to finding the threat actor before they actually hurt anyone. No one even accused OP of a crime, much less presumed their guilt. reply gary_0 19 hours agorootparentI don't mean in the judicial sense, I mean in terms of how they are treated by law enforcement. reply lolinder 19 hours agorootparentAgain: where is the presumption of guilt in OP's case? They got subpoenaed, they enlisted help to respond, life went on. Their lawyers warned them to prepare as though a raid would occur, but that's the lawyers' job: to prepare their clients for the worst just in case. reply shadowgovt 4 hours agorootparentprev> (Side note: I'm surprised how often attitudes on this site are at odds with the \"hacker\" part of \"Hacker News\".) When computing became predominantly online, hackers inherited a moral dimension: the need to consider whether they are doing harm to others via what they do with the shared global network. It's a different story when you're cobbling scraps together in your basement, and it's a different story when you're primarily phone phreaking \"the man,\" as it were. reply goodpoint 10 hours agorootparentprevHacker News is hacker like a hot dog is a dog reply Hizonner 20 hours agorootparentprev> law enforcement subpoenaing you every single time is one way to make it very clear what it is that you're choosing. That's not what subpoenas are for, and it would be a really stupid waste of time and resources. If you really want to do that, just send them an email. reply lolinder 19 hours agorootparentAn email can be filtered, doing that with a subpoena would be... silly. > a really stupid waste of time and resources Subpoenas are used all the time in cases where they're not expected to be inherently useful for acquiring information. If law enforcement is going to take 10x as long to find the perp because you hid them, I don't see a problem with them sharing that burden with you a bit—there are externalities here that should be internalized. reply ruthmarx 14 hours agorootparentprev> Is the burden undue? Yes. > A Tor exit node operator has made the ethical judgment call that they're doing more good than harm. They are. Absolutely. It's not really a question. > Making the decision that you're doing more good than harm requires you to fully understand the harm that you're justifying, and law enforcement subpoenaing you every single time is one way to make it very clear what it is that you're choosing. No, that's just harassment. reply dgfitz 20 hours agorootparentprevScenario: LEOs knock on your door and take everything connected to the internet. Why? Your home was running an exit node. Who? Your 12 year old. Yeah yeah “parents should know” but given the rash of shootings by young people, fuck that argument. reply creer 17 hours agorootparentprev> we should be expecting follow-up from a human being tasked with enforcing the law, right? That's very nice but until tor exit nodes are illegal, such police action is purely a harassment effort, right? One thing that struck me, years ago, is that the people running these actions (recipient of a death threat or police) are far more concerned with the fact that \"someone enabled this\", rather than the fact that someone was angry enough at them to issue a death threat. They had no visible concern about that wannabe murderer, apparently spending no effort trying to identify THEM. They just wanted retribution against the exit node operator. It was totally doing something for the sake of doing something, zero concern about solving any root problem. They had seemingly zero concern that their safety was a risk (I mean, from eventual action stronger than a death threat.) They also had zero awareness that anonymous email had allowed this ennemy to be revealed before any physical violence. reply snakeyjake 20 hours agoparentprevI ran an exit node back 2007-2008 ish after learning about Tor at a conference. I stopped running an exit node when I looked at the traffic flowing through it. I even sslstripped it back when that was much easier. No freedom fighters. No oppressed journalists. No free speech. Only porn and scams. Running a Tor exit node for freedom is like burning a village to save it or enriching your own uranium to solve the energy crisis. There's gotta be an answer, but this ain't it. reply ErikBjare 4 hours agorootparentMakes sense that's where the bulk of the volume is, not much different from the internet at large. Freedom fighters and oppressed journalists are exceedingly rare, but they do use Tor. I wonder what you expected? reply rcbdev 13 hours agorootparentprevJust because most stuff is botspam, that doesn't mean it's not worth it for the occasional Snowden or Panama Papers - those would have been next to impossible to safely execute without Tor. reply atemerev 2 hours agorootparentprevEnriching own uranium is an interesting project. I prefer nuclear simulations, but same vibes. reply jfengel 18 hours agorootparentprevThey were sending this in cleartext? reply qup 20 hours agoparentprevWhy don't lawyers just do this stuff? Then minor legal threats are not a concern. Alternatively, why don't we become lawyers, too? reply seb1204 5 hours agorootparentMy sarcastic self would say because lawyers became lawyers to earn good money and have social standing. Not to be benevolent to society. reply tonygiorgio 18 hours agorootparentprevAt the end of the day, lawyers are human too, with lives and families. They would know the full extent of the inconveniences regarding home raids and device seizures for long periods of time. This would disrupt their lives, work, and probably affect their ability to serve their clients’ legal troubles. At the very least, I’m thankful for the efforts of the EFF and others that do know the law and help. But I’d imagine there’s a good case for separations of concerns here. Stay out of the legal troubles yourself so you can help others that do get caught up in it. One degree away. reply ajross 21 hours agoparentprev> The 1st [subpoena] was someone sent a bomb threat email to a university. The 2nd one was someone sending a phishing email. ... > I one day hope to resume running exits as I find it rewarding to be able to help people from around the world in a small way. This really doesn't strike you as cognitive dissonance? I mean, yes, I get it, it's easy to construct a scenario where you're \"helping people\". But you're also \"helping\" people engage in terrorism and identity theft in exactly the same way. Surely that deserves at least a little thought and moral calculus, no? You're not making a first principles argument about fundamental rights or anything, you're saying you run exits because it's \"helping\". Well, shouldn't it help more than it hurts? reply loa_in_ 21 hours agorootparentDoesn't running a post office help people communicate coded messages about nefarious things? Doesn't running a telephone network help people do the same? What about cellular hardware providers and maintainers? reply krisoft 20 hours agorootparentThey do. But all of the above bend over backwards to help law enforcement. > post office help people communicate coded messages about nefarious thing The US postal service scans and stores the outside of every envelope and package they handle. Law enforcement agencies can query this metadata. https://en.m.wikipedia.org/wiki/Mail_Isolation_Control_and_T... > Doesn't running a telephone network help people do the same? They do, but they are not only share the metadata with law enforcement, but also let them wiretap. (Often they require a warrant for this, but that is not a hard burden for a LEO.) And this capability is not some aftertought, but deeply integrated into their tech stack. reply WarOnPrivacy 19 hours agorootparent> But all of the above bend over backwards to help law enforcement. We prefer they assist LEO operating under court order, instead. reply ajross 20 hours agorootparentprevTor isn't a post office or telephone network. We have post offices and telephone networks. Tor also isn't a replacement for a web browser or internet, we have those too. Tor's feature isn't \"communication\" in the abstract, it's anonymity. And yes, that can be used for good or for evil. But the upthread comment was saying how nice it was to run an exit node because it was \"helping people\". And to the extent that's true, I think correct thinking demands you also account for the harm. And let's be clear: Tor is definitely harmful. Almost all Tor traffic is some degree of nefarious. The tiny handful of dissidents are drowned in a sea of phishing and contraband. reply jacobsenscott 20 hours agorootparentprevYou don't need tor for terrorism or identity theft, and it probably isn't widely used in those circles. There are easier ways. But plenty of people use tor to avoid what amount to terrorist govenments and regimes. reply beart 18 hours agorootparentThis statement is made without basis. What percentage of tor traffic is used for terrorism, identity theft, or people avoiding persecution? I'm not going to make a value judgment on the use of tor, but I do think it's important to be honest about how it may be used. reply hwbehrens 20 hours agoprev> There are obviously still people working in German law enforcement today, who think that harassing a node-operator NGO would somehow lead to the de-anonymization of individual tor users. This is not why. > As a consequence, I am personally no longer willing to provide my personal address&office-space as registered address for our non-profit/NGO as long as we risk more raids by running exit nodes. This is why. It's basically a textbook example of a chilling effect. reply gea0 17 hours agoparentNo, that's not (necessarily) it. It only takes one person in LE to request to investigate this IP, and a single judge that isn't entirely convinced that it will be worthless to try to sign it off. If parts of the state wanted to harass operators systematically or organize to discourage TOR, they could do much worse. reply creer 16 hours agorootparentThe one person in LE is assisted by specialists (you know, if they really care to be.) reply tessierashpool9 11 hours agorootparentand the judge and the state attorney involved are controlled by the state's justice department which is run by politicians. yes, in germany the judiciary system is not politically independent ... https://www.transparency.de/aktuelles/detail/article/eugh-ur... reply tessierashpool9 11 hours agorootparentprev> If parts of the state wanted to harass operators systematically or organize to discourage TOR, they could do much worse. it is beginning to get much worse ... reply freilanzer 10 hours agorootparentprevMost judges don't really read what they sign if it comes from LE, I am convinced. reply walrus01 22 hours agoprevHistorical: \"Why you need balls of steel to operate a tor exit node\" http://web.archive.org/web/20100414224255/http://calumog.wor... The above is within the context of a western legal system, and certainly since it was written domestic law enforcement has become even more militarized and aggressive. I would be absolutely unsurprised if the same thing happened today and it resulted in a battering ram on the door at 0400 in the morning, flashbang grenades and the house being rampaged through by a SWAT team. reply chucksmash 22 hours agoparent> As a parent of very young children I have an extensive network of friends and contacts in my neighbourhood who also have children. As we know the subject of paedophilia is not one that can be debated with any rationality at all in the UK. It is surrounded by hysteria. I was terrified that people would find out that my computer had been taken because of that – ‘no smoke without fire’. reply numpad0 21 hours agoparentprevMany European countries have standing police armed forces, closer to army national guards than blue shirted civilian police. They're for suppressing resistance forces and revolutionary uprisings, and they tend to fill roles of FBI too. I think that contributes more to normalization of MP5 ninjas fast roping down through your chimney for Internet crimes in Europe than law enforcement over-militarization had done. reply tptacek 22 hours agoparentprevHas that ever happened to a Tor node operator? If it hasn't, what's the closest incident to a Tor node operator you're aware of where it has? reply dewey 21 hours agorootparentIn my country there was this famous case ~a few~ many years ago: https://www.zdnet.com/article/austrian-man-raided-for-operat... reply blub 13 hours agorootparentLooks like he was not only arrested, but actually sentenced: https://www.theregister.com/2014/07/04/austrian_tor_exit_rel... 3 years probation and 30.000€ fine. reply golergka 20 hours agorootparentprevhttps://lwn.net/Articles/720231/ reply Manuel_D 22 hours agoprevI'm not sure how a Tor exit node could operate legally. Tor is widely used for illegal activities. Like drug sales and CSE media. If a government goes on Tor, downloads such material they'll easily see the exit node as the last hop in the chain. It's a clear-cut case that the exit node operator facilitated illegal activity. My assumption is that Germany has some sort of common-carrier privileges for Tor node operators. In America, telecoms can't be sued for facilitating illegal activity. But they do have to assist law enforcement with finding criminals when requested. Would be happy to hear from someone who is more knowledgeable in this area. reply Hizonner 22 hours agoparent> I'm not sure how a Tor exit node could operate legally. Tor is widely used for illegal activities. How do ISPs operate legally? Every single thing that's ever been done over a Tor relay has crossed multiple ISPs. reply tensor 22 hours agorootparentISPs cooperate with law enforcement and often happily give out the information for people doing illegal things on their networks. I realize that operators of Tor exit nodes likely can't help track people on the Tor exit nodes, but I doubt law enforcement cares, they just see it as \"not helping\" while they see ISPs as \"helping.\" reply varenc 22 hours agorootparentThe core question here is w whether law enforcement actually believes, incorrectly, that the exit node operators are being intentionally unhelpful, or if they understand that due to Tor’s design the exit node operators have no valuable information but the police continue to raid them anyway as a scare tactic. reply aniviacat 21 hours agorootparent> The core question here is w whether law enforcement actually believes, incorrectly, that the exit node operators are being intentionally unhelpful They could keep logs, but they choose not to. They are intentionally unhelpful. The reason they aren't keeping logs is not for the privacy of others. If I run an exit node, I know I am not reading the logs to garner personal information of others. And unless someone hacks my server and goes through the logs, which is extremely unlikely, noone else will read the logs either. The only one reading the logs would be law enforcement. By not keeping logs, you are intentionally hindering law enforcement. reply alasdair_ 20 hours agorootparent>By not keeping logs, you are intentionally hindering law enforcement. This is why I keep a diary indicating every single person I've ever interacted with, along with the date, time and place. It's a pain to do so and it takes up a lot of storage space and it makes people wary about interacting with me but I'd certainly never want to hinder law enforcement. reply aniviacat 19 hours agorootparentUnless I'm misunderstanding your comment, you are arguing in bad faith. It is not a \"pain\" to set up logging. Most non-tor proxies implement logging. It would be a completely reasonable task for the tor project to implement logging by default. No one would be any more \"wary\" to interact with your tor node. Trusting your node not to log would be foolish anyway. So whether you make known that you are logging, or whether you claim not to log (but might secretly do anyway) doesn't make much of a difference. The storage space a log takes up is negligible (unless you keep logs for unreasonably long times) on anything but the smallest systems. And since running a tor node takes quite a bit of processing power, you won't be running your node on a system that can't handle a few megabytes of logs. reply gretch 19 hours agorootparentprev>It's a pain to do so and it takes up a lot of storage space The perspective is that in order for these actions to be ethical, you must log the traffic, or you should not bother setting up the node. It's irresponsible to setup the node (which takes some amount of effort) but not do the precautionary part which makes it ethical. You can believe otherwise if you'd like, but this is an ethical framework applied to many other parts of our society and it's the thing that sets you apart from the ISPs, and generally it's the thing accepted by the public at large. reply throwai 12 hours agorootparentI'd like to make the argument that \"we\" believe otherwise. It is legal to keep some logs for a limited amount of time if you run an IT service in Germany, mostly for the purpose of keeping the service running properly. If you have that data, you can give it to LE when they request it. The thing accepted by the public at large is often codified within a country's laws. German laws generally do not require you to store logs if you are an ISP. Storing them for too long can even be unlawful. There is no so called Vorratsdatenspeicherung anymore, and it is a recurring topic of political debate. So at least in Germany, the public view on storing data is more complex, and people don't believe not storing data or reducing the amount of data stored is clearly immoral when running IT services. https://de.wikipedia.org/wiki/Datenvermeidung_und_Datenspars... On another note, if LE requests you to log specific access patterns in advance, you might have to do it. If your ISP services are really big (lots of users), you might even have to provide some sort of interface for LE. IMO and under certain interpretations of the involved laws, the German state could ask every single node operator in Germany to log everything, but the political backlash would be quite high. reply numpad0 11 hours agorootparentprevThis is why you enable location history for Google Maps. It had genuinely saved few honest people from false accusations. reply dmichulke 8 hours agorootparentI'm stuck. Please add /s or the links or both. reply numpad0 4 hours agorootparentTelescreen works both ways. That could come in handy if you truly have nothing to hide, or I suppose if what you are hiding must be within His tolerances. Chalmers sent a copy of his timeline to Premier Park Ltd, the company that charged him with the crimes, and the defendant was able to prove his innocence. The charges against him were dropped.[1] the lawyer met with the detective in order to show him screenshots of McCoy’s Google location history, including data recorded by RunKeeper. The maps showed months of bike rides past the burglarized home, NBC News reports.[2] 1: https://www.phonearena.com/news/google-maps-keeps-user-from-... 2: https://news.sophos.com/en-us/2020/03/10/google-data-puts-in... 3: https://www.youtube.com/watch?v=d-7o9xYp7eE reply afh1 20 hours agorootparentprevLaw enforcement is also about going after whistleblowers, journalists, or, in most countries, just ordinary citizens the current people in power don't like, even if no crime was committed. reply aniviacat 19 hours agorootparentYou seem to have misinterpreted my comment. I was not making any moral judgement on people operating tor nodes. I was simply stating that you are, in fact, hindering law enforcement if you set up a non-logging proxy for the purposes of hindering law enforcement. Whether that's a good or a bad thing is up to you to decide. Clearly many people think it's a good thing; good enough to go through the efforts of setting up a proxy. reply sealeck 18 hours agorootparentprev> They could keep logs, but they choose not to. They are intentionally unhelpful. Some tech companies have extremely sophisticated observability which dumps huge volumes data about the internal state of a program. Some companies have very limited observability beyond maybe logging \"we just served a request\". Your argument suggests that companies who don't have the extensive logs of the former are being intentionally unhelpful? There are lots of reasons to not keep logs – lack of storage space, additional economic cost of doing so, slower response times due to overhead of observability, etc. reply gwd 21 hours agorootparentprev> incorrectly, that the exit node operators are being intentionally unhelpful I mean, exit node operators are being intentionally unhelpful? They're intentionally helping people who don't want to be tracked. \"I don't want to give you the papers\" and \"I can't give you the papers because I burned them so that I couldn't give them to anybody\" are equivalent morally; the only difference is that the latter is irreversible. There are good reasons to not want to be tracked, but there are also bad reasons to not want to be tracked. Exit node operators have chosen to help both. Police on the whole tend not to be the kinds of people who understand the \"good reasons not to want to be tracked\" thing. reply cesarb 20 hours agorootparent> \"I don't want to give you the papers\" and \"I can't give you the papers because I burned them so that I couldn't give them to anybody\" are equivalent morally; the only difference is that the latter is irreversible. There are other differences. One is after the fact, the other is a decision made before the fact; one is specific (rejecting that request in particular), the other is general (all requests of that type are guaranteed to be affected equally). It's the same with, for instance, email retention policies. We accept that old messages are irrevocably deleted after X days, even when we require them to be produced once requested if they still exist. reply gwd 20 hours agorootparent> It's the same with, for instance, email retention policies. Indeed it is. The intention and moral purpose of email deletion policies is to reduce the risk of embarrassing or incriminating emails being turned up as part of a lawsuit or investigation -- in other words, to be unhelpful. The legal justification for being unhelpful in both cases is that \"this is just policy, we're treating everyone the same\". That doesn't change the fact that in both cases the intent was to be unhelpful to investigators. reply gavindean90 13 hours agorootparentWhat if I just want to delete old information because it’s just noise now. My intent is to reduce my operational burden. I have long retention policies for things and life cycles for others. Information shouldn’t be permanently available to me if it’s not relevant or it’s a waste of resources. reply abc88889 19 hours agorootparentprevCould it not be that you don't want such emails exposed if you were hacked? Why does it have to be only law enforcement that you're hiding them from? reply Hizonner 22 hours agorootparentprevCourts, and even law enforcement, are actually smart enough to know that they have to enforce the laws as written and that they can't just act on their feels. At least most of the time. In many places. reply beaglesss 21 hours agorootparentThey can't say they're breaking the law but as long as they don't admit to wrongdoing they can accomplish the goal of picking up pretty much anyone for something.. It's best to assume the government is a hostile, rabid actor who will seize any reachable assets and your freedom at any point they wish and proceed accordingly. reply hnbad 10 hours agorootparentprevFWIW this is more of a concern if you expect the case to go to court. In the US most \"criminals\" accept plea bargains without a court ever seeing the case. So statistically you can likely \"act on your feels\" as long as the suspect does not think they can prove you wrong in a court of law (whether it's because they're guilty, because they don't fully understand the law or because they can't afford the time and money involved in a court process). reply Manuel_D 22 hours agorootparentprev> How do ISPs operate legally? I described exactly that in my second paragraph. reply Hizonner 22 hours agorootparentTor relay operators are, as a rule, entirely willing to give law enforcement all the information they have about connections that have gone through their relays. They simply don't have any. And there's no legal requirement for them to have any. ... or at least there never was in the past. The new wave of stupid and extremely broad \"duty of care\" laws that try to apply to the design of any and every communication service may change that. But it hasn't been litigated anywhere. reply Manuel_D 21 hours agorootparentWhether or not the exit node operators retained logs is besides the point. These exit nodes are facilitating illegal activities, and it's trivial to prove. How do they not get arrested? It sounds like Germany extends some sort of carrier protection to Tor exit node operators. E.g. if someone organizes a drug deal over the phone, Verizon is not liable. But Verizon does have to meet some minimum standards of records keeping and law enforcement assistance (wire tapping). reply lokar 21 hours agorootparentLots of people and organizations facilitate crime. That’s not generally the legal standard. They typically must be proved to done so intentionally (or with reckless disregard) reply numpad0 21 hours agorootparentNo, it is. There are more often specific laws that exempt platformers of liabilities on condition that they keep logs and cooperate with LE. Perhaps the most famous example is DMCA: [Google] is exempt from liabilities for hosting pirated movies on [YouTube] by US laws, on condition that it's not actively involved with it and fully robotic with takedowns. reply golergka 20 hours agorootparentIf a criminal rode on a bus to place of the crime, is the bus driver automatically liable? Bus company? Is his phone company liable because he talked about his crimes on the phone? reply numpad0 12 hours agorootparentIANAL, but \"legally\"? Bus companies has code of conduct posted on the wall at their depot for its users to read and agree, or state law regulating public transportation, and it always says using it for crimes is against the law. Those clauses let drivers and companies frame themselves as victims to escape prosecutions, unless there's going to be gross negligence or sorts that override them. It's not like courts treat popular businesses like buses and ISPs as sceneries just by gut feeling. There are always laws. reply betaby 17 hours agorootparentprevNo. But legal system treats 'on the Internet' in a more harshly way. reply codedokode 21 hours agorootparentprevDoes iMessage or WhatsApp has wire tapping feature? Are they \"facilitating illegal activities\"? reply Manuel_D 21 hours agorootparentIf a government investigator joins a WhatsApp channel where loads of people are sharing CSE, WhatsApp will help the government find the people responsible. WhatsApp encrypts the content of the data, but they retain message logs and do cooperate with law enforcement. Presumably the same for iMessage. This largely conforms with how the first telecoms received immunity for abuse of their services. They retain logs and assist the government with investigations, and in exchange they are shielded from liability. WhatsApp and iMessage would probably cooperate to the same extent, minus wire-tapping messages in transit (because they can't). That's vastly greater cooperation than a tor exit node operator that retains no logs.s reply bawolff 21 hours agorootparentprevThe original post mentioned facilitation, which from what i understand is when you assist comitting a crime but have no secific knowledge of the crime. I imagine for tor, the reason is that there are also good uses for tor. However i dont think \"i intentionally know nothing\" works as a defence in general. Ianal reply Hizonner 21 hours agorootparentI was answering something about assistance to law enforcement, which isn't the same issue as facilitation of crime. \"Facilitation\" as an offense in itself is one of those things that tends to be a real thing, but varies a lot depending on the jurisdiction. In most places, most of the time, you're only going to get in trouble for facilitating crime if your service is especially set up to be unusually useful for crime. You're especially vulnerable if you specifically designed it for crime. If those things apply, then knowing it's being used for crime (but not necessarily on which specific occasions) can make it worse for you. Give or take, depending on where you live. In the past, Tor nodes, even exit nodes, have mostly gotten a pass, at least in countries where most of them are located. They get raided all the time, but largely as cases of mistaken identity. That's probably because most Tor traffic has historically probably been people trying to hide from ad tracking or people worried about their perfectly legal activities being spied on. So it's hard to say the service is really aimed at illegal activity. Things are tightening up worldwide, in statute and probably in case law, mostly because of Tor and other services possibly being swept in by standards primarily aimed at social media. We may start seeing Tor nodes targeted because Tor is now considered \"too adapted to legal activity\", or even because node operators are \"not doing enough to prevent\" illegal activity (including redesigning the system if necessary). But until fairly recently that's been more what you'd expect to see in North Korea than what you'd expect to see in Germany (or the US). reply RobRivera 22 hours agorootparentprevWhat law mandates forced compliance outside subpoenas? reply Manuel_D 21 hours agorootparentCommunications act of 1934, among others: https://en.wikipedia.org/wiki/Communications_Act_of_1934#Wir... reply RobRivera 21 hours agorootparentBoth the communication acts of 34 and 96(?) Do not require software operators to legally do what LEO tells them to do without subpoena. reply Manuel_D 21 hours agorootparentThe question was about ISPs. reply RobRivera 43 minutes agorootparentYou are quite literally telling me what question I asked. How do you expect further fruitful dialogue? reply codexb 20 hours agoparentprevYou can make the same argument for developers of encryption. There are legitimate reasons for privacy. The fact that criminals want privacy, too, doesn't mean privacy should be illegal. reply mminer237 18 hours agorootparentThe difference is that with Tor you are physically downloading CSAM and forwarding it on to the offender. With encryption you're just providing tools for them to hide material. reply ponorin 22 hours agoparentprevExit node applies only to traffic that goes into a clearnet. You could to illegal stuff, but only tor users have protection and website owners are liable to raids should they allow illegal stuff to happen on their platforms. With Tor Hidden Service there's no exit node as such since traffic terminates inside the Tor network. The networking route is doubly anonymized so both the server and the client can't track each other down. reply Manuel_D 22 hours agorootparentPerhaps I'm not understanding something. I'm imagining this scenario: 1. Bob is running a Tor exit node. 2. Charlie is a government official investigating illegal content (use your imagination) 3. Charlie downloads illegal content via Tor 4. This content is sent to Charlie from Bob's exit node. 5. Charlie observes that Bob's exit node sent him illegal content. I understand that even if Bob is raided and his computer searched, they cannot find the website hosting the illegal content. But Charlie would know that Bob helped deliver the illegal content. Tor Hidden Service does not anonymize the exit node from the client. reply varenc 21 hours agorootparentYour mixing up general Tor use vs Tor hidden services. With hidden services there’s not really an exit node because the traffic never exits the Tor network. Charlie could only see the machine in the final step of requesting the illegal content it Charlie was hosting the hidden service themselves. These requests can come from many different Tor operators not just exit nodes. reply Manuel_D 21 hours agorootparentTo be clear, Bob is not the host of the illegal content. Bob is just the second-to-last hop before the content reaches the end destination (Charlie). My understanding of the tor network is that it obfuscates traffic across many hops. The path content takes from the host to Charlie: Host -> Node 1 -> Node 2 -> ... Bob -> Charlie this obfuscates the Host from Charlie. But Charlie knows that Bob sent him illegal content. Yes, Bob didn't host the content. The host is obfuscated. But Bob is still delivering illegal content and Charlie knows it. reply aniviacat 21 hours agorootparentExit nodes are not the nodes that are directly facing tor users. Those nodes are called \"Guard Relays\". Guard Relays usually don't have these issues, since you have to be somewhat technical to actively probe relays by requesting content through tor. And technical people know there isn't any point to rading an operator's home. reply fn-mote 18 hours agorootparentprev> Bob is still delivering illegal content and Charlie knows it Does BOB know they are delivering illegal content? No... is it even possible to send unencrypted traffic by Tor? If it's even possible, Charlie must be the only person in the world doing it. reply Manuel_D 16 hours agorootparent> Does BOB know they are delivering illegal content? He does when Charlie knocks on his door and informs him that he delivered CSE to him. Ignorance of the fact that one is breaking the law is rarely accepted as a defense. Carriers usually get this protection when when meet some standards of safeguards and cooperation with law enforcement. reply Zak 15 hours agorootparentIgnorance of the law is not generally accepted as a legal defense, but ignorance of facts is. Most crimes involve a mental state of knowledge or intent with respect to the wrongdoing, and an exit node operator does not know what users are accessing. Taking the wrong jacket by mistake is not theft, and operating the exit node through which someone downloads CSAM is not criminal possession of CSAM or knowing facilitation thereof. reply Manuel_D 13 hours agorootparentDo you think drug mules get off scot-free when they say \"I didn't know what was in that package\"? reply Zak 13 hours agorootparentIf the prosecutor can't convince the jury that they did know, yes. That rarely happens in practice because prosecutors are usually pretty good at their jobs, and tend not to bring cases they can't prove. reply Manuel_D 2 hours agorootparentThe prosecutor doesn't need to definitively prove that the mule knew he was transporting drugs. Only that a reasonable person should have known. Back to our Tor example: if you've been repeatedly told by the government that your node is being used for illegal activity, it's hard to plead ignorance. reply Vecr 21 hours agorootparentprevHidden service connections don't go through exit nodes. In theory it's two back-to-back Tor connection that meet somewhere in the network, but you can also think of it (possibly more correctly) as a six-hop Tor connection to an exit node that is only used to directly connect to the backend server. If set up right this prevents government sniffing at all points. reply Manuel_D 21 hours agorootparentThe final recipient is going to be able to decrypt the content, right? Regardless of \"hidden service connection\" or \"exit nodes\". Charlie is the final recipient and will be able to decrypt the content and know that it's illegal content. Is there some mechanism that prevents Charlie from knowing who sent the content to him? Fundamentally, you can't stop the government from sniffing at the endpoint. Because they're not really \"sniffing\" they're just requesting content like any normal Tor user. reply Hizonner 21 hours agorootparent> Is there some mechanism that prevents Charlie from knowing who sent the content to him? That is, in fact, the whole point of Tor. In the hidden service case, neither end can identify the other. reply Manuel_D 21 hours agorootparentSorry, in case I wasn't clear, I'm not talking about identifying the site hosting the content. I'm talking about the second-to-last hop in the traffic. My understanding is that Tor obfuscates traffic by sending through several hops, each one decrypting a layer of traffic (hence the \"onion\" network). So we have: Host -> Node 1 -> Node 2 -> .... -> Bob -> Charlie. Charlie doesn't know where the Host is. But Charlie does know that Bob sent him illegal content. Or is that final link, from Bob to Charlie, also obfuscated somehow? If so, how did OP get raided by police if he's supposed to be hidden? reply Hizonner 20 hours agorootparentOK, so there are basically three cases: 1. Charlie is running a client and downloads something. In which case Bob is an entrance node, not an exit node, but it's essentially the same thing. Charlie does know that the next hop is Bob. Depending on whether the ultimate destination is a hidden service or on the clearnet, Charlie may or may not know who's running that service. 2. Charlie is running a hidden service, and somebody uploads something. Charlie knows that it came via Bob, but doesn't know where it came from. 3. Charlie is running a regular clearnet Web server, and somebody uploads something to Charlie via Bob's exit node. Again Charlie sees that the traffic comes from Bob. In the first two cases, Charlie has to be actually running the Tor software, and knowingly using Tor. So Charlie also knows that (a) Bob is just a relay, (b) Bob doesn't actually host the content, (c) Bob doesn't handle more than a packet or two of the content at a time, and deletes those as soon as they've been relayed, (d) Bob doesn't know, and can't find out, what the content actually is, (e) Bob doesn't know, and can't find out, where the content originally came from, and (f) Bob is really unlikely to keep any record of the whole connection after the session is over, which means probably no more than 10 minutes or so. If that's enough to go after Bob, then it's enough to go after Bob... but historically it hasn't been. Bob can reasonably claim not only that he doesn't know what that particular traffic was, but that, although he knows there's probably some illegal traffic, most of the traffic he relays is probably legal. In the third case, it looks to Charlie like Bob is the ultimate user. Unless Charlie does some investigation, Charlie may go raid Bob. But Charlie should then find out all that other stuff. I think the most common actual case is that Charlie is running a honey pot, either as a hidden service or on the clearnet, and somebody gets the content from Charlie via Bob. But the same basic ideas apply. The main issue isn't that Charlie doesn't know what the content is, but that Bob doesn't. [Oh, and on edit, just to be clear: In the first two cases, that \"packet or two\" that Bob may ephemerally buffer is encrypted so that Bob can't read it, nor can any other relay. In the third case, where Charlie is a clearnet service, the end user is usually still using TLS, so Bob still can't read it. And none of the non-exit relays can read it no matter what.] reply Manuel_D 20 hours agorootparent> So Charlie also knows that (a) Bob is just a relay, (b) Bob doesn't actually host the content, (c) Bob doesn't handle more than a packet or two of the content at a time, and deletes those as soon as they've been relayed, (d) Bob doesn't know, and can't find out, what the content actually is, (e) Bob doesn't know, and can't find out, where the content originally came from, and (f) Bob is really unlikely to keep any record of the whole connection after the session is over, which means probably no more than 10 minutes or so.? But at the end of the day Charlie, the government agent, is catching Bob in the act of delivering illegal content. Imagine a government agent buys drugs on the dark web and arrests the courier. The courier protests, \"I didn't know it was drugs, I didn't ask what was in the package\". Do you think that defense is going to keep the courier out of prison? It sounds like Germany is treating Tor operators as common carriers, and not holding them liable for content they delivery. They're being quite generous in that regard, in most countries the node operators are probably not met with such leniency. reply Hizonner 20 hours agorootparent> Do you think that defense is going to keep the courier out of prison? Yes. That happens every day. > It sounds like Germany is treating Tor operators as common carriers, That's probably because they basically are common carriers. And the service isn't particularly designed for illegal activity, even it can be useful for that. It's especially not designed for activities that tend to be illegal in the \"free world\". > in most countries the node operators are probably not met with such leniency. The Tor network has been running for about 20 years. There are on the order of thousands of relays. Unlike users, relay operators aren't anonymous; there's a public list of their IP addresses. The relays are all over most of Europe, especially Western Europe, and the Americas, especially the US and Canada, with a not-insignificant number of them in other countries. So far as I know, nobody's ever been arrested, let alone convicted, for running a Tor relay. If they have, it's been in the sort of country where",
    "originSummary": [
      "On August 16th, 2024, German police raided the home and office of Artikel 5 e.V., aiming to de-anonymize Tor users, but seized no hardware.",
      "Artikel 5 e.V. plans to legally challenge the search warrant to prevent future raids and is calling for a general assembly on September 21st, 2024, to discuss the organization's future.",
      "The assembly will consider options such as finding new board members, stopping exit nodes, or liquidating the organization, with details available on their website."
    ],
    "commentSummary": [
      "A Tor Project forum user shared their experience of running Tor exit nodes for five years, during which their hosting provider received three subpoenas from law enforcement.",
      "The subpoenas were related to serious incidents, including a bomb threat, phishing email, and nation-state hackers from Qatar, leading the user to shut down their exit nodes due to stress over potential legal consequences.",
      "The discussion highlighted the ethical implications and the balance between privacy and crime prevention, with hopes to resume operations in the future despite challenges from law enforcement."
    ],
    "points": 485,
    "commentCount": 287,
    "retryCount": 0,
    "time": 1725999169
  },
  {
    "id": 41507879,
    "title": "The magic of DC-DC voltage conversion (2023)",
    "originLink": "https://lcamtuf.substack.com/p/the-magic-of-dc-dc-voltage-conversion",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.theme-dark{background-color:#222;color:#d9d9d9}body.theme-dark a{color:#fff}body.theme-dark a:hover{color:#ee730a;text-decoration:underline}body.theme-dark .lds-ring div{border-color:#999 transparent transparent}body.theme-dark .font-red{color:#b20f03}body.theme-dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.theme-dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.theme-dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.theme-light{background-color:#fff;color:#313131}body.theme-light a{color:#0051c3}body.theme-light a:hover{color:#ee730a;text-decoration:underline}body.theme-light .lds-ring div{border-color:#595959 transparent transparent}body.theme-light .font-red{color:#fc574a}body.theme-light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.theme-light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.theme-light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.theme-light.feedback-report{border:1px solid #959595}body.feedback-report{border-radius:5px}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem;padding-right:1.5rem;width:100%}.main-content .spacer{margin:2rem 0}.main-content .loading-spinner{height:76.391px}.feedback-content{align-content:space-between;display:inline-grid;height:100vh;margin:0;padding:0}.feedback-content .spacer{margin:0}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"lcamtuf.substack.com\",cType: 'non-interactive',cNounce: '34015',cRay: '8c19e85dad260ca8',cHash: '81dd8fa845a60c2',cUPMDTk: \"\\/p\\/the-magic-of-dc-dc-voltage-conversion?__cf_chl_tk=irOA8LZoxsDvIZp4Vml4NbBFOAh1FpjCxWVvUaSQDfc-1726081300-0.0.1.1-4137\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '120000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/the-magic-of-dc-dc-voltage-conversion?__cf_chl_f_tk=irOA8LZoxsDvIZp4Vml4NbBFOAh1FpjCxWVvUaSQDfc-1726081300-0.0.1.1-4137\",md: \"g0SMyJIcwi2iNLWiexGv_D3zB9SnklPUqUUjkUQiDkw-1726081300-1.1.1.1-Pp9AXErt3vY036isTLPUL_F2jjwxdY8uqUl.jXR1m47RBagmc09CuGGxqqzKLzbhPtDZADOGsC_pN24Lr15Z8HXiJ_4GLyrCiXHpr_ZeqjkIakzAr3DU8DpXrgIuw4xiHiShYoCb8HxByK6OWgYlEx_U7VQTWouJxvcGtUW.0tOCLghHO3lwtg3j8QhCOOFb16OsxM0QdqYjhUx7h8n9YN7QQOEejN8xmrYGKgmJA9eOocoeDLUteFDoKXXLxUQmbUsmiBrY2v4eTSKZYeagSpJjQoJnkqR2_xd6WplfZiCfCnI.9m8cyVYjX0PuSIcFXYgcEH85TThh8bCgyK963Wjh1abAZ_AZPTQ5aR4kG.gNsc6Fs6Mu60ajocZQf_8aM4YThtw57zLNXG3vrl451.X70249Nayx9oy60AKpZWsfneDqu35aVKXhvLadoEeA7OE92Hd7S1qoTtlOUunkQbrTpzJar6ehJFdoisWpiAOF_Sr_vqVVINHD4JoXecMrZ0KXtg3zGlgmlSYFKr5BPZ3P5R5CyZPY17DBR6rRy0Pyp81eEWgB67SexFrnBuzGjTR9__L4cuV00E10PEn6JNIytA3NO37f9_ij5Txvmwm2Qu.OdYhHZXPdbXNorXlh4k37QPUA1kIQiqKvoSiydVirMA.iDyAQyf2T724vKmuvSdQjHd6qOZubN66gCiDkq55q94uN.od2eN9NDPrGvDjFQWZmhnr8JAN2vlnbXXShLVUhtAlDU4dwZaQ59SRw4xrjpyFaUSo_hOBlRbgQJ1P6xOTbfj2oOF06_A9pTvHhkLxvK7vY2ztu95xUjBOuAwiI.s40lROBUWhf3w7Iw5WYygWktY8iuIXoCtEJcTZNZaoKKyM.zqd58GQkHvlD1rOLvY3.aZoYh_zItzWSIBVTeVEdhoRCtO7rchGhmczIzs5mJoq2gpVctK0fHTnv3slzSqEe5AxQmEGjMC8uYeDJKKeNoJTlke6fL7ZxXVpbTu2Vri1N5XejOj8JjTDcFhWsXca9JyHQvgz6ULyC4uQ0vJEJ5FdrAxHl72Somn8TgmzUkX0dT4IHaNmRu1b.DAvxrkC5lKoJJqNIBce8uJWSDvgwgx.bMEj2.ES5uLLfW0jrfQyE3B4jKHweD37hD8UQUS5dsJ.kc9D.jDrJ4pugJifxklCwCl_dxueYRJcLR8zrejyxVsvQejzMiLZwz2rTQQvU5ENZfMjj04Xwchqpv6_wOR9gc5olwwJPD7lRseNEZJOzUefQQwk2D5dsDPcNbTzzxUAIDZ.4gU84_MhLgg3WKFTzGvw5AA3URcmAMlhelSAwGpfEgAu4vMNNZC0q2IBIJaSF8sGeh2N0ow6SXJnPD2n1wQx9xdzrZMz0D7fjTvRjnpY.oF0UXbEjeSINqJ8asnXhwPTc6V2ZfOBjrdxOzAJIGA5Ure.j5irQIAm4.YzYRP1FKImIwklhFoQr9imJV7RSMbWCyJHejqW2gkiC3HZ2B8FnCr2fqSCXOHsuvTrhTYuaWlZsaEw24KGBqH6fXS7jBJmEAZUNavdUtcgD9VXlrUK.u1jKiLtyFhe4uw2ppf7FlJ9r8ZbjkNtQKMbeyxEFV0sAn9mw.LTHk48XkVrmzipK7FeRmzI3ezAAhVywyjciwh9sTmcX\",mdrd: \"F.D2LVRrqkbFaIeeR9VZvbfKcFU6wZIF2YsdyzRwP14-1726081300-1.1.1.1-BstmHeUjc0OtsxFYa9GW.jWNlNvJG79j7ERCgo0igpzvTdOSL7RRN706q_KbKjovjzv5nQkw6kr9Px7L_Tz.sO1naJQRbLXFkkZruKtLy4.6J0PXyZOp4LYKDBQCJIfI4CIx3hR5aZ9OhjPXwi9e9zlH_SBLp0wpacW2RouW4T.Lu4OrrIxvYCgrUOVZ05zZqOEqtQAJI_2xQnTYjEl_3wDEol6AAdntRmJYaZ0i29Fbiit8FqwufZRlas4t4gBBS5B5WabhOhlMPn2.WqvvE9kYxvVbqS_4JOB7_SDy_gBs3ZBTE2CsNT4ryffjarmS66kbKXt8zcoesaAHw9M3SDFP4V_.lG1p9O_hctpz0GMVBeF9WE9vpS8XDoPjOkb.s7MPITh.mFzyWmMVToii85yc0hHwtFLlKrR527nh7euBcHaNaZO5PCqSh66Vd81tQtJVdk2t67ZRmOjeqGzLs.Tp.0sdxQglR4aezoGQERTJEAZfiWa3WOGQPNMAXXkNoJYmNlRescbMT07s6BqDEXq3fudlXvdVJbvRMhR6.fjIUrf0ZvJpUErLDMemEFfDcQeHGiw.7LOVWDH1lgu0TKJFHtt4KAQwRNOkSmNZ39cPt_HfczDoKo97gC___pCz03tjhKaMphteVkbQ3I22aJbhWxVn4Rs1kQa03OX1ica8RQfd4d48teNnGzFhefkO51RkaC_JhkLuvhkx.R6i67k0fc0qzsccaSW7E_RHxGxS1UavVpwRWtgCTX_oXo3MzjsgP0StaQaJSl5qyVZqhOyxIRauqNlqw6_uW6UyMtSI4shUym4iCI13RpCtWoRcazJp10egjpTo._OvLRKawpH4ITYVxOIKY1mjsjra_zY9U2V7x.oMKk2lumaX18e0X36wrAfEHD81lNU6H3TMikyN.FkYAI9m0EqJIN0aNjdM9aJdHjJBVR7GrplE7Iq8EJcSoNjEiRXRPBAKABpdU4E9iFdNNXORb_Bp27Y5VYdUKC0mp4.OoziENXcIeuqE88FAjVUy3_jcaRGkqj2y_rkyKcjMRSDslN9YFI.d_ufAFtWema_oHhuYn.nI.6VU0hjWRgLrwq7d6gfNsbRIWAuKJLWBSGsF7.xO4N02QQzSmME31nDiAwcrDFV47MRMs.aCZk2e15OnR9FZNZm0tf0t__CSCHOWPEuBHVYCFbvVmsYt3ws6tjQUe6HU3bXXSbbkxF.Ly9d7RzAZ1AaPQdx0r.fDaSJpOZHuzng2NeGGpxZMdzl8epTOoDcXpHytVzDa8iw_xwSx452ClTKEsodWoBONTdrupN5fqDXiVnOip56pLvi6afObpbSElYS3gMyVHPPIz8h7mTdHA6QAnzm5aa_IB1TqSUuDLXrbdr2lejg4jvV7M0rv9Ar.KOvYtk88RLy7XML0dEbtO0stRB5qxMdKulpkY9j9zFfc93Tvaml7kFOkK70vbWtWBQkfig3iP4hyr.T4svUU.IpOiEU64c0v2qyAMXoa1XGquyw0EgNg9E7wtCJWE4nzkTjumpufZDqxFbyr_dNRUPe2mXwAgtJYud9.SMI8VGsHUJE4EYF3bB7HAWKoYCOP1zWzwcGbjJtfobFFu1kDKTLg.7t.2j.dK7AAW.jxcU1UBSfGs_wDdsUEJ6MXWRlbQs.8sjVQ3NKY_ACyhDTYKfNJ5BEPIEV8HBBVpQWb1LfO2EdBApUdrjgqkwDUaTCuuS0BZVpFG3J6IWtj2UaRa3NXUqkUBFf0Tlr4.1ESoFzmBwVihP995W2J8zib3JcIAFiqtRiJqB7ZU3EueZL5GgwV5_eLbNJqusDqQsE3is8a7Fd.4Ix3RGSMurIYRAZdxkO3Zg18pHYELNN4TWjG5k1RM.z0Qv20cJtHLD1LTWo10krw1hpjNgDlofF0Jsa7iTPft2zde0LgrhgY3sjwv2eNMk2SI0qgDosLPvsoxtLXc0FSi9UtKE3Z4j3XNBaEkNPVBdBhvErrMWX9rfPATTCy_zzxaGyU7M4n2QaIuFsihqG9RuDEvYa8mLN36MCHsgq7nbu5wO.ZX6HMwxnn2Om074uOPjwLquZaIaeaGzRUThFD5vOkDOeX1qYgYo5K06S2dzGKKVnUSWWiQbcqtx4n9PO4szbMwQnCfUW1Y9K5oJJtE_WCdVC3rgY6A1ABxY4xZVwr0eMR1RQD32lTXCLt0scPNiAUFQU1LWgTfoiASkaY4IJ9WWviDLAlABrXuZdG.9AG.fSkumgzLerWnp2zuFy8TYNhQjkDnhoJ62O0V80ubayvdOiHKta_XFGX8rxMwgJBwhplgiNFn_vLmXf_Kp5Mh_NanAJ22H4yCv54Z7g\",cRq: {ru: 'aHR0cHM6Ly9sY2FtdHVmLnN1YnN0YWNrLmNvbS9wL3RoZS1tYWdpYy1vZi1kYy1kYy12b2x0YWdlLWNvbnZlcnNpb24=',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: '0xx6va7mSCTtAIENRGOZ8Blig6A0P1xS67F3J+30JedsIPsTUjvV+zOSjOSAVG/U8R1JzIsl3ftsnG8Buk3atsz9GQ0DLps3y0OJCE5RUX3es4t7xw8FAn4CR5J4qtKGDAwrpPit3UtJpJ5kyNNFiG1tvcpjn+PefVJN7F8Jue/xZe47spAhrQHx0NUTev5PczR28s1ShR3khiBCd5N4LQZbtzhgiOfgaeex6Rqr7msw1csF20+YF0kShPrHRJb+RwJ5bmAvs5X+pTuKYOVHRchIF/4p3yvbmBImd7TXJ8aUDCe7hks0KLqeng7QK8QeXCNxzNqaKW4f4nqTSJbt2btGXgnKlebvkZzZujFSDq53SjfkEC08lwcSRKqJYrItC/ANiS2PtDuCciK7sJYPngF5lRppV+c7iGvOGg0Ov/6KT1IDD7w8jw/RMLawu2xLei5B8hGa6lyQWEa/Q5NAZUW6j7sAuIQmmxY/MFHNPMgzYhopS4XVpN1WOsFL8PqGrOKiAUBAfw1hFn1702WsEOkCUDjgu6Aeq53dJuU2jntANfORDezk4QcuIoawP/ykp/MysDTl4dahk7KfeuoivQ==',t: 'MTcyNjA4MTMwMC4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'vCIDGLg3rUnCPe9pYL0T6zsawkZ0Uosze54oKBjI414=',i1: 'S3F3wJNw/2i65+k9nZxOnA==',i2: 'l/cDExS4Tz6Bevv72W08Qw==',zh: 'o01jypKJQ++/gkxUTvC40nYpXBhuMc66cm0hd/Tc920=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'xqzCC+GW0ZJOpXryvIzY5YDV2FoqixzmNcp4OTLW0co=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=8c19e85dad260ca8';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/the-magic-of-dc-dc-voltage-conversion?__cf_chl_rt_tk=irOA8LZoxsDvIZp4Vml4NbBFOAh1FpjCxWVvUaSQDfc-1726081300-0.0.1.1-4137\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41507879",
    "commentBody": "The magic of DC-DC voltage conversion (2023) (lcamtuf.substack.com)322 points by _Microft 15 hours agohidepastfavorite94 comments Animats 12 hours agoDC-DC converters are hard, but fun. The basic concept is that when you put current through an inductor for a while, then disconnect it, you get a big voltage spike. That's a classic auto ignition system. You can put that spike through a diode and use it to charge a capacitor to get DC out. The neat thing about switching power supplies is that there's very little resistance in the power path. That's why the efficiencies are so good. The not-neat thing is that they are a dead short across the input for part of the cycle, which is why failures can cause fires and why you may need an inrush current limiter and/or a fuse. There are boost converters, buck converters, and ones with transformers. With a transformer you can isolate the input from the output, which is mandatory for safety if you're driving the thing from the AC power line. Here's one of mine. USB 5VDC in, 120 VDC out, to operate antique teletype machines that need 60mA 120VDC.[1] The basic circuit is simple, but there are multiple surface mount ferrite beads and small capacitors to keep the spikes from coming out via the input USB, output, or as RF. LTspice simulation was needed to pick the values for those, so as to minimize noise in both voltage and current. [1] https://github.com/John-Nagle/ttyloopdriver/blob/master/boar... reply petertodd 8 hours agoparent> The basic concept is that when you put current through an inductor for a while, then disconnect it, you get a big voltage spike. That's actually usually not true, as the vast majority of DC to DC converters are step-down converters: you do not want the voltage to spike. And in general, it isn't really a \"spike\". A better way to think about what is happening is that passing a current from a power supply through an inductor transfers energy into the magnetic field. When you stop doing that, the magnetic field diminishes, transferring energy back into current. But this time, you direct the current into the circuit. The trick is that by picking the timing and other parameters correctly, you can pick the voltage of the downstream current. Specifically, you can do this because the voltage across the inductor is a function of the slope of the strength of the magnetic field around the wire in the inductor. Pick a different slope, and you can pick a different voltage. Since you usually want a stable voltage, the graph of the magnetic field strength will be (roughly) a sawtooth, and the graph of the induced voltage will be (roughly) a square wave (I am simplifying here for understandability!). A sawtooth shape has a consistent current slope, which leads to a consistent voltage. reply Terretta 6 hours agorootparentDISCLAIMER: Described for entertainment value only. Some details omitted. Don't try this at home! That energy transfer makes an “interesting” party trick. Get the step up/down winding transformer from an old CRT TV. Get rid of other components*, and wire it with a 9 volt battery on one side, and connect the other with + to conducting surface on three sides of a box with - to the three opposing sides. Put a switch on the underside that opens the circuit. To pick up a box generally requires touching two opposite sides. Opening the circuit dumps the field into the person picking it up who gets a momentary jolt. It's enough to run through multiple people: hold hands in a ring of 2 - 10 people, and have two people at ends of the ring each press an opposite side of the box and pick it up, the whole ring gets the jolt! As a grade school science experiment, have the experiment display say something along the lines of \"Guess the weight\" so people pick up the box and get a surprise. For more about retro transformer circuits, see: https://hackaday.com/2016/07/04/retrotechtacular-dc-to-dc-co... This is sort of a single vibe (the switch opening) of a vibrator-transformer-rectifier transformer, to collapse the magnetic field that dumps into the still \"closed\" side through the person picking it up. No rectifier since it's not AC, it's just C. So the same principle, without the rest of the parts. * WARNING: Don't look up the rest of the owl. Don't build this. Don't try this. Don't let anyone touch this. reply PopAlongKid 6 hours agorootparentWay, way back, when I was in fifth grade, my dad (who was part owner of a car repair shop) brought an ignition coil (the old kind, that was connected to a distributor for the spark plugs) into the classroom, and I guess a 12-volt car battery. All 25 of us students held hands in a large circle and got the jolt. this was part of the teacher's ongoing study of electricity, which also involved winding wire around a hollow cardboard cylinder to make a magnetizer/de-magnetizer tube. reply Terretta 6 hours agorootparentYes! And same age when my dad taught me this. (Wasn't it great learning in an age before cars had seatbelts, before push mowers had kill bars, and when nothing had warning labels?) I was mostly tongue in cheek about the danger above, as the most dangerous step would be relieving a previously functional CRT of the transformer block. The CRT discharge can kill you. Using an ignition coil should work (I didn't try it) and is likely safer to source if you're getting it from something assembled instead of from a used parts bin. As for the rest of the owl, this is from memory, nearly half a century ago, so, yeah, disclaimers: --- # How to Build a Prank Shock Box for a Science Exhibit This fun project will surprise your friends with a harmless electric shock when they pick up a prank box to guess its weight. Here’s how you can build it and how it works. ## Materials: - 9-volt battery - Step-up transformer (designed to increase voltage) \\_ consider a flyback transformer from old CRT or auto ignition coil, talk to circuit electrician expert - Switch (spring-loaded or pressure-based) - Wires - Small box (to hold the circuit) - Electrical tape - Conductive foil or metal strips for accessible sides of box ## How It Works: This circuit uses a step-up transformer coil to generate a small electric shock when someone picks up the box. While transformers typically work with alternating current (AC), here you use direct current (DC) from the 9-volt battery. The trick happens when the circuit opens as the box is lifted, causing the transformer’s magnetic field to collapse and induce a voltage spike. When the box is lifted, the switch opens, cutting off the current from the battery. This sudden interruption collapses the transformer’s magnetic field, generating a quick, harmless jolt. ## Steps to Build: 1. Assemble the Circuit: - Connect the 9-volt battery to the primary side of the transformer, with a switch in between. The switch should stay closed when the box is at rest and open when it’s picked up. - Wire the secondary side of the transformer to two sets of exposed contact points on the outside of the box: one set connected to the positive side and the other set to the negative side of the transformer. 2. Add Conductive Surfaces: - To make it more effective, cover three sides or faces of the box with conductive material (like aluminum foil or metal strips) connected to the positive output of the transformer. Then cover the opposite three sides with conductive material connected to the negative output of the transformer. - When someone picks up the box, their hands will naturally touch both a positive and negative side, allowing the shock to pass through them. 3. Install the Switch: - Position the switch on the underside of the box so that it opens when the box is lifted. You can use a spring-loaded or pressure-based switch that triggers when the box is moved. 4. Test the Circuit: - With the box resting, the current will flow through the transformer, building up a magnetic field. Once someone lifts the box, the circuit breaks, causing the field to collapse and induce the shock. 5. Secure the Box: - Place and affix all the components securely inside the box, bringing your two wires through the sides and making sure the exposed contact points are positioned on opposite sides of the box. Tape down any loose wires. ## Science Explanation: This project uses Faraday’s Law of Induction, which states that a changing magnetic field induces voltage. The transformer converts the collapsing magnetic field into a brief, high-voltage spike, delivering a small shock to whatever is completing the high side circuit when the low side circuit is opened. Although transformers usually work with AC, you’re using the moment when the DC current stops to mimic that effect. https://en.wikipedia.org/wiki/Faraday%27s_law_of_induction The conductive material on the box ensures that when someone lifts the box, their hands make contact with both the positive and negative sides, completing the circuit for the jolt. ## Safety Note: When done correctly, this project delivers a tiny, harmless jolt, similar to static electricity. Always use low power, an appropriate transformer, and avoid using higher voltages or currents. Consult with a TV repair expert or similar on your design before starting. DO NOT TOUCH ASSEMBLED CRTs. Let the TV repair person do it. She'll have parts anyway. reply sfilmeyer 4 hours agorootparentprevI initially misread this as you proposing using a car battery rather than a 9 volt battery, which sounds like a much less fun party trick. reply kosma 6 hours agoparentprevThat voltage spike only applies to flyback converter. Your typical buck/boost converter doesn't do that - the current waveform is a sawtooth, and voltage ripple is designed to be in the mV range. reply retrac 3 hours agoparentprevI think of it as synthesizing a sine wave (AC power) with DC pulses, using an inductor or capacitor for smoothing. The result is then rectified back to DC. reply amelius 9 hours agoparentprevI see you made a current limiter from a mosfet + resistor. I wonder if there are ready-made components that do the same, and also monitor overheating. Maybe not necessary in this case (because you're only limiting the inrush current, not a continuous current). There are current-limiting diodes but as far as I've seen they are only available for smaller currents. reply michaelt 8 hours agorootparentYou can get single-chip current limiters for LED driver applications. A CL2N8-G for example. In some applications you can also use almost any linear voltage regulator - put a resistor between your linear regulator's ground and output pins, and you'll get a constant current. Of course if your application involves the amount of power dissipation that requires a heatsink, you'll probably end up with a discrete component for that anyway :) reply amelius 6 hours agorootparentLet's say I have a voltage source of 48V, and I want to limit current in my system to 4.5A, precisely, and with overheating protection. I could be wrong but I don't think the led-driver and voltage regulator solutions would fall in this range. Also, a heatsink would not be required if the duration in which the current needs limiting is small. reply rkagerer 11 hours agoparentprevCan't you also charge up capacitors then slam them together in series? Is there a name for that kind of supply? reply hakonjdjohnsen 11 hours agorootparentThis is known as a charge pump, and is the third concept described in the linked article. The article only mention one flying capacitor, but you can use more than one and connect them in series to get a higher multiple of the input voltage. Ben Eater also did a nice introduction to charge pumps by building a simple one on a breadboard: https://www.youtube.com/watch?v=4alV5LzHLE4&t=704s reply caf 11 hours agorootparentprevYes, it's called a charge pump. There's one specific sub-type called a Cockroft-Walton voltage amplifier. reply pfdietz 3 hours agorootparentThey won the Nobel prize using this invention (which wasn't theirs). https://circuitcellar.com/resources/quickbits/cockcroft-walt... Cockcroft went on to great acclaim for \"Cockcroft's Folly\". https://www.bbc.com/news/uk-england-cumbria-29803990 reply mindslight 4 hours agoparentprevDC-DC converters are not a \"dead short across the input for part of the cycle\" in normal operation - rather the voltage is across the inductor. If the switch stays on too long and the inductor reaches its saturation current, or one of the many other (cascading) failure modes, then can you end up with effectively a short across the input. This can happen to many kinds of electronics (eg a simple tantalum decoupling cap, or an IC's SCR latchup), but designing the power topology is a good place to think about these failure modes. (Although going 5V->120V with USB as the power source, I can understand how \"dead short\" was a decent intuition) reply 3dGrabber 11 hours agoprevThere exists an interesting connection between Boost Converters and Hydraulic Rams [1]. A Hydraulic Ram is device that can pump water from a stream to a higher location by harnessing the kinetic energy of the stream, no other power source required. The equations for the two devices are essentially the same, only the units change. 1 https://en.wikipedia.org/wiki/Hydraulic_ram reply wrycoder 39 minutes agoparentCurrent is analogous to momentum, because electron drift has net momentum. reply agumonkey 9 hours agoparentprevI love analogies between fields like this. reply SoftTalker 3 hours agorootparentWater flows in pipes, valves, etc. concepts transfer to a lot of basic electrical circuits and concepts. E.g. voltage is analogous to pressure. Current is analogous to the volume of water flowing. Bigger pipe (wire) can carry more current. Valves are like switches or resistors. It works to de-mystify concepts for kids who have no concept of what electricity is but can think about water flowing in a pipe. reply agumonkey 2 hours agorootparentThe base analogy working is cool but that other mechanisms on top also work similarly is what amazes me. reply nraynaud 7 hours agorootparentprevThere is a whole area of multi-domain simulation, where the simulator seamlessly jumps from one form of energy to another as long as the units match. I have always loved that. reply 3dGrabber 3 hours agorootparentModelica comes to mind. https://media.springernature.com/lw685/springer-static/image... https://en.wikipedia.org/wiki/Modelica reply agumonkey 4 hours agorootparentprevoh nice reply dgacmu 12 minutes agoprevI've moved a lot of my home computing to home-brewed 12V UPSes using these. LFP charger --> Battery --> 12V or 5V DC-DC buck or boost/buck regulator --> device. Most UPSes are designed for high wattage, short runtime, but things like my firewall or small proxmox box for SDN+DNS benefit from low-wattage, long-runtime, and getting the inverter out of the picture substantially improves runtime. Said proxmox box uses under 10W and gets about 20h of runtime from a $50 battery. reply progbits 12 hours agoprevI can highly recommend the MIT 6.622 Power Electronics course recently released on OCW: https://youtube.com/playlist?list=PLUl4u3cNGP62UTc77mJoubhDE... https://ocw.mit.edu/courses/6-622-power-electronics-spring-2... Prof. David Perreault is excellent. While the course gets into pretty advanced topics that simply won't matter unless you are designing multi-kW systems, it covers all the fundamentals and builds understanding from ground up so you will know what makes sense to use and when. reply tecleandor 7 hours agoparentWhat's the starting level? My electronics knowledge is very basic and I'd like to start \"designing\" some simple power circuits. I know the basics of resistors, diodes, capacitors, transistors... And I could explain the most simple classic power supply: transformer, full wave rectification, capacitors and so on. I've built basic digital circuits (LDO + arduino/ESP + leds and stuff) and know some basic physics. I'm good soldering, though :D reply EricE 1 hour agorootparentCheckout bigclivedotcom on youtube - he reverse engineers circuits all the time in entertaining and accessible ways; a great way to learn through practical applications. reply kleiba 12 hours agoparentprevThank you! I was just going to ask about recommended resources for getting into electronics. I've never been able to find anything that I personally found useful - often times, introductury courses are too basic and slow to keep me focused, or they lack exercises or are too theoretical, etc. There are many hobbyists who have learned all that stuff and can design and implement their own circuits (say, audiophiles or model train enthusiasts), so obviously they have all been able to get there. But I have never managed to learn anything about electronics, although I would really like to. reply tzs 3 hours agorootparentThe MITx version of MIT's 6.002, \"Circuits and Electronics\", is excellent. Its on MIT's OpenCourseWare [1], and on EdX where a session is starting today [2]. The EdX is divided into three parts, and that is part 1. Here are parts 2 [3] and 3 [4]. Caveat: when I took it at EdX the textbook was available online for free during the course, and it is an excellent textbook that I found very useful. That was back when all the MOOC platforms weren't too worried about monetization. Now the textbook is only free online for people enrolled in the \"verified certificate track\" of the course, which is $189. The book is $64.97 for the paperback or DRM-free PDF [5]. I'm not sure how well the course works without the book. [1] https://ocw.mit.edu/courses/6-002-circuits-and-electronics-s... [2] https://www.edx.org/learn/circuits/massachusetts-institute-o... [3] https://www.edx.org/learn/circuits/massachusetts-institute-o... [4] https://www.edx.org/learn/electronics/massachusetts-institut... [5] https://shop.elsevier.com/books/foundations-of-analog-and-di... reply progbits 11 hours agorootparentprevYeah it can be hard. As a self-taught hobbyist I've found a mix of university courses (not whole curriculum, just pick and choose and don't feel bad fast-forwarding over some of the math theory), books (art of electronics, practical electronics for inventors), and high quality youtube channels (eevblog, phil's lab, robert feranec, microtype engineering) to be a good way to learn. Also eevblog forums are great. I don't post much but just reading through the discussions you get a lot. My greatest annoyance is the flood of very low quality Arduino tutorials everywhere that polute the search results. Not to be ungrateful, Arduino got me into the hobby, but if you just learned about resistors last week the world doesn't need your blogpost on how to connect it to a breadboard. reply Max-q 11 hours agorootparentYou probably know about this already, buy in case you missed it: The Arts of Electronics is a wonderful book on electronics, starting from zero, ending up at bachelor level EE. I was introduced to the second edition (silver) when I attended college in the late 90s, and have later upgraded to the third edition (gold). It also has a companion book with more exercises and lab experiments. For everyone that wants to learn EE, it is highly recommend. Just beware: there are fake copies for sale on Amazon, so be sure you get a genuine copy. reply nraynaud 7 hours agoparentprevfunny, it got recently suggested to me too. I really feel like youtube is not individualizing the recommendations. reply crote 13 hours agoprev> Between the resulting thermal management issues and reduced battery life, linear regulation is seldom worth the pain. I'd argue the exact opposite. The article is targeting \"enthusiasts\", and a very large portion of enthusiast projects are going to be powered by a 5V USB charger and consume in the order of a few 100mA of power. LDOs are dirt cheap, widely available, have pretty decent output characteristics, and incredibly easy to use. If you have basically unlimited 5V and want 100mA of 3.3V, why not use one? On the other hand, buck converters require you to actually do some actual engineering. You can't just haphazardly throw in a single IC and expect it to work flawlessly on your first try. You either have to use an (expensive!) fully-integrated module, or do a decent bit of math and part sourcing yourself. Neither option is exactly attractive to a hobbyist building a fairly simple one-off PCB. reply michaelt 6 hours agoparent> On the other hand, buck converters require you to actually do some actual engineering. You can't just haphazardly throw in a single IC and expect it to work flawlessly on your first try. It used to be a hassle a few years ago - but these days you can haphazardly throw in a R-78K3.3-0.5 - which has the pinout of a classic three-pin 3.3v linear regulator, but it's actually an 80% efficient DC-DC converter with 500mA output and an input range that goes up to 36v. That's enough current even if you've got something like an ESP32 that needs 250mA - and for any type of hobby project, the $2.40 is fine. reply quacksilver 12 hours agoparentprevLinear regulation is also very good when you have situations where you want to avoid generating unwanted noise or stray RF. Cheap buck converters are very noisy and annoying if you are building an audio or radio related project, or have such things nearby. reply Animats 12 hours agorootparent> Cheap buck converters are very noisy and annoying... Yes. This is why the good ones have more parts. It's a totally fixable problem, and the parts cost to fix it isn't high, but it takes extra engineering effort. reply atoav 11 hours agorootparentYou are right. Yet, if you asked me how to get less noise on your audio circuit the LDO is the easier answer that will cost you less time to implement and likely give you the superior result. Especially for beginners without a ton of measuring equipment and experience having potentially bursty high frequency components in series can be an interesting way to not get the thing they were planning done, but instead have to deal with an entire new set of problems whose existence they didn't even know about. Technically you are correct, but \"just slap a LDO on it\" is probably the better advice. reply shiroiushi 11 hours agorootparentprevAll true, but for a hobbyist it probably isn't worth it if their goal is to just build some little audio project. reply f1shy 12 hours agorootparentprevIn my experience, not only noise in the RF sense, but also audible. I put together a little audio amplifier, and the sound of the DC/DC makes it unusable in quiet situations. The 12kHz (coming physically from the converter, amplifier off) really hurts the ears! reply arghwhat 12 hours agorootparentThat’s magnetostriction, components under switching load (caps, inductors) need to be secured in place with an appropriate glue/putty. Using a higher switching frequency can also help, plenty to choose from. reply schoen 10 hours agorootparentCan that also help with the emanations security issue where an adversary might be able to extract usable data from the audio produced by the electronic components? reply arghwhat 7 hours agorootparentThe noise would correlate with load, but this is the least of your worries. Unless you have a proper RF testing lab and skilled EMC engineers at your disposal, the only thing you can do is stuff everything into a properly designed faraday cage. reply kragen 9 hours agorootparentprevyes, but the audio usually doesn't travel as far as the rf; you'd almost have to be in a situation where the adversary can't put equipment near you but has managed to subvert a microphone reply foldr 9 hours agorootparentprevThe usable data would just be \"DC-DC converter is on/off\". In theory, if the converter uses a variable frequency or duty cycle, you might be able to extract some information about that too. But that's not very interesting. reply arghwhat 5 hours agorootparentA DC-DC converter always uses a variable duty cycle to maintain the target output voltage (or for CC, current). Without it, the voltage would vary wildly depending on load. For something like an audio amplifier, obtaining precise power supply load would in turn give you a curve over amplifier load, which effectively gives you the speaker amplitude. Input caps and filtering will likely remove the high frequency components entirely, but you might be able to construct at least part of the played waveform. reply foldr 5 hours agorootparentAll good points. I would say that it's a fairly outlandish scenario where you are (i) close enough to the device to listen to the caps whining but (ii) can't measure actual voltages within the circuit (which could be a lot more informative) and (iii) can't just listen to the audio output of the device directly. reply arghwhat 5 hours agorootparentAcoustic noise is one thing, but it's not at all outlandish to be within range of the EMI emitted from the same power supply which tells the same tale. What is outlandish is thinking anyone bothers listening in. :) reply mschuster91 10 hours agorootparentprevTEMPEST and other side-channel hardening is hard to do if you lack access to anechoic/RF isolated chambers, sensitive scopes/microphones and knowledge. reply jimmyswimmy 7 hours agorootparentprevThe other answer about magnetostriction is technically correct (the best kind) but Misses the actual cause, which is subharmonic oscillation. This occurs when you have not stabilized your control loop properly and is often the result of inadequate phase margin. A simple fix may be to allow the control bandwidth by increasing capacitance at the work amplifier output. But this may also make the response too slow. For most people designing DCDC converters, this is the most difficult part to understand and correctly tune. If you get the parts selection right and carefully lay out the circuit, this is the one that they can't get right. It takes some understanding of control theory or careful testing and tweaking. And it's what drives a lot of folk to the expensive and relatively inflexible power modules. reply megous 3 hours agorootparentNoise can also come from pulse-skipping mode of regulation, if you draw too little power from the DC-DC converter, and can go away under higher load. reply _fizz_buzz_ 9 hours agoparentprevIf you have 5V and have to step down to 3.3V using an LDO is a very reasonable choice (at 100mA you have about 170mW losses). However if you have e.g. 24V and need to step down to 3.3V, an LDO can get annoyingly hot (at 100mA you now have over 2W losses). But I agree, this is really a \"it depends\" situation. reply roaringraster 12 hours agoparentprevAnd 3.3/5 is approximately 66% efficiency, which isn't too horrible. So even if you get your buck converter working, getting those 95%+ efficiency numbers you see in datasheets out of the circuit is not trivial. reply exar0815 12 hours agoprevI do work in automotive EMC testing and it's nearly always the voltage conversion at fault when you fail tests or influence other devices. Buck-Boost converters are a noisy and finicky thing, and not easy to debug if you use a monolithic IC from the cheapest vendor. Quite annoying discussions. reply marcodiego 5 hours agoprevA commonly used alternative in the microcontroller world is to simply stack a few diodes. Very simple alternative which I have seen being used a few times. reply hcfman 12 hours agoprevI've been working with audio recently and found so many of the devices that convert 3.7V to 5V for example inject noise into the rail that make's it in the microphone input source. The battery support from pisource does this terribly. But so do many battery sources. It's not just microphones that get affected, but also other sensitive sensors like accelerometers. I hope that other people making DC-DC convertors put some effort into making sure the supply is so clean so as to prevent this in future. reply klysm 6 hours agoparentMaking a noise-free DC-DC converter is very difficult. Any buck/boost style converter is going to introduce ripple and switching noise into the system. This is inherently unavoidable, and it’s very sensitive to the layout of the board. Actively or passively filtering out all this broadband frequency content is far from trivial, and there is no general solution - only a large, high dimensional tradeoff space. You’re right that noise is a concern for any analog circuitry though, and if you want to, you can spend a lot of money on specialized DC/DC converter modules with integrated inductors that do their best to eliminate this noise. reply euroderf 12 hours agoparentprevIsn't this like 95% fixable with a capacitor ? Aren't there cables with small embedded caps ? reply magicalhippo 9 hours agorootparentAs the sibling comment mentions there are several aspects. You'll need proper input filtering which may require a non-trivial filter network. You'll also need proper output filtering, which does include slapping a lot of capacitors on there, but also careful selection of those capacitors both type and size. Parasitic inductance of larger packages can mean they can't filter high frequencies, and MLCC capacitors have a DC bias which means the effective capacitance is significantly reduced when they have a DC bias on them which they will have in a DC-DC converter. Then you need to take great care about component placement and board layout, to minimize the return path of the currents and such. You can skip all of that and get a board that functions as a DC-DC converter if you measure it with a multimeter, but actually be horrible. And you just can't fix bad layout by slapping more capacitors on there. And even with a not terrible layout, you can't fix it by using the wrong kind of capacitors. Like anything through-hole is just not gonna pass. reply dragontamer 12 hours agorootparentprevOh hell no. I think a lot of people are overly cautious of DC-DC conversion in this topic, but you've gone full-tilt in the opposite direction and are severely underestimating the problems that occur. 1. Its not \"power-conversion\" that's hard per se, its EMC that's very hard and not taught very well at a bachelor's level. 2. DC-DC Voltage Converters usually handle the entirety of your board's power, meaning they are the highest power component. 3. High power and high-frequency is a difficult EMC problem. This means that a bad design will absolutely send your electrons / energy out and radiate out like an antenna. And if things on the same board pick it up, it will be called crosstalk. And if things off-board pick it up, its called electromagnetic interference which almost certainly leads to a compliance problem. --------------- 1. Hobbyists don't care about compliance. So bam. We are already dealing with the biggest problem by simply not caring about it. (Maybe you can care and go into deeper studies, but... if you're a beginner just don't care. Learn this very difficult stuff later). 2. Prevent crosstalk by following good board design rules: have a 4-layer board. Use Power+Signal / GND / GND / Power+Signal stackup. Use two vias (one for signal-1 to signal-4 traversals), and a 2nd via for GND2 to GND3 traversal of the return current). Thinking of both the forward current and a tightly bound reverse current is basically all you need to do to avoid difficult crosstalk problems on board. Done. Point#2 requires deeper studies than is typical in bachelor's level electrical engineering. But it truly isn't very difficult once you learn the theory. Tight ground-planes reduce crosstalk (and EMI problems), and furthermore thinking of the return-current explicitly prevents problems. Now you could have some truly difficult \"ringing\" from trace inductance and other such nasty problems... but that tends to occur beyond 100MHz. I'm thinking most beginners are going to be under 20MHz for most of their designs and thus never deal with those advanced \"PDN\" / Power Delivery Network problems. Though if you do go into PDNs, its obviously a tough subject with huge amounts of study and reading involved. But most of the problems truly are at very high frequencies and/or at EMI compliance. Beginner Hobbyists avoid the most difficult issues entirely by nature of beginner (aka: low-speed) and hobbyist (and therefore don't have to follow regulators). ---------- I'm not a professional. But my understanding is that top-level EEs who work on PDNs will simulate the circuit-board itself to figure out trace inductances / capacitances in the board itself. (Closer planes of ground/power will create more capacitance. Long traces tend to increase trace inductance, etc. etc.). And tight simulations are the only way to truly understand the PCB and how it interacts at high frequencies with high-power. But such methodologies are gross overkill for a 1MHz boost converter with a pre-made PCB Layout, and a list of capacitors + inductors already picked out for you. (ex: https://www.microchip.com/en-us/product/mcp1640) Seriously: Page 17 (https://ww1.microchip.com/downloads/aemDocuments/documents/A...) already gives you the PCB-layout you need for this, with recommended components. Don't overthink it, just copy the design from the document. reply Max-q 11 hours agorootparentWe have a couple of challenges today. Hobbyists often go over 20MHz, because they put WiFi, BT or USB on their boards, giving EMC issues. Also, the speed of the modern ICs tend to be very high. If you have a 9600 Hz UART signal, that is not a 9600 Hz signal if it's a square wave with a modern IC with very short rise time on the pins. So a good old, slow serial line can with modern MCU emit noise up in the hundreds of MHz range. So your PCB layout tips are important, even on slow circuits these days. reply dragontamer 5 hours agorootparentUnless a beginner plans to sell a design on the public, there is no EMC (compliance) issue. Maybe EMI crosstalk. But WIFI and BT are supposed to eminate out like a radio and jump across boards. That's the point. --------- USB is a matched impedance differential pair. Are beginners really running high speed USB differential pairs down their circuits today? Because that's a really.... Erm.... strange.... definition of a beginner. IMO anyway. reply Youden 6 hours agorootparentprev> Use Power+Signal / GND / GND / Power+Signal stackup. I'm just a novice (maybe intermediate) so I'm wondering: the common 4-layer stackups available to hobbyists seem to be 1oz/0.5oz/0.5oz/1oz and I assume the outer layers have better thermal dissipation since they're only kept from the air by solder mask; so wouldn't it be better to put power/ground on the outer layers and keep signals in the middle? Also maybe I'm weird and this is pointless but I typically put a filled copper zone tied to ground on every single layer, unless I have a reason to put some other kind of zone in a particular area. Is it necessary to have a full, dedicated ground plane, rather than ground + signal or ground + power? reply aaronmdjones 5 hours agorootparent> so wouldn't it be better to put power/ground on the outer layers and keep signals in the middle? Signals must never cross a break or split in the plane they're referencing (usually 0V or \"\"ground\"\"). This creates huge EMI problems. Your proposal would have signals on layer 2 crossing a split in the ground plane on layer 1 (that split caused by power traces). Some interesting material on the subject: https://www.youtube.com/watch?v=ZYUYOXmo9UU https://www.youtube.com/watch?v=QG0Apol-oj0 https://www.youtube.com/watch?v=ySuUZEjARPY https://www.youtube.com/watch?v=0RyBCnowLsI reply dragontamer 4 hours agorootparentAll of those videos are great. I'd start with this one specifically: https://www.youtube.com/watch?v=ySuUZEjARPY . (Your 3rd link). reply dragontamer 5 hours agorootparentprevGround fill is counterproductive on the signal layer. If you accidentally get the return path on layer1 or layer4 instead of the designated layer2 or layer3, you've created noise. Power+Signal / GND / GND / Power+Signal is about consistency and braindead-easy tracking of return paths. The return path for layer1 is always layer2. The return path of layer4 is always layer3. Keeping track of both the forward signal (or power line) and the reversed return current (which was electrically induced onto the nearest reference plane) stops working if suddenly you have random reference ground-fill planes on the layer1 or layer4. DO NOT put GND on layer1 or layer4 if you're doing this methodology. --------------- Beginners likely aren't working with a hot enough circuit where thermal dissipation is an issue. If you do have thermal dissipation then I guess thermal ground on layer1 and layer4 ties with thermal vias will be needed. In practice, the thermal resistance across the PCB cross section is better than beginners expect anyway. Thermal conductivity is just one attribute, the other attributes of heat movement are distance and cross sectional area. So the shape favors you up and down the PCB. Yes the fiberglass has worse thermal conductivity but you win on shape. reply mglz 9 hours agoprevFor beginners it is super annoying that many tutorials say \"there is a magical switch or oscillator here which is integral to the function of the boost converter, but we will not tell you how to actually realize it\". Additionally, that needs to work at the voltage level you are starting out from and in many cases should be galvanically isolated from the converter. This is a lot to keep in mind and it is actually not trivial. The answer here is usually to find an IC that works at your desired input voltage or to have a linear regulator provide a small amount of power for the PWM generator. Also be wary of just running with an AI generated answer. Claude 3.5 Sonnet suggest you connect an Arduino straight to 230V and after some back and forth generates circuits which contain strange elements like \"antiparallel diodes\" which makes no sense. reply awjlogan 7 hours agoparentThe TI Power Designer[0] is a great resource. Obviously it will only show you TI parts, but it's very helpful to get a base design. You can filter by complexity (roughly BoM count), size, cost etc based on the parameters (input voltage range, output voltage range, power etc). The designs usually have a reference layout as well. 0: https://webench.ti.com/power-designer/ reply mglz 5 hours agorootparentVery convenient, thank you! reply posterboy 8 hours agoparentprevsounds like a spherical cow on a frictionless plane. reply mglz 5 hours agorootparentIt is a very hairy cow, which likes to bite and is stuck in the mud. Also it has a wierd high-frequency response. There is a description of tractors to get it out, but we'll skip how the controls work for now. reply londons_explore 7 hours agoprevI have often wondered if ideas from a buck/boost converter could be applied to a mechanical gearbox. Voltage and current in electrical circuits (where voltage x current = power) is completely analogous to torque and speed in mechanical shafts (where torque * speed = power). Every electrical component has a physical counterpart. Spring = capacitor. Inductor = mass with momentum. Resistor = friction brake. The goal would be a variable ratio gearbox using a fully mechanical system, using a spring and a hammer type mechanism to convert one torque/speed to another torque/speed. This is already done in impact wrenches, but I would hope that rather than having an impact rate of say 5 Hz, you have an impact rate of 50 kHz or more, allowing a smooth conversion from one speed to another. Obviously, the difficulty is in the details - designing parts to withstand 50k hammers per second for years of operating without failing from fatigue. Various other mechanical things already operate at high mechanical frequencies. SAW filters vibrate things mechanically at Ghz and don't suffer fatigue failures. reply hwillis 5 hours agoparentYou're overcomplicating it; you only need a single clutch and in/out springs[1] to do this. If you're spinning at 4000 rpm and your springs cover 6 degrees of rotation, then your clutch needs to be able to actuate at 4000 Hz. When the clutch is engaged, the engine-side springs compress to supply the torque and match the speed difference. When it's disengaged, the springs expand back out as it returns to engine speed. The obvious problem is that clutches do not smoothly click on and off like a transistor. However there are more specialized devices that use stick-slip dynamics like piezo actuators. Since there is a much more rapid transition between \"on\"/\"off\", they can be very efficient and allow relatively weak devices to exert very large forces. They're just only able to take very small steps. [1] Labeled 4 here: https://haynes.com/en-gb/sites/default/files/styles/blog_lan... reply londons_explore 35 minutes agorootparent> When it's disengaged, the springs expand back out as it returns to engine speed. What is it? I think you need an intermediate flywheel, with springs and clutches on each side. The intermediate flywheel's mass is tiny, so might be formed by just the masses of the springs and clutch mechanism. reply cushychicken 6 hours agoprevA decent article, but there’s a ton of misunderstanding in the comment section. For one thing: LDOs can be more efficient than buck converters, especially at very low current consumptions. If you’re drawing sub 1 mA, like a battery powered system, an LDO is going to be a more efficient step down converter, because it doesn’t have switching losses. Bucks are only better choices for stepping down voltage at higher currents because the switching losses become negligible. Second: a ton of people here are vastly exaggerating the difficulty of designing a step down buck converter. Integrated designs from TI or analog devices will tell you all the compensating components, output capacitor values, inductor values, etc. for common step down output voltages. Most will include reference layouts with a four layer six layer or even two layer stack up for optimal performance. It’s really not that hard to get a one spin win out of most common buck designs. Don’t be afraid. Just follow the manual. You’ll be fine. reply shellback3 3 hours agoprevInteresting, but I had expected to see a comparison of generating DC voltages using tubes, which were used in my university Electronics course, with solid state. In those days to generate a DC voltage from another DC voltage required generating an AC voltage from the DC and then rectifying it. reply kbouck 7 hours agoprevI want to power my 12V devices with USB PD. Looks like 12V is optional in the spec and is supported only by some devices (eg. UGREEN), and not by others (eg. Anker) Given a USB PD power supply which supports 15V but not 12V, and a usb-c/barrel-jack cable configured to negotiate for 15V, what would be the simplest (yet safe) circuit i could add via barrel jack to regulate the to voltage down to safe/consistent 12V? is a simple linear voltage regulator (LM7812) sufficient? would i need capacitors to smooth it out? reply _Microft 5 hours agoparentIt might be cheaper to get a power supply that supports later PD standards? E.g. IKEA is selling some cheap here for either 8€ (Sjöss, 1 USB-C port (max. 30W, up to 3A)) or 15€ (Sjöss, 2 USB-C port (combined power output of 45W, up to 3A, also on a single port)). Both support PD 3.0 and PPS (that's the fanciest PD standard that implements requesting arbitrary voltages from the power supply) They also stock nice and cheap USB-C cables. These power supplies work fine with USB-PD trigger boards set to 12V. reply 15155 5 hours agoparentprevThe one important thing missing from your query here is: How much current do you need? If you need, say, >100A, the possible architecture looks very different than ~1A or less. reply klysm 6 hours agoparentprevA 3V drop over an LDO is usually reasonable with low enough currents. Some LDOs require capacitors to be stable, and it’s usually a good idea to have some capacitance on your power rails anyway. reply ziofill 9 hours agoprevI'm a theoretical physicist and I swear electrical stuff is so hard to understand! I have a lot of respect for electrical engineers ^^' (and electricians) reply mikewarot 10 hours agoprevI've learned that the magic search word for 150ish volt boost converters is \"Nixie\". My friend needed that voltage for a Geiger counter B+ battery replacement. reply stonethrowaway 2 hours agoprevFriendly warning to people who aren’t electronics savvy: this blog post is written in a “now draw the owl” sort of way. I’m not sure who the audience is. Anyone who can read this stuff at the level presented inherently knows most of this and then some. Everyone else will need a book and that book will cover this material as it’s fairly fundamental and will derive equations used in here as well so you can make sense of it. reply moffkalast 9 hours agoprev96% efficiency sounds great on paper for synchronous converters, but as SBC current draw just keeps increasing and BLDC motors can run at higher voltages it starts to create a major heating problem when you have to supply both from the same source. Something like 12V down to 5V at 5A creates a managable amount of heat, but going higher, 20V, 30V on the battery side and things start to melt all around from heat losses from that large a drop. In some cases I've had to resort to using cascaded rails, stepping first down to 24, then 24 to 12 and then 12 to 5 just to keep the heating spread between different buck converters even if it multiplies losses. Would love to hear what the expert solution is to this that isn't just a massive heatsink. reply michaelt 3 hours agoparentYou can get 48V DC -> 5V DC 6.5A converters that are 92% efficient [1] You're dissipating 25W from your SBC already. You can dissipate the 2W from your DC-DC converter the same way. [1] https://www.meanwellusa.com/webapp/product/search.aspx?prod=... reply moffkalast 1 hour agorootparent> current range 0 ~ 6.5A > Fuse recommended (5A) I see they are very confident about going to 6A. These ratings are often just \"yeah it can technically do that but it will reach 100 degrees during it\", for any kind of stable continuous draw you just have to halve the rating to be safe. reply michaelt 27 minutes agorootparentIf you want you can buy the next size up [1] giving you 11A at 5V with the same 92% efficiency. Of course, it'll still dissipate the same amount of power, because 8% of 25W is still 2W. [1] https://www.meanwellusa.com/webapp/product/search.aspx?prod=... reply moffkalast 4 minutes agorootparentThat looks more like it (8A fuse), and relatively cheap on Mouser too interestingly enough. Thanks for the heads up I'll have to order and try one. It does puzzle me why they went with the 2.54mm pin layout though, those are rated for 3A max I think? So even if the draw is perfectly split between the two vout pairs they give it'll be melting at 6A already, probably more like 5 if not. reply posterboy 7 hours agoparentprevyou forgot to mention it should fit under a thumbnail, probably reply moffkalast 1 hour agorootparentIt would be a nice plus :P Honestly the size isn't such a big deal, as long as it doesn't weigh as much as two African elephants like the average mains PSU of this amperage. reply mschuster91 8 hours agoparentprev> Would love to hear what the expert solution is to this that isn't just a massive heatsink. A smaller heatsink with active cooling and parallel MOSFETs. At a certain power level, it's just physically impossible to rely on convection cooling alone - just look at audio amps or your average CPU/GPU... banks of MOSFETs, caps and inductors it is. While the BOM part count may be higher, you need lower-capability parts. The danger is, you need to carefully grade and match the MOSFETs, otherwise you risk them failing sequentially in a very short time if you're operating too close to their rated current - one burns out, the load distributes to the others, and then they fail because they cannot handle the additional load (or one fails into dead short instead of open, which instantly kills all of the others). reply minkles 6 hours agoprev [–] Lots of things in here which kill me a little: 1. You don't get a voltage spike when you disconnect an inductor. The field collapses and induces a current. If you measure it across a high impedance then it looks like a voltage spike. If you measure it across a low impedance then it's not necessarily much of a spike. Ergo depends on load impedance. 2. SMPS designs are not necessarily noisier than linear power supplies. It's always a design trade off. In fact you see SMPS in all modern RF test gear which is generally far more sensitive and has far more bandwidth than anything back when linear supplies were common. Also there is a lot of noise coming off the diodes in a basic bridge rectifier as well! Noise is a whole-system design consideration that has to be made. 3. Don't use any LLMs for designing circuits. Please go read a book on it designed by experts, not stuff scraped from thousands of idiots. I've seen some horrible stuff out there. 4. I'm sure I'll come up with more over time. reply hwillis 5 hours agoparent> If you measure it across a high impedance then it looks like a voltage spike. If you measure it across a low impedance then it's not necessarily much of a spike. \"Disconnect\" implies an open circuit and high impedance. reply Workaccount2 4 hours agoparentprev [–] This is really nit-picky. The fundamental action of a boost converter is from the inductors \"voltage spike\" behavior. The lowest noise linear regulator is less noisy than the lowest noise smps. I agree though that LLM's are not good at circuit design. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "DC-DC converters use inductors to create voltage spikes, which charge capacitors, similar to an auto ignition system.",
      "They are efficient but require safety measures like current limiters or fuses to prevent fires.",
      "Types include boost, buck, and transformer-based converters, with the latter offering input-output isolation for safety; practical uses include converting USB 5V to 120V for antique devices."
    ],
    "points": 322,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1726026815
  },
  {
    "id": 41506157,
    "title": "Chai-1: Decoding the molecular interactions of life",
    "originLink": "https://www.chaidiscovery.com/blog/introducing-chai-1",
    "originBody": "Careers Press Chai Discovery Team Sep 9, 2024 Introducing Chai-1: Decoding the molecular interactions of life We’re excited to release Chai-1, a new multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, covalent modifications, and more. The model is available for free via a web interface, including for commercial applications such as drug discovery. We are also releasing the model weights and inference code as a software library for non-commercial use. A frontier model for biomolecular interactions We tested Chai-1 across a large number of benchmarks, and found that the model achieves a 77% success rate on the PoseBusters benchmark (vs. 76% by AlphaFold3), as well as an Cα LDDT of 0.849 on the CASP15 protein monomer structure prediction set (vs. 0.801 by ESM3-98B). Unlike many existing structure prediction tools which require multiple sequence alignments (MSAs), Chai-1 can also be run in single sequence mode without MSAs while preserving most of its performance. The model can fold multimers more accurately (69.8%) than the MSA-based AlphaFold-Multimer model (67.7%), as measured by the DockQ acceptable prediction rate. Chai-1 is the first model that’s able to predict multimer structures using single-sequences alone (without MSA search) at AlphaFold-Multimer level quality. For more information, and a comprehensive analysis of the model, read our technical report. A natively multi-modal foundation model In addition to its frontier modeling capabilities directly from sequences, Chai-1 can be prompted with new data, e.g. restraints derived from the lab, which boost performance by double-digit percentage points. We explore a number of these capabilities in our technical report, such as epitope conditioning – using even a handful of contacts or pocket residues (potentially derived from lab experiments) doubles antibody-antigen structure prediction accuracy, making antibody engineering more feasible using AI. Releasing the model for all We are releasing Chai-1 via a web interface for free, including for commercial applications such as drug discovery. We are also releasing the code for Chai-1 for non-commercial use as a software library. We believe that when we build in partnership with the research and industrial communities, the entire ecosystem benefits. Try Chai-1 for yourself by visiting lab.chaidiscovery.com, or run it from our GitHub repository at github.com/chaidiscovery/chai-lab. What's next? The team comes from pioneering research and applied AI companies such as OpenAI, Meta FAIR, Stripe, and Google X. Collectively, we have played pivotal roles in the advancement of research in AI for biology. The majority of the team has been Head of AI at leading drug discovery companies, and has collectively helped advance over a dozen drug programs. Chai-1 is the result of a few months of intense work, and yet we are only at the starting line. Our broader mission at Chai Discovery is to transform biology from science into engineering. To that end, we'll be building further AI foundation models that predict and reprogram interactions between biochemical molecules, the fundamental building blocks of life. We’ll have more to share on this soon. We are grateful for the partnership of Dimension, Thrive Capital, OpenAI, Conviction, Neo, Lachy Groom, and Amplify Partners, as well as Anna and Greg Brockman, Blake Byers, Fred Ehrsam, Julia and Kevin Hartz, Will Gaybrick, David Frankel, R. Martin Chavez, and many others. © 2024 All rights reserved. Terms of Service Privacy Policy",
    "commentLink": "https://news.ycombinator.com/item?id=41506157",
    "commentBody": "Chai-1: Decoding the molecular interactions of life (chaidiscovery.com)257 points by glowingvoices 20 hours agohidepastfavorite62 comments xianshou 18 hours agoIn light of last week's fiasco with Reflection (https://venturebeat.com/ai/new-open-source-ai-leader-reflect...), I hope the community has a newfound enthusiasm for independent testing! This is extremely exciting news if true, so I'm eager to have it either confirmed or questioned. The one thing I hope we won't be doing is accepting SOTA evals from open-sourced models at face value. reply deisteve 18 hours agoparentI don't know how people like Matt Schumer can attempt what looks like fraud and deception being chalked off as a giant oopsies (which isn't really convincing) and not face any consequences. For rest of us, this is a privilege that we don't have. We can't deceive, defraud our investors because it has real consequences....but not for people like Matt Schumer, why is that? reply Loughla 17 hours agorootparentHaving mountains of money, in the US, is equated to being smart and better than. This means that failures, unless they purposefully exploit other better thans, are always forgiveable. Even when they're mildly intentional. reply mupuff1234 16 hours agorootparentPretty sure it's not a US only thing. reply mecsred 14 hours agorootparentprevJust imagine the legal system as a money duel. If you have little money you can be crushed at no cost. Trying to fight someone with big money, even if you're likely to win, will take a lot of time and money. Unless the fraud was black and white or you're in for the long haul it's easier just to lick the wounds. reply parentheses 12 hours agorootparentDoes that logic apply to the State - usu plaintiff? Doesn't seem so since they have seemingly endless capital but have limits in what they can bring to bear. You tell me... reply nayroclade 6 hours agorootparent\"The state\" is not a monolith. Anti-fraud enforcement is handled by agencies with limited budgets and resources. Often they are deliberately underfunded and understaffed precisely so they cannot cause too much damage and embarrassment by going after really big targets. reply wslh 18 hours agoparentprevTheranos everywhere? Except you can’t afford to mess up when it comes to health. reply f6v 9 hours agorootparentOh, pharma messes up all the time. But it’s an interesting question. You can’t be too risk-averse because there’re thousands of patients dying horrible deaths every single day. There’s simply a need for bold approaches in many areas of medicine. reply wslh 5 hours agorootparentI'd like to add a perspective that might not resonate with everyone, based on the famous quote: \"Any sufficiently advanced technology is indistinguishable from magic.\" I sometimes adapt this to say: \"Any sufficiently advanced technology is indistinguishable from a scam.\" reply mmmore 19 hours agoprevDoes the use of \"foundation\" and \"multi-modal\" for describing this model mean anything, or are those just used as buzzwords? Funnily enough, the only place those terms appear in the paper is in the abstract. Also the paper says they basically copied the methods used for AlphaFold, but then included the ability to input language embeddings, and input some other side constraints that I don't have the biology knowledge to understand. They don't show any data that indicate how much these changes improve performance. They show a very modest improvement over AF3 (small enough that I would think it could be achieve through randomness/small variations in the training parameters). So I don't think this is very revolutionary, but I suppose it replicates AF3. reply dekhn 18 hours agoparentIf by \"multi-modal\", you mean \"it takes several different datatypes as input or output\", then yes, it's multi-modal. See Figure 1 in the Tech Report. reply alexk101 18 hours agoparentprevFoundational maybe isn't the best label for this kind of model. My understanding of foundational models is that they are made to be a baseline which can be further fine tuned for specific downstream tasks. This seems more like an already fine tuned model, but I haven't looked carefully enough at the methodology to say. reply lainga 17 hours agorootparentWould you then call it a buzzword, or is there some gentler excluded-middle interpretation of that word's application to the project? reply IanCal 6 hours agorootparentI don't think it's a particular buzzword here. They claim it's useful across a range of tasks, and that's the key part imo. Now, \"predictions for parts of drug discovery\" isn't the widest range, so perhaps you need to consider \"foundation\" as somewhat context dependent, but I don't think it's a wild claim. Neither \"foundation\" nor \"fine tuned\" are really better than each other, but those are probably the two ends of a spectrum here. My get-out clause here is that someone with a better understanding of the field may say these are actually extremely narrowly trained things, and the tests are equivalent to multiple different coding problem challenges rather than programming/translation/poetry/etc. reply brookst 17 hours agorootparentprevIt’s about like referring to a famous person’s red carpet attire as “off the shelf [designer name]”. It downplays the effort that went into it more than anything. reply ashvardanian 15 hours agoparentprevThere is a pretty noticeable improvement for antibody-antigen interactions - looks like double-digit percents. Check out figure 4 here: https://chaiassets.com/chai-1/paper/technical_report_v1.pdf reply mmmore 9 hours agorootparentFigure 4 is comparing the model with itself, unless I'm misunderstanding it. The takeaway seems to be the model performs better if you give it extra \"constraints\", i.e. extra info already known about the protein. The table with a comparison to alpha fold gives a less than one percentage point improvement. reply bbstats 17 hours agoprevthe error bars are like 5-10x the size of that 'defeat' reply zan2434 19 hours agoprevThis is both awesome and feels very dangerous to release publicly, no? Can’t this be used to discover novel bioweapons as easily as it can be used to discover new medicines? Genuinely curious, would love to learn if that isn’t true / or is generally just not that big of a deal compared to other risks. reply matrix2003 19 hours agoparentWe already have some pretty horrific and documented/accessible bioweapons. This gets into the philosophy of restricting access to knowledge. The conclusion I keep arriving at is that we’re lucky that there don’t appear to be many Timothy McVeighs walking around. I don’t think there is a practical defense from people like that. reply cowsandmilk 18 hours agoparentprevI think you overestimate the difficulty of discovering bioweapons. There is a reason toxicology is the dead end for tons of drug molecules. It is very easy already to design molecules that will kill someone. reply emporas 5 hours agorootparentEven the word bioweapon is not accurate to describe a deadly (or harmful) biological agent. A weapon usually means that there is a source of deadly force, and a target. The source doesn't want to be hit by the same weapon it uses to hit others. This is vastly difficult to achieve using biology. Any organism on the planet has it's own agency, and it will hit anything to reproduce and eat. In addition this is not limited to toxicology and releasing toxins, because the agent can just eat tissue. For example phosphorus has been used in chemical warfare, but even that cannot be described 100% as a weapon. The phosphorus gas can hit people who released it the same as everyone else, it just depends on the wind. Right now, on everyone palms, there are thousands of organisms which create electricity, eat wood and kill animals. Given that the palms are washed, that number is reduced to some thousand different species. If the palms are not washed the last 24 hours, that number shoots up to hundred thousand different species, even millions. I do not see any difficulty for someone to enhance a harmful agent and make it deadly, using just regular computation and not even A.I.. However the person who facilitated this, will be a target too. reply whymauri 16 hours agorootparentprevAs someone who worked in molecular ADMET, this x1000. reply zan2434 16 hours agorootparentprevThis actually makes a lot of sense! Sounds like finding dangerous chemicals is easy and is not the actual limitation at all. reply taspeotis 19 hours agoparentprevThis is as unethical as that time JVC released VHS which allowed people to record videos but also pirate content!!1 reply mmmore 19 hours agorootparentYou'd have to work at the RIAA to think that piracy and bioweapons are comparable. I don't know how much releasing this model is a delta on safety, but we certainly need to do a better job of vetting who can order viruses; my understanding is there's very little restrictions right now. This will become more important as models get more capable. reply zan2434 19 hours agorootparentprevClear snark aside, content piracy has pretty bounded risks so isn’t a reasonable comparison reply peepeepoopoo84 19 hours agorootparentnext [3 more] [flagged] zan2434 19 hours agorootparentThis is a textbook bad faith comment / attacking the person but not the subject of the argument. I’m just asking about others’ assessment of the benefits and risks. What do you think? Or do you think it’s just not worth considering? reply Klonoar 19 hours agorootparentprevOr that hacker culture grew up and tries to weigh risks, since they lived through some shit. reply dekhn 18 hours agoparentprevNobody has really been able to make a convincing argument whether these sorts of tools haven't lead to large-scale terrorism through bioweapons because the underlying problem is hard (for a sufficiently motivated adversary), or that terrorists don't have the resources/knowledges/skill, and as far as we can tell, the sufficiently motivated adversaries who have tried either failed, succeeded secretly, or were convinced to walk back from the brink due to the potential consequences. In short there are other ways to negatively affect large numbers of people that are easier, and presumably those avenues are being explored first. But we don't know what we don't know. reply peterldowns 18 hours agoparentprevIf you're implying that the answer is \"yes this is too dangerous\", could you possibly give a few examples of technological developments that aren't \"very dangerous to release publicly\" by the same standard? For instance, would any of the following technologies be acceptably \"safe\"? - physical locks (makes it possible to keep work secret or inaccessible to the government) - solar power (power is suddenly much cheaper, means bad guys can do more with less money) - general workload computers (run arbitrary code, including bad things) - printing press (ideology spreads much more quickly, erodes elite hold over culture) - bosch-haber process (necessary for creating ammunition necessary to fight the world wars) reply mmmore 18 hours agorootparentYou left out the most relevant comparison: - nuclear fission, which provides an abundant source of environmentally friendly energy, but allows people to make bombs capable of wiping out whole cities at once (and potentially causing nuclear winter) But even in that case, I believe that it's a good thing that we have access to nuclear power, and I certainly want us to use more nuclear power. At the same time, I'm very glad that a bomb is hard enough to make that ISIS couldn't do it, let alone any number of lone wolf terrorists. So I think I would apply the same logic to biotechnology; speeding up medical progress seems extremely valuable and I'm excited about how AF and other AI systems can help with this, but we should mitigate the ability for bad actors to use the same tools for evil. An aspect that's unique about biotechnology that's different in comparison to the examples you gave is that most of those technologies help good and bad people approximately equally, and since there's many more reasonable than crazy people they're not super dangerous. There's a concern that technologies that make bioengineering easier could make it easier to produce and proliferated novel pathogens, much more so than they make it easier to prevent pandemics; in other words, it favors \"offense\" more than \"defense\". The only one example you listed that has a similar dynamic in my mind is the bosch-haber process, but that has large positive downstream effects separate from its use for ammunition. Again, this is not to say we should stop medical progress, but that we should act to mitigate the dangers, and keep this concept in mind as the technology progresses. That said, I'm not certain how much the current tools are dangerous in this way. My understanding is that there is lower hanging fruit in mitigating these issues right now; for example, better controls at labs studying viruses, and better vetting of people who order pathogens online. reply dosinga 12 hours agorootparentprevThe printing press indeed led to religious wars in Europe. The Ottomans banned it and avoided that fate. And the progress associated with it. reply f6v 9 hours agoparentprevThere’s still a long way from in-silico prediction to wet-lab validation. You need a full-blown molecular biology lab to test any of these. Then again, you can just release existing dangerous pathogens. Like, poison a water with something deadly. So you don’t need a new one if you’re a terrorist. reply crackalamoo 18 hours agoparentprevNot a solution, but maybe if a bad actor tried to create a bioweapon, a trusted organization could use this technology as an antidote. Unfortunately this still leaves the possibility of some kind of insidious, undetectable bioweapon. reply m00x 19 hours agoparentprevNo, it's a very small piece for what you'd need to make bioweapons. reply d_silin 19 hours agoparentprev...as difficult as discovering new medicines, you mean? Chemistry and molecular biology are fiendishly complicated fields, far more complex and less predictable than what general (and most of the non-biochem STEM majors) imagine them to be. How do I know? I thought of one brilliant startup idea that would solve so many of the world's problems if only we used computers to simulate biological systems. Result: https://xkcd.com/1831/ Reference materials: https://www.amazon.ca/Molecular-Biology-Cell-Loose-Version/d... I strongly recommend to treat it as introductory-level text on the same level as \"K&R - C Programming Language\". Yes, all 1464 pages of it. https://www.amazon.ca/Fundamentals-Systems-Biology-Synthetic... On the same level as above text, but with more math. https://www.amazon.com/Introduction-Computational-Chemistry-... That or any other book on computational chemistry will give you an understanding why it is difficult to design anything of value in biological systems. ML can only help so much. Also check out this page for entire field scope: https://en.wikipedia.org/wiki/Omics reply dekhn 19 hours agorootparentMBoC is more like Knuth's textbooks. It's a towering monument to the achievements of humanity over the past 150 years (molecular biology proper is less than 100 years old). As well as being highly accessible (readable). It's done in an interesting style, with lots of direct references to current literature. I was surprised to see a recent edition on IA: https://archive.org/details/alberts-molecular-biology-of-the... reply glowingvoices 16 hours agorootparentprevThank you for the textbooks! I've started studying Molecular Biology of the Cell to prepare for undergrad, but this is the first time I've heard about the others. Are there any other books you would recommend? reply d_silin 16 hours agorootparentSearch for \"computational biology\" on Amazon, but I'd say go first to online courses if you have time and commitment, like: https://www.coursera.org/specializations/bioinformatics https://www.coursera.org/specializations/systems-biology Also, checkout out https://www.coursera.org/courses?query=computational%20biolo... Then you will have a better understanding of the subject area and the literature to search. reply glowingvoices 15 hours agorootparentI'm still in high school, so I don't think I'll have time to fit the courses into my schedule. I'll definitely look for the books though! Thanks. reply IncreasePosts 19 hours agoparentprevThe saving grace of civilization is that, for the most part, terrorists are dumb. reply mmmore 18 hours agorootparentUnfortunately this is not always true. For example, one of the architects of the Tokyo subway sarin attacks[1], Masami Tsuchiya[2], had a masters in physical and organic chemistry. [1] https://en.wikipedia.org/wiki/Tokyo_subway_sarin_attack [2] https://en.wikipedia.org/wiki/Masami_Tsuchiya_(terrorist) reply IncreasePosts 17 hours agorootparentYes, a lot of terrorists have engineering degrees also. But they're also dumb, which is why they think terrorizing random people will positively I prove the world in some direction they care about. I won't go into details, but I think if I had 19 dudes with a death wish in America, and a few million dollars, I could do something far worse than 9/11. reply sudosysgen 16 hours agorootparentThe goal of an attack like 9/11 isn't really to kill the maximum number of civilians in order to terrorize random people. The attack had a significant degree of symbolism. The intended audience was twofold: the Western public and leadership, with a durable message that they weren't untouchable (hence the attacks on the Pentagon and attempt on the Capitol), hence targeting large landmarks; the combination of civilian and military targets was to signify that they held the two to he equivalent. Plans were actually presented to attack other targets that would lead to more casualties, notably a nuclear power plant. The other goal was to incite a religious conflict from the Muslim world against the US, and therefore probably from the US against as many Muslim countries as possible. So the primary goal really wasn't to kill as many random people as possible (though of course that was a consideration), it was actually to target the tallest buildings possible as well as the most important government institutions. Unfortunately, it really did move the world in the direction they wanted. Despite being extremely evil, they actually were remarkably successful at causing the social and geopolitical changes they wanted given the resources they had, and that caused yet more damage we shouldn't ignore. It also bears remembering (especially today) that terrorists often and unfortunately aren't as dumb as we think, and we underestimate them and simplify their motives to our peril. reply pfisherman 18 hours agorootparentprevThere is a big gap between a master’s and a PhD, and then another between a PhD and a seasoned pro. To do something like a bioweapon, you would need a reasonably sized team of pros w/ a lot of capital intensive infrastructure. It would be virtually impossible to do in secret. reply echelon 19 hours agoparentprevThe science to restrict is molecular biology (bacteria) or virology, not applied mathematics (AI). These folks can already do some wild things with the materials they have on hand and don't need fancy AI to help them. Structure prediction is just one small slice of all of the things you'd need to do. Choosing a vector, culturing it, splicing it into an appropriate location for regulation, making sure it's compatible with the environment, making sure your payload is conserved, study the mechanism of infection and make sure all of the steps are unimpeded, make sure it works with all of the host and vector kinetics, study the pathology, study the epidemiology. And that's just for starters. This would require a team and an enormous amount of resources. People motivated enough to do this can already do it and don't need the AI piece. reply throwup238 19 hours agoprevHow hard would it be for a biohacker to use these models to develop novel proteins? Let's say I wanted to take GFP and create another color fluorescent or something. reply glowingvoices 19 hours agoparentI don't think it'd be too difficult. Train a PLM to generate proteins, validate with AF3, and send them off to a lab. You might want to read the ESM-3 paper if you're interested in stuff like this (not affiliated in any way). reply pama 17 hours agoprevThe title in HN is inaccurate. Having a 1% higher score on one metric is not beating a previously published model. This is a replicate, which is fine enough. reply dang 14 hours agoparentAh yes - thanks! We've changed it to the article title now. Submitters: \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" - https://news.ycombinator.com/newsguidelines.html (Submitted title was \"Chai-1 Defeats AlphaFold 3\") reply drob 15 hours agoparentprevFwiw, the authors never actually claimed this. From their technical report [0]: > Chai-1 achieves a ligand RMSD success rate of 77%, which is comparable to the 76% achieved by AlphaFold3 [0] https://chaiassets.com/chai-1/paper/technical_report_v1.pdf reply dgfitz 18 hours agoprevIs there some sort of betting line I can make money off with all this? “-150 a new model isn’t released in the next month claiming it is currently the best at something” would let me retire years early. If there is another line that said “+500 thus model will be forgotten and useless in 6 months” could take my retirement from years to months. reply anitil 18 hours agoparentI believe Manifold does this sort of thing, though I've never used it myself. reply tfehring 18 hours agorootparentManifold [0] has markets on this sort of thing, but it primarily uses fake money. (They're working on a real-money \"sweepstakes\" thing, which I'm not super familiar with.) If you're outside the US and looking for a real-money market, Polymarket [1] is probably your best bet. In the US, real-money prediction market contracts are regulated by the CFTC in the US, so availability of contracts is pretty limited; Kalshi [2] would be the most likely option, but I doubt they have anything on this topic. [0] https://manifold.markets [1] https://polymarket.com [2] https://kalshi.com/ reply pants2 17 hours agorootparentYour best bet in the US is to use Polymarket with a VPN reply thefourthchime 17 hours agoparentprev-180 it’s a wrapper around alphafold with some pre prompt. reply talldayo 15 hours agoparentprev> “-150 a new model isn’t released in the next month claiming it is currently the best at something” would let me retire years early. An optimist and their seed funding are easily parted. reply trott 15 hours agoprevI'm the author of AutoDock Vina (the most cited docking program, and the \"runner-up\" in the AlphaFold 3 paper) Docking software is used to scan millions and billions of drug-like molecules looking for new potential binders. So it needs to be able to generalize, rather than just memorize. But the evaluation approach used here and in the original paper (1) does not test how well the software will perform on novel molecules, because the test set is related to the training set. If you understand the basics of ML and physics, you may be interested in my detailed critique here: https://olegtrott.substack.com/p/are-alphafolds-new-results-... I'm glad that Chai-1 has been released though, as this will probably help people evaluate the method better. (1) It looks like they are a bit different, as this paper allows 40% sequence identity. It's still high. I believe that sequences with 40% identity tend to have the same shapes, especially in the binding site, where it matters. reply uptownfunk 13 hours agoparentThanks for your work and also for your comments of AF3 and Chai-1. It sounds like you are implying there are potentially gross and subtle types of data set leakages taking place between the train and test which are resulting in what seem to be inflated performance metrics? These are pretty serious issues if so. Also I would agree with previous authors that marginal Improvement over sota is proof more that they have recreated something than really made significant new progress. But this has been an issue with LLMs for sometime now. But it sounds like they have some bright engineers from good brand name companies who are coming together with some VC backing of the team to try and do something in this space. I do appreciate that the weights are open. I would like to learn more about their future direction and their training methods reply marviel 19 hours agoprev [–] > We are releasing Chai-1 via a web interface for free, including for commercial applications such as drug discovery. We are also releasing the code for Chai-1 for non-commercial use as a software library. We believe that when we build in partnership with the research and industrial communities, the entire ecosystem benefits. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Chai-1 is a new multi-modal foundation model for molecular structure prediction, excelling in drug discovery tasks, and is available for free via a web interface for commercial use and as a software library for non-commercial use.",
      "It achieves a 77% success rate on the PoseBusters benchmark and outperforms AlphaFold-Multimer in folding multimers, with a 69.8% accuracy.",
      "Chai-1 can predict multimer structures using single sequences and doubles antibody-antigen structure prediction accuracy with epitope conditioning."
    ],
    "commentSummary": [
      "Chai-1, a new model for decoding molecular interactions, has been released, sparking significant interest in the tech community.",
      "The model claims to improve upon AlphaFold, a well-known protein structure prediction tool, but the improvements are marginal, with only a 1% higher score on one metric.",
      "Concerns have been raised about the potential misuse of such technology for creating bioweapons, although experts argue that the complexity of molecular biology makes this unlikely."
    ],
    "points": 257,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1726006410
  },
  {
    "id": 41507563,
    "title": "How economical is your local Taco Bell?",
    "originLink": "https://taconomical.com",
    "originBody": "Lava Taco Crunchy Taco 3 Crunchy Taco Combo 3 Doritos Locos Combo Crunchwrap Supreme Nachos BellGrande Build Your Own Cravings Box Cinnamon Twists Guacamole 🌮 How economical is your local Taco Bell? ℹ Info Legend [ - ] <$7.50 $7.50+ $8.50+ $9.50+ $10.50+ $11.50+",
    "commentLink": "https://news.ycombinator.com/item?id=41507563",
    "commentBody": "How economical is your local Taco Bell? (taconomical.com)233 points by surprisetalk 16 hours agohidepastfavorite254 comments architango 7 hours agoSurprising that the legendary Taco Bell in Pacifica isn’t the most expensive one, though it is close. It has a fireplace, it serves margaritas, and it has a walk-up window for hungry surfers. https://californiaisforadventure.com/pacifica-taco-bell/ reply hansvm 19 minutes agoparentAnd, critically, with the high volume they're able to keep supplies fresh and staff well-trained. Taco bell is still just salty, fatty fast-food, but that one is top-tier (locations right off major interstates in well-lot towns tend to have better food too). The vast majority of locations are worse -- bad burrito folding, sloppy ingredient measuring, stale tortillas, crusty beans, .... I think the public sentiment would be better if people hadn't experienced low-volume Taco Bells. reply iancmceachern 2 hours agoparentprevMy wife and I go sometimes. They also serve beer. There is also a pretty good Panda Express right there you can grab takeout from and Park in the beach parking lot. There is another Taco Bell cantina right near our apartment near oracle park. They have really good prices for alcohol compared to the surrounding bars. reply iav 2 hours agoparentprevIt’s still a Taco Bell with a few much better sit down places in the adjacent strip mall reply bitbckt 3 hours agoparentprevIt is also occasionally a wedding venue. reply klingoff 46 minutes agorootparentFor those who demand synchronous proof of low expectations to commit. reply seanmcdirmid 14 hours agoprevThey are missing the Seattle Lower Queen Anne Taco Bell/KFC combo store that is known to be the most expensive Taco Bell in the nation. https://locations.tacobell.com/wa/seattle/210-w--mercer-st-.... Anyways, you hear horror pricing stories about this one store. reply camkego 13 hours agoparentApparently the prices are scraped from the mobile app. I installed the mobile app and for some reason, and that location can’t be found inside the mobile app. Actually, you cannot (I can’t anyway) order from that location from the website either. It just goes to show data cleaning is hard. It’s true for statistics and it’s probably just as true for LLMs. reply Loughla 6 hours agorootparentWhy does Taco Bell have an app? What is its purpose? reply bregma 5 hours agorootparent(1) Studies have shown that patron will make larger and more frequent orders when placed through an app rather than in person. This increases revenues. (2) The app gathers statistics. Marketing and sales are so addicted to massive amounts of all kinds of information they have trouble finding veins that are not collapsed and their coworkers all carry naloxone. In most organizations, the marketing and sales people make most product decisions and a mobile app is a (free) product distributed by the chain. (3) Data collection. Not only can anonymized data be monetized, but selling data directly attributable to individuals is a major source of revenue for many organizations, and I doubt Taco Bell (or Yum! Brands in general) is not among them. reply JumpCrisscross 45 minutes agorootparentIn addition, price discrimination: you can offer deals in the app to price-sensitive consumers that don't cannibalise in-store sales from convenience-sensitive ones. Same concept as couponing. reply eldaisfish 3 hours agorootparentprevDid this come from an LLM? reply PaulHoule 2 hours agorootparent(1) My advisor in grad school taught me to speak in bulleted lists (2) It's a habit I've been trying to break in the LLM age. (3) I am always telling Copilot to knock it off with those stupid lists and just talk normal. reply square_usual 33 minutes agorootparentprevNo LLM would make the naloxone joke - they're all too sanitized for that. reply bregma 2 hours agorootparentprevIt came from someone for whom modern \"AI\" is indistinguishable from a Potemkin village. reply soperj 2 hours agorootparentprevthe LLM was trained on their posts. reply sidewndr46 5 hours agorootparentprevTheoretically you can order from it and you order is ready when you arrive. The last time I tried this, I had my passenger try and place our order. At the end of the process the app let us know we can't order online from that store. So my conclusion it is just a data harvesting scheme. reply SilasX 3 hours agorootparent>Theoretically you can order from it and you order is ready when you arrive. The last time I tried this... the app let us know we can't order online from that store. FWIW, as one data point, this is what I've used it for a few times, and it worked without issue. (A co-working space has it next door and that made it a lot more time-efficient, avoiding the line and having to wait in store.) Though I have run into the problem you've described with other services, where it rejects your request much later than it should. Like with AWS letting you configure a server for launch before informing you that you don't have permission to launch servers: http://blog.tyrannyofthemouse.com/2016/02/some-of-my-geeky-t... reply jt2190 6 hours agorootparentprevU.S. labor is expensive, and the app delegates the order-taking job from an employee to the customer, keeping food prices more competitive and/or keeping the restaurant profitable. (Land cost is the other big expense.) reply GuinansEyebrows 28 minutes agorootparentI wonder what the income disparity is between minimum-wage Taco Bell employees and Pepsi’s executive team is, and how much the difference has grown in the last 20 years. I would be willing to bet that the company could, in reality, afford not to replace human jobs and still pull in huge profits/pay their shareholders and corporate officers quite a lot of money without requiring a bunch of my personal information in order to buy the worst taco I’ve ever had. reply gowld 5 hours agorootparentprevNone of that justifies an app, vs an in-store kiosk or a website. reply javagram 5 hours agorootparentThe app makes it so your food is ready soon after you arrive. You can spend all the time browsing the menu and ordering before arriving at the physical location and then the food is already prepared or is made upon your arrival. Apps are used rather than PWA websites because most users find it difficult to save a mobile website to their homepage and mobile web push notifications etc add extra friction compared to native apps. reply PaulHoule 4 hours agorootparentI just typed \"taco\" in the search bar of Safari and the second autocomplete result was for the restaurant 1.7 miles from me, the third was for the tacobell.com. Easy! In contrast I struggle to find apps installed on my iPad because the icons look all the same. Apple has a leg up on Android but for me Apple's icons are mainly forgettable or meaningless and most icons from third parties are a forgettable stylized letter, forgettable anime character, or abstract icon. The colors on the default background often obscure the edges of some icons so I find it hard to spot even icons I use a lot. As a result I hide as many Apple icons as I can (What's the difference between the App Store, Apple Store, and iTunes store?) and avoid installing apps because each app I install makes it harder to find the ones I really use. The Taco Bell app would be brand destroying for me because I'd keep seeing it get in the way of finding the app I really need and would be popping up irrelevant and annoying notifications at all the wrong times -- you just don't want people associating your brand with petty annoyances. There is no reason it needs to be a PWA. People had plain ordinary web sites to order food online a decade using cgi-bin and the equivalent before there were things like Angular and React. reply jdminhbg 2 hours agorootparentYou found a link to tacobell.com, that's great, but you're like 1/10 of the way there now. You have to open it, load all the resources, give it permission to use your location so you can find which store to order from, place the order, enter your payment information, and then move to your email app to get order updates. An app caches all of that locally, has your information saved, is pre-cleared for location permissions, etc. > People had plain ordinary web sites to order food online a decade using cgi-bin and the equivalent before there were things like Angular and React. This is true. I wonder if there is any difference in the $ amount of food ordered using iOS/Android apps now vs food ordered using cgi-bin then. reply PaulHoule 16 minutes agorootparentI don't mind the minor inconveniences of the web like approving a location check. My email client provides very good tools for dealing with spam, notifications and spam notifications. If a brand wants to make their web site load excessive resources that is their loss because the site will be slow and drive people to another brand. Back in the day people were much less into the e-commerce habit and not using mobile technology so they weren't ordering food on the go. Another question is \"What relationship to people have to fast food?\" I worked at a company that did geospatial analysis such as retail location selection. We had a theory that people chose fast food because they were on the way from point A to point B so the right way to think about it was not about the density of commercial or residential development in an area but rather about the density of trips that pass by a point. I was involved in a pilot project to use touchscreens to collect data at the POS just before the mobile age made it possible to collect trip data directly. Thus my consumption of fast food is opportunistic: I eat at Taco Bell sometimes because it is in a neighborhood with a Wal-Mart, Gamestop, Petsmart, Staples, an illegal cannabis dispensary (not like I can't get better weed elsewhere), award-winning wine store, etc. I get hungry On most days I would go to the street taco stand on the other side of the parking lot. which has the best tacos I've seen outside Los Angeles but if it is Sunday maybe I go to the Bell. It's not like the scene in Demolition Man where Sylvester Stalone goes to a fancy dinner at Taco Bell because \"all restaurants are Taco Bell\" in the future. If I am traveling maybe I am driving down the freeway and see a sign for a Burger King and stop. Maybe I walk out of the Oculus at the WTC site and see both a Chopt and a BK and, even though I have a BK gift card in my pocket, the line looks really long at the BK and I go to the Chopt because it is really fast food. I can see that Taco Bell wants to develop a special relationship with me but I don't want to develop one with Taco Bell. Really I don't find it easy to install an app (until I broke my old iPad, my old iPad insisted that I log in with my Apple account password whenever I wanted to install an app despite the fingerprint scanner working just fine for everything else. I don't know my Apple account password because I keep it in a password manager and the app store was the one thing that would make me require to use it when I am on the go. When I bought a new iPad this cleared up. I go to the Bell maybe 4 times a year, it is just not worth having another app cluttering up my device making it harder to find the apps that really matter to me. yonaguska 3 hours agorootparentprevApps are used for data collection and marketing via notifications. That's the real value, not saving time for the user. The user is rewarded with the ability to avoid lines and targeted discounts. reply robertlagrant 4 hours agorootparentprevKiosks need an app as well, but also custom hardware, cleaning, etc, and customers can't order in advance. You could have a website, but lots of people currently expect an app. reply hedora 3 hours agorootparentprevThe app works better for the drive thru, and also for families with kids that like to push buttons on the giant ipad. Speaking of which, we stopped going to fast food burger places because 75% of them can’t make things like “a hamburger with no cheese or any other toppings except ketchup”, or “a cheeseburger with no other toppings”. I suspect an LLM that supports audio input would outperform most drive through window attendants. Oddly, I’ve noticed there is no correlation between speaking english as a first language and being able to understand those orders. reply PaulHoule 2 hours agorootparentI was reading Graham's On Lisp which works a nice example of Augmented Transition Networks which were popular in the 1970s for writing parsers that could parse controlled vocabularies. For that matter I remember similar kinds of grammar to control voice response applications for platforms like TellMe circa 2001. I bet it would do fine for food ordering. (This is why a non-native speaker can do this job well, you don't need to know a lot of the language to parse orders like 芥兰牛肉) reply hunter2_ 5 hours agorootparentprevFor customers to feel like they're saving time (versus having a cashier take their order) the kiosk isn't it: you can't on average discover and select on a screen faster than you can speak. The time savings of the app/website is from building the order on the way there, reducing the in-store interaction to speaking your name or order number (which is mostly constant across all methods of ordering). As for app vs website, I agree, website would be similarly good. This could be said about tons of apps. reply vundercind 4 hours agorootparentI don’t know anyone who likes having to order through the apps for fast food places, or regards them as a time-saving convenience—rather, they’re an inconvenience the places make you suffer to get what should be normal menu prices under the broader inflation rate, rather than the 300% markup above that all these places have applied to their menu prices. reply HeyLaughingBoy 3 hours agorootparentI do. Saves having to wait on a line at the drive thru. Or on a line inside the store. Nope, can't do that: they don't take orders at the cashier, so you have to use the kiosks which take even longer. So I use the app to order while at home, drive to the restaurant and grab my food and leave. reply hunter2_ 4 hours agorootparentprevOh for sure, speaking to a cashier is the quickest/best, and scrolling on a screen (kiosk or app) is slower/worst. I'm just pointing out that if the store will reduce conversations with cashiers by shifting ordering to a screen, then the app is far superior to the kiosk: order before you arrive, avoid germs, frictionless invocation of the loyalty program, etc. reply PaulHoule 2 hours agorootparentThey did some tests in this video https://www.youtube.com/watch?v=LLIj3pXOKjs and came to the conclusion that order accuracy is the best if you use the app. reply dylan604 4 hours agorootparentprevalso, there's only so many kiosks available just like there's only so many human staffed registers. This means there's potential for waiting in line. If every one has an app, there's no line. Ever. Well, except for when you show up to pick up your order and have to wait for everyone else. Of course all of that is just the icing on top of the data harvesting cake reply Rastonbury 1 hour agorootparentprevTargeted offers to make you order, think promos they offer through the app instead of blasting it on TV commercials it's much cheaper, loyalty rewards and also order for pick up to save time. I don't have taco bell app, experience is from international McDonald's app reply frogpelt 6 hours agorootparentprevCan’t tell if serious. Everything has an app. And pretty soon everything will have an “AI”. reply nemomarx 6 hours agorootparentprevlots of fast food have apps that track purchasing and give you coupons or deals that you can't get just ordering in the store now. It's like loyalty cards for McDonald's or tb. reply hunter2_ 5 hours agorootparentThis is the real answer. Price discrimination used to involve physical coupons, but now the coupons are in the app. Since apps are easier to obtain (and have in your pocket at all times) than physical coupons, the price discrepancy (between full price and discounted price) is wider than ever, to ensure revenue is sustained despite the higher percentage of discounted orders. That means high menu prices making the news, ostensibly due only to costs (inflation, minimum wage, rent, etc.) but actually due in part to app discounting. I installed the Wendy's app a while back, and by optimizing my selections, all of my orders since then have cost less than half of menu prices on average. For example, get a $3 item for $1 with any other purchase (so you tack on a $1 frosty), or buy a $6 item get another free. Taco Bell's app deals aren't quite as deep but you get the point. When tons of people are placing orders this way, menu prices must creep up to compensate. reply _DeadFred_ 1 hour agorootparentI've come to realize I don't want to put that kind of brain energy into ordering fast food. So I just stopped going. I make up some refried beans and some meat (normally chili verde) and freeze them (separately) for when I want 'fast food'. If I have to play (and learn how to maximize) some corporate dark pattern games then nah, I'm good bro. reply davio 3 hours agorootparentprevMy McDonald's app has a daily 25% off coupon. I feel like it's a glitch but it has been persistent for over a year. Two of us can eat for $13 reply yashap 2 hours agorootparentprevI’ve never used it, but I’ve used the Starbucks app a bunch. Order on the app when I’m 5-10 mins away, and can just roll up and grab my food/drink, vs waiting in line, ordering/paying, then waiting for prep. Useful when you’re tight on time or just don’t feel like waiting. I assume the Taco Bell app is similar. reply tialaramex 6 hours agorootparentprevI would imagine they let you order food and then when you arrive your food is already made? reply epiccoleman 6 hours agorootparentEssentially yes, except they make it when you arrive (at least if you pick up in the drive through). This is good design, because food from Taco Bell has an expiration date of about 3 minutes after it's cooked. It's good (well, not good, but it's food) right when it comes out, but once it cools down a bit it becomes essentially inedible. reply tiznow 4 hours agorootparentThe craziest thing anyone ever did was reheat a 5-layer beefy burrito. reply batch12 6 hours agorootparentprevThe purpose of the McDonalds app seems to be to replace cashiers. I assume the Taco Bell app was created for the same reason. reply RobRivera 6 minutes agorootparentThe goal of the mcd app is to generate growth metrics in the tech domain to inflate their ticker value through data mining, and coercion is happening through i flating prices and then offering discounts just by downloading the apps. It's absolutely rubbish short sightedness to price pressure people into using your app. Techno fascism never looked so bland reply ghaff 3 hours agorootparentprevI assume many/most of the people regularly eating at these places are very price sensitive. And, especially with demographic shifts in the West, non-premium places are going to be doing everything in their power to replace employees with automation and self-service. A lot of shoppers may grumble but they'll go with the lower prices. reply kevin_thibedeau 6 hours agorootparentprevMany of their stores don't have cashier's. You have to use a kiosk or the app. reply PaulHoule 4 hours agorootparentThe weirdest experience I had was a local burger joint I walked into that was completely empty with nobody eating there and nobody working at the counter and it didn't help that the decor was completely \"blacked out\". My first impression was that it wasn't open but I placed my order on a (ordinary sized) tablet and got my food and the people who came out to serve it were really nice. The other day I was at McD's and an elderly lady was stressin' it because she had no idea what to do. There were two cash registers but nobody staffing them and a disorganized group of people waiting for their food and not even a clear queue for the people behind the counter to see that somebody was waiting. She asked me what to do and I told her she could order at the register but a minute later I realized I was standing in front of the register waiting for my food and that probably the workers wouldn't see her. I stepped out of the way and made sure she was highly visible at the register and when I got my food I told the worker that this woman was waiting for her food and she seemed quite annoyed but I didn't feel I could take it for granted they were paying attention. reply Suppafly 3 hours agorootparentI've been to a couple of McD's where they outright refuse to acknowledge you if you just standing in front of the register. They really want you to use the kiosks, and I'm not even sure that they have people other than a supervisor trained to use the registers. The elderly lady was probably doing what mom does when presented with a kiosk, pretending to be too dumb to click on a picture of the food they want. reply HeyLaughingBoy 3 hours agorootparentprevMy local McD's won't even take orders at the register. They'll come over and show you how to use the kiosk if you're having trouble. For our rural area with relatively low population, it makes sense. But even in the suburbs nearby, Chipotle, etc., are going to online-ordering only. reply detourdog 6 hours agorootparentprevI stopped at roadside plaza and was completely cultural illiterate on how to order food. 100 miles from my home. reply gowld 5 hours agorootparentprevSomeone should invent a way to communicate without requiring a a 1:n app for every destination. They should make some kind of n:n web that connects everyone to every destination. reply shermantanktop 4 hours agorootparentThe n:n web won’t win unless the destinations make money directly from traffic. reply astura 5 hours agorootparentprevTo order food. You can also place your order on the website. reply tyingq 5 hours agoparentprevI wonder if there are any airport located Taco Bells. Those kind of captive locations are notorious for crazy prices. reply onlyrealcuzzo 3 hours agorootparentIIUC, virtually all of the crazy airport prices come from management companies like HMS Host, Sodexo, etc - which manage most of the restaurants and stores in airports. Their business model is - they big up the price of all the available spots such that no one can make money unless they charge exorbitant prices. After they capture the majority of the market, they're free to charge a ton of money. How they haven't been charged for collusion and price fixing is beyond me. Anyway, typically, if you find a huge brand like Starbucks or McDonald's - or a brand that owns all its locations (not franchised) like Chipotle, the prices will not be extreme. reply jdminhbg 2 hours agorootparent> How they haven't been charged for collusion and price fixing is beyond me. I'm pretty sure it's because they're cutting the (govt-owned/operated) airports in on the action. reply SllX 1 hour agorootparentUsually local government-owned. That shouldn’t be a factor with the Feds. reply jdminhbg 51 minutes agorootparentI think most investigations of operators on that level are done by state attorneys general. reply SllX 46 minutes agorootparentThey can be, but based off the description of events higher up the thread, I could see the Feds getting involved if this is accurate. reply wkat4242 2 hours agorootparentprevThe McDonald's in Barcelona airport used to have normal prices but recently they've started ripping off too :( reply mrgoldenbrown 3 hours agorootparentprevI was pleasantly shocked to find a 7-11 in an airport that charged their normal price for coffee. reply PaulHoule 2 hours agorootparentCertain airports (Salt Lake City, NY metro airports) make a choice to enforce pricing that is representative of outside pricing because the perception of gouging is so bad. reply seanmcdirmid 1 hour agorootparentPortland (PDX) had really good prices last time I was there. Seatac is OK, they have affordable fast food options and their starbucks is only slightly more expensive than on the outside. European airports are worse. I never spent so much on a coffee than at Zurich's airport. It was swiss markup over the usual swiss markup. reply jrm415 4 hours agorootparentprevThere is no price I would pay to eat Taco Bell before boarding a flight though. reply hn72774 1 hour agorootparentTaco bell plays nice for me. On the other hand, last time I took my family to Chipotle, my kid had liquid poo for 2 weeks with onset a couple hours after eating there. We have never been back since. That place scares the you know what out of me. reply teaearlgraycold 2 hours agorootparentprevI don't get this joke. I eat Taco Bell occasionally and it's fine on my gut. I don't order the meat, though. Usually I'm getting a black bean crunch wrap with guac instead of nacho cheese. reply raydev 1 hour agorootparentI'm with you but it's a common enough sentiment that there's probably something to it, and I recall friends having issues over the years. For these people at best it's painful gas and at worst is [worse], so there is a large group of people who can't digest a particular common Taco Bell ingredient well. I've seen speculation about undiagnosed lactose intolerance but frankly Taco Bell doesn't use that much cheese on their cheesiest items compared to say, a pizza, which is another very common food in the US and has way more cheese. reply jandrese 2 hours agorootparentprevI noted that the most expensive Taco Bell in the DC/MD/VA region is the one inside of Union Station near the Amtrak terminal. reply seanmcdirmid 4 hours agorootparentprevI’ve never seen a Taco Bell in an airport. But Seattle SeaTac has a McDonald’s and its prices are pretty reasonable compared to other Seattle McDonald’s. Actually, at airport is about the only time we eat at McDonald’s these days. reply MichaelZuo 3 hours agorootparentprevThey’re not crazy in terms of the cost structure, it’s like the $20 hot dogs in a billion dollar stadium that is half funded by taxes and half funded by loans/bonds. Someone has to pay back the loans and/or bonds plus interest… or a lot of someones chipping in with their hot dogs, tickets, drinks, etc... reply chasebank 4 hours agoparentprevIt’s only a matter of time before someone creates a Taco Bell dynamic pricing engine / realpage like price fixing service. I’m sure it’s already here considering most of these Taco Bell’s are owned by private equity groups. reply anon84873628 3 hours agorootparenthttps://www.npr.org/2024/02/28/1234412431/wendys-dynamic-sur... reply 0cf8612b2e1e 3 hours agoparentprevI am surprised I have not heard about more kinds of surge pricing or other price increases to exploit consumers. For example, raise the prices across the board $1 for an hour before the adjacent stadium opens. Do the Franchises put limits on the pricing power of individual stores? reply rurp 2 hours agorootparentWendy's recent plan to implement surge pricing in all of their stores faced a lot of public blowback. Consumers hate that kind of price gouging. Fast food is probably one of the least popular areas to have dynamic pricing because a huge part of the value prop is consistency. reply JumpCrisscross 41 minutes agorootparent> Fast food is probably one of the least popular areas to have dynamic pricing because a huge part of the value prop is consistency Absolutely. The way to do this is to have a higher menu price and then offer discounts/deals for low-volume times and items. reply 0cf8612b2e1e 1 hour agorootparentprevI was thinking more local franchise owners who might not be getting the same scrutiny as corporate. Especially if you can tap into transient audiences who are unlikely to bring repeat business anyway. reply royaltjames 1 hour agoparentprevI live up the hill from this one and spent >$70 on a few items last Friday for my gf. Insane price for satiation and eventual butt mud. reply edgyquant 4 hours agoparentprevIs it? I lived at Zella right next to here for years and just thought the prices of Taco Bell had skyrocketed since I was a kid. reply dannyw 8 hours agoparentprevHow expensive is it? reply pimlottc 7 hours agoparentprevIt does not look like any of the prices have been updated since 2023. reply icelancer 13 hours agoparentprevI've begrudgingly eaten there after a night out at Ozzie's. Such is life. reply brianjking 6 hours agoparentprevYeah, I used to live down the street from that Taco Bell. Two 3 soft taco combo meals and cinnamon twists was like $40 before I moved. It was nuts. reply smcin 7 hours agoprev- This is the same Pantry & Larder website as \"McCheapest: A site that tracks the price of a Big Mac in every US McDonald's\" (pantryandlarder.com) https://news.ycombinator.com/item?id=38980793 . Although Taconomical allows you to choose by 9 different menu items. - For comparisons to cost-of-living, see \"Big Macs and the Cost of Living Crisis\" (abc.net.au) https://news.ycombinator.com/item?id=41169538 reply jscheel 6 hours agoprevNot sure why almost all of Tennessee is unavailable, but I can say that their prices here have risen astronomically. Our Taco Bell used to have a line around the building pretty consistently, now it’s essentially empty all the time. reply phkahler 5 hours agoparentI have wondered how true this is: 1) Covid caused a huge reduction in business. 2) Businesses charged more per person to sustain themselves. 3) Post-Covid people are buying less due to the higher prices so its sticky. My local Qdoba is so expensive I won't eat there any more, but a few people do. What would happen if they dropped prices significantly and advertised that to bring people back? I don't know... There are only so many person-meals in a day, so people are eating somewhere, are they staying home? Are grocery sales up? reply ghastmaster 5 hours agorootparent1) Initially 2) Absolutely 3) False. Consumer spending on non durable goods is still rising. Disposable income is still available. Durable goods spending exploded after COVID. https://wolfstreet.com/2024/08/30/our-drunken-sailors-are-at... reply amock 1 hour agorootparentJust because spending is rising doesn't mean people are buying more things. Prices have increased, so people can be buying fewer goods and still spending more. reply gowld 5 hours agorootparentprevMBAs aren't that stupid. Either they make more money by selling less at a higher price (consider, a 20% increase in price might be a 50% increase in profit), or they are sowing losses for tax purposes. reply jandrese 2 hours agorootparent> MBAs aren't that stupid. I'm not sure. I've seen too many businesses mismanaged into the ground. It's way too common for big businesses to act like they suddenly have a monopoly even though competition still exists, and then go all surprised Pikachu face when revenue dries up. reply jprd 2 hours agorootparentprev> MBAs aren't that stupid. Citations? reply lotsofpulp 5 hours agorootparentprev>sowing losses for tax purposes. Please explain what tax purpose losing money (or earning less money than possible) serves. reply 1899-12-30 5 hours agoparentprevthe prices in tennessee are available for other menu items, just not the '3 .. combo' items for some reason reply queuebert 3 hours agorootparentDue to Tennessee's Right to Spork laws. reply t-writescode 14 hours agoprevI didn't realize there was this much price variance in Taco Bell. That's the most interesting feature, to me. reply cortesoft 14 hours agoparentI have an autistic kid who went through a phase where she would only eat McDonalds chicken nuggets... I live in a city and have 3 McDonald's pretty close to me, and the price for the same order of nuggets would vary by up to 50 cents between the 3... the cheapest place is only a half mile away from the most expensive one. It really is strange. reply wincy 14 hours agorootparentAh geez, we have a five year old who has a physical disability so ate via feeding tube for a long time, and when she finally started on actual foods had the McDonald’s chicken nuggets phase. We bought an air fryer and she prefers those nuggets now thank goodness, and has been branching out to other foods. And since we were getting her McDonald’s, we often ended up getting it, especially during Covid, and gained weight, and it was a vicious cycle. reply gscott 14 hours agorootparentYou can apparently eat them 15 years straight before it causes a problem https://www.cbsnews.com/news/british-teen-stacey-irvine-hosp... reply aucisson_masque 11 hours agorootparentI know you're joking but just in case someone miss it, even 1 meal of McDonald chicken nugget is bad for health. Its just that after 15 years you die of it, before you are very unhealthy, especially for kids. reply yathern 4 hours agorootparent> even 1 meal of McDonald chicken nugget is bad for health I think this isn't true in any meaningful way. Absolutely, making it (or any fast food) a big component of your diet is not going to promote good health. The more often you eat them, the more likely it will have an impact on your health, mostly from the increased sodium and trans fats. Consistently living with high blood sugar and higher LDL will increase your bad health outcomes. But in the example of the parent, it's not just the negative consequences of consuming fast food, but the negative consequences of not eating anything else (so lacking in many micronutrients). The good news is this makes the marginal impact of one meal over your lifetime is absolutely miniscule. It's not like each meal increases your risk of mortality a linear amount. reply whamlastxmas 5 hours agorootparentprevAlmost anything is fine in moderation. It’s honestly not that bad beyond the saturated fats, and even then an otherwise healthy and active person who eats other healthy things during the day would be totally fine having nuggets for dinner every day reply ranger_danger 13 hours agorootparentprevI don't mean to be rude but I'm just curious... did you try simply not giving them junk food? I have done some research on this and have not found any good evidence that most autistic children will actually starve themselves for any meaningful amount of time if their picky foods are not provided. reply SkyPuncher 5 hours agorootparentEvery child is different. Some kids you just need to let them burn themselves through a phase while supporting it was positive stuff elsewhere. I have one kid who will demand the giant tray of cupcakes from Sam’s Club then proceed to eat half of a cupcake twice and never think about them again (in this case, we actually did buy the giant tray for a BBQ and had left overs). We simply continue to offer her other, healthy food while she goes through a phase on something. The other kid remembers where everything is, despite being very young. She will scream about certain foods she wants (too young to talk) and work us to specific cabinets and drawers where that food was. We have to be a lot more mindful of what food we expose her to and be prepared to nudge her towards better options. Push her too hard and she simply gets stuck on that one specific food item she wants. While you’re right that kids won’t literally starve themselves, food can be a battle point in a day filled with other things. Sometimes you just have to read the kid and bend so you don’t ruin other priorities. reply crooked-v 10 hours agorootparentprevFraming it as \"being picky\" is really not helpful. As somebody on the spectrum, my (fortunately very few) food sensitivities aren't a mere matter of preference... they're the result of me experiencing visceral, nauseating revulsion at specific tastes/textures/smells. If it's the only option, I will absolutely skip a meal entirely rather than deal with it. I would probably have to be at the point of literally (not figuratively) starving to fight past that response, and even then I wouldn't be sure about keeping it down. reply SapporoChris 8 hours agorootparentNot giving them junk food doesn't equate to ignoring food sensitivities. If someone is tolerating chicken nuggets, there's a whole host of healthier food with similar textures, tastes and smells. reply forgotusername6 8 hours agorootparentYou have to battle the psychological aspect which says that they are different even if they smell, taste, feel the same. It really isn't that easy, if it was, parents of autistic kids would be doing it. There is also a reason why McDonald's nuggets are the go to for autistic kids the world over. They have been engineered over many decades to be the most acceptable taste and texture for children. reply ljf 7 hours agorootparentA friend of mine (with an autistic child) explained it as: If you give your kid a strawberry - even within the punnet the tastes and textures will vary - even mid summer some will be unpleasantly tart. If I make a sandwich it will be mildly different one day to the next, depending on the freshness of the items I put in, the brand of the ham, the spread, the bread. But junk food will ALWAYS BE THE SAME. If surprise and novelty is an issue for you/your child, then eating food like that removes so much stress for everyone involved. Yes it isn't healthy, but the meal gets eaten and no one cries. reply wrboyce 5 hours agorootparentThe difference between fresh McDonald’s nuggets and ones that have sat in the UHC/production bin for half an hour is night and day though, and that’s just the variance officially allowed by McDonald’s - don’t get me started on double-fried nuggets! reply cortesoft 3 hours agorootparentTrust me, I know. Some batches of nuggets were rejected based on being too crispy or too chewy. reply throwup238 1 hour agorootparentHave you tried ordering them \"fresh\"? It takes a few minutes longer, some cashiers won't know what that means, and they might not do it if its late and they're closing up, but I've always ordered \"fresh\" nuggets and french fries that are made to order instead of pulled from the baskets. Explaining that it's a food sensitivity issue will almost certainly get most of them to comply. It works at all the fast food places for fried items, as far as I can remember (except Seattle's Dick's). reply cortesoft 3 hours agorootparentprevAbsolutely true. My daughter will often like strawberries, but if they are too sour or mushy she will spit them out like she is literally eating poop. Going through the fruit to find ones that will be acceptable is a big part of our routine. reply account42 3 hours agorootparentprevThe psychological aspect is the part that is collogially referred to as being picky. reply astura 7 hours agorootparentprev>experiencing visceral, nauseating revulsion at specific tastes/textures/smells. It's definitely hard but this can be overcome with work. In my much younger days I had this reaction with a lot of foods that I now eat. I knew I was going to have to overcome a lot of my hangups about food in order to be at least semi-healthy, so I did in my late teens/early 20s. Before that the only thing I ate was pasta. reply cortesoft 3 hours agorootparentEveryone is different, and what an adult can overcome is different than what a child can overcome. reply astura 2 hours agorootparentExactly, it gets more and more difficult the older you are and the more established the habit is, so helping your children overcome these issues early is very important and sets them up for a healthy lifestyle in adulthood. I have a friend in his 40s who is having strokes and is vehemently unwilling to eat a single vegetable. His food adversions are much worse than any child's. I'm not saying it's easy, because it isn't. But it's very much possible. I'm also not saying all food aversions need to be worked through, however, allowing your children to have a severely limited diet doesn't set them up for long-term success. reply cortesoft 1 hour agorootparentprevTo add additional context to my other reply, I do not find this question rude, but I do get frustrated with people who seem to think our food problem is easy to solve. My wife and I have agonized for most of our daughter's life about feeding her. She has autism and ADHD, and will often forget to eat if we do not work hard to get her to eat. If she eats a food whose textures, smell, or taste trigger her, she will vomit immediately. She hates to vomit, and will refuse to eat ANYTHING after this happens (even her comfort foods). She doesn't want to be around food at all at that point. She is very small for her age and underweight (we have routine consultations with her endocrinologist on her growth, and have had countless conversations on whether we should start growth hormones with her. There are so many things to consider around that decision, it has been quite a challenge). We have a dietician that we consult with regularly, both about her eating and her growth. She, along with both our endocrinologist and pediatrician, feel that getting her calories is the most important thing, and that we can sacrifice quality for quantity, because even when she has freedom to eat whatever junk she wants, she has trouble eating enough. Both the dietician and her therapist think it is very important we never turn food into a battle, since she already has so many issues around eating that we don't want to make it worse. I appreciate your question being in good faith, but I do get frustrated when people make comments about my daughter's diet, as if we haven't agonized over this for the 8 years of her life. This is something we deal with every day, and I find it both frustrating and amusing when people think they can solve the problem in a single internet comment. Things that work for some kids don't work for all kids. reply ranger_danger 1 hour agorootparentVery well thought out response, I appreciate it. I think it's easy for people to form strong opinions about things they are shielded from the consequences of. When I made that comment I lacked all of this context of being underweight, having already struggled with this for so many years and also regularly seeing all the different types of doctors that you go to, so without that information I think it's easier for people to jump to conclusions, but I understand it can be time-consuming to add all that context every time you want to comment. As you can imagine a lot of people are quick to call out things that might look like bad parenting when they assume none of that context exists. Good luck! reply cortesoft 1 hour agorootparentThanks, and this is exactly why I took the time to type out the longer reply. I could tell from your phrasing of the question that it was made in good faith, and was not an unreasonable question in the abstract. I figured giving a more detailed reply would help you and others see all of the things that make the real situation more complicated than it might seem on the surface. reply criddell 5 hours agorootparentprevKids will definitely reduce how much they eat to the point when you take them in for their annual checkup, you will be asked why your child is underweight. The doctor will call it \"failure to thrive\". Depending on your relationship with your doctor, they might suspect neglect or abuse. Our daughter went through a phase were there were only a few things she wanted to eat. Our pediatrician said to feed her what she will eat and be patient because her tastes will change. He was right. reply cortesoft 3 hours agorootparentprevWe tried many things. Our final diet for her was based on recommendations from her pediatrician, dietitian, and therapist. While she might not have starved if we withheld chicken nuggets, she became extremely distressed and disregulated, which lead to other problems. She has moved on from chicken nuggets, but her eating pattern is still the same. She will have only one food at a time that she will eat for proper meals, and that food will rotate every few months. After a few months of only eating one type of food, she will suddenly declare she does not like it anymore and move on to something else. Her current food is actually Chicken Tikka Masala from one particular restaurant. Hopefully somewhat healthier than nuggets, although it gets expensive. reply rightbyte 11 hours agorootparentprevI guess McNuggets are better than a deficiency of calories or forced tube feeding. reply Yodel0914 11 hours agorootparentprevNot the OP, but I do have an food sensitive autistic son. He will absolutely not eat rather than eat something he despises. There are obvious moral limitations on testing his resolve, but he has skipped meals (without causing a fuss) plenty of times. reply Mountain_Skies 5 hours agoparentprevThere are two Taco Bell locations in walking distance of my home. One is independently owned and the other is a corporate location. The independent location is constantly priced higher than the corporate location. For some things like the Crave Box, the price can be as much as 67% more expensive but almost everything is at least 20% higher in price. The only exception are items that are part of an ad campaign that specifies price. In those cases, the independent franchise location fully participates. The corporate is located on a major collector street with many other fast food options while the franchise location is in the transition zone between a walkable downtown and car oriented development but both have a drive-thru. Not sure if it's just corporate policy to keep prices at a low baseline for their locations or it's due to there being competition from other fast food restaurants nearby but it's just half a mile between them so a 67% price difference is pretty strange. reply CalRobert 4 hours agoprevIncidentally, I was delighted to see a Taco Bell in Amersfoort, Netherlands, recently. Ten tacos for 14 euro isn't QUITE the 59, 79, 99 cent menu of my youth, but it isn't bad. I still bitterly miss the Chili Cheese burrito, Taco Bell's crowning achievement. RIP chilicheese.org :-( https://web.archive.org/web/20190313160757/http://chilichees... reply mordero 3 hours agoparentThere are still some Taco Bells that sell the Chili Cheese burrito (at least in the Mid-West US)! Unfortunately not as cheap as it used to be (its ~$3 here), but any time I go to a Taco Bell I always ask just in case. reply CalRobert 2 hours agorootparentGotta get some protests going reply zoover2020 3 hours agoparentprevOut of curiosity, what were you doing in Amersfoort? I've not seen it mentioned much (my grand parents are from there) reply CalRobert 2 hours agorootparentI live in Hilversum, a twelve minute train ride away! Moved here a year ago, I’m originally from California . Just went to check out the beautiful old town. We also have considered moving there. Honestly the Taco Bell was a real highlight, but that isn’t meant as a slight, I just loved Taco Bell growing up. They didn’t have Baja Blast though :-( Next time I want to check out the Mondrian museum. reply wkat4242 2 hours agorootparentprevNot the OP but it's a city that many people simply live and work in, just saying :) reply CalRobert 2 hours agorootparentBy any chance does that include yourself? I’ve been trying to find more nerds in the gooi reply wkat4242 2 hours agorootparentNope I'm Dutch but I live in Barcelona. I did use to work around Amersfoort. Lots of makerspaces and cosplay groups here if you want to geek out :3 reply whalesalad 54 minutes agoprevI just moved out to the rural countryside but god bless it we have a TB about a 5 minute drive - and the dot is green! reply twodave 15 hours agoprevI knew the Taco Bell by me had raised their prices. I didn't realize my city (and state, in general) has THE most expensive BYO cravings boxes (among other things) in the US. What the hell, Florida? reply smcin 7 hours agoparentThe 'Build Your Own Cravings Box' has by far the highest variation in price, in particular in Florida, it jumps abruptly from regions where it's identically $5.99 to $12.99. Unlike any other item. Previously discussed in 2023 on https://www.reddit.com/r/tacobell/comments/13xrssr/build_you... Seems like franchisees in different regions collude on pricing on that item. reply pimlottc 7 hours agoprevFor the umpteen millionth time, please please please do not use red/green for the scale, it is very difficult for colorblind people to distinguish, which is ~5% of the male population. Look at Colorbrewer for some alternative suggestions: https://colorbrewer2.org/ reply jdestaz 6 hours agoparentAs a developer with a red/green colorblindness, I've had to point this out to UX designers I've worked with over the years. I ended up making a small game to show how frustrating it can be to use UIs that rely on color alone to express information. https://jdestaz.itch.io/colorblind-curse reply pimlottc 5 hours agorootparentThis is great! Level 3 is so easy :) One small suggestion: keep score separately for each level so you can compare at the end and see how much icons helped. reply frogpelt 6 hours agorootparentprevVery well done. When I was color blind I just picked all 3s to get through level as quickly as possible. reply fexed 5 hours agorootparentprevReally interesting game! Well done reply Mumps 6 hours agorootparentprevThis is terrific. Thank you for sharing reply hunter2_ 4 hours agoparentprevIt basically comes down to using linear brightness (at sufficiently high contrast between steps) instead of random brightness. TFA could get away with almost any colors at all, even red to green, as long as it goes from light to dark. The problem is that they decided to use dark for BOTH ends of the scale, with light in the middle, so in the absence of color perception we can only tell whether a price is extreme or moderate. reply kansface 4 hours agoparentprevWhat percent of the rest of the population is confused by not using red & green? reply queuebert 3 hours agorootparentThe choice of color here is not obvious to me. For example, greener could mean more money, i.e. more expensive. Also red connotes debt/negative in accounting, while black is surplus/positive. If you're eating a tomato, red is good and green is bad (unless fried I guess). reply Suppafly 2 hours agorootparentprev>What percent of the rest of the population is confused by not using red & green? Presumably none if you build your site in an accessible manner. It costs basically nothing to accommodate the 1/12 of the population that has color-blindness. reply techwizrd 6 hours agoparentprevIt's about 1 in 12 or 8% of men and 1 in 200 women, but I agree. Pay special attention to the accessibility of your visual communication. Avoid red/green/yellow, and try to use color _and_ pattern if possible. reply everybodyknows 1 hour agoparentprevWhat us UI hackers would find helpful as an adjunct to this is a few polynomial coefficient terms fitted to each of the x->R,G,B color scheme mappings. Then we could generate our own scale with whatever number of steps an app requires. Or compute on the fly for a continuous scheme. reply areyousure 7 hours agoparentprevDo operating system accessibility controls help you distinguish the colors? For example, both Windows 10/11 and MacOS have \"color filters\". https://support.microsoft.com/en-us/ windows/use-color-filters-in-windows-43893e44 b8b3-2e27-1a29-b0c15ef0e5ce https://support.apple.com/ guide/mac-help/change-display-colors-easier-onscreen mchl11ddd4b3/mac reply thedougd 5 hours agorootparentThey can but keep in mind there are a variety of different types of color blindness and varying severity. For me close colors on a red / green scale are difficult to differentiate. If I enable accessibility features, it will make every photo look incorrect. iOS has an excellent option to adjust the degree of color filter. Mine is set to tritanopia (blue/yellow) the only about 5 percent intensity to give me a good balance. reply hansvm 4 hours agorootparentprevKind of. All those are able to do (however it's implemented) is map some (r,g,b) -> (r,g,b). If we pick on the common example of red-green colorblindness (of which there are many types; to have something concrete to work with, let's say all cones function at \"normal\" intensity, but the spectrum for the red cone has been shifted near to what the green cone picks up), what kinds of mappings are you able to do? The core problem is that many (r,g) pairs are equivalent, or nearly so. It's worth noting then that at least one of two properties holds: (a) Your mapping is bijective. You shift things around, e.g. by swapping the green and blue channels. Any bijective technique other than the identity will, by definition, add hue distortion, making things potentially hard to interpret. You're able to, e.g., gain the ability to distinguish red and green, but that comes at the cost of not being able to distinguish red and blue, since the confused pairs still exist in the output space. (b) Your mapping isn't injective. Many input colors map to the same output colors. One way this might be helpful is in pushing the (r,g) split toward its extremes. Maybe leave (50,50) alone, map (40,60) -> (10,90) and (30,70) -> (1,99). How much that helps varies [0], but it comes at the cost of reduced dynamic range. You traded telling colors apart for telling images with subtle variations apart. And, again, there's a hue distortion. If we don't have any good options, what levers do you have available to play with? 1. You can (ab)use the brightness channel to carry color information. This isn't very effective since brightness steps are harder to perceive than hue steps. Most implementations will instead prefer to keep the perceptual brightness the same (for the particular colorblindness described, reds will be less bright than in normal vision and greens more bright, so you need to add a correction factor). In the abstract, I do like using the brightness channel. When out at sea I'll wear strongly tinted orange sunglasses to make detecting buoys easy (everything else is dark, but the orange buoys are bright as day). 2. You can compress the (r,g) split as described above, making reds more red and greens more green. 3. You use the blue channel somehow. This is a catch-all of sorts, but if you're keeping brightness the same and not fixing the problem with just (r,g) (and, again, people want to keep brightness the same and can't fix the problem with just (r,g) [0]), then you're mixing blue into the equation. With a goal of minimizing hue distortion, no implementation does anything as extreme as my proposal of swapping the blue and green channels. They all, instead, trade some of the (r,g) discriminative ability for extra blue. Implementation details vary. I particularly like the ones which have a sequence of tests and do a little ML to come up with a nice (r,g,b) -> (r,g,b) scheme tailored for your eyes. However it's done though, you're saturating the blue channel with extra information. All mappings can be represented as some combination of (1,2,3), and mostly (3) in practice, which perhaps helps explain why the techniques aren't amazing in general. They all assume the goal is telling red from green, but your real goal is telling apart all the colors you need to tell apart in whichever UI you happen to be working with. The extra constraint of minimizing hue distortion helps with that, but you're still in a world where the colorblind filter helps for some UIs and doesn't for others, actively making others worse. God-forbid they have both off-red and off-blue buttons when the filter's solution was trading some red for some blue. And you can work around that a bit by not letting the filter be quite so strong, but that comes at the cost of not being as helpful in the actual red-green case. It's one more lever that helps a bit at the OS level. You'd really like customization for the particular UI you're looking at, kind of like what user style sheets were supposed to do for the web. [0] You don't really get \"pure\" colors from an LCD, so this is even less effective of a technique than it could be, and it really messes with the math (you want something kind of like an integral over relative response curves convolved with the LCD's spectrum). The particular flavor of red-green colorblindness described though, you can sometimes tell very pure reds from very pure greens. reply m2fkxy 6 hours agoparentprevit's also not the proper scale type for sequential data (red/green is diverging, but there is no central value defined in the linked map). reply detourdog 6 hours agoparentprevThank you for this resource. reply gowld 5 hours agoparentprevYou need to convince the tool makers, not the web app authors using their tools' defaults. reply TuringNYC 14 hours agoprevThis map needs to be scaled by rent cost/sqft for commercial real estate in the county. Otherwise, the results are really just a pass-thru of rent in the burrito price. reply gompertz 14 hours agoparentThere's always a comment like this I find. Some extra layer of data people want to see that explains the first layer, and it never ends - the data on data. Sometimes we just want to know the damn burrito prices and don't care about the excuses. reply denkmoon 14 hours agorootparentIf you want to know the burrito price, you open the app/website and look at the burrito price... you don't go to a \"Taconomics\" website comparing taco prices across a continent. As it stands, there are a number of uncontrolled variables that mean this is not an effective analysis of \"How economical is your local Taco Bell?\" reply chii 10 hours agorootparent> you don't go to a \"Taconomics\" website comparing taco prices across a continent but that's what people do, in aggregate. It's why manufacturing all went to china in the last 2 decades. reply lmz 13 hours agorootparentprevMaybe you just want to find the cheapest branch in a 10 mile radius. reply TaylorAlexander 11 hours agorootparentprevYah but if you want to click around on a neat lil web page this is great. reply smcin 8 hours agorootparentShift-click to zoom out. reply prepend 5 hours agorootparentprevIt’s hard to compare prices for 10 nearby Taco Bells in the Taco Bell app. This site makes it quite easy. reply danuker 8 hours agoparentprevMaybe Numbeo is more to your liking. https://www.numbeo.com/cost-of-living/ reply mhuffman 10 hours agoparentprevGenerally I would agree with you, but I just checked where I am at and it appears that the less the competition the higher the price. I compared a few small relatively poor towns that I am familiar with to larger ones that I know charge between 4 and 7 times as much per sqft for commercial real estate, and the more expensive tacos are in the poorer towns! Up to $0.40 cheaper in the larger more bougie towns. My only guess is that there is more competition in those larger towns. reply eonwe 9 hours agorootparentCould there be other explanations here? Like marginal cost of one taco when there might be less customers? reply lotsofpulp 8 hours agorootparentAlso, higher labor prices, since labor is also a pretty big component of cost in the restaurant business. That’s why SoCal has the highest land prices, but not the highest priced food at restaurants. reply ethbr1 14 hours agoparentprevSince when does San Diego have cheaper real estate than Wyoming? reply seanmcdirmid 13 hours agorootparentJackson Hole is a huge exception in Wyoming (also the only blue county they have). It basically compares to Aspen and other ultra high end luxury resort towns, except the Grand Tetons (and then Yellowstone) are right next door. reply rob74 11 hours agorootparentOr, to quote Wikipedia: > Jackson has become a second home for various celebrities, often due to Wyoming's income tax regime, including Sandra Bullock, RuPaul Charles, Kanye West and Kim Kardashian, Nikki Sixx, and Harrison Ford. I only knew about Harrison Ford living there before I read the article, guess it shows my age... reply seanmcdirmid 4 hours agorootparentA lot of states have no income tax, Washington and neveda for example. But they don’t have huge mountains and natural scenery (well, yes, they have that, but they don’t have any communities where you can buy into it as nicely as Jackson). reply fsckboy 14 hours agorootparentprevJackson WY https://www.zillow.com/wy/luxury-homes/ reply lotsofpulp 8 hours agorootparentprevSan Diego has a greater supply of labor than Wyoming. reply ourmandave 1 hour agoprevThe one near me doesn't do counter service unless you order through the phone app or kiosk. I finally started using the app after they screwed up my order a couple times. I park by the building, order and pay through the app, and then drive-thru. reply qazxcvbnmlp 3 hours agoprevI’d be curious to plot this next to home prices. It looks like places that are more expensive to live have higher Taco Bell prices.. which kinda checks out. reply motoxpro 3 hours agoparentWhile there is a correlation there, I think it's more about supply chain. Take a look at NM, Utah, Montana, AZ, etc. There is way more density on the East Coast. You can also see this by swapping the product to Guacamole reply jandrese 2 hours agoparentprevDoes Greenville, SC have high home prices? I was noticing how regional some high prices are. My theory is that there are areas where one person owns all of the franchises in the region and have started acting more monopolistic. Obviously there are other areas where the high prices make sense. The rich old tourist/snowbird areas of Florida? Obviously going to be expensive. Locations inside of cities that have high rents and operating costs are also likely to be more expensive. Although Minneapolis seems to have escaped the cost trap somehow. reply moonka 15 hours agoprevLooks like this only includes ones that allow you to use the app, so it is missing ones like the most expensive one in Seattle (possibly the US?). https://www.thestranger.com/food-and-drink/2023/05/16/789929... reply seanmcdirmid 14 hours agoparentAh, I should have looked through the comments before posting my own comment to similar effect. The mercer street taco bell in lower Queen Anne (aka uptown). I pass by it on the D line whenever I take my kid downtown, but we've never been there before (with Dick's a few blocks away I wonder how they survive). reply add-sub-mul-div 12 hours agoparentprevThere should be a term (if there isn't already) for the phenomenon of bias or flat out incorrect conclusions caused by sourcing whatever data happens to be easy/convenient rather than a complete or more apt data set. reply thenickdude 7 hours agorootparentSampling Bias: https://en.wikipedia.org/wiki/Sampling_bias reply ziml77 5 hours agorootparentprevWouldn't the ones you can't use the app to order from be outliers? reply wkat4242 2 hours agoprevTaco Bell here in Barcelona is ridiculously expensive and extremely poor quality. There's so many great taco restaurants here where you actually get great food for good money. I guess they're just aiming at rich tourists. reply gnulinux 1 hour agoparentI live in Boston, MA. Taco Bell is horrendously low quality, I can only imagine it's right at the border of being \"barely legal to sell\" it as food. If you eat it and don't get explosive diarrhea you should consider yourself lucky. People still eat it because (1) it used be (before COVID) actually extremely cheap (really, in Boston's expensive standards it was almost as cheap as cooking food at your home, but worse quality) now it's about the same as any other fast-food chain maybe slightly cheaper (2) some people like the comfort food aspect of it. reply varjag 2 hours agoparentprevFWIW Taco Bell places I tried in CA were poor quality as well. Guess it's one of the things you have to grow up with to enjoy. reply wkat4242 2 hours agorootparentAh ok I never visited the US. I just couldn't imagine people would like this quality so I assumed it was better over there reply foobarian 14 hours agoprevTaco Bell had felt pretty expensive for many years now. But there is just no alternative so I’m not too surprised they were able to pull it off. reply bee_rider 14 hours agoparentThere are a ton of alternatives to Taco Bell, “good, inexpensive tacos” are a whole thing, taco trucks, etc. Unless you are specifically looking for drain-o for your digestive system. reply foobarian 5 hours agorootparentNo arguing about tastes and all, but I never understood the digestive digs at TB. Maybe I'm an outlier but it's just like any other meal. I've had plenty of other tacos elsewhere and they are all tasty, but something about TB's meat is super unique and tasty to me I can't get enough of it. Mind you I can't stand all the new menu items they added past tacos in the last 30 years. But the tacos are special. :-) reply hombre_fatal 4 hours agorootparentI always assumed it was just people sharing their sensitivities to mildly hot sauces. Maybe all the extra lactose from sour cream and cheese too compared to a single slice you’d get on a burger. reply serf 14 hours agorootparentprevI live with a fan of Taco Bell. 'good, inexpensive tacos' isn't what they sell. they sell Taco Bell (TM). I could give this housemate an around-the-globe tour of pan-Latin cuisine and they would come back from their tour hungry for Taco Bell (TM). I can sort of relate -- sometimes I can be surrounded by some of the best food the world has to offer, but I crave a packet of Top Ramen. Some things offer something above and beyond what we perceive as quality, I suppose. reply wickedsight 10 hours agorootparentI love me some Crunchwrap Supreme (TM), but I also love me some fresh green chilli quesadilla with salsa verde from my favorite Mexican food place. I crave both at different times and can appreciate both of them in different ways. reply ranger_danger 13 hours agorootparentprevPeople like what's familiar. And some people's taste buds are just abnormal, and/or they are addicted to the (addictive) ingredients in fast food. reply standardUser 3 hours agorootparentprevTaco trucks/taquerias are an entirely different cuisine than Taco Bell. reply t-writescode 14 hours agorootparentprevMy usual time for Taco Bell is well past when taco trucks are open. For me it's \"the tastiest and healthiest [late night food] on the go\". reply duxup 14 hours agorootparentHealthiest? I haven’t eaten there in a while but I don’t recall thinking it was healthy at all, quite the opposite. reply sodality2 14 hours agorootparentDepending on what you get, it can actually be quite solid in macros and not super unhealthy. Particularly if you’re trying to hit protein goals, the $/cal or $/g protein is better than most other fast food places, and they had healthy alternatives to the typical grease-filled beef. reply sjoedev 13 hours agorootparentprevTaco Bell will never be the healthiest thing you can eat, but in terms of fast food, it’s not half bad. In my anecdotal experience, their quality has gone up across the board compared to years ago, and they’ve also added many healthier customization options (you have to ask for it, though). This is from their marketing, but it’s pretty honest based on my experience: https://www.tacobell.com/nutrition I pay some attention to nutrition, and I still eat Taco Bell sometimes even after cutting out most other fast food restaurants. reply t-writescode 14 hours agorootparentprevCompared to other late-night food places, such as McDonalds, Burger King, Jack in the Box, Shake Shack and similar. reply silisili 11 hours agorootparentprevThey've gotten better. I only order it rarely and was surprised last time by how much they were pushing chicken and black bean options that appeared... healthier. No doubt to save them money, but hey, two birds, one stone. reply hombre_fatal 3 hours agorootparentYeah, I went there for the first time in a decade last week and was impressed that they had a global “Veggie mode” button on their digital menu kiosk that limits the menu to only meatless items. Made it really easy to buy a bean crunchwrap with sour cream and cheese swapped for guacamole and potato. I went there so my Mexican girlfriend could try it. She was disappointed that the Dorito shell tacos are just unflavored orange tostadas, not actually a big Dorito like their ads / marketing suggests. How do yall let them get away with that? :/ reply mgkimsal 6 hours agoprev\"Price last checked June 5, 2023\" for stuff around me. reply qzx_pierri 6 hours agoparentsame reply cprayingmantis 3 hours agoprevLove this! I'd love to take the underlying data combine it with data like average income for a county and see which counties had the most affordable Taco Bell. reply poopsmithe 3 hours agoprevLove this! So interesting to see the variety of prices. Any chance we could get Puerto Rico and other US territories graphed? Thank you! reply wickedsight 10 hours agoprevHow much does this map correlate with how economical an area is in general? Say, I want to plan a road trip through the US, will hitting the green areas result in cheaper lodging, gas and food as a whole? reply mikewarot 10 hours agoparentI live in Munster, Indiana.... it's essentially the food court of Northwest Indiana. People stop here to avoid Illinois sticker shock. It turns out our Taco bell (1/4 mile south from the exit) is the cheapest in the area. They're ALWAYS busy. I assume they make it up in volume. reply EcommerceFlow 15 hours agoprevCool map. Taco Bell is no where near economical anymore though. reply reducesuffering 14 hours agoparentIdk, I'm getting a $6 combo with a drink and 3 items filled with potatoes, beans, cheese, tomatoes, lettuce, sour cream in one of the most expensive metros in the US. Not too bad. reply hadlock 9 hours agorootparentThat's not bad if you don't remember ordering those same items off of the 99¢ menu for the first ten years of your adult life reply reducesuffering 7 hours agorootparentOh of course I remember being broke and living off $1 Taco Bell and McDonalds menu. But with a drink, that would've still been $5 in 2010, in the aftermath of the 2008 recession where people had much less money and businesses had to compete on dirt cheap pricing to attract business. In my area, workers were getting paid $9/hour where they're at $20/hour right now. reply resource_waste 1 hour agorootparentprev'filled' False, you get a few sprinkles. My toddler unwrapped one of their burritos and... it was all shell. It was pretty traumatic. I stopped thinking I was getting a deal. My homemade burritos are literally 10x more filled. Taco bell is great at wrapping their shells. reply notjulianjaynes 14 hours agorootparentprevThey have this at my local expensive taco bell but it's online order only. reply lasc4r 14 hours agoparentprevThe box with a few things in it is not too bad. reply klaussilveira 2 hours agoprevIn the future, all restaurants are Taco Bell. reply fuzzy_biscuit 4 hours agoprevA lot of the data for my area is over a year old. Feels like this needs a refresh. reply resource_waste 1 hour agoparentBest of luck, these people arent making any meaningful money off their work. They are novelty websites that make the internet cool. These are good people, not paid people. Enjoy your free information, don't expect better. There is no money in it. At best, you can cheer on the owner for a few years before they realized they helped a million people save a few dollars at taco bell, and they had 15 minutes of internet fame. reply RandallBrown 14 hours agoprevThis seems to be missing the Taco Bell in the Queen Anne neighborhood of Seattle. I've seen posts saying it's the most expensive Taco Bell in the country. Maybe it's excluded because it includes a KFC? Edit: That Taco Bell doesn't allow in app ordering. reply flyinghamster 5 hours agoprevHugged to death? All I get is a spinning circle, on Chrome or Firefox. reply willcipriano 1 hour agoprevAnyone else remember when the tacos were under a dollar? 2 bucks each feels like robbery. reply jmpman 14 hours agoprevDid pinto beans and wheat (tortillas) prices increase that much at the commodity level? reply hombre_fatal 3 hours agoparentThose are the two cheapest things in Taco Bell food. Why not ask about any of the other ingredients? reply notjulianjaynes 14 hours agoprevIt seemed as if my favorite due to its cheapness fast food had become more expensive lately, and yup, every single location in my area is a red dot on that map. Bummer. reply guidedlight 10 hours agoprevWow. Most of these are really close together. The US clearly eats way too much fast food. reply infecto 6 hours agoparentAlthough the U.S. does have an abundance of fast food chains, I find these types of reductionist comments unhelpful. If you look at a map of a metropolitan area in Europe, you’d likely see several McDonald's or other popular fast food chains located fairly close to each other as well. reply wkat4242 2 hours agorootparentYes but they mainly focus on tourists here. Here in Spain we never go to fast food even for a quick lunch. There's so many better cheaper options reply resource_waste 1 hour agoparentprevAbundance shows itself. I semi-agree. We have too many people complaining about economics. Cut out fast food, and suddenly they have an extra few thousand dollars to spend. reply komali2 9 hours agoparentprevNot sure if you've been to the USA but it's not like many Americans have a choice. As an American from a smaller town it's something that depresses me a great deal. Basically every restaurant is an instance of a chain. Outside of the cities it's rare to find an original small business. Especially because the only place to get food is sometimes a parking lot \"food court\" or mall, and I guess only the franchisers can afford the rent there? Idk. Anyway that combined with the fact that fast food chains can leverage cost saving measures like putting their employees on food stamps and economies of scale mean nobody can beat them on prices, so for those that don't have a grocery store and wouldn't know how to cook a meal even if they did, for your daily dinner the 5$ McDonald's burger or whatever is genuinely your only choice unless you wanna eat gas station canned chili. Meanwhile now I live in Taiwan and every alley is chock full of original, small business restaurants all serving population that basically exclusively eats out for every meal and it's lovely. reply lotsofpulp 8 hours agorootparentWe have long had access to unlimited recipes and YouTube videos showing how to cook quick, easy, tasty meals. And we have online shopping. A small minority of Americans live so remote (either in blighted urban or rural areas) that they don’t have a choice in cooking. reply sidewndr46 5 hours agorootparentYes because while traveling from one jobsite to the next everyone has a full kitchen available to them. reply lotsofpulp 5 hours agorootparentWhat proportion of Americans are traveling from one job site to the next? Surely, the vast majority are coming home every day, and the vast majority of those presumably have a kitchen. reply throwway120385 4 hours agorootparentThe word \"surely\" is doing a lot of work for you. It sounds like you're arguing that the lived experience of an itinerant construction worker doesn't matter in your quest to ensure everyone cooks all of their meals from healthy fruits and vegetables no matter how much grind it takes to accomplish. That's probably not the best way to address GP's concerns. I once had to wait out a lease and commute an hour and a half one way when I got a new job, and I stopped at convenience stores a lot more often than I would have liked. It's not pretty but sometimes you have to compromise between many competing priorities in your life, especially if you really need the job as a springboard to something better or as a big pay increase on its own. I empathize with other people currently in a similar situation, even if it's by choice, because unless you live off of a trust fund or are supremely fortunate you will often have to take work at the intersection of your skill set and your willingness to compromise other aspects of your life in order to get ahead. reply lotsofpulp 4 hours agorootparentI’m not arguing that. komali2 wrote: > Not sure if you've been to the USA but it's not like many Americans have a choice. A small portion of Americans, such as itinerant construction workers or people with 90 minute one way commutes, might not have a choice, but to say “many Americans” don’t have the option to make simple lentil/rice/vegetable/etc meals is not correct in the sense that much of the restaurant business exists due to necessity rather than desire. The vast, vast majority of decisions to eat at restaurants are made out of convenience or preference, not time or money constraints. reply komali2 8 hours agorootparentprevWhere in the USA do you live? For my parent's house in Texas City there wasn't really a grocery store for ten miles and I'm not even sure delivery apps were available there. It's like trying to call an Uber in league city... MAYBE you'll get one in 30 minutes or something. I'm curious if you think it's because Americans are too lazy or undisciplined or stupid or something to find and cook food? Is it hard to believe there's structural issues making it harder there than in other places? I mean even if you just open a map the geographical obstacle should be pretty apparent... It's an enormous country. reply lotsofpulp 7 hours agorootparentPer Google maps, there is an HEB, Kroger, Aldi, and Walmart Supercenter within 5 miles of what looks like all the populous parts of Texas City, TX. I am not making value judgments. I just know that the availability of ingredients and knowledge of how to use those ingredients is there for the taking for most Americans. Maybe people don’t have time, maybe people don’t find it worthwhile to cook for households of 1 adult, maybe people prefer the taste of super sugary/salty/sat fat laden restaurant food. Edit: > I mean even if you just open a map the geographical obstacle should be pretty apparent... It's an enormous country The size of the country is irrelevant. All metropolitan areas have numerous options for purchasing groceries. https://www.statista.com/topics/7313/metropolitan-areas-in-t... > Nearly 83 percent of the U.S. population lived in an urban area in 2020, and that number is expected to reach nearly 90 percent by 2050. reply ghaff 3 hours agorootparentIn fairness, I live in an \"urban area\" with 2 neighbors on the surrounding 100+ acres of land. A lot of people hear the census \"urban area\" and they imagine a dense downtown. That said there is no shortage of grocery stores and restaurants (if not especially high-end restaurants) around where I live. I do suspect that most of the people who say they have no choice but to eat at McDonalds or wherever just don't want to make a meal at home. reply vel0city 6 hours agorootparentprevThere's also a lot of indie restaurants in Texas City. Sure right off 146 there's a sea of chain restaurants but head down Palmer to 6th and you'll find a number of indie places to eat along the way. reply bhawks 11 hours agoprevEven the rural and poor parts of California can't have cheap taco bell. reply complianceowl 3 hours agoprevThis is certainly something we need to taco 'bout. I'll walk myself out.... reply partiallypro 3 hours agoprevWhy is there no pricing information for Middle Tennessee? Seems odd that it's just that one area. Some sort of franchising thing? reply russellbeattie 14 hours agoprevPSA: These prices haven't been updated in over a year. Click on a dot and it'll tell you when. Same thing for the Big Mac version: https://mccheapest.com/ reply Eumenes 4 hours agoprevI grew up in a small town that had ordinances blocking fast food and drive thrus. It was really nice. More municipalities need to step up there. reply bschmidt1 4 hours agoprevThe T Bell in Pacifica might be a little pricey but has the best views and serves margaritas! reply nemo44x 5 hours agoprevIf you've ever looked into buying a fast food franchise you've maybe noticed that Taco Bell's are really expensive to franchise and startup. I'm guessing it's why we don't see saturation of them like we do Mcdonald's, etc. I don't think the expected yearly profit (generally around $100k/store) is any better either but they do have really high margins compared to others. reply drooby 15 hours agoprevThis feels like a map of real estate price reply wifipunk 3 hours agoparentTo a degree for sure. Definitely not in the context of Texas. Dallas, DFW, Plano areas are unfortunately not that cheap haha reply darknavi 14 hours agoprevThey should update the map with prices of nacho fries. It would be all black. RIP nacho fries, until next time. reply FrustratedMonky 6 hours agoprevWhy is Greenville SC in the red, but Greensboro NC in the Green. Both in south, both similar size cities. This is pretty fascinating showing how prices are skewed to what the local market will withstand. For max profits. If I'm reading this correctly. reply mminer237 6 hours agoparentUsually it's just who the franchisor is. Different owners have different philosophies. Every Taco Bell around me is operated by the same guy and is expensive regardless of economic condition, but most of the Taco Bells around it are much more reasonable despite some being in higher-cost-of-living areas. reply ndesaulniers 15 hours agoprevThey don't have data on cheesy gordita crunches. Literally unusable. /s reply ethbr1 14 hours agoparentI think I heard Taco Bell was taking the cheesy gordita crunch off the menu. It was going to be replaced by the volcano crunchy gordita cheese. reply excalibur 14 hours agoparentprevYou joke, but my orders are mostly chalupas and quesadillas, and this has neither. Also wtf is a lava taco? reply pwg 6 hours agorootparent> Also wtf is a lava taco Taco Bell's cheesy name for \"spicy\" -- although if one truly enjoys actual spicy these are just mild as far as \"spice\" goes. Runs more along the line of \"they tried to add some pepper flavor, but forgot to add any heat\". reply TulliusCicero 14 hours agoprev [–] > Restaurant is more expensive in places that are more expensive. Gee, thanks. If this factored in local cost of living this might actually be useful so you could see where Taco Bell is unusually cheap or expensive relative to the area. reply dhosek 13 hours agoparent [–] There are big variations in price within short distances that can be useful for someone who’s looking to get a cheap taco at 11p. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Discussion centers around the pricing and features of various Taco Bell locations, highlighting the unique Pacifica Taco Bell with amenities like a fireplace and margaritas.",
      "The Seattle Lower Queen Anne Taco Bell/KFC combo store is noted as the most expensive Taco Bell in the nation, with users sharing their experiences and frustrations with the Taco Bell mobile app.",
      "The Taco Bell app is criticized for its data collection practices, with users debating its necessity versus traditional ordering methods, and its role in price discrimination and targeted marketing."
    ],
    "points": 233,
    "commentCount": 254,
    "retryCount": 0,
    "time": 1726022367
  },
  {
    "id": 41505389,
    "title": "Why Not Comments",
    "originLink": "https://buttondown.com/hillelwayne/archive/why-not-comments/",
    "originBody": "September 10, 2024 Why Not Comments Why not \"why not\" comments? Not why \"not comments\" Logic For Programmers v0.3 Now available! It's a light release as I learn more about formatting a nice-looking book. You can see some of the differences between v2 and v3 here. Why Not Comments Code is written in a structured machine language, comments are written in an expressive human language. The \"human language\" bit makes comments more expressive and communicative than code. Code has a limited amount of something like human language contained in identifiers. \"Comment the why, not the what\" means to push as much information as possible into identifiers. Not all \"what\" can be embedded like this, but a lot can. In recent years I see more people arguing that whys do not belong in comments either, that they can be embedded into LongFunctionNames or the names of test cases. Virtually all \"self-documenting\" codebases add documentation through the addition of identifiers.1 So what's something in the range of human expression that cannot be represented with more code? Negative information, drawing attention to what's not there. The \"why nots\" of the system. A Recent Example This one comes from Logic for Programmers. For convoluted technical reasons the epub build wasn't translating math notation (\\forall) into symbols (∀). I wrote a script to manually go through and replace tokens in math strings with unicode equivalents. The easiest way to do this is to call string = string.replace(old, new) for each one of the 16 math symbols I need to replace (some math strings have multiple symbols). This is incredibly inefficient and I could instead do all 16 replacements in a single pass. But that would be a more complicated solution. So I did the simple way with a comment: Does 16 passes over each string BUT there are only 25 math strings in the book so far and most are <5 characters. So it's still fast enough. You can think of this as a \"why I'm using slow code\", but you can also think of it as \"why not fast code\". It's calling attention to something that's not there. Why the comment If the slow code isn't causing any problems, why have a comment at all? Subscribe Well first of all the code might be a problem later. If a future version of LfP has hundreds of math strings instead of a couple dozen then this build step will bottleneck the whole build. Good to lay a signpost now so I know exactly what to fix later. But even if the code is fine forever, the comment still does something important: it shows I'm aware of the tradeoff. Say I come back to my project two years from now, open epub_math_fixer.py and see my terrible slow code. I ask \"why did I write something so terrible?\" Was it inexperience, time crunch, or just a random mistake? The negative comment tells me that I knew this was slow code, looked into the alternatives, and decided against optimizing. I don't have to spend a bunch of time reinvestigating only to come to the same conclusion. Why this can't be self-documented When I was first playing with this idea, someone told me that my negative comment isn't necessary, just name the function RunFewerTimesSlowerAndSimplerAlgorithmAfterConsideringTradeOffs. Aside from the issues of being long, not explaining the tradeoffs, and that I'd have to change it everywhere if I ever optimize the code... This would make the code less self-documenting. It doesn't tell you what the function actually does. The core problem is that function and variable identifiers can only contain one clause of information. I can't store \"what the function does\" and \"what tradeoffs it makes\" in the same identifier. What about replacing the comment with a test. I guess you could make a test that greps for math blocks in the book and fails if there's more than 80? But that's not testing EpubMathFixer directly. There's nothing in the function itself you can hook into. That's the fundamental problem with self-documenting negative information. \"Self-documentation\" rides along with written code, and so describes what the code is doing. Negative information is about what the code is not doing. End of newsletter speculation I wonder if you can think of \"why not\" comments as a case of counterfactuals. If so, are \"abstractions of human communication\" impossible to self-document in general? Can you self-document an analogy? Uncertainty? An ethical claim? One interesting exception someone told me: they make code \"more self-documenting\" by turning comments into logging. I encouraged them to write it up as a blog post but so far they haven't. If they ever do I will link it here. ↩ If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here. Don't miss what's next. Subscribe to Computer Things: Subscribe Comments: Devam Manke Sept. 11, 2024, 1:01 a.m. I can't store \"what the function does\" and \"what tradeoffs it makes\" in the same identifier. Why not? Reply Report Comment and Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=41505389",
    "commentBody": "Why Not Comments (buttondown.com/hillelwayne)225 points by ghewgill 22 hours agohidepastfavorite229 comments narag 9 hours agoI comment everything that I think would be useful for me when revisiting the code a year later. Usually \"why\" and \"why not\". Sometimes a short \"what\" when the code is complex and it's nice to see the sequence more clearly. What's not so useful: mandatory comments. A public API should be thoroughly documented, but some shops insist on writing comments for every function in the code, even private ones and even if its purpose is so obvious that the comment just rephrases its name. This practice is not only a waste of time, but also insensitizes you about comments and teach you to ignore them. Other wasteful comments are added by some tools. I hate the one that marks every loop wiht a //for or //try comment. reply gspencley 23 minutes agoparent> I comment everything that I think would be useful for me when revisiting the code a year later. Do you code solo or do you work with a team? If so, how large is the largest team you've worked with? I used to be a dogmatic \"all comments are code smells\" person and, to a large degree I still am. But working on a very (and I mean VERY) large code-base that is actively developed and maintained by hundreds of other software developers, I have relaxed my position slightly into the \"if you need to do something weird, explain why\" ... because a large legacy system that lives in a business environment of tight deadlines means that there are often weird things that need to be done to keep things moving at a pace that the business is willing to pay for. Anyway, one of the many reasons that I argue AGAINST code comments is that the comments become part of the code and therefore require maintenance. But few people read comments unless they are stuck trying to understand something. This \"psychological invisibility\" is even enforced by the fact that most code editors will grey out comments in order to make them less distracting. And therefore, comments can easily become outdated. So I'm curious about your situation. Since you say that you like to give yourself useful context for \"future you\", what context does this process serve? Do you find it useful when working on a shared codebase with lots of other developers? Or is it something that only works well when there are few developers touching the code? reply Cthulhu_ 7 hours agoparentprevFor some reason a lot of syntax highlighting color schemes de-emphasize comments, making them low contrast, which is probably because a lot of mandatory / generated comments are low information. Get rid of mandatory and generated comments, and change your color scheme to make them a bright neon colour instead (on a dark theme) to draw the attention, because IF something is commented then it's important. reply dizhn 5 hours agorootparentI disagree with this. Comments are important but not every time you're in that part of the code. You might already know from just the code what's going on, or have already checked out the comments. There's no value to them always being emphasized. That would be like reading all NPC dialogs every time in an RPG. reply bluGill 4 hours agorootparentIf there is a comment is should be important enough to read it every single time you are near that area of code even if looking for something else in the file! They should be a reminder of something important and not obvious in the code - otherwise I'll just read the code. Note that I distinguish comments from API documentation even though they are often both in the same code and use the same comment syntax. reply qrobit 1 hour agorootparentIf there is something really important worth rereading over and over again, put «NOTE: » before it and let your editor highlight it for readers of your code reply Attrecomet 3 hours agoparentprev> Other wasteful comments are added by some tools. I hate the one that marks every loop wiht a //for or //try comment. Oh god, that's horrible, what kind of tool does that?! reply skipkey 5 minutes agorootparentIt was pretty common say 25 years ago when you would be developing in a terminal. When you were limited in the number of lines displayed, it sometimes made it easier to follow the code when functions and control structures were large. I know I had coworkers using brief configured to do that. reply narag 1 hour agorootparentprevI don't know, the guy is no longer around. Maybe some code-completion tool (write \"for\" and it completes the syntax) because it's all over the place. reply bbarnett 55 minutes agorootparentEventually, we find out he wrote it all manually. reply cjfd 9 hours agoparentprevYes, completely agree. Also, too many comments make it difficult to see what is in a class/function. If the comments make a class/function that would otherwise fit on one screen no longer fit on one screen there is a readability cost to this. reply BeetleB 3 hours agorootparentCollapsing/hiding comments should be a required feature in editors. In Leo[1], you can make nodes out of them and just hide them. If Emacs weren't so good, I'd be using Leo. Similar power when it comes to extensibility, but extended via Python, not elisp. [1] https://leo-editor.github.io/leo-editor/ reply exe34 8 hours agorootparentprevI wish they would put the comments before the function name in python, as otherwise the useful code is separated by useless verbiage. I also wish comments would include examples of the shape and dtype of inputs and outputs. reply gwervc 7 hours agoparentprev> Usually \"why\" and \"why not\" Related anecdote: yesterday I was on a code portion of a personal project with a \"Is this really useful?\" comment on a line that seemed it could easily be removed. I tried to use the newer and cleaner class instead and the particular old way was indeed needed. So I appended a \"=> yes!\" to the existing comment as well. I'm glad my former self documented the interrogation. At work, especially on bugfix, I often write a one or two lines comment with the ticket issue number over a non-obvious change. reply harry_ord 3 hours agoparentprevI find the why and what are so useful. If I know why what the code is aiming to achieve, it makes rewriting it or fixing a bug much easier reply renhanxue 20 hours agoprevI saw someone quip (on twitter, I think) many years ago something like: \"A junior engineer writes comments that explain what the code does. A mid-level engineer writes comments that explain why the code does what it does. A senior engineer writes comments that explain why the code isn't written in another way.\" (except punchier, of course. I'm not doing the quip justice here) reply kwhitefoot 9 hours agoparent> writes comments that explain why the code isn't written in another way.\" Exactly! I have written code that required comments five times as long as the code itself to defend the code against well meaning refactoring by people who did not understand the code, the domain, or the care needed to handle large dynamic ranges. I have also written substantial functions with no comments at all because it was possible and practical to name the function and all the variables so that the meaning was clear. reply yxhuvud 6 hours agorootparentI've also written that kind of comments, and then promptly had it refactored by people not reading comments. reply klyrs 34 minutes agorootparentIt's true! If you come across a well-documented and complex piece of code, it's always easier to delete it all and write a simpler (and less correct) replacement than reading all of that code and documentation. And since your replacement is simpler, you're free to delete the inapplicable documentation that you saved so much time not reading! If you didn't back that documentation up with some tests to reflect the necessity of that complexity, this is an avoidable tragedy. But if the junior then comes in and deletes your tests because they don't pass, that's a firin'. reply cesaref 9 hours agoparentprevJunior programmers tend to either document nothing, or document everything. With experience you realise that you just want to document the unusual stuff, and as you get more experienced, you realise there is less and less unusual stuff, so the amount of comments drop down. So, less comments wins out, but faced with a code base without comments you have to inspect it to tell the difference between a beautifully crafted piece of brilliance or a totally flaky codebase written by a bunch of newbie hacks. reply andoando 5 hours agorootparentYou have to consider who the comments are for. Senior engineers comments are more useful for senior engineers, but the junior comments will be more useful for junior engineers. reply seanw444 4 hours agorootparentI don't know. Senior engineers may understand the point of the code already, and \"why it's not written a different way.\" The comment could be explaining to the less-experienced devs why they shouldn't waste time on another intuitive-at-first-glance approach. reply lawn 7 hours agorootparentprev> and as you get more experienced, you realise there is less and less unusual stuff, so the amount of comments drop down. This is a common experience when I use a new language or framework. At the start I comment a lot of things because I think it's useful for future me, but as I learn more I realize that many of the comments are redundant so I end up removing them. reply abc-1 20 hours agoparentprevI do all of the above. Summary comments are incredibly helpful and have been validated by empirical research. Too bad many have drunk deeply from the Clean Code koolaid and can’t be saved. reply hughesjj 18 hours agorootparentI got some flak at a prior job for saying I had some quibbles with clean code (a few years after I had read it), and I'm glad this opinion is more popular today. There's so much cargo culting hype with \"best practices\" and style, I hate it. Same with how overly dogmatic people were with OOP paradigms when it came out (remember using anonymous interfaces to pass a function around in java?). Same with the functional backlash to that. It's fun and enlightening to go ham on any particular style/framework/philosophy, but actually living by dogma in prod gets kinda dangerous and imo is counter to the role of a senior+ engineer reply answerheck 45 minutes agorootparentCopy that. CC has done a lot of harm in some ways. reply AtlasBarfed 13 hours agorootparentprevThe very term \"best practice\" is such a loaded term. It implies: 1) empirical measurement compared to a large number of alternatives ... the empirical measurement or study is never mentioned: because they do not exist 2) the best practice is valid in all measurements and criteria of comparison: performance, elegance, simplicity, correctness 3) since there is no data, the reasons for why the practice was designated best are rarely even explained 4) nor are the circumstances or individual or source of \"best practice\" detailed 5) it will always be the best practice: it is the BEST! It CANNOT be improved. So we have an unsubstantiated, unargued, unsourced, non-authoritative, exaggerated declaration in virtually every case of \"best practice\" reply Prickle 20 hours agorootparentprevI was taught to not leave comments in finished code. I have regretted following that lesson ever since. reply acbart 20 hours agorootparentIf code were ever finished, then perhaps this would make sense :) reply hi_hi 9 hours agorootparentprevIf you're commenting out code, don't If you're commenting about the code, do. There's a huge difference in the value between one or the other. reply spacechild1 20 hours agorootparentprevWho taught you that!? reply Prickle 14 hours agorootparentI was specifically taught that good, readable code could explain itself; that it would make comments redundant. Therefore, comments should only be used for things like psuedocode, or as help for fellow developers during dev. Then, they should be removed once the code is done. But yea, not a good idea. reply thayne 14 hours agorootparentThis is an example of taking something that is contextually good advise, applying it to all situations, which turns it into bad advise. If you can make your code more clear, so that comments aren't necessary to explain what it does or how it works, that is probably (but not always!) something you should do. But that doesn't mean you shouldn't have comments. At the very least there should be comments explaining why (or why not) things were done a certain way. reply RandallBrown 14 hours agorootparentI have almost never run into a situation where a comment was better than a well named function or variable. It happens occasionally, but it's usually a sign that I'm doing something wrong. reply moring 11 hours agorootparentThis is fine. A problem arises when you assume that this will always be the case, for all developers, and then mandate that they omit comments, _without checking if your assumption is true for their case_. IMHO that rule can be generalized: Whenever you make rules for other devs, make sure that the assumptions on which those rules are based are true, lest you interfere with their work in a negative way. reply netdevnet 8 hours agorootparentprevA line of code can tell you what it does but not why. Unless you are on a newish codebase, you will likely need comments to explain why certain decisions were made reply bccdee 3 hours agorootparentprevReally? There's a perfectly good example in the article. RunFewerTimesSlowerAndSimplerAlgorithmAfterConsideringTradeOffs() That's a horrible way to name a function. Function names should be short, punchy, and unambiguous. They should create a simplified abstract narrative, and all the details should be put into the docstring, so that they can be easily accessible without having to (a) be squashed into an identifier, or (b) be repeated every time you want to call the function. reply atoav 11 hours agorootparentprevThere are indeed those situations where a comment would not increase the clarity of the code. But one shouls be careful not to mentally think of this as a zero sum dichtomy, where you either have well named functions XOR you have comments, because in reality choosingn both is often the golden path to success. The danger is of course that code that is totally obvious to you now will take far more time to become as obvious later, be it to your future self or to your psychopathic lunatic co-worker who knows where you live. So very often code can be made more readable by adding comments, even if it is just saying the same thing with other words, just by reducing ambiguity. Comments can also bridge higher level concepts in a good way, e.g. by you explaining in a few lines how a component fits into a concept that is spread out over multiple files etc. In the end code is text and like regular prosaic text you can both make it harder to understand by not mentioning things or by mentioning too many or the wrong things. This is why it is not irrelevant for programmers to be good and empathic communicators. Sure in the end readability doesn't matter to the computer, but it certainly matters to all people involved. reply exe34 8 hours agorootparentprevwell-named works great while you're writing the code. come back to it in a few years, or hand it over to somebody new, and you would realise that what looks like a good name to you means nothing to somebody else. reply atoav 11 hours agorootparentprevThat is like saying: \"A perfectly good road needs no road markings\". The point of of good comments is that they make the code faster to read and less ambigous. While good code should indeed already be readable and unambiguos, I have rarely seen code that couldn't be made even easier to understand and faster to parse by writing the appropriate comment. But of course you will have some individuals who think it is cooler not to, and they are probably the same people who think use after free bugs can be avoided by the shere willpower of the solo-male-genius that they are. reply dspillett 7 hours agorootparentprev> I was specifically taught that good, readable code could explain itself; that it would make comments redundant. Good readable code removes the need for comments about what the code does, if the working of the code needs extra explanation then perhaps it is being too clever or overly terse, but there are other classes of comment that the code simply explaining itself can't cover. Some of my comments cover why the code does what it does, perhaps linking it to a bigger picture. That could be as simple as a link to work ticket(s) which contain (or link to) all the pertinent details, though I prefer to include a few words of explanation too in case the code is separated from whatever system those work items are logged in. Many comments state why things were not done another way. This can be very similar to “why the code does what it does” but can be more helpful for someone (perhaps your future self) who comes along later thinking about refactoring. These can be negative notes (“considered doing X instead, but that wouldn't work because Y or interaction with Z” – if Y and Z become irrelevant that future coder can consider the alternative, if not you've saved them some time and/or aided their understanding of the bigger picture), helpful notes for future improvement (“X would be more efficient, but more complex and we don't have time to properly test the refactor ATM” or “X would be more efficient but for current use patterns the difference would be too small to warrant spending the time” – the “but” parts are not always stated as they are usually pretty obvious). A comment can also highlight what was intended as a temporary solution to mitigate external problems (“extra work here to account for X not being trapped by Y, consider removing this once that external problem is fixed” or “because X should accept Y from us but currently doesn't”). reply f1shy 6 hours agorootparentprevIF the code is self-explanatory, then the comments are redundant, and is ok to delete them. But from time to time, there are things that are at least not so obvious in the code. Then is good to leave a comment. That could be used to see how good a language is for specific tasks. If you need to write lots of comments, maybe you have the wrong language. reply netdevnet 8 hours agorootparentprevCode explains the what but not the why. And even then, the what might not be so clearly obvious. This is one of those blindspots devs have in that they believe their code to be good and obvious to everyone but in reality it is not even good and obvious to their future selves who will be the ones maintaining that code reply dodos 11 hours agorootparentprevI had a professor in college who would grade you down if there were any comments in your code. reply dspillett 7 hours agorootparentSounds like a concrete example of the phrase “Those that can, do. Those that can't (try to) teach.” Far from true for all teachers, of course. That and “those who actually use power are likely to be those who shouldn't have been given it”! reply forgotpwd16 9 hours agorootparentprevPerhaps professor was fed up with over-commenting (comments made up a large part of submitted code), especially if comments were like in https://news.ycombinator.com/item?id=41506466. Unless the course is \"practical software engineering\" or similar, that good programming practices are a focus, and if the why/why-not parts can contribute to better assessment, an associated paper can be asked. reply imp0cat 13 hours agorootparentprevCode is never finished, only abandoned. reply readthenotes1 20 hours agorootparentprevI have regretted when you fail to follow that lesson. I did a survey once in about 3/4 of the comments were either wrong or useless. Examples: //Add 1 to x x+=1; //Add 1 to x x+=2; //Seconds per normal day x = 86400; -- \"Why not\" comments are incredibly valuable except they suffer from explanatory decay as much as other comments. The hope behind Intention Revealing Names is that the dissonance will be too great for the subsequent developers to ignore when they change the code. Of course, that isn't always true. reply gus_massa 18 hours agorootparentx = 86400 If I were forced not to write comments, I'd write that as x = 24 * 60 * 60 and let the compiler optimize that. reply hansvm 5 hours agorootparentWhen writing it inline, I like this approach. I like even better when these things have names. Something like `std.time.s_per_day` or `time_utils.s_per_day`. Then in the one place they're defined, use a pattern like the above to make them easy to reason about. reply DanHulton 2 hours agorootparentprevOne step further: DAY_IN_SECONDS = 24 * 60 * 60 reply jjav 17 hours agorootparentprev> I did a survey once in about 3/4 of the comments were either wrong or useless. > Examples: > //Add 1 to x > x+=1; If 3/4ths of comments are like this, maybe show a sampling of public source code (e.g. from github) that shows how prevalent comments like this are in any real codebase. I've been programming since 1982 and have never seen this type of \"add 1 to x\" comment in real code, outside chapter 1 of some intro to programming book. reply dllthomas 12 hours agorootparentI once came to a complicated, multi threaded C++ program and saw: using namespace std; // using namespace standard reply f1shy 6 hours agorootparentI'm working RIGHT NOW in a codebase where before each function definition there is a comment \"// Function\" reply bccdee 3 hours agorootparentI wonder if that's intended for a specific workflow, e.g. ctrl-f \"Function\" enter-enter-enter-enter to cycle through all the functions in the file. That's the only reason I can think of for writing those comments. reply g-b-r 19 hours agorootparentprevAn outdated comment is at least a very strong signal that the code might be wrong And, you definitely had little experience with under-documented code reply GianFabien 18 hours agorootparentFor me the quality of comments, somewhat based on the metrics that @renhanxue mentions, is a code smell. If code is poorly commented (by my standards) then I treat the actual code with suspicion. reply dspillett 7 hours agorootparentprev> An outdated comment is at least a very strong signal Also: if the code and the comments appear to disagree, there is a reasonable likelihood that both are wrong in some way. reply appplication 12 hours agorootparentprevYes I write comments like a maniac. Long doc strings that are informally written. It’s more important for me to say what I need someone else (or future me) to know about a function and its context than it is for me to have some beautiful, sterile 300-line autogenerated soulless docstrings. reply kazinator 33 minutes agorootparentI write detailed git commit messages like a maniac. Git commit messages are better than comments, because they tag the specific baseline of code where a decision was made, and you can write multiple paragraphs to explain something (including all the \"why not\"), without cluttering the program text. The problem with comments is that they also pertain to a revision that existed around the time they were written, but they stick around, pointing to newer revisions, perhaps falsely. They add clutter. Unless you use a folding editor, comments can separate pieces of code so that you see a smaller window of the program. One line of code can be touched by many, many commits. Each of those commits should have something to say about that, and all that talk cannot possibly be put into a giant, ever-growing comment next to that line of code. In regard to my previous point, a lot of that talk won't even be relevant to the current version of that line! I've taken the view that the thing I'm developing is a git repo, not the source tree. A source tarball is just something for building and deploying, not for development. If someone wants to understand why something was done, they must use the repo, and not a source tarball. If they insist on just working with the source snapshot, but ask questions that are answerable in the git history, I cannot support them. reply answerheck 42 minutes agorootparentprevYes, we should all be kinder to both others and to future me (you). Do we know who will be maintaining this code? Obi-Wan meme: Of course I know him. He's me! reply cjfd 9 hours agorootparentprevThis 'empirical research' is highly doubtful. The first question to ask is what the code with summary comments looked like. While summary comments can sometimes be helpful, this is mostly the case in functions that are relatively long. A question that always arises in that case is whether it is a good idea to split them instead of commenting. reply __MatrixMan__ 20 hours agorootparentprevI like two of the three, but what is the advantage of commenting what the code is doing, when you can use a Trace or Debug message for that instead? reply PlunderBunny 20 hours agorootparentNot the person you are replying to, but I would say that: - The code 'tells you' what it does - The comment for the code tells you what the author intended it to do. The gap between the two is where bugs can be found. reply __MatrixMan__ 17 hours agorootparentI've got no trouble with that perspective, but wouldn't a log message scratch the same itch, plus more? reply PlunderBunny 14 hours agorootparentI've never worked in a company where the commit log message wasn't just a link/reference to something in a bug tracker. I feel like a 'what this block of code does' comment is different from 'what is this change, and why did I make it' commit message. reply porridgeraisin 13 hours agorootparentThey meant logging-logs in the code itself: log.Debugf(\"Foo is: %#v\", ...) //You think: this is probably filtering code log.Debugf(\"Foo without X is: %#v\", ...) reply readthenotes1 19 hours agorootparentprev\"The comment for the code tells you what the author intended it to do.\" Not quite. The comment for the code tells you what the author of the comment understood the code to do when hen wrote the comment. reply kristiandupont 13 hours agorootparentNot the comments I write. I don't write what the code does or even what \"I understand the code to do\". I explain choices, especially ones that the next developer or my future self is likely to misunderstand when looking at the code. reply ykonstant 12 hours agorootparentprev>when hen wrote the comment. I've always said coding is a cottage industry! reply PlunderBunny 19 hours agorootparentprevIt could be that too, but I think that presumes an order - that the comment was written after the code. If the comment was written before the code, then it would describe what the author was trying (intended) to do. Which also implies an order of course. reply g-b-r 18 hours agorootparentprevYeah, absolutely check who wrote a comment before relying on it It's luckily rare for people to add wrong comments, though (and those who do should be publicly fustigated). By the way, please never state something as it were the truth if you're not sure that it is. Saying \"I think\" is perfectly fine, and might save people days of investigation. reply edflsafoiewq 20 hours agorootparentprevA function generally tells you what it does three times: once in the doc comment, once in the function name, and once in the body. reply XorNot 10 hours agorootparentprevThis is what I tell people: if you're writing a one line code comment, write a debug log message instead. There's vanishingly few cases where this extra logging statements will ever be a problem and they can all be handled autonomously if they ever are - but it will save everyone else a ton of time in deployment. reply kristiandupont 8 hours agorootparentYou must have extremely busy logs, then? Do you also do this with all the code that runs in loops? reply XorNot 8 hours agorootparentDo you routinely run production services at debug or trace log level? The point is there's a big difference between \"we've got a problem, we need to add logging and redeploy to try and isolate it\" versus \"we might have a problem, bump the logging level up on that service to see what's going on\" (which with the right system you can do without even restarting). reply seanmcdirmid 20 hours agorootparentprevWe could have the best of both worlds if comments could be easily hidden, or better yet, just additional meta-data on rich text code. But nope, we can't get away from ascii. reply bccdee 3 hours agorootparentYou don't need to abandon plain text to hide comments. Comments are detectable with a regex; an extension to hide comments would be trivial to make in most editors. I think it's not a common feature because people generally just don't want it. Rich text code would create so many problems, too. You get locked into a special editor. You need special version control. Grepping becomes difficult. Diffs become difficult. You'd need a whole separate ecosystem, and for what? We have treesitter; we can already treat code like data. reply readthenotes1 20 hours agorootparentprevTrouble with comments is that they drift from the code over time because most people do not update the comments - - based upon my surveying production code bases. If they are hidden, it will drift even quicker and become even more useless faster reply yen223 11 hours agorootparentI would urge developers to err on the side of too many comments over having too few comments, even if there's a risk of them going stale. I can deal with drifting comments, but I can't deal with missing comments. reply wtetzner 4 hours agorootparentI've found the opposite. Misleading comments can be far worse than no comments. reply shiroiushi 18 hours agorootparentprevThis is caused by poor or nonexistent code review practices. Reviewers should be ensuring that related comments are updated if code functionality is changed. reply seanmcdirmid 19 hours agorootparentprev...and the main reason people don't like comments is because they clutter up the code that gives them the truth of the matter. But yes, if they aren't in your face forcing you to look at them rather than the code, then they are slightly more likely to not be ignored when the code is changed. It would be nice if they could be like footnotes, or boxed out-takes, that could be pushed to side notes. We have had the typography, even if it was just markdown with a rendered code reading mode. reply convolvatron 19 hours agorootparentprevi think in sean's proposed world we'd have metadata about that too! the comment in the context it was written in would be available, as well as all of the surrounding changes that potentially invalidate it. as well as potentially a whole discussion thread about what they meant when they wrote it, and suggestions about how to change it. reply kmoser 15 hours agorootparentprevYou just need a \"hide/show comments\" extension for your IDE. (I would argue that such a feature should already be built in.) reply steveBK123 19 hours agoparentprevThere's also the comments that tell you why the code does something obviously stupid but needs to continue replicating this stupid behavior because something else depends on it behaving this way. reply saurik 12 hours agorootparentThat sounds like \"explain[ing] why the code isn't written in another way\" where the other way is \"the way you are right now thinking it should be as you read this: the way that isn't obviously stupid\". reply steveBK123 7 hours agorootparentI'm thinking less in \"way the code is written\" and more \"the way the code works\". That is things like cases it doesn't support, or weird results in some cases. One example is a shop I worked many years ago that had a function which did time conversion on date times without adjusting the date portion. Obviously, objectively, wrong. However the callers to this function expected this errant behavior. Actually changing it to behave correctly would require a coordinated change. Putting that comment in place prevents a new hire from going \"oh this is obviously wrong!\" and fixing it, causing an outage. reply paulryanrogers 18 hours agorootparentprevThis is the way :mando: If it's not answering why or at least providing a very concise how to very verbose code, then it's just adding noise reply wrwatson 11 hours agoparentprevI think of comments as an apology to the engineer reading the code. I'm apologising because the code isn't obvious, or the language not sufficiently expressive, or the good-idea-at-the-time no longer is. Ideally I wouldn't need to write many comments, but I often find myself sorry things are not simpler. reply atoav 10 hours agorootparentIn the end a comment should be like a shortcut that allows people to understand your code faster or a reminder why certain choices were made. You are right in spirit to say sorry when the code cannot be self-explainatory, but consider that sometimes even if the code is self explainatory, comments can help your reader to see the grand picture faster or to avoid interpersonal ambiguity by allowing you to use two different ways of phrasing a thing. reply belorn 18 hours agoparentprevI find that purpose of comments is to give context to the written code. Sometimes the context explains what the code does, and more often it explains why, but the best comments gives me the reader an insight so that I can have a intuitive feeling for what the author are writing. reply anotherevan 17 hours agoparentprevCursor cursor = getCursor(); // Cursor. reply erik_seaberg 17 hours agorootparentI have seen a linter that requires class User { public String getName() { ... } } to have a doc comment explaining what getName does and also what it returns. Before git, they sometimes needed to see who wrote it and when. reply pjerem 10 hours agorootparentWell, so, does this method return the login name of the user ? Or is the \"name\" field in fact a company convention to return firstname and lastname concatenation ? Oh and if it’s that, what will be the concatenation order ? It doesn’t seems like it’s a parameter but is it configurable somewhere ? Is this automatically defined somehow ? Is it hardcoded ? Or is there a name field in the database and if yes, what does it represent ? Etc … reply franciscop 5 hours agorootparentWithin the code you'd have `return this.user.firstName + ' ' + this.user.lastName;` or similar, which can explain the details of how it works when you look into it. reply DanHulton 2 hours agorootparentprevI'm not super opposed to that, honestly. Doc comments are coloured differently from code, and really help you scan through a file quickly to find the next function. It honestly doesn't take more than a couple seconds to write, and (I find) it really helps readability enough to be worth it. reply MyFirstSass 5 hours agoparentprevTrue! Also i sometimes also do \"yes this looks weird, but it's because : somebugtracker.issue\" reply mschuster91 20 hours agoparentprev> A senior engineer writes comments that explain why the code isn't written in another way. And C-level engineers write a comment \"X hours have been wasted on refactoring this code. Should you decide following the example of your priors, please increment this counter.\" Sometimes, even what appears to be an utter hack job actually is the best you're gonna get. reply incontrol 11 hours agoparentprevWhen I feel the need to add a comment, I convert that piece of content into the named function. reply kaoD 9 hours agorootparentSo how do you document what is not in the code? reply tubthumper8 4 hours agorootparentObviously in the function name! void vendorReturnsDataInInvalidFormatAndButShouldBeResolvedByVendorOpenTicketXYZ_123ExpectedResolutionQ42024(String input) {} /s reply david-gpu 7 hours agoparentprev> A senior engineer writes comments that explain why the code isn't written in another way I suggest providing that sort of high-level decision information in a separate design document. This sort of documentation is written before the code and is presented/discussed with management and peers. That is how it was done at a couple of companies I worked at and it was very effective. Naturally, this was done at the feature level, not on a function to function basis. reply bccdee 3 hours agorootparentThen you end up with a design document that corresponds to the code as it existed conceptually before v1 of that code was even written. By the time v1 is written, the design doc will be slightly out of date; by the time the code reaches v3, the document is more than 2 versions out of date. The nice thing about comments is that they're in the code. You can't update the code without at least looking at them; that's more visibilty than you'll get from anything else. reply david-gpu 1 hour agorootparentDesign documents were updated as the code changed, obviously. Changing the code without updating the documents would not pass code review if somebody was shortsighted enough to try. reply solidninja 7 hours agorootparentprevThat still needs discipline though - or you end up with N half-finished Confluence pages describing the intention behind the design, all of which are now out of date (and naturally in completely different places). The best way I've seen to keep track of changing things is to have the design linked to the ticket somehow (and if it's a link, then that needs to be a permalink to something that will not go away in a year's time). reply anhner 6 hours agorootparentprevI understood that to mean \"I've tried doing it x way and it didn't work because y.\" rather than the functional part. At that point, why not keep the documentation together with the code? reply zo1 2 hours agorootparentprevI really have to push back against this \"design document\" stuff. Unless you're writing some sort of uber-complicated, mission-guidance-systems-level code with multiple standards and audit compliance and and and, then you don't this. Alternatively, if your org or reach for this feature is so large that you need to communicate and decide its internals with a lot of people and as such require something like a design document, then you've already failed and you have way too-many cooks in your kitchen. Design by committee is always doomed to failure. Thirdly... if you need a design document to communicate your feature to \"management\" then that means you don't have autonomy to design this feature as an expert, and again, you have too many \"fingers\" in your pie. Does this mean you should go into a basement and design your feature like a hermit? No, but a design document shouldn't be your answer, it should be a better team and process around you, as well as a clear business specification. reply david-gpu 1 hour agorootparentWe did and it worked great. The process led to not only better designs, but also better visibility for all stakeholders, from technical leads to individual contributors. I'm talking about products that shipped on billions of devices, products that you have most certainly used. It worked. reply kazinator 26 minutes agoprevMost comments belong in the git log message. That's where you want to discuss the \"why not\". You have all the space you need in order to do that, without cluttering the code. The log message will accurately pertain to the change made at that time. When commits are rebased, the log message must be revisited and revised. Changes can disappear on rebasing; e.g. when a change goes into a baseline in which someone else made some of the exact same changes in an earlier commit, so that the delta to the new parent is a smaller patch. In my experience, commit messages stay relevant under most rebasing. Comments are (largely) an obsolete version of version control log messages. In the 1980s, there was a transitional practice: write log messages, but interpolate them into the checked out code with the RCS $Log$ thing. This was horrible; it practically begs for merge conflicts. It was understandable why; version control systems were not ubiquitous, let alone decentralized. You were not getting anyone's RCS \",v\" file or whatever. Today, we would be a few decades past all that now. No $Log$ and few comments. Mainly, the comments that make sense today are ones which drive automatic API documentation. It would not be reasonable to reconstruct that out of the git history. These API comments must be carefully structured so the documentation system can parse them, and must be rigorously maintained up-to-date when the API changes. reply thehappyfellow 11 minutes agoparentHow am I supposed to be aware of a commit message specifying the “why not” when reading the code later down the line? I could easily imagine somebody refactoring code to an obviously better version, finding out it doesn’t work for subtle reasons, running got blame and cursing the person who left that information in a commit message instead of a comment. reply anotherevan 17 hours agoprevI think the favourite type of comment I've ever left in my code follows this template: DEAR MAINTAINER: This code is the way it is because of . Once you are done trying to 'fix' this, and have realised what a terrible mistake that was, please increment the counter as a warning to the next person: total_hours_wasted_here = n I'm not the original author, but have gratefully used it once or twice, and been amused when there was a single line commit incrementing the counter. reply tombert 3 hours agoparentI wish I had had this years ago. I one time wrote this fairly elaborate SQL generation thing that required pretty liberal use of recursion in order to fulfill all the requirements. It ended up being a lot of mutual recursion and the code was admittedly kind of messy but it was a necessary evil to do everything asked of me. A more senior engineer ended up taking over the codebase, \"fixed\" all my code to be this iterative thing, made a point to try and lecture me about why recursion is bad in an email, only for his code to not actually do everything required and him reinventing effectively everything I did with recursion. In fairness, he did actually apologize to me for some of the comments he made, but if I had thought about putting this comment on the top maybe we could have avoided the whole thing. reply tombert 3 hours agoprevMy rule of thumb has been \"comment stuff that isn't the naive solution\", basically anything that would make someone think \"wtf is this\" the first time they read it. My biggest headache right now has been getting high-throughput with SQL and as such I've had to do a lot of non-obvious things with batching and non-blocking IO in Java to get the performance I really need, and as such a of the \"obvious\" solutions don't work (at least with a reasonable amount of memory). Consequently I've been pretty liberally commenting large segments of my code so that someone doesn't come in and start bitching about how \"bad\" my code is [1], \"fix\" it, and then make everything worse by rewriting it in a more naive way that ends up not fulfilling the requirements. [1] I have since stopped doing this, but I'm certainly guilty of doing this in the past. reply Timwi 10 hours agoprevThis is just a special case of a broader, more general advice that I follow: Comment on whatever would be surprising when you read the code. When I write code, a voice in the back of my head constantly asks “will I understand this code later?”. (People who just instinctively answer ‘yes’ every time are arrogant and often wrong.) Whenever the answer is ‘not sure’, the next obvious question is “why not?”. Answering that question leads you directly to what you need to write in your comment. Sometimes the answer is “because the reader of the code might wonder why I didn't write it another way”, and that's the special case this article covers. But sometimes the answer is “because it's not obvious how it works or why it's correct” and that clearly requires a different type of comment. reply albrewer 37 minutes agoparent> “will I understand this code later?” My driving principle in the same vein is \"where do I have to look when/if this doesn't behave as expected?\" - if the answer is not in the docs (wiki -> package/module -> file -> class -> function / method) or is otherwise > 10 lines away, it gets an inline comment (or the docs are updated). Usually this happens when chopping up strings or during intermediate navigation steps over an odd data structure. reply perlgeek 7 hours agoparentprevAs an addition, if you first try to write the code one way, and then it doesn't work and you need a second approach, that's a really good indication that you want some kind of comment there. You were surprised while writing it, so if you'll forget that surprise in a year, you'll be surprised by reading it too. reply ghewgill 21 hours agoprevI agree that the title is ambiguous - it's what piqued my interest to read the article in the first place. Personally I lean toward fewer comments overall - perhaps to a fault - but explanatory comments as shown in the article are absolutely valuable. It's a good reminder to explain the whys and the why nots. This especially applies to your own code that you write and still have to maintain 5, 10, 15 years later. Just the other day I was reviewing a coworker's new code and thought \"why choose to do it this way?\" when the reason was 10 lines up where I did it the same way, 8 years ago. She was following the cardinal rule of maintenance - make the code look like the existing code. reply sparrish 20 hours agoparent> make the code look like the existing code. This is so undervalued when maintaining an older codebase. Please, for the sanity of those who come after you - make the code look like the existing code. reply sbuttgereit 19 hours agorootparent\"Please, for the sanity of those who come after you - make the code look like the existing code.\" I think it's a great rule of thumb... but there are exceptions. For example, Funnily enough I was reviewing some code just today written by someone else and that I'm going to be asked to expand on or maintain. It looks like this: var example1 = Object; var example2 = Object; var example3 = Object; ... var example147 = Object; And then later there are corresponding variables assigned to different objects which are: var tempExample1 = ; var tempExample2 = ; var tempExample3 = ; ... var tempExample147 = ; And this goes on and on. It's real special once we get into the real business logic. (and to be clear... \"example1\" are the actual names, I'm not just obfuscating for this comment.) The reason it looks like this is because the original developer copied the examples from the developer technical documentation for what they were doing verbatim; this documentation only had one variable so the numbering was the way to get multiple variables. Knowing this system, they didn't have to do that, they could have very easily assigned meaningful names for all of this. (To be fair, the original developer isn't a developer but a consultant business analysist saying, \"Oh yeah, I can do that!\" to the client.... billing all the way). I can tell you with great certainty and righteousness: I'm not going to make my code look like the existing code. I may well do some refactoring to make the existing code vaguely scrutable. I appreciate that what I'm describing is an extreme case and not really what you or parent comments really were addressing. I just stop to point out that what you describe is a rule of thumb... a good one... but one nonetheless. And, as an absolute rule of thumb about rules of thumb, there are no absolute rules of thumb. Ultimately, experience and judgement do matter when approaching the development of new code or decades old code. reply PeterisP 7 hours agorootparentThe concept of \"make the new code look like the existing code\" still applies - in the example you gave, if you need to add examples148-200, and want to do it in a better way, then it would be wrong to do that new way for the new code; either you are willing and able to refactor the previous 147 cases as well (so that the new code matches the existing code, because the existing code was updated), or you keep the existing structure. reply Tainnor 4 hours agorootparentprevUnless of course you work in a wild-west codebase where you can basically tell who wrote what code because everyone has a distinct style and they never converge. ugh reply gregmac 19 hours agoprev> I see more people arguing that whys do not belong in comments either, that they can be embedded into LongFunctionNames or the names of test cases. Virtually all \"self-documenting\" codebases add documentation through the addition of identifiers. Identifiers can go a _long_ way, but not _all_ the way. I personally am a fan of requiring documentation on any public methods or variables/fields/parameters (using jsdocs/xmldoc/etc). Having a good name for the method is important, but having to write a quick blurb about what it does helps to make it even clearer, and more importantly, points out obvious flaws: * Often even the first sentence will cause you to realize there's a better name for the method * If you start using \"and\" in the description, it is a good indication that the method does too much and can be broken down in a more logical way People often think properties are so clear they don't need docs, then write things like: /** The API key */ string ApiKey; But there's so much missing: where does this key come from? Is this only internal or is it passed from/to external systems? Is this required, and can it be null or empty? Is there a maximum? What happens if a bad value (whatever that is) is used? Is there a spot in code or other docs where I could read more (or all these questions are already answered)? This is stuff that as the original author of the code you know and can write in a minute or two, but as a newcomer -- whether modifying it, using it, or just parachuted in to fix a bug years later -- could take _hours_ to figure out. reply pdpi 5 hours agoprevI find myself following only two or three different patterns in comments: There's often a fairly small kernel of very dense code that abstracts away a bunch of complexity. That code tends to have well north of a 1:1 comment to code ratio, discussing invariants, expectations, which corner cases need special handling and which ones are solved through the overall structure, etc. Then there's a bunch of code that build on that kernel, that is as close to purely declarative as possible, and aims for that \"self-documenting code that requires no comments\" ideal. Finally, there's the business logic-y code that just can't be meaningfully abstracted and is sometimes non-obvious. Comments here are much more erratic and often point at JIRA tickets, or other such things. reply ok_dad 20 hours agoprevI personally don't care what anyone says, I use comments and doc comments ALL OVER the place; I do it in reverse, though. I write a list of steps for the application as comments, a rough draft at first, then as I develop the code I take the big steps and split them into little steps, sometimes removing the original comment and sometimes not, and I continue to split comments into smaller steps until I have nearly a complete algorithm. Then I just code the logic in there. I normally will code from the outside in, so I'll also be writing code as I do the comment-splitting stuff. Sometimes I get off on a tear and I code a bunch of stuff at once, but then later I go back and comment it down to a level that I think most of you would find annoying. Every function and variable has a comment about what it does, even the `deg_to_rad` function has a comment `\"\"\"Converts degrees to radians.\"\"\"`. Why not, storage is cheap! I know most people don't like it, and that is fine, they can deal with it! I they don't want to see my comments, they can remove them from their version of my code with a script, and if my co-workers and boss don't like them they can remove them in a code review! However, I can say that I enjoy reading my old code way more than I enjoy reading other's code which have zero comments. I work in Python, so a lot of the simple non-algorithm code (boilerplate stuff for apps, like flask APIs for example) is mostly \"self-documenting\" since the old saying goes, \"write some pseudo-code and 95% of the time it runs in Python.\" The most important comments are sometimes on the boilerplate stuff because that's where a lot of changes happen versus the algorithms where I find there is a lot more wholesale rewriting in my industry. I will always love comments and doc comments! reply lifthrasiir 14 hours agoparentI also tend to do so but only for top-level constructs, where such comments are most useful. I think I do so because I like to conceptualize entirely within my mind; I found it rather slow to experiment with various design choices by actually writing them down, at least initially. So I necessarily have to document these design choices once they are settled (as others don't yet have access to my mind :-), while I expect details become much clearer after others also have conceptualized them to their minds. This approach does have a downside of making it possibly harder to read when such conceptualization couldn't be done for any reason, so I do additional tweaking to maintain the baseline readability to the minimum required. reply gleenn 20 hours agoparentprevComments are great when they are well maintained. But for every codebase that isn't basically some open source software where the eyeballs-reading to comment-maintaining ratio is really high and people spend the time, everyone eventually forgets and/or is too lazy to maintain them. It can be almost as much work updating the comments as adjusting the code a lot of the time. So the ground truth reality is that comments are usually \"lies waiting to happen\". Eventually, the comments and the code won't be in sync, and this can be potentially worse than having either bare, uncommented code, or better, having actual automated tests that show intent. The tests mostly can't lie, otherwise you wouldn't have merged them presumably. If you show me a bunch of decent tests laying out how the code is intended to be used, that is what I want to see. Because it is explanatory AND it is nearly guaranteed to be truthful. reply pavon 20 hours agorootparentI'll take the out of date comments. They provide a red flag that either the original author was confused, or the behavior changed over time. At least then you can do code archeology to piece together descriptions of what the original intent was, and how it changed using the commit history and figure out where things went off the rails, and thus determine how to actually fix it rather than patch over it even worse. The problem with unit test as documentation is that over time they end up reflecting the same misconceptions that the code has. Someone does a refactor, misunderstands how the original code works, and \"fixes\" the unit tests to pass. Now you have tests that lie just like comments can lie. reply gleenn 19 hours agorootparentNeither comments nor lacking unit tests are going to fix that, that's a strawman argument. What I'm saying is if someone doesn't know 100% what their doing, which can be relatively often, then I would hands-down take a bunch of unit tests over some misguided comments because at least the unit tests have to /pass/, so there is some grounding in reality. A comment has absolutely no such requirement and therefore can stray in any direction forever. Also, if you refactor a name of a variable or inputnor function, you will be required to update the tests, so they are dar safer from becoming stale due to refactoring compared to an opaque string from the IDE's point of view. reply rich_sasha 20 hours agorootparentprevI never found that to be a problem in practice. Yes, comments do get slightly out of sync, and not always corrected. But typically enough comments are correct that you can make sense of the whole thing, and can even fix the comments then. By contrast, I have really never seen truly self documenting code. The comments may be up to date by virtue of not existing, but the end result is just more confusing. YMMV reply awfulneutral 19 hours agorootparentSame, my coworker used this argument when I tried to get them to write more comments. The number of hours I've wasted due to there being no comments at all is way, way, way more than the very small number I've wasted due to a no-longer-accurate comment. And I usually find that people who write self-documenting code choose variable/function names that might make sense to them at the time, but are vague and confusing to me when I need to read the code. reply bottlepalm 12 hours agorootparentSame here as well, I hear this argument a lot, but have yet to see a comment I didn't appreciate. And if there's something wrong with the comment, fix it! Self documenting code is a joke. reply Salgat 19 hours agorootparentprevCode that breaks is much more likely and damaging than comments that are out of date. In my life I've never had a case where the bad outweighed the good for comments. reply 8n4vidtmkvmk 12 hours agorootparentCode breaks because people read the comment and assume it speaks truth when in fact it's full of lies. And as soon as you don't trust comments you have to start reading the code. And if you're at that point now..... Why have the comment? reply Salgat 1 hour agorootparentIf you ever trust comments over code you really need to adjust your approach. Comments are there to help guide you towards the intent of code, not to let you ignore code. Same reason why if a function has a bug, you don't ignore that bug just because the function's doc says otherwise. reply bottlepalm 12 hours agorootparentprevComments don't replace reading code, they prime your brain for the code you're about to read. There is often context and reason behind the code that is not in the code itself. reply kmoser 15 hours agorootparentprevAlso, if you're doing regular code reviews, wouldn't that include making sure the comments are up-to-date? As for the argument that it wastes time to update comments, I've never seen a codebase where the comments were so voluminous that it would be a significant burden to update them. Devs avoid updating comments because there is usually no penalty for doing so (boss doesn't call them on it, and the code still runs), and they think they've saved a few minutes when in fact they're just kicking the can down the road for the next dev who has to decipher the incorrect (or missing) comment. reply 8n4vidtmkvmk 12 hours agorootparentNo. If the comment isn't highlighted in green or red in the commit, I'm not likely to read it. reply PeterisP 7 hours agorootparentWell, that's a problem. reply buggy6257 20 hours agoparentprev> I know most people don't like it, and that is fine, they can deal with it! I they don't want to see my comments, they can remove them from their version of my code with a script, and if my co-workers and boss don't like them they can remove them in a code review! It was all great until you got here. This is a big red flag for me for a teammate. reply ok_dad 18 hours agorootparentWell I don’t think we’ll end up being teammates, and I haven’t had issues before :) reply AcerbicZero 20 hours agoparentprevHah, this is me :) Half the time I start by just writing comments explaining what I'm about to try and do, then I go back and add comments about how things did not go as expected, and what I had to do to get it to actually work. Super helpful 5 weeks later when I have to actually see it again. reply slaymaker1907 19 hours agoparentprevI don't think doc comments are warranted in every case and definitely don't think you need to document every single parameter, return value, etc. for every single function, but they sure do come in handy for complicated APIs. reply phito 11 hours agoparentprevYou're wasting so much time doing this... Nobody will ever read 99% of it. Then the comments will get out of date and it'll end up worse than having no comments. reply bottlepalm 12 hours agoparentprevAh yes, Comment Driven Development (CDD), I practice it as well. You start with comments of how you want things to work, and fill in the code. It's a perfect combination of why/how and communicates to the next developer the high level thought process behind the code perfectly. reply JohnMakin 21 hours agoprevI often write comments like this when I can predict what an overly nitpicky reviewer will say in a code review - \"I didn't do X because Y\" hoping to save some annoying back and forth about it. reply lainga 21 hours agoparentIME you get the same amount of back and forth but I get to write \"as the comment says\" a few times reply JohnMakin 21 hours agorootparentLOL precisely reply rplnt 21 hours agoparentprevI add those preemptively in the PRs, but not sure they have much value in the code. reply not2b 21 hours agorootparentThey have value in the code because they save time when someone has to deal with that code a couple of years later. Certainly the explanation could be in the code reviews or the commit message, but it's easiest if it is right there. reply panopticon 20 hours agorootparentYup. Our operating principle was that if a question was asked in a code review, someone will likely have the same question when reading the code weeks/months/years from now and there should be a comment. reply TillE 19 hours agorootparentprevIt's an obvious cliche, but that \"someone\" is very frequently you. It's really easy to forget why you made some non-obvious decision and waste time poking at the exact same stuff you already did a year or two ago. It's happened to me several times. When you figure out something tricky, leave a comment. reply JohnMakin 15 hours agorootparentcompletely agree every person who leaves detailed “unneccessary” comments like this has been bitten by coming back to a codebase a year+ later and going “who was the idiot that wrote this and why didnt they leave any clue behind” and realizing that yes, you were the idiot. or has had to come behind someone that left zero documentation or readable code and been tasked with cleaning it up. breadcrumbs are useful and comments cost nothing. yes, there are commit messages, but commits often aren’t super clean, explicit, coherent, or even looked at. reply jesse__ 19 hours agoprevI ascribe to the notion that 'comments are apologies' (to my future self). If a piece of code is weird, or slow, or you'd say \"yeah, it's kinda janky\" when describing to somebody, I usually write a comment about it. Especially if I've changed it before; to document some case that didn't work, or I fixed, or whatever. When you operate on this basis, superfluous comments just melt away, and you typically end up documenting 'why' only when it's really necessary. Try it out in your own codebase for a month and see how it feels :) reply Terr_ 21 hours agoprev> Does 16 passes over each string BUT there are only 25 math strings in the book so far and most areWhen I was first playing with this idea, someone told me that my negative comment isn't necessary, just name the function RunFewerTimesSlowerAndSimplerAlgorithmAfterConsideringTradeOffs. Wow, someone actually suggested that?! Do people write whole programs like this? reply mgsouth 13 hours agoparentThere are many, many developers who are, deep in their hearts, \"programmers\"--someone who creates a plan of action, a program, for very complex machines to follow. They are skilled at discerning The Right Way to solve a problem. Actually, THE Right Way. And since they are Programmers, the Right Way is to find the Best Way to tell the computer what to do. Computers are completely oblivious to comments, so they aren't The Best Way. Comments are ambiguous, so they can't be THE Right Way. Computers parse identifiers, so a good variable name is telling the computer _something_ at least, and so is vastly preferrable to comments. If there's a mismatch between comment and code, the code is Reality so the comment must be wrong, and is thus misleading and worse than useless. In reality, of course, software development is about getting large groups of people in-sync as to problem, solution, and implementation. That takes lots and lots of communication. Ambiguous, messy people-to-people stuff, without The Right Solution to contents or wording. Because there's a bias towards thinking of the Program as Reality, it never occurs to such developers that an identifier can become just as outdated or wrong as a comment, and in fact it is easier to correct a comment than to globally rename identifiers, or that a 20-word comment carries vastly more information and nuance than a 20-character function name, or that trying to shoehorn information important to developers into a language used to tell _computers_ how to function is worse than trying to drop assembly language into a SQL query. reply Wowfunhappy 28 minutes agorootparent> Computers parse identifiers, so a good variable name is telling the computer _something_ at least, and so is vastly preferrable to comments. I realize you're trying to explain a way of thinking that you don't actually share—but this doesn't make sense to me either. Computers parse identifiers in only the simplest sense—they care if two identifiers are identical to each other or different. So, any identifier longer than one or two characters is in some sense a comment, because everything longer the minimum necessary to make the identifier unique is ignored. They're literally stripped from the output if you don't retain debug symbols or use a minifier (depending on the type of language). reply herpdyderp 19 hours agoparentprevYes. I currently work in a codebase where the previous tech lead did that and promoted such behavior (among many other insanity-inducing practices). reply Pinus 9 hours agoparentprevIn my experinece, veryLongVariableNamesThatContainADetailedExplanation are almost always a sign that someone didn’t quite know what they were doing. Unfortunately, short names do not indicate that someone did know what they were doing! =) reply mkoubaa 20 hours agoparentprevThis is griefing your coworkers who would actually do that? reply Wowfunhappy 20 hours agorootparentI wasn't being sarcastic, if that's what you mean. (And I'm not a professional programmer, so my coworkers don't write code.) To me, the suggestion seems so incredibly bad that it leaves me wondering about the headspace of someone who would suggest that. Is it somehow less bad than it seems? reply W0lf 54 minutes agoprevMy rule of thumb for code comments is to comment what's not in the code. reply mark-r 3 hours agoprevI once wrote a bubble sort into production code, because the total number of elements to sort rarely exceeded 4 and it was what I could do off the top of my head. I don't remember if I left a comment explaining the reasoning, but I think I did. A year later a new feature invalidated my assumption about the number of elements and the sort was way too slow. I'm sure the person who inherited that code cursed me a few times. reply jakub_g 19 hours agoprevApart from what/why/why not: Something I started doing recently is to put URLs in the comments: - URL of documentation for a complex feature - URL with a dashboard with telemetry - URL of a monitor which checks if the given feature, CI job, GitHub action etc. works correctly. In a big project, figuring this stuff out is not trivial, requires a lot of searching with proper search terms and/or asking the proper knowledgeable person. I find it weird that code is often so detached from everything non-code. reply flerchin 3 hours agoprevComments and docs are lies that I dearly love. I want and need them, but I never forget that they're, at best, helpful lies. The code does exactly what's written, the comments adhere more or less, sometimes. reply gary_0 21 hours agoprevThe title might be clearer with a hyphen: \"Why-Not Comments\". reply agentultra 5 hours agoprevMay also sometimes, when designing algorithms, be useful for documenting pre- and post-conditions and invariants in procedural languages that lack embedding such specifications. You can't really reason about these things in the language itself and end up having to use something else (predicate calculus usually) but it's nice to at least have an indicator, as tfa suggests! What code doesn't do is important! reply AcerbicZero 20 hours agoprevI have learned to enjoy narrating my struggles in the comments, probably to excess; but it certainly makes it much easier to pick up a task again after I leave it alone for a week or two. People rarely touch what I write, but if they do, and they want to strip the comments out, thats totally fine with me, just don't ask me how it works after you do :P reply deodar 26 minutes agoprevThere is no hope for the software industry to mature if we cannot agree on some basic coding practices. Like the judicious use of comments to improve maintainability. reply ufo 21 hours agoprevWhen I see a sequence of string replacements, instead of performance the main thing I worry about is if the output of one replaces matches a pattern for another replacement. I see variations of this often during code review. Doesn't seem to be a problem here though because they're replacing macros by symbols that are known ahead of time. reply kqr 11 hours agoprevAnother big one I wish I saw more often is \"these are the circumstances under which this assumption was made and here are the steps you can take to check if those circumstances have meaningfully changed.\" In other words, the comment allows the author to reach into the future and co-debug with the reader, even if the author is no longer there. reply 8organicbits 21 hours agoprevAnother approach is ADRs, which document alternatives considered, but these are documentation, not comments. I've found them useful for building consensus around architecture decisions. https://adr.github.io/ reply dekervin 5 hours agoprevI am working on the abandonned idea of a rapgenius for code. I think it still is a useful idea for the open source world. You can join the HN learn discord [1] if you want, so I can keep you posted. [1] https://discord.gg/ks4yfPgbyn reply k__ 3 hours agoprevWould be cool if literate programming caught on. It seems only to be a thing with Jupyter notebooks, and even there it mostly describes the results and not the code. reply yawnxyz 4 hours agoprevin the age of AI and Cursor, I make my function name as expressive as I can, and I make sure to add a couple of lines of comments, either generated or manual. It makes it way easier to send these into Claude (it seems, at least). I hope they introduce a semantic/vibes search too as I can never remember what I name my classes and functions... reply ktosobcy 11 hours agoprevI somewhat dislike the notion of \"self-explanatory code\" (especially if someone has tendency to be \"smart\")... optimise for reading and add comments! reply kstrauser 21 hours agoprevThe title's a little odd, unless it was to grab attention. It's saying \"Why [I use] 'not comments'\", not asking \"why not comments?\" A \"not comment\" here is an explanation of why the programmer didn't choose the obvious approach. I agree: that's a very valuable thing to document for the next person. For instance, you might write something like: # I used a bubble sort instead of a quick sort here because # the constraint above this guarantees there will never be # more than 5 items, so it's faster to use the naive # algorithm than to implement a more complex algorithm that # involves more branching. or # Normally we'd do X, but that broke customer Y's use case # based on their interpretation of our API docs which we # had kind of messed up. So now we do Y because it works # under both interpretations, at least until we can get # them to upgrade. Basically, tell your audience why you're not using the expected method. It's not because you didn't know about it, but because you do know and you've determined that it's not a good fit for this use case. reply lkrubner 21 hours agoparentThe problem with the title is simply that the English language allows open compound words. I know many Germans wonder why we allow this. Germans push the words together, for clarity. I've suggested that we use hyphens. Hyphens feel natural in English, and could remove the ambiguity that exists whenever we use open compound words (that is, open-compound-words). In this case \"not-comments\" would have added clarity. Likewise, the title \"World's longest DJ set\" was confusing, because most people will assume that the compound word is \"DJ-set\". But if you read the whole article, then you realize that a python snake fell on the mixing board and accidentally mixed some tunes. So the compound word was actually \"longest-DJ\" -- a 2.5 meter python. We should all consider using hyphens for all compound words. https://x.com/krubner/status/1828155852773113942 reply BobaFloutist 20 hours agorootparentI think the \"Longest DJ Set\" was intentional wordplay. reply thayne 20 hours agorootparentAs was the title of this article. reply shiroiushi 18 hours agorootparentprev>I've suggested that we use hyphens. Hyphens feel natural in English, and could remove the ambiguity that exists whenever we use open compound words We do use hyphens in English. Well, some of the time, and some of us. I could be wrong, but I do feel that, given my age and also my readings of older texts, that the use of hyphens in this way has become less common, and that this was much more common decades ago to avoid ambiguity. reply Tainnor 4 hours agorootparentprev\"longest DJ\" isn't a compound, it's a noun modified by an adjective, and as such it would be written as two different words in German as well (\"längster DJ\"). reply aidenn0 20 hours agorootparentprevAs always, there's a relevant xkcd: https://xkcd.com/37/ reply thayne 20 hours agoparentprevI think the title is intentionally ambiguous. Because the article is a rebuttal to those who oppose writing comments (that is where \"why not comments\" means \"why you shouldn't write comments\") where the main argument is that you should write \"why not\" comments (that is \"why not comments\" means write comments explaining why you didn't do something or do something a certain way). reply philipwhiuk 19 hours agoprevPersonally I think you're better off putting 'not comments' in git commit information. Git commit comments are like normal comments except they don't get detached from the code. reply jakub_g 19 hours agoparentI believe this can work in slow paced opensource projects which do squash-merges (one PR = one commit), every commit is green and beautiful and well described with properly formatted message. I worked on such project in my first job and I cared a lot about my commit messages, and changelogs. On the other hand, in fast paced corporate development I barely ever see someone make commit messages like this. Even PR titles often leave a lot to be desired. reply keybored 12 hours agorootparentSquash merging is a practice I mostly see recommended (I mean: mandated) from those corporate environments which churn out code like a paper mill trying to put the Amazon out of existence. OSS projects that can take their time with nice commit messages—which even review them, not just the code—don’t have to limit themselves like that. reply keybored 20 hours agoprevYou can still document the why in commit messages![1] I feel like I’m getting off the self-documenting code ride. In our own codebase we rely way too much on “descriptive names”. Like full-on sentence-names. And is the code self-documenting? Often not. You indeed cannot describe three or more axes of concerns in one name. Do comments go stale? Well why does it? Too loose code reviews? Pull requests that have fifty lines of diff noise that you glaze over? We have the tools to do better on that front than some years ago at least. It’s a joy to find a corner of the code base where things are documented with regular sentences. Compared to having to puzzle through five function call layers. [1] But yeah, really. But also: sometimes also in comments. Sometimes both. reply oxidant 20 hours agoparentI mostly like a comment with a succinct explanation and a longer commit message. Comments are less likely to be refactored and lose the immediate git blame, while the function might change over time. Ideally the future user would trace the git blame back to the original commit of they really had questions. A long comment is really helpful sometimes though. I like to put ascii truth tables for complex boolean logic, both to ensure I cover all cases when writing the code (tests, too) and to make it easier for future me to understand what's going on at a glance. reply klingoff 20 hours agoparentprevRight, commit messages are at least always right for the context that comes with them. I think long descriptive names are an anti pattern. As per word puzzles you can't actually read the middle of a blob of text so you are basically left with a dozen variables that are cognitively the same as isDatabaseHidingDragonsSetter(). reply slaymaker1907 20 hours agoprev> In recent years I see more people arguing that whys do not belong in comments either, that they can be embedded into LongFunctionNames or the names of test cases. Who is arguing this? Usually, if I'm adding a comment on why something is done a particular way, it's something that is going to take at least a full sentence to explain if not a whole paragraph. reply icambron 20 hours agoprevThe “why not” form just seems to be a special case of “explain why this code is weird”, which is my commenting metric in its entirety reply spencerchubb 16 hours agoprevMy team comments way too much. I constantly see stuff like this def fetch_data(comment_id): Args: - comment_id: The id of the comment to fetch. Returns: The comment data # Fetch comment data data = fetchCommentData() reply Minor49er 3 hours agoparentMine do this as well, but in PHP. They adopted docblocks over a decade ago and never defined any best practices or used any tools to check them, so comments don't even match the code half of the time. We're on PHP 8 now which can enforce types in the code itself but still have to include docblock types because that's how it has always been done, so the problem persists. It's maddening reply aurelien 4 hours agoprevBBBbbbbeeeeccccaaausssseeeeEEEE!!!! reply remot_human 21 hours agoprevMaybe what we need is a single vocabulary word that means “I’m doing something that won’t scale well to large inputs but is still worth writing for now” then you could name the function replaceEscapeCharsNewWord() reply Jtsummers 21 hours agoparentI've named these things \"naive\" sometimes. Like \"naiveQueryBuilder\" or whatever the appropriate term would be. They're also useful for creating tests because the naive version is usually \"obviously\" correct (still write some tests, but you don't need much more than sanity checks) and can be the oracle for the faster version (you might want to cache results rather than run the naive one each time, though). reply xelxebar 20 hours agoparentprevIthkuil to the rescue? https://en.wikipedia.org/wiki/Ithkuil reply breck 20 hours agoprevI resisted putting comments in my languages for years. My reasoning was it was always a flaw of my code (or the language) if I couldn't express myself in typed code. Then I realized that my languages will never be perfect, and having comments is an essential escape hatch. I was wrong and I changed my mind. Also, 99.9% of languages have comments: https://pldb.io/blog/a-language-without-comments.html reply yodsanklai 18 hours agoprevThis seems a bit like a strawman argument. I don't think anybody say that we should never use comments. The problem with comments is that they can become stale, and it's often possible to self-document or write simpler code that causes less surprise. But of course, it's totally fine to put comments. And I think comments should be mandatory for interfaces functions/types unless their behavior is obvious. I don't want to read the code to understand what a function does, or what invariant a class maintains. And if it's too complex to document in a few lines, probably this isn't the right interface. But apparently, this isn't obvious for everybody. In my company, most of the code isn't documented. reply golergka 20 hours agoprev> This is incredibly inefficient and I could instead do all 16 replacements in a single pass. But that would be a more complicated solution. So I did the simple way with a comment: > Does 16 passes over each string > BUT there are only 25 math strings in the book so far and most areSo it's still fast enough. I've been in this exact situation quite a few times — use a bad algorithm because your n is low. However, instead of commenting, I did something like this instead: function doStuff(items: Item[]) { if (items.length > 50) { logger.warn(\"there's too much stuff, this processing is O(n^2)!\"); } // ... do stuff } reply pwdisswordfishz 6 hours agoprev> Why not \"why not\" comments? Not why \"not comments\" Or you know, you could have just used a hyphen instead of clickbaiting. reply boerseth 18 hours agoprev>The negative comment tells me that I knew this was slow code, looked into the alternatives, and decided against optimizing. I admire the honesty, but will continue to phrase these \"why not\" comments as insincere TODOs. reply veltas 10 hours agoprevWhy not question-mark? reply at_a_remove 11 hours agoprevAlthough I had started programming when I was nine, in high school I was fortunate enough to have a computer science teacher with a PhD in the field. Among one of the habits drilled into me was extensive commenting. Every function (or procedure) starts with a comment block. It first talks about the what and why. Then, a line for the inputs and another for the outputs. Next -- and this is done closer to the end of the writing -- I describe what it calls and what it is called by. The comment block optionally finishes with room for improvement. The function itself probably has other comments. Usually for anything which is not blindingly obvious. Because I write code like a caveman, wherein only one thing happens on one line, most everything is quite clear. If there's anything weird or magical that has to happen, it gets a comment. Elegance and cleverness is reserved for data structures, algorithms, and so on, rather than doing a lot of stuff in as few lines as possible. I do this for Future Me, who might be having a bad day, or for anyone who wants to adapt my code to something else. One of the last steps in a finished program is going through and making sure that my comments match my code. I am a very boring kind of programmer. reply mentalgear 20 hours agoprevAn article not not to overlook. reply mentalgear 9 hours agoparentActually, there's a not to much (it was late), it's definitely a great article ! reply matt_lee 20 hours agoprev> Why not \"why not\" comments? Not why \"not comments\" I nearly exploded trying to grok this reply giraffe_lady 19 hours agoparentu can tell this dude loves algebra reply zwnow 12 hours agoprevComments tend to get outdated quickly as your app grows. If you are not careful with your phrasing you might even introduce misinformation into your app. I'd rather read the code instead of comments. reply jimmaswell 12 hours agoparentI love when a docstring saves me from reading code. \"Takes parameter x which should be in state s, has side effect e, returns j, throws exception if ..\" Seeing that in my IDE popup and moving on with all the info I need is so satisfying compared to having to waste my time on an unwanted spelunk. It's also impossible to embed all this in a name that isn't a camelcase paragraph. reply zwnow 11 hours agorootparentI can imagine that this is helpful to some people, but when coding I ignore comments usually. Good docs and code is all I need. reply jerhewet 20 hours agoprev [–] Comments should never be \"what\". They should always be \"why\". reply edflsafoiewq 20 hours agoparentComments should be whatever you think warrants commentary. If that's \"what\", that's fine. reply zimpenfish 20 hours agoparentprev [–] That works if the code wasn't written by, well, lunatics who decided that clear simple code was The Work Of Satan and if you couldn't have a tower of interfaces or write your own bafflingly complex ORM, what was even the point? Then the people who follow will almost certainly need some \"what\" commentary (as well as the \"why\" but IME the lunatics who spurn clear and simple code never think they have to explain \"why\" either.) reply mschuster91 20 hours agorootparent [–] > or write your own bafflingly complex ORM Oftentimes, the reason for that is that 20 years ago stuff like Doctrine, Liquibase or whatever just didn't exist. You know, the time when PHP developers shipped straight mysql_query calls with direct interpolation of $_GET, and most \"enterprise\" Java application came with a ton of SQL scripts and a dedicated multi page UPGRADE file explaining in which order you had to run the schema migrations, reboot systems, run manual migration scripts and whatnot to get an upgrade done. Some times, upgrades could literally take days. Naturally, people invented their own stuff to make stuff just suck a little bit less, and it got more and more used in a company, only ever extended in functionality... the dreaded \"corpname-utils\" JAR dependency (if you're really unlucky, the JAR having been semi-restored from a half-broken decompile because the sources got lost along the way) or util.php that just got copied over from project to project. And that's how you end up in 2024, still maintaining some ORM that has its origins in Perl code written in the 90s by someone deceased in the '00s. (Yes, I've been there, although not that bad) reply pantulis 10 hours agorootparent [–] > the dreaded \"corpname-utils\" JAR dependency In my first job, my older colleagues, most of them now managers, had managed to write their own library. It included its own timezone management, a wrapper o top of DEC's OSF/1 AXP concurrency primitives, a realtime memory-mapped database format, a compiler that run not on files but in expressions stored in an Oracle databse, and even their own CORBA-like object sharing over TCP/IP. These people were wizards, and probably did a lot of stuff just to show their coding prowess, but a decade later when I joined that company most of the younger programmers did not dare to touch that code. I had to do that when the software was being deployed in Brazil, where nobody had expected how DST changes in the south hemisphere. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Logic For Programmers v0.3\" has been released, focusing on improved book formatting.",
      "The release emphasizes the importance of comments in code, particularly for explaining \"why\" decisions and tradeoffs, which cannot always be self-documented through function or variable names.",
      "An example is provided where a comment explains the choice of an inefficient method for replacing math notation with Unicode symbols, highlighting the tradeoff and future optimization potential."
    ],
    "commentSummary": [
      "Comments in code should focus on explaining \"why\" and \"why not\" to aid future understanding, especially in large, complex codebases.",
      "Mandatory comments for obvious functions are seen as wasteful and can lead to ignoring comments altogether.",
      "While some prefer long function names or commit messages, the author finds comments essential for clarity, maintenance, and documenting decisions and trade-offs."
    ],
    "points": 225,
    "commentCount": 229,
    "retryCount": 0,
    "time": 1726001537
  },
  {
    "id": 41505670,
    "title": "Flipper Zero Gets Major Firmware Update, Can Eavesdrop on Walkie-Talkies",
    "originLink": "https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update",
    "originBody": "Flipper has released a substantial 1.0 firmware update for its hobby hacking multi-tool, the Flipper Zero, bringing a number of quality-of-life improvements and expanded utility to the device. The firmware update doubles data transfer speeds via Bluetooth from Android, makes Bluetooth firmware update installations 40% faster, and overhauls the device's near-field communication (NFC) engine so that it can support more card types and read data faster. The team completely redesigned the Flipper Zero's NFC system, restructuring its library and making it compatible with the FreeRTOS operating system. It can also support ICODE SLIX and FeliCa Lite-S chip types. This firmware update means the Flipper Zero can read and emulate \"tap\" cards like room keys faster than before. The Flipper Zero can now eavesdrop on analog walkie-talkie audio, and will play it through its built-in speaker. It can decode 89 different radio protocols with the Sub-GHz app. Its infrared capabilities mean it can be used as a universal remote for TVs, projectors, ACs, or audio systems. The firmware update brings new remote layouts, and enables external infrared hardware support to increase the Flipper Zero's range as a remote. The Flipper Zero will now be able to run apps from microSD cards directly, and the device's low-power mode now has a battery life of a month instead of just a week. All of these upgrades in Flipper's 1.0 firmware update have been three years in the making, according to the company. RECOMMENDED BY OUR EDITORS Canada Walks Back Ban of Flipper Zero, Targets 'Illegitimate' Use Cases Turn Flipper Zero Into a Game Controller With This Raspberry Pi-Powered Module iOS 17.2 Finally Prevents Those Annoying Flipper Zero Attacks There are hundreds of third-party apps in the Flipper Zero's app catalog, and developers can now code apps in JavaScript in addition to C or C++. Flipper's organized Apps Catalog can be accessed via its mobile app, which is on Google Play and Apple's App Store. Flipper Zero apps are sorted into categories by function like USB, RFID, Infrared, iButton, NFC, Sub-GHz, GPIO, and Games, to name a few. About Kate Irwin Reporter I’m a reporter covering early morning news. Prior to joining PCMag in 2024, I was a reporter and producer at Decrypt and launched its gaming vertical, GG. I have previous bylines with Input, Game Rant, and Dot Esports. I’ve been a PC gamer since The Sims (yes, the original). In 2020, I finally built my first PC with a 3090 graphics card, but also regularly use Mac and iOS devices as well. As a reporter, I’m passionate about uncovering scoops and documenting the wide world of tech and how it affects our daily lives. Read Kate's full bio Read the latest from Kate Irwin Microsoft Warns of 79 Security Flaws, Patches Windows After 4 Bugs Exploited Wix to Block Russian Users, Take Down Their Sites in Wake of US Sanctions Samsung Laying Off Thousands in Global Cuts Despite AI Chip Boom Former Samsung Staff Arrested for Allegedly Sharing Chip Secrets With China Polaris Dawn Mission Finally Begins as SpaceX Launches Crew Into Orbit More from Kate Irwin",
    "commentLink": "https://news.ycombinator.com/item?id=41505670",
    "commentBody": "Flipper Zero Gets Major Firmware Update, Can Eavesdrop on Walkie-Talkies (pcmag.com)216 points by CharlesW 21 hours agohidepastfavorite46 comments _ache_ 19 hours agoI'm really happy about the firmware updates. When Flipper Zero was crowdfunded, the deal was that the tool will be available once the hardware is good enough to fulfill the promises. And so the software will be updated later. I'm happy that they keep they promises. reply zactato 14 hours agoprevIs the Flipper Zero still legit? I know it’s only quasi legal in some places. I see a lot of ads for it to the point where I kind of suspect it’s some sort of FBI entrapment program. Maybe I’m just being paranoid. reply somat 13 hours agoparentIt is an software defined radio with a nice interface built around it. Perhaps the nice interface has lowered the bar enough for it to now be in the crosshairs of the state, this has happened before, sometimes just making the tool easy enough for anybody to use is enough to make it illegal/controlled. The point being the flipper does not do anything that is not possible with other tools, it just makes it easy. reply aeonik 5 hours agorootparentIt's not actually an SDR, it just has a discerning collection of hardware radio modules on it. If it was a self contained SDR you would expect a much beefier processor and power draw from the device from all the digital signal processing. reply system2 13 hours agoparentprevNot as small or sleek, but you can still get everything Flipper Zero is doing with Arduino or a small laptop with basic antenna hardware. Flipper Zero is the iPhone of radio pentest gadgets. reply ChrisArchitect 19 hours agoprevOfficial release: https://news.ycombinator.com/item?id=41500279 reply daghamm 12 hours agoprevStill looking for a cheaper alternative. All I need is a cheap portable SDR I can program. No need for a fancy enclosure or UI. reply jvanderbot 5 hours agoparentAre you not satisfied with the myriad of cheap baofeng/clone UHF/VHF handhelds that support CHIRP? reply big-green-man 15 hours agoprevAnybody can eavesdrop on walkie talkies. I can do it with my baofeng. It's not that interesting. What is interesting is everything else that that device can do. reply georgyo 14 hours agoparentI think this is undercutting the announcement. A baofeng is a specialized tool specifically for RX/TX 2m and 70cm analog audio radio waves. Of course it can listen to walkie talkies which are transmitting on 70cm. I read your comment as someone posting about making toast with an iron and saying \"Anyone can make toast. I can do it with my toaster. It's not that interesting\" All things that device can do is interesting because it is so many things, including this new ability. reply system2 13 hours agoparentprevUV5R with antenna upgrade works perfectly. I was able to listen to ISS from my backyard too. Hard to believe it is $20 for the quality of its hardware. reply SV_BubbleTime 14 hours agoparentprev>What is interesting is everything else that that device can do. Leaving me hanging. reply lupusreal 20 hours agoprev [–] Walkie-talkies can \"eavesdrop\" on walkie-talkies... reply vueko 20 hours agoparent [–] Seriously. People really need to understand that CTCSS and DCS aren't actually privacy features, but convenience features for filtering out _other_ people's transmissions a user isn't interested in. It's the exact opposite of privacy. I guess the marketing as \"privacy codes\" worsens the situation. reply solardev 20 hours agorootparentI just got my GPRS radio license and this was a really strange phenomenon to encounter. Apparently the FCC doesn't allow actual encrypted comms in this part of the spectrum, so the \"privacy\" codes, like you said, are really more just convenience codes, more noise cancelation than anything for privacy or security. It was weird trying to explain this to my family, too. Basically just had to tell them \"Nothing you say is private, and you should all say my call sign at the end of each transmission.\" We all felt like dorks, but it was super convenient in a place with no cell service. reply imroot 19 hours agorootparentGet an itinerant frequency -- $300, requires no coordination, and you can encrypt your comms. One of the (ham) radio clubs that I'm a member of does this as a benefit for the group, and it's something that's nice to have: I can give my wife a radio and not worry about what she may or may not say if we have to take two separate cars when we road trip. I've been meaning to do the process myself, but, I haven't had the time (and honestly, I'd want someone else to do the paperwork for me so I'm more likely to pay someone else to do it) recently, but, this might be the thing that prompts me to go and do it. 73 de K4IMW/WQZQ315 reply ac29 2 hours agorootparent> Get an itinerant frequency -- $300 FCC fees are actually only $205. I do these all the time for work. Also, you can get multiple frequencies on the same license, and you should if you are getting itinerants because there are so few and they are heavily shared. The application takes around 15 mins if you're familiar with it and can be done completely online. If you've never done it before, it might take you an hour or more to figure out. All that being said, the itinerants I am familiar with are in part 90 and that is only for commercial use. Its not clear to me that an individual would be eligible for this type of license for personal communications. reply vueko 18 hours agorootparentprevInteresting! I was under the impression that the FCC was actually somewhat strict about part 90.35 eligibility, in that you have to provide fairly detailed specifics of your business use case or how you fall under the various educational/nonprofit exemptions, and that if you told them you wanted it for personal use or supplied a thinly veiled excuse they'd tell you to get lost. Maybe that understanding is outdated. I can imagine a HAM club having an easier time justifying that than you would as a random individual. reply ac29 2 hours agorootparentYou dont have to provide detailed specifics of your business use case, usually just a sentence is fine. But 90.35 does not allow for individuals to be licensed for personal communications. My guess is that if a ham club is offering this to members, it is doing so as an educational institution, or a public safety organization. If the license is granted that way, using it for personal communications would be impermissible. reply wferrell 18 hours agorootparentprevCool! What sort of radio do you use? My friend and I tried to do -- the two of us in different cars driving down the same stretch of highway this with GPRS radios and very quickly we were not able to pick up the other's broadcast. We assumed it was that we didn't have big enough antennas and were not using a repeater. Note - others were driving we were each a passenger in our respective cars. reply solardev 18 hours agorootparentDid you have car mounted external antennas? I'm totally new to the hobby so I might be wrong here, but I believe the handheld units are kinda trapped in a Faraday cage if you use them inside the car. If you have the 20W+ mobile (not handheld) kind and mount it to an external antenna though, there should be enough range even just with GMRS as long as you're not on the other side of the mountain or something? Edit: Sorry, it's GMRS, not GPRS (facepalm) reply wferrell 10 hours agorootparentWe were not using car mounted antennas. We will try this next time! reply kotaKat 18 hours agorootparentprevIs there an actual EASY guide to dealing with itinerant licensing? The FCC paperwork is beyond confusing to try to get a single itinerant freq. reply ac29 2 hours agorootparentI do these all the time for work, you can email me if you have a question. It takes about 15 minutes if you are familiar with the process. Individuals using radios for personal use are not eligible for the types of licenses I am familiar with, though. reply Aloha 18 hours agorootparentprevI'm using EFJ 5100's and have a full KVL, and software that can make Christmas Tree radios. P25 Encryption is pretty good, in terms of quality. reply vueko 15 hours agorootparentWhile definitely better than the non-encryption of CTCSS etc, P25 encryption has some relatively concerning implementation problems. It's possible none of these actually matter for your usecase as many are related to UX around configuring and using encryption in an institutional setting, but the unauthenticated traffic injection, induced transmission and jamming issues are, well, not great no matter how you look at it. https://www.mattblaze.org/blog/p25 https://www.mattblaze.org/papers/p25sec.pdf I will grant that the open-source kfdtool keyloader boxes are neat. I have been meaning to see if I can repro the induced transmission via retransmission requests thing when the data packet stuff is fully disabled via CPS, but a friend permanently borrowed my hackrf so that project is on hold for now. I'm not optimistic, though, due to the comments in the paper about where in the stack the retransmission request is processed. reply jasonjayr 20 hours agorootparentprevI got my hands on one of those cheap UV-K5 Radios and the first thing I did was try to listen in to one of my FRS Radios and discovering all their channels & sub channels/privacy codes. Discovered this list while learning & researching: https://www.k0tfu.org/reference/frs-gmrs-privacy-codes-demys... reply runjake 18 hours agorootparentI was previously unaware of these particular radios. You just inadvertently sent me down one hell of a rabbit hole. It arrives Thursday. reply washadjeffmad 17 hours agorootparentAnd they keep improving - I think the latest revisions come with expanded flash for custom firmware. I've got a few UV-K5(99)s right now, and I end up giving them away to friends (even if I replace the duckies and keep the tri-bands) when new models release. With the custom bandpass and filter mods, they're reignited a very old interest of mine. reply cdchn 16 hours agorootparentThe new UV-5RM have a much better display and cool features like one-key frequency copy and USB-C charging. reply runjake 15 hours agorootparentCan you link me? The only 5RM I am pulling up is a Baofeng. Is the 5RM as hackable as these? reply windexh8er 15 hours agorootparentThe Baofeng is what the parent you're replying to is talking about. I own it as well and it does have a nice display and one button copy. But I prefer the TIDRADIO TD-H3. Not as cool as the Quansheng in terms of hackable - but a phenomenal radio for the money. I bought more radios in the last year than I've bought in the last decade. I've got both Ham and GMRS licensing in the US. reply cdchn 3 hours agorootparentThats TIDRADIO TD-H3 is pretty cool looking. Yeah we live in an interesting time where you can get powerful HAM handhelds for a fraction of what they used to cost. reply shadowpho 17 hours agorootparentprevVery interesting! Where did you get them? reply runjake 15 hours agorootparentApologies if this has some sort of referral in it. I copied straight from the Amazon app. https://a.co/d/gEGW5gr reply vueko 16 hours agorootparentprevYou're going to want to look into CHIRP: https://chirpmyradio.com/projects/chirp/wiki/Home reply gh02t 15 hours agorootparentprevThose are really good radios for the money, have fun. I have nicer radios but still use my UVK5 just because it's easy to use and works well. Plus all the firmware mods are fun to play with. FYI if you're not aware... not to be the no fun police, but you do need a license to transmit with one. And it's technically not legal to use them on FRS/GMRS/MURS frequencies even with a license, though that's probably the least enforced rule on the FCC books. Assuming you're in the US at least. reply jasonjayr 15 hours agorootparentI found the UVMOD website that lets you make a patched Firmware to enable all sorts of hacks. Since I don't have a license yet, the next time I'm playing with it, I am going to enable the TX block on all bands, so as to prevent any accidents. Of course, much to the FCC's annoyance, in the \"dangerous mods\" list, you can also choose to disable the TX block on all bands. You really shouldn't do that, though. The folks that run this project are trying to do the right thing and warn anyone that goes down that path ... https://whosmatt.github.io/uvmod/ reply gh02t 15 hours agorootparentI actually did install the frequency block disable mod. Mine lives in my car as an emergency radio, so I figured might as well have the ability in an emergency (which is allowed) and just stick to frequencies covered by my amateur license normally. Granted the mod isn't really very useful since the radio performs very poorly out of band and sprays out interference, so I should probably just take it back off. I don't think the FCC cares too much about mods like that on hardware that's meant for licensed amateurs. Amateur license covers all sorts of radio modifications, you're supposed to police yourself and manage what you have the right to do. It's actually using the mod to transmit out of band they would fine you for (theoretically... you're unlikely to get caught), not just having the ability. The thing that they get pissed off about is when people without a license can buy hardware meant for amateurs and easily use it out of band without really needing to do anything. That's what they've cracked down on in the past, especially shady importers on Amazon marketing unlocked amateur radios for bands like FRS. reply sholladay 18 hours agorootparentprevWhat would you call them, then? “Isolation codes” or “subchannels”, maybe? I’ve seen some use of the former, but both are imperfect terms. I’ve yet to hear a suggestion that’s particularly better. A term that’s only slightly better won’t gain any traction. reply paulmd 17 hours agorootparent\"squelch codes\" probably conveys the meaning more correctly. people understand the idiom that just because your radio has squelch set (too high, perhaps) that it doesn't mean someone else can't hear it. otherwise, CTCSS codes works fine, that's the technical description of what it is. and actually they call them \"squelch tones\" there. it's only motorola that branded them as \"private lines\", that's their trademark for an adequately-described term. Much like Tesla \"Full Self Driving\"/\"Autopilot\", it's kind of a misnomer and definitely breeds (deserved) confusion. https://en.wikipedia.org/wiki/Continuous_Tone-Coded_Squelch_... reply teeray 17 hours agorootparentprev“Selector codes” would be more descriptive. They select which conversations you want to hear. reply blackeyeblitzar 17 hours agorootparentprev [–] What prevents people from transmitting encrypted information? Isn’t that just like speech that might travel over the airwaves? reply lxgr 16 hours agorootparent [–] Technically nothing, but legally it's just not allowed in the US (neither for FRS/GMRS, nor for ham radio operators, with very few exceptions). reply stackedinserter 2 hours agorootparent [–] Interesting, is speaking in a language that nobody else understands counts as \"encrypted transmission\"? reply blackeyeblitzar 2 hours agorootparent [–] That’s what I mean. Isn’t any data the same as speech in general? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Flipper has released a major 1.0 firmware update for its Flipper Zero multi-tool, significantly enhancing its functionality and user experience.",
      "Key improvements include doubling Bluetooth data transfer speeds from Android, a 40% increase in Bluetooth firmware installation speed, and a revamped NFC engine supporting more card types and faster data reading.",
      "The update also introduces new features such as the ability to eavesdrop on analog walkie-talkie audio, decode 89 radio protocols, run apps directly from microSD cards, and extend battery life to a month in low-power mode."
    ],
    "commentSummary": [
      "Flipper Zero, a crowdfunded device, has received a significant firmware update enabling it to eavesdrop on walkie-talkies, fulfilling its promise of ongoing software enhancements.",
      "The device is notable for its versatility and user-friendly interface, making it accessible for various radio frequency tasks, unlike traditional software-defined radios (SDRs) which require more powerful processors.",
      "The update has sparked interest due to Flipper Zero's ability to perform multiple functions beyond eavesdropping, positioning it as a comprehensive tool for radio frequency enthusiasts and professionals."
    ],
    "points": 216,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1726002945
  },
  {
    "id": 41504885,
    "title": "Tutorial on diffusion models for imaging and vision",
    "originLink": "https://arxiv.org/abs/2403.18103",
    "originBody": "Computer Science > Machine Learning arXiv:2403.18103 (cs) [Submitted on 26 Mar 2024 (v1), last revised 6 Sep 2024 (this version, v2)] Title:Tutorial on Diffusion Models for Imaging and Vision Authors:Stanley H. Chan View PDF Abstract:The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems. Subjects: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2403.18103 [cs.LG](or arXiv:2403.18103v2 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2403.18103 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Stanley Chan [view email] [v1] Tue, 26 Mar 2024 21:01:41 UTC (3,221 KB) [v2] Fri, 6 Sep 2024 19:58:27 UTC (3,822 KB) Full-text links: Access Paper: View PDF TeX Source Other Formats view license Current browse context: cs.LGnewrecent2024-03 Change to browse by: cs cs.CV References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=41504885",
    "commentBody": "Tutorial on diffusion models for imaging and vision (arxiv.org)208 points by Anon84 23 hours agohidepastfavorite15 comments maCDzP 3 hours agoSo anyone up for a discussion on this paper at: https://www.alphaxiv.org/abs/2403.18103 reply redblacktree 22 hours agoprevDoes something similar exist for LLM/GPT? Edit to add: I'm mostly interested in this aspect: \"The target audience of this tutorial includes [those] who are interested in [...] applying these models to solve other problems.\" reply BaculumMeumEst 21 hours agoparentAndrej Karpathy has a youtube playlist: https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThs... He is building new learning materials under his new company \"Eureka Labs\": https://eurekalabs.ai Sebastian Raschka's book \"Build a Large Language Model (From Scratch) just released: https://www.manning.com/books/build-a-large-language-model-f... All of these resources are excellent. reply humansareok1 4 hours agoparentprev3Blue1Brown has a pretty great video series walking through Transformers: https://www.youtube.com/watch?v=wjZofJX0v4M reply roninorder 22 hours agoparentprevAndrej Karpathy has very good video tutorials on how to write your own GPT: https://www.youtube.com/watch?v=kCc8FmEb1nY reply israrkhan 18 hours agorootparentThis is excellent reply GaggiX 21 hours agoprevA very useful guide about how diffusion models work and implementation: https://keras.io/examples/generative/ddim/ I find the explanation in this article very intuitive. reply yinser 21 hours agoprev [–] > see tutorial on diffusion > get excited > it's all math in latex > despair reply slashdave 20 hours agoparentLatex is great in this case, because the equations are all written out clearly. If you want to understand diffusion, it's a little difficult to avoid math. reply isaacfung 12 hours agoparentprevYou may find the huggingface course more approachable https://huggingface.co/learn/diffusion-course/en/unit0/1 reply isaacfung 7 hours agorootparentAlso this blog post https://yang-song.net/blog/2021/score/ reply sva_ 20 hours agoparentprevMaybe check out the fast ai course. Jeremy Howard has a way of explaining stuff. reply whimsicalism 20 hours agoparentprevthe math is learnable and with gpt nowadays, extremely so reply mdp2021 13 hours agorootparentYou are right! LLMs accept the formulas and may explain them... It will be more difficult to tell when they are wrong, though - when you cannot verify directly. But it will be a device for people to get acquainted with the math. reply Sharlin 12 hours agoparentprev [–] Diffusion is math. It’s difficult to avoid math if you want to understand math. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The tutorial by Stanley H. Chan focuses on diffusion models, which are pivotal in generative tools for text-to-image and text-to-video applications.",
      "It is aimed at undergraduate and graduate students interested in machine learning and computer vision, providing foundational knowledge for research or practical applications.",
      "The tutorial has been updated twice, with the latest version submitted on September 6, 2024, and is available on arXiv for further reading."
    ],
    "commentSummary": [
      "A tutorial on diffusion models for imaging and vision has been highlighted, sparking interest among tech enthusiasts and researchers.",
      "Various resources and discussions are shared, including Andrej Karpathy's YouTube tutorials, Sebastian Raschka's new book on building large language models, and 3Blue1Brown's video series on Transformers.",
      "The tutorial emphasizes the mathematical foundation of diffusion models, with suggestions for more approachable resources like the Hugging Face course and blog posts for better understanding."
    ],
    "points": 208,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1725998379
  },
  {
    "id": 41504832,
    "title": "Git Bash is my preferred Windows shell",
    "originLink": "https://www.ii.com/git-bash-is-my-preferred-windows-shell/",
    "originBody": "Infinite Ink Updates Portals Git Bash Is My Preferred Windows Shell Updated by nm  2024-January-19 Page contents News About shells🐚 About bash What is Git Bash Why I Prefer Git Bash Communities Managing Git for Windows and Git Bash Installing Launching Inspecting Updating Uninstalling Mintty customizations Mintty color scheme🎨 Mintty font and character set Git Bash customizations Display some of Git Bash’s default settings ~/.bash_profile Aliases Git Bash tips Case insensitivity Paths The start command Launching any app on your path Environment variables Scripting Where to put scripts Example script No need to chmod Unicode Case insensitivity exceptions --help and other command arguments exit See also Endnotes Please share & discuss Page meta Site meta News Ongoing According to Repology, the latest packaged Git for Windows is version . To keep up with releases of Git for Windows, which includes Git Bash and the Mintty terminal emulator, see github.com/git-for-windows/git/releases or groups.google.com/g/git-for-windows. 2023-November-18 As of today, this evolving⁠[1] article has been on the web for 3 years.🕯🕯🕯 About shells🐚 A shell is a layer or interface between you and an operating system. There are a lot of command-⁠line shells for Windows, including: COMMAND.COM, also known as “the DOS Prompt” (used in Windows 9x and earlier) Cygwin UWIN cmd.exe, also known as CMD and “Command Prompt” PowerShell[2] WSL Bash, also known as “Bash on Ubuntu on Windows” and “Bash on Windows’ Subsystem for Linux”⁠[3] Over the years, I’ve used all six of the above Windows command-⁠line shells. In 2020, I installed Git for Windows and can now add a seventh shell to the list of command-⁠line Windows shells I’ve used: Git Bash About bash Bash is an acronym for “Bourne again shell” and it is a command-line shell that was created for Unix-like systems. Details are at wikipedia.org/wiki/Bash_(Unix_shell). What is Git Bash Git Bash is a Windows command-line shell that is packaged with Git for Windows. It emulates the bash shell that is familiar to nix-nux users.⁠[4]⁠[5] 💡Git Bash is useful even if you do not use git. ‼Git Bash is not one of Windows’ subsystems. When you are in Git Bash, you are in your regular Windows environment. And your Windows environment variables are available.⁠👍 Why I Prefer Git Bash I like Git Bash because… I’m more comfortable with bash commands than CMD or PowerShell commands (because I’ve been using Bourne shell (sh) and friends for more than 30 years). It’s easy to install. It has a small footprint. It installs a Git Bash Here command that is available when right-⁠clicking⁠[6] on a folder in Windows File Explorer. I can pipe the output of commands through the less pager, which is better than the more pager.⁠[7] In addition to the less command, the commands awk, cat,⁠[8] cd, clear, cp, curl, cut, cygpath, date, dd, diff, dos2unix, echo, file, find, git, gpg, grep and friends,⁠[9] gunzip, gzip, head, history, jobs, kill, ls, md5sum, mkdir, mv, mount, nano, od, openssl, perl, printenv, ps, pwd, rm, rmdir, scp, sed, sftp, sha1sum, sha256sum, sha512sum, sort, split, ssh, stat, tail, tar, tig, touch, uname, uniq, vim and friends,⁠[10] wc, which, whoami, xz, and more[11] nix-⁠nux commands are available. I can view built-in help for a bash command with, for example, less --help or pwd --help. Almost any app that’s on my path — including hugo, figlet, and tree.com[12] — can be launched either within a Git Bash script or at a Git Bash command prompt (possibly with the help of winpty).[13] I can write bash or sh⁠[14] scripts and… my Windows environment variables are available, and I do not have to worry about WSL file system issues[15] because Git Bash uses the regular Windows file system (but be aware that Git Bash uses forward-slash-delimited paths to refer to the Windows file system). I can use output redirection (with, for example, >) to create a file that is UTF-8 Unicode text rather than Little-endian UTF-16 Unicode text, with CRLF line terminators (which is the CMD and PowerShell default encoding⁠🙀). Communities Git Bash has helpful communities, including: github.com/git-for-windows/git/discussions stackoverflow.com/questions/tagged/git-bash stackoverflow.com/questions/tagged/git-for-windows superuser.com/questions/tagged/git-bash github.com/git-for-windows/git/issues?q=is%3Aissueissues?utf8=✓&q= Almost everything I know about Git Bash, I learned in these five communities. Managing Git for Windows and Git Bash Installing To install Git Bash, install Git For Windows, which you can download from either… git-scm.com/downloads or gitforwindows.org. Think carefully about what options you choose during the installation. If you are a power user, the defaults are probably not what you want. For example, I like to change the default install directory, which is C:\\Program Files\\Git\\, to C:\\Program Files\\GitForWindows\\ because this helps me remember the full name of the app in this directory. ℹGit for Windows requires Windows Vista or newer. Windows Vista was released in 2007 (~17 years ago). Launching To learn about launching Git Bash in various terminal emulators, including Mintty (which is Git Bash’s default terminal emulator), see Launching Git Bash in Mintty, ConEmu, VS Code’s Terminal, and Other Terminal Emulators. Inspecting To learn about the Git for Windows that you are using, run some or all of the following at a Git Bash⁠[16] command-line prompt: git --version git --version --build-options uname -a To see what options it was installed with, run the following at a Git Bash prompt: cat /etc/install-options.txt Updating To update Git for Windows, run the following at a Git Bash⁠[16] command-line prompt: git --version git update-git-for-windows git --version ‼ When you update Git for Windows… Your home (~) directory should be preserved, but to be safe, back it up before you update. Your Git Bash /etc/ and other root-level Git Bash directories will not be preserved (and ideally you should not edit these directories). Uninstalling To uninstall Git for Windows, see How to uninstall git? in github.com/git-for-windows/git/discussions. Mintty customizations Mintty is Git Bash’s default terminal emulator. Details are at wikipedia.org/wiki/Mintty. Mintty color scheme🎨 Since I prefer the Solarized Light color scheme, I edited ~/.minttyrc so it includes the following. ForegroundColour=101, 123, 131 BackgroundColour=253, 246, 227 CursorColour= 220, 50, 47 Black= 7, 54, 66 BoldBlack= 0, 43, 54 Red= 220, 50, 47 BoldRed= 203, 75, 22 Green= 133, 153, 0 BoldGreen= 88, 110, 117 Yellow= 181, 137, 0 BoldYellow= 101, 123, 131 Blue= 38, 139, 210 BoldBlue= 131, 148, 150 Magenta= 211, 54, 130 BoldMagenta= 108, 113, 196 Cyan= 42, 161, 152 BoldCyan= 147, 161, 161 White= 238, 232, 213 BoldWhite= 253, 246, 227 This is the contents of the .minttyrc.light file that is part of github.com/mavnn/mintty-colors-solarized. Thank you Michael Newton (@mavnn@github.com) for making this color scheme available on GitHub! Mintty font and character set To change the character set and fonts used in the mintty terminal emulator, right click anywhere in a mintty window or title bar and choose Options. In the left pane, choose Text and then… Click Select… and choose DejaVu Sans Mono, 10pt (or whatever you prefer) for the font. Under Character Set, choose UTF-8 (Unicode) (or whatever you prefer). Click Save These and other settings will be appended to the bottom of your ~/.minttyrc file. Git Bash customizations Display some of Git Bash’s default settings Before you customize Git Bash, check out some of its default settings by running the following four commands at a Git Bash command-line prompt. alias mount printenv set -o ~/.bash_profile If ~/.bash_profile exists, the commands in it are run each time you launch a Git Bash terminal. To create and edit this file do the following. Launch a Git Bash terminal. At the Git Bash command-line prompt, run one of these commands: nano ~/.bash_profile vim ~/.bash_profile Note that the TUI (terminal user interface) editors nano and vim are included in Git Bash and are on your PATH. (If you do not know how to use vim, I recommend that you use nano because, otherwise, it may take you a while to figure out how to exit vim.⁠🤣) Add whatever commands you’d like to this file. Here is an excerpt of my ~/.bash_profile: ## Created: 2024-05-15 ## Use `gvim filename` to edit filename with GUI Vim alias gvim=\"/C/Windows/gvim.bat\" ## Use `cdi` to cd to the Infinite Ink project root ## Note I set INFINITEINKROOT environment variable via Control Panel alias cdi=\"cd `cygpath $INFINITEINKROOT`\" ## Do not let me overwrite a file with > redirection set -o noclobber ## Make Backspace and other things work in VS Code's & IntelliJ's terminals export TERM=cygwin # export TERM=xterm # export TERM=xterm-256color ## NOTE: All 3 of the above TERM settings work for me Save your edits and close the editor. At the Git Bash prompt, use the following file command to check the file type. file ~/.bash_profile This will probably display ASCII text or UTF-8 Unicode text. If either “BOM” or “CRLF line terminators” are mentioned in the output of this file command, convert it to Unix-format with one of the following commands. dos2unix ~/.bash_profile nano --unix ~/.bash_profile To learn about these commands, run the following: dos2unix --help nano --help 💡After you change your ~/.bash_profile, I recommend that you close all Git Bash sessions and all Windows File Explorer windows, and then relaunch Git Bash and make sure your new ~/.bash_profile settings worked. Aliases To see what aliases are available, run: alias On my system, this displays: alias gvim='/C/Windows/gvim.bat' alias cdi='cd /c/Users/USERNAME/path/to/infiniteink/project/root' alias ll='ls -l' alias ls='ls -F --color=auto --show-control-chars' The first two aliases are the ones I created above in my ~/.bash_profile. The last two are Git Bash’s default aliases. ‼Default aliases (and more) are specified in files that are located in the /etc/profile.d/ directory. It is best not to edit files under /etc/ because they might be replaced when Git for Windows is updated. Git Bash tips Case insensitivity Normally Unix-like shells are case sensitive, but Git Bash, in general, is case insensitive. Paths To view your current path, run the following command, which means “print working directory.” pwd To go to your Git Bash home directory, which is also known as ~, run either of the following equivalent commands. cd cd ~ On most systems, your Git Bash home directory is… /c/users/USERNAME/ with USERNAME replaced with your user name. Note that in Git Bash… The path delimiter is slash (/), which is also known as forward slash. The C: drive is mounted as /c/. To list all mounts, run the mount command. You can display a Windows-type path of the current working directory with pwd -W. The format of the path this command displays is sometimes called “mixed type” because it starts with a drive letter, such as C:, and uses forward slashes (/) rather than backslashes (\\) as path separators. For example, this sequence of commands… cd pwd pwd -W displays… /c/Users/USERNAME C:/Users/USERNAME The start command When you are at a Git Bash prompt, you can launch a file or directory in your system’s default app by using the Start command, which is equivalent to the start command (thanks to case insensitivity). Below are some examples. To open the current directory in Windows File Explorer, run: start . 👆 Notice this dot (.) To open your home directory in Windows File Explorer, run: start ~ To open your $APPDATA directory in Windows File Explorer, run: start $APPDATA To open /c in Windows File Explorer, run either of these equivalent commands: start /c start /c/ To open the parent directory of the current directory in Windows File Explorer, run: start .. To open an HTML file in your default web browser, run something like this: start ReleaseNotes.html Launching any app on your path You can launch any app that’s on your path from a Git Bash prompt. For example, if Visual Studio Code is installed on your system and is on your path, you can use the following to open the current directory in VS Code. code . 👆 Notice this dot (.) Environment variables To find out the environment variables available to Git Bash, run: printenv This includes the PATH environment variable, which lists the directories that are searched for executables. Scripting I’d rather write a bash or sh⁠[14] shell script than a Windows batch file to deal with apps, files, and folders that live in my Windows file system. Thanks to Git Bash, It’s easy to do this. (I have learned the hard way that it’s not so easy to do this in WSL.⁠[15]) Where to put scripts When you run printenv, you can see that the ~/bin/ directory (usually /c/users/USERNAME/bin/) is on your PATH. This is a reasonable place to put your Git Bash scripts. To create this directory, run this command: mkdir ~/bin/ Example script One of my scripts is called gvim-winpath and it looks like this: #!/bin/sh ## Created: 2024-05-15 ## Filename: gvim-winpath ## Usage: gvim-winpath \"C:\\path\\to\\filename\" /c/windows/gvim.bat `cygpath \"$1\"` I use this to launch gvim on a file that is specified using a Windows-style backslash path. To learn about the cygpath command, which is used in this script, run one of the following commands at a Git Bash prompt. cygpath --help cygpath --help |less If you pipe the cygpath --help output to the less pager, you need to know how to use less: The essentials are that within less, you can press Space to page down, press b (for back) to page up, and press q to quit. 💡cygpath is analogous to wslpath, which I use in Infinite Ink’s qutebrowser Userscripts on Windows. No need to chmod In most Unix-like shells, you need to chmod +x executables but in Git Bash this is not needed. Unicode If you have issues with non-ASCII Unicode characters, run the following at a Git Bash prompt and make sure each setting includes UTF-8. locale Also, you can try to solve Unicode issues by running the following sequence of chcp.com⁠[12] commands at a Git Bash prompt. chcp.com chcp.com 65001 chcp.com This changes the code page to 65001, which supports UTF-8 encoding. ℹNowadays UTF-8 is the standard file encoding of the internet (and of Unicode in general). Case insensitivity exceptions --help and other command arguments Arguments to nix-⁠nux commands, in general, are case sensitive. For example, in the following cygpath --help The command cygpath is case insensitive, but its argument --help is case sensitive. For example, this works: Cygpath --help but this does not work: cygpath --Help exit To quit Git Bash, you can either click the close-window X in the upper right corner of the terminal window or type the following at the command prompt: exit This exit command is a system call and must be all lower case.⁠ ◊ See also For more related to Git Bash, see Infinite Ink’s… I Use This⁠🛠 qutebrowser Userscripts on Windows Launching Git Bash in Mintty, ConEmu, VS Code’s Terminal, and Other Terminal Emulators #git-bash Portal #nix-nux Portal Endnotes 1. Many Infinite Ink pages, including this one, are evergreen 🌲 and regularly updated. 2. I have experience using Windows PowerShell (powershell.exe), but do not yet have experience using the cross-⁠platform PowerShell Core (pwsh). To learn about these command-⁠line shells, see wikipedia.org/wiki/PowerShell. 3. It would probably make more sense if WSL were called LSW (Linux Subsystem for/of/on Windows), but it is not because, according to Rich Turner in this tweet, Microsoft “cannot name something leading with a trademark owned by someone else.” Rich’s tweet has inspired me to interpret the WSL acronym as meaning Windows’ Subsystem for Linux (notice the apostrophe (’)). 4. Git Bash uses MSYS2 (Minimal SYStem 2) and MinGW (Minimalist GNU for Windows) to do this emulation. 5. Git Bash is sometimes called “bash on MSYS” or “MSYS bash.” 6. In Windows 11, you need to Shift+Right-click on a folder name to see the Git Bash Here command. 7. The more command is not included in Git Bash, but the less command, which can do everything more can do (and more), is included in Git Bash. 8. cat stands for concatenate.😸 9. grep stands for “get regular expression.” Its friends include egrep and fgrep. 10. vim's friends include view, vimdiff, and vimtutor. 11. To list most available commands, run ls /usr/bin and ls /mingw64/bin. 12. tree.com and chcp.com are located in /c/windows/system32/ and are on your Git-Bash path. Each must be called with the .com file extension. 13. When launching an app from Git Bash, it’s sometimes useful to append & to the end of the command so it will be launched as a background job. 14. bash is “Bourne again shell” and sh is “Bourne shell”. 15. To learn about some problems with WSL, see Windows Subsystem for Linux: The lost potential by Julio Manuel Merino Vidal. Also see the Hacker News comments about that article at news.ycombinator.com/item?id=25154300. 16. Alternatively, you could use a PowerShell or CMD command-line prompt for this command. Because this article is about Git Bash, I suggest using a Git Bash command-line prompt throughout the article. Note that many of the commands discussed in this article must be run from a Git Bash prompt. Please share & discuss 📝 👎 👍 📯 Your comment or question via this form might immediately improve this page or help me to (eventually) improve this page. On Mastodon, I’m @nm@mathstodon.xyz and I’ll see your toot if you mention @nm@mathstodon.xyz or #InfiniteInk in it. You could also Share this page on Mastodon. Page Meta Git Bash Is My Preferred Windows Shell https://www.ii.com/git-bash-is-my-preferred-windows-shell/ Published 2020-Nov-18 l © Nancy McGough l Updated 2024-Jan-19 Made with Asciidoctor & Hugo More Infinite Ink Pages “Edit This Page” With vscode:// URLs (featuring a Hugo partial) The For-Profit Mozilla Corporation, aka Mozilla DOT COM Connect with Infinite Ink @ Mastodon, GitHub, etc. The Cloud, WebApps, and Desktop Apps qutebrowser Userscripts on Windows Embedding Mastodon Timelines « Hugo and sitemap.txt (& a bit about robots.txt)⁠🤖 Scoop: A Windows Package Manager » Portals:  #activism-hacktivism   #alpine-mail   #asciidoc   #backup-sync   #browsers   #business-economics-money   #cheatsheet   #cli   #elsewhere   #emoji   #forked   #git   #git-bash   #gohugo   #golang   #intellij   #iusethis   #joplin   #longform   #markdown   #mastodon   #mathematics   #mathjax   #messaging   #meta🔮   #nix-nux   #privacy-security   #productivity   #qutebrowser   #tech   #tumblelog   #twitter   #unicode   #utf-8   #vim   #vscode   #webdev   #windows   #words   #zeitgeist  🆕 @ 🚪 🔝",
    "commentLink": "https://news.ycombinator.com/item?id=41504832",
    "commentBody": "Git Bash is my preferred Windows shell (ii.com)193 points by indigodaddy 23 hours agohidepastfavorite158 comments locusofself 17 hours agoI started using Linux in 1997. I've been working at Microsoft for a little over 4 years. There are some annoying things about Powershell (verbosity and slow loading of tab completions etc), but in some ways it's vastly superior to bash, as it deals with structured data and not just strings you have to parse, and it has an extremely powerful standard library API (.NET) at your fingertips. I still love bash and use it as well. But the hate powershell often gets is misguided. reply skinner927 15 hours agoparentThere was a time in Powershell’s life when stdout redirection to a file (>) would write UTF-16, then that changed to UTF-8 with BOM and now it’s UTF-8 no BOM. During those times some PS versions would allow to change this globally, some would not. Even if you could change it globally, the names for the same encodings have changed. PowerShell 5.1 “UTF8” actually meant “UTF-8 with BOM” PowerShell 7 “UTF8” meant no BOM. If you read a file in PowerShell that does not have a BOM it decodes it as ANSI. Yes, ANSI, not Ascii. Which is a hilarious choice because you can’t even write an ANSI file in PowerShell. The closest encoding you can use is “Default” which will use the system default but your systems default might not be ANSI. PowerShell is a hot mess. Go read the official docs for character encoding[1] if you don’t believe me. I have CI scripts that have to use .NET calls[2] in PowerShell to write ssh keys to disk because it’s impossible to write utf-8 no BOM (or ascii) with out carriage returns (\\r) being inserted before every new line () even when the incoming string didn’t have them. File encoding is just ONE of the reasons I hate PowerShell and why it’s so obviously clear people who were not qualified to design a shell, designed a shell. [1] https://learn.microsoft.com/en-us/powershell/module/microsof... [2] https://stackoverflow.com/questions/5596982/using-powershell... reply unscaled 13 hours agorootparentI'm not sure if we should blame the designers of PowerShell (Jeffrey Snover and his team). If you followed the history of their project, it seemed like they had very good ideas, but it was very hard to get buy-in for anything CLI-oriented in Longhorn/Vista-era Microsoft[1]. They've all had experience with Unix shell and their ideas for object based pipes were truly innovative as far as I know. I can't speak for Jeffrey and his team, but I feel like a lot of their decisions came from trying to get corporate behind the shell and present it as a shell for Windows. They avoided picking political battles outside of their main goals (a modern shell to replace cmd.exe and the object-based pipeline model). What we've got are a set of decisions that aligned with Windows and Microsoft practices of that day and age: Microsoft is focusing on .Net as their general platform? We'll implement PowerShell on .Net. Windows's standard for Unicode text files is either UTF-16 LE or having a BOM for any other Unicode transformation? We'll do UTF-16 by default and always add a BOM if you choose UTF-8. Windows is using CRLF? Well, we'd pipe CRLF-delimtied text by default. Visual Basic and C# programmers expect functions to have names like \"GetChildItem\" instead of something like \"dir\" or \"ls\"? No problem, we will set the canonical command name to be a long, programming-language-like name and set up aliases that look (but don't behave) like Unix and cmd.exe commands. The result was not pretty, but I still appreciate the ideas we got from PowerShell. nushell took these ideas and implemented them in a more modern way. [1] https://corecursive.com/building-powershell-with-jeffrey-sno... reply cma 5 hours agorootparentLots of explanations, but why can't you globally configure reading with no BOM if writing with no BOM is now the default? It doesn't seem like it could be for backwards compatibility, as now an old script that writes a file with defaults can't later read it with defaults? reply beart 15 hours agorootparentprevIt's also worth pointing out that those Microsoft docs differentiate between the two major versions of powershell using the names \"Windows Powershell\" and \"Powershell\". reply delusional 14 hours agorootparentI also enjoy how the online manual pages doesn't even try to tell me which version it's about. It's all just powershell https://learn.microsoft.com/en-us/powershell/module/microsof... reply nullindividual 12 hours agorootparentThe version picker is in the ToC on the upper left. reply wodenokoto 13 hours agorootparentprevI think MS has a team dedicated to confusing naming. reply unscaled 13 hours agorootparentThe original project name was \"Monad\", but once it was nearing release, it got into the hands of the notorious naming prodigies in Microsoft marketing. Then it got changed to Microsoft Shell (MSH) and finally to Windows PowerShell. Great times. reply 7bit 2 hours agorootparentIt did not get changed. Windows Powershell is up until 5.1 because it runs ONLY on Windows. Powershell 6 was named Powershell Core due to being built with .NET Core. It was not fully compatible with 5.1 and lacked some functions based on the WinApi. PowerShell 7 is still built on .NET Core, but has a much better compatibility with 5.1. They dropped Core because nobody cared and it's shorter and works equally well on all platforms. So Powershell is the v7 and Windows Powershell is the old 5.1. Nowhere is that confusing. reply pathartl 47 minutes agorootparentAlso worth noting that PowerShell is completely independent from Windows now. Not only can you install it in almost every other platform, since it's .NET you can embed the runtime in any .NET application. It's incredibly powerful. reply ozim 11 hours agorootparentprevWait - you might just have helped me to fix my Invoke-WebRequest testing 3rd party api we have to integrate with. Unexpected character encountered while parsing error line 0 position 0 seems like there is a BOM I wasn't expecting to send in my request and receiver definetly was not expecting to receive it. Edit: yup, just did small cmd line client and all works. Thanks. reply sevensor 5 hours agorootparentprevUTF8 with bom is a terrible idea. “Why does the name of the first column in my csv file not match?” is a question I answer at least once a month. It’s as if they’re trying to EEE plain text. reply zo1 1 hour agorootparentprevHey let's not pretend like it's better in the world of Linux/Unix/etc. Now that is one hot mess of everything, just as much as Powershell. It's just those have things we're used to, and kinda learned to (mostly) abstract away without the presence of an over-bearing behemoth such as Microsoft which makes such a thing almost impossible. As a comparison (to most of us semi-knowledgeable windows / powershell users), ask any semi-knowledgeable Linux user about the difference between Ksh/Bash (or that weird one that Ubuntu pushed and aliased to one of the above for extra confusion). Or ask them if any of them remember how tf to use SED or AWK without looking up the docs because those damn things are made by satan in the depths of hell. Or how to use parameters inside shell scripts, and the various incantations they have to slice and dice the params. reply 7bit 2 hours agorootparentprev> File encoding is just ONE of the reasons I hate PowerShell and why it’s so obviously clear people who were not qualified to design a shell, designed a shell. I find that unfair towards their creators. They invented a truly innovational shell, whereas everybody else continued the text-based approach. The object-oriented approach is so much simpler when it comes to process results of Get-ChildItem (ls) or Get-Process (ps) when you get objects with properties, instead of just text. Especially when in text-based shells it matters how you call e.g., ps (aux or -efH or whatever). Now, I find your statement unfair, because in Linux world, you just create a new shell and whoever wants to use it, can. At Microsoft, a successor to cmd.exe had to be shipped with Microsoft, otherwise it would never have been adopted. Most big companies would never allow a third-party open-source shell on their Windows servers. Therefore, you must navigate a big ocean of politics and powers, guarantee nackwards-compatibility and meet expectations of thousands of companies. This inevitably leads to behaviour like encoding that is frustrating to use. Until you read the docs, which state quite comprehensively what you have to expect - as you discovered yourself. reply cassepipe 5 hours agoparentprevNushell is plenty fast, works with structured data (also has a vi mode if you care, I do) and works equally well on Windows and Linux. You can install it with winget on windows and with cargo or the system's package manager on linux. https://www.nushell.sh/ reply Yodel0914 13 hours agoparentprevI've used powershell a bit, particularly for scripting CI pipeline steps. No matter how much I use it, I can never remember anything about the syntax. I primarily use C#, so you'd think there'd be some affinity, but nope. It's like a black hole. I've written in a lot of different languages in my time and never had this issue with anything else. It's weird. reply Fire-Dragon-DoL 11 hours agorootparentTo be fair, I have a similar problem with bash. There is no way to remember how to write a for loop for me reply rewgs 12 hours agorootparentprevExact same here. I write roughly the same amount of Powershell as I do Bash, and yet to this day I feel like I barely know Powershell and constantly look up the simplest things, whereas loads of dark corners in Bash stick in my mind perfectly fine. reply devnullbrain 13 hours agoparentprevCan Powershell tee a background task yet? reply lnxg33k1 16 hours agoparentprevI feel it's nice to have the power of choice, I can deal with structured data in bash as much as I need, whenever I need, with awk, but it isn't there when I don't reply irunmyownemail 16 hours agoparentprevBash rocks because it is about the user and the OS. Powershell is about one thing, Microsoft and the ugliness of ItS CumBerSomE-SynTaX Du-Jour is proof. reply delta_p_delta_x 21 hours agoprev> I like Git Bash because… > I’m more comfortable with bash commands than CMD or PowerShell commands (because I’ve been using Bourne shell (sh) and friends for more than 30 years). This is the only argument I'm willing to accept, and honestly it's the only argument the writer needs, really: un-learning 30 years of experience to use something else even if the latter is objectively superior, would be a pain in the neck for anyone. That being said I believe that seriously attempting to use the native tools would help the author understand Windows at a more fundamental level. Also, this does raise a few questions, though—if the writer has been using UNIX shells for that long, why are they using Windows at all? As I commented elsewhere, why shoehorn the whole set of UNIX core utilities and binary utilities onto Windows when the latter comes with its own shell, command-line utilities, and management tools? More importantly, the author going by the rest of their posts (assuming it was the same author) doesn't strike me as the type (i.e. normie, for lack of a better word) to use Windows as a daily driver, but they seem to use it all the same. Therefore, why not an OS where these utilities feel more native and more useful, like Linux or one of the BSDs? In contrast to the author I've been using and programming almost nothing but Windows (with a brief stint on Linux in university and now at work, but only for work), and on Windows I use PowerShell and if I really need to, CMD.exe (good riddance). As far as compilation goes, I use Visual Studio with MSVC and Clang-Cl, because they are the only complete development environment for Windows—all the others (MinGW, Cygwin, MSYS2) are incomplete or worse in some way, or make life harder than necessary in some way. > I have experience using Windows PowerShell (powershell.exe), but do not yet have experience using the cross- platform PowerShell Core (pwsh). For the record, functionality between powershell.exe and pwsh.exe is almost identical, with some minor differences[1]. Additionally they differ in the .NET runtime they were written against (.NET Framework 4.8 for powershell.exe, and .NET 7 and later for pwsh.exe). [1]: https://learn.microsoft.com/en-gb/powershell/scripting/whats... reply heavyset_go 21 hours agoparent> Also, this does raise a few questions, though—if the writer has been using UNIX shells for that long, why are they using Windows at all? As I commented elsewhere, why shoehorn the whole set of UNIX core utilities and binary utilities onto Windows when the latter comes with its own shell, command-line utilities, and management tools? I'm a Linux user and developer who writes software targeting Windows. Tools like Git Bash and MSYS2 let me write one set of build scripts that work across Windows, macOS and Linux, because all of the tools exist on the three platforms. I don't have a need to understand Windows on a more fundamental level, nor do I want to. I just want to meet my users where they are at, which is Windows/macOS/Linux. reply grepfru_it 5 hours agorootparentGit bash has a terminal emulation problem. It’s fixable, but annoying problem that breaks tools. I recommend Cygwin or more recently WSL. YMMV reply delta_p_delta_x 20 hours agorootparentprevFair point. I've edited my comment to show my impression that the author uses Windows as a daily driver, rather than merely 'for work', which I think makes the cause for my confusion a little clearer. reply wilsonnb3 20 hours agorootparentprev> Tools like Git Bash and MSYS2 let me write one set of build scripts that work across Windows, macOS and Linux, because all of the tools exist on the three platforms. Powershell is cross platform too these days, which makes this kind of a moot point. reply doubled112 19 hours agorootparentPowershell on Windows is great because there’s a module for everything. I’d rather parse structured data. Powershell on Linux isn’t worth is because I end up having to parse everything as text anyway, completely negating any benefit I can come up with. It doesn’t feel as natural as the *nix tools to me. reply partdavid 19 hours agorootparentFor what it's worth, I've had the opposite experience, maybe because I work in a different domain. Personally, I still think using Powershell functions and treating lines as string objects is an improvement over bash and its mini-languages, anyway; and most standard Unix commands have Powershell close equivalents. Some Powershell expressions might be abstruse but so is, say, awk; but when you've learned awk you only know awk; whereas the Powershell technique you used (calculated fields in select-object, for example) is permanently useful. I do spend a lot of my time with various random APIs so Powershell is pretty joyful compared to (say) curl/jq or yq-ish stuff. There was a real adjustment period, of course, getting used to a new shell; but it was the best thing about my experiment with Windows (discovering a new shell that I now use on my Unix-like systems, because I did not end up wanting to adopt Windows). So I'm curious what kinds of things you mean when you say you have to parse everything as text anyway. I suspect you've needed to a lot more of those kinds of tasks than I have. reply doubled112 16 hours agorootparentPowershell's CSV, XML and JSON handing is my favourite Powershell feature. Completely agree with you that it beats a *nix shell and things like jq. If I was interacting with those all day I might be convinced to use it more. What I mean by parsing as text was more about actual commands. I wasn't expecting 1:1, but as an example, let's say I wanted to get a list of local users on a machine. On Windows I can use \"Get-LocalUser\". It returns an object with properties you can filter, output with a format, etc. Basic stuff that saves me doing any thinking. I like that. It's amazing how column titles add to the user friendliness. On Linux, Get-LocalUser doesn't exist, so I could run something like \"getent passwd\". It's only option is to return a screen of text. There's no structure yet, just lines and colons. Now I can (even though I shouldn't) grab fields with cut, or awk, and filter with grep, and maybe output something pretty at the end with column. What does the 5th column mean again? So on the Linux side, I've never been very motivated to use it, since I often have to fall back to old ways anyway. reply rastignack 13 hours agorootparentprev> Powershell on Windows is great because there’s a module for everything. I’d rather parse structured data. It’s also really slow for text processing, borderline unusable reply notpushkin 18 hours agorootparentprev> Powershell on Linux isn’t worth is because I end up having to parse everything as text anyway Does it at least integrate with DBus a bit? reply heavyset_go 15 hours agorootparentprevBash + coreutils/busybox/etc are everywhere, I can take them for granted. I will have to jump through hoops to deploy Powershell outside of Windows. I also have no desire to use Powershell when I don't need to. reply dangus 15 hours agorootparentBut they aren’t on windows, so they can’t be taken for granted there. You have to install git bash on windows. What’s the difference between installing git bash on windows and installing powershell on Mac/Linux? I would argue that there is none. And obviously it’s fine that you don’t want to use PowerShell, I don’t want to use it either. But personally I also dislike git bash on windows. It doesn’t feel like it belongs there, just like powershell doesn’t really belong on Linux/Mac. If I had to write a build script for all three platforms I would just write them for their native environments. I wouldn’t be all that willing to add another tool to be installed just to save myself the effort of simple build script. reply heavyset_go 15 hours agorootparentI can run an MSYS2 installer on a Windows VM once and be done with it. I will have to roll my own containers, for example, if I want to use Powershell elsewhere, versus just using vanilla or vendor-provided images. For my use case, I don't see the need to bend my builds around Powershell just to use Powershell. This plays into my other point about not wanting to dig into Windows fundamentals when I don't need to. > If I had to write a build script for all three platforms I would just write them for their native environments. I wouldn’t be all that willing to add another tool to be installed just to save myself the effort of simple build script. This was my initial approach, but it became too much work to maintain separate build systems, especially when it came to making changes over time. One change becomes three, along with three new opportunities for things to break in three separate ways. For what I'm doing, moving to Unix-y toolchains simplified builds and made it much easier to reason about it. reply dangus 6 hours agorootparentI totally get that your solution works for you and that is fantastic. I’m only questioning the pure logic behind it when you strip away opinion/preferences. You picking one OS that you’re okay with modifying but the other one you’re not. If we inversed that preference and said that I’m willing to modify my Linux containers but unwilling to install software on my Windows VM, the logic is the same. By the way, Microsoft publishes Linux images that already have PowerShell pre-installed. https://learn.microsoft.com/en-us/powershell/scripting/insta... So, technically, if your build scripts were 100% PowerShell you could have it where both Windows VMs and Linux docker images could be unmodified. Since your build scripts sound overly complex, they could possibly benefit from a scripting language that’s got support for objects and object data pipelines, more advanced error handling, and more advanced object parsing than tools like sed and awk. Just playing “devil’s advocate” here, the logic checks out. reply MichaelRo 13 hours agorootparentprev>> What’s the difference between installing git bash on windows and installing powershell on Mac/Linux? I would argue that there is none. Big, big difference. My Windows laptop is basically a terminal to the Linux development server. I can install anything (within reason) on Windows, can't install shit on the Linux server unless I beg the IT admin for days and eventually he just says \"no\". He did install the \"mc\" (Midnight Commander) utility on it though and made my life like 100x easier. If MC were bundled with git bash, that would be something I would cheer for. And yeah, I use git bash too on Windows. For accessing the Linux shell through ssh when I remotely connect to the server and for convenience locally. Main workhorse are the \"less\" and \"grep\" utilities to examine logs. What am I goint to use otherwise, Notepad? reply dangus 6 hours agorootparentJust because you aren’t familiar with Windows commands doesn’t mean they don’t exist. Microsoft actually gives you SSH without the need to install git bash. https://learn.microsoft.com/en-us/windows/terminal/tutorials... grep on windows is Select-String https://learn.microsoft.com/en-us/powershell/module/microsof... More command on windows: https://learn.microsoft.com/en-us/windows-server/administrat... What will you use to view logs, Notepad? Well, you could use the #1 most popular text editor that Microsoft happens to develop, VSCode. Or a wide array of Windows GUI software that isn’t available on Linux, like Notepad++. The fact that your IT department has those specific policies in place and is inflexible with your developers isn’t really relevant to this discussion. The truth is that if you wanted PowerShell on Linux it is trivial to install. Microsoft even publishes Linux images that already include PowerShell. reply metadat 19 hours agorootparentprevPowershell had potential, but it didn't pan out. It's annoying and very challenging to remember all the one-off flags. With bash, you only need to learn 10 or 20 short commands and a few flag variants, over a gently long period of time. Then you're all set for 97% of cases likely to be encountered. reply ants_everywhere 19 hours agorootparentI just wrote a two line bash script to process some large csv files that Python/Pandas were taking far too long to process [0]. A lot of people (myself included) tend to underestimate the power of the heavily optimized GNU utilities and the ability to manipulate them with bash. [0] It was two lines for readability. It could have easily been a single line. reply partdavid 18 hours agorootparentprevThis is the opposite of how I see it. There's really nothing universal about flags, especially short ones, in different random utilities (what's -n mean? -f? file, filter or force?). Whereas in Powershell since there actually is some standardization, your knowledge about what -WhatIf or -Verbose do is actually pretty leverageable. And more powerful with the ability set default parameters and so forth. And the discoverability and help in Powershell is also standardized. So I get why unfamiliarity would make it not worth it to switch from whatever shell you prefer, but I don't think I can see your point about flags. reply addicted 19 hours agoparentprevI used bash in college and then didn’t use it for around 5 years. And then I used powershell. And bash is still a lot better as a shell language. Powershell commands are long and annoying. If you remember all the aliases (most of which are annoyingly bash commands but hardly behave that way) then the parameters are long and annoying. Stringing commands together requires too much API digging because objects mean you need to run a variety of incantations on the results to know what members you need to pull out and what data format they have, etc. in Bash it’s always strings and if you know a couple of useful string manipulation commands such as cut, that you probably know anyways for other reasons, you can get by with stringing most commands fairly trivially. Powershell OTOH is a lot better when I’m trying to write an actual reusable script. But at that point why wouldn’t I just use something like Python anyways (or if it will be associated with a front end repo I will often just use nodejs since it’s definitely installed). Python has much more usable comparison, control flow, etc operators, much bigger libraries, better multi OS compatibility etc. The only advantage really is using powershell commands/applications, so sometimes I will write my scripts in Powershell. But in Linux I will also sometimes write my scripts in bash for the same reason, so it’s not really an advantage inherent to powershell, but the fact that it’s the “standard” of the OS. I do like Powershell, but the problem with it is that it tried to be both a great scripting language and a great shell language, but the things that make it a good scripting language, such as verbose naming, objects, etc make it a worse shell language than shell languages designed to be shell languages first, and the things that it does to make it a useable shell language, makes it a worse scripting language than other scripting languages. On average it may be the best language across all environments and use cases. The problem is that for any specific environment and use case it’s rarely one of the better languages. reply partdavid 18 hours agorootparentI'm not sure I understand your point regarding having to look at the output of commands to know how to use them. At least with Powershell you have tab-completion suggesting your object members; you have to at least look at whatever flavor of semi-parsable text is coming from your command in bash to use cut or awk or whatever on it (and the semantics of what you're looking at are not discoverable, so you're likely to have to some API digging of your own). 'cut' and the like is sadly fragile for a number of reasons, and you generally won't discover them in advance (e.g. when the date \"field\" starts containing a year, sizes overflow or start being indicated with human-readable abbreviations after a threshold, which is exactly the sort of thing you can't tell by inspection). And bash's failure modes are really sharp. To be honest, something like $titled = gci *.md| ?{ (gc $_)[0] -like '# *' } seems short and less error-prone than the bash equivalent. Not sure what it would be. Something like titled=(); for file in *.md; do if head -1 \"${file}\"grep -sq '^# .*'; then titled+=(\"${file}\"); fi; done I think there's a lot of little gotchas in there, and not a little \"API digging\" for options, though it's simple in concept. reply tdeck 17 hours agorootparent> you have to at least look at whatever flavor of semi-parsable text is coming from your command in bash to use cut or awk or whatever on it Having to parse things is definitely a pain, at least until you get good at it. But the critical thing is that all the output is right there on the screen, and often in a format that's at least somewhat designed to be parsed. In PowerShell you have to go diving through the object hierarchy. That would be OK I guess if things were intuitive and the help and documentation were great but that's often not the case. And often the API semantics are designed for a different language altogether (C#) and the things you have to do to consume the API in PowerShell are ugly. reply lenkite 10 hours agorootparentYour \"diving through object hierarchy\" has a _consistent_ command called \"Get-Member\". Not the same where you need to dive through several hundred text output structures for different commands and/or read man page options. I am sorry, but your statement is factually false. The simplest technique for analyzing the objects that a command returns is to pipe the output of that command to the Get-Member cmdlet. The Get-Member cmdlet shows you the formal name of the object type and a complete listing of its members. reply tdeck 19 hours agorootparentprevThis is a very succinct dissection of the problems but I will add one more: PowerShell's frustrating behavior where any return value of a function call is part of the enclosing function's return value if not explicitly swallowed. I understand what they were going for by analogy with other shells here, but in my opinion in a language like PowerShell (with actual return values) this is unintuitive and makes it really easy to introduce bugs. The whole object pipeline thing they were going for feels like an evolutionary dead end that we shouldn't be saddled with. reply partdavid 18 hours agorootparentI agree, this is a place where the desired features are in tension. I will say that it took me a while to find it, and I don't find it limiting now that I understand it, but it was a real gotcha when I encountered it. reply jaaron 21 hours agoparentprev> why are they using Windows at all? In my case: - I had 10+ years of experience in linux/unix development - Career change to AAA game development which is predominantly Windows based - Day to day do Unreal C++ developing in Windows with Windows toolchains - Still use WSL (and thus bash) for everything else, including DevOps related work, cloud/container development work, and personal notetaking/productivity via emacs+org-mode. So, some of us have reasons to co-exist in multiple OS's. I tend to write up scripts in Python if I can so that there's at least a chance that I can run them in both Windows + Unix environments. reply jamesfinlayson 18 hours agorootparentI feel similarly - I first learned about the shell with Linux and all my career has been with Linux, but at home I use Windows. I am making an effort to be better at PowerShell but if I just want to do something quickly, I already know how to do it with bash and I just want to get on with my day. reply scintill76 21 hours agoparentprev> if the writer has been using UNIX shells for that long, why are they using Windows at all? Maybe the IT department policies force them to. At least that's why I use Windows but do most of my development in an ssh terminal to a Linux system. reply kccqzy 20 hours agorootparentPerhaps the company is otherwise all in on Microsoft's ecosystem. Perhaps they use Outlook, Word, Excel and similar apps where the Linux version doesn't exist and the Mac version is an afterthought. So IT forces everyone to have a Windows system, and then give only the developers a separate Linux machine. reply chiefalchemist 20 hours agorootparentprevAt this point, this should be the industry standard. \"Local\" should be whatever you can SSH into. You can get as much hardware as you need, and you don't have to worry about anything private (i.e., customer information) and/or proprietary being on your lost / stolen laptop. reply hnlmorg 20 hours agoparentprev> un-learning 30 years of experience to use something else even if the latter is objectively superior, would be a pain in the neck for anyone. It’s “subjectively superior”. If there’s one thing I can’t stand in IT, it’s people who think their personal preferences are equivalent to impartial facts. You disagreeing with the author’s reasoning is evidence of just how subjective this topic is. reply delta_p_delta_x 20 hours agorootparentThat was poor wording on my part—I should have used the subjunctive mood and written '… if the latter were objectively superior …'. reply hnlmorg 20 hours agorootparentNot just poor wording but I think you misunderstood the authors context too. Some of his comments were comparing git bash to other POSIX layers like WSL, rather than Bash vs PowerShell. reply delta_p_delta_x 19 hours agorootparent> I think you misunderstood the authors context too This I did not. The post was about Git Bash as a command-line shell for Windows and everything it entailed, I understood that and that's why I wrote the paragraph that followed. I was limiting the scope of my comment to the author's familiarity on Bash versus CMD or PowerShell because I considered that the cornerstone argument of the entire post. Everything else that the author mentions is expected by default in a *sh-type shell on a Unix-like OS; there's nothing special there. People wouldn't blog about being able to use `grep`, `ls`, etc on Linux; it's just everyday business. reply hnlmorg 11 hours agorootparent> > I think you misunderstood the authors context too > This I did not. The post was about Git Bash as a command-line shell for Windows and everything it entailed, I understood that and that's why I wrote the paragraph that followed. I literally just said he wasn’t just comparing Git Bash with Powershell though. So you can’t have understood their context if you took Powershell to be the “cornerstone”. It’s a bit of a muddled blog because he is constantly switching focus between “Git Bash” (the package / distribution; whatever term you want to describe it as) and Bash.exe (the shell). There are a lot of the comparisons are with things like WSL, for example when he discusses the underlying file system. And when he talks about the ease of installing and keeping things up to date. None of that was directed at Powershell specifically, nor even at all. reply amaccuish 2 hours agorootparent> I literally just said he wasn’t just comparing Git Bash with Powershell though That's great that you think that, maybe delta_p_delta_x had a different takeaway. Why do you believe your opinion about what the cornerstone of the article is to be fact? > If there’s one thing I can’t stand in IT, it’s people who think their personal preferences (or opinions) are equivalent to impartial facts. reply delta_p_delta_x 10 hours agorootparentprevI think you're misunderstanding my comment. I said the cornerstone argument for their blog was 'they were familiar with Bash, so they don't use the shells that come with Windows', not 'they are comparing Bash with PowerShell'. Given that everything else is detailing what about Git Bash is familiar, which as far as I am concerned is everyday business. The author discusses things like Unicode and the other GNU coreutils. Again, this is what you'd expect on a Unix-like. reply hnlmorg 9 hours agorootparent> I said the cornerstone argument for their blog was 'they were familiar with Bash That wasn’t the cornerstone either. That was just the only argument of theirs you chose to accept: I quote: > This is the only argument I'm willing to accept, I do understand your point of view, I honestly do, but you’re not being charitable to the author nor genuine when discussing it with me. reply wilsonnb3 20 hours agorootparentprevIn principal I agree with you but if there is one commonly used technology that is actually objectively inferior to the myriad of replacement options and is only still around because its been ubiquitous for decades, its bash. reply hnlmorg 19 hours agorootparent“Inferior” and “superior” are broad terms. Architecturally speaking, Powershell (and other modern shells) are a huge improvement. But that’s only half the story. Syntax wise, I find powershell a step backwards. It’s too verbose. Bash is too far the other way but if I’m using a REPL then I’d rather have something too terse than too verbose. Granted that’s just my preference but we are back to subjective arguments. Ubiquity is another consideration. If you’re writing multiplatform bootstrapping code then Bash is a better option than Powershell. And we are back to subjective arguments again. Broad terms are only true if you can agree on a specific context. But then they’re no longer broad and instead you’re relying on everyone agreeing to that narrow subjective definition. reply throwaway2037 17 hours agorootparentprevWhat is better than Bash shell? What I like most about it: It exists on every single Linux box that I use, so I can always run my Bash-specific shell scripts. reply wwweston 20 hours agoparentprev> if the writer has been using UNIX shells for that long, why are they using Windows at all? Desktop applications would probably be the usual reason. It's all well and good if your application needs are covered by open source, but there's a fair bit of software with value add beyond open source that's never received a Linux version and WINE doesn't support well enough. Of course, as you say, then you have the problem of the shortcomings of various *nix environments for Windows. MacOS can be a great solution... if your app still runs under MacOS (currently I'm trying to figure out how to get Fireworks to run under a minimal windows or mojave VM). reply drums8787 19 hours agoparentprevI make a living writing software that's hosted on Windows. I've never felt as comfortable with the command line options on Windows as I do with macos/Linux. Git bash with vim was such a relief. It's an idiosyncratic mix of ergonomics and habit probably. I don't really care since I get the job done efficiently (more so than a lot of \"Windows natives\" I observe). reply ethagnawl 18 hours agorootparentIn my experience, it really does come down to the ergonomics: the cursor, the casing, the forward slashes, whatever key combo is required to paste (shift+insert?). I'm sure PowerShell and friends are great but it's enough of a shift to not be worth the learning curve if it's an environment you're just dipping into to do the bare minimum required to make something work. reply beart 14 hours agoparentprevThe minor differences can be a major issue. Powershell 7 still doesn't come installed on any windows system as far as I know, so you are better off targeting Powershell 5, and unable to take advantage of the features offered by 7. reply ethagnawl 18 hours agoparentprev> why are they using Windows at all? I'm a back-end developer/DevOps/Linux sysadmin who runs a small consultancy. I've been brought into a few Windows projects -- primarily on-prem/interactive installations. The past few occasions because the client was using Windows because the FE was being built using TouchDesigner. (Not sure it's still Windows-only but it was previously.) So, it happens. Clients have many reasons for running Windows and depending on the project and its needs, I may accept the requirement or not. To use a recent couple of examples, if I can use GitBash, Docker, WSL and Python locally, then I'll probably take it on. If they're set on using PowerShell, C# and Azure, I'm probably out. That all being said, if the project is interesting or high profile enough, I'll jump through whatever hoops are put in front of me. reply irunmyownemail 16 hours agoparentprev\"shoehorn\" Git Bash is an elegant solution that brings some of the awesome Gnu utilities and does it on Windows terms and effortlessly. No shoehorn needed. WSL is handy (those times I'm forced to use Windows - which is still better than when I'm forced to use Mac) but you have to remember the whole time you're in WSL, you're in Linux proper and all things work as Linux, generally, sort of. Git Bash is the power of Gnu right on Windows. reply devnullbrain 13 hours agorootparentWSL also adds yet another restart loop to the already lengthy process of turning a fresh machine into a workable Windows development station. reply CoolCold 9 hours agorootparentMy take here - when working with stable system, changing OS once a several years (last one was when changing laptops basically, so 4+ years in my case), I don't care much on _single_ reboots. For those who tirelessly do distrohopping, things may look different of course. reply throwaway2037 17 hours agoparentprevYears ago, I had a teammate that was always pushing me to try Cygwin. While I love a UNIX shell, Cygwin just felt... weird. The whole paths thing really turned me off. What pushed me to try it? He said to me: Why bother learning two shells (UNIX + DOS), when you can just use one (UNIX) everywhere? I couldn't find a good argument against it. I tried Cygwin, and, eventually (never?) got over the weirdness of paths. reply rcarmo 20 hours agoparentprevI find this interesting because, well, I’ve used bash on Windows for _ages_ (literally before the EMWACs toolkit came out for NT 4.0, and I spent many, many years running Cygwin). I have to use PowerShell, but since it essentially wraps .NET services I very much prefer to either write C# (and have vastly more maintainable and testable code) or, when removing, invoke the APIs using a Unix scripting language. And these days I can write nice “portable” (ok, retargetable) binaries inside WSL and output a Windows build if I need to… reply devnullbrain 13 hours agoparentprev>Also, this does raise a few questions, though—if the writer has been using UNIX shells for that long, why are they using Windows at all? An answer that I haven't seen offered yet: because Microsoft spend billions trying to gain and retain developer marketshare reply sytelus 18 hours agoparentprevPowerShell should be taught in university as example of how poorly designed system devoid of taste and aesthetic looks like. It's an ugly monstrosity that makes me puke every time I try to use it. The only reason some people might like it is because they have been tortured by Windows defaults for many years and finally they got used to this utter mediocracy. reply delta_p_delta_x 17 hours agorootparentIt'd be good if you could actually justify why you think PowerShell is inferior to UNIX shells, instead of going on an impassioned but unsubstantiated rant. reply keithnz 16 hours agorootparentprevI think powershell is actually really well designed. Some of the syntax leaves a bit to be desired, but other than that I think it's really good. What do you think is poorly designed? reply devnullbrain 13 hours agorootparent> Some of the syntax leaves a bit to be desired That's quite poor design for a shell! It's like saying an OS is well designed except the mouse and keyboard input leaves a bit to be desired - because they were designed for scripting first and users second. reply grimgrin 20 hours agoparentprevwhat about https://github.com/microsoft/terminal you use that too right? you better! reply delta_p_delta_x 20 hours agorootparentWithout question. I was so glad when 22H2 allowed users to change the default console host to Terminal; it was the single biggest reason why I decided to stay on Windows 10. reply fsckboy 19 hours agoparentprev>if the writer has been using UNIX shells for that long, why are they using MSWindows at all? MSwindows is posix, why not? windows supports tons of software that is not available on unix. People have to work with other people, spouses, university colleagues, whoever, maybe those people use MSWindows reply jiggawatts 20 hours agoparentprev“When in Rome, do as the Romans do.” I don’t respect the attitude of doing whatever you’ve always done, even when working in a wildly different environment. Imagine how Linux people would react if there were endless blog posts about using VB6 scripts on Linux by running them under WINE because “that’s what feels familiar.” Sure, for a lark that’s hilarious, but not if you share your work with anyone else for any reason, even open source. reply devnullbrain 13 hours agorootparent>Imagine how Linux people would react if there were endless blog posts about using VB6 scripts on Linux by running them under WINE because “that’s what feels familiar.” Alternatively, we could ask why the POSIX-sphere produces enough power users impassioned to, interested in and capable of enabling their pet projects on a closed and hostile platform for blog posts like the OP's but the reverse does not. reply delta_p_delta_x 20 hours agorootparentprev> Imagine how Linux people would react if there were endless blog posts about using VB6 scripts on Linux by running them under WINE because “that’s what feels familiar.” Heh, that would be hilarious. In fact someone should come up with a Linux distribution that uses NTFS, PowerShell, comes with .NET, WINE, and no coreutils and blog about it. reply jiggawatts 20 hours agorootparentHah… I just realised that all of the pieces are now there for a “Windows” distro that uses the Linux kernel instead of the NT kernel. reply makeitdouble 17 hours agorootparentprev> “When in Rome, do as the Romans do.” Parent is obviously choosing the \"Veni Vidi Vici\" side of the coin. If they can pull it off, why not ? It's their machine after all. reply sevensor 21 hours agoprevIts biggest advantage is how easy it is to convince corporate IT that you need Git for Windows, compared to msys2, Cygwin, or WSL. reply mid-kid 20 hours agoparentThis, corporate has been insisting on using ThreatLocker to block almost everything from running - Git For Windows is the only comfortable environment remaining, at least until I can convince them that this is stupid. reply jfdjkfdhjds 19 hours agoparentprevgitshell is a opinionated msys2 install. they just picked one of the variants and removed the package manager. this could serve as a wake up call to msys2 maintainers. every time i install it, i have absolutely no f ideia what the 6 variant icons mean and I couldn't care less since all of them mostly work... (thankfully I don't use windows much to reach a point the difference matters?) it's very overdue they just pick one and keep the others with better names in a compatibility folder or something. also, a better name. reply sevensor 18 hours agorootparentIf you’re not sure, you probably want UCRT64. At any rate I used it to pull a rabbit out of a hat at work by building some software with it that didn’t have an official windows build. reply xeromal 21 hours agoparentprevSo Say We All! reply doctorpangloss 15 hours agoprevBusybox for Windows is much better. - Paths look much better: C:/Users/doctorpangloss == /Users/doctorpangloss == C:\\Users\\doctorpangloss. - The coreutils are just there. - Works flawlessly as an SSH or Dockerfile shell. - Fast and tiny. So small you can check it into git. - The maintainer is very responsive and fixes bugs quickly, even in upstream. git-bash has problems because of paths, terminal weirdness, and it doesn’t use native Windows APIs for a variety of things where it should. /c/ this, /mnt/ that. It’s not really possible to write multi platform scripts with it. With Busybox for Windows you can. reply JackMorgan 18 hours agoprevAs a long time personal and professional Linux user, lately my work has led me to work for companies that are Windows-only. I use git bash as my main shell on Windows. It just works for what I need, and in the rare cases I need powershell I just open that. Most of the windows-based programmers I interact with don't know powershell any better than I do. They use GUI tools for interacting with git and the filesystem. So for me, git bash for git and filesystem interaction is a superpower in these places. I could learn powershell, and I'm pretty sure it's much better than bash for scripting, but I'm almost never really scripting in bash anyway, I use tools like F# for that. Powershell seems fine, but it's less well supported on Linux and I don't really need a shell based language. So in the end, git bash keeps on working and I can focus my learning elsewhere. It's stable, still works the same way 15+ years later, so no need to change. reply oldsecondhand 17 hours agoparentAnother thing to consider is availability: many companies don't even allow powershell for dev users. reply bena 17 hours agoparentprevPowershell lets you install extensions and run commands against the clr. It’s basically a clr repl. If you need to interact with windows/microsoft specific things (i.e. Active Directory), it’s pretty convenient reply niobe 18 hours agoprevWell I took a different route. Always beem 'forced' to use Windows as my workstation by a couple of specialty apps without Linux alternatives that need direct hardware access. But have always used linux on everything else (servers, even my media centre). Because I'm sometimes a late adopter of new tech, until recently I used cygwin as my normal command prompt. There were few downsides to that if you just need access to unix tools, except package management is a bit annoying. Well, discovered the MS Terminal and WSL this year, and happily moved off cygwin. More recently decided to switch over from WS1 to WSL2 but still evaluating the move. Now I have a unix \"more native\" environment AND a linux distro I'm familiar with. I've been accessing the Windows filesystem under /mnt for two decades so this is normal for me, and if filesystem access is slower, it's not a dealbreaker. All my existing scripts were trivial to update. The integration of WSL and VSCode is also pretty cool. It means my vs code environment environment is also working natively in my WSL distro. Overall can understand some lingering negativity about MS efforts in this dept but after years of rejecting all things linux and open source, in general the 180 from the top is much appreciated and it has kept me on Windows a while longer...so mission accomplished microsoft. Just gotta do something about those pesky apps I still need... reply kennethrc 5 hours agoprevI'm \"All Linux, all the time\" at home, develop in it, help out with the Kernel a little bit, only use Windows at home in a VM, etc. I'd just started working for a client that is fully entrenched in Windows, but we're doing bare-metal and Linux-y work. When I'd discovered I couldn't use Linux as my daily driver due to security/policy(/tooling) reasons, one of my co-workers told me about Git Bash, and it's worked out so well that ...... when we were given the opportunity to use Linux but VM Windows, I kept the Windows machine. GitBash has made Windows painless for my work's use-case. I should probably throw a few $$ at them .... reply jmkni 5 hours agoparentCan't you use WSL? reply 4oo4 4 hours agoprevGit Bash is really nice for when I want a unix utility that will do the job simpler and better than Powershell, however it's painfully slow for larger I/O operations. I end up barely using it on my Windows 11 work machine because I'm allowed to have Linux VMs. Even with the VM overhead, and having fewer CPU cores and less RAM than the host, things still end up being way faster there. I remember one time running a grep command on a large-ish (~ 1 GB) log file in Git Bash and waiting at least 5 minutes for it to complete. After getting impatient I did the same thing in a VM and it took about 30-45 seconds, at which point Git Bash still had not finished. reply steve1977 4 hours agoparentDid you use endpoint protection in both systems? In my experience, that is often a factor of performance issue on the Windows side (and an unfair comparison when it is not used on Linux) reply dundarious 14 hours agoprevgit bash's main benefit being that it's a commonly installed variant of the wonderful msys2 system, then yes, I tend to agree. Another wonderful benefit of git bash, is that because it is so commonly installed, if you want scripting for a dev/build tool, you can 99% of the time just use bash for your scripts, on linux and windows. I use this all the time for my build scripts. build/x86_64-linux-gnu-gcc.sh, build/x86_64-windows-msvc-cl.sh, etc. (These are generally simple single command line unity builds, parameterized only by debug, opt, etc.) Then from pwsh I can run these scripts with `& \"$env:GIT_INSTALL_ROOT\\bin\\bash.exe\" .\\build\\x86_64-windows-msvc-cl.sh`, similarly from cmd.exe. (Looks like a ton of typing, but it's not, I always Ctrl-R search and get a hit by just typing \"git_\"). No need to write a bat script, or even a pwsh one, pretty much ever again. Even if all you want is logic to cd to the script directory, parse a single script argument, check it's valid, provide a default, make some directories if not present, etc., bash beats all the alternatives on many fronts. reply Hackbraten 10 hours agoprevFor me, the biggest downsides to Git Bash are: 1. I don’t understand how it relates to msys2 and where the boundary is, so I’m always reluctant to apply msys2 practices (documentation, advice, blog posts, etc.) to Git Bash. 2. I’ve been unable to figure out how to package scripts and other software for Git Bash so other users can install them. reply pauliusj 20 hours agoprevComing from mac to windows, a big surprise was how ridiculously slow printf is on any of the ms terminals (cmd, powershell, windows terminal). Git bash does not have this problem when you are running something pretty verbose yet want to view the output in the terminal reply ruthmarx 14 hours agoprevI've been pretty happy with clink [0]. I'm not looking to have the same environment everywhere, I'm fine to have windows stuff optimized for windows on windows, and vice versa on Linux. By that reasoning, I think clink is a much better option. [0] https://github.com/chrisant996/clink reply shortlived 6 hours agoparentClink + windows terminal + Git tools is the perfect setup IMO. reply WalterBright 16 hours agoprevI constantly switch back and forth between Windows and Linux and it's always \"dir\" mixed up with \"ls\" coming from my fingertips. Grump grump grump Edit: I know I can use aliases and .bat files and whatever to make one look like the other, sort of, but then I learn some patchwork system and will be unable to use any other computer's login. reply SPBS 16 hours agoparentI almost never used `dir` because `ls` is supported by PowerShell. Is there a reason you have to use CMD? reply WalterBright 15 hours agorootparentMainly because it isn't the default. reply mixmastamyk 13 hours agoparentprevI’ve made the dir and cls aliases on Unix for as long as I can remember. And keep dotfiles in git. Yes, computers out of your control won’t have them, but better to have them 90% (for me 99%) of the time rather than 50%—which is just annoying. reply WalterBright 10 hours agorootparentcp and copy, and \\ and /, etc. My sanity is preserved, however, by having the same editor on all the machines I use. Although I can't get copy/paste to work in a remote text putty window. Sigh. reply mixmastamyk 3 hours agorootparentYes, many windows things accept /, as I’m sure you know. Have used the ssh command instead, easy install now but twenty years ago had to hunt for openssl/ssh for windows. Console/Winterm should allow paste, though console puts on wrong mouse button of course. reply WalterBright 45 minutes agorootparentmany windows things accept /, as I’m sure you know I do know. But it isn't complete, whether it works or not depends on which command or program you're using. It's not really predictable. Posix programs ported to Windows have similar problems - they often remain case sensitive with filenames. At least all the programs I write work accept both \\ and /, and are case sensitive on Posix and case insensitive on Windows. reply NelsonMinar 21 hours agoprevgit bash is very cool. It's based on good ol' MSYS2 and MinGW which has always been a nice way to do minimalist Unix-like stuff in Windows. WSL2 is great these days and can interact fine with Windows files (despite what this article says). But it's a lot more stuff, a full Linux install in a VM. reply exac 21 hours agoparentI just had an IDEA nx plugin crash my editor until it was removed because it can't handle WSL directories, so I would not say it can interact fine. reply okl 21 hours agorootparentTip: Install the IDE inside WSL2. Should also run somewhat faster, especially any operation which has to do with many small files, at the expense of a tiny bit of latency in the GUI. reply wnevets 21 hours agoparentprev> WSL2 is great these days and can interact fine with Windows files (despite what this article says) is that true when using Docker with WSL2? reply lmz 20 hours agorootparentWsl2 mounts the Windows filesystem via 9p in the Linux VM, so exposing it to your Docker container is just a config issue. reply gchamonlive 21 hours agoparentprev> a full Linux install in a VM Really? I always thought the subsystem worked translating OS calls back and forth without virtualizing an entire machine. It sure has an entire OS, kernel and all, but not a virtualization layer. Maybe that was the case for WSL but not WSL2? reply JonathonW 21 hours agorootparentWSL2 is \"just\" Linux running in a Hyper-V VM (with some special sauce on top of it to handle things like interacting with the filesystem or doing Wayland and X11 graphics, plus containerization stuff to allow multiple distributions to be installed and run under one VM and one kernel). WSL1 was a completely different approach, adding a Linux compatibility layer to Windows itself. There, you never had a Linux kernel running at all-- Linux syscalls would call into WSL, which would talk directly to the NT kernel to do whatever that syscall needed to do. WSL1 didn't last very long (still present, but not actively being developed)-- turns out that reimplementing one operating system on top of another is a Hard Problem (see also: Wine). WSL2 avoids this entirely, and also avoids most of the impedance mismatches that you get when trying to reimplement POSIX on top of NT. WSL2 solved a whole bunch of compatibility problems essentially overnight that WSL1 never even got around to. reply jborean93 21 hours agorootparentprevWSL1 used a concept called a pico process [1] and the pico driver that is associated with the process is forwarded the syscalls to translate to the required NT APIs. WSL2 is a VM running through Hyper-V but integrated in a way that mostly looks like a normal process. It was introduced to improve filesystem performance (on the Linux mounts) and avoid having to translate and maintain the syscalls required by Linux [2]. The tech behind WSL1 is quite fun but WSL2 certainly has better compatibility and aside from FS performance between Linux and Windows is mostly a positive. [1] https://learn.microsoft.com/en-us/archive/blogs/wsl/pico-pro... [2] https://devblogs.microsoft.com/commandline/announcing-wsl-2/ reply akdev1l 21 hours agorootparentprevThat was WSL1 and as a result had a bunch of unimplemented syscalls. Microsoft realized trying to keep up with the ever-growing list of syscalls on Linux is actually not practical hence they opted for the virtualized approach with an actual Linux kernel to achieve 1:1 syscall compatibility. They released that as WSL2. Technically the name is not accurate anymore as there aren’t any “subsystems” at work anymore. reply dgunay 21 hours agorootparentprevWSL 1 worked that way. WSL2 is a full Linux vm. reply mschuster91 21 hours agorootparentprevYep, WSL1 was a translation layer. Extremely awesome but unfortunately dog slow for anything involving npm packages or large(r) PHP environments as all file accesses would go through the translation layer, the Windows filter chain (aka virus scanners) and then finally NTFS. WSL2 in contrast is a VM, which avoids all the slow paths in file accesses - all that Windows sees is a bunch of bulk block I/O to the backing file of the VM disk. reply rcarmo 20 hours agorootparentOn the other hand, running Python, Go and .NET on it was a dream. Awesome development environment, and much more battery friendly, to the point where I still use it. reply konfekt 11 hours agoprevFor those without admin rights preferring ZSH to Bash in the Windows Terminal: install MSYS2 with scoop.sh, then the ZSH package with `pacman -S zsh` and add a shell with commandline `%USERPROFILE%\\\\scoop\\\\apps\\\\msys2\\\\current\\\\msys2_shell.cmd -defterm -here -no-start -ucrt64 -shell zsh` to launch it in Windows Terminal. reply mid-kid 20 hours agoprevMy favourite shell environment for windows thus far is combining Git For Windows with scoop[1]. A simple \"scoop install git\" will get the environment installed, and give you a bash shell and full access to all sorts of windows-native utilities from scoop. Some would say I'd be better off with msys2 or cygwin, but the former is meant more as a development environment and lacks misc utilities, and the latter has what is possibly the worst package manager that is still in use (and generally less stellar integration with windows programs). [1]: https://scoop.sh/ reply indigodaddy 17 hours agoparentOh snap you can `scoop install busybox` and get a bunch of nix builtins! https://paste.almalinux.org/2A reply wilsonnb3 19 hours agoparentprevAny reason to choose scoop over chocolatey, which has been around forever, or winget which is included in windows these days? reply beart 18 hours agorootparentScoop favors portable installs and typically manages upgrades and migrations for you. It does support non portable tools but these live in a different bucket. Chocolatey, and to a greater extent, winget typically defer to the individual applications. In my experience they are more like glorified download indexes than actual repositories for managing software. reply microflash 4 hours agorootparentprevScoop rarely pollutes Windows registry which is what often triggers the corporate blocklists. The portable install is a feature here, compared to chocolatey or winget. reply dundarious 14 hours agorootparentprevInstalling something with chocolatey means downloading some powershell script, with somewhat arbitrary contents, and running that to do all sorts of install, restart, migration, etc., logic, of varying quality, and with varying philosophies of what should be done \"automatically\". Installing something with scoop means downloading a declarative json manifest, with occasional, short pre-install/post-install levels of scripting (in all cases I've seen, it amounts to testing if a file is present, and copying it if so, etc.). The complexity that is tolerated for the scripting is much much lower. I've also found chocolatey to be less well curated, with some odd packages with simple names that come from random people who don't seem to do a great job at maintaining the package. scoop is preferable on every metric, IMO. reply amenhotep 17 hours agorootparentprevWhen I tried using chocolatey, you had to pay them in order to specify a default install directory rather than needing to include it explicitly in every command. I don't know if this is still the case but I formed a deeply held grudge over it. When I tried using winget, it mostly worked quite well, but occasionally would fail horribly. It'll happily detect and offer to upgrade programs that were installed outside winget, for example - great feature!! - except one time when I tried doing this it installed a new instance to a default location instead of upgrading in place, so now I had two installations. I don't trust it. Scoop is simple and sensible and just works in exactly the way I expect it to. With the exception of needing to install scoop-search. reply speilberg0 19 hours agorootparentprevscoop generally installs to your local data, so most programs don't require admin access. reply keithnz 16 hours agorootparentprevwinget seems to have replaced chocolatey for the most. Chocolatey is a bit weird with their play to make money, I guess they want to milk it for as much as they can till it dies. reply daghamm 9 hours agoprevWSL inside the new Microsoft Terminal is my preferred Windows Shell. reply AdeptusAquinas 21 hours agoprevHuh, didn't know it was basically bash commands for windows - I thought it was a faked up linux environment. I mostly just use ubuntu under wsl2, though the file performance across the windows filesystem isn't the best. reply yamapikarya 17 hours agoprevi love windows terminal + git bash combo reply bricss 6 hours agoprevGit Bash + Windows Terminal = one luv reply grepfru_it 5 hours agoparentGit bash breaks password entry on certain tools that hides character input. reply esalman 14 hours agoprevGit SDK for Windows is even more awesome, complete with a package manager for fetching more common unix tools with ease. reply rr808 17 hours agoprevThe reason I used to use Git Bash was to ssh to Unix boxes. Since Windows Terminal came out I dont use any more, Windows Terminal is much better. reply Ayesh 17 hours agoparentThe post is about the shell; Windows terminal is only... Well ... A terminal. reply jborean93 16 hours agorootparentAnd ssh is a binary so isn’t fully shell related :) reply fud101 3 hours agoparentprevits good but it has that bug with tmux that can be very frustrating. reply aurelien 11 hours agoprevYou should try ArchLinux or Gentoo if you really want a shell as you desire ;) reply dietr1ch 20 hours agoprevSame, but NixOS is my preferred Windows too. reply lynguist 20 hours agoprevIn this article I learned that _start_ is the equivalent of _open_ (from Mac). I didn’t know _start_ before. reply nikau 18 hours agoprevGitbash is handy for simple things, but it's a slug compared to wsl2. reply bena 21 hours agoprevIt's weird, I mostly associate git bash with VS Code because it's the terminal I use most often with it. It doesn't occur to me to use it outside of that context. I use the command shell or Powershell depending on what I need to do. reply amatecha 20 hours agoprevOh yeah, I've been using Git Bash for years, it's great. It's nice being able to use a set of basic cli utils I'm used to (gnu utils) which are bundled in (not sure if it's 100% exhaustive in that regard, but still enough for me to do everything familiar). And yes, agree with and have benefited from every single bullet point TFA mentions! reply indigodaddy 20 hours agoparentmobaxterm has the basic cli tools as well perhaps even more so than git bash. For example it additionally has rsync and I think mc too reply cafard 19 hours agoprevI was just using Git Bash to run grep... reply Tempest1981 15 hours agoparentAnd ripgrep reply veltas 21 hours agoprevmintty is a great TTY emulator for Windows reply johnea 19 hours agoprevWhy not just ditch the \"windows\" part entirely, and work on an OS that's not designed for the purpose of exploiting you as a user? reply gruturo 10 hours agoparentFor a lot of people, this is your employer's choice and not yours unfortunately. I jokingly remark that most of my contacts with microsoft products are non consensual. reply philipwhiuk 19 hours agoparentprevSpeaking personally, it's because the company only supports Windows for everything else. reply righthand 20 hours agoprev [–] Honestly there are so many differences and little things on Windows that make using any of the CLIs not worth it. You will run up against gotchas developing on native Windows cmd, Powershell, git bash, and WSL. I left for Linux a long time ago and never have had to look back at the utter mess that is Windows. People stay on that platform for what? A graphical menu in the corner? Compatibility you don’t actually need? Use a real shell and OS and save your sanity. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Git Bash has been highlighted as a preferred Windows shell, offering Unix-like command-line functionality within the Windows environment.",
      "Key advantages include familiarity with bash commands, easy installation, small footprint, and integration with Windows File Explorer.",
      "Git Bash supports many Unix-like commands and scripts, making it a versatile tool for developers working in a Windows environment."
    ],
    "commentSummary": [
      "Git Bash is favored by many Windows users for its familiarity with Unix commands, making it a comfortable choice for those with Linux experience.",
      "While PowerShell is praised for its structured data handling and .NET API, it is often criticized for verbosity and encoding issues.",
      "Alternatives like WSL, MSYS2, and Busybox for Windows exist, but Git Bash remains popular due to its simplicity and ease of use."
    ],
    "points": 193,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1725998078
  },
  {
    "id": 41505266,
    "title": "Some of us like \"interdiff\" code review",
    "originLink": "https://gist.github.com/thoughtpolice/9c45287550a56b2047c6311fbadebed2",
    "originBody": "Why some of us like \"interdiff\" code review I am currently in the process of evaluating Gerrit Code Review for work. Gerrit is an open source code review tool, as the name might imply, and works with Git repositories. What that means is that given some repository, you can: Write patches to the codebase, and submit them for review Other people look at the code you wrote They leave comments telling you to fix various problems You might even fix some of them Code review is a pretty good idea, in general If it's open source, someone might merge your code, and you let out a sigh of relief, knowing that you are off the hook and have increased their responsibilities and technical debt for all time If it's open source, you then disappear into an eternal void, never to be seen or heard from again by the maintainer There are many tools you can use to increase the burden on all future developers like this, such as: Gerrit GitHub (the website you are on) Phabricator (RIP) Uploading .patch files into a bug tracker and waiting until the maintainer sees it, downloads it, and reads it Emailing someone via git send-email telling them to pull from a git:// URL that is hosted on a 7 year old \"server\" somewhere in your house and then reading the email they send back Implainting the idea in your coworkers head with a long discussion so that they end up implementing it while you do something else I have done all of these, both as the person writing and reviewing the patches, and they are all workable to various degrees. Some open source projects also use Gerrit, such as the Go programming language. Some, like KDE and LLVM, used Phabricator. We have entertained the idea of using Gerrit for Jujutsu. Linux, the kernel itself, uses the email workflow in a particular and special way. Most projects use GitHub, because it is easy and has zero activation energy. But why are these tools important? Doesn't everyone just use GitHub? Isn't that good enough? Are all these really the same? No, they are not. The ideal patch series The following image indicates a series of 3 patches, to be submitted to a software project. They have a typical child/parent relationship, like any linear sequence of commits in Git would have. We call this a series because it isn't just a patch. It's a \"series\" of \"three\" \"patches.\" Get it? This series represents, believe it or not, something close to the ideal patch series — for the author, the reviewer, and the future schmuck who will inherit your codebase. You can ignore the exact numbers for a second and whether 500LOC is too much (we'll get back to it.) You're going to do a \"thing\", but notice you can clean up some code, coincidentally. You write a bunch of code — in this case, you add a new API to the codebase. Maybe it's a fast data structure, intended to replace a slower one. You migrate the users of the old API to the new API. Your program is now faster. The most important point is that changes are: Logically separated. And, I can read the code as if each patch was individually applied in the series. So I start with the first patch, then the second, then the third. The series represents an evolution of the code, step by step. The specifics here aren't too important. Sometimes, there will be no refactoring. Sometimes, it will be 100 lines. Sometimes, it will be 500 lines because it's a core API change and you have to break every call site and fix them. And so on and so forth. But even if this is the ideal series in a logical sense, it still needs to go through code review. Which brings us to... The GitHub school of code review: \"diff soup\" GitHub encourages you to do code review by adding new commits on top of the original commits to address reviews. GitHub encourages this both explicitly but also implicitly, for a few reasons and due to the way the UX is designed. Two of your coworkers, Alice and Bob, leave comments on the GitHub review. They tell you to make various fixes and adjustments, which you dutifully perform. Also, you didn't add tests to the new API yet; it worked before, but everyone wants tests of course. So you need to add those on top as well. Also, you made a small tweak after all that, a change to the implementation API, which is very small. Maybe you noticed a simple tweak in your fast, cutting edge data structure. Surely it can't a chain reaction that leads massive regression in production that will cause a pager to light up at 3am, right? It's only a +/- 2 lines delta! That's silly. This is now what your commit series has become, and this is what you will push to your branches on GitHub for review. The original 3 commits, then several \"address review\" commits (orange), and finally your regression (red). The black lines represent edges in the commit graph, i.e. the things that show up in git log. While the dotted colored lines represent the \"implicit dependencies\" caused by review. Bob for example told you to change one extra call site in the third commit (blue dotted line) and one thing in the refactored code (purple dotted line). Alice however told you to fix something in the API you added, and fix something in the refactoring commit (purple and green dotted lines.) Your freshly written tests are really just related to your lack of tests in the new API (green dotted line.) Finally, the \"minor\" change is a tweak to the new API code (red dotted line). This is what your code has become, and this sucks. It sucks because: The implicit relationship between the \"fix review\" commits and the original is not really visible. Only the actual parent/child relationship between Git commits remains, but that isn't the whole story, because now you don't know why these commits are related. (This is essentially a loss of \"provenance.\") Notice how all the original commits were a single color, because they had one \"purpose.\" But the orange commits have multiple arrows, with different colors, establishing implicit relationships. The fact that a single orange commit touches multiple original commits means that the conceptual model is now more complex; some commits represent single changes, while others might address multiple changes at once. You can fix this by having each Orange commit only address exactly one complaint-per-change, i.e. only one dotted line leaving it. However, now you have 6 fixup commits instead of 4. git blame now sucks completely because changes overlap. blame works on the level of a line, so if \"fix alice review\" changes a single bit in a line that came from \"minor refactoring\", that line is now misattributed. You have to go N commits deeper to find the real change. git bisect now also sucks. When you do bisection on these commits, the \"minor\" commit may not be the actual root cause commit; it may only be a trigger for a bug that existed from the moment the new API was added. It's unclear whether \"minor\" or \"new API\" are at fault, but realistically you probably have to just revert both after figuring it out because the word \"minor\" does not tell you anything about why the change was made. Backing out both changes may not always be desirable or even possible if, for example, the new API is a massive performance uplift and backing it out will cause cascading downstream effects. And also this regression may only pop up weeks later. If I am your coworker, and I do git bisect on a 3am production issue to find the root cause of a regression, and I land on a commit that says \"minor\" from the above PR that landed 3 weeks ago, and it has your name on it? You are going to get a phone call from me within seconds where I will give you a monologue about how scary I can be. Even if the \"minor\" commit was never added, maybe the regression would still happen, and the above might all still be true, but at least you didn't have to spend an extra 15 minutes doing archaeology bullshit while you are in a P1 conference call with 5 other people. Just to be clear, this example is what it looks like when only two reviews take place. If your review is cycling 5 times, well, the above becomes far, far worse. And yes, in open source contexts, you will often go back and forth multiple times with someone, because they may not have context you do, and there may be no point in doing further review until other nits are addressed. Many projects like this go through multiple review cycles. Part of the problem, the reason why this happens, is due to some design and UX flaws: The new commits are the only way to do incremental reviews on GitHub. In the above model, if you don't want to review the 500 lines of code in the new API over and over again, the only way to do it is by adding new commits on top, and viewing those. It's much easier to read the \"fix alice review\" patch, which is +/- 10 lines, than it is to read the new API — which is 500 LOC. Github always shows you the whole diff by default: When you have a PR on GitHub and view the \"diff\" tab, it shows you all the commits crammed together as a single diff; it is the moral equivalent of running git diff master..foo-branch and looking at that. But that isn't how people write changes, and it isn't how people read them either. This behavior further encourages you to add new commits on top of old ones, and then the reviewers just read those commits individually because the alternative is to re-read things you already reviewed, constantly. It's difficult to view the differences between anything except branches. In general, all of GitHub's whole UX works on the named branch model — and to a large extent so does Git itself. Because of that, it's difficult to use workflows like git rebase, because force-pushing a branch removes the old branch entirely. And at that point, you can't do things like git range-diff between branches. Actually, this isn't fully true. If you force push on GitHub, you can see a diff, if you click on this weird little un-identifiable \"Compare\" button that pops up on the \"Force Push\" line that appears in a PR: Except it this button only shows you the whole diff from the previous branch head to the current branch head. Again, this means you end up re-reading You're on your own figuring out the Commit IDs and punching them into the URL bar if you want something more granular. Why is this button so hard to see? Nobody knows. I call this review model \"diff soup\", because that's what it is: GitHub just shows you a big bundle of changes mixed together in a big bowl of gruel, and then you are expected to live with it and shovel it down your throat. Beggars can't be choosers. A better way: \"interdiff\" review (AKA git range-diff) The idea is actually simple: instead of publishing new commits on top of the three original commits, just publish a new version of the three commits, addressing the changes. So you start off with \"version 1\" or \"v1\" of your patch, like above. In Git terminology, the orange v1 on the side is, roughly speaking, the name of a branch, though normally it would be something like aseipp/new-foobar-api or something. Next, Bob leaves comments, asking you to fix the refactoring and update a new call site (before Alice is done.) So you address his review by updating the first commit and third commit. This creates a new series, which would be \"version 2\", which we send out wholesale: There is now a v2 branch, containing 3 commits, and the patches are updated as Bob wanted them. The orange dotted arrows take place of the previous colored dotted arrows; they effectively represent the fact the given commit has \"evolved\" and changed. Note that there are only dotted orange lines, and no other colors; the \"implicit relationship\" established by the earlier graph is no longer a concern, because there aren't any extra commits. The relationship between v1 and v2 is obvious. Commit A version 1 becomes Commit A version 2, and so on and so forth. The entire issue somewhat vanishes. We repeat this process to address Alice's concerns, to add tests, and to introduce the \"minor\" regression. The result looks like this, where we end up with a final version of the three patches, version 5: The entire evolutionary process of the series has been inverted. At the end of this process, there are still only three commits, that address all review comments. Note that: git blame will now assign lines to changes with less noise. You no longer have to worry about running git show abcdefg on a blamed line and seeing it came from \"fix alice review\", it will come from a change like the first commit, \"refactor and deduplicate controller code\" or whatnot. git bisect is now far more likely to tell you the new API code introduced this regression, without having to dig further. Two of your best tools now work much more reliably because there is simply a better signal to noise ratio. But there's a very important and subtle difference that only the code reviewer, not author, can appreciate, which is... You now can review code incrementally, and not re-read the whole diff When Alice leaves her review on v2, telling you to change the green commit with 500 lines of code, you will eventually respond by doing so. But for Alice, re-reading the 500 lines of code again in v3 is a waste of time. She wants to see an incremental diff that can prove that you actually listened to her. This means she might only need to read a 50 line diff — a 10x difference in code. Alice can use a tool called git range-diff to do this, like so: git range-diff \\ main..v1 \\ main..v2 In English, this means \"Take the 3 commits from branch v1, and show the pairwise diff between the 3 commits from branch v2.\" So if v1 has the following commit IDs: A -> B -> C And v2 has the commit IDs: X -> Y -> Z You will see the changes between commit 1 diff(A, X), followed by commit 2 diff(B, Y), and commit 3 diff(C, Z). In contrast, Github always shows you diff(main, C) or diff(main, Z), i.e. it shows you the entire branch as one diff, AKA diff soup. This is the essence of \"interdiff code review.\" You Don't publish new changes on top, you publish new versions You don't diff between the base branch and the tip of the developer's branch, you diff between versions of commits Now, reviewers get an incremental review process, while authors don't have to clutter the history with 30 \"address review\" noise commits. Your basic diagnostic tools work better, with a better signal-to-noise ratio. Interlude: Strategies for merging patches TODO: Explain how the above is independent of your merge strategy (e.g. git rebase on tip versus multi-parent git merge commits). Interlude: Can you please just tell me if git rebase is evil or not so that we can derail the entire discussion over it? It's fine. Just don't use it on public branches that you expect others to base their own commits off of. It's that simple. Interdiff review systems typically encourage smaller, more \"juicy\" patches that land in the main branch more quickly than the alternative. You don't have to wait on all 5 commits to be ready to go; maybe the first 3 are OK, and the last 2 need more work. You merge 3 out of 5. The intent of the system is that others will just base their work off the main branch, mitigating the need to have cases where you merge a remote that merged 5 remotes that merged... and so on and so forth. So you don't have long lived patches, typically, and most users don't base branches off other branches. Note Some projects tend to explicitly publish branches off of other branches, or merge public branches across repos. The most famous example is the Linux kernel. Therefore, most Linux developers will use git rebase to create a patch series and refine it, but they won't force-push any public branches after they create them, because they might get merged into someone elses tree (without them knowing.) They will just create new branches with new commits. Other notes Conclusion That's the sales pitch. I'll write more here later.",
    "commentLink": "https://news.ycombinator.com/item?id=41505266",
    "commentBody": "Some of us like \"interdiff\" code review (gist.github.com)191 points by todsacerdoti 17 hours agohidepastfavorite132 comments Vinnl 9 hours agoI'm using mostly this workflow with GitHub, with the main disadvantages being that it's more work on my side, and not obvious to my collaborators. But it does carry the same advantages of allowing reviewers to view diffs with just their feedback incorporated, without breaking `git blame` and `git bisect`. When I incorporate a reviewer's feedback, I'll commit that with `git commit --fixup `. I'll then push that up and leave a comment reply to the review feedback sharing the fixup commit hash. Then when the PR is approved and I'm about to merge, I'll do git rebase --interactive origin/main --autosquash This will then combine the fixup commits with the correct original commits. I then do a final `git push --force-with-lease` and merge it. (Make sure to note force push before the review is done, because then reviewers lose the ability to see what you added since their last review.) This relies heavily on autocomplete in my terminal, so that I only have to type `git re` to get to that long command above, for example. And it's a bit clunky, so using a tool that supports and encourages this workflow would be nice. But given that I'm stuck with GitHub, it's OK. reply mizzao 5 hours agoparentI was going to say... interactive rebase addresses a lot of the \"diff soup\" comments that the writer complains about. It's really only done by disciplined engineering teams though (who bother to learn some more advanced features of git) reply jrochkind1 3 hours agorootparentI realized reading the first part of this article that I often want to set up a sequence of PR's, for very similar reasons as in the begininng of OP. Say, a prefatory refactor, then the main work, then some data cleanup. When I do this, the problem with rebase is that it kind of breaks the additional \"next in sequence\" PRs \"on top\", or at least requires (confusing to me) cleanup in all of them when I rebase the base. reply aseipp 5 hours agorootparentprevInteractive rebase is fine, I used it probably 50x a day for many years. The problem isn't really with Git here, it's more a bunch of interrelated problems with GitHub's UX, and the fact that many people culturally have never approached the problem in any other way. A lot of places basically just outright copy GitHub's UX, which I think is a good example of this. For example, using 'git absorb' or autosquash doesn't solve the problem of the review UX just doing badly when you say \"What is the difference between the last version of commit Y, and the new version.\" If those changes include a rebase, it shows you the entire diff including the rebase. That's often not good, but sometimes extremely necessary. (See my other posts in the thread about how Gerrit does this better.) And if you have to do something like rebase, then solve a conflict as a result of the rebase, it is basically unavoidable that you have to do all of them at once if you want the CI to stay clean (which I find important, at least.) Also, a lot of teams just culturally hate shit like autosquash, or any squash/rebasing at all. Do I think they're misinformed? Yes. Do I think they could use tools that make those flows 1000x better and faster? Yes. But at the end of the day default user interface and design philosophy of one of the most popular software forges in the world does actually matter and have downstream cultural impact. So you have to interrogate that and break it down for people. GitHub does not really encourage, explain, or smooth out workflows like this. People learn it, then hate anything else. (I mean, fucking hell, Git rebase didn't even have --update-refs, a huge QOL improvement for stacked diffs, until like 2022!) So, you often don't have a choice. I've had many places where engineers despised GH force pushes, but only because... GitHub's UX is bad at tracking addressed/reviewed comments on a PR after a force push! They get hung by their own noose, so to speak, and don't even know it. I do have other problems with Git, but this post is mostly about GitHub/Gitlab style PR review flows. The same problems still happen even with autosquash or tools that accomplish the same thing. reply riquito 2 hours agoparentprevYou can add to your ~/.gitconfig [rebase] autosquash = true from then on `git rebase -i origin/main` automatically reorder fixup and squash commits. It's a small thing but greatly improved my workflow reply dietr1ch 2 hours agorootparentSquashing is nice because it keeps the amount of commits minimal, but as mentioned in the article, it has the problem that now reviewers have to figure out what changed since their last review, which can get hard to do if the tree of changes originally proposed needed updates in dependencies of the commits they reviewed. I really like the idea that I'm working on a tree (often a stack/list) of changes, and as the review progress I get this cross product of iterations over time like [a-better-way-interdiff-review-aka-git-range-diff](https://gist.github.com/thoughtpolice/9c45287550a56b2047c631...) showcases. In the end, the time dimension is useless after things get submitted, so the repository only gets the latest state of the change tree committed, which is simple and like the auto-squashed version you mention, but during review reviewers get to see the change tree evolve in an easy way. reply pinkorchid 1 hour agorootparent> the problem that now reviewers have to figure out what changed That's something that the review tool can solve, and I agree that github doesn't handle it well. But other code review tools can show diffs between revisions independently of how the commits have been rebased or squashed over time. reply faangguyindia 4 hours agoparentprevBiggest problem working with Git is remembering all these flags and commands. If you do it everyday for years, sure you remember most of them. But for weekend hackers or those who specialise in some other areas, it creates a lot of frustration. I often forget all these flags etc.... My brother made this https://github.com/zerocorebeta/Option-K This enables me to simply write in thermal, \"interactive rebase main, auto squash. And then I hit option+k and it replaces it with the above command. reply ipaddr 50 minutes agorootparentIf you are a weekend warrior you are not working with a team. Just commit and push. reply aseipp 5 hours agoparentprevYes, fixup commits are a good way to approach this, though I don't personally like them, though. I think Sapling's \"absorb\" command which works on an underlying SCCS weave to automatically absorb changes into the relevant diff is much more elegant. It prompts you with an interactive UI. (For me, it sorta falls into the same space as rebasing a series that has multiple branches on it. You need --update-refs for that. Because otherwise it's like, why am I, the human, doing the work of tracking the graph relationships and manually punching in the commits, and moving the branches, and doing all this bullshit? Computers are good at graphs! Let them handle it!) There is also a `git absorb`, but it isn't as robust as Sapling's implementation[1]. Really the problem isn't interactive rebase or not. It's mostly a problem of the UX of the review tool itself more than anything, and the kind of \"cycle\" it promotes. I mentioned it elsewhere here, but fixup commits for example still won't solve the problem of GitHub showing you diffs between baselines, for example, which can absolutely ruin a review if the baseline is large (e.g. you rebased on top of 10 new commits.) I do have problems with Git's UX beyond this, but the original post is mostly a gripe about GitHub. [1] There is an example in this GitHub issue that captures the difference between the two underlying algorithms: https://github.com/martinvonz/jj/issues/170 reply Guvante 1 hour agoparentprevGit aliases are magical BTW reply Piraty 7 hours agoparentprevhttps://news.ycombinator.com/item?id=37086022 reply tome 1 hour agorootparentWow, my jaw is on the floor. This is such a good idea! I already had the idea of tracking issues in the same repo as the code -- not that I actually use that idea -- but I didn't have the idea of doing PRs in the repo itself. Love it! reply mstachowiak 12 hours agoprevIt's always exciting to see new approaches to code reviews - GitHub has its strengths, but it’s far from perfect. For the scenario you’ve outlined, have you thought about splitting the 3 patches into separate, dependent pull requests? While GitHub doesn’t natively support this, the right code review tool (shameless plug - I’m part of a team building one called GitContext) should allow you to keep pull requests small while maintaining dependencies between them. For example, patch 3 can depend on patch 2, which in turn depends on patch 1. The dependency tracking between them - provided by the code review tool - can ensure everything is released in unison if that's required. Each patch can then be reviewed on its own, making feedback more targeted and easier to respond to. You can even squash commits within a pull request, ensuring a clean commit history with messages that accurately reflect the individual changes. Better still, with the right tool, you can use AI to summarize your pull request and review, streamlining the creation of accurate commit messages without all the manual effort. A good code review tool also won’t get bogged down by git operations like rebases, merges, or force pushes. Reviewers should always see only the changes since their last review, no matter how many crazy git operations happen behind the scenes. That way, you avoid having to re-review large diffs and can focus on what’s new. The review history stays clean, separate from the commit history. I'd be curious if this approach to splitting up pull requests and tracking their inter-dependencies would address your needs? reply dogleash 5 hours agoparent> It's always exciting to see new approaches to code reviews - GitHub has its strengths, but it’s far from perfect. This is nice sentiment, it's positive reception to an idea and polite to the incumbent. But it's so thoroughly not a new idea. It's literally the workflow git was designed to support, and is core to many long-standing criticisms about GitHub's approach for as long as GitHub has had pull requests. And I'm over here wondering why this idea took *checks calendar* over 15 years to graduate from the denigrated mailing list degens and into hip trendy development circles. I thought we were knowingly choosing shit workflows because we had to support the long-standing refusal by so many software devs to properly learn one of their most-used tools. That's why I chose the tools I chose, and built the workflows I built, when I migrated a company to git. Nobody gets fired for buying IBM after all. reply aseipp 4 hours agorootparentI mean, the answer is simple. Even if email-based flows use range-diff, which is the correct conceptual model, all the actual details of using email are, I would estimate, about 1,000x shittier in 2024 than using GitHub in 2008 when I signed up for the beta as user #3000-something. Email flows fucking suck ass. Yes I have used them. No, I won't budge on this, and no, I'm not going to go proselytize on LKML or Sourcehut or whatever about it, in Rome I'll just do as the Romans even if I think it sucks. But I've used every strategy you can think of to submit patches, and I can't really blame anyone for not wading through 500 gallons of horrendous bullshit known as the mailing list experience in order to glean the important things from it (like range-diff), even if I'm willing to do it because I have high pain tolerance or am a hired gun for someone's project. Also, to be fair, Gerrit was released in 2009, and as the creator of ReviewBoard (in this thread!) also noted it supports interdiffs, and supported them for multiple version control backends, was released in 2006! This was not a totally foreign concept, it's just that for reasons, GitHub won, and the defaults chosen by the most popular software forge in history tend to have downstream cultural consequences, both good and bad, as you note. reply dogleash 4 hours agorootparentDude, I'm not making a defense of mailing list workflows here. I'm just pondering the nature of the world where despite all the yapping about git I've seen floating around on the internet for as long as I've been lurking social media, the yappers are just recently keying in on something. reply aseipp 4 hours agorootparentIf you're asking \"Why did this take 15 years for people to understand\" and my reply is \"Because it was under 1000 layers of other bullshit\", then that's the answer to your pontification. It has nothing to do with whether you think email is good or not. You pondered, I answered. That simple. reply juped 2 hours agorootparentprevNope, none of it was knowingly done, and plenty of teams are almost trivially convertible to the normal workflow, even without inventing a buzzword like TFA did! Though plenty aren't. I get it. (But one of the magic phrases that really works well is \"this is what git, itself, does, and there's a man page installed on your system at this very moment explaining it\") reply torarnv 9 hours agoparentprevAs far as I know, splitting the series into individual PRs only works if you have commit rights to the repository, so you can base one PR on a different branch (in the main repository) than main. As an outside contributor, with a fork of the repository, your three PRs will incrementally contain change A, A+B, and A+B+C, making the review of the last two PRs harder, because you need to review diffs for code you're already reviewed in another PR. reply codethief 11 hours agoparentprevAs mentioned elsewhere in this thread, this is also the approach that Sapling follows. As for GitContext, how do you keep track of commits across fixups, rebases, reordering, etc.? reply fHr 12 hours agoparentprevWhy not just do good old mergetrains with pullrequest A points to branch B amd then B points to master, merge B into master and thereafter point A back to master or am I missing the point? reply steveklabnik 6 hours agorootparentThis is called \"stacked diffs\" and it's a good workflow; the issue is that it's annoying to use on GitHub without tooling. The \"point A back to master\" bit isn't easy/obvious with pull requests. reply ajkjk 15 hours agoprev100% agree that this is ideal, the way Github does it is completely godawful and it's a tragedy that so many people have it normalized for them. We did this with Phabricator, although it was a somewhat-manual process, helped along by having some command line macros for updating all the reviews at once. But better still would be an explicit UI for it. reply aseipp 15 hours agoparentI am the author and used the phrase \"Code review is a pretty good idea, in general\" in the opening very specifically, because it used be one of the selling points listed on the Phabricator homepage. :) I miss it. reply Groxx 5 hours agorootparentEvery day I have to use Jira and GitHub instead of Phab is a rather painful day. Sadly those days are increasing. Maybe one day I'll be able to use Gerrit (it sounds great), and then I can be only annoyed at Jira. reply taspeotis 14 hours agorootparentprev> Grab ahold of tasks, literally. Place them in confusing, new orders. Make a column just for interns! Ignore the backlog forever. I swear the Herald selling points also had keeping tabs on the pesky interns, or something to that effect. reply aseipp 14 hours agorootparentOne of my favorites was something like: > Phabricator is well received, and has raging reviews from users inside Facebook, such as \"Mandatory\" and \"OK\" Also, Evan Priestley still uses the https://secure.phabricator.com instance to occasionally post life updates, so I check in on it. Last I checked he was learning about PCB design and printing. I think he wanted to learn how to actually physically manufacture his own boards (e.g. etching your own conductive copper layers) and then he posted some update like \"I'm not sure I'll go down this route, because it turns out you can send your design to PCBWay, and it will come back within 5 days, because there is a magical PCB Faucet somewhere in Shenzen apparently.\" What a guy. reply deathanatos 14 hours agoprevYes! This is what I imagine in my head as a real code review style, not the stuff Github does. Glad to have a name for it. I'd add I'd also like my review system to be able to kick patches \"out\" of the review once they're ready. E.g., the small bugfixes that you make while working on that bigger feature should hopefully be small, isolated patches, ones that are going to find consensus with a reviewer quite quickly. Once that happens … I want to just eject that patch from the entire series & cherry-pick it to main, and rebase the review on the new HEAD. (Or, differently, rebase the latest version of the patch series on main, reordered such that the agreed patch is fist, and then FF main onto that agreed upon patch.) Essentially, narrow the scope of the review to \"the part we're still talking about\", but let bug fixes see merging as soon as they're ready. The argument I'd have against this is \"just make that a separate review/PR\". But then you get into the hairiness of patchset A depends on patchset B, until B is merged, then it just depends on main. reply aseipp 14 hours agoparentI keep saying this over and over but, Gerrit basically does that. :) You can see the relationships between any two patches on Gerrit, and more importantly, Gerrit shows you each patch individually. So you can see in a series A -> B -> C that yeah, B is small, let's go ahead and get that in. Part of this is that UX has some really smart ideas like the \"Attention Set\". The attention set is basically \"Which people need to take the next step?\" Like a turn-based game. So, if you just did a review, you're not in the attention set for that patch anymore -- the author is. That means Gerrit puts it down at the bottom of your queue in the UX. And what's at the top of the queue? Things where you are in the attention set! So it naturally groups things this way. I didn't get into all the other really annoying papercuts with GitHub's UX, but even the pull request listing is worse than the alternatives. How do you know what state anything is in? You don't, you have to go read the whole thing. reply SlySherZ 23 minutes agorootparentDo you know how to create a local branch that tracks a gerrit commit? Usually I just do git commit --amend to update gerrit, but then I lose access to my patch's history (it's still on gerrit, but I want it locally). reply deathanatos 13 hours agorootparentprevI guess I missed it in your article, and I've never had the opportunity otherwise to use Gerrit. (Since Github is essentially so pervasive. I've only used that, Gitlab, and an internal review system that didn't do interdiff.) reply zeotroph 14 hours agorootparentprevAttention Set I do not know if GitLab does anything different; I've never used it in anger. I'd bet $10 the answer is \"no, it's basically just the same as GitHub\", though. You would bet incorrectly then. GitLab does essentially what you're describing, the only difference being that it compares different iterations of the force-push \"naively\", so if your force-push includes for example a rebase onto master because another MR has been merged ahead of yours, the diff will include the changes that have been rebased onto. If you decide to register an account on GitLab, simulate the MR and prove to yourself that ~90% of your interdiff post has been implemented by GitLab for about a decade, kindly donate the $10 to your nearest homeless shelter. reply zeotroph 14 hours agorootparentprevRight, for Graphite $20/dev/month is nothing (I wonder if Enterprise is less or more more than that...), considering an ounce of review (prevention) is worth a pound of bugfixes (cure). And when you can not get corporate to switch away from GH, then that is it. In hindsight an obvious way to (almost) print money, congratulations, but also a sad state of affairs. But I imagine the $20k/yr is something you can easily spend on a 1/5 of a dev doing Gerrit maintenance. reply amstan 4 hours agorootparentprevDoes Graphite have a gerrit instance or something? I'm prepared to say \"shut up and take my money\" compared to the other 20k/year offer. reply fHr 12 hours agorootparentprevNever going to understand those finance teams who think like 20k/yr for any enterprise deal is a good deal to onboard more customers and increase reach. reply sureglymop 7 hours agorootparentBecause they don't need that much reach if even a single customer nets them 20k/yr. reply rosmax_1337 15 hours agoparentprevWhat is the meaning of life? reply aseipp 14 hours agorootparentI know this is in jest, but I'll just take the opportunity to respond by posting my favorite poem. The relationship between it and your question -- well, that's for you to decide. The birds have vanished down the sky. Now the last cloud drains away. We sit together, the mountain and me, until only the mountain remains. -- Zazen on Ching-t’ing Mountain reply rosmax_1337 21 minutes agorootparentThe question was sincere yet playful. I appreciate your answer. Here's a poem I like. O'er all the hilltops Is quiet now, In all the treetops Hearest thou Hardly a breath; The birds are asleep in the trees: Wait, soon like these Thou too shalt rest. -- Wanderer's Nightsong II, Goethe reply xelxebar 9 hours agorootparentprevBeautiful. I wasn't aware of Li Bai (李白). Took a bit of searching, but the original looks to be called 独坐敬亭山: 獨坐敬亭山 衆鳥高飛盡 孤雲獨去閒 相看兩不厭 只有敬亭山 This is gorgeous. Thank you for sharing! reply zdw 16 hours agoprevI've generally found that code review first, and rebase-centric systems like Gerrit tend to be much easier to review code in. One of the best parts of this is native support for stacking multiple patches, so people make smaller patches that are easier to review. Code review in Github feels like a bad afterthought - the space-wasting interface that looks more like a forum thread, the inability to track over rebases, etc. reply zeotroph 14 hours agoparentThere is one thing I miss on Gerrit when you push a stack of commits: A central place to talk about the whole of the stack, not just individual commits. This \"big picture\", but still technical stuff, too often happens in the issue tracker. But where to place it, I have no idea. This stack is just too ephemeral and and can be completely different on the next push. reply lima 8 hours agorootparentMany teams use topics for this. reply aseipp 5 hours agorootparentYeah, but you can't really discuss the topic itself, right? I do think this is a weakness of Gerrit. It doesn't really capture \"big picture\" stuff nearly so well. At least on GH you can read the top-level comment, which is independent of the commits inside it. Most of the time I was deep in Gerrit doing review or writing patches, it was because the architectural decisions had already been made elsewhere. I guess it's one of the tradeoffs to Gerrit only being a code review tool. Phabricator also didn't suffer from this so much because you could just create a ticket to discuss things in the exact same space. Gerrit is amazingly extensible though so plugging this in is definitely possible, at least. reply strken 14 hours agoprevThis is interesting. At work we use PRs like the author uses commits, and in fact we squash-and-merge them at the end, but our approach requires rebasing the later PRs whenever we make a change to the earlier PRs. This can be quite laborious, falls afoul of the \"don't force-push\" rule, takes a long time for engineers to learn, and tends to break existing code review comments in the GitHub interface, but works out okay for two or three PRs. In our workflow a commit is less a unit of work and more a savepoint. I also use heavily use stashes, as well as undo-tree-mode in Emacs. This means we have four different ways of tracking the history of source code, which sounds redundant but works out okay in practice. The ergonomics of doing this in Git are pretty bad. I found Phabicator better but still unnecessarily difficult. Perhaps a new source control management tool could have first class support for higher level concepts than just commits and branches, or perhaps that would be even worse to use. reply steveklabnik 6 hours agoparent> Perhaps a new source control management tool could have first class support for higher level concepts than just commits and branches, or perhaps that would be even worse to use. You should check out jj, sapling, or mercurial. reply eddd-ddde 5 hours agorootparent+1 for jj It simplifies my workflow a lot when compared to plain git. I love not having to track and stage manually, plus undo is godsend. reply dusted 7 hours agoprevWe're working with gitlab and not a day goes by where I don't miss gerrit with it's fast and functional ui, good and practical diff viewer, patch-sets, topics, review workflow, and manifests... The thing that gitlab does really well, is making it clear how good gerrit is. reply ams92 15 hours agoprevMost of the complaints here could be solved by having smaller pull requests and then squashing commits when it’s time to merge. reply eximius 15 hours agoparentWell, yes. Or by having stacked branches and rebasing your branches, etc. The point is that GitHub/git's default experience makes this harder to do than a system that bakes it in. reply lozenge 15 hours agoparentprevNot really. The idea is to split work into separate stages which are reviewed separately, but as a whole. In the example: \"small refactor 25LOC -> new API 500LOC -> migrate API users 50LOC\" Making a PR of the small refactor will probably garner comments about \"why is this necessary\". Opening two PRs at the same time is clutter as GitHub presents them as separate. As well, sometimes CI won't pass on one of the stages meaning it can't be a separate PR, but it would still be useful in the code review to see it as a separate stage. reply danparsonson 14 hours agorootparentI'd be quite happy with seeing the three jobs in the article as three separate PRs. Fixing a bug and adding a feature are two jobs that, as I think we all agree, need to be tracked individually - so work on them individually. > As well, sometimes CI won't pass on one of the stages meaning it can't be a separate PR Could you give an example of this? Not sure what you mean. reply Forge36 13 hours agorootparentCommits aren't always perfect. Sometimes I'll make the unit test first, which fails CI and the next set of commits implements the behavior. reply trashburger 10 hours agorootparentBy doing this, you break commit atomicity and make bisects hell. Please don’t do this. Commits aren’t perfect at first for sure, but they should be by the time you make them reviewable. reply snatchpiesinger 10 hours agorootparentIt's fine to break commit atomicity on feature branches. You can use git bisect --first-parent on you development/master branch. reply trashburger 8 hours agorootparentI completely disagree. In doing so you lose all visibility into the components and gradual evolution of the code that atomic commits provide. Same thing with squashing (which is just the worst). reply john_the_writer 14 hours agorootparentprevthe comments about \"why is this necessary\" can be handled with a decent PR template, and a comment. What I tend to do is make the changes locally with different commits and then cherry pick the refactor into a PR branch and wait for that to be accepted. Then I rebase the FULL branch with \"master\" after the merge and create the PR. reply gloryjulio 15 hours agoparentprevWe use stacked commits + rebase only in our company. The commit history is linear and it's very easy to revert changes. I don't see any advantage of using merging instead of rebase I am not sure why we need to squash commits. We encourage the opposite where you should commit small and often. So if we need to revert any commit, it's less painful to do so. reply majormajor 15 hours agorootparentWithout squashing it's hard for me to commit as small and often as I would like. Some things I want out of the final series of commits: 1) everything builds. If I need to revert something or roll back a commit, the resulting point of the codebase is valid and functional and has all passing tests. 2) features are logically grouped and consistent - kinda similar to the first, but it's not just that I want the build to pass, I don't want, say, module A to be not yet ready for feature flag X but module B to expect the feature flag to work. In the original article, this is to say that I want the three commits listed, but not one halfway through the \"migrate API users\" step. But when I'm developing I do want to commit halfway through steps. I might commit 50 lines of changes that I'm confident in and then try the next 50 lines and decide I want to throw them away and try a different way. I might just want to push a central copy of what I've got at the end of the day in case my laptop breaks overnight (it's rare, but happens!). I might want to push something WIP for a coworker to take an initial look at with no intent of it being ready to land. But I don't want any of those inconsistent/not-buildable/not-runnable states to be in the permanent history. It fucks with things like git bisect and git blame. reply tome 11 hours agorootparentI think there's an ambiguity here between squashing every commit in the PR into a single one, and squashing fixup commits made as responses to review into the commits that originated them. For example, if the original commit series was Do a small refactor before I can start adding the test Add the test for the feature Do a small refactor before I can start adding the feature Work in progress Complete sub-feature 1 Work in progress lint lint Complete sub-feature 2 Respond to reviewer 1 comments Respond to reviewer 2 comments Then you can either squash the entire PR down to Implement feature or you can, using interactive rebase, squash (or more precisely fixup) individual WIP, lint, and response commits into where they belong to obtain Do a small refactor before I can start adding the test Do a small refactor before I can start adding the feature Complete sub-feature 1 Complete sub-feature 2 where each commit individually builds and passes tests. I far prefecr the latter! reply gloryjulio 15 hours agorootparentprevWhen we publish a stack of commits, our ci ensures that every commit is build and tested individually. There is no consistency issue Squash and merge actually makes the above goal harder. With rebase + small commits, all we need to make sure is that every commit pass all the build signals and tests during ci reply john_the_writer 14 hours agorootparentThis only works if your commit in a green state. Sometimes we have to change when things are still \"Yellow\".. I tend to add all my tests in one go and commit the RED. \"tests are written\" Then as I pass each test, I commit that. This pattern works really well for me because if I mess up, then rolling back to the last yellow is easy. I can also WIP commit if I have to fix an urgent bug, and then get back to the WIP later. reply gloryjulio 14 hours agorootparentNot sure what you mean... When we ship a stack of commits, every commit has to pass everything in CI. You are not suppose to ship a commit that's not passing the ci bar. There is a escape hatch that you can bypass but it's rarely used. You can make changes before you ship however you wanted as long as they pass ci. If you already shipped the code and want to make changes later, that means making new commit or reverting a bad commit. It's simple as that reply PoignardAzur 14 hours agorootparentprevMy experience is that systemically squashing PRs enables a \"fire and forget\" style where you can add a bunch of small commits to your PR to address reviews and CI failures without worrying about making them fit a narrative of \"these are the commits my PR is made of\". On a more concrete level, squashing PRs means every single commit is guaranteed to pass CI (assuming you also use merge queues) which is helpful when bisecting. reply gloryjulio 14 hours agorootparentWith stacked commits, every commit is already passing CI though. To us the mental model is minimum. All you need to do is to make sure each commit pass CI. You can ship any number of stacked commits together ---------------------------------------------------------------------------------------------------- Not sure why I can't reply in a technical discussion. I have to edit to answer your question @danparsonson > if I'm working on a long series of changes across multiple days, and halfway through it the code doesn't build yet? That's why you break them down into small commits. The early you push it to CI, the earlier you will know whether each commit builds. For example, push commit 1 2 3 to the CI when they are ready. When the CI is running, you are working on commit 4 5 6 > The code won't pass CI because I'm not finished, but I want to commit my progress If your commit 1,2,3 are ready, just ship them. It doesn't stop you have a few commits in reviews and a few WIP commits. There is no down time reply danparsonson 14 hours agorootparentPerhaps I misunderstand you but what if I'm working on a long series of changes across multiple days, and halfway through it the code doesn't build yet? The code won't pass CI because I'm not finished, but I want to commit my progress so I don't lose it if something goes wrong, and I can roll back if make mistakes. reply tome 11 hours agorootparentThen fix up the commit history at the end, for example like this: https://news.ycombinator.com/item?id=41509051 reply globular-toast 11 hours agoparentprevThat's like a caveman approach to the problem. Imagine the extra overhead required to submit the \"refactor\" commit. The result world be either nobody refactors or refactors are just bundled into the feature commit so it's never clear what you're actually reviewing. reply fHr 12 hours agoparentprevYeah I don't see the point, why not just use mergetrains? reply enasterosophes 15 hours agoprevNice, this taught me about `git range-diff` which wasn't on my radar before. Is the conclusion likely to be that the author thinks Gerrit is good, or is there some nuance I didn't pick up? I've used Gerrit before and in hindsight I much prefer it to other ways of doing code review. reply aseipp 14 hours agoparentYes, Gerrit is fucking great. If you actually want to do code review and not just rubber stamp shit on GitHub because your eyes are going to bleed after reading the same thing for the 15th time, just use Gerrit. The thing is, I just never got around to finishing this article because what's there right now is \"good enough\" to get the ideas across. reply loeg 2 hours agoprevThis workflow is exactly what Phabricator[1] facilitates, for what it's worth. Also, if I remember correctly, ReviewBoard. (Though I have not used that in some time and might misremember. Also, it has its own flaws.) Sadly the open source version is unmaintained now. It is still used by FreeBSD. Facebook uses it internally for every single diff, of millions. [1]: https://secure.phabricator.com/book/phabricator/article/diff... reply hoten 3 hours agoprevSome of the problem stated in the post is a little forced, namely the issues with bisect and blame. That's only an issue if the review doesn't end with a squash. Also as a user of gerrit via the Chromium project, I'm not aware of a way to structure the patch sets uploaded as individual changes without infinite foresight. It's always appended into the last. Whereas with github, at least you can rebase. I fully admit I could be missing a gerrit feature. That said... I would really love an interdiff feature for the GitHub rebase-to-fold-in-feedback-to-meaningful-commits workflow. reply fosterfriends 15 hours agoprevI agree with the argument laid out here. Series of small diffs with versions is a fantastic clean model. When creating Graphite on top of GitHub, we chose to only support rebase model (despite the chaos that creates in GitHub timeline events). We also added “versions” support, which wasn’t too hard because GitHub holds on to old commits even if you force push over them. A lot of what we try to build is the exact ideas this author is championing, in a way that’s compatible on top of GitHub. My dream is us and others help usher Eng back towards the patterns Phabricator and Gerrit helped start :) reply aseipp 15 hours agoparentWe strongly considered Graphite as an alternative to Gerrit at my job that I mentioned at the start of this post (which I am no longer at, actually) because it does look like an absolutely excellent product, I will admit. You should all be proud of a smart design and smart set of tools. But there's a really really really really really really big problem. Me and the other main engineer on our team used a custom frontend to Git called Jujutsu[1] for all development. Jujutsu is about 1000x better than Git. So that's nice. (I'm also one of the developers, so I'm not going to abandon it anytime soon.) But gt, the graphite client, is not open source. I have no idea how to make them work together. I have no idea how to extend Jujutsu to handle Graphite stacks, because I don't even think there's an API to handle any of this. I even wrote a Gerrit integration for Jujutsu because JJ works so well at stacking, and Gerrit + Jujutsu is absolutely a force to be reckoned with IMO, even if the UX isn't as nice as Graphite's. (I'm happy to show the people at Graphite why that is, too, if anyone has time or is interested, but I suspect you could all grasp Jujutsu quite easily :) Please! Make gt open source and make it possible for third parties to make and update stacks. This isn't just useful for jj but all kinds of automation that wants to contribute patches -- imagine tools like Google's internal \"Code Review ML models\" for Critique that might recommend you rename a variable based on context. They will suggest the fix for you or even apply it! You can get around some of those workflows with \"Incorporate suggested edits\" which is great on Gerrit (and Graphite?), but not all of them. [1] https://github.com/martinvonz/jj reply jacobegold 13 hours agorootparentWe (Graphite) love Jujutsu – comes up in conversation all the time here. A prior version of the CLI is open source, the core data model (using git refs to store some extra data about what a branch's parent is) is still the same. https://github.com/withgraphite/graphite-cli We've talked about supporting other clients, but don't currently have the bandwidth to build something like that – definitely something I am personally passionate about making sure happens at some point. reply tombl 9 hours agorootparentI think the cli repo went private a while ago. reply jrochkind1 3 hours agoprevI have found myself arranging PR's on github in sequence, for the exact sort of use case OP talks about. (Each PR might have more than one commit, so it's not exactly the same practice). And been frustrated that Github's interface does not make it very easy to make the sequence apparent or have good DX for the reviewers. One could definitely imagine a UX/DX that would. (I do apprecaite github's UI generlaly). reply codeapprove 12 hours agoprevLove the blog post, it's great to see people actually thinking about how code review should work! I've used four different code review systems extensively, all with different strengths and weaknesses: Critique (Google internal), Gerrit (at Google, but same as external), GitHub (duh), and CodeApprove (the one I built). Critique was far and away the best, but it only works because it's perfectly fit to Google's monorepo and the custom VCS they've built as well as all of their custom lint/test tooling. I designed CodeApprove to bring as much of that as I could to GitHub, but it will never really be close. Gerrit was the second best in terms of the reviewer experience ... but as an author I always hated it. It just seemed to be so author-hostile. There were more wrong ways to do something than right ways. And the UI is not exactly beautiful. GitHub is extremely author friendly, it works how we think. You write code, you get feedback, you write more code, etc. If you squash and merge at the end of a PR you don't have the history problems the author mentioned. It's not very reviewer or team friendly though. Incremental diffs are not highlighted. Diffs and conversation are in different tabs. Force pushes and rebases destroy history. Comments are lost as \"outdated\". You can't comment on files outside the diff window. Large files are hidden by default, etc etc. They clearly don't care about this too much and maybe they know something I don't. In the end, the thing I find most frustrating is how many teams just accept whatever code review tool is built in to their VCS platform. That would be like using whatever IDE shipped with your laptop! There are so many better options out there today. My favorites (besides CodeApprove) are GitContext, Reviewable, and Graphite but I can name half a dozen other excellent choices. Don't accept the defaults! reply clktmr 2 hours agoprevI agree in general, but running git bisect on individual PR commits is just doing it wrong. There will always be commits that break stuff temporarily. Run git bisect only on the merge commits instead, which are typically already tested by CI. reply cryptonector 11 hours agoprev> Interlude: Can you please just tell me if git rebase is evil or not so that we can derail the entire discussion over it? Ha, that's funny. But yes, please we need interdiffs in GitHub and GitLab. I want to have my PRs/MRs always rebased and I don't want \"fix code review comments\" commits. reply typeofhuman 4 hours agoprevI just wish I could group related files in the my PR so the reviewer has a better way of reading the changes. It would be like a \"PR Journey\". reply snatchpiesinger 10 hours agoprevAnother way to look at this that this is 3 linearly dependent PRs masquerading as one. Make each a distinct PR and the problem goes away, especially if you can mark the PR to depend on another one (on Gitlab you can, not sure about Github). If you want to see each change as a single logical unit, then they will each be a distinct merge commit on your master branch, use `git log --first-parent-only master` to only see these kind of changes. reply aseipp 8 hours agoparent> Another way to look at this that this is 3 linearly dependent PRs masquerading as one. The first commit in the example is not dependent on anything else and may not exist at all. Rather its existence is illusory to show that sometimes when you write commits, a few things happen at once. You might just churn out a doc fix, a bug fix, and a small other thing all at once. It's just the nature of the work. > (on Gitlab you can, not sure about Github) You cannot do this on GitHub without write access to the repository, so it's effectively a non-option for anyone who is not a committer in an open-source context. Don't ask me why this limitation exists. If you do have write access, you can kind of do some similar things like open N pull requests where each PR 1...N has commits 1...N. Then you do something like \"Read every PR, then merge only the last one which contains all N commits, and close all the others.\" Weird but OK, I guess. Still, organizing this is a pain, and I don't think GitHub really emphasizes it -- and also rebasing the dependent branches really requires you to use something like --update-refs to make it sane at all. So, you can also use tools likein order to organize it for you. Not the end of the world considering everyone uses 50 different Git wrappers, but not ideal. reply zeotroph 10 hours agoparentprev> [GitLab] you can mark the PR to depend on another How much user interaction does that require, and how is this visualized in the review UI? Gerrit creates this dependency with a single `git push`. reply snatchpiesinger 9 hours agorootparentIt's a bit cumbersome, and I think only recently you can make longer dependency chains. It's certainly not automated away with just git commands, but maybe there there is a Gitlab API way. The only way I know is to \"edit\" the PR (or MR in Gitlab speak) and paste the URL into some \"depends on\" field, then save. There are certainly other problems as well, like you might have an MR 1 from feature1 to master, and MR 2 from feature2 to master which in turn depends on MR 1. Most likely your feature2 branch is off your feature1 branch, so it contains feature1's changes when compared to master, and that's what is shown in the Gitlab review UI. This makes reviewing MR 2's changes in parallel to MR 1 frankly impossible. Having said that, I still think that this would be the right way to organize this kind of work, however Gitlab's execution is not great, unfortunately. Any of this is probably impossible in Github too. I wonder if Gerrit gets this right, I have no experience with it. edit: One interesting point of MR dependencies in Gitlab is that I think you can depend on MRs from other projects. This is sometimes useful if you have dependent changes across projects. reply OJFord 15 hours agoprevI create my GitHub PRs like a reviewer's going to look at the commits if it's too large overall. I'm also fairly sure they don't, because it's basically never worth doing so in their PRs ('fix the test', 'merge origin/master', 'address review comment', etc. commits). I suppose I agree GitHub doesn't help with this / implicitly opposes it (I don't see the 'explicitly and' claim justified though?) but there's nothing about a PR that isn't a 'series' of 'patches'. Maybe GitHub's just responding to the way most people use it, and making that easier/better instead of being principled? Doesn't mean we can't be. reply swolchok 2 hours agoprevhttps://github.com/ezyang/ghstack reply pronoiac 14 hours agoprevI'm midway through, but a nitpick: > You're on your own figuring out the Commit IDs and punching them into the URL bar if you want something more granular. There's a \"commits\" tab at the top, like: Conversation[Commits]ChecksFiles changed Example: https://github.com/raspberrypi/linux/pull/6330/commits reply aseipp 14 hours agoparentI more meant that it's hard to diff between arbitrary commits without using the URL bar. The results are also... Weird. For example, let's say you have the base B, and commit X: B ---> X Now someone pushed to main, so you rebase on B' B' ---> X Now, you modify X to address something (maybe just a spelling error) B' ---> X' Now you push the new rebased branch. Question: how do you view the difference between X' and X? Well, you have to use that little \"Compare\" button, but there's a really big problem with it: it shows you the diff from X to X' and the diff from B to B' at the EXACT same time. Which is really bad! Imagine if the difference between B and B' is 500 lines; it will completely dwarf the 1 line typofix from X to X', making it impossible to read. Now, this is kind of a problem in Gerrit too. But they use a UX technique to make it manageable, which is very smart: they color-code the lines of the diff, depending on if the diff comes from B..B' or from X..X' -- so you can see at a glance if the hunk is relevant. More broadly, interdiffing between commit X and commit Y can be tricky, because what you really want to do is something like \"Rebase Y onto the parent of X, then diff X and Y\", because otherwise you get the included differences between their baselines. (We do \"Rebase Y on X's parent\" for Jujutsu's \"interdiff\" command IIRC?) But sometimes you DO want to include the base diff, because the changes from B..B' can be VERY relevant to your patch. So, you need all these options, really. But once you go off this beaten path where you want to compare X to Y... yeah, you have to start typing into the URL bar, I think. But I will say, the commits tab and the little n/p keyboard shortcuts to \"flip through the commits\" like book pages is at least a HUGE improvement over the basic UX though. I use that all the time on GH projects these days, even if I have tons of other problems. reply skydhash 6 hours agorootparentAt my last job, the workflow was mostly merge based instead of rebasing. After you create a branch, you merge from develop of there’s anything new you want or to resolve conflict. And the whole PR will be squashed and merged (there’s an ID referring to the ticket). Force pushing to an open PR was frowned upon. I think it reflected the email based approach a bit better as you can’t alter the first patch you sent, only send new ones. So even if the history of a PR is messy, we preserve its chronological aspect alongside the discussion. reply wodenokoto 11 hours agoprevWhen doing code reviews, I think it is annoying that every time I comment on a line, PR author gets a notification. This is not a simultaneous, real-time thing. I'm in the middle of doing my review, and my comments are not ready to be read. Maybe I'll change my mind on my comment on line 8 when I reach line 80. reply aseipp 5 hours agoparentGitHub lets you do this, but only when writing a review for someone else. Not when addressing someone else's review, in which case your complaint 100% stands. And yes, it absolutely drives me nuts, honestly. Why can I batch review comments, but not resolutions! Another thing Gerrit gets right! GitHub... Please... reply globular-toast 11 hours agoparentprevGitLab lets you compose the entire review before submitting it so the author won't see anything until you're done. reply djur 11 hours agorootparentGithub does, too. You just click \"Start a review\" when filling out your first comment. reply senko 11 hours agorootparentprevGitHub as well. reply kgeist 7 hours agoprevAfter JetBrains discontinued Upsource, we tried several code review tools and it shocked us that many of the tools (both commercial and open source) don't have built-in tools to review incrementally. It's just just on big soup of code, or you have to create new PR's for each small change. Now we have to use JetBrains SpaceCode, which still lacks many of the niceties of Upsource. reply codethief 11 hours agoprevNice article, it captures the issues I've had with code reviews very well. I'm just not sure the \"pairwise diff\" would work well in practice. Sometimes you do forget a change which should be separate commit (in between the existing commits), etc. I recently got introduced to Sapling, specifically to the approach of never merging more than a single commit and also doing code reviews commit by commit. I like that idea even better! Of course with existing tools like git & GitHub this seems rather difficult to implement, but Sapling automatically keeps track of how commits change across fixups & rebases, and it also handles entire \"stacks\" of merge requests / commits. reply outsomnia 8 hours agoprevGuys... look into stgit if you like the sound of this iconoclasm https://stacked-git.github.io/ reply Izkata 6 hours agoprevAs someone who has been on a maintenance team for years and regularly has to dig through the history to figure things out, I strongly prefer the original \"bad\" version with 7 individual commits. Yes \"git blame\" takes a little bit of extra work to get through all the commits, but knowing what initial mistakes were made and refactors done makes it much easier to tell what the original intent was. For example, if \"fix bob review\", \"fix alice review\", or \"minor\" introduced something that wasn't noticed until later, by having them separate we can tell whether it was intended functionality or a bug. This has happened to us a whole bunch of times with rarely seen edge cases, so the bug wasn't found until years later, or some other part of the code was masking the issue so it didn't manifest as a bug until years later. At least one of these was even caused in a \"linting\" commit, and all of these were much more easily fixable because we could tell the bug was introduced in one of these code-review-update commits, rather than the core feature commit. reply lynguist 11 hours agoprevI did code reviews on Azure devops and on Bitbucket. I noticed that Azure devops shows exactly this diff soup, and Bitbucket shows interdiffs, and comments do point to previous points in time etc. It is much better from both points of view. reply sesuximo 14 hours agoprevIf someone brought back phabricator i think they’d make easy $ reply roryokane 12 hours agoparentPhorge (https://phorge.it/) already exists. It’s a still-maintained fork of Phabricator. reply senko 11 hours agoprevI usually deal with “isolated patch series followed by a bunch of fixups” by letting them pile on top, then rebasing just before merging (the setting is usually feature branches all branched from main so that’s fine). It is extra work and not everyone appreciates the benefits, so it’s hard to convince coworkers to do the same. reply tuckerpo 3 hours agoprevLooks like the author really wants `rebase -i` reply Piraty 7 hours agoprevusing github's UI as preferred way to interact with code in review is a bad idea, as it encourages lazy from-the-couch-just-yolo-approve-it-looks-alright style of review. this is where gerrit+mail based workflows shine as reviewer is more encouraged to apply the series, compile/run it in their env (which might differ from your's); here is an example [0]. here are some useful notes on how to have a purely branch centric review process regardless of a webUI: [1] [0]: https://drewdevault.com/2022/07/25/Code-review-with-aerc.htm... [1]: https://news.ycombinator.com/item?id=37086022 reply samatman 2 hours agoprevPosts like this add fuel to the fire of my conviction that the future of revision control is Pijul, or something much like it. I can't completely justify the intuition here, but this seems like an artificial distinction forced by a snapshot model of a codebase. It's literally about the same patches, but arranged differently to give clarity to the history of the project. So it seems obvious on that level that a system where patches are the fundamental stratum has an advantage there, although I confess that I have yet to use Pijul for a substantial project, so I can't describe in detail how it might make this better. I'm just going off things like how a cherry-pick in Pijul applies the actual change from one branch to the other, rather than duplicating it over. I get frustrated a bit when I think about the gap between the kind of difference a leap forward in revision control could make for our profession, and the amount of resources the Pijul project has to work with. So I'm just putting this out there: MSFT bought GitHub for about 9 billion dollars in 2024 money. Anything which displaces it would be worth more than that. That's a lot of leverage for anyone forward-looking enough to apply it. reply travisb 1 hour agoparentWhile patches and snapshots are duals of each other, patches are generally less easy to reason about and work with, in no small part because compilers and humans work on the snapshot, not the patch. Having worked extensively with patches as the semantic unit via patch(1) and quilt and stgit, I'm very skeptical that a VCS based on patches is actually superior in most circumstances. In this particular case, perhaps it would be helpful to view code reviews as a snapshot where the differences (possibly intra-review revision differences) are highlighted instead of as patches. reply keybored 2 hours agoprevAs the author is aware of[1][2] the Git project uses this interdiff approach with email. - Patch series (PR equivalent) go through a round of reviews - Each version has a cover letter (like PR desription) unless it’s only a one-patch series - Each version has that optional cover letter with each patch as a reply email to it - The next version is a reply to the previous cover letter - And each version 2 and above cover letter has a git-range-diff in it (courtesy of git-format-patch) - And also a human/manual summary of changes between versions - Optionally you can have little comments of each patch that are not part of the commit/patch message: just put it between the three dashes and the diff. Or use Git Notes and let it handle it for you (it will put it in the same space). In turn there are no “address feedback” commits in the final (merged) series. Only the changes themselves. Just look at any `[PATCH v2 0/...` or higher (v3...) email on the mailing list: https://lore.kernel.org/git/ Of course this isn’t the easiest workflow: - Email - You ought to keep track of the base commit between versions (you could have rebased on the main branch) - You need to store versions of your branches - You need to keep track of who to CC on the emails. Well, perhaps not if they are the same people throughout, but it is good courtesy to add people who reply to these versions to the CC list - You need to harvest the email message id on the cover letters and use that `In-Reply-To` Phew! But this is quite sublime for reviewers and people who come back to the series years later:[3] - All of the review in the same thread overview - Each version moves the thread to the right - Each patch (to be commit) is commented on individually - You can reply to the commit message and the diff by quoting them directly - The contributor will both give you the range diff (which will highlight diff changes and metadata changes like edited commit messages) and a manual summary of the changes [1] https://news.ycombinator.com/item?id=41511649 [2] And it is with some trepidation that I bring this up because of the aversion some people have to email workflows. Because the “interdiff style” of PRs is useful! [3] Git Notes `amlog` records the message id of the patch email where the commit came from reply EPWN3D 12 hours agoprevIs it just me or is the author just arguing for fixup commits and squashing them, something GitHub and Stash/Bitbucket handle just fine? I don't see how the idea of \"publish a new version of the three commits\" is new with or exclusive to Gerritt. reply planetpluta 5 hours agoprevI’m confused by how pushing to new branches would work on GitHub (or is the point that it doesn’t…)? Are you able to change the branch of a PR from `v1` to `v2` without making a new PR? reply aseipp 5 hours agoparentYes, the point is that it basically doesn't support that. Well, OK. You can push two branches, v1 and v2, each with the commits. Then to do pairwise diffs, you type in the commit object hashes directly into the URL bar to diff the two objects in the repository using the 'blobs' API but like... I don't think that qualifies so much as \"supporting\" it as much as an absurd hack, right? > Are you able to change the branch of a PR from `v1` to `v2` without making a new PR? No, you have to open a whole different PR. reply planetpluta 4 hours agorootparentOkay that makes sense. Agreed, wouldn’t quite consider that “supporting” as far as I’m concerned. reply globular-toast 11 hours agoprevGitLab supports this. Every time someone pushes or force pushes it tags that as a version which you can diff. If your developers know how to generate new commits then you can do it right away with GitLab. The problem is generating the new commits. Developers just aren't very good at doing this. They can modify a single commit just fine, but modify a commit that isn't the latest commit involves a rebase. Magit has the \"instant fixup\" option which is basically like amending an arbitrary commit instead of just the latest. What is actually doing is doing a commit with `--fixup` then `rebase --autosquash`. This technique can be used manually. Fixup/squash commits should be part of all developers' toolkits. reply kspacewalk2 1 hour agoparentRewriting history and breaking N sloppy commits into M well-thought-out, logical commits is an essential git-based version control skill for developers. Thus, interactive rebase should be considered essential for anyone using git for anything non-trivial. It's TUI-like interface is a bit quirky for some people, but it is rock-solid once you figure it out and therefore worth investing a bit of time into learning. (That also describes git in its entirety quite well). reply zeotroph 9 hours agoparentprev> Developers just aren't very good at doing this. GitHub provided a way to contribute, but also to avoid learning to rebase, thus making it more welcoming to devs who only know about commit and pull - that is what made it so popular. The squash then rebase or merge step is done on server side. Plus it has a very \"harmless\" UI, but that hides a lot of details (patchsets) and the layout wastes so much space imo. This also means devs could avoid learning more about git, and this lowest common denominator git workflow makes it so frustrating for those of us who learned git all the way. I can't even mark a PR as \"do not squash\" to prevent it being merged in the default way which throws out all history. reply globular-toast 5 hours agorootparentYeah, this annoys me too. I actually find the \"forced merge\" (`--no-ff`) style even worse because you can never tell if you're looking at \"real\" commits or just crap because they couldn't be bothered to rebase. Must say, though, I think \"history\" is completely the wrong way to think about version control. It's not about tracking history, it's about tracking versions. History is the crap, unrebased commits. Rebasing turns the history (throwaway, works in progress) into versions. reply jeffbee 15 hours agoprev [–] Someone should do a deep dive into developer productivity after LLVM switched from Phabricator to GitHub. How many other major projects have done a switch like that? reply ajkjk 15 hours agoparent [–] Better it be a deep dive into developer sanity, a more important but comparatively under-valued metric. reply jeffbee 15 hours agorootparent [–] For me, GitHub PR review drives me crazy. It's good for exactly one round of exchange. After that nobody can tell what the heck is going on. So my self-reported mental health would be worse. But on non-subjective metrics it seems like LLVM PRs on GitHub are gathering noticeably less discussion than they used to enjoy as Phabricator diffs. reply zeotroph 14 hours agorootparent [–] And just visually, GitHub wastes so much vertical space, so even trying to place what belong to which patchset becomes hard. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Gerrit Code Review is an open-source tool compatible with Git repositories, facilitating patch writing, submission, feedback, and fixes.",
      "Traditional GitHub code reviews can lead to \"diff soup,\" complicating commit histories and making tools like git blame and git bisect less effective.",
      "The \"interdiff\" review method, which publishes new versions of original commits, maintains cleaner commit histories and simplifies the review process using tools like git range-diff."
    ],
    "commentSummary": [
      "The discussion highlights the use of \"interdiff\" code review workflows on GitHub, which allows reviewers to see diffs with feedback incorporated without breaking `git blame` and `git bisect`.",
      "The workflow involves using `git commit --fixup`, `git rebase --interactive --autosquash`, and `git push --force-with-lease` to manage and merge changes efficiently.",
      "The conversation underscores the limitations of GitHub's UX in handling advanced Git features like rebasing and autosquash, and suggests that better tools or workflows could improve the code review process."
    ],
    "points": 190,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1726000791
  },
  {
    "id": 41505593,
    "title": "Lottery Simulator (2023)",
    "originLink": "https://perthirtysix.com/tool/lottery-simulator",
    "originBody": "Lottery Simulator Interactively explore lottery probabilities and simulate thousands of tickets in seconds Shri Khalpada May 9, 2023 A New Daily Game! We built a daily poll game called the Communal Plot! We hope it's a fun way to engage with the community and see how your opinions stack up. Check it out and let us know what you think! Every so often, a lottery jackpot will get so high that I'll hear about it on the news or from a friend. When this happens, I immediate start wondering about two things: what I would do with hundreds of millions of dollars and what the odds of winning really are. While major lotteries publish some of this information, I wanted to build something that would make it easier to play around with the data in a more exploratory way. With that, here is the PerThirtySix Lottery Simulator! This tool is broken up into two sections: Setup and Simulation. The Setup section lets explore probabilities for an existing American lottery or for your own lottery with custom rules. The Simulation section lets you pick some numbers and play up to thousands of tickets per second, and visualizes the returns for you. Note that this tool makes some simplifying assumptions, like that there's only one jackpot winner and that taxes are ignored. 1) Setup You can set up the rules for an existing lottery system or for your own lottery with custom rules. Lottery: Mega MillionsPowerballMegaBucks PlusLotto AmericaCustom Rules Select 5 numbers from 1 to 70 and one bonus number from 1 to 25 Ticket Ticket Cost: $2 Breakeven Probability: 4.2% ✔✔✔✔✔✔ $330,000,000 * 1 in 302.6M odds (0.0%) ✔✔✔✔✔❌ $1,000,000 1 in 12.6M odds (0.0%) ✔✔✔✔❌✔ $10,000 1 in 931.0K odds (0.0%) ✔✔✔✔❌❌ $500 1 in 38.8K odds (0.0%) ✔✔✔❌❌✔ $200 1 in 14.5K odds (0.0%) ✔✔✔❌❌❌ $10 1 in 606.1 odds (0.2%) ✔✔❌❌❌✔ $10 1 in 692.7 odds (0.1%) ✔✔❌❌❌❌ $0 1 in 28.9 odds (3.5%) ✔❌❌❌❌✔ $4 1 in 89.4 odds (1.1%) ✔❌❌❌❌❌ $0 1 in 3.7 odds (26.9%) ❌❌❌❌❌✔ $2 1 in 36.6 odds (2.7%) ❌❌❌❌❌❌ $0 1 in 1.5 odds (65.5%) * Jackpot value roughly estimated based on recent historical data. You can use the \"Custom\" option to set your own payout values. LinearLogarithmic 2) Simulation Once you're done setting up the rules above and picking your numbers, hit Start below! Start Simulation Speed: Slowest Slow Medium Fast Turbo Pick your own numbersPick random numbers Show Animation LinearLogarithmic We're just getting started. Subscribe for more thoughtful, data-driven explorations.",
    "commentLink": "https://news.ycombinator.com/item?id=41505593",
    "commentBody": "Lottery Simulator (2023) (perthirtysix.com)187 points by airstrike 21 hours agohidepastfavorite87 comments shriracha 21 hours agoHi! I made this tool. I saw it had way more traffic than usual and then realized it was from HN, very cool! Would love to hear any feedback. I've been super interested in how well-designed web apps and visualizations can communicate things like probability, which I think is very hard to intuit for many of us. The most surprising thing I learned from the tool was just how bad your payouts usually were even if you cut the pool of numbers to pick from in half (by using the \"Custom\" option). reply BitwiseFool 19 hours agoparentI would very much like to see an even faster \"turbo\" option because even though the current max speed of 1,000 tickets a second really drives home the point of how long it would take to actually win, I still want to see the simulation hit the jackpot a few times. reply Vullun 19 hours agoparentprevI noticed that when you select pick random numbers, it only picks it once. Can we run the simulation where it picks new numbers every time? I would love to see if that would make an impact on the odds. reply sfilmeyer 18 hours agorootparent>I would love to see if that would make an impact on the odds. It won't make an impact on the odds of your tickets coming up as winners, unless they have a bug in their simulation. In the real world the probability of single versus multiple jackpot winners might vary with number choices, but they've already said they're assuming a single jackpot winner. reply rerdavies 8 hours agoparentprevSelect random numbers greater than 31, and run them against past winning numbers, using the actual prize pool payouts (histories available for all major lotteries). Repeat millions of times. You'll be amazed how much more money you win. ;-P (And how much money you still lose even with this impressive advantage).Unfortunately, there aren't enough historical draws to determine whether there are more fine-grained statistically-significant differences. My tests were actually run on first-half vs. last half. But > 31 seems like a defensible rule as well. reply Popeyes 11 hours agoparentprevReally good tool, I made a very basic text one and showed a few friends and it put them off the lottery. One suggestion I would make is allowing the random numbers selected by the user to be random for each draw. I understand in the scheme of things that it doesn't make difference to your odds but that's a strategy that some people play. It would also be cool to track how many other people chose the same numbers that you did and then divide the jackpot winnings by that amount of people. Another thing (I'm really just giving you a todo list of things that I had) was to organise the number selectors to mirror the pattern on the lottery tickets. People have very different approaches, picking columns, going over 31, sequences and you could track the user behaviour behind number selection. reply Retric 17 hours agoparentprevReading people’s replies it seems like many are misunderstanding the expected value of a ticket. You may want to add another column to the table of odds showing what percentage a ticket’s hypothetical payout is from each prize, or just what the average loss per ticket is. Also, at one point I messed up which pattern on Mega Millions tickets were winners. Something about that big X in a separate column just subconsciously seemed like it was showing winners. Perhaps dashes vs checks or groupings winning and non winning tickets together. reply airstrike 14 hours agorootparentI did the same when I tried to create my own lottery. Thought the \"correct\" icons were the \"wrong guess\" ones and vice-versa reply earle_wa 17 hours agoparentprevCan you track how many people have hit the jackpot? Or how long folks stuck around to see if they hit the jackpot? reply athorax 4 hours agoparentprevIt might just be me, but it took me awhile to even realize there were interactive components on the page. Something about it made it seem like they were screenshots. reply hakonslie 9 hours agoparentprevI never use Lotteries, but I was curious and wanted to test a domestic one, however the bonus number can be from 1 to 5, but I cant put it lower than 10 on your page :'( reply SillyUsername 11 hours agoparentprevWhat probability distribution are you using for the random numbers? reply tirant 13 hours agoparentprevCustom option needs more options: some big lotteries use two extra numbers (e.g. Euromillions). Other lotteries just pick up a single number from 00000 to 99999 (or even lower). reply is_true 19 hours agoparentprevwow, it's really good and the rest of your site is even better reply hitthejackpot 7 hours agoparentprevnext [2 more] [flagged] robofanatic 7 hours agorootparentbuy a real lottery ticket reply zzanz 21 hours agoprevI used to work at a lotto counter in my towns supermarket. When I started I noticed alot of older regular buyers, a weekly lotto purchase like the daily newspaper. However, as the younger generation started bringing in kids I didn't see this habit, instead just an occasional purchase for a birthday gift or rolling the dice because the jackpots gotten big enough (funnily enough the time when the chance of winning is actually lowest). Overall I would consider lotto small next to the scratch cards (our countries version at least). I have never seen a more predatory marketing strategy, and completely swept under the rug next to lotto being berated with anti-gambling campaigning. To be fair, lotto is bad, but scratch cards are much, much worse. A memory that stuck for me was a customer blowing well over $100 bucks on scratchcards over 20 minutes, just pulling over and over, then getting card declined at the grocery checkouts. reply IncreasePosts 21 hours agoparent> funnily enough the time when the chance of winning is actually lowest Not really? The odds of winning are the same regardless, because you need to match every number to get a jackpot. Really, there is just an increased chance of splitting a jackpot with another person when the prize gets really large, since more tickets are generally sold. But I imagine EV of a lottery ticket with a $1B jackpot is still higher than the same lottery ticket when the jackpot is $100M. reply function_seven 20 hours agorootparentThere’s a balance between jackpot size and a given drawing’s popularity for sure. There are also bad number choices and good number choices. 1,2,3,4,5,6 is a terrible selection, for example. Not because it is somehow “less random”, but because you’re guaranteed to be splitting that jackpot with a 1,000 other nerds who were trying to prove a point! To a lesser degree, choosing numbers under 31, or under 12, will put you in a collision space with other players who like to choose birthdays. Just use the random pick and don’t think about it. If you do win the jackpot, you have higher odds of being the only one. reply RulerOf 18 hours agorootparent> 1,2,3,4,5,6 is a terrible selection, for example. Not because it is somehow “less random”, but because you’re guaranteed to be splitting that jackpot with a 1,000 other nerds who were trying to prove a point! Uh... so at first I saw your point, but if your odds of winning never actually change, how is not winning better than splitting a jackpot? reply function_seven 18 hours agorootparentI guess if you only play one drawing, you’re right. Winning is always better than losing. But if you play the lottery week after week, year after year—and you always play the same numbers—then you’re ensuring a mediocre prize should you actually get the jackpot. Playing the lottery is not a mathematically sound decision in any case, but there’s no reason to make it even worse by chopping your potential jackpot winnings down by over 99% reply bongodongobob 17 hours agorootparentprevThe odds of winning don't change, the odds of splitting the pot change. Certain numbers are picked more than others so to have the best odds of not splitting the pot, random numbers are best. reply rerdavies 8 hours agorootparentNumbers greater that 31 are better. Almost 30% better! Because you are less likely to split a pot when you win. But not good enough to make playing a lottery ticket a winning propostition. reply jamie_ca 20 hours agorootparentprevMaybe, \"the time when expected value is the lowest\"? The BC 6/49 lottery (6 balls 1-49, one bonus ball) for example has 53% of the common \"prize pool\" split amongst all 4-ball matchers, so if you're not hitting the jackpot you get less cash out of a high-demand drawing. And given the prize pool is something like 18% of net receipts... yeah EV is still well in the negatives. reply euroderf 12 hours agorootparentprev> > funnily enough the time when the chance of winning is actually lowest > Not really? A big jackpot draws more players, and that reduces the payouts at the intermediate levels. reply krisoft 6 hours agorootparent> that reduces the payouts at the intermediate levels Which has nothing to do with your chance of winning. reply euroderf 1 hour agorootparentBut has something to do with the expected winning making it worth the investment. reply gosub100 5 hours agorootparentprevWhat do you mean by intermediate levels? In the 2 main US lotteries the only award that gets split is the single jackpot. Even the 2nd place award is $1 million and is not divided among multiple winners. reply moduspol 6 hours agoparentprevAnd now we also have the long-term effects of online sports betting to look forward to. reply navark 3 hours agoprevI won! That's it, the lottery is worth playing if only I can repeat this performance IRL. I won the $340,000,000 Powerball Grand Prize after buying 11,651,310 $2 tickets leaving me with a grand total of $319,781,766 in earnings ($27.45 per ticket). reply FireBeyond 1 hour agoparentInteresting. You got it at 11.6M tickets... I got it at 10,847,948. reply islewis 20 hours agoprevCan someone explain to me how the EV can be so incredibly low? I know the answer is because people will buy the tickets no matter what, but even compared to other losing games the lottery comes away looking like an absolute bandit. A run on the simulation (n=1000000) comes back with -92% EV. It looks like -10% [1] is a rough estimate for slot machine EV, which I would ballpark into the same game genre (-EV, no skill entertainment) as the lottery. What accounts for this payout discrepancy in what I would consider similar games? On that train of thought, what prevents a new lottery from coming in and offering a _generous_ -50% lottery, offering ~5x as much money as before? [1]* https://www.888casino.com/blog/expected-value reply cataflam 20 hours agoparentBecause you shouldn't use the simulator to calculate the EV, or said differently your n=1000000 is too small. Assuming you used the first lottery example (Mega Millions), the EV is easy to calculate directly and is -$0.66/ticket, ie -33% The jackpot is a whole $1 of that EV! Without it, the EV is -$1.75/ticket, ie -87%, which is closer to what you got in the simulation. reply thephyber 16 hours agorootparentExactly. In short, the simulator doesn’t buy enough ticket-draws to approach the Law of Large Numbers. But that’s also a feature of the lottery — most people overestimate their ability to win or underestimate how many lifetimes of consistent play is required to statistically win a jackpot. reply onion2k 13 hours agorootparentI don't think people actually make that mistake. They know the chance of winning is tiny. The point is more that a non-zero chance of life changing money (plus the entertainment of fantasising about a win) is worth more to them than the cost of the ticket. reply gamepsys 4 hours agorootparentExactly, winning the lottery is massively life changing. This is actually something I think people don't understand about the psychology of lottery. In some regards it doesn't matter if the money is $50M or $500M for most players even though that has a huge impact on the EV. reply serf 20 hours agoparentprev>On that train of thought, what prevents a new lottery from coming in and offering a _generous_ -50% lottery, offering ~5x as much money as before? federal-level gambling syndicate isn't something that a private party can easily jump into. so the answer is : a mix of 'grandfather'd-in' and protectionism, if we're talking U.S. here. reply thephyber 16 hours agoparentprev> What accounts for this payout discrepancy Mega lotteries draw once every 2-8 days. Slot machines / video poker / etc are happy to draw as fast as you can push the button. They are designed to take your money, but their rewards systems are completely different. Also, the mega lotteries benefit from viral marketing, “earned media”, and water cooler talk. Slot machines are just a way for bored people to pass the time, much like video games or doomscrolling. reply elseweather 20 hours agoparentprevIn the US at least they're a state monopoly reply gosub100 5 hours agoparentprevBecause a lot of the proceeds go to school districts. reply Aeolun 21 hours agoprevI think this lottery simulator is a scam. I played a hundred thousand games and never made my money back. reply madamelic 21 hours agoparentYou obviously haven't played enough. You stopped right before you hit it big! reply frogpelt 19 hours agoparentprevYou were due for a big win. reply phs318u 16 hours agoparentprevYou forgot the /s. reply itake 21 hours agoprevthe expected value of each ticket is negative, but going from 0 tickets to 1 ticket, increases your chances of a big win by infinity. Going from 1 to n tickets, isn’t necessarily wise reply GrantMoyer 17 hours agoparentOn the other hand, if I don't buy a lottery ticket, I have no chance to waste my money on the lottery, but if I do buy a ticket, the odds of wasting money are infinitely higher. reply caseyy 21 hours agoparentprevEstimated monetary value might be a wiser metric to use :) Going by the calculator, EMV of going from buying 0 tickets to 1 is -$1.85 for Mega Millions, same as going from n to n+1; n >= 0. The first ticket is the first one you lose with, statistically. reply ff317 20 hours agoparentprevThat's kind of how I look at it, in practice. I get the mathematical reality that buying lotto tickets is a financial waste. However, if I never buy a single ticket, there is a definite 0% chance I'll ever win the big prize. Whereas if I play at all, at least there's a chance, however remote, of a quite life-changing positive event happening. So, therefore, it makes sense to put a very small amount of totally throw-away income into big-prize lotto tickets, just so you're in the game at all. Based on this kind of thinking, my personal rules are: never spend more than 0.1% of take-home pay per time-period buying tickets, and only buy big-prize lotto tickets that have potentially-life-changing payouts. reply seagullriffic 20 hours agorootparentThis is almost exactly how I think about it too - a good repeatable mental model is \"infinite upside / near-zero downside\". These massively asymmetric choices occur elsewhere in life, e.g. \"asking them out on a date\"; \"asking for a raise\", and are good to look out for. reply cwillu 19 hours agorootparentDunno, seems wild to compare positive-but-still-unlikely ev things with straight up negative ev things like gambling. reply krisoft 6 hours agorootparentprev> However, if I never buy a single ticket, there is a definite 0% chance I'll ever win the big prize. I'm not sure about that. There is some chance of someone random buying a ticket and gifting it to you and then that ticket winning. Not a big chance. But it is not 0. reply avidiax 14 hours agorootparentprevThere's a near-zero possibility that the next potato chip in the bag has been laced with cyanide. But if you never eat any potato chips, there's a definite 0% chance that you'll die of cyanide-laced potato chips. This kind of thinking never holds up when it's a small chance of a horrible outcome. reply ryanjshaw 13 hours agorootparentHow many potato chip killers have there been? How many lottery winners? reply BigParm 20 hours agoparentprevYour chances were 0, you buy a ticket, and now your chances are x. Where x is not infinitely greater than 0, it's x greater than 0. Pretty sure your proposition implies that 2 tickets provide the same odds as one ticket (2 infinity vs 1 infinity). I haven't gotten into the discrete math in many years. If you're right do you mind explaining please? I can intuit what you're getting at. 0 odds * inf $200. I think I'm taking the wrong lesson from this - time to get gamblin' Screenshot: https://ibb.co/6mm3hqh reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The PerThirtySix Lottery Simulator allows users to explore lottery probabilities and simulate thousands of tickets in seconds.",
      "Users can set up simulations for existing American lotteries like Mega Millions and Powerball or create custom rules, including ticket cost and breakeven probability.",
      "The tool provides visualizations of returns and includes simplifying assumptions such as a single jackpot winner and ignoring taxes."
    ],
    "commentSummary": [
      "A new Lottery Simulator tool has been created, generating significant interest and feedback from users on Hacker News.",
      "Users are suggesting various improvements, such as a faster simulation option, random number selection for each draw, and tracking the number of people who hit the jackpot.",
      "The tool highlights the poor payout odds of lotteries, even when using custom number pools, and stimulates discussions on probability, expected value (EV), and the impact of jackpot size on winnings."
    ],
    "points": 188,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1726002544
  },
  {
    "id": 41508040,
    "title": "I wish I didn't miss the '90s-00s internet",
    "originLink": "https://rohan.ga/blog/early-internet/",
    "originBody": "I Wish I Didn't Miss the '90s-00s Internet 06 Sep, 2024 about me I am 18, born in 2006. This is generally a good thing as I am in the prime of life currently. I am not one of those people who think they were “born in the wrong decade”, I think I was born at the perfect time to take advantage of superlinearly growing technological advancements. the internet today I generally greatly dislike social media, although I am an avid user of it. When social media caught on, the people running these companies were tasked to make it profitable. In doing this, social media got completely ruined. Our data got commodified[1], our attention got commodified, and a substantive part of who we say we are got commodified. This, in general, has led to a degradation in the quality of the internet. They basically made social media like a drug, as addictive as possible. They do this by promoting FOMO and comparison in Instagram’s case. Instagram is a game, it is extremely performative. People carefully curate each part of their insta to give certain impressions. What’s the ratio of followers to following you have? Are your story highlights organized and “aesthetic”? What reels are you liking? There are a lot of “rules” in this game, which are enforced by social “ins” and mutual respect. When it comes to shortform content, hundreds of people compete for slivers of our attention. We are not agents in this, they are just presented to us. Completely depersonalized. They are forgotten within seconds. Ask someone watching tiktok to describe the previous tiktoks they just watched, they would be hard pressed to tell you more than a few minutes in the past. Something about tiktok is unusually addictive. all the while providing absolutely no value. It has become so shallow, you can tell almost nothing about who someone actually is through Instagram or tiktok. You can only tell how they want to portray themselves to the general population and, by how they organize their profile, if they are eligible to be a part of your social circle. the appeal of simplicity I wish I was around when people had blogs or even myspace. This era was deeply personal and creative. Most writing on the internet was individual, not written in search of “SEO” or profit but driven by the need and want of people to share knowledge–pure curiosity. I want the thrill of finding new websites searching through web rings; when the web was truly the wild west and not another arm of control by mega corporations. This is also reflective in the quality of content. There was little incentive to lie, to manipulate truth, and each blog entry or piece of information was tied to identity. (except in the cases of anonymity). Even the content written by normal people for normal people has been commodified by sites like reddit and quora. What happened to an old fashioned forums or even usenet groups? (granted, especially for cars and hacking, there still exists plenty of forums) Also, websites just simply looked cooler. Occasionally I scroll on the geocities archive and wonder, how did we get here? What happened to the patterned backgrounds, the bright maximalist jpegs and gifs? This is sort of contradictory to my website, as it’s almost annoyingly minimalist, but this more has to do with social norms and simplicity. Having a personal blog is already out of the ordinary, but the simple design and clear technical direction/theme gives me an excuse. I also am not that personal on here, because only a few of my friends frequent my blogs, and I want the site to be as simple and to the point as possible if a random person wants to know who I am. a niche resurgence There is neocites, and a small community of people who share this philosophy about the web (and that are relatively young), but I have not met anyone my age, in the real world, that would choose to do something like this. The majority of people (my age) today would think sites like those (and, by extension, their creators) are weird. [1] you can substitute “commodified” with “bought and sold”",
    "commentLink": "https://news.ycombinator.com/item?id=41508040",
    "commentBody": "I wish I didn't miss the '90s-00s internet (rohan.ga)183 points by ocean_moist 14 hours agohidepastfavorite229 comments i_c_b 8 hours agoI went to college in 1995, and my very first week of school, I was introduced to the internet, usenet, ftp, and netscape navigator. A few months later, I was downloading cool .mod files and .xm files from aminet and learning to write tracker music in Fast Tracker 2, downloading and playing all sorts of cool Doom wads, installing DJGPP and pouring over the source code for Allegro and picking up more game programming chops, and getting incredibly caught up in following the Doom community and .plan files for the release of Quake. Then Quake came out, and the community that grew up around it (both for multiplayer deathmatch and for QuakeC mods) were incredible. I remember following several guys putting up all sorts of cool experiments on their personal webpage, and then being really surprised when they got hired by some random company that hadn't done anything yet, Valve. There was really just this incredible, amateur-in-the-best-sense energy to all those communities I had discovered, and it didn't seem like many people (at least to my recollection) in those communities had any inkling that all that effort was monetizable, yet... which would shortly change, of course. But everything had a loose, thrown off quality, and it was all largely pseudo-anonymous. It felt very set apart from the real world, in a very counter cultural way. Or at least that's how I experienced it. This was all, needless to say, disastrous to my college career. But it was an incredible launching pad for me to get in the game industry and ship Quake engine games 2 years later, in many cases with other people pulled from those same online communities. I miss that time too. But I think there's something like a lightning in a bottle aspect to it all - like, lots of really new, really exciting things were happening, but it took some time for all the social machinery of legible value creation / maximization to catch up because some of those things were really so new and hard to understand if you weren't in at the ground floor (and, often, young, particularly receptive to it all, and comfortable messing around with amateur stuff that looked, from the outside, kind of pointless). reply JeremyNT 5 hours agoparentSimilar story here, with similarly disastrous impacts on my GPA. There was something magical about that time - technology was moving so rapidly and access to information was exploding. It was all so very early that it seemed like anything was possible for an aspiring computer nerd with a good computer and a fast internet connection. Of course, it was also really unevenly distributed. If you were on the \"have\" side of the equation - i.e. in a setting like a college campus, already working in the industry, or in the right IRC channels, with access to modern hardware - you could hop along for the ride and it felt like anything was possible. Otherwise, you were being left behind at a dramatic rate. Overall things are better now, because so many more people have access to data and resources online. It's trivially easy to learn how to code, information is readily available to most of humanity, and access to good quality internet access has exploded. But I can't deny that it was kind of amazing being one of the lucky ones able to ride that wave. reply ryandrake 2 hours agorootparentSame here, the Internet, game modding, early LAN->Internet bridges for multiplayer gaming, IRC and all that probably reduced my GPA by about -1.0 and that caused me to miss out on the \"premium\" tech employers early in my career, ultimately set me back decades. Thank you, rec.games.computer.quake.* hierarchy and Quake-C mailing lists. reply Dalewyn 5 hours agoparentprev>It felt very set apart from the real world, in a very counter cultural way. We hate the internet today because it became mainstream. https://en.wikipedia.org/wiki/Eternal_September reply JohnFen 5 hours agorootparentI hate the internet today not because it became mainstream, but because it became commercialized and that squeezed out too much of the best stuff. reply marcellus23 2 hours agorootparentThat was a result of it becoming mainstream. reply JohnFen 2 hours agorootparentIt's a different thing nonetheless. I don't think that the thing that makes the modern web bad is that the \"unwashed masses\" are using it (as several commenters here assert), it's the commercialization. The web is no longer a place for people to be able to interact freely with each other. It's a place to monetize or be monetized. That means that a lot of the value of the web is gone, because it's value that can't be monetized without destroying it. reply DrillShopper 5 minutes agorootparentThe \"unwashed masses\" (your words) are only here because companies that want to advertise to them made their systems just good enough to draw them in but just bad enough they exploit the worse instincts in people to make more advertising money. If the web was not commercial then it wouldn't be mainstream. While they are different they are fundamentally linked. canucker2016 2 hours agorootparentprevThe Green Card spam on Usenet is my line in the sand. Usenet got a lot more annoying after that. see https://en.wikipedia.org/wiki/Laurence_Canter_and_Martha_Sie... reply beowulfey 5 hours agorootparentprevI think this is genuinely true. The internet today appeals to the lowest common denominator, in the same way that blockbuster movies often do. It is less appealing because it is less specific to our tastes. reply vbo 13 hours agoprevI miss the 00s internet. I miss IRC and geeking out for the sake of it. Maybe i'm just missing my younger years, but I think there was a distinct feeling back then, of wonder and being amongst the first to tinker with these promising technologies that were going to change the world for the better and now it's 2024 and we've screwed it all up. reply surgical_fire 9 hours agoparentA lot of things got worse, it's not just nostalgia. The spread of social media from mid-00's onwards, and especially in 10's was a tragedy, but not for the main reasons people normally think. The way people organized back then (forums, IRC channels, blogs, etc) was more authentic, as there was no tangible corporate interest in keeping you hooked to it through underhanded algorithmic manipulation to drive engagement. There were no sponsored content, no farming of every piece of data about users to feed an endlessly greedy advertisement machine. It was just people and their genuine interests. Part of the problem is that geek culture became mainstream. When I was a kid in the 90's, me and my friends were considered the weird bunch for liking videogames, computers, tabletop RPG, etc. Sometime around mid-00s it became mainstream, and brought along with it people that prior to that had no interest in that niche of culture, and along with it that culture meaningfully changed for the worse. There's more to it, but I rambled enough. If there's one positive thing I can think of, is that at least the general positivity surrounding tech is gone. This skepticism is healthy, especially considering how things worsened since then. reply mschuster91 7 hours agorootparentOn the other side, a lot of it wasn't sustainable. Just how many forums just vanished all of a sudden as the owner died, ran out of money or was simply fed up moderating bullshit and infights, not to mention the ever increasing compliance workload/risk (yeeting spam, warez and especially CSAM)? A lot of the early-ish Internet depended on the generosity of others - Usenet, IRC, Linux distros or SourceForge for example, lots of that was universities and ISPs - and on users keeping to the unwritten contract of \"don't be evil\". Bad actors weren't the norm, especially as there were no monetary incentives attached to hackers. Yes, you had your early worms and viruses (ILOVEYOU, remember that one), you had your trolls (DCC SEND STARTKEYLOGGER 0 0 0), but in general these were all harmless. Nowadays? Bad actors are financially motivated on all sides - there's malware-as-a-service shops, bitcoin and other cryptocurrencies attract both thieves and money launderers, you can rent out botnets for a few bucks an hour that can take down anyone not hiding behind one of the large CDNs. CSAM spreaders are even more a threat than before... back in the day, they'd fap off in solitude to teen pageants, nowadays virtually every service that allows UGC uploads has to deal with absurd amounts of CSAM, and they're all organized in the darknet to exchange tips about new places / ways to hide their crap in the clearnet because Tor just is too slow. And honestly it's hard to cope with all of that, which means that self-hosting is out of the question unless you got a looot of time dealing with bad actors of all kinds, and people flock to the centralized megapolises and walled gardens instead. A subreddit for whatever ultra niche topic may feed Reddit and its AI, but at least Reddit takes care about botnets, CSAM and spam. I think that Shodan and LetsEncrypt (or rather, Certificate Transparency) are partially to blame for the rise of cybercrime. Prior to both, if you'd just not share your domain name outside your social circle, chances were high you'd live on unnoticed in the wide seas of the Internet. But now, where you all but have to get a HTTPS certificate to avoid browser warnings, you also have to apply for such a certificate, and your domain name will appear in a public registry that can, is and will be mined by bad actors, and then visited by Shodan or by bad actors directly, all looking for common pitfalls or a zero-day patch you missed to apply in the first 15 minutes after the public release. reply morkalork 4 hours agorootparentI remember when admins of phpBB boards asked for PayPal donations to pay server bills every 6 months! I feel like running the same forums now should cost almost nothing for infrastructure. The moderation is still a killer though. reply surgical_fire 6 hours agorootparentprevI don't disagree that it was not really sustainable outside that small-ish timeframe of mid-90s to mid-00s. The change for the worse was perhaps an unavoidable change for the worse. And there are things that changed for the worse that neither you nor me talked about. For example, I really miss how online gaming worked back in the early 2000s (no matter how janky it was), qhen there was no real monetary incentive of companies trying to keep people playing on their online platforms. Maybe the fact that I recognize that the way things changed were unavoidable fuels my general disdain internet culture nowadays, and my skepticism to tech innovations in a broader sense. Oh well. reply mschuster91 6 hours agorootparent> For example, I really miss how online gaming worked back in the early 2000s (no matter how janky it was), qhen there was no real monetary incentive of companies trying to keep people playing on their online platforms. I'd also blame rampant cheating for that. It's damn expensive to keep up with pirates, but cheaters are an entirely different league... the most advanced cheats these days are using dedicated PCI cards to directly manipulate memory with barely any ability for the host to detect or prevent it [1]. From the grapevines, there are developers charging hundreds of dollars per month to develop and maintain these things. On top of that, up until the late '00s no one cared too much about racist slurs, sexism or other forms of discrimination. Maybe you'd get yeeted off from a server if you'd overdo it. But nowadays? Ever since GTA SA and its infamous Hot Coffee mod, there are a loooooot of \"concerned parent\" eyeballs on gaming, there's advertisers/sponsors looking for their brand image, and game developers also don't want to be associated with such behavior. And so, they took away self-hosted servers so that they could moderate everything that was going on... and here we are now. [1] https://github.com/mbrking/ceserver-pcileech reply skydhash 5 hours agorootparent> I'd also blame rampant cheating for that. It's damn expensive to keep up with pirates, but cheaters are an entirely different league I'm not an avid gamer, but it's not hard to notice that multiplayer games nowadays means \"all the player in the world\". Most games don't have a local version to either play with multiple controllers or through LAN. They don't even want to allow custom groups to play with. Cheating is way easier to manage at small scale. reply Kye 5 hours agorootparentprevPeople certainly cared about racism, sexism, and other discrimination back then. They just put up with it because there was no movement to change it. It got worse any time I spoke up, so I learned to keep my head down. Do not mistake my tolerating slurs and other insults for enjoying Nintendo games with being okay with it or the people who did it, or the people who did it and still remember being able to do it without consequence as a better time. reply surgical_fire 4 hours agorootparentBack in the day you didn't typically play in massivevly populated online servers with matchmaking against complete anonymous strangers. You typically played with a small group of people. LAN houses with people that were there physically, or groups of friends (even if they were online friends). Even for stuff such as bnet when I played Diablo 2 or WC3, you typically created a game instance, and over time you could recognize the people playing. You curated friends lists, so you would know to avoid the ones that behaved in a way that didn't jive with the rest of the group. Perhaps it was not scalable, and a change for the worse was unavoidable. There was a simplicity in those interactions that is completely lost and may be impossible to capture again. An echo of a time long past. reply Kye 4 hours agorootparentEven then, it had the same problem you still face with in-person tabletop groups. If you find a good group that does a session 0 where everyone respects what's laid down, it's fantastic. If not, it's no better than a matchmaking lobby with the worst teenagers. In-person or online or with a small group makes no difference if the norms they all agree on are trash. Things are better now because you can find that group that aligns with your values. You aren't stuck with the shitty guild that tolerates your differences (at best) because there are enough people online and gaming to where there's probably another that fits better. And it's even better offline because you can connect with those few people in your nowhere little town who aren't butts. edit: for example https://news.ycombinator.com/item?id=40347601 reply surgical_fire 4 hours agorootparentEh, I think things are much worse now. It's the reason why I seldom play online, and when I do I have absolutely no desire to communicate with anyone (when I play online the first thing I do is muting everyone else. I don't want to read what they write and much less listen to their voices). There is no community, I am in a centralized server being matched against random people. And when there is a community, it's normally a cesspool where online interaction is at best meaningless. See Twitter for example (no matter if it before or after the retarded buffoon that acquired it, it was always a toxic dump). Anyway, what is past is past. I talk about those times without much nostalgia (I was a broke teenager at the time, not really the happiest of times). I just rationalize about how things got worse since then. reply nickpsecurity 5 hours agorootparentprev“ And honestly it's hard to cope with all of that, which means that self-hosting is out of the question unless you got a looot of time dealing with bad actors of all kinds” That doesn’t follow from the points you made. What follows is that you have to deal with whatever percentage of bad actors you get. If not, you have to contract someone to handle that part of the job or do the entire job. Plenty of opportunities on those sentences that look nothing like today’s feudalism. For example, I have several sites I self-host on cheap VM’s with lighttpd and BunnyCDN. I can do anything I want with the whole site, including moving suppliers. They have no comments. I have both an email and Facebook messaging if they want to contact me. For comments, the main problem is catching spam or illegal content. That just means a 3rd-party provider needs to see the content, make a decision based on customer’s needs, and customer’s server needs to post the edit they made. Disqus already implemented much of this concept but it could be modified for more owner control. The stronger control some want over comment quality requires more time and controls. There’s tools to help with that. The Lobste.rs’s site had great moderation tools. MetaFilter added a cheap, paid system for account creation that filtered tons of spam. Most implementations just need laborers to enforce their view of social norms with might or might not be easy, and might not be right or worth keeping around either. That leads to countering the last assumption some commenters have: the methods used should keep the sites around as long as Google or Facebook. Most, human activity is temporary. Much has little, long-term value. Many sites will serve their purpose for a specific time. Others might go up and down. These possibilities are fine for non-mission-critical uses. Life will go on. reply mschuster91 5 hours agorootparent> That doesn’t follow from the points you made. What follows is that you have to deal with whatever percentage of bad actors you get. If not, you have to contract someone to handle that part of the job or do the entire job. Plenty of opportunities on those sentences that look nothing like today’s feudalism. Well, the \"eternal september\" problem... back in the '00s you could reasonably run a forum or a blog even if you're some high school kid, all you needed was your parents and 10 bucks a month for some shitty virtuozzo/UML VPS. No need to deal with stuff like setting up a CDN just to survive some random asshat thinking they can DDoS you off the 'net. reply nickpsecurity 1 hour agorootparentYou’re right that the problems increased. Although, I needed a phone line tied up for as long as I was online. I used to DDOS myself. reply Dalewyn 5 hours agorootparentprev>When I was a kid in the 90's, me and my friends were considered the weird bunch for liking videogames, computers, tabletop RPG, etc. Sometime around mid-00s it became mainstream, This does make sense, of course: 1990s->2005-ish is ~15 years, it's 2024 today. The \"weird\" kids became adults and replaced the previous and outgoing generation and their norms. reply pc86 5 hours agorootparentThat's not how it works. If a minority of people like Thing A when they're teenagers that doesn't mean suddenly when they're adults everyone will like Thing A. It just means a minority of adults will like Thing A. Put another way, what do you think happened to all the \"normal\" kids? They would have become adults too, so wouldn't you expect the \"normal\" to replace the previous and outgoing generation rather than this one particular minority? reply Dalewyn 4 hours agorootparent>Put another way, what do you think happened to all the \"normal\" kids? Silent majority. It wouldn't surprise me if most \"normal\" kids simply minded their own \"weird\" business and waited for the winds to shift more in their favour. What is mainstream today was counterculture 20~30 years ago, which coincidentally is about right for generational shifts in trends. reply surgical_fire 4 hours agorootparentNah, in the 90s nerdy kids were definitely the minority, even among kids. What happened is that in early to mid 2000s, careers that nerdy kids flocked to became desirable because they were well paid. To this day I think there is something vaguely amusing regarding the push to get more girls to code, and how it is implied that women don't flock to it as some kind of conspiracy to keep them away from nice jobs or whatever. By all means, I think this push is a good thing. Especially as I have a daughter and I'll certainly teach her the ropes when she is a little older, maybe try to code some silly games with her, that sort of stuff. But in the 90s when I was a kid? Girls were absolutely repeled by anything nerdy. When my group of friends found a girl that had any remote interest in nerdy things, they would fall over one another to try to accommodate her. Fairly pathetic when I remember in hindsight. There was this active desire to feel less as outcasts by having our own tastes validated by someone from the outgroup, that sort of thing. It was a different world. Weird to think that it was a mere 3 decades ago. reply fipar 4 hours agoparentprevI miss the 90s internet and I think even though some things have objectively gotten worse (most people interact in proprietary networks, as opposed to using open standards), as a parent, I think part of that feeling is still there, since my kids are doing some of what I was doing back then, only that instead of irc they're mostly using discord now. The one thing I do believe is legit to miss and not just rose-colored nostalgia lenses is that, back in the 90s, I remember all or the vast majority of what I found only was not tied to profit in any way. I'm not against profit per se, but I do believe you get a very different network when people create content because they want to share something they're interested in as opposed to them trying to make a living out of that. reply hattmall 3 hours agorootparentYeah, half of my Facebook feed is shit that is intentionally wrong so that people will interact with it because they get paid based on engagement. reply intelVISA 9 hours agoparentprevFirefox was good, Electron wasn't a thing and Microsoft didn't own most game studios... we really messed up huh. reply mglz 8 hours agorootparentWell, who can realistically oppose billions of dollars coming to ruin something? reply HackerQED 9 hours agoparentprev> now it's 2024 and we've screwed it all up. Moved to tears. A strong sense of 'How Time Flies'. reply hi_hi 9 hours agoparentprevI agree somewhat. There really was a sense of wonder. Whats coming next? Where will it go? I don't think it's all screwed up though. The difference were seeing is what happens when the marketeers take over (no offence intended) the technologists. Everything has to have a point, be commercialised. Learn to filter that stuff out, and the nerds and geeks are still there, doing interesting things, you just have to fight more to see it. reply ZaoLahma 10 hours agoparentprev> ... to tinker with these promising technologies that were going to change the world for the better and now it's 2024 and we've screwed it all up. Perhaps I'm overly optimistic but considering what we do have, I'd hardly call it a screw up. Far from it. I grew up in the 90s and looking around I'm amazed at what we have. Last week was the first time in 5 years that I physically went to the bank, and it was only due to a rare edge case scenario that their online services (until now) don't cover. Just about all admin in my life is done online. And there's so much tech to tinker with. Raspberry pi, Arduino, PCs, ... Connect it to your mobile device and it just explodes what you can do, if you have the energy and time for it. Fun / nerdy tech is (for the most part) dirt cheap now. Sensors, electric motors, microcontrollers - it's all there readily available for basically nothing. ... and considering the personal tech / mobile devices. I remember interviewing for a job in the biggest city in the country some 16 years ago. Printed paper map, getting paper tickets for the subway, getting lost and almost missing the interview. That's unthinkable nowadays. I'd have the map on my phone and I'd let my mobile phone guide me through the subway, with the ticket on the phone. reply aleph_minus_one 9 hours agorootparent> Printed paper map, getting paper tickets for the subway, getting lost and almost missing the interview. That's unthinkable nowadays. I'd have the map on my phone and I'd let my mobile phone guide me through the subway, with the ticket on the phone. The kind of surveillance that walks hand in hand with this is what hackers of the 90s intended to prevent from happening. reply josephd79 6 hours agoparentprevI miss it too. reply j45 5 hours agoparentprevMaybe folks were waiting for someone else to make the internet better for the many when it’s now that group itself who could. reply gipsies 13 hours agoparentprevIn the future people might think the same about Bitcoin and AI. reply aleph_minus_one 10 hours agorootparent> In the future people might think the same about Bitcoin and AI. Perhaps, but these will be different people than those who miss the 90-00 internet. reply xboxnolifes 15 minutes agorootparentOfc, because it will be the people growing up in the 20s. reply ocean_moist 13 hours agorootparentprev> In the future people might think the same about Bitcoin In 2017 I thought I missed bitcoin, still managed to mine a meager amount on my parents computers. In the modern day I was proven wrong. reply anthk 9 hours agorootparentprevNot even close. No one cares about Bitcoin even today. reply gruturo 5 hours agorootparentprevYeah, we will fondly reminisce about the planet-destroying ponzi scheme which made it so convenient to pay for illegal goods, scams and ransoms to cryptolockers. What a nice unnecessary ecological catastrophe we managed to concoct out of nothing. reply 0points 11 hours agorootparentprevWe already do.. reply j45 11 hours agorootparentprevWon't compare reply darthrupert 13 hours agorootparentprevI already think like that of the first years of Bitcoin. It had the same energy. reply AlexandrB 5 hours agorootparentIt did. I'm not sure AI has this energy. We quickly skipped the \"early tinkering\" phase of AI and jumped straight to the annoying \"let's put this technology into everything\" stage like the blockchain craze of ~2018+. Perhaps the difference is how \"top-down\" AI has been. Most of the push has come from massive companies trying to get people to use it instead of people finding it organically. reply SkyBelow 4 hours agorootparent>We quickly skipped the \"early tinkering\" phase of AI Was it skipped, or was it spread over many decades with AI winters interspersed throughout? reply Saturnial22 13 hours agorootparentprevI miss when \"fed-pegged lightning side chains\" peddled by Blockstream, Luke, Greg et al was the biggest load of buzzwordy self-serving bullshit in the space. reply dav_Oz 12 hours agoprevSurprisingly it is obvious for Gen Z that social media in its current form is highly addictive and destabilizing in terms of well-being because (usually framed as \"mental health\"). Since I'm older I had a more of a choice in terms of social media presence (and get away with basically none) the younger folks practically don't. Basically, I could have got \"hooked\" as my pre-frontal cortex was already fully developed and I kindly declined. Gen Z for the most part was confronted with the \"choice\" of small dopamine hits designed after the newest slot machine research [0][1] when they were underage. As others have pointed out the 90s-00s had its own limitations and frustrations so going back to that nobody is really nostalgic about that part but back then you had to at least choose video games (install it, meet the hardware requirements and get sufficiently proficient in it ;) ) to get to today's level of addiction which permeates mainstream online social interactions. [0]https://ihpi.umich.edu/news/social-media-copies-gambling-met... [1]https://link.springer.com/article/10.1007/s11245-024-10031-0 reply jdthedisciple 9 hours agoparent> Surprisingly it is obvious for Gen Z that social media in its current form is highly addictive and destabilizing in terms of well-being because (usually framed as \"mental health\") Is it? It certainly is obvious to this particular 18 year old, but perhaps he is just above the 99th percentile of his generation in terms of intelligence. Most others seem oblivious to this reality in my observation. reply dav_Oz 7 hours agorootparentGenZ are mostly aware[0] but feel powerless about it so they don't act accordingly which may seem that they are oblivious. From personal experience in a controlled setting (tutoring) if I'm strict about the form: no phone and all learning material prepared beforehand I get mostly positive feedback and some even feel relief for that time. Imo the deeper truth of the matter is that they are used to adults struggling to give them full attention, too, a two-way-street but all the blame is usually given to the younger folk. I find it surprising because it took e.g. smokers a lot longer although the evidence was overwhelming [1] in 1964. Today (almost) every tobacco smoker acknowledges the negative health effects. It is a insidious kind of addiction: a massive amount of very short-lived, small dopamine spikes throughout the day seamlessly incorporated into your \"normal\" functional life which makes it extremely hard to get out of the loop. [0]https://talker.news/2024/08/28/why-3-in-4-gen-z-blame-social... [1]https://onlinelibrary.wiley.com/doi/full/10.1111/add.16007 reply umbra07 8 hours agorootparentprevJust about everyone I know understands that social media is addictive, and can lead to a littany of/exacerbate mental health issues. All my friends constantly joke about being addicted to x social media app reply jajko 7 hours agorootparentprevIt is. Its normalization of failure / suffering. in a village full of alcoholics, drinking with your family/neighbors was part of greeting, social contracts, or venting out frustrations. It was evident to everybody how things end up down the line without exception, but when all are in the suck, mentally it feels better. Our herd social behavior which make humans such a successful species are showing its darker, and easy to abuse side. reply donw 10 hours agoparentprevWe are very careful with our kids in terms of they interact with technology for this reason. Raising luddites that can't type won't serve them well over the course of their lives, but neither will allowing them to become tap-and-scroll dopamine zombies. It's a difficult balance. My strategy will undoubtedly evolve over time, but I suppose it could be summarized as \"permit supportive technology, aggressively deny anything else\" Of particular note is that most, if not all, \"for kids\" content is actively harmful. The key to making things work is having a cohort of parents that have similar priorities. If the parents in your social group default to shutting Junior up with an iPad, you're going to have a bad time. reply scruple 5 hours agorootparent> The key to making things work is having a cohort of parents that have similar priorities. If the parents in your social group default to shutting Junior up with an iPad, you're going to have a bad time. This has been our priority as parents forming peer groups with other parents. But it's very hard to find the kids that your kids like and are friends with who aren't constantly inundated with tech. reply 1over137 4 hours agorootparentprevShutting junior up with iPad seems to be the default. When those parents were kids, they were shut up with television. In a way, nothing’s changed. reply metaltyphoon 2 hours agorootparentThe diversity of what is brings shown has changed. If your tv show wasn’t on then oh well. Meanwhile with an iPad you can pick and choose at an instant. The attention span is ridiculous less using an iPad reply ericd 5 hours agorootparentprevOn the last bit, it seems like more parents are coming around to your way of thinking, and our public school system just sent out a survey about how hard they should ban personal electronics from our public schools. reply bayareateg 4 hours agoprevOne thing that's happening currently that has been alluded to by other people in the comments is the rapid disappearance of forums. Instead of finding a forum dedicated to something you're interested in and getting access to a ton of structured, easily searchable information that has built up over time, things have moved to discord. Discord's primary advantage is real-time communication, which comes at the cost of structured, long-term, and (generally) on topic knowledge. This is not a good trade off, imo. Discord is also a social media platform, which in itself comes with issues common with modern platforms. reply layer8 3 hours agoparentI’m curious what the general demographic preference is regarding real-time/chat vs. async/longer-form. I never much liked real-time/chat, even back in the times of IRC, BSD talk, and ICQ, and have always preferred mailing lists/Usenet/forums. reply pera 8 hours agoprev> There is neocites, and a small community of people who share this philosophy about the web (and that are relatively young), but I have not met anyone my age, in the real world, that would choose to do something like this. When I was a teen at least half of my classmates had some kind of personal website (trends changed very fast back then but for my generation it all started with geocities). The idea of making something unique and sharing it online was very fun. It felt like you had a lot of freedom to do almost anything you wanted. There were no expectations, no standards to follow, nor \"successful\" people to imitate. Probably my nostalgia is distorting my perception here but to me modern internet looks extremely homogeneous: everything seems to come from the same cookie-cutter, and the only degree of freedom you have is to either follow the formula for ranking higher or sink into the algorithmic oblivion. reply mglz 8 hours agoparentWell, the old internet was partially based on people knowing each other. Then the large players made it all about mass engagement. Maybe we should go back to linking your friends webpages from your own and have a small amount of readers. reply bojan 8 hours agorootparentI'd rather say in was based on common interests, and you'd go out of your way to find \"your\" community. I'm still friends with people I \"met\" on various phpBB forums 20 years ago. reply madaxe_again 8 hours agoparentprevWhen I was a teen, most of my peers were not online and were not interested in being online, and actively derided anyone who even touched a computer without it being under duress - U.K., mid to late 90’s. A scattered few, usually those with bands, had a MySpace. I can count the geocities sites on one hand. A big part of what the author is lamenting for, and touches upon with his final paragraph, is the Internet of weirdos, before the cool kids and your mother also got online. It’s gone. September happened. It also still exists, in niches and pockets here and there - but the Wild West days are done. reply illwrks 7 hours agorootparentBingo, you hit the nail on the head. At that point in time my friend group were skaters and rockers with bands - the internet was a connection to that world that was in short supply in the small town we grew up in. reply xnorswap 8 hours agorootparentprevIt's funny how the mind plays tricks with our memories too. I could have sworn myspace was around ~2000/1, but apparently it wasn't founded until 2003. reply johnisgood 8 hours agorootparentTo be honest, a 2 year difference is not much. I could not tell much between 2016 and 2018, for example. reply veunes 8 hours agoparentprevThe web was a diverse and dynamic space. Users were free to experiment with design, content, and interactivity without worrying about conforming to algorithms or best practices. reply bottlepalm 13 hours agoprevAI has me super jealous of how quickly kids can learn things today. Back in the 90s banging you head against the wall trying to get the simplest things in Linux to work, and trying to learn programming from a book with minimal resources online. It was 'hard mode'. I'm glad I was there to appreciate what we have today, but not sure I'd want to go back to that.. though I would like to play some Quake deathmatch again on a populated server.. There are still lots of underground nooks and crannies of the internet today, arguably more than ever. It's what you make of it, and where you choose to spend your time. reply bigstrat2003 13 hours agoparent> I'm glad I was there to appreciate what we have today, but not sure I'd want to go back to that.. I would. Having the answer handed to you doesn't actually teach you much. It's the struggle to figure out the answer that makes it stick. The kids today who can just get easy answers from AI aren't going to have anywhere near the skills we do. reply nyarlathotep_ 41 minutes agorootparentDunno, I never saw the value of learning some esoteric piece of language/library/sysadmin trivia I'd use once or twice after spending hours poking at things trying to figure it out. reply bottlepalm 12 hours agorootparentprevThe problem is you struggle to hack together a sub optimal solution because you don't have the resources to figure out anything better. It didn't make me better, it was a waste of time. Remember experts exchange? Uhg.. Today I can quickly be productive in unfamiliar domains, and learn while asking questions to an AI that is available 24/7, and and has an extremely deep knowledge of so many things. Personally even as an experienced programmer I have learned so much in the last year, greatly accelerated by AI. Kids are going to be better off for it, and with it will achieve incredible things. reply CalRobert 9 hours agorootparentHell, I'm old and had been meaning to learn React for years but never really stuck with it (I have things to do). With GPT it was pretty easy. I still read the docs, but I don't get stuck for hours on syntax minutiae and the like. reply aleph_minus_one 10 hours agorootparentprev> Today I can quickly be productive in unfamiliar domains Today, you can quickly get a shallow, superficial understanding of the domain that might impress some people who know barely anything about the respective domain. > and learn while asking questions to an AI that [...] has an extremely deep knowledge of so many things. ROFL reply anthk 9 hours agorootparentprev>better Bullshit. I've seen blind idiots kids pasting Solaris commands into Linux servers, without having any clue of what were they doing. Kids are going to be really screwed when the AI output converges into more Markov-chains like bullshit as it's being self-feeding with it's own output, creating something like a big but pompous Megahal/Hailo clone. If any, we the older Millenials are trained to do hard tasks with just the manuals sitting on a table. The rest will be clueless. reply Philpax 5 hours agorootparent> Kids are going to be really screwed when the AI output converges into more Markov-chains like bullshit as it's being self-feeding with it's own output, creating something like a big but pompous Megahal/Hailo clone. This isn't going to happen. The literature on model collapse suggests it occurs when your model is fed on majority synthetic data, which is not how anyone is training models. Even if they were, do you think they're going to ship something that performs noticeably worse than its predecessor? reply anthk 3 hours agorootparentYou said it. Traning. But later, most of the input of data it's being inputted back from AI's output. Because the end users will use far more the output data from AI than feeding it from remote sources far from the original. Disasters will happen, just wait. reply hattmall 3 hours agorootparent> do you think they're going to ship something that performs noticeably worse than its predecessor? This happens all the time. Once the first gen devs are off, the releases tend to alternate. A decline from the existing version and then an improvement that mostly fixes what they made worse in the prior release. e.g. Windows XP -> Vista -> 7 -> 8 -> 10 The nature of AI though may make it more difficult to distinguish the failings immediately which could result in an irreversible inflection point. reply sirsinsalot 10 hours agoparentprevI cried my eyes out when I was about 8 years old and finally got my Unix install connected to the Internet with only the man pages to help. reply steve_adams_86 4 hours agoparentprevI’m so glad I lived through hard mode. I have a kind of perseverance and resilience in the face of difficult learning challenges that my teenagers are hardly developing (despite my best efforts), and it has made my life so much richer. reply ggambetta 8 hours agoparentprevI left Windows for Linux in 1999 and I do NOT miss how difficult everything was to get working, even with the reasonably user-friendly Red Hat (5? 6?). I even became the maintainer of a Linmodem driver by accident (it was abandoned, the author was uncontactable, and I needed the modem to dial). I'll take present-day Ubuntu any day. I install it on my desktop, and it just works, GPU and all. I install it on my weird laptop with a touchscreen that swivels 360 degrees and can rotate to a portrait desktop, and it also just works. reply ocean_moist 13 hours agoparentprevThis is true. I am lucky to have learned as much as I have by this point. My Dad reminds me almost daily how lucky I am. There is something to be said about doing things \"the hard way\" and I think AI is, in the short term, going to decrease the amount of medium-skilled software developers out there. reply bregma 8 hours agorootparentJust think: AI is being used to train doctors too. reply system2 13 hours agoparentprevHey, you can still play Q2DM at tastyspleen.net Not as glamorous as before but there are still players every day. http://tastyspleen.net/quake/servers/list.cgi reply paulpauper 13 hours agoparentprevyeah but it also meant that the job market may have been easier, lower barriers to entry reply rainingmonkey 6 hours agoprevThe vibes of the early internet are still out there, you just won't be directed there by Google or any of the other \"social\" silos. Gemini (https://geminiprotocol.net/) is almost entirely made up of personal blogs where you can email the author and get a response. Perhaps Gemini is in its early days, like the web used to be, but maybe the format (NO styling whatsoever) is inherently resistant to commercialisation and commodification. reply NikkiA 3 hours agoparentGemini to me feels to be dying, every time I open lagrange there's another 10% or so of the default bookmarks that are perma-offline; and the sites like geminispace.info have fewer and fewer results reply kazcaptain 13 hours agoprevDial-up connection, ICQ, and 10mb files left to be downloaded through the night were my connection to the world. I so badly wanted that future of the Internet, but somehow we ended up in a place where corporations ate it all. Is it nostalgia, or is there something more? Who knows at this point. I’m 30. reply cutthegrass2 10 hours agoparentI've been thinking about this too, perhaps it's my age (43) and the fact I vividly remember earlier times. If I were to pick an inflection point, a point at which the internet started going to shit, i'd say it was around 07/08 with the birth of the iPhone and Appstore. That's when \"pay to publish\" really started to take off. reply aleph_minus_one 9 hours agorootparent> If I were to pick an inflection point, a point at which the internet started going to shit, i'd say it was around 07/08 with the birth of the iPhone and Appstore. That's when \"pay to publish\" really started to take off. That's very plausible. I additionally want to add that before the iPhone, having a locked-down device where the vendor decides which app(lication)s you are allowed to install caused huge outcries and shitstorms. Example: Microsoft's initiatives for \"Next-Generation Secure Computing Base\" (formerly Palladium) [1] and attempting to enforce a TPM on computers (keyword: trusted computing). When the iPhone came out, this all suddenly became perfectly accepted. [1] https://en.wikipedia.org/wiki/Next-Generation_Secure_Computi... reply user3939382 9 hours agorootparentI don’t know if Palm was technically a walled garden, I’m guessing you could load from wherever, but practically it was and I don’t think anyone had a problem with it. Not sure if it’s a counterpoint but something to consider. reply aleph_minus_one 8 hours agorootparent> I don’t know if Palm was technically a walled garden, I’m guessing you could load from wherever, but practically it was According to this Reddit thread [1], you could easily install applications to a Palm from a memory stick. Additionally, I am not aware that Palm applications needed to be signed by the device producer (i.e. the device producer could not decide which applications are allowed vs forbidden on the device). [1] https://www.reddit.com/r/Palm/comments/tmvm9z/is_there_a_way... reply marklubi 8 hours agorootparentprevI'm 45 and have been in the industry for more than 25 years. I think it was closer to 2010/11 when things went sideways. The birth of the iPhone changed a lot of things, but it took a few years to reach critical mass. reply Clubber 7 hours agorootparentFor me it was around this time, but 2013 when the Snowden revelations came out. The internet became creepy because of all the spying and collection. It was a distinct change in my attitude I remember distinctly. reply Foobar8568 9 hours agorootparentprevFor me, it was the buy out of geocities by Yahoo was the inflexion point with a sure and slow demise. Death of ezboard was also a pain. So corporate profit over users. Another acceleration was google turning to shit as well. reply Izkata 5 hours agorootparentprevOne of the possible names for Gen Z that was thrown around about a decade ago was \"iGen\", a reference to the iPhone and noticing there was a cultural shift in those that came of age on/after 2008. reply BoingBoomTschak 9 hours agorootparentprevSome people pointed to 2007 for lots of simultaneous reasons: http://0x0.st/Xx1H.png reply DanielleMolloy 5 hours agorootparentNot sure how it was 2007, but tumblr feels like relief nowadays because it still has some glimpses of that old internet. You can customize a tumblr down to the HTML, including JavaScript. Not using the social parts (likes) etc. though. I'm using it as a way to share photos and what I'm up to with family without forced login. Like a homepage basically. It is not trying to distract visitors with pointing to other blogs either. reply TheAceOfHearts 8 hours agorootparentprevThis is interesting to me because I vividly remember the Summer of 2006 as being one of the most fun times I had on the Internet. I wasn't aware of these details but probably would've also said around 2007 or 2008 there was a vibe shift. reply JohnMakin 13 hours agoparentprevI’m not that much crazy older than you, but I do remember well being a teenager at the peak of the dotcom craze, being a heavy internet user, and even back then having a vague feeling that the ad riddled, desperately monetized (and shitty) websites built on the back of mostly investment capital run amok felt a little shitty as an end user. It’s hazy now and lost to time (the internet is not actually forever it’s like ~10 years old at most now) but i vividly remember pages that became harder and harder to navigate because of invasive ads, and sites that’d somehow embed malware on your computer that’d spam you with weird porn popups when your parents used the machine - it all feels vaguely similar to now, albeit much sleeker. Adware in my opinion is bordering on malware to the point I find the definitions indistinguishable. It collapsed then for good reason, and IMHO similar conditions as to now. I don’t want to live through that as a fully grown adult with a tech career now, and it worries me a lot. What arose from the ashes of that bubble event became great so maybe a reset is needed, but for me personally, it’d be a disaster. reply echelon_musk 11 hours agorootparent> the internet is not actually forever it’s like ~10 years old at most now The internet has existed for longer than the web which is already 30 yrs at this point. Not sure I catch your point that it's 10 yrs old. reply Izkata 5 hours agorootparentThey're talking about a very similar idea as the article, that the early internet is disappearing as everyone moves to new stuff. reply RGamma 11 hours agorootparentprev> it’s like ~10 years old at most Wikipedia is from 2001, Archive from 1996. IRC still exists as do retro games or the demo scene. There's still some of that good stuff around, but you can't really imagine it being founded today, at least not with the same cultural enthusiasm. Such an optimistic time... reply dmead 12 hours agoparentprevHow old were you? I'm 41 and I also miss that stuff. reply kazcaptain 10 minutes agorootparentI was a child really so it’s not simple to differentiate between nostalgia and “real” world for me. reply johnisgood 7 hours agorootparentprevI'm in my very late 20s and I do miss this, too. I used MSN more than ICQ, however. I still use IRC just like I did when I was ~13 years old, without much knowledge of English. reply G3rn0ti 13 hours agoparentprev> I so badly wanted that future of the Internet, but somehow we ended up in a place where corporations ate it all. The high bandwidths we got today are exactly the result of a full commercialization of the Internet. If there was no money to earn here, we would still be stuck with dial-up connections and had no YouTube and Netflix. I understand that advertisements and online tracking suck but it’s the result of consumers not willing to pay a penny for many online services. But I think that’s already changing with all those subscription models and SaaS businesses out there. reply JohnMakin 13 hours agorootparentBlaming consumers for the state of things and for “not spending money” is a common refrain, and honestly is gaslighting and revisionist. The internet functioned fine for over a decade, profitably, without invasive adware like we see now, which is to the point of total degradation of the core service itself - this is not the cause of consumers, but rather the current mindset of the wall street landscape. As far as “not willing to pay for it” what do you call subscription models of popular language models like chatGPT? I would pay an embarrassing amount for a google that worked like google did 10 years ago. That isn’t my fault that product doesn’t exist anymore, the demand is there. This angry tone isn’t directed at you, I just find it so frustrating that people believe it’s such a binary choice. Google had the literal monopoly on tech talent and knowledge for 20 years and decided to divert that into the most cannibalistic and predatory business model around. Can you imagine had they directed the same efforts to making an actual competitor to AWS? I am speaking as a career cloud infra guy and business owner running on cloud - as much as I hate MS products I’d sooner migrate to azure than ever spend a penny on a google cloud product for anything more critical than running the office coffee maker. That, I think, was a tremendously bad decision for the internet, and the decision absolutely was not binary. Make a good product people find useful and people pay for it. That’s how the market has worked for all of human history, there’s nothing different about the internet. reply G3rn0ti 5 hours agorootparent> The internet functioned fine for over a decade, profitably, without invasive adware like we see now What time period are you even talking about? In the mid nineties the Internet was largely a research network between university computers and paid for by tax payers. The internet only started growing exponentially with commercial services appearing in the late nineties. This was the time when people started to demand high speed Internet connections (ASDL) and were willing to pay for that. But this was not the case for its services. Google's primary business model way back in 2000 was already showing ads related to your search terms. Even back in 2003 Google introduced the free GMAIL service that showed ads based on your email content. That's 21 years ago. IMHO it is your view on the Internet history that is being \"revisionist\" and \"gaslighting\". Take this from sb who had his private Internet access as early as 1998. reply jmwilson 1 hour agorootparent> In the mid nineties the Internet was largely a research network between university computers and paid for by tax payers. Your timeline is off by at least half a decade, and things were changing very rapidly in that time. By the mid90s, NSFNET was formally dead, after years of accepting commercial traffic. Local ISPs for home users started popping up and AOL opened its access to USENET in 1993. The push for residential broadband also started almost immediately; @Home was offering residential cable internet in 1996, it was the future at the time, just not very evenly distributed. This was well before PCs had the processing power to do standard-definition video. reply JohnMakin 4 hours agorootparentprev> Google's primary business model way back in 2000 was already showing ads related to your search terms. Even back in 2003 Google introduced the free GMAIL service that showed ads based on your email content. That's 21 years ago. The adtech of today is nowhere near as invasive or pervasive as it was then - this is an extremely dishonest or ignorant framing of what is happening in today's internet vs the one of yesterday. There are plenty examples of paid-for subscription services on the internet doing profitably without jamming adware/malware down your throat. Would you like examples? reply lomase 9 hours agorootparentprevI have paid maybe 10k in my lifetime to be connected to internet. That is why the telecom company has been able to invest in infra. I don't have good fiber thanks to Netflix. reply femto 12 hours agoprevI was on the 'net pre-WWW and wrote my first web page in 1992. The thing that strikes me in hindsight is the hope everyone had. I looked forward to the things the Internet might enable. I looked forward to whatever was going to replace my slow dial-up connection. I looked forward to an always-on connection, so I could run my own servers. I do think the solution is not to look back wistfully at what was, as that's not the path to hope. Restore the hope by ignoring the noise (such as social media) and looking forward to what interesting things might be, as that was the essence of the early 'net. reply ffsm8 12 hours agoparentI joined in 2003, so I'm probably around 10 yrs younger (born 88). From my perspective, the only thing that changed is that most millennials grew up and realized that their quality of life mostly peaked in their teenage years and that most will never be able to provide even a fraction of that quality of life to their offspring. It's honestly not really about the Internet itself, that's just a place where the same people ultimately communicate on. If they're hopeful in real life, they're gonna be hopeful on forums etc. I make this statement under the expectation that millennials are even now the biggest fraction of Internet users. Though I'd expect that to change within the next few years. I doubt the sentiment would change however, as the zoomers/Gen alpha will ultimately come to the same conclusions, as their prospects are even worse reply nathias 10 hours agoparentprevFor me, that hope was realized, we have access to most of the books in existence, incredible projects of knowledge sharing, and countless other resources to learn any subject you want. If the majority of people don't want to use this, but instead just want to yell about politics and the news cycle, that's their problem and doesn't really diminish the achievements of the web. reply chaosist 8 hours agorootparentExactly. I got online in 1995 and the promise was kept by the internet. I have taken so many free classes from Ivy league schools along with all the books, pdfs, tutorials, physical books I would have never found otherwise. What I highly overestimated was people's thirst for knowledge. I really thought that by this time I would be unemployable as the average young person would just be so learned that I wouldn't be able to keep up. The advantage of growing up with the internet would be just so huge. I would have never guessed that the average young person instead almost has a type of learning disability from being addicted to political nonsense, stupid videos and gossip. Everything is there from what I envisioned in 1995 though. It is just this useless, pernicious aspect dwarfs what I envisioned then in terms of popularity. reply aleph_minus_one 9 hours agorootparentprev> For me, that hope was realized, we have access to most of the books in existence As long as you are willing to enter a legal gray area, you can get access to some interesting books; but these are still an insanely small fraction of \"most of the books in existence\". reply johnisgood 7 hours agorootparentLibgen is great. :P reply pessimizer 4 hours agorootparentprevLooking up arbitrary ebooks from my library of a couple thousand physical books (accumulated over 40 years from obscure places) on shadow libraries, I get something like a 90% hit rate. They're almost all English, however, but there are enough nonenglish illicit sources to make me think that \"most\" is probably right on the money. Also, tbh, from the 10% I can't find maybe 1/5 of those is worth reading. Unfindable stuff tends to be the dregs, although plenty of the dregs are also findable. Recently got into late 19th century Mexican literature, and I find virtually everything I look for. I can find books from small political presses who may have printed only 100 copies. Here's another kink that I have, which basically would have been my most self-indulgent dream as a child: I can look at the ads for other books that are in the backs of 50-200 year-old books, or listings of the rest of the books in a series, and find those books instantly. Between shadow libraries, actual libraries, hobbyist public domain wranglers, etc, it's hard not to find a book. reply anal_reactor 11 hours agoparentprev> The thing that strikes me in hindsight is the hope everyone had. Precisely this. I grew up in a post-communist country and I can definitely say that the democratic transformation gave everyone a sense of hope. Sure, there was the ozone hole, but other than that, when people thought \"the future\" they imagined all the technological advancement that would not only make life comfortable, but also solve most of social issues. And then we saw the opposite happening. reply shakna 13 hours agoprevI'll go against the grain, here. I do miss a simpler time. I run some very modern hardware, with modern games and hardware. But I also play around with my GameBoy, maintain a DOS VM for writing my novels, reach for Lynx as often as Firefox. Simplicity isn't just an aesthetic though. I find it to be a requirement just for getting through life. I shouldnt need adblockers and popup blockers and consent optouts just to view a site. They're worse than the era of iframe popups. Granted, I am prone to overstimulation and get seizures when it happens. But that just reinforces what I already want. A space to enjoy what little life I've got. reply bugthe0ry 12 hours agoprevWhenever I come across posts like this, I need to remind myself that it's actually a very small minority of people online who feel this way. Most people are perfectly happy with how the internet is right now and don't care for going back to things you used to do online a decade or two ago. On social media: today it's no longer even about ordinary individuals sharing their ordinary lives online like they used to 15 years ago. It's about consuming content around your interests (which includes entertainment). Very few people on my instagram make personal posts anymore (myself included) and when they do, they're few and far in between. It's all brands and content creators. I also think people's mentality has shifted and they no longer care about sharing their lives online. It was a new concept to many between 2006-2014 to be able to do it, and so many did. Now they're past it. It was interesting in 2008 to see that Johnny is currently sipping on a latte in front of his window. Now nobody gives a fuck unless Johnny is a celebrity. On scrolling TikTok-like feeds: not to glorify the concept but it makes it easier for the anti-social-medias to accept if you think of it as the equivalent of when your parents would get home, turn on the TV and start flipping through the channels trying to find something interesting to watch. TikTok and Reels put the TV in your pocket. 80% of the content on them is of no substance but occasionally a valuable post does come up and provide me with something I need/enjoy. The other day, I had a spontaneous date night because a reel showed up for my wife about some food truck nearby and she shared it with me (we went there an hour later and it turned out to be a good one - we're going again next week). P.S. it didn't even cross my mind to bother posting a snap of my meal as a story on IG :) reply ocean_moist 12 hours agoparent> I also think people's mentality has shifted and they no longer care about sharing their lives online. I tend to disagree. I have a few hundred friends/people I follow on Instagram and I see people post stories and posts about their life all the time. People are generally trying to portray \"Look at me! I have such a cool life!\". This is how it is for young people. As for TikTok, I find myself scrolling with no end in sight. I think the studies on the addiction level of shortform content and its long term effects is going to be extremely shocking. I can watch an episode or two of a show, maybe 30m-1hr, and turn it off. But TikTok keeps me constantly stimulated and I can easily forget what I am watching and \"doomscroll\" for hours. With TV I make the conscious decision to watch it. With TikTok I have to make the conscious decision to not watch it or open the app. This may be a side effect of how my brain works, but a lot of young people have similar experiences. I think the main difference in our experiences is a generational gap in how social media is used. reply sureglymop 8 hours agorootparentBut why wouldn't the solution with TikTok just be to delete your account and the app? I am young like you and that works perfectly fine for me. If friends want to share videos/tiktoks with me, they download them and then send them with another messaging app so I don't have to go to the platform. And that's not something I force on them but something they do because they really want me to see those videos I suppose. reply ocean_moist 2 hours agorootparentYeah, I find myself redownloading it a lot though. I think I did finally just stop a few weeks ago, but I still have Instagram reels. reply tarsinge 12 hours agoparentprevAnd what is the content people scroll to? And how does one become a celebrity in the first place on IG or TikTok? And what teens do all day long? Yep, sharing their live. I share your experience, but for me it is just the one of a millennial getting older. reply bubblebeard 11 hours agoprevIt’s refreshing to see a young person with such sound reflections about social media. This post made me reminisce about learning web development when I was 11. Finding resources online was tricky, especially since I didn’t always have access to the Internet. I got books from my parents, and not very good ones either, but I loved them all the same. I miss those days, when everything was more of an adventure. reply h6do84go83b83 20 minutes agoprevNostalgia can be a “now” killer. careful with it. reply INTPenis 13 hours agoprevThe only thing I miss from the 90s-00s was the fun we had on IRC. And that was mostly due to our carelessness and youth, so I think I'm good in this future. The funny thing is I'm still on IRC, with about the same size channel, but everyone is \"new\" and we're all old. I don't have any contact with the peeps from IRC back then. reply Gigachad 12 hours agoparentI was a big user of IRC in the past, owned some channels that had thousands of daily messages, IRC may be mostly gone, but the experience and vibe is still as alive as ever on other IM apps. They all essentially work the same as IRC did but with modern bells and mobile support. reply meiraleal 10 hours agorootparentIt is not the same. For me the magic of IRC was to not be online all time. People needed to actively connect and join the channels every day reply Gigachad 10 hours agorootparentYou still can use it that way. I don't read the backlogs of large chats. I just talk to the people actively there in the moment. It's pretty much the same as it was but you don't get the spam of \"Hey, anyone online?\" on low activity groups like you used to. reply jhy 13 hours agoparentprevWhat IRC network(s) do you like nowadays? reply INTPenis 12 hours agorootparentI'm on a private one with local friends that I met through hacker spaces. The rest like indymedia, libera, oftc are mostly to stay updated on tech and ask questions about tech. reply alex1138 13 hours agoprevLet's be honest, one reason many of us loved it was because of Flash. Thankfully there's Ruffle for that, now Also, it would be helpful if Google hadn't steadily degraded their product. +words +that +must +be +included +should +have +those +words +in +the +search. Not \"fuzzy search\". Not \"can't find my result even when I explicitly tell you\". Which includes surfacing of more obscure sites (like what wiby/marginalia do now). (Which also includes censorship but that's a different story) reply whywhywhywhy 5 hours agoparentDon’t know if it’s sad or sickening to see Google pretend it only has 10 results for a query when we all know every single query got 10k-1m results before they nerfed it reply voidfunc 13 hours agoprevNerdy teen laments being born too late. I remember when I said I regret not being born in the 70s because I missed the 80s. Oh before that it was the 60s and I missed all those Dad Rock err Classic Rock acts. Live your life, not the one you think you should have had. reply ocean_moist 13 hours agoparentI generally like being born in 2006. Before iPad kids and such. We had the Wii and other nice things. I am also at the age where I can take advantage of the shift towards AI and related technological advancements. I talk about this in the second sentence. I just think the internet is much worse than it was. reply voidfunc 13 hours agorootparentThe internet is different, I hesitate to say worse. Yes advertising is constant, but it's not like the old internet was free of it. We had banners and bullshit galore. The monetary model has changed for sure but that was inevitable. The 90s internet was slow. Stuff like IRC and email are a product of their time. Stuff was designed to work on 28 or 56k modems. The lucky few had ISDN. I don't miss the old internet that much. I miss some of the apps and the stripped down low bandwidth UX of stuff. What I really miss is th era of native desktop applications that had a consistent look and feel. reply muwtyhg 13 hours agorootparent> We had banners and bullshit galore. The problem now is not the visible aspect of advertising on the internet. It's the obsessive tracking and categorization of people to make advertising more effective that I'm upset with. There was no such apparatus in the 90s/00s. Most legitimate websites wouldn't be plastered with banner spam, but all legitimate websites now will attempt to track you. reply paulpauper 13 hours agorootparentprevbanners are way preferrable to the annoying video ads or ads that pretend to appear as content. reply stackghost 13 hours agorootparentprev>I just think the internet is much worse than it was. It's definitely worse than it was, in some ways. In other ways it's better. TLS is everywhere now. uBlock origin exists. The bar for pwning web apps is much higher today because we learned collectively from our mistakes/our fun times defacing PHP-Nuke sites. But I miss pirating warez on IRC and scoring game rips from FTP topsites, and LAN parties. Facebook and Twitter were really fun in the early years, but these days Big Social really sucks and \"growth focus\" in tech is exhausting. reply doublerabbit 8 hours agorootparent> TLS is everywhere now. TLS is a racket. The protocol is important, it's required, sure. The companies who issue the certificates however have been printing money ever since SSL came to exist. Each certificate issued is a money note. LetsEncrypt only exists now and only because the internet is an heap of security mess. And caused by the same SSL certificate companies. Domains are too. reply Swizec 13 hours agorootparentprev> I generally like being born in 2006. Before iPad kids and such I was a high school senior in 2006. Growing up with computers becoming exponentially faster and better every 6 months was freakin’ amazing and I’m happy to have seen it. > I am also at the age where I can take advantage of the shift towards AI and related technological advancements Me too! This shift comes every few years. The first successful mass deployment of modern AI technology was in the 1970’s when the post office started using computer vision to route mail. The shift to AI that I got to see was Google. That was amazing. Big fan of the current LLM shift too, of course, but it feels less magical than Google did. A little because I understand it better, a little because it benefits me less personally than Google did. Feels more like a marginal improvement whereas Google felt like omg everything is different now the olds are so behind the times they can’t even comprehend!! But I think it’s more that the big shift happens with your newfound ability to grok things, leverage information, and get shit done in your late teens / early 20’s than the actual technology available. Whatever new tech happens around that age will feel like a profound hugely impactful change in the world (but it’s actually you, not the tech). > I just think the internet is much worse than it was It is. And also it isn’t. Depends where you go and who you talk to. reply Rinzler89 13 hours agoparentprevI don't wish I was born earlier. I wish I was born later to have had access to the much better mental, dental and healthcare in my childhood than what was available back then. reply runjake 13 hours agoparentprevI was born in the early 70s and lament missing the computers of the 60s and the genesis of usable UNIX. So it goes. reply tdeck 13 hours agoparentprevThe second sentence of TFA: \"I am not one of those people who think they were “born in the wrong decade”, I think I was born at the perfect time to take advantage of superlinearly growing technological advancements.\" reply ranger_danger 13 hours agoparentprevHilarious given the second sentence is: I am not one of those people who think they were “born in the wrong decade” reply zorrolovsky 13 hours agorootparentExactly... the whole point of the post is to elaborate on the problems of the modern internet (addiction, fakeness, shallowness...). It's not a subjective nostalgic rant but a good analysis of everything that's wrong with today's internet. reply nutshell89 7 hours agoprevI think what's missing from a lot of these discussions is how much more commerce-driven the present day internet is over the 90s-00s. Social media is highly addictive and destabilizing in order to get it's audience to eventually pay for something in TikTok Shop, or view sponsored content, for example. Dark patterns were introduced to increase revenue or to get users to dole out their personal information for advertising effectiveness. I personally think that these sorts of changes were inevitable, especially since the development of internet-native payments infrastructure lagged (and continues to lag) the development of web technologies, as well as humanity spending more of our time on the internet — if the society revolves around accumulation and transfer of capital, the internet would eventually change to facilitate trade reply 7222aafdcf68cfe 8 hours agoprevI have been on the internet since the mid-90s and computers have defined my life , personally and professionally, since I was 4 years old. And still, this kind of messages make me uncertain about what to really think. There is a nostalgia for the authenticity of the earlier web, and I have felt that myself. Things were hard in the 90s, but maybe that is what made it feel more worthwhile. It is also true that a lot of the current internet is dominated by financial and corporate interests. BUT ! everything that was possible in the 90s is still possible today. Even more so. Access to technology and software has never been easier, neither has the opportunity to learn about pretty much any topic. We have access to the output of so many people in an instant. This can be liberating, but it can also be paralyzing. An ugly mix of FOMO and impostor syndrome making many of us paralized on most days and scroll for a quick dopamine hit instead. But what if we consciously choose to focus on the positives by focusing on what we feel truly engaged by, and to ruthlessly ignore the rest ? We can only make that choice ourselves. reply aucisson_masque 8 hours agoparentThis forum is literally the proof that old web still exists, its ugly but efficient. People there aren't chasing for social recognition or karma and genuinely want to share their opinion. And back then website were still chasing for advertising money, the popups and ads were absolute cancer. More than today. IMO the tool (internet) hasn't changed, it's the people using it who changed for the worst. reply johnisgood 8 hours agorootparent[REDACTED] reply tim333 7 hours agorootparentExample? I see little woke stuff or politics in general here. reply Al-Khwarizmi 7 hours agoparentprev> everything that was possible in the 90s is still possible today. Even more so. This is only true in a narrow technical sense. But the Internet is not only the technology, it's its users. Many social interactions that were possible in the 90s are not possible today because, even if the infrastructure that facilitated them is still there or can be built, the people are not there. reply veunes 8 hours agoparentprevYou've witnessed the internet’s dramatic transformation and I think that it's one of a kind experience reply tracerbulletx 2 hours agoprevI've been scratching this itch by getting into https://atproto.com/ and https://activitypub.rocks/ the platforms are much smaller than the mainstream ones, but the older internet was smaller too. I'm not sure you CAN have something like the old internet above a certain user penetration, it will always devolve to grocery checkout tabloids and afternoon talk show without a filter. reply mexicocitinluez 4 hours agoprev> born in 2006 not to be pedantic, but how do you miss something you never experienced to begin with? reply atlantic 4 hours agoparentWhen you miss something, you're not referring to the original experience, but to a memory of an experience. Memories being mental artifacts, it's perfectly feasible to miss an imagined experience. Hence the current wave of nostalgia for the Roman empire. reply vegadw 4 hours agoparentprevI, born in '98, wrote something about being \"nostalgic\" for the 80's : https://opguides.info/posts/xx80/ reply debugnik 4 hours agoparentprevI read miss here as \"failed to be on time for\", not as \"long for\". They specifically say \"I wish I was around when [...]\" reply maurits 10 hours agoprevI think I just miss being 14-ish when technology felt mesmerizing, and world seemed infinite because I was learning Pascal. Now I just want to wake up, have my coffee, and not read about some new machine learning framework. reply beloch 13 hours agoprevSome random observations: 1. Relatively few people were on the internet before midway through the 90's. BBS's were probably of greater interest in the early 90's. If you had a specific interest, there might just nothing online about it yet. 2. Dial-up sucked. It was slow, not terribly reliable, and it monopolized your phone-line. Many can probably remember dialing into their university and hanging up 10 times until they finally got a faster modem on the other end. (You could tell how fast a modem was by its handshaking sounds.) A lot of people first experienced the internet on university dialup, because home service wasn't there yet or was really expensive. 3. The late-90's internet was sometimes very difficult to navigate. Search engines generally sucked. Even if you had their inflexible syntax correct and had perfect search terms, their indexing was often just not up to the task. 4. Protocols were heavily balkanized. HTML and WWW were not yet dominant. There were other things too, like gopher. Gopher had it's own search engines... that sucked. 5. People actually used usenet to have discussions. Usenet really was better in the late 90's. There were enough people using it that you could learn some really interesting stuff, but it hadn't been rendered unusable by bots, spam, and copyright trolls yet. It was like reddit, but way geekier and far less comprehensive. 6. Chatting with people in real-time was a thing. Imagine discord, but text-only. You guessed it, that was it's own protocol :IRC. 7. In general, everything was splintered and needed it's own programs. You could talk to other people in a dozen different ways, and they all had their own protocols and programs. Nothing was truly dominant. Many here can probably still remember their ICQ number. 8. A lot of the awesome stuff we take for granted now just wasn't there back then. Wikipedia was not a thing. If you wanted info on anything local like restaurants, etc., you could just forget about it. Multimedia was rudimentary as heck because even just adding one 60 kilobyte image to your site would add half a minute to the load time for users on a relatively fast modem, and much more for those that weren't. Text was king! 9. Malicious code was truly hazardous back then. Browsers of the day were like natives of the Americas before smallpox arrived. They had no immunity at all. By the late 90's you could really F' up royally if you weren't careful. reply 8bitsrule 9 hours agoparentEvery now and then as I find a piece of scarce, non-trivial information on the net in a minute or 5, I stop to think about how long that would have taken 30 years ago. If I did have some appropriate physical reference books to look in, it might still take 15 minutes to an hour. But if not, I'd have to travel to a library (weather permitting) while/if it was open, find the appropriate section of books, read through the index or chapter titles, etc. and scan - in hopes the answer was there. Today, in some spheres (except e.g. those where the answers are still hidden) I'm much -much- freer to ask many more questions and deep-dive into subjects that, back then, I could only dream of learning about. Yes, I could read a book, if I knew the book existed. And where to order it from. And wait for it to arrive in 2 to 6 weeks. If only I'd had this 30 years ago. Hell, I imagine, what if I'd had this power as a child in a small town. reply gmuslera 6 hours agoprevIt is not just technology, but people, and culture, and mindsets, and more. I started in the local BBS arena and from there all the stages till now. And even something similar from the technological side is done, catching a critical, and big enough, and diverse community may be something short lived, as we live with the rest of internet, that is trying to grab our attention and set our agendas reply mark336 9 hours agoprevOne of the things you didn't mention from back then was P2P file-sharing. I don't know when the RIAA won, but suddenly downloading mp3s was illegal. And now kids think that paying for iTunes songs was always normal. reply haunter 9 hours agoparent> And now kids think that paying for iTunes songs was always normal. Maybe paying for Spotify which is dirt cheap and satisfies what most people are looking for What makes you think piracy is worse nowadays? We have more legal obtions but piracy is absolutely booming with high quality rips. Young me would have killed to have the whole Nintendo ROM library a click away and now I can just download it in 2 minutes. High quality: I don’t miss the old days of searching for music > no ID3 tag just file names > turns out my wholenight download was a peruvian pipe band not the one I was looking for. Every single torrent tracker has now better in quality options and absolutely no fakes reply KingOfCoders 13 hours agoprevBeen there, got on the internet ~90 (pre-www), my first web page ~93, first web coder job ~95. I do think it is overglorified (the web-part) - 80s BBS were more of a wild world with excitement to me. What I did love though was getting on IRC, sitting in front of an amber VAX terminal late at night till morning, talking to people across the world. Reading NEWS and discussing things with people around the globe. What an eye opener for me. reply mglz 5 hours agoprevIf we want back the old internet we must build it. Create your own website, link to those you like, get in contact with the people running them. Mine is mglz.de, check out the link collection and get in touch about linking between sites :) reply aproductguy 5 hours agoprevReminds me of this article from Salon published back in 2002: https://www.salon.com/2002/05/31/back_in_the_day/ reply AndrewSChapman 5 hours agoprevA better version of this article: https://vhsoverdrive.neocities.org/essays/oldweb reply dlevine 13 hours agoprevI lived through that time (was in high school and college). It was pretty neat to read about the Internet and web browsers in Newsweek, but we didn’t have it at home for a few years. Then it was just a 28.8 dial up connection that dropped randomly. It wasn’t until I got to university that I had a real broadband connection, and even that was slow compared to what I get on my phone today. Programming (for me) was mostly basic and a bit of C, although I never totally figured out pointers. By the time I got to university, we had early versions of Java and HTML. That time was fun, but right now is fun in different ways. The tools are much better. Computing is massively cheaper. Laptops weren’t nearly as good as desktops until the early 2000s. Also, you couldn’t do much with AI back then. reply 0dayz 13 hours agoprevThe only thing that sucked about 00s web was the tech. Compared to today, I really wish that you could've had the creativity and general attitude of the 00s web with the tech of today. But I guess this is why I don't regret spending so much time on the 00s web. reply almost_usual 13 hours agoprevI was about 10 years old when I first started using the Internet (Netscape) in the mid 90s. Around 97-98 was really when I went deep into it mostly because of Starcraft. That led to battle.net which led to IRC and the rest is history. reply ruthmarx 8 hours agoprevThe reason I miss the net from that time is not due to web design, but just the types of people that were online. I'm too young to have experienced Eternal September, but it seems every generation has their own...well, the current generation probably won't. Nowadays you have every idiot spouting off half-baked opinions with horrible grammar and spelling, everyone arguing just to be right and not to test their own ideas, being ridiculously tribal, calling anything that disagrees with them fake news, etc etc. When there was a little more barrier to entry, the net was vastly better. As someone that cares about truth, fact and good faith discourse, I miss that time terribly. reply swsieber 8 hours agoparentI mostly agree; but we are experiencing the Eternal September of AI and bots. reply soh3il 13 hours agoprevMan, I feel you. The old internet had this raw, curious energy that just isn’t the same today. That’s kind of what led us to create Bettermode. We still believe in that mission of giving people the tools to build, connect, and create something meaningful—like the good ol’ days but with today's tech. It’s not about looking back, though—it’s about bringing that same spirit into the future. We’re still here building towards that, even if the landscape looks a bit different now. reply keybored 12 hours agoprevThere are enough problems right now. Wishing you were born in time to see Led Zeppelin in the 70’s? Before some mass-Web consumption turning point? No, being born in the 80’s/90’s was late enough for me. Some kids these days are so neurotic about their future outlook that climate change lives rent-free in their minds, worsening their daily mental health. Have fun with the ever-worsening Anthropocene, kid. I mean, we both will. reply ChrisRR 6 hours agoprevI miss when ads were just a single banner image reply qiqitori 13 hours agoprevSocial media is like TV zapping, just... probably more addicting because you're likelier to find something mildly interesting (to you). It's also described as junk food for the brain. Don't go there, instead spend time in the weird and fun niches, which are probably just fine still! (Note, even back then they had people who were wrong on the internet.) If you use Social Media sites sometimes, I recommend at least getting rid of \"recommendations\"/\"sponsored\" boxes. To see what that looks like e.g. on Twitter without installing an extension and keeping it up-to-date (cause Twitter keeps changing their stuff), you can try a bookmarklet like this: (javascript:document.querySelectorAll('div[aria-label=\"Trending\"]').forEach(function(v) { v.remove() })) Not all is worse in this decade though. If you're actually looking to learn stuff that involves e.g. physically building stuff (rather than just doing stuff on your computer), YouTube et al. have improved that by a lot! (Of course, this kind of content can be addicting too. But at least you learn a thing or two!) Note: the YouTube of the 00s was very low-resolution and the comments often were really bad! (https://xkcd.com/202/) reply skydhash 12 hours agoprevI was late to the party (but with my country’s tardiness in anything technology, it was pretty much the same) as I started around 2010, but the one nice thing I do remember was how easy it was to find random blogs or forums about your subject of interest. Even though I was connected for only an hour or so a day, information was readily available although sparse. Now, we have more data, but it feels more like a humid jungle full of tarpits and biting insects. Even corporate sites have the slimy feeling you’d associate with shady sites that wanted you to update flash. reply haxiomic 7 hours agoprevA thread to pull on for anyone who misses this unconstrained creativity and humanness of online communities. It exists again, in a new form in VR. It's absolutely wild and fantastic and unbelievable. Perhaps start here https://youtu.be/hVWlgh8QP5s?si=z4RD6FnMpPGQYuem&t=1523 And feel free to email (see bio) if you want to connect in there reply stuaxo 10 hours agoprevWell, the clean, handed coded page posted is a good start. It's refreshing to see - I think the web got boring once bootstrap hit and everything started looking that way. reply MrVandemar 7 hours agoparentIt's not hand coded, but I can see why you think that. It's a Bear Blog, which has sane readable defaults. reply singularity2001 10 hours agoprevThe first time I entered a random chat in the 90s someone typed Italian and I panicked thinking it may be the mafia and unplugged the modem. reply tim333 7 hours agoprevAs a counter example I'm old and I prefer the internet now to the 95-00s. The problem I had back then was there was a lot of potential but the content was mostly junk. There's a lot of great content now. reply glimshe 9 hours agoprevI miss Delphi/C++ Builder for professional application development... :'( reply Barrin92 13 hours agoprevWhen it comes to technologies or cultural periods there's usually a Goldilocks zone where new developments open up possibilities for people to participate but complexity and the usual market capture hasn't yet set in. Basically points where ideas are up for grabs and the culture is especially dynamic and the barrier to entry is low. For computing and the internet I think the author is spot on. The late 80s to mid 2000s had that these kinds of features. For electronic, rock or punk it was probably the 60s to 80s. I don't really like the relativism of writing people just off as nostalgic in this case funnily enough applied to both people too young or too old. There really are time periods that suck and some that don't. An interesting observation is that there are very few, if any Gen Z hackers or founders comparable to say Carmack or entrepreneurs like Zuckerberg. Virtually every tech company today is run by Gen X / Millenials. Maybe reflecting that the \"product culture\" Gen Z grew up in has resulted in not very technology oriented social media businesses. Maybe a little bit uncharitably put we went from young people writing Doom and PageRank to bored ape nfts being hustled on Discord servers. reply chaosist 8 hours agoparentI would imagine Doom and PageRank was the right person finding the right problem to work on at the right time in the right domain. If Page, Carmack and Zuckerberg were all trying to do something in nuclear physics I think they would have been where young people are today with computing. A past problem with the low hanging fruit picked and continued progress having undesirable higher order effects. I am sure their are just as brilliant young people today. If I had to bet, I would bet the reason we don't feel this is because they are in a domain that is not purely computer science related and a really good chance what they are doing is not in English but Mandarin. reply Karrot_Kream 12 hours agoparentprev> I don't really like the relativism of writing people just off as nostalgic in this case funnily enough applied to both people too young or too old. It's not a relativism as applied to a specific technology, it's a relativism as applied to development as a whole. The 60s-80s represented a flowering of semiconductor applications, before which we had a huge period of radio and telephony experimentation. Each era had its low-hanging fruit ripe for innovation which became a mature, hard-to-innovate technology in the future. There's new trends out there now. On the creative side of things, bloggers, artists, and influencers are toppling the old ideas of media empires and stodgy publishing houses. A Soundcloud god can sell out a stadium and virtual avatars are filling concert halls. Non-web-tech fields like climate tech are seeing crazy investment throughout the globe. It's true that webtech now is seen as a safe, stable, lucrative career but there's always something on the bleeding edge. It just might not be your thing and that's okay, that's what the kids are for. Or, pivot your career and try something new if that's what you want. reply ocean_moist 13 hours agoparentprev> There really are time periods that suck and some that don't. An interesting observation is that there are very few, if any Gen Z hackers or founders comparable to say Carmack or entrepreneurs like Zuckerberg. Virtually every tech company today is run by Gen X / Millenials. Maybe reflecting that the \"product culture\" Gen Z grew up in has resulted in not very technology oriented social media businesses. AI (LLMs) represents a platform shift on a large scale and I feel the environment is similar to that which birthed Zuckerberg type entrepreneurs. I think in the next 5 years Gen Z will have its heyday in terms of tech startups... At least, that's what I tell myself. Yeah, I agree the Web3.0 stuff was overhyped--not really a platform or thinking shift. There are a lot of black-hat hackers who are Gen Z, because generally only young people take risks like that while simultaneously having the skills to get paid a tech salary. reply grafelic 11 hours agoprevI am hoping and waiting for the 80s punk equivalent anti-SoMe counterculture to appear. reply carabiner 13 hours agoprevAll nostalgia is a longing for youth. reply svantana 8 hours agoparentbut it ain't what it used to be reply paulpauper 13 hours agoprevThe internet nowadays is run by gatekeepers, as traffic is increasingly funneled though a dozen or so popular sites. If you get banned from Facebook, twitter, Amazon, reddit, youtube etc. you're SOL. You either have to use in read-only mode or make new account and hope they do not notice it is tied to the old one (also, you're putting all your eggs in a single basket). A social media ban is effectively a ban on the person if it's tied to a real name/identify. The only people who get to use social media to its full capability are those who have the connections or clout to negotiate/dispute the bans that are inevitable. reply tim333 7 hours agoparentI have multiple accounts on fb, twitter, reddit, maybe youtube. It's not that big a deal in practice. And it's quite unusual to get banned unless you do something quite bad. reply tayo42 13 hours agoparentprevAfter myself being banned, it is a weird experience. There is no recourse if you think it's unfair. There are more niche places, but it does get strange seeking out this tiny communities online reply anthk 9 hours agoparentprevTwitter.com -> xcancel.com Youtube.com -> invidio.us Reddit -> teddit.net or gopher://gopherddit.com And so on. reply tessierashpool9 9 hours agoprev90s internet versus now is like eating steaks and potatoes with red wine versus chocolate and gummy bears with coca cola. even those who are hooked on the dopamine rush despise it cause it just makes you sick even after a few hours already. sleeping is the tolerance break and then you need it again like a chain smoker needs his first cigarette - but after a few rounds one already anticipates the hollowness it leaves you with at the end of the day. and it's not just the internet. the 90s and even more so the 80s were for sure the better time to be young (at heart). reply boffinAudio 9 hours agoprevI've been on the \"Internet\" since the mid-80's, when I was lucky to gain my very first shell account on an Internet-connected system at the electronics assembly line where I apprenticed as a junior programmer, building test software and eventually the DOS driver for a locally designed and produced modem. I wrote a lot of software on that shell account, relying on access to USENET and my email address for a lot of the research and hand-holding that booted me from junior to somewhat competent developer. During the 90's, I set up and maintained my own Linux machine at home, pushing my 28.8k modem connection into service to put my little 486 on the Internet as a dedicated host, running Majordomo and mailman list-servs devoted to topics I was interested in - mostly music-making technology and the like. Those listservs (and also, of course, USENET) gave me access to a vast and awesome array of folks around the world, some of whom were my early guru's, some of whom became my nemeses, but most of whom were a part of a worldwide community of folks I knew I could rely on for an entertaining and educational hour or two, each day, of reading. (Some of those listservs still exist, but have gone into comatose, dormant state with the last decades' rise of social media..) Without question, I feel that the quality of the community is directly related to the involvement of that community in the methods used by the community to sustain itself - as the \"AOL'ization\" of the Internet occurred, the quality when way, way down. Yes, I was there that fateful September when the Internet was invaded by the hoards of unwashed masses. I still feel that phenomenon was a turning point in social community - it went from being a community, to a media. I also still think there is a place for these kinds of locally-built and run communities. I often wonder how viable it would be for me to set up and run another little box, locally, and invite some friends to join me on it, discussing whatever we want, through a shell-only account, or at the very least, using only email/listservs for distribution ... I think there's still a lot of room for that style of community building, personally, since: technology doesn't get old - only its users do. reply cjbgkagh 12 hours agoprevWas tough to get past the first sentence. I wouldn’t consider 18 the prime of your life, that’s still far too young to know better. The trope of old man yells at cloud should have the dual of young person thinks they have discovered something new. It takes a while to learn there is nothing new under the sun. Sure, normies have invaded the internet, but there are plenty of niches in various fields that are sufficiently avant-gard that even the most dissenting hipsters can still find a home. But it’s never easy, almost by definition such places are defined by their difficulty of entry. reply hembarker432 13 hours agoprevLet’s fix it reply system2 13 hours agoparentIn your dreams. Corps are running the internet and small fish have no place to go but the social networks. I am surprised why forums even exist anymore at this point. reply nonrandomstring 10 hours agorootparentThe Internet was dream once, back when giant telcos ran circuit switched copper line monopolies. It remains a dream today - unfinished work we still have not realised. Because it's not a thing, it's a social project. There's a line in \"Leave the world behind\" [0] where Rose says about Friends that it's feel is \"nostalgic for a world that never happened\" It's called \"anemoia\" [1] It's many things. One is that it's a way of expressing latent values that don't fit into the current culture. \"Small fish have no place to go\" is a comment on a perceived reality that misses (and so reveals) two important points; the Internet didn't change [2], people did. It's not a place you go, it's a thing you do and the attitude you bring to that. But ultimately the Internet is nothing but people connected by wires. People have moods we call \"culture\". Back then it was unbelievably optimistic and giddy. People wanted to reach out. Everything was ripe with possibility and fun. The OP is justified to wonder what it was like to live then. Now we know what kind of things are at the other end of that wire and it feels more like a dark forest. It's more than that people like me \"just get older\", the culture has palpably changed for the worse. Climate change maybe. It's become objectively more pessimistic and neurotic. Sure there's a few hold-outs, but that takes a lot of Prozac to keep up. Anyway, the future of the Internet is all about \"small fish\". That's all it's ever been about. [0] book: Rumaan Alam screen: Sam Esmail [1] https://www.sciencefocus.com/the-human-body/anemoia-nostalgi... [2] Of course the Internet did change technically, it got much bigger, faster, cheaper, more reliable etc. reply dackdel 10 hours agoprevcheck out nostr. gives me the feeling of 90's internet reply aleph_minus_one 10 hours agoparent> check out nostr. gives me the feeling of 90's internet I'm not so sure about that: in the 90s, there was no social media. reply leetsbehonest 9 hours agoprevAh yes, the short period of time in human history where global connectivity and individuality coexisted reply fzeroracer 11 hours agoprevI miss the 90s/2000s internet too, but I'm also realistic about it. Dialup sucked, search engines sucked, it was easy for random stuff to brick your computer and it was hard for anything you published to reach an audience. If you think ads now are bad, they were even worse back then. What people miss is the slower sense of community. So",
    "originSummary": [
      "An 18-year-old expresses nostalgia for the '90s-00s internet, contrasting it with today's commodified social media landscape.",
      "The author criticizes modern platforms like Instagram and TikTok for promoting superficiality and FOMO (Fear of Missing Out), longing for the creativity and individuality of personal blogs and MySpace.",
      "They mention a niche community on Neocities that appreciates the old web, but note that most peers find such interests unusual."
    ],
    "commentSummary": [
      "The author reminisces about the '90s-00s internet, highlighting its amateur energy, pseudo-anonymity, and counter-cultural feel, which significantly influenced their career in the game industry.",
      "They express nostalgia for the early internet's sense of wonder and community, contrasting it with today's commercialized and algorithm-driven web.",
      "Despite technological advancements and increased access, the author and others feel that the original value of the internet has been diminished by its mainstreaming and commercialization."
    ],
    "points": 183,
    "commentCount": 229,
    "retryCount": 0,
    "time": 1726029143
  },
  {
    "id": 41508752,
    "title": "AppleWatchAmmeter",
    "originLink": "https://github.com/jp3141/AppleWatchAmmeter",
    "originBody": "AppleWatchAmmeter Turn your Apple Watch or any watch with an accessible magnetometer into an ammeter to measure DC currents. Apple Watches newer than Series 5 which have a compass also contain a magnetometer. The magnetometer is also sensitive to magnetic fields from nearby currents. This demonstration uses a coil of wire around the watch to alter the magnetic field sensed. A circular coil of wire with N turns and a diameter D will generate a magnetic field of B = u0.I/D (u0 is defined to be 4.π.10^-7); so for a watch with a diameter of about 48 mm (approximately Apple Watch 5), 5 turns with 1 A will generate a field of 5 * 4.π.10^-7 * 1/0.048 = 131 uT (or 1.3 gauss). Because the magnetometer is not centered in the coil (it's usually in the very bottom right corner of the watch), the actual sensitivity is a bit lower; 100 uT/A is a reasonable approximation that can be improved by calibrating. An app could easily be written to perform some calibration and zero offset as well as display current in Amperes. however nearly any app that can display raw magnetometer data can be used. I use 'Sensor-App' which is free. Note that you can only detect DC currents; the magnetometer is too slow to react to AC (e.g. household power) currents. In this demonstration, a 5-turn coil can show curent changes of about 10 mA (about 1 uT in the Z direction). The Earth's background magnetic field is around 60 uT (not only in a North-South direction). With a 5-turn winding, each 1 A generates an additional ~ 100 uT in the Z direction; there is some noise in the readings, but with care, 10 mA (1 uT) changes can be discerned.",
    "commentLink": "https://news.ycombinator.com/item?id=41508752",
    "commentBody": "AppleWatchAmmeter (github.com/jp3141)172 points by rcarmo 5 hours agohidepastfavorite24 comments boomskats 3 hours agoOnly somewhat related, but wasn't there a DIY biohacking[0] craze a few years ago, with people implanting rare earth magnets into the tips of their pinky fingers, waiting for the nerve endings to heal around the implant, and then supposedly gaining a 'sixth sense' of being able to detect the flow of current via their bionic pinky? Is that still a thing? [0]: https://www.wired.com/story/diy-biohacking/ reply Gracana 2 hours agoparentI recall Zoe Quinn wrote about having that done ~10 years ago. Article's still up on her site, it's a good read: http://www.beesgo.biz/magnet.html reply shagie 15 minutes agoparentprev> ... with people implanting rare earth magnets into the tips of their pinky fingers ... Imagine the joy of getting an MRI and forgetting about that bit of biohacking. reply Onavo 1 hour agoparentprevYou can experience the same thing by supergluing a magnet to your pinky. reply surfingdino 19 minutes agoparentprevMust me useful for emergency proctologists. Neodymium magnets would probably work better for such applications. reply cmsjustin 2 hours agoparentprevhttps://www.youtube.com/shorts/17l9BucbXec reply Spivak 2 hours agoparentprevGiven how well-trodden putting hardware in humans is, I'm surprised this wouldn't be aA circular coil of wire with N turns and a diameter D will generate a magnetic field of B = u0.I/D (u0 is defined to be 4.π.10^-7); That u0 is meant to symbolize μ_0 (mu zero, with zero being subscript) the vacuum magnetic permeability [0], in case you were wondering where that magic constant came from. [0] https://en.wikipedia.org/wiki/Vacuum_permeability reply lapetitejort 1 hour agoparentSee also ε_0 [0]. 1/sqrt(ε_0*μ_0) = c, the speed of light [0]: https://en.wikipedia.org/wiki/Vacuum_permittivity reply bri3d 3 hours agoprevSee also: https://phyphox.org/wiki/index.php/Smartphones_as_ammeters - I've heard of some physics instructors using this in teaching settings. I'm quite surprised that nobody has made an app which can do some baseline calibration and produce real-time current measurements yet; it would just be a few hundred lines of code at most. reply _Microft 3 hours agoparentPhyphox is amazing: there is a wide variety of different experiments available using all sorts of sensors found in phones. The iPhone accelerometer seems to saturate at roughly 13g, easily achievable by pulling your phone very fast in an increasingly narrow arc towards yourself. What were you thinking how I found out? (Fun fact: this is another project that Sebastian Staacks of http://there.oughta.be worked(s?) on. You might remember his wooden Gameboy shell, the Gameboy interceptor to screenshare the Gameboy screen to a PC or his bullettime video booth.) reply cdchn 3 hours agoprevIt would be cool to use this as a voltage sensor, to check for live circuits, but the downside being you might not want to crane your wrist towards said live circuits. reply atoav 3 hours agoparentAnd: measuring current can be done with any odd hall effect sensor, voltage not so much. I imagine a smart warch with two 4mm banana-jacks isn't going to be the next big thing. reply ComputerGuru 2 hours agoparentprevAs TFA mentions, detecting AC current is rather different from detecting DC current (the article is about the latter). reply kstrauser 3 hours agoprevThat’s genius in its simplicity. Wow. Well done! reply 8338550bff96 3 hours agoparentSimple genius is the only kind I recognize reply brcmthrowaway 1 hour agoprev [–] I wish Nokia was still around, this would be turned into a feature. Think N95 was a smorgasboard of random features. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple Watch Series 5 and newer can be used as an ammeter to measure DC currents by leveraging their built-in magnetometer.",
      "By wrapping a coil of wire around the watch, the magnetic field generated by nearby currents can be detected and measured, with a sensitivity of approximately 100 uT/A.",
      "An app like 'Sensor-App' can be used for calibration and to display the current in Amperes, allowing detection of current changes as small as 10 mA."
    ],
    "commentSummary": [
      "Discussion revolves around the concept of using smart devices, like the Apple Watch, to measure electrical current, with references to DIY biohacking and historical experiments involving rare earth magnets.",
      "Participants mention various methods and tools, such as Hall effect sensors and smartphone apps like Phyphox, for measuring current and voltage, highlighting the innovative yet risky nature of these experiments.",
      "The conversation includes humorous and speculative comments about the practicality and safety of such biohacking techniques, reflecting a mix of curiosity and skepticism."
    ],
    "points": 172,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1726038061
  },
  {
    "id": 41509713,
    "title": "Radicle 1.0 – A local-first, P2P alternative to GitHub",
    "originLink": "https://radicle.xyz/2024/09/10/radicle-1.0.html",
    "originBody": "Radicle is a peer-to-peer, local-first code collaboration stack built on Git. Radicle 1.0 10.09.2024 On March 26th, we announced the first release candidate for Radicle 1.0. Today, after five months of feedback and 17 release candidates, we are ready to launch Radicle 1.0. If you’ve been waiting for the right moment to try Radicle or to reintroduce yourself to the stack, now is a great time to dive in: our website and guides should have all the information you need to get started. You can also browse the latest code, hosted on Radicle. If you’ve been tagging along, thank you, it’s been a bumpy ride and we couldn’t have done it without you! Try it out! You can grab the latest release with the following command: curl -sSf https://radicle.xyz/installsh Or head over to the download section to download and verify the binaries yourself. What’s in the box 📦 Radicle 1.0 represents the culmination of years of experimentation and hard work from our team and community, where we set out to ensure that free and open source software ecosystems can flourish without having to rely on the whims of Big Tech. We designed Radicle with a first-principles approach, as a natural extension to Git, expanding it to work in a collaborative, local-first, peer-to-peer setting. This milestone includes: An extensible, homegrown, peer-to-peer gossip and sync protocol built on the Git protocol. Social interactions such as issues, patches and code review, using our extensible Collaborative Objects system which keeps all artifacts in the repository. A secure authentication and authorization protocol using public key cryptography, which allows all published content to be verified locally, without centralized authority. An intuitive CLI that should be familiar to users of Git, as well as a web frontend for browsing Radicle repositories and seed nodes. Privacy at the protocol level, with truly private repositories and built-in Tor support. Reproducible and signed builds for all Radicle binaries. To us, Radicle 1.0 means that Radicle is ready to use. It stands as a testament that sovereign code forges are possible today, and in our opinion, necessary. We feel comfortable now inviting you to join us, replicate, and collaborate within the Radicle network or even run a seed node. For an in-depth explanation of how Radicle works, check out our Protocol Guide. Stability ⛰ Radicle 1.0 marks our commitment to stability: from this release onwards, all changes to the protocol will be designed in backwards compatible way, and any necessary change on the CLI will include a seamless upgrade path. We are aware that the release candidate phase was rockier than expected for some, but we are now in a good place to slow things down and improve stability. Along with this commitment will come a more dependable and streamlined release process which starts with this release! Future plans 🔮 There are several things in the pipeline that we intend to release when ready: Native CI/CD capabilities The Radicle TUI (Terminal User Interface) Advanced code review functionality An inbox system for repository notifications Multi-device support and user profiles Support for other canonical references, such as tags Seed node moderation and management tools The Radicle desktop application Growing ecosystem 🌱 Outside of the core stack, the ecosystem is growing nicely: an independent team working on integrations & tooling for Radicle has developed a VS Code and JetBrains plugin. The Radicle network now also comprises several deployments of the Radicle frontend, and there are more than 40 seed nodes operating on the network, freely replicating user content. Invitation to forge the future 🤝 Once you install Radicle and set up your identity, you’ll have access to all public repositories on any public node, making it easier to explore and contribute to the ecosystem. Compared to traditional self-hosted forges, which often result in fragmented collaboration environments, Radicle represents an evolutionary step for Git-based collaboration, with a single cryptographic identity that works across nodes. Lastly, in the spirit of free and open source software, we believe that power lies in community. As we embark on further iterating the protocol and stack, we also invite you to shape the future of Radicle with us. Consider joining us on our Zulip instance. Your ideas and insights are invaluable to our mission of creating a sovereign forge. Together, we can make significant progress towards reclaiming the internet. Free your code! 👾👾👾 Follow us on 🐘 Mastodon, 🦋 Bluesky or 🐦 Twitter to stay updated. Contribute to Radicle as a 🌱 seeder, 🧙 developer or by 🪞 mirroring your repositories on the Radicle network. Join our community on 💬 Zulip and discuss your ideas to improve Radicle.",
    "commentLink": "https://news.ycombinator.com/item?id=41509713",
    "commentBody": "Radicle 1.0 – A local-first, P2P alternative to GitHub (radicle.xyz)168 points by aiw1nt3rs 9 hours agohidepastfavorite54 comments bongobingo1 7 hours ago> For this guide, we recommend installing Radicle under /usr/local. This will require you to have write permissions to /usr/local/bin and /usr/local/man. You can give yourself these permission by changing ownership of these directories to the current user and group: $ sudo chown $(whoami): /usr/local/{bin,man,man/man1} Seems kinda weird to chown all that in the install instructions? Or am I the weird one. Edit: This is from the seed guide, the user guide suggests installing to ~/.radicle. Also booo for using ~/.radicle instead of ~/.config|local|whatever. Some quirks and bumps, but the software is good at first impression. I really hope something like this can take off. Needs a good (and somehow trustable/authoritative) searchable index though as that's 90% of what most users want from Github I think. I do wonder if the protocol could be adopted into something like Forgejo to aid adoption of the P2P idea. reply vrighter 7 hours agoparentthis is just horribly wrong reply jazzyjackson 7 hours agorootparentFYI the guide they're quoting is not for the usual installation but for hosting a seed box. Presumably the kind of thing you would spin up on a VPS and not use for anything else. reply smartmic 7 hours agorootparentprevYeah, I wonder how much of the long-standing Unix philosophy & consistency is lost by breaking with best practices. autotools seems to be written off by many younger (and non- C) developers and maintainers, but still, it would be good to stick to consistent conventions on that platform. reply taikahessu 7 hours agorootparentprevHow would you do it? reply stephenr 7 hours agorootparentBuild as a regular user; invoke root to install. ie: make sudo make install reply CodeCompost 7 hours agoparentprevAh, a Mac user I see. reply stephenr 7 hours agorootparentWho are you suggesting is a Mac user? The parent commenter or the author of the install \"instruction\" they quoted? reply amiga386 3 hours agorootparentThe author of the install instruction, because \"sudo chown -R $(whoami) /usr/local\" first gained popularity in how to install Homebrew in macOS. Homebrew assumed it was being installed on a single-user-at-a-time system, and it didn't want the risk of a bug in its ruby scripting being exploited somehow by insisting on running as root, and it chose /usr/local to install to as it was already in the path... all of these are anathema to Linux users. * A package manager should leave /usr/local alone as its purpose is to host the user's own software, not distributed software (that's /usr) or vendor software (that's /opt) * A package manager should run as root, or run its install-the-files portion as root, because it should require the user's admin authority _at that time_ to be permitted to install software that will be in the path of _other users of the system_. If the user takes ownership of the system directory instead, then all their other processes can mess with it too, not just the package manager. The right thing to do is to install to e.g. /opt/homebrew, and then require users to add /opt/homebrew/bin to their own PATH, which is what Homebrew now does. It only did it after Apple fought against them by using System Integrity Protection to prevent users changing the ownership of /usr/local -- for a while Homebrew told users to write \"sudo chown ... /usr/local/*\" rather than \"sudo chown ... /usr/local\" to get around that, and that's what we see here. The right place for this software on Linux is under /opt or /srv, and it should have the equivalent of \"sudo make install\", where root is required to install the software to the right place (and after installed there, it can't be modified by regular users), and root is _only_ required for that install step, nothing else. EDIT: I'm unable to reply to the person whose question I answered, but I'd commend the GP for spotting the macOSism, and recommend everybody be alert to these sorts of tells and impedence-mismatches. You can often spot experienced Windows users doing \"Windowsisms\" in Linux... and vice-versa! You can see people writing Rubyisms in Python, Cisms in Java, and so on. There's a risk that if they don't adopt the appropriate native idioms for the language/environment/OS, they will introduce subtle bugs, sometimes security bugs. Be on the lookout for this. reply stephenr 1 hour agorootparent> first gained popularity in how to install Homebrew in macOS That doesn't really make the argument that it's a macOS user who wrote it though does it? It just makes the argument that it was potentially inspired by Homebrew, which has existed on Linux for half a decade. I didn't really read the rest of your spiel in detail. I asked why you have an opinion about a person I didn't ask for a breakdown of package management and software installation best practices for the last few decades. reply dang 7 hours agoprevRelated: Radicle is an open source, peer-to-peer code collaboration stack built on Git - https://news.ycombinator.com/item?id=40166736 - April 2024 (53 comments) Radicle: Peer-to-Peer Collaboration with Git - https://news.ycombinator.com/item?id=39868504 - March 2024 (10 comments) How Radicle Works Under the Hood - https://news.ycombinator.com/item?id=39837117 - March 2024 (16 comments) Radicle: Sovereign code forge built on Git hits v1.0 - https://news.ycombinator.com/item?id=39829736 - March 2024 (3 comments) Radicle: Open-Source, Peer-to-Peer, GitHub Alternative - https://news.ycombinator.com/item?id=39600810 - March 2024 (284 comments) Understanding Peer-to-Peer Git Forges with Radicle - https://news.ycombinator.com/item?id=25322584 - Dec 2020 (14 comments) Radicle: A peer-to-peer alternative to GitHub - https://news.ycombinator.com/item?id=25313010 - Dec 2020 (255 comments) Radicle-Link: Extending Git with Peer-to-Peer Network Discovery - https://news.ycombinator.com/item?id=24382589 - Sept 2020 (37 comments) Radicle. A decentralized alternative to GitHub built on IPFS - https://news.ycombinator.com/item?id=19591011 - April 2019 (1 comment) Radicle Architecture - https://news.ycombinator.com/item?id=19511525 - March 2019 (18 comments) Radicle: A decentralized alternative to GitHub built on IPFS - https://news.ycombinator.com/item?id=19367916 - March 2019 (82 comments) reply mg 8 hours agoprevThese days when I collaborate with other developers, we do it all in a git repo that is simply hosted on a VM everyone has ssh access to. Project management is done in a plan/ directory which has task entries like 1000-add_logout_button.txt Where 1000 is the priority. There is also a directory plan/done/ where tasks go that have been completed. In the plan/done/ directory, the priority gets replaced by the completion date, so it looks like this: 2024-09-04-add_logout_button.txt This has a bunch of nice consequences. Some of them are: Task management does not need any software or logins. Task management can be done directly in Vim by simply looking into and editing the plan/ directory. Task history is nicely versioned by git automatically. All the tooling that comes with bash and git can be used on the plan/ dir. Like finding all tasks that contain the term \"rounded corners\" in the description is just grep -r 'rounded corners' plan/ reply imiric 7 hours agoparentI like the simplicity, but does everyone actually log in to the server and edit the files directly in the same repo, or is the repo exposed over SSH and everyone works in forks on their own machines? The first one doesn't really scale beyond a handful of people, since you risk losing work if the same file is being worked on by multiple people. Vim shouting at you that the file you're editing has changed on disk is not good enough. I can see how this could work with a central repo, though. It does require everyone aligning on the same conventions and workflow, so you likely still need some software to ensure this is done correctly. reply mg 6 hours agorootparentEverybody works on a fork on their own machine. Since everybody pulls the changes of the others and sees if something collides with their own changes, its pretty much possible to do it manually. Thats how I started this approach. It's really just a 0815 git based workflow. Just not for code but for a tasklist. Meanwhile I wrote a bit of tooling around it. But using it is not at all a must. reply keybored 30 minutes agorootparent0815? reply stephenr 7 hours agorootparentprevI assumed they meant a directory within the git repo not just a regular directory on the server filesystem. reply hoherd 6 hours agoparentprevThat is a really cool system, thanks for sharing it! I've been looking for a text based way to do project management and this is giving me a bunch of ideas. reply vincnetas 7 hours agoparentprevi love the simplicity. What do you do with team members who are terminal illiterate? Project manager/client? reply bordumb 4 hours agorootparentThere are plugins for JetBrains and VSCode https://plugins.jetbrains.com/plugin/19664-radicle https://marketplace.visualstudio.com/items?itemName=radicle-... reply ilc 6 hours agorootparentprevThe prophylactic effect of this could actually be a real benefit when the org is small. As it grows, this type of as system will break down anyways :) reply Borg3 7 hours agorootparentprevYou just lay them off.. ;) reply huijzer 7 hours agoparentprevDo the reviewer comments also go into plan? reply mg 3 hours agorootparentYes. Say Joe just finished the 1000-add_logout_button.txt task and committed it with the following content: owner: mg info: 2024-09-03 17:02 I added the logout button. Settled on rounded corners which we haven't used before but I think they look good here. /Joe I might change the file like this and commit it, so it goes back to Joe and he sees my comment: owner: Joe info: 2024-09-03 17:02 I added the logout button. Settled on rounded corners which we haven't used before but I think they look good here. /Joe 2024-09-04 09:10 Fine with the rounded corners. The styles should go into styles.css instead of the html though. /mg reply baq 7 hours agoparentprevsee also: https://fossil-scm.org reply BSDobelix 7 hours agorootparentYep, i use fossil for small to mid customer-projects, documentation, code and task all in one single file, it's just excellent for jobs like that. Personal projects however are all on bitkeeper. reply boramalper 5 hours agoprevSurprised no one mentioned ForgeFed [0]: > ForgeFed is a federation protocol for software forges and code collaboration tools for the software development lifecycle and ecosystem. This includes repository hosting websites, issue trackers, code review applications, and more. ForgeFed provides a common substrate for people to create interoperable code collaboration websites and applications. It's based on ActivityPub [1], the same protocol that powers Mastodon [2], Lemmy [3], and Pixelfed [4]. [0] https://forgefed.org/ [1] https://activitypub.rocks/ [2] https://joinmastodon.org/ [3] https://join-lemmy.org/ [4] https://pixelfed.org/ reply bestouff 4 hours agoparentAgreed. It was quite a disappointment seeing Radicle uses its own protocol instead. reply nunobrito 7 hours agoprevHello, Is support for NOSTR planned on the roadmap? Because they have hundreds of relays and with one single identity we can combine git and text publications, as well as combine with the rest of the open ecossystem built around NOSTR. Right now is OK, just that is makes everyone keep separate key accounts and data. reply alphazard 6 hours agoprevThis is a great idea and exactly what software development needs right now. However there are a lot of things that seem more complicated than necessary. https://radicle.xyz/guides/protocol I'm reading through the delegate consensus model for branches and I don't see why any of that is necessary. It's encroaching on the P2P that Git already does well, which is who I decide to push and pull from. I think this project could learn a lot from the KeyBase model, which is to just be a transport for Git. - Nodes in the network should host local Git remotes. In Git you would configure the remote (origin is the conventional default) to be the address of the local Radicle daemon + a public key for the device that owns it + the repository name. Something like `http://localhost://repository_name`. If the device_id is the local device then it stores the repository internally, otherwise it calls out to the P2P network. - You can add multiple remotes to a Git repository, so you could have a remote for each copy of the repository on each Radicle device you care to sync with. Git push pull just works. - You can use the very simple regexp branch protection model that GitHub and friends use on each device. e.g. master in MyRepo on DeviceA only allows pushes from DeviceB. DeviceIDs become the primitive for authentication. Groups of Device IDs become users. - There is plenty of existing tooling for managing mirroring and syncing on top of Git. What we are missing is a way to securely push and pull between arbitrary devices on the network by cryptographic id. e.g. I'm on my laptop, I want to pull from new-feature4 on my friend's desktop, I know its public key, not its IP address, and I know the branch name, figure it out for me. - This doesn't mean that the source of truth device has to be online to access the repository. Since there is a single source of truth it can sign any changes, and everyone interested can cache those changes, using the signature to know the refs are legitimate. reply intelfx 3 hours agoparentThis strikes me as entirely missing the idea of Radicle. What you are saying is basically \"why automate things if you can just do them manually\". But worse is not better; there is value in having the P2P transport layer work _transparently_. If you intend to force user to manually (or semi-automatically) juggle remotes for every single device on the network, you have already lost. TL;DR What you describe is just Git on top of a generic P2P overlay network. I'm sure it can be done (and I might even believe it could be enough for some limited use-cases), but it is not Radicle. reply Jnr 8 hours agoprevWould be great if there was a short video/asciinema/charts or some other short form guide on how you are supposed to use it. It was not clear from the home page what exactly it is and how it works, and I don't really want to read the whole documentation. For me pesonally Github, Gitlab and Gitea is all about discovery. Their interface is easy to understand, fast to search for repositories and easy to contribute. I am not sure what problem Radicle is solving, other than \"reliance on big tech\". And it has always been possible to mirror git repos from github and other platforms. You could not mirror issues and CI pipelines, but not sure if Radicle solves that. reply XorNot 8 hours agoparentFrom the linked page here I couldn't find an obvious link through to the docs except by clicking \"Install\" which dropped me onto the user guides where I found this[1]. It looks like the main page[2] has the site map but I don't know why that top banner isn't on every page. The main page[2] seems to be much more useful since it drops you to radicle browser page which is more of showing the actual product as you'd see it in terms of Gitlab/Github. But I'm just going to say...they really need to drop the blue theming IMO because it's very hard to read and also just...too blue - everything just washes out. EDIT: Just tried turning off --color-background-default, --color-background-float and --color-foreground-contrast and without changing anything else man does it get so much more readable - black text on white with a pleasant blue high-lighting scheme. [1] https://radicle.xyz/guides/user [2] https://radicle.xyz/ [3] https://app.radicle.xyz/nodes/seed.radicle.xyz/rad:z3gqcJUoA... reply v3ss0n 7 hours agoprevForgjeo/Gitea support would be nice. I was waiting for Radicle for long but for our self hosting needs we are using Gitead then moved to Forgejo and never looking back. reply diggan 6 hours agoparent> Forgjeo/Gitea support What exactly are you asking for here? A import/export tool to move all data between the two? AFAIK, both use Git already, so moving the actual code would be trivial and already possible. reply bongobingo1 6 hours agorootparentIMO: running a Foregjo instance would also export to the rad network. reply diggan 5 hours agorootparent> would also export Export what? The data stored by git or something else? reply bongobingo1 4 hours agorootparentAs in it would act as a seed to the network and interop their issues/patches into the network too (probably a big ask). reply udev4096 7 hours agoprev> The Radicle team is funded by Radworks. To date, around $7m has been granted towards the development of Radicle, by Radworks. That's quite impressive. reply jazzyjackson 6 hours agoparentSome kind of crypto token is involved, bless them for doing something useful with the spoils. https://docs.radworks.org/community/rad-token reply jhvkjhk 7 hours agoprevIt's great to collaborate on code in a P2P way. I think in the end, we also need a way to share binaries in a P2P manner using Git (so we can track history with the beloved Git). Maybe it's a good idea to integrate git-annex into this? reply lnenad 8 hours agoprevI am stupid, can someone explain the purpose of this as I am not getting it from their website? reply BSDobelix 8 hours agoparent>I am stupid, Naaa just lazy https://radicle.xyz/faq reply lnenad 49 minutes agorootparentAs far as I understand there is no the why in the faq? > Radicle is an alternative for people and organizations who want full control of their data and user experience, without compromising on the social aspects of collaboration platforms. Since this can be achieved by self-hosting other non p2p solutions I don't see this as a why for the product. I am trying to understand the actual benefit of p2p here that is it. reply lucideer 8 hours agoprevHas anyone done a comparative breakdown between Radicle & git-ssb? This looks really nice & polished - love the web ui - but I know there's a pretty active network & community on SSB. reply bongobingo1 7 hours agoparentI had not heard of SSB or git-ssb, but this gives me a lot of pause... >This seems to work well: the SSB network thrives off of being a group of kind, respectful folks who don't push to each other's master branch. :) https://github.com/hackergrrl/git-ssb-intro?tab=readme-ov-fi... Can't imagine that working at any real level of popularity, but maybe that just goes in the \"if you have to solve that problem, its a good problem to have\" bucket. reply lucideer 3 hours agorootparentIt seems odd at first until you realise that it's a host setting & a convention, rather than an inherent limitation. The familiar model of \"centralised\" Git hosts like Github come bundled with their own protocol-specific permissions models. E.g. if you were to imagine a team working from a simple .git directory hosted on a local SMB/whatever LAN share, the permissions model would be file permissions on the networked filesystem. Blocking users committing to an \"owned\" branch on SSB would require implementing an ACL-tracking & ownership model attached to namespaces (likely branch prefixes here by convention) - very doable, nothing about the protocol prohibits it architecturally. So if it ever becomes one of those \"good problems\", it's not an inherently concrete design decision. reply amiga386 8 hours agoprevNot to be confused with Radicale 3.0 (a CalDAV/CardDAV server) https://radicale.org/v3.html reply dotancohen 7 hours agoparentThank you, I've been looking for a CardDAV server to replace an entire NextCloud install that was used only for that purpose. Serendipity at its finest! reply ImHereToVote 5 hours agoprevWhy was this censored from HN? What is going on? reply otabdeveloper4 6 hours agoprevWhere is the documentation for running a private instance? [The main use case for this software] reply bongobingo1 5 hours agoparenthttps://radicle.xyz/guides/seeder https://radicle.xyz/guides/user#3-selectively-revealing-repo... reply vouaobrasil 7 hours agoprev [–] Looks nice. I already terminated my Github account due to their support of AI in coding, which I hate. This looks like a good alternative. reply geysersam 7 hours agoparent [–] Just curious, why don't you like ai in coding? Because they're making money from open source code without contributing? reply viraptor 7 hours agorootparent [–] GitHub effectively contributes a significant amount of money and work hours by hosting close to all the open source software for free, including some CI time and the collaboration features. For some projects like Nix that's a really non-trivial system on its own. It's not just the code that counts towards FOSS contributions. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Radicle 1.0, a peer-to-peer, local-first code collaboration stack built on Git, has officially launched after five months of feedback and 17 release candidates.",
      "Key features include a peer-to-peer gossip and sync protocol, social interactions (issues, patches, code reviews), secure authentication, an intuitive CLI and web frontend, privacy features, and reproducible signed builds.",
      "Future plans for Radicle include native CI/CD, a Terminal User Interface, advanced code review, and more, with growing ecosystem integrations like VS Code and JetBrains plugins."
    ],
    "commentSummary": [
      "Radicle 1.0 is introduced as a local-first, peer-to-peer (P2P) alternative to GitHub, sparking discussions on installation quirks and comparisons with tools like Forgejo and Homebrew.",
      "Users debate the practicality and philosophy of Radicle's decentralized code collaboration, with some preferring simpler task management using git repositories on virtual machines (VMs).",
      "The conversation also touches on Radicle's funding, potential improvements, and integration with tools like ForgeFed and NOSTR."
    ],
    "points": 168,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1726047638
  }
]
