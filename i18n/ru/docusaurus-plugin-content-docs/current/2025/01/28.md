---
slug: '/2025/01/28'
---

# 2025-01-28

## [Мы возвращаем Pebble обратно](https://repebble.com/)

### [Реакции](https://news.ycombinator.com/item?id=42845091)

Pebble возрождается при поддержке Google, сосредотачиваясь на своих первоначальных сильных сторонах, таких как возможность взлома, длительное время работы от батареи и функционирование в качестве расширения телефона. Возрождение направлено на сохранение открытого исходного кода Pebble и избегание обязательных облачных подписок, что привлекает хакеров и технических энтузиастов. Сообщество с нетерпением ожидает возвращения Pebble, вспоминая его уникальные особенности и влияние на носимую технологию.

## [Google открывает исходный код операционной системы Pebble](https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html)

### [Реакции](https://news.ycombinator.com/item?id=42845070)

Google открыла исходный код Pebble OS, что вызвало энтузиазм среди поклонников и разработчиков в отношении потенциальных новых разработок в технологии смарт-часов. Релиз на GitHub не включает в себя проприетарные компоненты, такие как системные шрифты и стек Bluetooth, поэтому его нельзя скомпилировать в текущем виде. Этот шаг рассматривается как положительный жест со стороны Google, приписываемый внутренним усилиям, и воспринимается как шаг к возрождению экосистемы смарт-часов Pebble.

## [Запустите DeepSeek R1 Dynamic 1.58-бит](https://unsloth.ai/blog/deepseekr1-dynamic)

### [Реакции](https://news.ycombinator.com/item?id=42850222)

DeepSeek R1 Dynamic 1.58-bit достигает 80% сокращения размера и работает со скоростью 140 токенов в секунду, используя два H100, но его медленная скорость и проблемы с повторением вызывают вопросы о его практичности. Динамическое квантование способствует повышению производительности, однако сохраняются опасения по поводу доступности, стоимости и заявлений о стоимости обучения модели, что приводит к тщательному анализу. Модель оказывает заметное влияние на рынок, при этом предпринимаются усилия по воспроизведению её результатов, хотя её производительность обсуждается по сравнению с более крупными моделями.

## [Многообещающие результаты от DeepSeek R1 для кода](https://simonwillison.net/2025/Jan/27/llamacpp-pr/)

Запрос на слияние (PR) от Xuan-Son Nguyen для llama.cpp улучшает скорость WebAssembly (WASM) с использованием инструкций Single Instruction, Multiple Data (SIMD), с значительными вкладами от DeekSeek-R1. PR включает в себя динамическую модель model_map, построенную на основе ответов API, что устраняет необходимость в жестко закодированных версиях и демонстрирует инновации в разработке плагинов. Веблог Саймона Уиллисона также охватывает недавние темы, такие как проекты с открытым исходным кодом, API цитирования от Anthropic и проекты с использованием больших языковых моделей (LLM), что указывает на акцент на обсуждении передовых технологий.

### [Реакции](https://news.ycombinator.com/item?id=42852866)

DeepSeek R1 демонстрирует потенциал ИИ в программировании, написав 99% запроса на слияние (PR) для llama.cpp, что подчеркивает растущую роль ИИ в разработке программного обеспечения. Такие инструменты, как aider, теперь отвечают за создание 70-82% нового кода в релизах, что свидетельствует о значительном повышении производительности благодаря помощи ИИ. Несмотря на эти достижения, ИИ все еще требует человеческого контроля для решения сложных задач и интеграции с существующими кодовыми базами, что предполагает изменение динамики рабочих мест и требований к навыкам в отрасли.

## [Иллюстрированный DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

DeepSeek-R1 — это недавно выпущенная модель ИИ, акцентирующая внимание на улучшенных способностях к рассуждению через структурированный трехэтапный процесс обучения: языковое моделирование, контролируемая тонкая настройка (SFT) и настройка предпочтений. Модель включает в себя длинные цепочки данных рассуждений, промежуточную модель рассуждений и крупномасштабное обучение с подкреплением (RL), превосходя в задачах рассуждения за счет генерации токенов мышления. Она использует архитектуру смеси экспертов, что позволяет эффективно справляться со сложными задачами рассуждения, отмечая значительный прогресс в дизайне моделей ИИ.

### [Реакции](https://news.ycombinator.com/item?id=42845488)

DeepSeek-R1 вызывает обсуждения благодаря своей производительности и экономической эффективности по сравнению с моделями, такими как GPT и Gemini, при этом некоторые пользователи отмечают типичные проблемы крупных языковых моделей (LLM). Модель примечательна низкими требованиями к вычислительным ресурсам и открытым исходным кодом, что потенциально может изменить ландшафт ИИ и сделать разработку ИИ более доступной. Разработанная китайским хедж-фондом, DeepSeek-R1 вызывает вопросы о своих обучающих данных и геополитических последствиях, несмотря на смешанные отзывы о ее возможностях в области кодирования.

## [Машинное обучение в производстве (курс CMU)](https://mlip-cmu.github.io/s2025/)

Университет Карнеги-Меллона предлагает курс под названием «Машинное обучение в производстве/инженерия ИИ» на весну 2025 года, сосредоточенный на создании, развертывании и поддержке программных продуктов с использованием машинного обучения. Курс подчеркивает важность ответственных практик ИИ и MLOps (операции машинного обучения), охватывая весь жизненный цикл от прототипа до производства. Он предназначен для студентов с навыками в области науки о данных и базового программирования, включает лекции, лабораторные работы и групповой проект, с ресурсами, доступными на GitHub.

### [Реакции](https://news.ycombinator.com/item?id=42847834)

Курс CMU по машинному обучению в производстве представляет практические инструменты, такие как Kafka, Docker, Kubernetes и Jenkins, с акцентом на MLOps (операции машинного обучения), объяснимость, справедливость и мониторинг. Он служит мостом между машинным обучением и производственными системами, хотя некоторые считают его начальным уровнем и более ориентированным на интеграцию инструментов, чем на овладение. Выражаются опасения по поводу долгосрочной актуальности некоторых инструментов и ограниченного внимания курса к качеству данных, однако он считается новой точкой входа для студентов компьютерных наук.

## [Open-R1: открытая репродукция DeepSeek-R1](https://huggingface.co/blog/open-r1)

Open-R1 — это инициатива по воспроизведению DeepSeek-R1, модели рассуждений, сопоставимой с o1 от OpenAI, с акцентом на прозрачность и сотрудничество с открытым исходным кодом. Проект стремится воссоздать наборы данных и тренировочный конвейер DeepSeek-R1, которые в настоящее время не раскрыты, используя обучение с подкреплением (RL) без человеческого контроля. Open-R1 поощряет вклад сообщества для расширения применения модели за пределы математики, включая такие области, как программирование и медицина.

### [Реакции](https://news.ycombinator.com/item?id=42849536)

Open-R1 — это инициатива, направленная на воссоздание модели DeepSeek-R1 с использованием принципов открытого исходного кода, хотя это еще не является фактической моделью. Обсуждение подчеркивает трудности и потенциальные преимущества воспроизведения моделей ИИ при ограниченном бюджете, а также влияние ИИ на образование и более широкие социальные последствия. Разговор также подчеркивает волнение вокруг технологических достижений и роль движения с открытым исходным кодом в том, чтобы сделать ИИ более доступным для широкой аудитории.

## [Будущее Rebble](https://rebble.io/2025/01/27/the-future-of-rebble.html)

### [Реакции](https://news.ycombinator.com/item?id=42845017)

Обсуждение подчеркивает ностальгию по умным часам Pebble, которые ценились за их экраны, похожие на электронные чернила, и длительное время работы от батареи, и задается вопросом, почему подобные технологии не были более широко приняты. Существует интерес к потенциалу нового оборудования от Rebble, проекта, управляемого сообществом, и открытой природе связанных проектов умных часов. Упоминаются альтернативы, такие как Watchy и PineTime, при этом пользователи отмечают программные трудности, с которыми сталкиваются в области смарт-часов с открытым исходным кодом.

## [Миф об альфе: как плененные волки ввели нас в заблуждение](https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves)

### [Реакции](https://news.ycombinator.com/item?id=42844619)

Концепция «альфа-самца» у волков, изначально основанная на исследованиях в неволе, была опровергнута; дикие волчьи стаи функционируют скорее как семейные единицы, а не как иерархические структуры. Несмотря на то, что идея «альфа» была развенчана, она продолжает существовать из-за своей привлекательности в конкурентных средах, таких как Силиконовая долина, и своего отклика на определенные социальные и психологические потребности. Продолжающаяся вера в миф об «альфе» подчеркивает, как нарративы могут влиять на наше восприятие социальных динамик, даже когда они основаны на неверных предположениях.

## [Go 1.24's go tool является одним из лучших дополнений к экосистеме за последние годы](https://www.jvt.me/posts/2025/01/27/go-tools-124/)

Go 1.24 вводит новую команду `go tool` и директиву `tool` в `go.mod`, улучшая управление инструментами проекта в экосистеме Go. Это обновление решает проблемы с шаблоном `tools.go`, такие как влияние на производительность и раздувание дерева зависимостей, позволяя более эффективно управлять инструментами и сокращая ненужные зависимости. Хотя команда `go tool` улучшает производительность за счет кэширования вызовов `go run`, существуют опасения, что зависимости инструментов могут рассматриваться как косвенные, что потенциально может привести к конфликтам зависимостей.

### [Реакции](https://news.ycombinator.com/item?id=42845323)

Введение «go tool» в Go 1.24 вызвало дебаты о его влиянии на управление зависимостями, с опасениями по поводу слияния инструментальных и проектных зависимостей, что может привести к конфликтам. Критики предлагают альтернативы, такие как отдельные файлы модулей или использование инструментов, таких как Nix, для улучшенного контроля версий. Сторонники подхода Go утверждают, что он предлагает простоту и эффективность, отражая более широкие проблемы управления зависимостями в различных языках программирования.

## [Я доверился LLM, теперь у меня уже 4-й день дневного проекта.](https://nemo.foo/blog/day-4-of-an-afternoon-project)

Автор приступил к проекту под названием Deskthang, намереваясь создать настольное устройство с использованием Raspberry Pi Pico, ЖК-дисплея и RGB-светодиодов, одновременно проверяя возможности ИИ. Инструменты ИИ, такие как ChatGPT и Claude, изначально помогали, но в конечном итоге привели к ошибочной реализации, вызывая проблемы, такие как конфликты буфера и повреждение данных. Ключевые уроки включают в себя признание ИИ как инструмента, а не как второго пилота, понимание ценности трений и ошибок в обучении, а также важность терпения над чрезмерной самоуверенностью.

### [Реакции](https://news.ycombinator.com/item?id=42845933)

Большие языковые модели (LLMs) могут быть полезны для простых задач, но могут увеличить сроки выполнения проекта, если полагаться на них для решения сложных проблем без надлежащего контроля. Они эффективно синтезируют информацию, но могут испытывать трудности с узкоспециализированными темами или новыми знаниями, требуя от пользователей наличия прочных основ и опыта. Пользователи должны сохранять контроль, предоставляя четкие запросы и критически оценивая результаты, чтобы эффективно использовать весь потенциал LLM.

## [Компания Nvidia потеряла почти 600 миллиардов долларов в рыночной капитализации.](https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html)

Рыночная капитализация Nvidia понесла исторические убытки почти в 600 миллиардов долларов, акции упали на 17% из-за опасений конкуренции со стороны китайской AI-лаборатории DeepSeek. Распродажа повлияла на более широкий технологический сектор США, вызвав снижение в таких компаниях, как Dell и Oracle, и способствовав падению индекса Nasdaq на 3,1%. Новая модель ИИ от DeepSeek, разработанная с использованием чипов Nvidia H800, усилила опасения по поводу конкуренции, что повлияло на акции Nvidia, несмотря на их предыдущий рост, и сократило чистую стоимость активов генерального директора Дженсена Хуанга на 21 миллиард долларов.

### [Реакции](https://news.ycombinator.com/item?id=42845681)

Рыночная капитализация Nvidia испытала значительное падение почти на 600 миллиардов долларов, что привело к дебатам о стоимости компании и о том, была ли она переоценена. Несмотря на реакцию рынка, графические процессоры Nvidia продолжают быть важными для задач, связанных с искусственным интеллектом, подчеркивая их значимость в технологической индустрии. Сосредоточение СМИ на крупных финансовых потерях без учета инфляции может вводить в заблуждение, но падение Nvidia примечательно даже среди крупных корпораций.

## [Janus Pro 1B работает на 100% локально в браузере на WebGPU](https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/)

### [Реакции](https://news.ycombinator.com/item?id=42852400)

Janus Pro 1B — это модель, работающая локально в браузере с использованием WebGPU, демонстрирующая возможность выполнения моделей ИИ в браузерной среде. Несмотря на небольшое количество параметров, что ограничивает её возможности, модель может работать на маломощных графических процессорах, что подчёркивает её доступность. Хотя результаты генерации изображений непоследовательны, возможность запуска таких моделей локально в браузере является значительным технологическим достижением, хотя в настоящее время она не поддерживает мобильные устройства.

## [Исследователи из Беркли воспроизвели основную технологию DeepSeek R1 всего за $30: небольшая модификация](https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek)

### [Реакции](https://news.ycombinator.com/item?id=42855283)

Исследователи из Беркли успешно воспроизвели основную технологию DeepSeek R1 всего за $30, сосредоточившись на конкретных задачах, таких как игра в Countdown. Инновация заключается в использовании обучения с подкреплением, типа машинного обучения, при котором агент обучается, взаимодействуя со своей средой, для улучшения моделей рассуждений, хотя его применение ограничено областями с проверяемыми решениями. Обсуждение подчеркивает потенциал самоулучшения ИИ и его последствия для будущего развития ИИ, несмотря на критику в адрес вводящего в заблуждение заголовка статьи и отсутствия надлежащих ссылок на источники.

<head>
  <meta property="og:title" content="Мы возвращаем Pebble обратно" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://og.cho.sh/api/og/?title=%D0%9C%D1%8B%20%D0%B2%D0%BE%D0%B7%D0%B2%D1%80%D0%B0%D1%89%D0%B0%D0%B5%D0%BC%20Pebble%20%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE&subheading=%D0%B2%D1%82%D0%BE%D1%80%D0%BD%D0%B8%D0%BA%2C%2028%20%D1%8F%D0%BD%D0%B2%D0%B0%D1%80%D1%8F%202025%20%D0%B3.%3A%20%D0%A1%D0%B2%D0%BE%D0%B4%D0%BA%D0%B0%20%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B5%D0%B9%20Hacker%20News" />
</head>
