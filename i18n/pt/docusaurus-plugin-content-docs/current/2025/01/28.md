---
slug: '/2025/01/28'
---

# 2025-01-28

## [Estamos trazendo o Pebble de volta](https://repebble.com/)

### [Reações](https://news.ycombinator.com/item?id=42845091)

Pebble está sendo revivido com o apoio do Google, focando em suas forças originais, como hackabilidade, longa duração da bateria e servindo como uma extensão do telefone. O renascimento visa manter a natureza de código aberto do Pebble e evitar assinaturas obrigatórias na nuvem, atraindo hackers e entusiastas de tecnologia. A comunidade está animada com o retorno do Pebble, refletindo sobre suas características únicas e influência na tecnologia vestível.

## [Google disponibiliza o código-fonte do Pebble OS](https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html)

### [Reações](https://news.ycombinator.com/item?id=42845070)

Google tornou o Pebble OS de código aberto, gerando entusiasmo entre fãs e desenvolvedores por potenciais novos desenvolvimentos na tecnologia de smartwatches. O lançamento no GitHub não inclui componentes proprietários, como fontes do sistema e a pilha de Bluetooth, portanto, não pode ser compilado em sua forma atual. Este movimento é visto como um gesto positivo da Google, atribuído a esforços internos, e é considerado um passo para reviver o ecossistema do smartwatch Pebble.

## [Execute DeepSeek R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)

### [Reações](https://news.ycombinator.com/item?id=42850222)

DeepSeek R1 Dynamic 1.58-bit alcança uma redução de tamanho de 80% e opera a 140 tokens por segundo usando dois H100s, mas sua lentidão e problemas de repetição levantam questões sobre sua praticidade. Quantização dinâmica ajuda no desempenho, no entanto, preocupações sobre acessibilidade, custo e as alegações de custo de treinamento do modelo persistem, levando a um exame minucioso. O modelo tem um impacto notável no mercado, com esforços em andamento para replicar seus resultados, embora seu desempenho seja debatido em comparação com modelos maiores.

## [Resultados promissores do DeepSeek R1 para código](https://simonwillison.net/2025/Jan/27/llamacpp-pr/)

A pull request (PR) de Xuan-Son Nguyen para o llama.cpp melhora a velocidade do WebAssembly (WASM) usando instruções de Single Instruction, Multiple Data (SIMD), com contribuições significativas de DeekSeek-R1. O PR inclui um model_map dinâmico construído a partir de respostas da API, eliminando a necessidade de versões codificadas, demonstrando inovação no desenvolvimento de plugins. Weblog de Simon Willison também aborda tópicos recentes, como projetos de código aberto, a API de Citações da Anthropic e projetos de Modelos de Linguagem de Grande Escala (LLM), indicando um foco em discussões sobre tecnologia de ponta.

### [Reações](https://news.ycombinator.com/item?id=42852866)

DeepSeek R1 demonstra o potencial da IA na codificação ao escrever 99% de um pull request (PR) para o llama.cpp, destacando o papel crescente da IA no desenvolvimento de software. Ferramentas como o aider são agora responsáveis por gerar 70-82% do novo código em lançamentos, indicando um aumento significativo na produtividade através da assistência de IA. Apesar desses avanços, a IA ainda requer supervisão humana para a resolução de problemas complexos e integração com bases de código existentes, sugerindo uma mudança na dinâmica de trabalho e nos requisitos de habilidades na indústria.

## [O Ilustrado DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

DeepSeek-R1 é um modelo de IA recém-lançado que enfatiza capacidades de raciocínio aprimoradas através de um processo de treinamento estruturado em três etapas: modelagem de linguagem, ajuste fino supervisionado (SFT) e ajuste de preferências. O modelo incorpora longas cadeias de dados de raciocínio, um modelo de raciocínio intermediário e aprendizado por reforço em larga escala (RL), destacando-se em tarefas de raciocínio ao gerar tokens de pensamento. Ele utiliza uma arquitetura de mistura de especialistas, que lhe permite lidar eficientemente com tarefas complexas de raciocínio, marcando um avanço significativo no design de modelos de IA.

### [Reações](https://news.ycombinator.com/item?id=42845488)

DeepSeek-R1 está gerando discussões devido ao seu desempenho e eficiência de custo em comparação com modelos como GPT e Gemini, com alguns usuários observando problemas típicos de grandes modelos de linguagem (LLM). O modelo é notável por seus baixos requisitos de computação e natureza de código aberto, potencialmente disruptivo no cenário de IA e tornando o desenvolvimento de IA mais acessível. Desenvolvido por um fundo de hedge chinês, o DeepSeek-R1 levanta questões sobre seus dados de treinamento e implicações geopolíticas, apesar das críticas mistas sobre suas capacidades de codificação.

## [Aprendizado de Máquina em Produção (Curso CMU)](https://mlip-cmu.github.io/s2025/)

Carnegie Mellon University oferece um curso intitulado "Machine Learning in Production/AI Engineering" para a primavera de 2025, focando na construção, implantação e manutenção de produtos de software habilitados para aprendizado de máquina. O curso enfatiza práticas de IA responsável e MLOps (Operações de Aprendizado de Máquina), cobrindo todo o ciclo de vida, do protótipo à produção. É projetado para estudantes com habilidades em ciência de dados e programação básica, apresentando palestras, laboratórios e um projeto em grupo, com recursos disponíveis no GitHub.

### [Reações](https://news.ycombinator.com/item?id=42847834)

O curso da CMU sobre Aprendizado de Máquina em Produção introduz ferramentas práticas como Kafka, Docker, Kubernetes e Jenkins, enfatizando MLOps (Operações de Aprendizado de Máquina), explicabilidade, justiça e monitoramento. Ele serve como uma ponte entre aprendizado de máquina e sistemas de produção, embora alguns o vejam como de nível básico e mais focado na integração de ferramentas do que no domínio completo. Preocupações são levantadas sobre a relevância a longo prazo de certas ferramentas e a ênfase limitada do curso na qualidade dos dados, ainda assim, é considerado um novo ponto de entrada para estudantes de ciência da computação.

## [Open-R1: uma reprodução aberta do DeepSeek-R1](https://huggingface.co/blog/open-r1)

Open-R1 é uma iniciativa para replicar o DeepSeek-R1, um modelo de raciocínio comparável ao o1 da OpenAI, com foco em transparência e colaboração de código aberto. O projeto busca recriar os conjuntos de dados e o pipeline de treinamento do DeepSeek-R1, que atualmente não são divulgados, utilizando aprendizado por reforço (RL) sem supervisão humana. Open-R1 incentiva contribuições da comunidade para expandir as aplicações do modelo além da matemática, incluindo áreas como programação e medicina.

### [Reações](https://news.ycombinator.com/item?id=42849536)

Open-R1 é uma iniciativa que visa recriar o modelo DeepSeek-R1 usando princípios de código aberto, embora ainda não seja um modelo real. A discussão enfatiza os desafios e os potenciais benefícios de reproduzir modelos de IA com um orçamento limitado, assim como o impacto da IA na educação e as implicações mais amplas para a sociedade. A conversa também destaca a empolgação em torno dos avanços tecnológicos e o papel do movimento de código aberto em tornar a IA mais acessível a um público mais amplo.

## [O futuro do Rebble](https://rebble.io/2025/01/27/the-future-of-rebble.html)

### [Reações](https://news.ycombinator.com/item?id=42845017)

A discussão destaca a nostalgia pelos smartwatches Pebble, apreciados por suas telas semelhantes a e-ink e longa duração da bateria, e questiona por que uma tecnologia semelhante não foi mais amplamente adotada. Há interesse no potencial de novos hardwares da Rebble, um projeto impulsionado pela comunidade, e na natureza de código aberto dos projetos relacionados a smartwatches. Alternativas como Watchy e PineTime são mencionadas, com usuários observando os desafios de software enfrentados no espaço de smartwatches de código aberto.

## [O Mito do Alfa: Como lobos em cativeiro nos desviaram](https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves)

### [Reações](https://news.ycombinator.com/item?id=42844619)

A noção de "macho alfa" em lobos, originalmente baseada em estudos com lobos em cativeiro, foi desmentida; matilhas de lobos selvagens funcionam mais como unidades familiares do que como estruturas hierárquicas. Apesar de ter sido desmentida, a ideia de "alfa" persiste devido ao seu apelo em ambientes competitivos, como o Vale do Silício, e sua ressonância com certas necessidades sociais e psicológicas. Continua a crença no mito do "alfa" destaca como as narrativas podem influenciar nossa percepção das dinâmicas sociais, mesmo quando são baseadas em suposições incorretas.

## [Uma das melhores adições ao ecossistema em anos é a ferramenta go do Go 1.24.](https://www.jvt.me/posts/2025/01/27/go-tools-124/)

Go 1.24 introduz um novo comando `go tool` e uma diretiva `tool` em `go.mod`, aprimorando o gerenciamento de ferramentas de projeto no ecossistema Go. Esta atualização aborda problemas com o padrão `tools.go`, como impactos de desempenho e inchaço da árvore de dependências, permitindo um gerenciamento de ferramentas mais eficiente e reduzindo dependências desnecessárias. Embora o comando `go tool` melhore o desempenho ao armazenar em cache as invocações de `go run`, há preocupações sobre as dependências de ferramentas serem tratadas como indiretas, o que pode levar a conflitos de dependências.

### [Reações](https://news.ycombinator.com/item?id=42845323)

A introdução do 'go tool' no Go 1.24 gerou debates sobre seu impacto no gerenciamento de dependências, com preocupações sobre a fusão de dependências de ferramentas e projetos causando conflitos. Críticos propõem alternativas como arquivos de módulos separados ou o uso de ferramentas como o Nix para um melhor controle de versão. Os defensores da abordagem do Go argumentam que ela oferece simplicidade e eficácia, refletindo desafios mais amplos na gestão de dependências em linguagens de programação.

## [Confiei em um LLM, agora estou no dia 4 de um projeto da tarde](https://nemo.foo/blog/day-4-of-an-afternoon-project)

O autor embarcou em um projeto chamado Deskthang, com a intenção de criar um dispositivo de mesa usando um Raspberry Pi Pico, display LCD e LEDs RGB, enquanto testava as capacidades da IA. Ferramentas de IA como ChatGPT e Claude inicialmente ajudaram, mas acabaram levando a uma implementação com falhas, causando problemas como conflitos de buffer e corrupção de dados. Principais lições aprendidas incluem reconhecer a IA como uma ferramenta em vez de um co-piloto, entender o valor do atrito e dos erros no aprendizado, e a importância da paciência em vez do excesso de confiança.

### [Reações](https://news.ycombinator.com/item?id=42845933)

Modelos de Linguagem de Grande Escala (LLMs) podem ser benéficos para tarefas simples, mas podem prolongar os prazos de projetos se forem utilizados para problemas complexos sem a devida supervisão. São eficazes na síntese de informações, mas podem ter dificuldades com tópicos de nicho ou novos conhecimentos, exigindo que os usuários tenham fundamentos sólidos e experiência. Os usuários devem manter o controle fornecendo instruções claras e revisando criticamente os resultados para aproveitar todo o potencial dos LLMs de forma eficaz.

## [Nvidia perde quase $600 bilhões em valor de mercado](https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html)

Nvidia sofreu uma perda histórica de quase US$ 600 bilhões em valor de mercado, com as ações caindo 17% devido a preocupações de concorrência com o laboratório de IA chinês DeepSeek. O movimento de venda impactou o setor de tecnologia mais amplo dos EUA, causando quedas em empresas como Dell e Oracle, e contribuindo para uma queda de 3,1% no índice Nasdaq. Novo modelo de IA da DeepSeek, desenvolvido com chips H800 da Nvidia, aumentou os temores de concorrência, afetando as ações da Nvidia apesar dos ganhos anteriores, e reduzindo o patrimônio líquido do CEO Jensen Huang em US$ 21 bilhões.

### [Reações](https://news.ycombinator.com/item?id=42845681)

Nvidia viu sua capitalização de mercado cair significativamente em quase $600 bilhões, levando a debates sobre a avaliação da empresa e se ela estava supervalorizada. Apesar da reação do mercado, as GPUs da Nvidia continuam a ser cruciais para tarefas relacionadas à IA, destacando sua importância na indústria de tecnologia. A atenção da mídia em grandes perdas financeiras sem considerar a inflação pode ser enganosa, mas o declínio da Nvidia é notável mesmo entre as grandes corporações.

## [Janus Pro 1B rodando 100% localmente no navegador com WebGPU](https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/)

### [Reações](https://news.ycombinator.com/item?id=42852400)

Janus Pro 1B é um modelo que roda localmente no navegador usando WebGPU, demonstrando a capacidade de executar modelos de IA em um ambiente de navegador. Apesar de seu baixo número de parâmetros, o que limita suas capacidades, o modelo pode rodar em GPUs de baixo desempenho, destacando sua acessibilidade. Embora os resultados de geração de imagens sejam inconsistentes, a capacidade de executar tais modelos localmente em um navegador é um avanço tecnológico significativo, embora atualmente não suporte dispositivos móveis.

## [Pesquisadores de Berkeley replicam a tecnologia central do DeepSeek R1 por apenas $30: uma pequena modificação](https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek)

### [Reações](https://news.ycombinator.com/item?id=42855283)

Pesquisadores de Berkeley replicaram com sucesso a tecnologia central do DeepSeek R1 por apenas $30, concentrando-se em tarefas específicas, como jogar o jogo Countdown. A inovação envolve o uso de aprendizado por reforço, um tipo de aprendizado de máquina onde um agente aprende interagindo com seu ambiente, para aprimorar modelos de raciocínio, embora sua aplicação seja limitada a áreas com soluções verificáveis. A discussão enfatiza o potencial de autoaperfeiçoamento da IA e suas implicações para o desenvolvimento futuro da IA, apesar das críticas ao título enganoso do artigo e à falta de links de fontes adequadas.

<head>
  <meta property="og:title" content="Estamos trazendo o Pebble de volta" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://og.cho.sh/api/og/?title=Estamos%20trazendo%20o%20Pebble%20de%20volta&subheading=ter%C3%A7a-feira%2C%2028%20de%20janeiro%20de%202025%3A%20Resumo%20do%20Hacker%20News" />
</head>
