---
slug: '/2025/01/28'
---

# 2025-01-28

## [Vi tar tillbaka Pebble](https://repebble.com/)

### [Reaktioner](https://news.ycombinator.com/item?id=42845091)

Pebble återupplivas med stöd från Google, med fokus på sina ursprungliga styrkor som hackbarhet, lång batteritid och att fungera som en förlängning av telefonen. Återupplivningen syftar till att behålla Pebbles öppen källkod-natur och undvika obligatoriska molnabonnemang, vilket tilltalar hackare och teknikentusiaster. Gemenskapen är entusiastisk över Pebbles återkomst och reflekterar över dess unika egenskaper och inflytande på bärbar teknik.

## [Google släpper Pebble OS som öppen källkod](https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html)

### [Reaktioner](https://news.ycombinator.com/item?id=42845070)

Google har gjort Pebble OS till öppen källkod, vilket har skapat entusiasm bland fans och utvecklare för potentiella nya utvecklingar inom smartklocksteknik. Utgåvan på GitHub inkluderar inte proprietära komponenter som systemtypsnitt och Bluetooth-stacken, så den kan inte kompileras i sin nuvarande form. Detta drag ses som en positiv gest från Google, tillskrivet interna ansträngningar, och betraktas som ett steg mot att återuppliva Pebble-smartklockans ekosystem.

## [Starta DeepSeek R1 Dynamic 1,58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)

### [Reaktioner](https://news.ycombinator.com/item?id=42850222)

DeepSeek R1 Dynamic 1.58-bit uppnår en 80% storleksreduktion och arbetar med 140 token per sekund med hjälp av dubbla H100s, men dess långsamma hastighet och repetitionsproblem väcker frågor om dess praktiska användbarhet. Dynamic kvantisering hjälper till med prestanda, men oro över tillgänglighet, kostnad och modellens träningskostnadskrav kvarstår, vilket leder till granskning. Modellen har en märkbar inverkan på marknaden, med ansträngningar på gång för att replikera dess resultat, även om dess prestanda debatteras jämfört med större modeller.

## [Lovande resultat från DeepSeek R1 för kod](https://simonwillison.net/2025/Jan/27/llamacpp-pr/)

Ett pull-begäran (PR) av Xuan-Son Nguyen för llama.cpp förbättrar WebAssembly (WASM) hastighet genom att använda Single Instruction, Multiple Data (SIMD) instruktioner, med betydande bidrag från DeekSeek-R1. PR:en inkluderar en dynamisk model_map som byggs från API-svar, vilket eliminerar behovet av hårdkodade versioner och visar på innovation inom pluginutveckling. Simon Willisons Weblog täcker också nyligen ämnen som open source-projekt, Anthropics Citations API och projekt med stora språkmodeller (LLM), vilket indikerar ett fokus på diskussioner om den senaste tekniken.

### [Reaktioner](https://news.ycombinator.com/item?id=42852866)

DeepSeek R1 demonstrerar AI:s potential inom kodning genom att skriva 99% av en pull request (PR) för llama.cpp, vilket visar på AI:s ökande roll inom mjukvaruutveckling. Verktyg som aider står nu för att generera 70-82% av ny kod i utgåvor, vilket indikerar en betydande ökning i produktivitet genom AI-assistans. Trots dessa framsteg kräver AI fortfarande mänsklig övervakning för komplex problemlösning och integration med befintliga kodbaser, vilket tyder på en förändring i jobbdynamik och kompetenskrav inom branschen.

## [Den illustrerade DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

DeepSeek-R1 är en nyligen släppt AI-modell som betonar förbättrade resonemangsförmågor genom en strukturerad trestegs träningsprocess: språkmodellering, övervakad finjustering (SFT) och preferensjustering. Modellen integrerar långa resonemangskedjor, en interimistisk resonemangsmodell och storskalig förstärkningsinlärning (RL), och utmärker sig i resonemangsuppgifter genom att generera tänkande tokens. Den använder en blandning-av-experter-arkitektur, vilket gör det möjligt för den att effektivt hantera komplexa resonemangsuppgifter, vilket markerar ett betydande framsteg i designen av AI-modeller.

### [Reaktioner](https://news.ycombinator.com/item?id=42845488)

DeepSeek-R1 väcker diskussion på grund av sin prestanda och kostnadseffektivitet jämfört med modeller som GPT och Gemini, där vissa användare noterar typiska problem med stora språkmodeller (LLM). Modellen är anmärkningsvärd för sina låga beräkningskrav och sin öppen källkod, vilket potentiellt kan störa AI-landskapet och göra AI-utveckling mer tillgänglig. Utvecklad av en kinesisk hedgefond, väcker DeepSeek-R1 frågor om dess träningsdata och geopolitiska implikationer, trots blandade recensioner om dess kodningsförmågor.

## [Maskininlärning i produktion (CMU-kurs)](https://mlip-cmu.github.io/s2025/)

Carnegie Mellon University erbjuder en kurs med titeln "Machine Learning in Production/AI Engineering" för våren 2025, med fokus på att bygga, distribuera och underhålla mjukvaruprodukter med maskininlärning. Kursen betonar ansvarsfulla AI-praktiker och MLOps (Machine Learning Operations), och täcker hela livscykeln från prototyp till produktion. Den är utformad för studenter med datavetenskap och grundläggande programmeringskunskaper, och innehåller föreläsningar, laborationer och ett grupprojekt, med resurser tillgängliga på GitHub.

### [Reaktioner](https://news.ycombinator.com/item?id=42847834)

CMU-kursen om maskininlärning i produktion introducerar praktiska verktyg som Kafka, Docker, Kubernetes och Jenkins, med betoning på MLOps (Machine Learning Operations), förklarbarhet, rättvisa och övervakning. Det fungerar som en brygga mellan maskininlärning och produktionssystem, även om vissa ser det som en nybörjarnivå och mer fokuserat på verktygsintegration än på mästerskap. Det finns oro över de långsiktiga relevansen av vissa verktyg och kursens begränsade fokus på datakvalitet, men den betraktas ändå som en ny ingångspunkt för datavetenskapsstudenter.

## [Open-R1: en öppen reproduktion av DeepSeek-R1](https://huggingface.co/blog/open-r1)

Open-R1 är ett initiativ för att replikera DeepSeek-R1, en resonemangsmodell jämförbar med OpenAI:s o1, med fokus på transparens och öppen källkodssamarbete. Projektet syftar till att återskapa DeepSeek-R1:s dataset och träningspipeline, som för närvarande är hemliga, genom att använda förstärkningsinlärning (RL) utan mänsklig övervakning. Open-R1 uppmuntrar gemenskapsbidrag för att utöka modellens tillämpningar bortom matematik, inklusive områden som kodning och medicin.

### [Reaktioner](https://news.ycombinator.com/item?id=42849536)

Open-R1 är ett initiativ som syftar till att återskapa DeepSeek-R1-modellen med hjälp av öppen källkod-principer, även om det ännu inte är en faktisk modell. Diskussionen betonar utmaningarna och de potentiella fördelarna med att reproducera AI-modeller med en begränsad budget, samt AI:s påverkan på utbildning och bredare samhälleliga konsekvenser. Konversationen belyser också entusiasmen kring teknologiska framsteg och den roll som open-source-rörelsen spelar för att göra AI mer tillgänglig för en bredare publik.

## [Framtiden för Rebble](https://rebble.io/2025/01/27/the-future-of-rebble.html)

### [Reaktioner](https://news.ycombinator.com/item?id=42845017)

Diskussionen lyfter fram nostalgi för Pebble-smartklockor, uppskattade för sina e-ink-liknande skärmar och långa batteritid, och ifrågasätter varför liknande teknik inte har blivit mer allmänt antagen. Det finns intresse för potentialen för ny hårdvara från Rebble, ett community-drivet projekt, och den öppna källkoden för relaterade smartklockprojekt. Alternativ som Watchy och PineTime nämns, där användare noterar de mjukvaruutmaningar som finns inom området för öppen källkod smartklockor.

## [Alfamyt: Hur fångenskap av vargar vilseledde oss](https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves)

### [Reaktioner](https://news.ycombinator.com/item?id=42844619)

Konceptet "alfahanne" hos vargar, som ursprungligen baserades på studier av vargar i fångenskap, har blivit motbevisat; vilda vargflockar fungerar mer som familjeenheter snarare än hierarkiska strukturer. Trots att den har blivit motbevisad, fortsätter idén om "alpha" att existera på grund av dess dragningskraft i konkurrensutsatta miljöer, såsom Silicon Valley, och dess resonans med vissa samhälleliga och psykologiska behov. Den fortsatta tron på "alfa"-myten understryker hur berättelser kan påverka vår uppfattning om sociala dynamiker, även när de är grundade på felaktiga antaganden.

## [Go 1.24:s go-verktyg är en av de bästa tilläggen till ekosystemet på flera år](https://www.jvt.me/posts/2025/01/27/go-tools-124/)

Go 1.24 introducerar ett nytt `go tool`-kommando och `tool`-direktiv i `go.mod`, vilket förbättrar hanteringen av projektverktyg i Go-ekosystemet. Denna uppdatering åtgärdar problem med `tools.go`-mönstret, såsom prestandapåverkan och uppblåsta beroendeträd, genom att möjliggöra mer effektiv verktygshantering och minska onödiga beroenden. Även om kommandot `go tool` förbättrar prestandan genom att cachelagra anrop av `go run`, finns det oro över att verktygsberoenden behandlas som indirekta, vilket potentiellt kan leda till beroendekonflikter.

### [Reaktioner](https://news.ycombinator.com/item?id=42845323)

Införandet av "go tool" i Go 1.24 har lett till debatter om dess påverkan på beroendehantering, med oro för att sammanslagning av verktygs och projektberoenden kan orsaka konflikter. Kritiker föreslår alternativ som separata modulfiler eller att använda verktyg som Nix för förbättrad versionskontroll. Förespråkare av Go:s metod hävdar att den erbjuder enkelhet och effektivitet, vilket speglar bredare utmaningar inom beroendehantering över olika programmeringsspråk.

## [Jag litade på en LLM, nu är jag på dag 4 av ett eftermiddagsprojekt](https://nemo.foo/blog/day-4-of-an-afternoon-project)

Författaren påbörjade ett projekt kallat Deskthang, med avsikt att skapa en skrivbordsapparat med hjälp av en Raspberry Pi Pico, LCD-skärm och RGB-LEDs, samtidigt som AI:s kapaciteter testades. AI-verktyg som ChatGPT och Claude hjälpte initialt men ledde slutligen till en buggig implementering, vilket orsakade problem som buffertkonflikter och datakorruption. Viktiga lärdomar inkluderar att se AI som ett verktyg snarare än en medpilot, att förstå värdet av friktion och misstag i lärande, samt vikten av tålamod över överdrivet självförtroende.

### [Reaktioner](https://news.ycombinator.com/item?id=42845933)

Stora språkmodeller (LLM) kan vara fördelaktiga för enkla uppgifter men kan förlänga projekttidslinjer om de används för komplexa problem utan ordentlig övervakning. De är effektiva på att syntetisera information men kan ha svårt med nischämnen eller ny kunskap, vilket kräver att användare har starka grundläggande kunskaper och erfarenhet. Användare måste behålla kontrollen genom att ge tydliga instruktioner och kritiskt granska resultat för att effektivt utnyttja LLM:ers fulla potential.

## [Nvidia tappar nästan 600 miljarder dollar i börsvärde](https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html)

Nvidias börsvärde drabbades av en historisk förlust på nästan 600 miljarder dollar, med aktier som föll med 17% på grund av konkurrensbekymmer från det kinesiska AI-labbet DeepSeek. Utförsäljningen påverkade den bredare amerikanska tekniksektorn, vilket orsakade nedgångar i företag som Dell och Oracle, och bidrog till en nedgång på 3,1 % i Nasdaq-indexet. DeepSeeks nya AI-modell, utvecklad med hjälp av Nvidias H800-chip, har ökat konkurrensrädslan, vilket påverkar Nvidias aktie trots dess tidigare vinster och minskar VD Jensen Huangs förmögenhet med 21 miljarder dollar.

### [Reaktioner](https://news.ycombinator.com/item?id=42845681)

Nvidias börsvärde upplevde en betydande minskning på nästan 600 miljarder dollar, vilket ledde till debatter om företagets värdering och huruvida det var övervärderat. Trots marknadsreaktionen fortsätter Nvidias GPU:er att vara avgörande för AI-relaterade uppgifter, vilket understryker deras betydelse inom teknikindustrin. Mediernas fokus på stora finansiella förluster utan att ta hänsyn till inflation kan vara vilseledande, men Nvidias nedgång är anmärkningsvärd även bland stora företag.

## [Janus Pro 1B körs 100% lokalt i webbläsaren på WebGPU](https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/)

### [Reaktioner](https://news.ycombinator.com/item?id=42852400)

Janus Pro 1B är en modell som körs lokalt i webbläsaren med hjälp av WebGPU, vilket visar på förmågan att köra AI-modeller i en webbläsarmiljö. Trots sitt låga antal parametrar, vilket begränsar dess kapacitet, kan modellen köras på enklare GPU:er, vilket understryker dess tillgänglighet. Även om resultaten för bildgenerering är inkonsekventa, är förmågan att köra sådana modeller lokalt i en webbläsare en betydande teknologisk framsteg, även om den för närvarande inte stöder mobila enheter.

## [Berkeley-forskare replikerar DeepSeek R1:s kärnteknik för bara 30 dollar: En liten modifiering](https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek)

### [Reaktioner](https://news.ycombinator.com/item?id=42855283)

Forskare vid Berkeley har framgångsrikt replikerat DeepSeek R1:s kärnteknologi för endast 30 dollar, med fokus på specifika uppgifter som att spela spelet Countdown. Nyheten innebär att använda förstärkningsinlärning, en typ av maskininlärning där en agent lär sig genom att interagera med sin miljö, för att förbättra resonemangsmodeller, även om dess tillämpning är begränsad till områden med verifierbara lösningar. Diskussionen betonar potentialen för AI:s självförbättring och dess konsekvenser för framtida AI-utveckling, trots kritik mot artikelns missvisande titel och brist på korrekta källhänvisningar.

<head>
  <meta property="og:title" content="Vi tar tillbaka Pebble" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://og.cho.sh/api/og/?title=Vi%20tar%20tillbaka%20Pebble&subheading=tisdag%2028%20januari%202025%3A%20Sammanfattning%20av%20Hacker%20News" />
</head>
