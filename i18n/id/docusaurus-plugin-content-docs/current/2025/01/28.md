---
slug: '/2025/01/28'
---

# 2025-01-28

## [Kami mengembalikan Pebble](https://repebble.com/)

### [Reaksi](https://news.ycombinator.com/item?id=42845091)

Pebble sedang dihidupkan kembali dengan dukungan dari Google, berfokus pada kekuatan aslinya seperti kemampuan untuk di-hack, daya tahan baterai yang lama, dan berfungsi sebagai perpanjangan dari ponsel. Revitalisasi ini bertujuan untuk mempertahankan sifat open-source Pebble dan menghindari langganan cloud wajib, menarik bagi para peretas dan penggemar teknologi. Komunitas sangat antusias dengan kembalinya Pebble, merenungkan fitur uniknya dan pengaruhnya terhadap teknologi yang dapat dikenakan.

## [Google merilis kode sumber terbuka untuk Pebble OS](https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html)

### [Reaksi](https://news.ycombinator.com/item?id=42845070)

Google telah membuka sumber kode Pebble OS, yang memicu antusiasme di kalangan penggemar dan pengembang untuk potensi pengembangan baru dalam teknologi jam tangan pintar. Tidak termasuk komponen proprietary seperti font sistem dan stack Bluetooth dalam rilis di GitHub, sehingga tidak dapat dikompilasi dalam bentuknya saat ini. Langkah ini dipandang sebagai isyarat positif dari Google, yang dikaitkan dengan upaya internal, dan dianggap sebagai langkah menuju menghidupkan kembali ekosistem jam tangan pintar Pebble.

## [Jalankan DeepSeek R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)

### [Reaksi](https://news.ycombinator.com/item?id=42850222)

DeepSeek R1 Dynamic 1.58-bit mencapai pengurangan ukuran sebesar 80% dan beroperasi pada 140 token per detik menggunakan dual H100s, tetapi kecepatan yang lambat dan masalah pengulangan menimbulkan pertanyaan tentang kepraktisannya. Kuantisasi dinamis membantu dalam kinerja, namun kekhawatiran tentang aksesibilitas, biaya, dan klaim biaya pelatihan model tetap ada, yang mengarah pada pengawasan. Tidak dapat dipungkiri bahwa model ini memiliki dampak yang signifikan di pasar, dengan upaya yang sedang dilakukan untuk mereplikasi hasilnya, meskipun kinerjanya masih diperdebatkan jika dibandingkan dengan model yang lebih besar.

## [Menjanjikan hasil dari DeepSeek R1 untuk kode](https://simonwillison.net/2025/Jan/27/llamacpp-pr/)

Sebuah permintaan penarikan (PR) oleh Xuan-Son Nguyen untuk llama.cpp meningkatkan kecepatan WebAssembly (WASM) menggunakan instruksi Single Instruction, Multiple Data (SIMD), dengan kontribusi signifikan dari DeekSeek-R1. Teks PR mencakup model_map dinamis yang dibangun dari respons API, menghilangkan kebutuhan untuk versi yang dikodekan secara manual, menunjukkan inovasi dalam pengembangan plugin. Weblog Simon Willison juga membahas topik terbaru seperti proyek sumber terbuka, API Citations dari Anthropic, dan proyek Model Bahasa Besar (LLM), menunjukkan fokus pada diskusi teknologi mutakhir.

### [Reaksi](https://news.ycombinator.com/item?id=42852866)

DeepSeek R1 menunjukkan potensi AI dalam pemrograman dengan menulis 99% dari permintaan penarikan (PR) untuk llama.cpp, memperlihatkan peran AI yang semakin meningkat dalam pengembangan perangkat lunak. Alat seperti aider sekarang bertanggung jawab untuk menghasilkan 70-82% kode baru dalam rilis, menunjukkan peningkatan produktivitas yang signifikan melalui bantuan AI. Meskipun ada kemajuan ini, AI masih memerlukan pengawasan manusia untuk pemecahan masalah yang kompleks dan integrasi dengan basis kode yang ada, yang menunjukkan adanya pergeseran dalam dinamika pekerjaan dan persyaratan keterampilan di industri.

## [The Illustrated DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

DeepSeek-R1 adalah model AI yang baru dirilis yang menekankan kemampuan penalaran yang ditingkatkan melalui proses pelatihan tiga langkah yang terstruktur: pemodelan bahasa, penyetelan halus yang diawasi (SFT), dan penyetelan preferensi. Model ini menggabungkan rantai data penalaran yang panjang, model penalaran sementara, dan pembelajaran penguatan skala besar (RL), unggul dalam tugas penalaran dengan menghasilkan token berpikir. Model ini memanfaatkan arsitektur campuran ahli, yang memungkinkannya menangani tugas penalaran yang kompleks secara efisien, menandai kemajuan signifikan dalam desain model AI.

### [Reaksi](https://news.ycombinator.com/item?id=42845488)

DeepSeek-R1 sedang menjadi bahan diskusi karena kinerja dan efisiensi biayanya dibandingkan dengan model seperti GPT dan Gemini, dengan beberapa pengguna mencatat masalah khas model bahasa besar (LLM). Model ini terkenal karena kebutuhan komputasinya yang rendah dan sifatnya yang open-source, yang berpotensi mengganggu lanskap AI dan membuat pengembangan AI lebih mudah diakses. Dikembangkan oleh sebuah hedge fund Tiongkok, DeepSeek-R1 menimbulkan pertanyaan tentang data pelatihannya dan implikasi geopolitik, meskipun ulasan tentang kemampuan pengkodeannya beragam.

## [Pembelajaran Mesin dalam Produksi (Kursus CMU)](https://mlip-cmu.github.io/s2025/)

Carnegie Mellon University menawarkan kursus berjudul "Machine Learning in Production/AI Engineering" untuk Musim Semi 2025, yang berfokus pada pembangunan, penerapan, dan pemeliharaan produk perangkat lunak yang didukung pembelajaran mesin. Kursus ini menekankan praktik AI yang bertanggung jawab dan MLOps (Operasi Pembelajaran Mesin), mencakup seluruh siklus hidup dari prototipe hingga produksi. Kursus ini dirancang untuk siswa dengan keterampilan ilmu data dan pemrograman dasar, menampilkan kuliah, laboratorium, dan proyek kelompok, dengan sumber daya tersedia di GitHub.

### [Reaksi](https://news.ycombinator.com/item?id=42847834)

Course CMU tentang Machine Learning dalam Produksi memperkenalkan alat praktis seperti Kafka, Docker, Kubernetes, dan Jenkins, dengan penekanan pada MLOps (Operasi Pembelajaran Mesin), keterjelasan, keadilan, dan pemantauan. Itu berfungsi sebagai jembatan antara pembelajaran mesin dan sistem produksi, meskipun beberapa orang melihatnya sebagai tingkat pemula dan lebih berfokus pada integrasi alat daripada penguasaan. Keprihatinan muncul mengenai relevansi jangka panjang dari alat-alat tertentu dan penekanan terbatas pada kualitas data dalam kursus tersebut, namun kursus ini dianggap sebagai titik masuk baru bagi mahasiswa ilmu komputer.

## [Open-R1: sebuah reproduksi terbuka dari DeepSeek-R1](https://huggingface.co/blog/open-r1)

Open-R1 adalah inisiatif untuk mereplikasi DeepSeek-R1, sebuah model penalaran yang sebanding dengan o1 milik OpenAI, dengan fokus pada transparansi dan kolaborasi sumber terbuka. Tugas proyek ini adalah untuk mereplikasi dataset dan jalur pelatihan DeepSeek-R1, yang saat ini tidak diungkapkan, dengan menggunakan pembelajaran penguatan (RL) tanpa pengawasan manusia. Open-R1 mendorong kontribusi komunitas untuk memperluas aplikasi model ini di luar matematika, termasuk bidang seperti pemrograman dan kedokteran.

### [Reaksi](https://news.ycombinator.com/item?id=42849536)

Open-R1 adalah inisiatif yang bertujuan untuk menciptakan kembali model DeepSeek-R1 dengan menggunakan prinsip-prinsip sumber terbuka, meskipun saat ini belum menjadi model yang sebenarnya. Teks tersebut menekankan tantangan dan potensi manfaat dari mereproduksi model AI dengan anggaran terbatas, serta dampak AI terhadap pendidikan dan implikasi sosial yang lebih luas. Percakapan tersebut juga menyoroti kegembiraan seputar kemajuan teknologi dan peran gerakan sumber terbuka dalam membuat AI lebih dapat diakses oleh khalayak yang lebih luas.

## [Masadepan Rebble](https://rebble.io/2025/01/27/the-future-of-rebble.html)

### [Reaksi](https://news.ycombinator.com/item?id=42845017)

Teks tersebut menyoroti nostalgia terhadap jam tangan pintar Pebble, yang dihargai karena layar mirip e-ink dan masa pakai baterai yang lama, serta mempertanyakan mengapa teknologi serupa belum diadopsi secara lebih luas. Terdapat minat terhadap potensi perangkat keras baru dari Rebble, sebuah proyek yang digerakkan oleh komunitas, dan sifat sumber terbuka dari proyek smartwatch terkait. Alternatif seperti Watchy dan PineTime disebutkan, dengan pengguna mencatat tantangan perangkat lunak yang dihadapi dalam ruang jam tangan pintar sumber terbuka.

## [Mitologi Alpha: Bagaimana serigala yang ditawan menyesatkan kita](https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves)

### [Reaksi](https://news.ycombinator.com/item?id=42844619)

Teks 'alpha male' dalam serigala, yang awalnya didasarkan pada studi di penangkaran, telah dibantah; kawanan serigala liar berfungsi lebih seperti unit keluarga daripada struktur hierarkis. Meskipun telah dibantah, gagasan "alpha" tetap bertahan karena daya tariknya di lingkungan yang kompetitif, seperti Silicon Valley, dan resonansinya dengan kebutuhan sosial dan psikologis tertentu. Tetap adanya kepercayaan pada mitos "alpha" menyoroti bagaimana narasi dapat mempengaruhi persepsi kita terhadap dinamika sosial, bahkan ketika narasi tersebut didasarkan pada asumsi yang salah.

## [Alat go 1.24 adalah salah satu tambahan terbaik untuk ekosistem dalam beberapa tahun terakhir.](https://www.jvt.me/posts/2025/01/27/go-tools-124/)

Go 1.24 memperkenalkan perintah `go tool` baru dan direktif `tool` dalam `go.mod`, yang meningkatkan pengelolaan alat proyek dalam ekosistem Go. Pembaharuan ini mengatasi masalah dengan pola `tools.go`, seperti dampak kinerja dan pembengkakan pohon ketergantungan, dengan memungkinkan pengelolaan alat yang lebih efisien dan mengurangi ketergantungan yang tidak perlu. Walaupun perintah `go tool` meningkatkan kinerja dengan menyimpan cache pemanggilan `go run`, ada kekhawatiran tentang ketergantungan alat yang diperlakukan sebagai tidak langsung, yang berpotensi menyebabkan bentrokan ketergantungan.

### [Reaksi](https://news.ycombinator.com/item?id=42845323)

Pengantar "go tool" dalam Go 1.24 telah memicu perdebatan tentang dampaknya terhadap manajemen dependensi, dengan kekhawatiran tentang penggabungan alat dan dependensi proyek yang dapat menyebabkan konflik. Kritikus mengusulkan alternatif seperti file modul terpisah atau menggunakan alat seperti Nix untuk kontrol versi yang lebih baik. Pendukung pendekatan Go berpendapat bahwa pendekatan ini menawarkan kesederhanaan dan efektivitas, mencerminkan tantangan yang lebih luas dalam manajemen ketergantungan di berbagai bahasa pemrograman.

## [Saya mempercayai sebuah LLM, sekarang saya sudah memasuki hari ke-4 dari proyek sore hari.](https://nemo.foo/blog/day-4-of-an-afternoon-project)

Penulis memulai sebuah proyek bernama Deskthang, dengan tujuan untuk membuat perangkat meja menggunakan Raspberry Pi Pico, layar LCD, dan LED RGB, sambil menguji kemampuan AI. Alat AI seperti ChatGPT dan Claude awalnya membantu tetapi pada akhirnya menyebabkan implementasi yang bermasalah, menimbulkan masalah seperti konflik buffer dan kerusakan data. Pelajaran penting yang dipetik meliputi pengakuan AI sebagai alat daripada sebagai co-pilot, memahami nilai dari gesekan dan kesalahan dalam pembelajaran, serta pentingnya kesabaran dibandingkan dengan rasa percaya diri yang berlebihan.

### [Reaksi](https://news.ycombinator.com/item?id=42845933)

Model Bahasa Besar (LLM) dapat bermanfaat untuk tugas-tugas sederhana tetapi dapat memperpanjang waktu proyek jika diandalkan untuk masalah kompleks tanpa pengawasan yang tepat. Mereka efektif dalam mensintesis informasi tetapi mungkin kesulitan dengan topik khusus atau pengetahuan baru, sehingga memerlukan pengguna untuk memiliki dasar yang kuat dan pengalaman. Pengguna harus mempertahankan kontrol dengan memberikan petunjuk yang jelas dan meninjau keluaran secara kritis untuk memanfaatkan potensi penuh LLM secara efektif.

## [Nvidia kehilangan hampir $600 miliar dalam kapitalisasi pasar](https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html)

Nvidia mengalami kerugian bersejarah dalam kapitalisasi pasar hampir $600 miliar, dengan saham turun 17% karena kekhawatiran persaingan dari laboratorium AI China, DeepSeek. Penjualan tersebut mempengaruhi sektor teknologi AS yang lebih luas, menyebabkan penurunan pada perusahaan seperti Dell dan Oracle, dan berkontribusi pada penurunan 3,1% pada indeks Nasdaq. Model AI baru DeepSeek, yang dikembangkan menggunakan chip H800 dari Nvidia, telah meningkatkan kekhawatiran persaingan, mempengaruhi saham Nvidia meskipun sebelumnya mengalami kenaikan, dan mengurangi kekayaan bersih CEO Jensen Huang sebesar $21 miliar.

### [Reaksi](https://news.ycombinator.com/item?id=42845681)

Nvidia mengalami penurunan kapitalisasi pasar yang signifikan hampir $600 miliar, yang memicu perdebatan tentang valuasi perusahaan dan apakah itu dinilai terlalu tinggi. Meskipun reaksi pasar, GPU Nvidia tetap menjadi krusial untuk tugas-tugas terkait AI, menekankan pentingnya mereka dalam industri teknologi. Fokus media pada kerugian finansial besar tanpa mempertimbangkan inflasi dapat menyesatkan, tetapi penurunan Nvidia tetap menonjol bahkan di antara perusahaan-perusahaan besar.

## [Janus Pro 1B berjalan 100% secara lokal di peramban menggunakan WebGPU](https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/)

### [Reaksi](https://news.ycombinator.com/item?id=42852400)

Janus Pro 1B adalah model yang berjalan secara lokal di peramban menggunakan WebGPU, menunjukkan kemampuan menjalankan model AI di lingkungan peramban. Meskipun jumlah parameternya rendah, yang membatasi kemampuannya, model ini dapat berjalan pada GPU kelas bawah, menyoroti aksesibilitasnya. Meskipun hasil generasi gambar tidak konsisten, kemampuan untuk menjalankan model semacam itu secara lokal di peramban adalah kemajuan teknologi yang signifikan, meskipun saat ini tidak mendukung perangkat seluler.

## [Peneliti Berkeley Mereplikasi Teknologi Inti DeepSeek R1 Hanya dengan $30: Sebuah Modifikasi Kecil](https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek)

### [Reaksi](https://news.ycombinator.com/item?id=42855283)

Para peneliti Berkeley telah berhasil mereplikasi teknologi inti DeepSeek R1 hanya dengan biaya $30, dengan fokus pada tugas-tugas spesifik seperti memainkan permainan Countdown. Teks tersebut membahas inovasi yang melibatkan penggunaan pembelajaran penguatan, sebuah jenis pembelajaran mesin di mana agen belajar dengan berinteraksi dengan lingkungannya, untuk meningkatkan model penalaran, meskipun penerapannya terbatas pada area dengan solusi yang dapat diverifikasi. Diskusi menekankan potensi peningkatan diri AI dan implikasinya untuk pengembangan AI di masa depan, meskipun ada kritik terhadap judul artikel yang menyesatkan dan kurangnya tautan sumber yang tepat.

<head>
  <meta property="og:title" content="Kami mengembalikan Pebble" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://og.cho.sh/api/og/?title=Kami%20mengembalikan%20Pebble&subheading=Selasa%2C%2028%20Januari%202025%3A%20Ringkasan%20Berita%20Peretas" />
</head>
