---
slug: '/2025/01/28'
---

# 2025-01-28

## [Chúng tôi đang mang Pebble trở lại](https://repebble.com/)

### [phản ứng](https://news.ycombinator.com/item?id=42845091)

Pebble đang được hồi sinh với sự hỗ trợ từ Google, tập trung vào những điểm mạnh ban đầu như khả năng tùy biến, thời lượng pin dài và hoạt động như một phần mở rộng của điện thoại. Việc hồi sinh nhằm duy trì tính chất mã nguồn mở của Pebble và tránh các đăng ký đám mây bắt buộc, thu hút các hacker và những người đam mê công nghệ. Người dùng đang rất hào hứng với sự trở lại của Pebble, nhớ lại những tính năng độc đáo và ảnh hưởng của nó đối với công nghệ đeo tay.

## [Google mở mã nguồn hệ điều hành Pebble](https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html)

### [phản ứng](https://news.ycombinator.com/item?id=42845070)

Google đã mã nguồn mở hệ điều hành Pebble, tạo ra sự hào hứng trong cộng đồng người hâm mộ và các nhà phát triển về những phát triển tiềm năng mới trong công nghệ đồng hồ thông minh. Phiên bản phát hành trên GitHub không bao gồm các thành phần độc quyền như phông chữ hệ thống và ngăn xếp Bluetooth, vì vậy nó không thể được biên dịch ở dạng hiện tại. Động thái này được coi là một cử chỉ tích cực từ Google, được cho là nhờ vào những nỗ lực nội bộ, và được xem như một bước tiến để hồi sinh hệ sinh thái đồng hồ thông minh Pebble.

## [Chạy DeepSeek R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)

### [phản ứng](https://news.ycombinator.com/item?id=42850222)

DeepSeek R1 Dynamic 1.58-bit đạt được giảm kích thước 80% và hoạt động ở tốc độ 140 token mỗi giây khi sử dụng hai H100, nhưng tốc độ chậm và vấn đề lặp lại đặt ra câu hỏi về tính thực tiễn của nó. Định lượng động hỗ trợ hiệu suất, tuy nhiên những lo ngại về khả năng tiếp cận, chi phí và các tuyên bố về chi phí đào tạo của mô hình vẫn tồn tại, dẫn đến sự giám sát chặt chẽ. Model này có tác động đáng kể đến thị trường, với những nỗ lực đang được tiến hành để tái tạo kết quả của nó, mặc dù hiệu suất của nó đang được tranh luận so với các mô hình lớn hơn.

## [Những kết quả đầy hứa hẹn từ DeepSeek R1 cho mã](https://simonwillison.net/2025/Jan/27/llamacpp-pr/)

Một yêu cầu kéo (PR) của Xuan-Son Nguyen cho llama.cpp cải thiện tốc độ WebAssembly (WASM) bằng cách sử dụng các lệnh Đơn Hướng Dẫn, Dữ Liệu Đa (SIMD), với những đóng góp đáng kể từ DeekSeek-R1. PR bao gồm một model_map động được xây dựng từ các phản hồi API, loại bỏ sự cần thiết của các phiên bản mã hóa cứng, thể hiện sự đổi mới trong phát triển plugin. Weblog của Simon Willison cũng đề cập đến các chủ đề gần đây như các dự án mã nguồn mở, API Citations của Anthropic, và các dự án Mô hình Ngôn ngữ Lớn (LLM), cho thấy sự tập trung vào các cuộc thảo luận về công nghệ tiên tiến.

### [phản ứng](https://news.ycombinator.com/item?id=42852866)

DeepSeek R1 thể hiện tiềm năng của AI trong lập trình bằng cách viết 99% của một yêu cầu kéo (PR) cho llama.cpp, cho thấy vai trò ngày càng tăng của AI trong phát triển phần mềm. Những công cụ như aider hiện nay chịu trách nhiệm tạo ra 70-82% mã mới trong các bản phát hành, cho thấy một sự gia tăng đáng kể về năng suất thông qua sự hỗ trợ của AI. Mặc dù có những tiến bộ này, AI vẫn cần sự giám sát của con người đối với việc giải quyết các vấn đề phức tạp và tích hợp với các cơ sở mã hiện có, điều này cho thấy sự thay đổi trong động lực công việc và yêu cầu kỹ năng trong ngành.

## [Minh họa DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

DeepSeek-R1 là một mô hình AI mới được phát hành, nhấn mạnh khả năng suy luận nâng cao thông qua quy trình đào tạo ba bước có cấu trúc: mô hình hóa ngôn ngữ, tinh chỉnh có giám sát (SFT) và điều chỉnh ưu tiên. Mô hình này tích hợp các chuỗi dữ liệu suy luận dài, một mô hình suy luận tạm thời và học tăng cường quy mô lớn (RL), xuất sắc trong các nhiệm vụ suy luận bằng cách tạo ra các token suy nghĩ. Nó sử dụng kiến trúc hỗn hợp chuyên gia, cho phép xử lý hiệu quả các nhiệm vụ suy luận phức tạp, đánh dấu một bước tiến quan trọng trong thiết kế mô hình AI.

### [phản ứng](https://news.ycombinator.com/item?id=42845488)

DeepSeek-R1 đang tạo ra nhiều cuộc thảo luận do hiệu suất và hiệu quả chi phí của nó so với các mô hình như GPT và Gemini, với một số người dùng lưu ý các vấn đề điển hình của mô hình ngôn ngữ lớn (LLM). Mô hình này nổi bật với yêu cầu tính toán thấp và tính chất mã nguồn mở, có khả năng làm gián đoạn cảnh quan AI và làm cho việc phát triển AI trở nên dễ tiếp cận hơn. Được phát triển bởi một quỹ đầu tư mạo hiểm Trung Quốc, DeepSeek-R1 đặt ra câu hỏi về dữ liệu đào tạo của nó và các tác động địa chính trị, mặc dù có những đánh giá trái chiều về khả năng mã hóa của nó.

## [Máy học trong sản xuất (Khóa học CMU)](https://mlip-cmu.github.io/s2025/)

Đại học Carnegie Mellon cung cấp một khóa học có tiêu đề "Học Máy trong Sản Xuất/Kỹ Thuật AI" cho mùa xuân năm 2025, tập trung vào việc xây dựng, triển khai và duy trì các sản phẩm phần mềm có khả năng học máy. Khóa học nhấn mạnh các thực hành AI có trách nhiệm và MLOps (Vận hành Học Máy), bao quát toàn bộ vòng đời từ nguyên mẫu đến sản xuất. Nó được thiết kế cho sinh viên có kỹ năng khoa học dữ liệu và lập trình cơ bản, bao gồm các bài giảng, phòng thí nghiệm và một dự án nhóm, với các tài nguyên có sẵn trên GitHub.

### [phản ứng](https://news.ycombinator.com/item?id=42847834)

Khóa học về Machine Learning trong Sản xuất của CMU giới thiệu các công cụ thực tiễn như Kafka, Docker, Kubernetes và Jenkins, nhấn mạnh vào MLOps (Vận hành Máy học), khả năng giải thích, tính công bằng và giám sát. Đó là cầu nối giữa học máy và các hệ thống sản xuất, mặc dù một số người coi nó là cấp độ đầu vào và tập trung nhiều hơn vào tích hợp công cụ hơn là sự thành thạo. Những lo ngại được nêu ra về sự liên quan lâu dài của một số công cụ và sự nhấn mạnh hạn chế của khóa học về chất lượng dữ liệu, tuy nhiên nó được coi là một điểm khởi đầu mới cho sinh viên khoa học máy tính.

## [Open-R1: một bản tái tạo mở của DeepSeek-R1](https://huggingface.co/blog/open-r1)

Open-R1 là một sáng kiến nhằm tái tạo DeepSeek-R1, một mô hình suy luận tương đương với o1 của OpenAI, tập trung vào tính minh bạch và hợp tác mã nguồn mở. Đề án tìm cách tái tạo các tập dữ liệu và quy trình huấn luyện của DeepSeek-R1, hiện đang chưa được công bố, bằng cách sử dụng học tăng cường (RL) mà không cần sự giám sát của con người. Open-R1 khuyến khích sự đóng góp từ cộng đồng để mở rộng ứng dụng của mô hình ra ngoài lĩnh vực toán học, bao gồm các lĩnh vực như lập trình và y học.

### [phản ứng](https://news.ycombinator.com/item?id=42849536)

Open-R1 là một sáng kiến nhằm tái tạo mô hình DeepSeek-R1 bằng cách sử dụng các nguyên tắc mã nguồn mở, mặc dù hiện tại nó chưa phải là một mô hình thực sự. Cuộc thảo luận nhấn mạnh những thách thức và lợi ích tiềm năng của việc tái tạo các mô hình AI với ngân sách hạn chế, cũng như tác động của AI đối với giáo dục và những hệ quả xã hội rộng lớn hơn. Cuộc trò chuyện cũng làm nổi bật sự phấn khích xung quanh những tiến bộ công nghệ và vai trò của phong trào mã nguồn mở trong việc làm cho AI trở nên dễ tiếp cận hơn với đông đảo khán giả.

## [The future of Rebble](https://rebble.io/2025/01/27/the-future-of-rebble.html)

### [phản ứng](https://news.ycombinator.com/item?id=42845017)

Cuộc thảo luận nhấn mạnh sự hoài niệm về đồng hồ thông minh Pebble, được đánh giá cao nhờ màn hình giống e-ink và thời lượng pin dài, và đặt câu hỏi tại sao công nghệ tương tự chưa được áp dụng rộng rãi hơn. Hiện có sự quan tâm đến tiềm năng của phần cứng mới từ Rebble, một dự án do cộng đồng điều hành, và tính chất mã nguồn mở của các dự án đồng hồ thông minh liên quan. Những lựa chọn thay thế như Watchy và PineTime được đề cập, với người dùng lưu ý đến những thách thức về phần mềm mà họ phải đối mặt trong không gian đồng hồ thông minh mã nguồn mở.

## [The Alpha Myth: Cách những con sói nuôi nhốt đã dẫn chúng ta đi sai đường](https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves)

### [phản ứng](https://news.ycombinator.com/item?id=42844619)

Khái niệm "con sói đầu đàn" ban đầu dựa trên các nghiên cứu trong điều kiện nuôi nhốt đã bị bác bỏ; các bầy sói hoang dã hoạt động giống như các đơn vị gia đình hơn là các cấu trúc phân cấp. Mặc dù đã bị bác bỏ, ý tưởng "alpha" vẫn tồn tại do sức hấp dẫn của nó trong các môi trường cạnh tranh, chẳng hạn như Thung lũng Silicon, và sự cộng hưởng của nó với một số nhu cầu xã hội và tâm lý nhất định. Niềm tin tiếp tục vào huyền thoại "alpha" nhấn mạnh cách mà các câu chuyện có thể ảnh hưởng đến nhận thức của chúng ta về động lực xã hội, ngay cả khi chúng được xây dựng trên những giả định sai lầm.

## [Go 1.24's go tool là một trong những bổ sung tốt nhất cho hệ sinh thái trong nhiều năm qua](https://www.jvt.me/posts/2025/01/27/go-tools-124/)

Go 1.24 giới thiệu một lệnh `go tool` mới và chỉ thị `tool` trong `go.mod`, nâng cao việc quản lý các công cụ dự án trong hệ sinh thái Go. Phần cập nhật này giải quyết các vấn đề với mẫu `tools.go`, chẳng hạn như ảnh hưởng đến hiệu suất và sự phình to của cây phụ thuộc, bằng cách cho phép quản lý công cụ hiệu quả hơn và giảm bớt các phụ thuộc không cần thiết. Trong khi lệnh `go tool` cải thiện hiệu suất bằng cách lưu vào bộ nhớ đệm các lần gọi `go run`, có những lo ngại về việc các phụ thuộc công cụ được xử lý như gián tiếp, có thể dẫn đến xung đột phụ thuộc.

### [phản ứng](https://news.ycombinator.com/item?id=42845323)

Việc giới thiệu "go tool" trong Go 1.24 đã dẫn đến các cuộc tranh luận về tác động của nó đối với quản lý phụ thuộc, với những lo ngại về việc hợp nhất công cụ và phụ thuộc dự án gây ra xung đột. Những người phê bình đề xuất các giải pháp thay thế như các tệp mô-đun riêng biệt hoặc sử dụng các công cụ như Nix để cải thiện việc kiểm soát phiên bản. Những người ủng hộ cách tiếp cận của Go lập luận rằng nó mang lại sự đơn giản và hiệu quả, phản ánh những thách thức rộng lớn hơn trong quản lý phụ thuộc trên các ngôn ngữ lập trình.

## [Tôi đã tin tưởng một LLM, giờ tôi đang ở ngày thứ 4 của một dự án buổi chiều](https://nemo.foo/blog/day-4-of-an-afternoon-project)

Người tác giả đã bắt tay vào một dự án có tên là Deskthang, với ý định tạo ra một thiết bị để bàn sử dụng Raspberry Pi Pico, màn hình LCD và đèn LED RGB, đồng thời kiểm tra khả năng của AI. Những công cụ AI như ChatGPT và Claude ban đầu đã hỗ trợ nhưng cuối cùng dẫn đến một triển khai lỗi, gây ra các vấn đề như xung đột bộ đệm và hỏng dữ liệu. Những bài học quan trọng đã học được bao gồm việc nhận ra AI như một công cụ hơn là một người đồng hành, hiểu giá trị của sự cọ xát và sai lầm trong học tập, và tầm quan trọng của sự kiên nhẫn hơn là tự tin thái quá.

### [phản ứng](https://news.ycombinator.com/item?id=42845933)

Những Mô hình Ngôn ngữ Lớn (LLMs) có thể có lợi cho các nhiệm vụ đơn giản nhưng có thể kéo dài thời gian dự án nếu dựa vào chúng cho các vấn đề phức tạp mà không có sự giám sát thích hợp. Họ có hiệu quả trong việc tổng hợp thông tin nhưng có thể gặp khó khăn với các chủ đề ngách hoặc kiến thức mới, đòi hỏi người dùng phải có nền tảng vững chắc và kinh nghiệm. Người dùng phải duy trì quyền kiểm soát bằng cách cung cấp các hướng dẫn rõ ràng và xem xét kỹ lưỡng các kết quả đầu ra để khai thác hiệu quả tiềm năng đầy đủ của các mô hình ngôn ngữ lớn (LLM).

## [Nvidia mất gần 600 tỷ USD vốn hóa thị trường](https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html)

Nvidia đã chịu một tổn thất lịch sử về vốn hóa thị trường gần 600 tỷ USD, với cổ phiếu giảm 17% do lo ngại cạnh tranh từ phòng thí nghiệm AI Trung Quốc DeepSeek. Việc bán tháo đã ảnh hưởng đến toàn bộ lĩnh vực công nghệ của Mỹ, gây ra sự sụt giảm ở các công ty như Dell và Oracle, và góp phần vào mức giảm 3,1% của chỉ số Nasdaq. Việc phát triển mô hình AI mới của DeepSeek, sử dụng chip H800 của Nvidia, đã làm gia tăng lo ngại về cạnh tranh, ảnh hưởng đến cổ phiếu của Nvidia mặc dù trước đó đã có những tăng trưởng, và làm giảm giá trị tài sản ròng của CEO Jensen Huang xuống 21 tỷ USD.

### [phản ứng](https://news.ycombinator.com/item?id=42845681)

Nvidia đã chứng kiến vốn hóa thị trường giảm mạnh gần 600 tỷ USD, dẫn đến các cuộc tranh luận về định giá của công ty và liệu nó có bị định giá quá cao hay không. Mặc dù phản ứng của thị trường, các GPU của Nvidia vẫn tiếp tục đóng vai trò quan trọng cho các nhiệm vụ liên quan đến AI, nhấn mạnh tầm quan trọng của chúng trong ngành công nghệ. Việc truyền thông tập trung vào các khoản lỗ tài chính lớn mà không xem xét lạm phát có thể gây hiểu lầm, nhưng sự suy giảm của Nvidia vẫn đáng chú ý ngay cả trong số các tập đoàn lớn.

## [Janus Pro 1B chạy hoàn toàn cục bộ trong trình duyệt trên WebGPU](https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/)

### [phản ứng](https://news.ycombinator.com/item?id=42852400)

Janus Pro 1B là một mô hình chạy cục bộ trong trình duyệt sử dụng WebGPU, thể hiện khả năng thực thi các mô hình AI trong môi trường trình duyệt. Mặc dù có số lượng tham số thấp, điều này giới hạn khả năng của nó, mô hình có thể chạy trên các GPU cấp thấp, nhấn mạnh tính khả dụng của nó. Mặc dù kết quả tạo hình ảnh không nhất quán, khả năng chạy các mô hình như vậy cục bộ trong trình duyệt là một tiến bộ công nghệ đáng kể, mặc dù hiện tại nó không hỗ trợ các thiết bị di động.

## [Nhà nghiên cứu Berkeley tái tạo công nghệ cốt lõi của DeepSeek R1 chỉ với 30 đô la: Một sửa đổi nhỏ](https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek)

### [phản ứng](https://news.ycombinator.com/item?id=42855283)

Những nhà nghiên cứu tại Berkeley đã thành công trong việc tái tạo công nghệ cốt lõi của DeepSeek R1 chỉ với 30 đô la, tập trung vào các nhiệm vụ cụ thể như chơi trò chơi Countdown. Đổi mới này liên quan đến việc sử dụng học tăng cường, một loại học máy mà trong đó một tác nhân học bằng cách tương tác với môi trường của nó, để nâng cao các mô hình lý luận, mặc dù ứng dụng của nó bị giới hạn trong các lĩnh vực có giải pháp có thể kiểm chứng. Cuộc thảo luận nhấn mạnh tiềm năng của AI trong việc tự cải thiện và những tác động của nó đối với sự phát triển AI trong tương lai, mặc dù có những chỉ trích về tiêu đề gây hiểu lầm của bài viết và thiếu các liên kết nguồn thích hợp.

<head>
  <meta property="og:title" content="Chúng tôi đang mang Pebble trở lại" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://og.cho.sh/api/og/?title=Ch%C3%BAng%20t%C3%B4i%20%C4%91ang%20mang%20Pebble%20tr%E1%BB%9F%20l%E1%BA%A1i&subheading=Th%E1%BB%A9%20Ba%2C%2028%20th%C3%A1ng%201%2C%202025%3A%20T%C3%B3m%20t%E1%BA%AFt%20tin%20t%E1%BB%A9c%20v%E1%BB%81%20hacker" />
</head>
